# Paper List of Terms(reward model+Reinforcement learning)
- [25/07] **Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities**  
[[Paper](http://arxiv.org/pdf/2507.13158v1)] [[Code/Page]()] [[TLDR/Notes](#inverse-reinforcement-learning-meets-large-language-model-post-training--basics--advances--and-opportunities)]

- [25/07] **Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**  
[[Paper](http://arxiv.org/pdf/2507.11155v1)] [[Code/Page]()] [[TLDR/Notes](#bridging-the-gap-in-vision-language-models-in-identifying-unsafe-concepts-across-modalities)]

- [25/07] **Tiny Reward Models**  
[[Paper](http://arxiv.org/pdf/2507.09973v1)] [[Code/Page]()] [[TLDR/Notes](#tiny-reward-models)]

- [25/07] **Enhancing RLHF with Human Gaze Modeling**  
[[Paper](http://arxiv.org/pdf/2507.09016v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-rlhf-with-human-gaze-modeling)]

- [25/07] **One Token to Fool LLM-as-a-Judge**  
[[Paper](http://arxiv.org/pdf/2507.08794v1)] [[Code/Page](https://huggingface.co/sarosavo/Master-RM)] [[TLDR/Notes](#one-token-to-fool-llm-as-a-judge)]

- [25/07] **Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization**  
[[Paper](http://arxiv.org/pdf/2507.07723v1)] [[Code/Page]()] [[TLDR/Notes](#stable-preference-optimization-for-llms--a-bilevel-approach-beyond-direct-preference-optimization)]

- [25/07] **Bradley-Terry and Multi-Objective Reward Modeling Are Complementary**  
[[Paper](http://arxiv.org/pdf/2507.07375v1)] [[Code/Page]()] [[TLDR/Notes](#bradley-terry-and-multi-objective-reward-modeling-are-complementary)]

- [25/07] **Perception-Aware Policy Optimization for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2507.06448v2)] [[Code/Page](https://mikewangwzhl.github.io/PAPO.)] [[TLDR/Notes](#perception-aware-policy-optimization-for-multimodal-reasoning)]

- [25/07] **Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease**  
[[Paper](http://arxiv.org/pdf/2507.06326v1)] [[Code/Page]()] [[TLDR/Notes](#sample-efficient-reinforcement-learning-controller-for-deep-brain-stimulation-in-parkinson-s-disease)]

- [25/07] **A Technical Survey of Reinforcement Learning Techniques for Large Language Models**  
[[Paper](http://arxiv.org/pdf/2507.04136v1)] [[Code/Page]()] [[TLDR/Notes](#a-technical-survey-of-reinforcement-learning-techniques-for-large-language-models)]

- [25/07] **ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization**  
[[Paper](http://arxiv.org/pdf/2507.03069v1)] [[Code/Page]()] [[TLDR/Notes](#arf-rlhf--adaptive-reward-following-for-rlhf-through-emotion-driven-self-supervision-and-trace-biased-dynamic-optimization)]

- [25/07] **Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2507.01551v2)] [[Code/Page]()] [[TLDR/Notes](#self-guided-process-reward-optimization-with-redefined-step-wise-advantage-for-process-reinforcement-learning)]

- [25/07] **Activation Reward Models for Few-Shot Model Alignment**  
[[Paper](http://arxiv.org/pdf/2507.01368v1)] [[Code/Page]()] [[TLDR/Notes](#activation-reward-models-for-few-shot-model-alignment)]

- [25/07] **Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy**  
[[Paper](http://arxiv.org/pdf/2507.01352v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy)]

- [25/07] **SAFER: Probing Safety in Reward Models with Sparse Autoencoder**  
[[Paper](http://arxiv.org/pdf/2507.00665v1)] [[Code/Page](https://github.com/xzy-101/SAFER-code.)] [[TLDR/Notes](#safer--probing-safety-in-reward-models-with-sparse-autoencoder)]

- [25/07] **Residual Reward Models for Preference-based Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2507.00611v1)] [[Code/Page](https://sunlighted.github.io/RRM-web/.)] [[TLDR/Notes](#residual-reward-models-for-preference-based-reinforcement-learning)]

- [25/06] **Generalist Reward Models: Found Inside Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.23235v1)] [[Code/Page]()] [[TLDR/Notes](#generalist-reward-models--found-inside-large-language-models)]

- [25/06] **Listener-Rewarded Thinking in VLMs for Image Preferences**  
[[Paper](http://arxiv.org/pdf/2506.22832v2)] [[Code/Page](https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.)] [[TLDR/Notes](#listener-rewarded-thinking-in-vlms-for-image-preferences)]

- [25/06] **TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.22008v1)] [[Code/Page]()] [[TLDR/Notes](#trofi--trajectory-ranked-offline-inverse-reinforcement-learning)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.18841v1)] [[Code/Page](https://huggingface.co/THU-KEG/LongWriter-Zero-32B)] [[TLDR/Notes](#longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning)]

- [25/06] **Reflective Verbal Reward Design for Pluralistic Alignment**  
[[Paper](http://arxiv.org/pdf/2506.17834v1)] [[Code/Page]()] [[TLDR/Notes](#reflective-verbal-reward-design-for-pluralistic-alignment)]

- [25/06] **ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2506.16712v1)] [[Code/Page]()] [[TLDR/Notes](#reasongrm--enhancing-generative-reward-models-through-large-reasoning-models)]

- [25/06] **AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning**  
[[Paper](http://arxiv.org/pdf/2506.15651v1)] [[Code/Page](https://github.com/cxcscmu/AutoRule.)] [[TLDR/Notes](#autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning)]

- [25/06] **Reward Models in Deep Reinforcement Learning: A Survey**  
[[Paper](http://arxiv.org/pdf/2506.15421v1)] [[Code/Page]()] [[TLDR/Notes](#reward-models-in-deep-reinforcement-learning--a-survey)]

- [25/06] **Adaptive Accompaniment with ReaLchords**  
[[Paper](http://arxiv.org/pdf/2506.14723v1)] [[Code/Page]()] [[TLDR/Notes](#adaptive-accompaniment-with-realchords)]

- [25/06] **SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.14648v1)] [[Code/Page]()] [[TLDR/Notes](#senior--efficient-query-selection-and-preference-guided-exploration-in-preference-based-reinforcement-learning)]

- [25/06] **TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization**  
[[Paper](http://arxiv.org/pdf/2506.14574v1)] [[Code/Page](https://github.com/dvlab-research/TGDPO.)] [[TLDR/Notes](#tgdpo--harnessing-token-level-reward-guidance-for-enhancing-direct-preference-optimization)]

- [25/06] **GRAM: A Generative Foundation Reward Model for Reward Generalization**  
[[Paper](http://arxiv.org/pdf/2506.14175v2)] [[Code/Page]()] [[TLDR/Notes](#gram--a-generative-foundation-reward-model-for-reward-generalization)]

- [25/06] **VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training**  
[[Paper](http://arxiv.org/pdf/2506.13888v1)] [[Code/Page]()] [[TLDR/Notes](#vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.13741v1)] [[Code/Page]()] [[TLDR/Notes](#pb$^2$--preference-space-exploration-via-population-based-methods-in-preference-based-reinforcement-learning)]

- [25/06] **$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts**  
[[Paper](http://arxiv.org/pdf/2506.15733v1)] [[Code/Page]()] [[TLDR/Notes](#$\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts)]

- [25/06] **Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory**  
[[Paper](http://arxiv.org/pdf/2506.12350v1)] [[Code/Page]()] [[TLDR/Notes](#theoretical-tensions-in-rlhf--reconciling-empirical-success-with-inconsistencies-in-social-choice-theory)]

- [25/06] **TreeRL: LLM Reinforcement Learning with On-Policy Tree Search**  
[[Paper](http://arxiv.org/pdf/2506.11902v1)] [[Code/Page](https://github.com/THUDM/TreeRL.)] [[TLDR/Notes](#treerl--llm-reinforcement-learning-with-on-policy-tree-search)]

- [25/06] **Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11425v2)] [[Code/Page]()] [[TLDR/Notes](#agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards)]

- [25/06] **Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.21560v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning)]

- [25/06] **Unsupervised Elicitation of Language Models**  
[[Paper](http://arxiv.org/pdf/2506.10139v1)] [[Code/Page]()] [[TLDR/Notes](#unsupervised-elicitation-of-language-models)]

- [25/06] **Learning to Reason Across Parallel Samples for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.09014v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-across-parallel-samples-for-llm-reasoning)]

- [25/06] **GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO**  
[[Paper](http://arxiv.org/pdf/2506.08965v1)] [[Code/Page]()] [[TLDR/Notes](#gfriend--generative-few-shot-reward-inference-through-efficient-dpo)]

- [25/06] **Intra-Trajectory Consistency for Reward Modeling**  
[[Paper](http://arxiv.org/pdf/2506.09096v3)] [[Code/Page](https://github.com/chaoyang101/ICRM.)] [[TLDR/Notes](#intra-trajectory-consistency-for-reward-modeling)]

- [25/06] **Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints**  
[[Paper](http://arxiv.org/pdf/2506.08266v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-from-human-feedback-with-high-confidence-safety-constraints)]

- [25/06] **Explicit Preference Optimization: No Need for an Implicit Reward Model**  
[[Paper](http://arxiv.org/pdf/2506.07492v1)] [[Code/Page]()] [[TLDR/Notes](#explicit-preference-optimization--no-need-for-an-implicit-reward-model)]

- [25/06] **ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.07459v2)] [[Code/Page]()] [[TLDR/Notes](#proteinzero--self-improving-protein-generation-via-online-reinforcement-learning)]

- [25/06] **Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance**  
[[Paper](http://arxiv.org/pdf/2506.05748v1)] [[Code/Page]()] [[TLDR/Notes](#efficient-online-rft-with-plug-and-play-llm-judges--unlocking-state-of-the-art-performance)]

- [25/06] **Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models**  
[[Paper](http://arxiv.org/pdf/2506.06395v3)] [[Code/Page]()] [[TLDR/Notes](#confidence-is-all-you-need--few-shot-rl-fine-tuning-of-language-models)]

- [25/06] **Customizing Speech Recognition Model with Large Language Model Feedback**  
[[Paper](http://arxiv.org/pdf/2506.11091v1)] [[Code/Page]()] [[TLDR/Notes](#customizing-speech-recognition-model-with-large-language-model-feedback)]

- [25/06] **TreeRPO: Tree Relative Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2506.05183v1)] [[Code/Page](https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.)] [[TLDR/Notes](#treerpo--tree-relative-policy-optimization)]

- [25/06] **RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation**  
[[Paper](http://arxiv.org/pdf/2506.05070v1)] [[Code/Page]()] [[TLDR/Notes](#rival--reinforcement-learning-with-iterative-and-adversarial-optimization-for-machine-translation)]

- [25/06] **LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.04821v1)] [[Code/Page]()] [[TLDR/Notes](#logicpuzzlerl--cultivating-robust-mathematical-reasoning-in-llms-via-reinforcement-learning)]



# TLDR/Notes
## inverse-reinforcement-learning-meets-large-language-model-post-training--basics--advances--and-opportunities
### Abstract
In the era of Large Language Models (LLMs), alignment has emerged as a
fundamental yet challenging problem in the pursuit of more reliable,
controllable, and capable machine intelligence. The recent success of reasoning
models and conversational AI systems has underscored the critical role of
reinforcement learning (RL) in enhancing these systems, driving increased
research interest at the intersection of RL and LLM alignment. This paper
provides a comprehensive review of recent advances in LLM alignment through the
lens of inverse reinforcement learning (IRL), emphasizing the distinctions
between RL techniques employed in LLM alignment and those in conventional RL
tasks. In particular, we highlight the necessity of constructing neural reward
models from human data and discuss the formal and practical implications of
this paradigm shift. We begin by introducing fundamental concepts in RL to
provide a foundation for readers unfamiliar with the field. We then examine
recent advances in this research agenda, discussing key challenges and
opportunities in conducting IRL for LLM alignment. Beyond methodological
considerations, we explore practical aspects, including datasets, benchmarks,
evaluation metrics, infrastructure, and computationally efficient training and
inference techniques. Finally, we draw insights from the literature on
sparse-reward RL to identify open questions and potential research directions.
By synthesizing findings from diverse studies, we aim to provide a structured
and critical overview of the field, highlight unresolved challenges, and
outline promising future directions for improving LLM alignment through RL and
IRL techniques.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹å¯¹é½ä¸­çš„é€†å¼ºåŒ–å­¦ä¹ ï¼šåŸºç¡€ã€è¿›å±•ä¸æœºé‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ä»£ï¼Œå¯¹é½ï¼ˆalignmentï¼‰å·²æˆä¸ºè¿½æ±‚æ›´å¯é ã€å¯æ§å’Œæœ‰èƒ½åŠ›çš„æœºå™¨æ™ºèƒ½è¿‡ç¨‹ä¸­ä¸€ä¸ªåŸºç¡€ä¸”å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œå¤§è§„æ¨¡æ•°æ®é©±åŠ¨æ¨¡å‹ï¼ˆå¦‚LLMï¼‰åœ¨å¤šé¢†åŸŸå–å¾—æˆåŠŸï¼Œä½†LLMå­˜åœ¨æ— æ³•è‡ªä¸»æŒç»­æ”¹è¿›ç­‰ä¸è¶³ï¼›å¦ä¸€æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼—å¤šé¢†åŸŸå±•ç°è¶…äººç±»æ€§èƒ½å´é¢ä¸´ç³»ç»Ÿé€æ˜æ€§ç­‰æŒ‘æˆ˜ã€‚é‰´äºRLå’ŒLLMåœ¨å„è‡ªé¢†åŸŸçš„æˆåŠŸï¼Œå°†äºŒè€…ç»“åˆæå…·å‰æ™¯ã€‚åŒæ—¶ï¼Œå½“å‰åœ¨å°†RLæ‰©å±•åˆ°æ›´å¹¿æ³›LLMä»»åŠ¡å’Œåº”ç”¨ä¸­å­˜åœ¨ç¼ºä¹å¥–åŠ±ä¿¡å·ã€è®¡ç®—æˆæœ¬é«˜ã€RLç®—æ³•éœ€é€‚é…LLMå¯¹é½ä»»åŠ¡ç‰¹æ€§ç­‰å…³é”®æŒ‘æˆ˜ï¼Œå› æ­¤æœ¬æ–‡ä»é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰è§†è§’å…¨é¢ç»¼è¿°LLMå¯¹é½çš„æœ€æ–°è¿›å±•ï¼Œè¯•å›¾å¼¥åˆIRLä¸LLMå¯¹é½é—´çš„å·®è·ä»¥åŠ©åŠ›æœªæ¥ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»¥é€†å¼ºåŒ–å­¦ä¹ è§†è§’ç»¼è¿°LLMå¯¹é½è¿›å±•  
æœ¬æ–‡èšç„¦äºä»é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰çš„è§’åº¦æ¥å®¡è§†å¤§è¯­è¨€æ¨¡å‹å¯¹é½çš„æœ€æ–°ç ”ç©¶æˆæœï¼Œçªå‡ºLLMå¯¹é½ä¸­æ‰€ç”¨RLæŠ€æœ¯ä¸ä¼ ç»ŸRLä»»åŠ¡ä¸­æŠ€æœ¯çš„åŒºåˆ«ï¼Œå¼ºè°ƒä»äººç±»æ•°æ®æ„å»ºç¥ç»å¥–åŠ±æ¨¡å‹çš„å¿…è¦æ€§ï¼Œå¹¶æ¢è®¨è¿™ç§èŒƒå¼è½¬å˜åœ¨å½¢å¼å’Œå®è·µä¸Šçš„å½±å“ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†å…¨æ–°çš„å®¡è§†ç»´åº¦ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢è¦†ç›–å¤šæ–¹é¢å†…å®¹æ„å»ºçŸ¥è¯†ä½“ç³»  
å…ˆä¸ºä¸ç†Ÿæ‚‰RLé¢†åŸŸçš„è¯»è€…ä»‹ç»RLåŸºæœ¬æ¦‚å¿µï¼ˆå¦‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç­‰ï¼‰å¥ å®šåŸºç¡€ï¼›æ¥ç€å®¡è§†è¯¥ç ”ç©¶è®®ç¨‹çš„æœ€æ–°è¿›å±•ï¼Œè®¨è®ºä¸ºLLMå¯¹é½è¿›è¡ŒIRLçš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ï¼›é™¤æ–¹æ³•è€ƒé‡å¤–ï¼Œè¿˜æ¢ç´¢æ•°æ®é›†ã€åŸºå‡†ã€è¯„ä¼°æŒ‡æ ‡ã€åŸºç¡€è®¾æ–½ä»¥åŠè®¡ç®—é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ç­‰å®è·µæ–¹é¢ï¼›æœ€åä»ç¨€ç–å¥–åŠ±RLæ–‡çŒ®ä¸­æ±²å–è§è§£ä»¥ç¡®å®šå¼€æ”¾æ€§é—®é¢˜å’Œæ½œåœ¨ç ”ç©¶æ–¹å‘ï¼Œå…¨æ–¹ä½æ„å»ºèµ·LLMå¯¹é½ä¸IRLç»“åˆé¢†åŸŸçš„çŸ¥è¯†ä½“ç³»ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªæ˜ç¡®æåŠä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®éªŒç»“æœå‘ˆç°ï¼ˆå¦‚å¯¹æ¯”å®éªŒæ•°æ®ç­‰ï¼‰ï¼Œä¸»è¦æ˜¯ä»ç»¼è¿°è§’åº¦æ¢³ç†é¢†åŸŸå†…ç›¸å…³å·¥ä½œã€æŒ‘æˆ˜ä¸æœºé‡ç­‰å†…å®¹ï¼Œé€šè¿‡å¯¹RLåœ¨å¯¹è¯AIï¼ˆå¦‚RLHFæå‡LLMèƒ½åŠ›ï¼‰ã€æ•°å­¦æ¨ç†ï¼ˆå¦‚AlphaProofç­‰åœ¨æ•°å­¦ç«èµ›è¡¨ç°ï¼‰ç­‰åœºæ™¯åº”ç”¨çš„åˆ†æï¼Œå±•ç°RLä¸LLMç»“åˆçš„æ½œåŠ›ä¸ç°çŠ¶ï¼Œä¸ºåç»­ç ”ç©¶æä¾›å‚è€ƒä¾æ®ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸäº¤å‰è§†è§’ï¼šå°†é€†å¼ºåŒ–å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹å¯¹é½ç»“åˆè¿›è¡Œç»¼è¿°ï¼Œä¸ºç ”ç©¶è€…æä¾›äº†è·¨é¢†åŸŸèåˆçš„æ€è€ƒè§’åº¦ï¼Œå¯å‘åœ¨AIä¸åŒå­é¢†åŸŸé—´å¯»æ‰¾å…³è”ä¸åˆ›æ–°ç‚¹ã€‚  
2. çŸ¥è¯†ä½“ç³»æ„å»ºï¼šä»åŸºç¡€æ¦‚å¿µåˆ°å‰æ²¿è¿›å±•ï¼Œå†åˆ°å®è·µå±‚é¢ï¼ˆæ•°æ®é›†ã€åŸºç¡€è®¾æ–½ç­‰ï¼‰å’Œæœªæ¥æ–¹å‘ï¼Œå®Œæ•´çš„çŸ¥è¯†è„‰ç»œæ¢³ç†æœ‰åŠ©äºæ–°æ‰‹å¿«é€Ÿå…¥é—¨è¯¥é¢†åŸŸï¼Œä¹Ÿè®©èµ„æ·±ç ”ç©¶è€…å…¨é¢æŠŠæ¡é¢†åŸŸç°çŠ¶ä¸è¶‹åŠ¿ã€‚  
3. æŒ‘æˆ˜ä¸æœºé‡åˆ†æï¼šå¯¹LLMå¯¹é½ä¸­RLåº”ç”¨é¢ä¸´çš„ç¼ºä¹å¥–åŠ±ä¿¡å·ã€è®¡ç®—æˆæœ¬ã€ç®—æ³•é€‚é…ç­‰æŒ‘æˆ˜çš„å‰–æï¼Œä»¥åŠå¯¹æ½œåœ¨è§£å†³æ–¹å‘çš„æ¢è®¨ï¼Œä¸ºåç»­ç ”ç©¶é€‰é¢˜å’ŒæŠ€æœ¯çªç ´æä¾›äº†æ¸…æ™°çš„é—®é¢˜å¯¼å‘ã€‚

## bridging-the-gap-in-vision-language-models-in-identifying-unsafe-concepts-across-modalities
### Abstract
Vision-language models (VLMs) are increasingly applied to identify unsafe or
inappropriate images due to their internal ethical standards and powerful
reasoning abilities. However, it is still unclear whether they can recognize
various unsafe concepts when presented in different modalities, such as text
and images. To address this, we first compile the UnsafeConcepts dataset,
featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and
``Assaults,'' along with associated 1.5K images. We then conduct a systematic
evaluation of VLMs' perception (concept recognition) and alignment (ethical
reasoning) capabilities. We assess eight popular VLMs and find that, although
most VLMs accurately perceive unsafe concepts, they sometimes mistakenly
classify these concepts as safe. We also identify a consistent modality gap
among open-source VLMs in distinguishing between visual and textual unsafe
concepts. To bridge this gap, we introduce a simplified reinforcement learning
(RL)-based approach using proximal policy optimization (PPO) to strengthen the
ability to identify unsafe concepts from images. Our approach uses reward
scores based directly on VLM responses, bypassing the need for collecting
human-annotated preference data to train a new reward model. Experimental
results show that our approach effectively enhances VLM alignment on images
while preserving general capabilities. It outperforms baselines such as
supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope
our dataset, evaluation findings, and proposed alignment solution contribute to
the community's efforts in advancing safe VLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è·¨æ¨¡æ€è¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µï¼Œå¼¥åˆè§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›é¸¿æ²Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å†…å®¹å®¡æ ¸ç­‰ç°å®åœºæ™¯çš„å¹¿æ³›åº”ç”¨ï¼Œç¡®ä¿å…¶èƒ½å¯é è¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µè‡³å…³é‡è¦ã€‚ç„¶è€Œç›®å‰å­˜åœ¨ä¸¤å¤§å…³é”®é—®é¢˜ï¼šä¸€æ˜¯VLMsèƒ½å¦æœ‰æ•ˆè¯†åˆ«å„ç±»ä¸å®‰å…¨æ¦‚å¿µï¼ŸäºŒæ˜¯å½“ä¸å®‰å…¨ä¿¡æ¯ä»¥æ–‡æœ¬æˆ–å›¾åƒç­‰ä¸åŒæ¨¡æ€å‘ˆç°æ—¶ï¼Œæ˜¯å¦å­˜åœ¨â€œæ¨¡æ€é¸¿æ²Ÿâ€ï¼ˆå³å¯¹æ–‡æœ¬å’Œå›¾åƒä¸­åŒç±»ä¸å®‰å…¨æ¦‚å¿µåˆ¤æ–­ä¸ä¸€è‡´ï¼‰ï¼Ÿè‹¥å­˜åœ¨ï¼Œåˆè¯¥å¦‚ä½•åœ¨ä¸å½±å“æ¨¡å‹é€šç”¨èƒ½åŠ›çš„å‰æä¸‹å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Ÿè¿™äº›é—®é¢˜å…³ä¹VLMsçš„å®‰å…¨æ€§ä¸ä¼¦ç†åˆè§„æ€§ï¼Œæ¨åŠ¨äº†æœ¬æ–‡ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºUnsafeConceptsæ•°æ®é›†  
é¦–æ¬¡æ•´åˆè¦†ç›–9å¤§ç±»åˆ«ã€75ä¸ªç»†ç²’åº¦ä¸å®‰å…¨æ¦‚å¿µï¼ˆå¦‚â€œçº³ç²¹ä¸‡å­—ç¬¦â€â€œæ€§éªšæ‰°â€â€œæš´åŠ›è¢­å‡»â€ç­‰ï¼‰åŠå¯¹åº”1500å¼ å›¾åƒçš„æ•°æ®é›†ï¼Œä¸ºè¯„ä¼°VLMsè¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µèƒ½åŠ›æä¾›äº†å…¨é¢ä¸”ç²¾ç»†æ ‡æ³¨çš„èµ„æºï¼ŒåŒºåˆ†äº†â€œè§†è§‰ä¸å®‰å…¨æ¦‚å¿µâ€ï¼ˆå›¾åƒå‘ˆç°çš„ä¸å®‰å…¨ä¿¡æ¯ï¼‰ä¸â€œæ–‡æœ¬ä¸å®‰å…¨æ¦‚å¿µâ€ï¼ˆæ–‡å­—æè¿°çš„ä¸å®‰å…¨ä¿¡æ¯ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç³»ç»Ÿæ€§è¯„ä¼°VLMsåŒæ ¸å¿ƒèƒ½åŠ›  
é’ˆå¯¹8æ¬¾ä¸»æµVLMsï¼ˆå«å¼€æºä¸å•†ç”¨ï¼‰ï¼Œä»**æ„ŸçŸ¥ï¼ˆPerceptionï¼‰**å’Œ**å¯¹é½ï¼ˆAlignmentï¼‰**ä¸¤å¤§ç»´åº¦è¯„ä¼°ï¼šæ„ŸçŸ¥èšç„¦æ¨¡å‹æ£€æµ‹å›¾åƒä¸­ä¸å®‰å…¨æ¦‚å¿µæ˜¯å¦å­˜åœ¨çš„èƒ½åŠ›ï¼ˆè®¾è®¡å•é€‰æ‹©é¢˜è®©æ¨¡å‹åŒ¹é…æ¦‚å¿µï¼‰ï¼›å¯¹é½åˆ™è¡¡é‡æ¨¡å‹åˆ¤æ–­æ˜¯å¦å¥‘åˆäººç±»ä¼¦ç†æ ‡å‡†ï¼ˆè®¾è®¡æç¤ºè¯è¯¢é—®åœ¨ç¤¾äº¤å¹³å°ç­‰åœºæ™¯æ˜¯å¦å®‰å…¨ï¼‰ï¼Œè¿˜çº³å…¥åœºæ™¯åŒ–è¯­å¢ƒåˆ†æå¯¹å¯¹é½è¡¨ç°çš„å½±å“ï¼Œæ¸…æ™°æ­ç¤ºç°æœ‰VLMsåœ¨è·¨æ¨¡æ€è¯†åˆ«æ—¶çš„èƒ½åŠ›ä¸çŸ­æ¿ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç®€åŒ–å¼ºåŒ–å­¦ä¹ å¯¹é½æ–¹æ³•  
ä¸ºå¼¥åˆæ¨¡æ€é¸¿æ²Ÿï¼Œæå‡ºåŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„ç®€åŒ–å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆã€‚æ— éœ€æ”¶é›†äººå·¥æ ‡æ³¨åå¥½æ•°æ®è®­ç»ƒæ–°å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ç”¨å“åº”åˆ†ç±»å™¨è¯„ä¼°VLMå›ç­”çš„æ­£ç¡®æ€§å¹¶åˆ†é…å¥–åŠ±åˆ†ï¼Œé€šè¿‡â€œç”Ÿæˆï¼ˆroll - outï¼‰- è¯„ä¼° - ä¼˜åŒ–â€ä¸‰é˜¶æ®µè¿­ä»£æ›´æ–°æ¨¡å‹ï¼šç”Ÿæˆé˜¶æ®µé‡‡æ ·æ¨¡å‹å¯¹å®‰å…¨/ä¸å®‰å…¨æ¦‚å¿µçš„å“åº”ï¼›è¯„ä¼°é˜¶æ®µç”¨åˆ†ç±»å™¨åˆ¤æ­£è¯¯å¹¶æ‰“ rewardï¼›ä¼˜åŒ–é˜¶æ®µç»“åˆå¥–åŠ±ã€ç†µå¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰ä¸KLæ•£åº¦ï¼ˆé˜²æ­¢åç¦»åŸèƒ½åŠ›ï¼‰ä¼˜åŒ–æ¨¡å‹ï¼Œåœ¨å¼ºåŒ–å›¾åƒç«¯ä¸å®‰å…¨æ¦‚å¿µè¯†åˆ«å¯¹é½æ€§çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™é€šç”¨èƒ½åŠ›ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ„ŸçŸ¥ä¸å¯¹é½èƒ½åŠ›åˆ†ç¦»ï¼šå¤šæ•°VLMsåœ¨æ„ŸçŸ¥ä»»åŠ¡ï¼ˆè¯†åˆ«å›¾åƒé‡Œæœ‰æ— ä¸å®‰å…¨æ¦‚å¿µï¼‰å¾—åˆ†é«˜ï¼ˆå¦‚LLaVA - 7Bè¾¾0.93ï¼‰ï¼Œä½†å¯¹é½ä»»åŠ¡ï¼ˆåˆ¤æ–­è¯¥å†…å®¹åœ¨ç¤¾äº¤ç­‰åœºæ™¯æ˜¯å¦å®‰å…¨ï¼‰å¾—åˆ†éª¤é™ï¼ˆLLaVA - 7Bä»…0.37ï¼‰ï¼Œè¯´æ˜â€œèƒ½è¯†åˆ«æ¦‚å¿µâ€ä¸ä»£è¡¨â€œèƒ½ä¼¦ç†å¯¹é½åˆ¤æ–­â€ã€‚  
2. æ¨¡æ€é¸¿æ²Ÿæ™®éå­˜åœ¨ï¼š8æ¬¾VLMsåœ¨åŒºåˆ†è§†è§‰ä¸æ–‡æœ¬æ¨¡æ€çš„å®‰å…¨/ä¸å®‰å…¨æ¦‚å¿µæ—¶ï¼Œä¸€è‡´æ€§åœ°è¡¨ç°å‡ºåˆ¤æ–­å·®å¼‚ï¼Œå³å¯¹åŒæ¦‚å¿µçš„æ–‡æœ¬æè¿°å’Œå›¾åƒå‘ˆç°ï¼Œæ¨¡å‹åˆ¤æ–­æ˜¯å¦å®‰å…¨çš„ç»“æœå¸¸ä¸ä¸€è‡´ã€‚  
3. ç®€åŒ–RLæ–¹æ³•ä¼˜åŠ¿ï¼šå¯¹æ¯”æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰åŸºçº¿ï¼Œæœ¬æ–‡PPOæ–¹æ³•åœ¨å¼ºåŒ–å›¾åƒç«¯ä¸å®‰å…¨æ¦‚å¿µå¯¹é½èƒ½åŠ›ä¸Šæ›´ä¼˜ï¼Œä¸”èƒ½æ›´å¥½ä¿ç•™æ¨¡å‹é€šç”¨èƒ½åŠ›ï¼›åœ¨å¤–éƒ¨æ•°æ®é›†æµ‹è¯•ä¹Ÿå±•ç°æ›´å¼ºæ³›åŒ–æ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†å»ºè®¾ï¼šUnsafeConceptsä¸ºå®‰å…¨AIé¢†åŸŸæä¾›äº†é¦–ä¸ªç»†ç²’åº¦ã€å¤šæ¨¡æ€å…³è”çš„ä¸å®‰å…¨æ¦‚å¿µåŸºå‡†é›†ï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤æ‹“å±•æ›´å¤šå®‰å…¨ç›¸å…³ä»»åŠ¡ï¼ˆå¦‚æ›´å¤æ‚åœºæ™¯ä¸‹çš„å†…å®¹å®¡æ ¸ï¼‰ã€‚  
2. è¯„ä¼°ç»´åº¦è®¾è®¡ï¼šä»â€œæ„ŸçŸ¥ï¼ˆèƒ½ä¸èƒ½è¯†åˆ«ï¼‰â€åˆ°â€œå¯¹é½ï¼ˆåˆä¸åˆä¼¦ç†ï¼‰â€çš„åŒç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œä¸ºåˆ†æå¤šæ¨¡æ€æ¨¡å‹å®‰å…¨èƒ½åŠ›æä¾›äº†æ¸…æ™°èŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å¦‚å¤šæ¨¡æ€åè§ã€è¯¯å¯¼æ€§ä¿¡æ¯è¯†åˆ«ç­‰å®‰å…¨ç›¸å…³è¯„ä¼°åœºæ™¯ã€‚  
3. ç®€åŒ–å¼ºåŒ–å­¦ä¹ æ€è·¯ï¼šæ— éœ€äººå·¥åå¥½æ•°æ®çš„RLå¯¹é½æ–¹æ¡ˆï¼Œä¸ºèµ„æºæœ‰é™æˆ–éœ€å¿«é€Ÿè¿­ä»£çš„å®‰å…¨å¯¹é½ä»»åŠ¡æä¾›äº†è½»é‡é«˜æ•ˆçš„æŠ€æœ¯è·¯å¾„ï¼Œå¯å‘ç ”ç©¶è€…åœ¨â€œå°‘æ ‡æ³¨ã€å¼ºæ•ˆæœâ€æ–¹å‘æ¢ç´¢æ›´å¤šæ¨¡æ€å¯¹é½ã€å®‰å…¨å¢å¼ºæ–¹æ³•ã€‚

## tiny-reward-models
### Abstract
Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°æ¨¡å‹ä¹Ÿèƒ½æ‰“ï¼ŸTinyRMï¼šè½»é‡çº§å¥–åŠ±æ¨¡å‹æŒ‘æˆ˜å¤§æ¨¡å‹éœ¸æƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œå…¶è´¨é‡ç›´æ¥å½±å“LLMä¸äººç±»åå¥½çš„å¯¹é½æ•ˆæœã€‚ç„¶è€Œï¼Œå½“ä¸‹ä¸»æµåšæ³•æ˜¯ç”¨å¤§å‚æ•°çš„ decoder - å‹è¯­è¨€æ¨¡å‹åšå¥–åŠ±æ¨¡å‹ï¼Œè™½æ€§èƒ½å¼ºï¼Œä½†åœ¨æ¨ç†é˜¶æ®µéƒ¨ç½²æ—¶ï¼Œè®¡ç®—æˆæœ¬é«˜ä¼æˆäº†å¤§é—®é¢˜ã€‚è€Œä¸”æ–°çš„æµ‹è¯•æ—¶è§£ç ç­–ç•¥ä¸æ–­æ¶Œç°ï¼Œå¥–åŠ±æ¨¡å‹çš„é‡å¤è°ƒç”¨è®©æ¨ç†å¼€é”€æ„ˆå‘ä¸å¯å¿½è§†ã€‚åŒæ—¶ï¼Œä¹Ÿä¸ç¡®å®šåå¥½å»ºæ¨¡æ˜¯å¦å’Œä¸€æ¬¡æ€§çš„ä¸‹ä¸€ä¸ªtokenç”Ÿæˆä¸€æ ·éµå¾ªâ€œè¶Šå¤§è¶Šå¥½â€çš„ç¼©æ”¾å®šå¾‹ï¼Œæ‰€ä»¥å¼€å‘é«˜æ•ˆçš„å°å¥–åŠ±æ¨¡å‹å¾ˆæœ‰å¿…è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”¨åŒå‘æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰åšå¥–åŠ±æ¨¡å‹  
ä»¥å¾€å¥–åŠ±æ¨¡å‹å¤šåŸºäº decoder - å‹å¤§è¯­è¨€æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºTinyRMï¼Œé‡‡ç”¨å°å‚æ•°è§„æ¨¡ï¼ˆæœ€å°‘4äº¿å‚æ•°ï¼‰çš„åŒå‘æ©ç è¯­è¨€æ¨¡å‹æ¥æ‰¿æ‹…å¥–åŠ±æ¨¡å‹è§’è‰²ã€‚è¿™ç±»åŒå‘æ¨¡å‹èƒ½åŒå‘æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåœ¨ç†è§£ä»»åŠ¡ä¸Šæœ‰å¤©ç„¶ä¼˜åŠ¿ï¼Œå®éªŒä¸­åœ¨æ¨ç†å’Œå®‰å…¨åå¥½å»ºæ¨¡ä»»åŠ¡ä¸Šèƒ½åª²ç¾è¶…è‡ªèº«175å€å‚æ•°è§„æ¨¡çš„å¤§æ¨¡å‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»„åˆé«˜æ•ˆå¾®è°ƒæŠ€æœ¯  
ç»“åˆFLANé£æ ¼æç¤ºï¼ˆFLAN - style promptingï¼‰ã€å®šå‘ä½ç§©é€‚åº”ï¼ˆDirectional Low - Rank Adaptationï¼ŒDoRAï¼‰ä¸å±‚å†»ç»“ï¼ˆlayer freezingï¼‰æŠ€æœ¯æ¥é€‚é…ä»»åŠ¡ã€‚FLANé£æ ¼æç¤ºä¸ºæ¨¡å‹æä¾›åˆé€‚çš„ä»»åŠ¡å¼•å¯¼èŒƒå¼ï¼›DoRAæ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œåˆ†è§£æ¨¡å‹æƒé‡ä¸ºå¹…åº¦å’Œæ–¹å‘ç»„ä»¶ï¼Œç”¨LoRAæ›´å¯æ§åœ°æ›´æ–°æ–¹å‘å‘é‡ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šæ¯”å…¨ç§©å¾®è°ƒå¢ç›Šæ˜æ˜¾ï¼›å±‚å†»ç»“åˆ™å†»ç»“æ¨¡å‹ä½å±‚ä»¥ä¿ç•™é€šç”¨è¯­è¨€è¡¨å¾ï¼Œèšç„¦ä¸Šå±‚å¾®è°ƒä»»åŠ¡ç‰¹å®šçŸ¥è¯†ï¼Œä¸‰è€…ç»“åˆè®©å°æ¨¡å‹åœ¨å°‘èµ„æºä¸‹ä¹Ÿèƒ½åœ¨RewardBenchåŸºå‡†ä¸Šæœ‰å¼ºè¡¨ç°ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒåŸºäºModernBERT - Baseï¼ˆ1.5äº¿å‚æ•°ï¼‰å’ŒModernBERT - Largeï¼ˆ4äº¿å‚æ•°ï¼‰åˆå§‹åŒ–æ¨¡å‹ï¼Œåœ¨Chatã€Reasoningã€Safetyç­‰å…¬å¼€åå¥½æ•°æ®ä¸Šè®­ç»ƒé¢†åŸŸä¸“ç²¾æ¨¡å‹ï¼Œè¿˜è®­ç»ƒäº†â€œAll - At - Onceâ€ï¼ˆAAOï¼‰å¤§æ¨¡å‹åšå¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼šå°æ¨¡å‹åœ¨æ¨ç†å’Œå®‰å…¨ä»»åŠ¡ä¸Šé€šè¿‡ç‰¹å®šè°ƒä¼˜ç­–ç•¥èƒ½å±•ç°å‡ºæƒŠäººå®åŠ›ï¼Œæ¯”å¦‚ç»“åˆDoRAå’Œå±‚å†»ç»“åœ¨æ¨ç†ä»»åŠ¡æå‡æ˜¾è‘—ï¼›è™½åœ¨é€šç”¨æ¨¡å‹æ„å»ºå’Œå¯¹è¯åå¥½å»ºæ¨¡ä¸Šè¿˜æœ‰æŒ‘æˆ˜ï¼Œä½†è¯æ˜äº†è½»é‡åŒå‘æ¶æ„åœ¨åå¥½å»ºæ¨¡ä¸Šä½œä¸ºé«˜æ•ˆã€å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œå°æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ¨ç†ï¼‰ç”¨è½»é‡å¾®è°ƒæ–¹æ³•æ•ˆæœæ‹”ç¾¤ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹æ¶æ„é€‰æ‹©å¯ç¤ºï¼šåŒå‘æ©ç è¯­è¨€æ¨¡å‹è¿™ç±»ä¾§é‡ç†è§£çš„æ¶æ„åœ¨å¥–åŠ±æ¨¡å‹ä»»åŠ¡æœ‰å¾ˆå¤§æŒ–æ˜ç©ºé—´ï¼Œä¸ä¸€å®šéå¾—ä¾èµ–å¤§çš„decoderå‹æ¨¡å‹ï¼Œä¸ºé«˜æ•ˆå¥–åŠ±æ¨¡å‹æ¶æ„é€‰å‹æ‹“å®½æ€è·¯ã€‚  
2. å¾®è°ƒç­–ç•¥ç»„åˆï¼šFLANé£æ ¼æç¤ºã€DoRAã€å±‚å†»ç»“çš„ç»„åˆæ–¹å¼éªŒè¯äº†é¢†åŸŸç‰¹å®šè°ƒä¼˜ç­–ç•¥å¯¹å°æ¨¡å‹çš„ä»·å€¼ï¼Œåœ¨èµ„æºæœ‰é™æ—¶ï¼Œè¿™ç±»è½»é‡å¾®è°ƒæŠ€æœ¯çš„æ­é…èƒ½è®©å°æ¨¡å‹ç²¾å‡†é€‚é…ä»»åŠ¡ï¼Œä¸ºå°æ¨¡å‹ä¼˜åŒ–æä¾›äº†æŠ€æœ¯ç»„åˆèŒƒå¼å‚è€ƒã€‚  
3. ä»»åŠ¡é¢†åŸŸè®¤çŸ¥ï¼šæ˜ç¡®äº†å°æ¨¡å‹åœ¨æ¨ç†ã€å®‰å…¨ç­‰ä»»åŠ¡çš„ä¼˜åŠ¿ä¸å¯¹è¯ç±»ä»»åŠ¡çš„å½“å‰å±€é™ï¼Œåç»­å·¥ä½œå¯é’ˆå¯¹æ€§åœ¨ä¸åŒä»»åŠ¡åŸŸä¼˜åŒ–å°æ¨¡å‹ï¼Œæ¯”å¦‚å¼ºåŒ–å°æ¨¡å‹åœ¨å¯¹è¯åå¥½å»ºæ¨¡çš„èƒ½åŠ›æ—¶æ¢ç´¢æ–°ç­–ç•¥ã€‚

## enhancing-rlhf-with-human-gaze-modeling
### Abstract
Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨äººç±» gaze å»ºæ¨¡æå‡ RLHFï¼šæ›´å¿«æ”¶æ•›ï¼Œæ›´ä½æˆæœ¬

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Reinforcement Learning from Human Feedbackï¼ˆRLHFï¼‰æ˜¯è®©è¯­è¨€æ¨¡å‹å¯¹é½äººç±»åå¥½çš„æœ‰åŠ›èŒƒå¼ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¾ˆå¤§ç¨‹åº¦æºäºåé¦ˆä¿¡å·ç¨€ç–â€”â€”å¥–åŠ±æ¨¡å‹å¸¸ç»™æ•´ä¸ªåºåˆ—ä¸€ä¸ªæ ‡é‡å€¼ï¼Œéš¾ä»¥æŒ‡å¯¼æ¨¡å‹è¾“å‡ºå“ªéƒ¨åˆ†å¯¹äººç±»åå¥½è´¡çŒ®æ­£è´Ÿã€‚è€Œäººç±»è§†è§‰æ³¨æ„åŠ›ï¼ˆå¦‚çœ¼åŠ¨ã€ gaze æ¨¡å¼ï¼‰èƒ½åœ¨æ–‡æœ¬å¤„ç†æ—¶æä¾›ç»†ç²’åº¦è®¤çŸ¥åŠªåŠ›æŒ‡å‘ä¿¡æ¯ï¼Œå°†å…¶èå…¥ RLHF æˆ–å¯æå‡ç­–ç•¥è®­ç»ƒæ•ˆç‡ï¼Œæœ¬æ–‡å°±æ­¤å±•å¼€æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGazeRMï¼ˆ gaze æ„ŸçŸ¥å¥–åŠ±æ¨¡å‹ï¼‰
åŸºäº LÃ³pez - Cardona ç­‰äººï¼ˆ2025ï¼‰å·¥ä½œï¼Œå°†é¢„æµ‹çš„ gaze ç‰¹å¾æŠ•å°„åˆ°å¥–åŠ±æ¨¡å‹éšè—è¡¨ç¤ºç©ºé—´ï¼ˆé€šè¿‡å¯è®­ç»ƒå‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ·»åŠ æˆ–æ‹¼æ¥è‡³å¥–åŠ±æ¨¡å‹ç¬¬ä¸€å±‚ token åµŒå…¥ï¼‰ã€‚æ­¤å‰è¿™ç±»èå…¥ gaze çš„å¥–åŠ±æ¨¡å‹ä»…åœ¨åŸºå‡†æµ‹è¯•éªŒè¯ï¼Œæœ¬æ–‡é¦–æ¬¡å°†å…¶çº³å…¥ RLHF  pipelineï¼ˆå¦‚ PPOã€GRPO æ¡†æ¶ï¼‰ï¼ŒéªŒè¯æ˜¯å¦åŠ é€Ÿæ”¶æ•›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šGazeDistribï¼ˆåŸºäº gaze çš„ token çº§ç¨€ç–å¥–åŠ±åˆ†é…ï¼‰
å— Chan ç­‰äººï¼ˆ2024ï¼‰ç”¨å¥–åŠ±æ¨¡å‹æ³¨æ„åŠ›åšå¥–åŠ±å½’å› å¯å‘ï¼Œæœ¬æ–‡ä¾é é¢„æµ‹çš„äººç±» gaze åœ¨ RLHF ä¸­åˆ†é… token çº§ä¿¡ç”¨ã€‚ä¸æ”¹å˜å¥–åŠ±æ¨¡å‹æœ¬èº«ï¼Œä»…æ”¹å˜åˆ†æ•°åˆ†é…æ–¹å¼ï¼Œè®©å¥–åŠ±ä¸äººç±»æ³¨æ„åŠ›å¯¹é½ï¼Œæä¾›æ›´æœ‰é’ˆå¯¹æ€§åé¦ˆä»¥åŠ é€Ÿæ”¶æ•›ã€æå‡æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ PPO å’Œ GRPO ç®—æ³•ä¸‹å®éªŒï¼Œä¸æ ‡å‡†æ–¹æ³•ç›¸æ¯”ï¼Œèå…¥ gaze ä¿¡æ¯çš„ RLHF æ”¶æ•›é€Ÿåº¦æå‡ 1.5 - 2 å€ï¼ŒåŒæ—¶ä¿æŒæˆ–ç•¥å¾®æå‡æœ€ç»ˆæ€§èƒ½ã€‚å¦‚å¯¹ LLaMa - 7B - open - instruct æ¨¡å‹åœ¨ HH - RLHF è®­ç»ƒï¼ŒéªŒè¯é›†å’Œè®­ç»ƒé›†ä¸Š PPO + GazeRMã€PPO + GazeDistrib å‡æ¯” PPO åŸºçº¿æ›´å¿«æ”¶æ•›ä¸”è¡¨ç°æ›´ä¼˜ï¼ˆä»å®éªŒå›¾ä¸­ä¸åŒè®­ç»ƒæ­¥æ•°ä¸‹å½’ä¸€åŒ–å¥–åŠ±å¯¹æ¯”å¯è§ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. äººç±» gaze æ˜¯æ”¿ç­–ä¼˜åŒ–ä¸­æœªå……åˆ†åˆ©ç”¨çš„æœ‰ä»·å€¼ä¿¡å·ï¼Œä¸ºæå‡ RLHF æ•ˆç‡æŒ‡æ˜æ–°æ–¹å‘ï¼Œåç»­å¯æ¢ç´¢æ›´å¤š gaze èå…¥ RLHF çš„æ–¹å¼ã€‚
2. æå‡ºçš„ä¸¤ç§å°†äººç±» gaze é¢„æµ‹æ¨¡å‹èå…¥ RLHF çš„æ–¹æ³•ï¼Œä¸ºè§£å†³ RLHF åé¦ˆç¨€ç–ã€è®­ç»ƒä½æ•ˆé—®é¢˜æä¾›äº†åˆ›æ–°æ€è·¯ï¼Œå¯å¯å‘ç ”ç©¶è€…ä»äººç±»è®¤çŸ¥ä¿¡å·è§’åº¦ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆç»“åˆçš„èŒƒå¼ã€‚
3. å¯¹äººç±»æ³¨æ„åŠ›åœ¨è¯æ€§ä¸Šåˆ†å¸ƒçš„åˆ†æï¼ˆå¦‚å†…å®¹ä¸°å¯Œè¯å’Œæ ‡ç‚¹æ›´å—å…³æ³¨ï¼‰ï¼Œæœ‰åŠ©äºç†è§£ gaze è¾…åŠ©è®­ç»ƒæ›´å¿«æ”¶æ•›çš„åŸå› ï¼Œä¹Ÿä¸ºåç»­ä»è¯­è¨€ç»“æ„è§’åº¦è®¾è®¡æ›´ä¼˜åé¦ˆæœºåˆ¶æä¾›å‚è€ƒã€‚
```

## one-token-to-fool-llm-as-a-judge
### Abstract
Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸€ä¸ªTokenå°±èƒ½éª—è¿‡LLMè£åˆ¤ï¼Ÿæ­ç§˜ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„è„†å¼±æ€§ä¸åº”å¯¹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„èŒƒå¼ä¸­ï¼Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆä¹Ÿè¢«ç§°ä¸ºâ€œLLMä½œä¸ºè£åˆ¤â€ï¼‰å‡­å€Ÿå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„ç”Ÿæˆä¸æ³›åŒ–èƒ½åŠ›ï¼Œé€æ¸å–ä»£åƒµåŒ–çš„è§„åˆ™å¼æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ç­”æ¡ˆè´¨é‡ã€‚ä½†ç ”ç©¶å‘ç°ï¼Œè¿™ç±»ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹å­˜åœ¨æƒŠäººçš„è„†å¼±æ€§ï¼šä»…éœ€ä¸€äº›è¡¨é¢æ“ä½œï¼ˆæ¯”å¦‚éæ–‡å­—ç¬¦å·â€œ:â€â€œ.â€ï¼Œæˆ–è€…åƒâ€œThought process:â€â€œLetâ€™s solve this problem step by step.â€è¿™ç±»æ¨ç†å¼€å¤´ï¼‰ï¼Œå°±èƒ½è®©æ¨¡å‹ç»™å‡ºé”™è¯¯çš„æ­£å‘å¥–åŠ±ã€‚è¿™ç§å¼±ç‚¹åœ¨ä¸åŒLLMã€æ•°æ®é›†å’Œæç¤ºæ ¼å¼ä¸­å¹¿æ³›å­˜åœ¨ï¼Œå¯¹ä¾èµ–ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒç®—æ³•ï¼ˆå¦‚æ‹’ç»é‡‡æ ·ã€åå¥½ä¼˜åŒ–ã€RLVRç­‰ï¼‰æ„æˆä¸¥é‡å¨èƒã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨æ­ç¤ºè¯¥é—®é¢˜å¹¶æå‡ºæ”¹è¿›æ–¹æ³•ï¼Œæ¨åŠ¨æ›´å¯é çš„åŸºäºLLMçš„è¯„ä¼°æ–¹å¼å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°â€œä¸‡èƒ½é’¥åŒ™â€æ¼æ´  
è®ºæ–‡é¦–æ¬¡æ˜ç¡®æŒ‡å‡ºï¼Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åœ¨RLVRåœºæ™¯ä¸‹å­˜åœ¨ç³»ç»Ÿæ€§è„†å¼±æ€§â€”â€”ä»…å«éæ–‡å­—ç¬¦å·æˆ–æ¨ç†å¼€å¤´çš„â€œå€™é€‰ç­”æ¡ˆâ€ï¼Œå¸¸èƒ½éª—å–æ­£å‘å¥–åŠ±ã€‚è¿™ç±»èƒ½è§¦å‘é”™è¯¯å¥–åŠ±çš„å¯¹æŠ—å¼å“åº”è¢«ç§°ä¸ºâ€œmaster keysï¼ˆä¸‡èƒ½é’¥åŒ™ï¼‰â€ï¼Œä¸”è¯¥é—®é¢˜åœ¨å¤šæ¨¡å‹ã€å¤šæ•°æ®é›†ä¸Šæ™®éå­˜åœ¨ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç³»ç»Ÿæ€§è¯„ä¼°æ¼æ´çš„æ™®éæ€§  
å›¢é˜Ÿç”¨åç§â€œä¸‡èƒ½é’¥åŒ™â€å“åº”ï¼Œåœ¨å¤šæ¨¡å‹ï¼ˆé€šç”¨æ¨¡å‹å¦‚Qwen2.5 - 72Bã€GPT - 4oï¼Œä¸“ç”¨éªŒè¯å™¨å¦‚Omni - Judgeï¼‰å’Œå¤šæ•°æ®é›†ï¼ˆæ•°å­¦æ¨ç†ã€é€šç”¨é¢†åŸŸç­‰ï¼‰ä¸Šå±•å¼€æµ‹è¯•ï¼ŒéªŒè¯äº†æ¼æ´çš„å¹¿æ³›æ€§ï¼›è¿˜åˆ†æäº†è¯¥ç°è±¡çš„è§„æ¨¡æ•ˆåº”ä¸ç”Ÿæˆæ–°â€œä¸‡èƒ½é’¥åŒ™â€çš„æŠ€å·§ï¼ŒåŒæ—¶è¯æ˜æ¨ç†æ—¶ç­–ç•¥éš¾ä»¥å¯é é˜²å¾¡è¿™ç±»æ”»å‡»ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®å¢å¼º+é²æ£’å¥–åŠ±æ¨¡å‹  
ä¸ºç¼“è§£æ¼æ´ï¼Œæå‡ºç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼šæ„é€ â€œç±»å¯¹æŠ—â€å“åº”ï¼ˆæˆªæ–­æ¨¡å‹è¾“å‡ºï¼Œä¿ç•™ä»…åšæ³›åŒ–é“ºå«ã€æ— å®é™…è§£é¢˜çš„å¼€å¤´ç‰‡æ®µä½œä¸ºè´Ÿæ ·æœ¬ï¼‰æ¥æ‰©å……è®­ç»ƒæ•°æ®ã€‚åŸºäºæ­¤è®­ç»ƒå‡ºé€šç”¨é¢†åŸŸçš„Master - RMå¥–åŠ±æ¨¡å‹ï¼Œå¤§å¹…æå‡äº†å¯¹â€œä¸‡èƒ½é’¥åŒ™â€çš„é²æ£’æ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”å¤§æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå«GSM8Kã€MATHç­‰æ•°å­¦æ¨ç†æ•°æ®é›†ï¼ŒåŠMulti - subject RLVRç­‰é€šç”¨é¢†åŸŸæ•°æ®é›†ï¼‰ä¸­ï¼Œä¼ ç»ŸLLMè£åˆ¤é¢å¯¹â€œä¸‡èƒ½é’¥åŒ™â€æ”»å‡»æ—¶ï¼Œé”™è¯¯æ­£ä¾‹ç‡ï¼ˆFPRï¼‰æœ€é«˜è¾¾80%ï¼›è€Œæ–°è®­ç»ƒçš„Master - RMåœ¨æ‰€æœ‰åœºæ™¯ä¸‹é”™è¯¯æ­£ä¾‹ç‡æ¥è¿‘0ï¼Œé²æ£’æ€§æ˜¾è‘—è¶…è¶ŠåŒç±»æ¨¡å‹ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¼æ´å‘ç°è§†è§’ï¼šæé†’ç ”ç©¶è€…ä¸å¼€å‘è€…é‡æ–°å®¡è§†â€œLLMä½œä¸ºè£åˆ¤â€çš„é²æ£’æ€§å‡è®¾ï¼Œåœ¨ä¾èµ–ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„ç³»ç»Ÿè®¾è®¡ä¸­ï¼Œéœ€ä¼˜å…ˆè€ƒè™‘å¯¹æŠ—å¼æµ‹è¯•ä¸é²æ£’æ€§éªŒè¯ï¼›  
2. æ•°æ®å¢å¼ºæ€è·¯ï¼šè®ºæ–‡ç”¨â€œæ„é€ ç±»å¯¹æŠ—è´Ÿæ ·æœ¬â€æ¥å¢å¼ºæ¨¡å‹é²æ£’æ€§çš„æ–¹æ³•ï¼Œä¸ºè®­ç»ƒæ›´å¯é çš„å¥–åŠ±æ¨¡å‹æä¾›äº†è½»é‡å´æœ‰æ•ˆçš„èŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€æŠµå¾¡è¡¨é¢å¹²æ‰°çš„è¯„ä¼°ç±»æ¨¡å‹è®­ç»ƒä¸­ï¼›  
3. å¼€æºèµ„æºä»·å€¼ï¼šå‘å¸ƒçš„Master - RMæ¨¡å‹ä¸åˆæˆè®­ç»ƒæ•°æ®ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ç›´æ¥å¯ç”¨çš„é²æ£’å¥–åŠ±æ¨¡å‹åŸºçº¿ï¼Œé™ä½äº†é¢†åŸŸå†…é‡å¤é€ è½®å­çš„æˆæœ¬ï¼Œæ¨åŠ¨ç›¸å…³æ–¹å‘å¿«é€Ÿè¿­ä»£ã€‚

## stable-preference-optimization-for-llms--a-bilevel-approach-beyond-direct-preference-optimization
### Abstract
Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¶…è¶Šç›´æ¥åå¥½ä¼˜åŒ–ï¼šå¤§è¯­è¨€æ¨¡å‹çš„ç¨³å®šåå¥½ä¼˜åŒ–åŒçº§æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§æµè¡Œä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºå°†è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ï¼Œæ›¿ä»£äº†å¥–åŠ±å»ºæ¨¡å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç„¶è€Œå…¶ç†è®ºç‰¹æ€§ä¸å†…åœ¨å±€é™æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ­¤å‰ç ”ç©¶è¡¨æ˜DPOè®­ç»ƒä¸­å­˜åœ¨åå¥½ä¸éåå¥½å“åº”æ¦‚ç‡åŒæ—¶ä¸‹é™çš„â€œä¼¼ç„¶ä½ç§»â€ç°è±¡ï¼Œä¸”ç°æœ‰å¯¹è¯¥ç°è±¡çš„ç†è®ºåˆ†æå¤šåŸºäºå¼ºå‡è®¾ï¼ˆå¦‚å•tokenè¾“å‡ºï¼‰ï¼Œé™åˆ¶äº†é€‚ç”¨æ€§ã€‚åŒæ—¶DPOå¯¹åˆå§‹åŒ–æ•æ„Ÿã€æ˜“å‡ºç°æ¦‚ç‡è´¨é‡é”™é…ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åå‘æ— å…³æˆ–ä¸æœŸæœ›çš„å“åº”ã€æŸå®³å¯¹é½ç¨³å®šæ€§ä¸åå¥½ä¸€è‡´æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»æ¦‚ç‡æ¼”åŒ–è§†è§’æ·±å…¥åˆ†æDPOåŠ¨æ€ï¼Œå¹¶æå‡ºæ”¹è¿›æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDPOçš„å…¨é¢ç†è®ºåˆ†æ  
ä¸åŒäºä»¥å¾€åŸºäºå¼ºå‡è®¾æˆ–ç‰¹å®šåœºæ™¯ï¼ˆå¦‚å•tokenã€é«˜åº¦ç›¸ä¼¼å®Œæˆï¼‰çš„ç ”ç©¶ï¼Œæœ¬æ–‡æ— å¼ºå‡è®¾åœ°åˆ†æDPOä¼˜åŒ–è¡Œä¸ºï¼Œèšç„¦åå¥½ï¼ˆywï¼‰ä¸éåå¥½ï¼ˆylï¼‰æ ·æœ¬æ¦‚ç‡å˜åŒ–åŠ¨æ€ï¼ˆâˆ†Ï€Î¸(yw|x)å’Œâˆ†Ï€Î¸(yl|x)ï¼‰ã€‚æ­ç¤ºå‡ºDPOå¯¹åˆå§‹åŒ–æ•æ„Ÿã€æ— æ³•æŒç»­æå‡åå¥½è¾“å‡ºæ¦‚ç‡ã€æ˜“å°†æ¦‚ç‡è´¨é‡ä»ä¸æœŸæœ›æ–¹å‘é‡æ–°åˆ†é…ç­‰å…³é”®å±€é™ï¼Œä¸ºåç»­æ–¹æ³•æ”¹è¿›ç­‘ç‰¢ç†è®ºæ ¹åŸºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç¨³å®šåå¥½ä¼˜åŒ–åŒçº§æ¡†æ¶  
å—å…ƒå­¦ä¹ å¯å‘ï¼Œæå‡ºç†è®ºé©±åŠ¨çš„åŒçº§ä¼˜åŒ–æ¡†æ¶ï¼Œç»Ÿä¸€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸å¢å¼ºå‹DPOç›®æ ‡ï¼ˆå³ç¨³å®šåå¥½ä¼˜åŒ–ï¼‰ã€‚ä¸‹å±‚SFTæä¾›å¼ºåˆå§‹åŒ–ï¼Œä¸Šå±‚å¢å¼ºDPOç›®æ ‡æ­é…åŸºäºæ¢¯åº¦çš„æ­£åˆ™åŒ–é¡¹ï¼Œæ˜¾å¼é¼“åŠ±åå¥½è¾“å‡ºæ¦‚ç‡æå‡ã€ç¼“è§£æ¦‚ç‡é”™é…é—®é¢˜ã€‚é€šè¿‡è€¦åˆä¸¤çº§ä¼˜åŒ–ï¼Œå¤§å¹…å¢å¼ºç›´æ¥å¯¹é½å­¦ä¹ çš„ç¨³å®šæ€§ä¸æœ‰æ•ˆæ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä¸æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æŒç»­æå‡æ¨ç†å‡†ç¡®æ€§ï¼Œä¸”èƒ½æ›´å¥½åœ°ä½¿è¾“å‡ºåˆ†å¸ƒä¸é¢„æœŸåå¥½å¯¹é½ï¼Œæ€§èƒ½è¶…è¶Šæ ‡å‡†DPOæ–¹æ³•ã€‚å®éªŒç»“æœéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨æ”¹å–„æ¨¡å‹å¯¹é½æ•ˆæœä¸ç¨³å®šæ€§ä¸Šçš„ä¼˜åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç†è®ºå±‚é¢ï¼Œæœ¬æ–‡å¯¹DPOæ¦‚ç‡æ¼”åŒ–åŠ¨æ€çš„æ·±å…¥åˆ†æä¸ºç†è§£åå¥½å¯¹é½æ–¹æ³•çš„å†…åœ¨æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¯å‘åç»­å¯¹åå¥½å¯¹é½ç›®æ ‡è®¾è®¡çš„ç†è®ºç ”ç©¶ï¼›ä»æ–¹æ³•å±‚é¢ï¼ŒåŒçº§ä¼˜åŒ–æ¡†æ¶å°†SFTä¸åå¥½ä¼˜åŒ–ç»Ÿä¸€çš„æ€è·¯ï¼Œä»¥åŠå¼•å…¥æ­£åˆ™åŒ–æ˜¾å¼å¼•å¯¼æ¦‚ç‡æ”¹è¿›çš„ç­–ç•¥ï¼Œä¸ºæ‰“é€ æ›´å¯é ã€å¯è§£é‡Šçš„è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•å¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œç›¸å…³è®¾è®¡ç†å¿µå¯è¿ç§»è‡³å…¶ä»–åŸºäºåå¥½çš„æ¨¡å‹å¯¹é½ä»»åŠ¡ä¸­ã€‚

## bradley-terry-and-multi-objective-reward-modeling-are-complementary
### Abstract
Reward models trained on human preference data have demonstrated strong
effectiveness in aligning Large Language Models (LLMs) with human intent under
the framework of Reinforcement Learning from Human Feedback (RLHF). However,
RLHF remains vulnerable to reward hacking, where the policy exploits
imperfections in the reward function rather than genuinely learning the
intended behavior. Although significant efforts have been made to mitigate
reward hacking, they predominantly focus on and evaluate in-distribution
scenarios, where the training and testing data for the reward model share the
same distribution. In this paper, we empirically show that state-of-the-art
methods struggle in more challenging out-of-distribution (OOD) settings. We
further demonstrate that incorporating fine-grained multi-attribute scores
helps address this challenge. However, the limited availability of high-quality
data often leads to weak performance of multi-objective reward functions, which
can negatively impact overall performance and become the bottleneck. To address
this issue, we propose a unified reward modeling framework that jointly trains
Bradley--Terry (BT) single-objective and multi-objective regression-based
reward functions using a shared embedding space. We theoretically establish a
connection between the BT loss and the regression objective and highlight their
complementary benefits. Specifically, the regression task enhances the
single-objective reward function's ability to mitigate reward hacking in
challenging OOD settings, while BT-based training improves the scoring
capability of the multi-objective reward function, enabling a 7B model to
outperform a 70B baseline. Extensive experimental results demonstrate that our
framework significantly improves both the robustness and the scoring
performance of reward models.
### ğŸŒŸ è®ºæ–‡è§£è¯» |  Bradley - Terryä¸å¤šç›®æ ‡å¥–åŠ±å»ºæ¨¡ï¼šä¼˜åŠ¿äº’è¡¥åº”å¯¹RLHFå¥–åŠ±é»‘å®¢éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ¡†æ¶ä¸‹ï¼ŒåŸºäºäººç±»åå¥½æ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹èƒ½æœ‰æ•ˆè®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»æ„å›¾å¯¹é½ï¼Œä½†RLHFæ˜“å—â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜å›°æ‰°ï¼Œå³ç­–ç•¥åˆ©ç”¨å¥–åŠ±å‡½æ•°ç¼ºé™·è€ŒéçœŸæ­£å­¦ä¹ é¢„æœŸè¡Œä¸ºã€‚è¿‡å¾€ç¼“è§£è¯¥é—®é¢˜çš„å·¥ä½œå¤šèšç„¦åˆ†å¸ƒå†…åœºæ™¯ï¼ˆå¥–åŠ±æ¨¡å‹è®­ç»ƒä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒç›¸åŒï¼‰ï¼Œè€Œåœ¨æ›´å…·æŒ‘æˆ˜çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚åŒæ—¶ï¼Œå¼•å…¥ç»†ç²’åº¦å¤šå±æ€§åˆ†æ•°è™½æœ‰åŠ©äºåº”å¯¹æŒ‘æˆ˜ï¼Œä½†é«˜è´¨é‡æ•°æ®æœ‰é™å¯¼è‡´å¤šç›®æ ‡å¥–åŠ±å‡½æ•°æ€§èƒ½å¼±ï¼Œæˆä¸ºç“¶é¢ˆã€‚æ­¤å¤–ï¼Œç°æœ‰ç ”ç©¶åœ¨OODåœºæ™¯ä¸‹å¥–åŠ±æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å±€é™æ€§æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå¤šç›®æ ‡å¥–åŠ±æ¨¡å‹ï¼ˆMORMsï¼‰å› æ•°æ®é™åˆ¶æ€§èƒ½ä¸å¦‚å•ç›®æ ‡å¥–åŠ±æ¨¡å‹ï¼ˆSORMsï¼‰ï¼Œä¸”ä¸¤è€…ç‹¬ç«‹ä½¿ç”¨å­˜åœ¨è®¡ç®—æ˜‚è´µå’Œæ€§èƒ½ç“¶é¢ˆç­‰é—®é¢˜ï¼Œè¿™äº›éƒ½æ„æˆäº†æœ¬æ–‡ç ”ç©¶çš„åŠ¨æœºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºSOTAå¥–åŠ±æ¨¡å‹åœ¨OODåœºæ™¯çš„ç¼ºé™·  
é€šè¿‡å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨PPOå’ŒBest - of - Né‡‡æ ·æ—¶ï¼Œè‹¥æ‰€ç”¨promptæ¥è‡ªä¸è®­ç»ƒæ•°æ®ä¸åŒåˆ†å¸ƒçš„OODåœºæ™¯ï¼Œä¼šå‡ºç°å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œå‡¸æ˜¾äº†ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨OODåœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›çš„å…³é”®å±€é™ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSMORMç»Ÿä¸€å¥–åŠ±å»ºæ¨¡æ¡†æ¶  
æå‡ºè”åˆå•ç›®æ ‡å’Œå¤šç›®æ ‡å¥–åŠ±æ¨¡å‹ï¼ˆSMORMï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å…±äº«åµŒå…¥ç©ºé—´è”åˆè®­ç»ƒåŸºäºBradley - Terryï¼ˆBTï¼‰çš„å•ç›®æ ‡å¥–åŠ±å‡½æ•°å’ŒåŸºäºå¤šç›®æ ‡å›å½’çš„å¥–åŠ±å‡½æ•°ã€‚ä»ç†è®ºä¸Šå»ºç«‹äº†BTæŸå¤±å’Œå›å½’ç›®æ ‡ä¹‹é—´çš„è”ç³»ï¼Œå®ç°äº’è¡¥ä¼˜åŠ¿ï¼šå¤šç›®æ ‡å¤´è®­ç»ƒä¼˜åŒ–åµŒå…¥ç©ºé—´ï¼Œä½¿è¡¨ç¤ºèƒ½æ•æ‰å¤šå±æ€§è´¨é‡å·®å¼‚ï¼Œå¢å¼ºå•ç›®æ ‡å¤´åœ¨OODåœºæ™¯æŠ—å¥–åŠ±é»‘å®¢çš„é²æ£’æ€§ï¼›å•ç›®æ ‡å¤´è®­ç»ƒèƒ½ä¿®æ­£åµŒå…¥ç©ºé—´ä¸­å“åº”çš„å®šä½ï¼Œè®©å¤šç›®æ ‡å¤´åœ¨æ•°æ®æœ‰é™æ—¶ä¹Ÿèƒ½æœ‰ç«äº‰åŠ›è¡¨ç°ã€‚ä¸”SMORMè®­ç»ƒçµæ´»ï¼Œè®­ç»ƒä¸¤ä¸ªå¤´çš„prompt - responseå¯¹æ— éœ€å®Œå…¨ç›¸åŒï¼Œåªéœ€å•æ¬¡å‰å‘ä¼ æ’­å…±äº«éª¨å¹²ç½‘ç»œï¼Œè§£å†³äº†è®¡ç®—æ˜‚è´µé—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§å’Œè¯„åˆ†æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ç›¸åŒå¤šç›®æ ‡æ•°æ®é›†æ—¶ï¼ŒåŸºäºSMORMçš„7Bæ¨¡å‹èƒ½è¶…è¶Š70Bçš„åŸºçº¿æ¨¡å‹ï¼›åŒæ—¶åœ¨åº”å¯¹OODåœºæ™¯å¥–åŠ±é»‘å®¢é—®é¢˜ä¸Šï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ï¼ŒéªŒè¯äº†å¤šç›®æ ‡å¤´å¯¹å•ç›®æ ‡å¤´æ³›åŒ–èƒ½åŠ›çš„å¢å¼ºï¼Œä»¥åŠå•ç›®æ ‡å¤´å¯¹å¤šç›®æ ‡å¤´åœ¨æœ‰é™æ•°æ®ä¸‹æ€§èƒ½çš„æå‡ç­‰ç†è®ºåˆ†æç»“è®ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç ”ç©¶è§†è§’ä¸Šï¼Œå…³æ³¨åˆ°OODåœºæ™¯ä¸‹å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½é—®é¢˜ï¼Œå¡«è¡¥äº†è¿‡å¾€ç ”ç©¶åœ¨è¯¥åœºæ™¯ä¸‹çš„ç©ºç™½ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹åœ¨æ›´å¤æ‚åœºæ™¯çš„ç ”ç©¶æä¾›äº†æ–¹å‘å‚è€ƒã€‚
2. æ–¹æ³•åˆ›æ–°ä¸Šï¼ŒSMORMæ¡†æ¶ä¸ºç»“åˆå•ç›®æ ‡å’Œå¤šç›®æ ‡å¥–åŠ±å»ºæ¨¡æä¾›äº†æœ‰æ•ˆèŒƒå¼ï¼Œå…¶ç†è®ºå±‚é¢å»ºç«‹çš„BTæŸå¤±ä¸å›å½’ç›®æ ‡çš„è”ç³»ï¼Œä¸ºå¥–åŠ±æ¨¡å‹çš„è”åˆè®­ç»ƒæä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œåç»­åœ¨å¤„ç†å¤šç±»å‹å¥–åŠ±å‡½æ•°ç»“åˆã€æå‡æ¨¡å‹é²æ£’æ€§ç­‰æ–¹é¢å¯å€Ÿé‰´è¯¥è”åˆè®­ç»ƒæ€è·¯ã€‚
3. æ•°æ®åˆ©ç”¨ä¸Šï¼Œåœ¨é«˜è´¨é‡å¤šå±æ€§æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´è”åˆè®­ç»ƒæå‡å¤šç›®æ ‡å¥–åŠ±å‡½æ•°æ€§èƒ½çš„æ–¹å¼ï¼Œä¸ºè§£å†³æ•°æ®ç“¶é¢ˆé—®é¢˜æä¾›äº†åˆ›æ–°æ€è·¯ï¼Œå¯åº”ç”¨äºå…¶ä»–å› æ•°æ®é™åˆ¶å¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™çš„åœºæ™¯ã€‚

## perception-aware-policy-optimization-for-multimodal-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€æ¨ç†çš„æ„ŸçŸ¥æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼šPAPO å¦‚ä½•çªç ´è§†è§‰æ„ŸçŸ¥ç“¶é¢ˆï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çº¯æ–‡æœ¬é¢†åŸŸçš„æ¨ç†èƒ½åŠ›å·²é€šè¿‡å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¾—åˆ°æœ‰æ•ˆå¢å¼ºï¼Œä½†ç°æœ‰ RLVR è®¾è®¡å’Œä¼˜åŒ–ä¸»è¦é’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ã€‚å½“å‰å¤šæ¨¡æ€æ¨ç†çš„ä¸»è¦è¯¯å·®æ¥æºæ˜¯å¯¹è§†è§‰è¾“å…¥çš„æ„ŸçŸ¥é—®é¢˜ï¼šæ¨¡å‹è™½èƒ½å®Œæˆé€»è¾‘æˆ–ä»£æ•°æ¨ç†ï¼Œä½†å¸¸å› æ— æ³•å‡†ç¡®ç†è§£è§†è§‰è¾“å…¥ï¼ˆå¦‚ç©ºé—´å…³ç³»ã€æ ‡ç­¾å…³è”ï¼‰å¯¼è‡´é”™è¯¯ã€‚æ­¤å‰é’ˆå¯¹æ„ŸçŸ¥ä¼˜åŒ–çš„æ–¹æ³•è¦ä¹ˆä¾èµ–é¢å¤–å¥–åŠ±æ¨¡å‹æˆ–æ•°æ®å¤„ç†ï¼Œè¦ä¹ˆå°†æ„ŸçŸ¥ä¸æ¨ç†ç”Ÿç¡¬åˆ†ç¦»ï¼Œå­˜åœ¨è®¡ç®—å¼€é”€å¤§æˆ–é€‚é…æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡æ›´é€‚é…å¤šæ¨¡æ€é¢†åŸŸçš„ RLVR ç®—æ³•ï¼Œæˆä¸ºå…³é”®ç ”ç©¶é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º Perception - Aware Policy Optimizationï¼ˆPAPOï¼‰ç®—æ³•  
PAPO æ˜¯å¯¹ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰çš„ç®€æ´ä¸”é«˜æ•ˆçš„æ‰©å±•ï¼Œæ—¨åœ¨è®©æ¨¡å‹åœ¨å­¦ä¹ æ¨ç†çš„åŒæ—¶å­¦ä¹ æ„ŸçŸ¥ï¼Œä¸”å®Œå…¨ä¾èµ–å†…éƒ¨ç›‘ç£ä¿¡å·ï¼Œæ— éœ€é¢å¤–æ•°æ®æ•´ç†ã€å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹ã€‚æ ¸å¿ƒæ˜¯å¼•å…¥**éšå¼æ„ŸçŸ¥æŸå¤±ï¼ˆImplicit Perception Lossï¼‰**ï¼Œä»¥ KL æ•£åº¦å½¢å¼åŠ å…¥ GRPO ç›®æ ‡å‡½æ•°ã€‚é€šè¿‡å¯¹æ¯”æ¨¡å‹åœ¨åŸå§‹å›¾åƒå’Œâ€œæŸåå›¾åƒï¼ˆå¦‚éšæœºé®ç›–å›¾åƒå—çš„ç‰ˆæœ¬ï¼‰â€ä¸‹ç”Ÿæˆè¾“å‡ºçš„æ¦‚ç‡å·®å¼‚ï¼Œå¼•å¯¼æ¨¡å‹æ›´ä¾èµ–æœ‰æ•ˆè§†è§‰ä¿¡æ¯æ¨ç†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£å†³æŸå¤±é»‘å®¢ï¼ˆLoss Hackingï¼‰é—®é¢˜  
ç”±äº KL ç›®æ ‡çš„æ— ç•Œæ€§ï¼Œè‹¥éšå¼æ„ŸçŸ¥æŸå¤±ç³»æ•°è¿‡é«˜ï¼ŒPAPO å¯èƒ½è¿‡åº¦ä¼˜åŒ– KL é¡¹ï¼Œå¯¼è‡´å¥–åŠ±å´©æºƒã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æ·±å…¥åˆ†æè¯¥é—®é¢˜å¹¶æå‡º**åŒç†µæŸå¤±ï¼ˆDouble Entropy Lossï¼‰**æ¥ç¼“è§£ï¼Œä¿éšœè®­ç»ƒç¨³å®šæ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼šåœ¨ 8 ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šï¼ŒPAPO ç›¸æ¯” GRPO å¹³å‡æå‡ 4.4%ï¼›åœ¨è§†è§‰ä¾èµ–åº¦é«˜çš„ä»»åŠ¡ä¸­ï¼Œæå‡å¹…åº¦æ¥è¿‘ 8.0%ã€‚  
- æ„ŸçŸ¥è¯¯å·®å‡å°‘ï¼šäººå·¥åˆ†æ 200 ä¸ªé”™è¯¯æ¡ˆä¾‹æ˜¾ç¤ºï¼ŒPAPO ä½¿æ„ŸçŸ¥ç›¸å…³é”™è¯¯é™ä½ 30.5%ï¼Œè¯æ˜æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›æå‡ã€‚  
- æ”¶æ•›é€Ÿåº¦ï¼šPAPO æ—©æœŸï¼ˆçº¦ 25 æ­¥ï¼‰å°±å±•ç°å¢ç›Šï¼Œæ”¶æ•›æ›´å¿«ï¼›ç»“åˆç§»é™¤å‚è€ƒ KL ç­‰ç­–ç•¥ï¼Œåœ¨ 3B è§„æ¨¡æ¨¡å‹ä¸Šæœ€é«˜å¯æå‡ 11.2%ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€ä»»åŠ¡ä¼˜åŒ–æ€è·¯ï¼šé’ˆå¯¹å¤šæ¨¡æ€åœºæ™¯ç‰¹æœ‰é—®é¢˜ï¼ˆå¦‚è§†è§‰æ„ŸçŸ¥ç“¶é¢ˆï¼‰ï¼Œä»ç®—æ³•å±‚é¢èåˆæ„ŸçŸ¥ä¸æ¨ç†ç›‘ç£ï¼Œä¸ºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æä¾›æ–°èŒƒå¼ã€‚  
2. è½»é‡é«˜æ•ˆè®¾è®¡ï¼šä¸ä¾èµ–é¢å¤–å¤æ‚ç»„ä»¶ï¼ˆå¦‚å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼‰ï¼Œä»…é€šè¿‡ä¿®æ”¹ç›®æ ‡å‡½æ•°å¼•å…¥æ„ŸçŸ¥ç›‘ç£ï¼Œåœ¨å·¥ç¨‹å®ç°ä¸Šæ›´å…·å¯æ“ä½œæ€§ã€‚  
3. é—®é¢˜å‘ç°ä¸è§£å†³ï¼šé€šè¿‡ error analysis å®šä½æ ¸å¿ƒç—›ç‚¹ï¼ˆæ„ŸçŸ¥è¯¯å·®ä¸»å¯¼å¤šæ¨¡æ€æ¨ç†é”™è¯¯ï¼‰ï¼Œå†é’ˆå¯¹æ€§è®¾è®¡æŸå¤±å‡½æ•°ï¼›åŒæ—¶å‘ç°å¹¶ç¼“è§£â€œæŸå¤±é»‘å®¢â€è¿™ç±»æ–°é—®é¢˜ï¼Œä½“ç°äº†ä»é—®é¢˜è¯Šæ–­åˆ°æ–¹æ³•è¿­ä»£çš„å®Œæ•´ç ”ç©¶æ€è·¯ï¼Œä¸ºåç»­ç®—æ³•ä¼˜åŒ–æä¾›å‚è€ƒèŒƒå¼ã€‚

## sample-efficient-reinforcement-learning-controller-for-deep-brain-stimulation-in-parkinson-s-disease
### Abstract
Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¸•é‡‘æ£®ç—…è„‘æ·±éƒ¨åˆºæ¿€çš„æ ·æœ¬é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ˜¯ä¸€ç§è¿›è¡Œæ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œè„‘æ·±éƒ¨åˆºæ¿€ï¼ˆDBSï¼‰æ˜¯æœ‰æ•ˆçš„å¹²é¢„æ‰‹æ®µï¼Œä½†ä¼ ç»Ÿå¼€ç¯ç³»ç»Ÿç¼ºä¹é€‚åº”æ€§ã€èƒ½é‡æ•ˆç‡ä½ä¸”ä¸ªæ€§åŒ–ä¸è¶³ã€‚è‡ªé€‚åº”DBSï¼ˆaDBSï¼‰è™½ç”¨ç”Ÿç‰©æ ‡å¿—ç‰©åŠ¨æ€è°ƒèŠ‚åˆºæ¿€ï¼Œä½†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç°æœ‰æ–¹æ³•å­˜åœ¨æ ·æœ¬å¤æ‚åº¦é«˜ã€äºŒå…ƒåŠ¨ä½œç©ºé—´æ¢ç´¢ä¸ç¨³å®šã€èµ„æºå—é™ç¡¬ä»¶éƒ¨ç½²éš¾ç­‰é—®é¢˜ï¼Œå› æ­¤éœ€è¦æ›´é«˜æ•ˆçš„RL - aDBSæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSEA - DBSæ¡†æ¶
SEA - DBSæ˜¯æ ·æœ¬é«˜æ•ˆçš„æ¼”å‘˜ - è¯„è®ºå®¶æ¡†æ¶ï¼Œé’ˆå¯¹åŸºäºRLçš„è‡ªé€‚åº”ç¥ç»åˆºæ¿€æ ¸å¿ƒæŒ‘æˆ˜ã€‚æ•´åˆé¢„æµ‹å¥–åŠ±æ¨¡å‹ï¼Œä»çŠ¶æ€ - åŠ¨ä½œå¯¹ä¼°è®¡æœªæ¥ç»“æœï¼Œå‡å°‘å¯¹å®æ—¶ç¯å¢ƒäº¤äº’åé¦ˆçš„ä¾èµ–ï¼ŒåŠ é€Ÿç­–ç•¥å­¦ä¹ ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºGumbel - Softmaxçš„æ¢ç´¢ç­–ç•¥
åœ¨äºŒå…ƒåŠ¨ä½œç©ºé—´é‡‡ç”¨è¯¥ç­–ç•¥ï¼Œå®ç°ç»“æ„åŒ–ã€å¯å¾®åˆ†çš„åŠ¨ä½œé‡‡æ ·ï¼Œæ”¹å–„è®­ç»ƒç¨³å®šæ€§ä¸æ”¶æ•›æ€§ï¼Œè§£å†³äºŒå…ƒåˆºæ¿€æ§åˆ¶ä»»åŠ¡æ—©æœŸç­–ç•¥å­¦ä¹ é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¸•é‡‘æ£®ç—…åŸºåº•ç¥ç»èŠ‚æ´»åŠ¨çš„ç”Ÿç‰©é€¼çœŸæ¨¡æ‹Ÿä¸Šè¯„ä¼°SEA - DBSï¼Œç»“æœæ˜¾ç¤ºå…¶æ”¶æ•›æ›´å¿«ï¼Œå¯¹ç—…ç†æ€§Î²æ³¢æ®µåŠŸç‡æŠ‘åˆ¶æ›´å¼ºï¼Œä¸”å¯¹è®­ç»ƒåFP16é‡åŒ–æœ‰éŸ§æ€§ã€‚ä¸æ ‡å‡†DDPGåŸºçº¿ç›¸æ¯”ï¼Œå†…å­˜å ç”¨æ˜¾è‘—å‡å°‘ï¼Œèƒ½åœ¨èµ„æºå—é™ç¡¬ä»¶éƒ¨ç½²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°çœ‹ï¼Œå°†é¢„æµ‹å¥–åŠ±å»ºæ¨¡ä¸åŸºäºGumbel - Softmaxçš„æ¢ç´¢ç»“åˆæå‡æ ·æœ¬æ•ˆç‡ï¼Œä¸ºè§£å†³RLåœ¨æ ·æœ¬å—é™åœºæ™¯ï¼ˆå¦‚ä¸´åºŠé¢†åŸŸï¼‰é—®é¢˜æä¾›æ€è·¯ï¼›åœ¨ç¡¬ä»¶é€‚é…æ–¹é¢ï¼Œå¯¹FP16é‡åŒ–çš„é²æ£’æ€§è¯æ˜å…¶åœ¨èµ„æºå—é™ç¥ç»è°ƒèŠ‚ç¡¬ä»¶éƒ¨ç½²çš„æ½œåŠ›ï¼Œä¸ºåŒ»ç–—è®¾å¤‡ç­‰èµ„æºå—é™åœºæ™¯çš„RLåº”ç”¨æä¾›å€Ÿé‰´ï¼›é’ˆå¯¹ç‰¹å®šé¢†åŸŸï¼ˆç¥ç»åˆºæ¿€ï¼‰çš„RLæ”¹è¿›ï¼Œä¸ºé¢†åŸŸç‰¹å®šçš„RLæ–¹æ³•è®¾è®¡æä¾›å‚è€ƒï¼Œå³ç»“åˆé¢†åŸŸéœ€æ±‚å®šåˆ¶RLç»„ä»¶ï¼ˆå¦‚åŠ¨ä½œç©ºé—´æ¢ç´¢ã€å¥–åŠ±æœºåˆ¶ç­‰ï¼‰ã€‚

## a-technical-survey-of-reinforcement-learning-techniques-for-large-language-models
### Abstract
Reinforcement Learning (RL) has emerged as a transformative approach for
aligning and enhancing Large Language Models (LLMs), addressing critical
challenges in instruction following, ethical alignment, and reasoning
capabilities. This survey offers a comprehensive foundation on the integration
of RL with language models, highlighting prominent algorithms such as Proximal
Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally,
it provides an extensive technical overview of RL techniques specifically
tailored for LLMs, including foundational methods like Reinforcement Learning
from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced
strategies such as Direct Preference Optimization (DPO) and Group Relative
Policy Optimization (GRPO). We systematically analyze their applications across
domains, i.e., from code generation to tool-augmented reasoning. We also
present a comparative taxonomy based on reward modeling, feedback mechanisms,
and optimization strategies. Our evaluation highlights key trends. RLHF remains
dominant for alignment, and outcome-based RL such as RLVR significantly
improves stepwise reasoning. However, persistent challenges such as reward
hacking, computational costs, and scalable feedback collection underscore the
need for continued innovation. We further discuss emerging directions,
including hybrid RL algorithms, verifier-guided training, and multi-objective
alignment frameworks. This survey serves as a roadmap for researchers advancing
RL-driven LLM development, balancing capability enhancement with safety and
scalability.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å…¨æ™¯ï¼šä»å¯¹é½åˆ°èƒ½åŠ›å¢å¼ºçš„æŠ€æœ¯å·¡ç¤¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå±•ç°å¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨**å¯¹é½äººç±»ä»·å€¼è§‚**ï¼ˆå¦‚é¿å…æœ‰å®³è¾“å‡ºã€éµå¾ªæŒ‡ä»¤ï¼‰ä¸**å¢å¼ºæ¨ç†èƒ½åŠ›**ï¼ˆå¦‚æ•°å­¦æ¨ç†ã€å¤æ‚ä»»åŠ¡æ‹†è§£ï¼‰æ–¹é¢ä»é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ã€‚ä¼ ç»Ÿç›‘ç£å­¦ä¹ ä¾èµ–æ ‡æ³¨æ•°æ®ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚å¤šç›®æ ‡ä¼˜åŒ–ä¸éå¯å¾®åé¦ˆä¿¡å·ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºâ€œè¯•é”™ä¸­å­¦ä¹ â€çš„èŒƒå¼ï¼Œä¸ºLLMsè§£å†³å¯¹é½ä¸èƒ½åŠ›æå‡éš¾é¢˜æä¾›äº†å…³é”®è·¯å¾„ï¼Œä½†LLMsçš„é«˜ç»´ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆè¯æ±‡è¡¨ï¼‰ã€ä¸»è§‚ Reward é‡åŒ–ï¼ˆå¦‚æ–‡æœ¬è´¨é‡ã€æ— å®³æ€§ï¼‰ç­‰ç‰¹æ€§ï¼Œä¹Ÿè®©ä¼ ç»ŸRLæ–¹æ³•â€œæ°´åœŸä¸æœâ€ã€‚å› æ­¤ï¼Œç³»ç»Ÿæ¢³ç†RLä¸LLMsç»“åˆçš„æŠ€æœ¯ä½“ç³»ï¼Œæˆä¸ºæ¨åŠ¨LLMå®‰å…¨ä¸èƒ½åŠ›ååŒå‘å±•çš„è¿«åˆ‡éœ€æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢æŠ€æœ¯è°±ç³»æ¢³ç†ï¼Œè¦†ç›–ä»åŸºç¡€åˆ°å‰æ²¿çš„RL-LLMæ–¹æ³•  
è®ºæ–‡é¦–æ¬¡ç³»ç»Ÿæ€§æ•´åˆRLåœ¨LLMsä¸­çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ—¢åŒ…å«**åŸºç¡€èŒƒå¼**ï¼ˆå¦‚äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ RLHFã€AIåé¦ˆå¼ºåŒ–å­¦ä¹ RLAIFï¼‰ï¼Œä¹Ÿæ·±å…¥è§£æ**å‰æ²¿ä¼˜åŒ–ç­–ç•¥**ï¼ˆå¦‚ç›´æ¥åå¥½ä¼˜åŒ–DPOã€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–GRPOï¼‰ã€‚ä¾‹å¦‚RLHFé€šè¿‡â€œç¤ºèŒƒå¾®è°ƒâ†’å¥–åŠ±æ¨¡å‹è®­ç»ƒâ†’ç­–ç•¥ä¼˜åŒ–ï¼ˆå¦‚PPOï¼‰â€ä¸‰é˜¶æ®µï¼Œæˆä¸ºå¯¹é½äººç±»åå¥½çš„â€œäº‹å®æ ‡å‡†â€ï¼›è€ŒDPOåˆ™è·³è¿‡æ˜¾å¼å¥–åŠ±å»ºæ¨¡ï¼Œç›´æ¥ç”¨åå¥½å¯¹ä¼˜åŒ–ç­–ç•¥ï¼Œå¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦ä¸è®­ç»ƒä¸ç¨³å®šæ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢†åŸŸçº§åº”ç”¨ä¸æŠ€æœ¯åˆ†ç±»æ¡†æ¶ï¼Œæ‰“é€šâ€œæ–¹æ³•-åœºæ™¯-è¯„ä¼°â€é“¾è·¯  
è®ºæ–‡ä¸ä»…èšç„¦ç®—æ³•ï¼Œæ›´å°†RL-LLMæŠ€æœ¯æ˜ å°„åˆ°**å¤šé¢†åŸŸåº”ç”¨**ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€å·¥å…·å¢å¼ºæ¨ç†ï¼‰ï¼Œå±•ç°RLåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ³›åŒ–åŠ›ã€‚åŒæ—¶ï¼Œæå‡ºåŸºäº**å¥–åŠ±å»ºæ¨¡ã€åé¦ˆæœºåˆ¶ã€ä¼˜åŒ–ç­–ç•¥**çš„æ¯”è¾ƒæ€§åˆ†ç±»æ³•ï¼Œä¸ºç ”ç©¶è€…æä¾›â€œæŠ€æœ¯-åœºæ™¯â€åŒ¹é…çš„æ¸…æ™°æ¡†æ¶â€”â€”æ¯”å¦‚â€œç»“æœå¯¼å‘RLï¼ˆå¦‚RLVRï¼‰â€é€šè¿‡å¯¹æ¨ç†æ­¥éª¤çš„é€é˜¶æ®µåé¦ˆï¼Œæ˜¾è‘—æå‡æ•°å­¦æ¨ç†ç²¾åº¦ï¼ˆå¦‚GSM8KåŸºå‡†ä¸ŠGPT-3.5ä»56.8%â†’72.5%ï¼‰ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡è·¨æ–¹æ³•ã€è·¨ä»»åŠ¡çš„å¯¹æ¯”ï¼Œæ­ç¤ºå…³é”®è¶‹åŠ¿ï¼š  
- RLHFåœ¨â€œäººç±»å¯¹é½â€åœºæ™¯ä¸­ä»å ä¸»å¯¼ï¼ŒéªŒè¯å…¶åœ¨æŒ‡ä»¤éµå¾ªã€æ— å®³æ€§ä¼˜åŒ–çš„æ ‡æ†åœ°ä½ï¼›  
- ç»“æœå¯¼å‘RLï¼ˆå¦‚RLVRï¼‰åœ¨**åˆ†æ­¥æ¨ç†ä»»åŠ¡**ä¸­è¡¨ç°çªå‡ºï¼Œè¯æ˜â€œè¿‡ç¨‹åé¦ˆâ€å¯¹å¤æ‚é€»è¾‘èƒ½åŠ›çš„å¢å¼ºä»·å€¼ï¼›  
- æ–°å…´æ–¹æ³•ï¼ˆå¦‚DPOï¼‰åœ¨æ•ˆç‡ä¸æ€§èƒ½é—´å–å¾—å¹³è¡¡ï¼Œåœ¨æƒ…æ„Ÿæ§åˆ¶ã€æ‘˜è¦ç­‰ä»»åŠ¡ä¸Šæ¯”è‚©ä¼ ç»ŸRLHFï¼Œå´å¤§å¹…ç®€åŒ–è®­ç»ƒç®¡çº¿ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æŠ€æœ¯é€‰å‹å‚è€ƒ**ï¼šç ”ç©¶è€…å¯æ ¹æ®â€œå¯¹é½/æ¨ç†â€ç›®æ ‡ã€è®¡ç®—èµ„æºã€åé¦ˆå¯è·å¾—æ€§ï¼Œä»åˆ†ç±»æ¡†æ¶ä¸­å¿«é€ŸåŒ¹é…RLæ–¹æ³•ï¼ˆå¦‚è¿½æ±‚ä½æˆæœ¬å¯¹é½å¯é€‰RLAIFï¼Œè¿½æ±‚æ¨ç†ç²¾åº¦å¯é€‰RLVRï¼‰ï¼›  
2. **æŒ‘æˆ˜ä¸æ–¹å‘æ´å¯Ÿ**ï¼šè®ºæ–‡ç‚¹æ˜â€œå¥–åŠ±é»‘å®¢ï¼ˆæ¨¡å‹é’»Rewardæ¼æ´ï¼‰ã€è®¡ç®—æˆæœ¬ã€è§„æ¨¡åŒ–åé¦ˆé‡‡é›†â€ç­‰ç—›ç‚¹ï¼Œä¸ºåç»­ç ”ç©¶é”šå®šåˆ›æ–°æ–¹å‘ï¼ˆå¦‚æ··åˆRLç®—æ³•ã€éªŒè¯å™¨å¼•å¯¼è®­ç»ƒã€å¤šç›®æ ‡å¯¹é½æ¡†æ¶ï¼‰ï¼›  
3. **äº§ä¸šè½åœ°è§†è§’**ï¼šä¼ä¸šåœ¨LLMäº§å“åŒ–æ—¶ï¼Œå¯å‚è€ƒâ€œèƒ½åŠ›å¢å¼º+å®‰å…¨+å¯æ‰©å±•â€çš„å¹³è¡¡æ€è·¯ï¼Œä»RLæŠ€æœ¯ä¸­æ±²å–å¯¹é½ä¸æ¨ç†ä¼˜åŒ–çš„å®è·µè·¯å¾„ï¼ˆå¦‚ç”¨DPOç®€åŒ–è®­ç»ƒã€ç”¨RLAIFé™ä½äººå·¥æ ‡æ³¨ä¾èµ–ï¼‰ã€‚  


è¿™ç¯‡ç»¼è¿°æ—¢æ˜¯RL-LLMé¢†åŸŸçš„â€œæŠ€æœ¯åœ°å›¾â€ï¼Œæ›´æ˜¯æ¨åŠ¨å¤§æ¨¡å‹å®‰å…¨ä¸èƒ½åŠ›ååŒè¿›åŒ–çš„â€œç ”ç©¶è·¯çº¿å›¾â€ï¼Œä¸ºå­¦æœ¯æ¢ç´¢ä¸äº§ä¸šè½åœ°æ¶èµ·æ¡¥æ¢ã€‚

## arf-rlhf--adaptive-reward-following-for-rlhf-through-emotion-driven-self-supervision-and-trace-biased-dynamic-optimization
### Abstract
With the rapid advancement of Reinforcement Learning from Human Feedback
(RLHF) and autoregressive transformers, state-of-the-art models such as
GPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and
personalization. However, most existing RLHF approaches (e.g., PPO, DPO) still
rely on a binary-preference (BT) paradigm, which, while reducing annotation
costs, still requires substantial human effort and captures only group-level
tendencies rather than individual preferences. To overcome these limitations,
we propose Adaptive Reward-Following (ARF), a self-assessment framework that
leverages a high-precision emotion analyzer achieving over 70% accuracy on
GoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback
into continuous preference scores. We further enrich and debias these signals
through lightweight data augmentations, including synonym replacement, random
trace truncation, and score bias annotation algorithm. A Dynamic Adapter
Preference Tracker continuously models evolving user tastes in real time,
enabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly
on these tracked rewards instead of coarse binary labels. Experiments on
Qwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate
that ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,
TB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF
presents a scalable, personalized, and cost-effective approach to RLHF LLMs
through autonomous reward modeling.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ARF-RLHFï¼šç”¨æƒ…æ„Ÿé©±åŠ¨è‡ªç›‘ç£ä¸è½¨è¿¹åç½®åŠ¨æ€ä¼˜åŒ–é©æ–°RLHF

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œè‡ªå›å½’Transformerçš„å¿«é€Ÿå‘å±•ï¼ŒåƒGPT - 4.0ã€DeepSeek R1ã€Llama 3.3ç­‰å‰æ²¿æ¨¡å‹æ„ˆå‘é‡è§†å›ç­”æ·±åº¦ä¸ä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰å¤šæ•°RLHFæ–¹æ³•ï¼ˆå¦‚PPOã€DPOï¼‰ä¾èµ–äºŒå…ƒåå¥½ï¼ˆBTï¼‰èŒƒå¼ï¼Œè™½é™ä½äº†æ ‡æ³¨æˆæœ¬ï¼Œä½†ä»éœ€å¤§é‡äººåŠ›ï¼Œä¸”ä»…èƒ½æ•æ‰ç¾¤ä½“å±‚é¢è¶‹åŠ¿è€Œéä¸ªä½“åå¥½ï¼Œè¿˜å­˜åœ¨æ ‡æ³¨åå·®ã€æ›´æ–°æ»åç­‰é—®é¢˜ã€‚ä¸ºå…‹æœè¿™äº›å±€é™ï¼Œè®ºæ–‡æå‡ºARF - RLHFæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªé€‚åº”å¥–åŠ±è·Ÿéšï¼ˆARFï¼‰è¯„åˆ†å™¨
åŸºäºäººç±»äº¤æµä¸­éšå«æ»¡æ„åº¦ä¿¡å·è¿™ä¸€è§‚å¯Ÿï¼Œåˆ©ç”¨åœ¨GoEmotionsã€Sentiment140ã€DailyDialogç­‰æ•°æ®é›†ä¸Šå‡†ç¡®ç‡è¶…70%çš„é«˜ç²¾åº¦æƒ…æ„Ÿåˆ†æå™¨ï¼Œå°†è‡ªç”±å½¢å¼çš„ç”¨æˆ·åé¦ˆè½¬åŒ–ä¸ºè¿ç»­åå¥½åˆ†æ•°ã€‚è¯„åˆ†å™¨åŸºäºè½»é‡çš„RoBERTa - miniæ¶æ„æ„å»ºï¼Œå¹³è¡¡ä½å»¶è¿Ÿä¸å¼ºè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå®ç°ä»QAå¯¹çš„åŠ¨æ€äº¤äº’åˆ†æä¸­è‡ªåŠ¨è¿›è¡Œåå¥½è¯„åˆ†ï¼Œæ›¿ä»£ä¼ ç»ŸBT - RLHFçš„äºŒå…ƒæ¯”è¾ƒåˆ†æ•°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¢å¼ºå‹æ•°æ®å¤„ç†ä¸åŠ¨æ€åå¥½è·Ÿè¸ª
é€šè¿‡è½»é‡æ•°æ®å¢å¼ºï¼ˆåŒä¹‰è¯æ›¿æ¢ã€éšæœºè½¨è¿¹æˆªæ–­ã€åˆ†æ•°åå·®æ ‡æ³¨ç®—æ³•ï¼‰æ¥ä¸°å¯Œå’Œå»åä¿¡å·ï¼›å€ŸåŠ©å¸¦ç»éªŒå›æ”¾ï¼ˆERï¼‰æœºåˆ¶çš„åŠ¨æ€é€‚é…å™¨åå¥½è·Ÿè¸ªå™¨ï¼Œç»“åˆè½¯æ ‡ç­¾å­¦ä¹ å‘¨æœŸæ€§æ›´æ–°è¯„åˆ†å™¨ï¼Œå®æ—¶å»ºæ¨¡ç”¨æˆ·å˜åŒ–çš„åå¥½ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼Œä¸ºåç»­ä¼˜åŒ–æä¾›æ›´ä¼˜çš„å¥–åŠ±ä¿¡å·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šTrace Biasï¼ˆTBï¼‰å¾®è°ƒç®—æ³•
æ— éœ€ä¾èµ–BTå¯¹æ•°æ®ï¼ŒåŸºäºåŠ¨æ€è¯„åˆ†å™¨åé¦ˆã€éšæœºè·¯å¾„æˆªæ–­å’Œè·¯å¾„åå·®æ ¡æ­£æå‡ºæ–°é¢–ä¼˜åŒ–ç­–ç•¥ï¼Œç›´æ¥åœ¨è·Ÿè¸ªåˆ°çš„å¥–åŠ±ä¸Šä¼˜åŒ–ï¼Œè€Œéç²—ç³™çš„äºŒå…ƒæ ‡ç­¾ï¼Œä¸”åœ¨ç†è®ºä¸Šä¸PPOå’ŒDPOç›®æ ‡ä¿æŒä¸€è‡´ï¼Œå®ç°ç¨³å®šä¸”æœ‰ç†è®ºä¾æ®çš„å¾®è°ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Qwen - 2/2.5ã€Gemma - 2ã€Llama - 3.2ç­‰æ¨¡å‹åŠå››ä¸ªåå¥½é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒARFç›¸å¯¹PPOæå‡3.3%ï¼Œç›¸å¯¹DPOæå‡7.6%ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æƒ…æ„Ÿé©±åŠ¨çš„è‡ªç›‘ç£æ€è·¯ï¼šåˆ©ç”¨ç”¨æˆ·äº¤äº’ä¸­éšå«çš„æƒ…æ„Ÿã€æ»¡æ„åº¦ç­‰ä¿¡å·æ¥æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œä¸ºå‡å°‘äººå·¥æ ‡æ³¨ä¾èµ–æä¾›äº†æ–°æ–¹å‘ã€‚
2. è½»é‡æ•°æ®å¢å¼ºä¸åŠ¨æ€è·Ÿè¸ªç»“åˆï¼šé€šè¿‡ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ‰‹æ®µä¸°å¯Œæ•°æ®å¹¶å»åï¼Œæ­é…åŠ¨æ€è·Ÿè¸ªæœºåˆ¶é€‚åº”ç”¨æˆ·åå¥½å˜åŒ–ï¼Œåœ¨æ•°æ®å¤„ç†å’Œæ¨¡å‹é€‚åº”æ€§æå‡ä¸Šæœ‰å€Ÿé‰´ä»·å€¼ã€‚
3. æ— äºŒå…ƒæ ‡ç­¾ä¾èµ–çš„ä¼˜åŒ–ç®—æ³•ï¼šTrace Biasç®—æ³•è·³å‡ºä¼ ç»ŸRLHFä¾èµ–äºŒå…ƒæ¯”è¾ƒçš„å±€é™ï¼Œä¸ºLLMçš„RLHFä¼˜åŒ–å¼€è¾Ÿäº†æ›´è‡ªä¸»ã€å¯æ‰©å±•çš„è·¯å¾„ï¼Œåœ¨ç®—æ³•åˆ›æ–°å±‚é¢æä¾›äº†å‚è€ƒã€‚

## self-guided-process-reward-optimization-with-redefined-step-wise-advantage-for-process-reinforcement-learning
### Abstract
Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— é¢å¤–å¼€é”€ï¼SPROé©æ–°æµç¨‹å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ä¼°è®¡ä¸å¥–åŠ±ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æµç¨‹å¼ºåŒ–å­¦ä¹ ï¼ˆProcess Reinforcement Learning, PRLï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œä½†å½“å‰å­˜åœ¨ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ˜¯å¼•å…¥é¢å¤–æµç¨‹å¥–åŠ±æ¨¡å‹ä¼šå¸¦æ¥é«˜æ˜‚è®¡ç®—å¼€é”€ï¼›äºŒæ˜¯ç¼ºä¹ç»Ÿä¸€çš„æµç¨‹çº§ä¼˜åŠ¿ä¼°è®¡ç†è®ºæ¡†æ¶ã€‚ç°æœ‰åŸºäºè¾…åŠ©æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰çš„æ–¹æ³•ï¼Œå­˜åœ¨è®­ç»ƒéš¾ï¼ˆäººå·¥æ ‡æ³¨æˆ–è‡ªåŠ¨æ ‡æ³¨éƒ½æœ‰å±€é™ï¼‰ã€è®¡ç®—æˆæœ¬é«˜ï¼ˆé¢å¤–æ¨¡å‹å ç”¨æ˜¾å­˜é™åˆ¶batchå¤§å°ï¼‰ã€åœ¨çº¿å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸‹éš¾æ‰©å±•ç­‰é—®é¢˜ã€‚åŒæ—¶ï¼ŒåƒPRIMEè¿™ç±»æ–¹æ³•è™½æ”¹è¿›äº†éšå¼PRMï¼Œä½†ä»ä¾èµ–è¾…åŠ©æ¨¡å‹å¸¦æ¥è®¡ç®—å¼€é”€ï¼Œä¸”ä¼˜åŠ¿ä¼°è®¡å­˜åœ¨åå·®ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§æ—¢æ— é¢å¤–è®¡ç®—è´Ÿæ‹…ï¼Œåˆèƒ½ç²¾å‡†ä¼°è®¡æµç¨‹ä¼˜åŠ¿çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæµç¨‹å¥–åŠ±è‡ªå¼•å¯¼ï¼Œæ‘†è„±é¢å¤–æ¨¡å‹ä¾èµ–  
SPROä»ç†è®ºä¸Šè¯æ˜ï¼Œæµç¨‹å¥–åŠ±å¯ç›´æ¥ä»ç­–ç•¥æ¨¡å‹æœ¬èº«æ¨å¯¼å¾—åˆ°ï¼Œæ— éœ€é¢å¤–è®­ç»ƒè¾…åŠ©æµç¨‹å¥–åŠ±æ¨¡å‹ã€‚è¿™æ—¢è§„é¿äº†PRMsè®­ç»ƒéš¾ã€è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œåˆä¿ç•™äº†ç»“æœç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚GRPOï¼‰çš„ç®€æ´æ€§ä¸å¯æ‰©å±•æ€§ï¼Œå¥‘åˆå·¥ä¸šè½åœ°éœ€æ±‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡å®šä¹‰é€æ­¥ä¼˜åŠ¿ï¼Œç²¾å‡†ä¼°è®¡æµç¨‹ä¼˜åŠ¿  
æå‡º**ç´¯ç§¯æµç¨‹å¥–åŠ±ï¼ˆCumulative Process Reward, CPRï¼‰**ä¸**æ©ç é€æ­¥ä¼˜åŠ¿ï¼ˆMasked Step Advantage, MSAï¼‰**ã€‚CPRéšå¼èšåˆå‰ç¼€åºåˆ—ä¸­æ‰€æœ‰å‰ç½®æ­¥éª¤çš„æµç¨‹å¥–åŠ±ï¼Œä½œä¸ºå„æ—¶é—´æ­¥é¢„æœŸå›æŠ¥çš„ä»£ç†ï¼Œæå‡ä¼°è®¡å‡†ç¡®æ€§ï¼›MSAåˆ™åœ¨å…±äº«promptçš„é‡‡æ ·ç»„å†…ï¼Œå¯¹ç›¸åŒæ­¥éª¤çš„å¥–åŠ±è¿›è¡Œåˆ†ç»„å½’ä¸€åŒ–ï¼Œå®ç°ä¸¥æ ¼çš„é€æ­¥åŠ¨ä½œä¼˜åŠ¿ä¼°è®¡ï¼Œå¥‘åˆåŸºäºä¼˜åŠ¿çš„ç­–ç•¥æ¢¯åº¦æ¡†æ¶ï¼ˆå¦‚PPOï¼‰ï¼Œå‡å°‘ä¼°è®¡åå·®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPROå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼š  
- è®­ç»ƒæ•ˆç‡æ¯” vanilla GRPO é«˜ 3.4 å€ï¼Œæµ‹è¯•å‡†ç¡®ç‡æå‡ 17.5%ï¼›  
- è®­ç»ƒå…¨ç¨‹ä¿æŒç¨³å®šä¸”è¾ƒé«˜çš„ç­–ç•¥ç†µï¼Œä¿è¯äº†å……åˆ†æ¢ç´¢ï¼›åŒæ—¶å¹³å‡å“åº”é•¿åº¦ç¼©çŸ­çº¦ 1/3ï¼Œé¿å…â€œå¥–åŠ±é»‘å®¢â€ï¼ˆreward hackingï¼‰é—®é¢˜ï¼›  
- ä¸ GRPO ç­‰ç»“æœç›‘ç£RLæ–¹æ³•ç›¸æ¯”ï¼Œæ— é¢å¤–è®¡ç®—å¼€é”€ï¼Œæ›´é€‚é…å·¥ä¸šåœºæ™¯ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ— é¢å¤–æ¨¡å‹çš„å¥–åŠ±è®¾è®¡æ€è·¯**ï¼šSPROè¯æ˜ä»ç­–ç•¥æ¨¡å‹è‡ªèº«æŒ–æ˜æµç¨‹å¥–åŠ±çš„å¯è¡Œæ€§ï¼Œä¸ºåç»­å‡å°‘å¼ºåŒ–å­¦ä¹  pipeline å¤æ‚åº¦æä¾›äº†æ–°æ€è·¯ï¼Œå°¤å…¶é€‚åˆå¯¹ç®—åŠ›ã€æ˜¾å­˜æ•æ„Ÿçš„å·¥ä¸šçº§åº”ç”¨ï¼›  
2. **é€æ­¥ä¼˜åŠ¿çš„ç†è®ºæ¡†æ¶**ï¼šé€šè¿‡CPRå’ŒMSAé‡æ–°å®šä¹‰æµç¨‹çº§ä¼˜åŠ¿ä¼°è®¡ï¼Œæ—¢è´´åˆä¼ ç»Ÿç­–ç•¥æ¢¯åº¦æ¡†æ¶ï¼Œåˆåˆ©ç”¨æ©ç æ³¨æ„åŠ›ç­‰ç»“æ„ç¼–ç å‰ç¼€ä¿¡æ¯ï¼Œè¿™ç§â€œåˆ†ç»„+å½’ä¸€åŒ–â€çš„é€æ­¥ä¼˜åŠ¿ä¼°è®¡èŒƒå¼ï¼Œå¯ä¸ºå…¶ä»–åºåˆ—å†³ç­–ç±»ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€é•¿æ–‡æœ¬æ¨ç†ï¼‰çš„ä¼˜åŠ¿è®¡ç®—æä¾›å‚è€ƒï¼›  
3. **æ•ˆç‡ä¸æ¢ç´¢çš„å¹³è¡¡**ï¼šåœ¨æå‡è®­ç»ƒæ•ˆç‡åŒæ—¶ä¿è¯ç­–ç•¥ç†µä¸æŠ‘åˆ¶æ— æ•ˆå“åº”é•¿åº¦ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨â€œæ¢ç´¢-åˆ©ç”¨â€ä¸â€œå¥–åŠ±åˆç†æ€§â€ä¸Šçš„å…¼é¡¾èƒ½åŠ›ï¼Œå¯¹æ„å»ºæ›´é²æ£’çš„å¤§æ¨¡å‹æ¨ç†ç­–ç•¥æœ‰å€Ÿé‰´ä»·å€¼ã€‚

## activation-reward-models-for-few-shot-model-alignment
### Abstract
Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°æ ·æœ¬æ¨¡å‹å¯¹é½æ–°èŒƒå¼ï¼šæ¿€æ´»å¥–åŠ±æ¨¡å‹ï¼ˆActivation RMsï¼‰

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§ multimodal æ¨¡å‹ï¼ˆLMMsï¼‰çš„å®é™…åº”ç”¨ä¸­ï¼Œè®©æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½æ˜¯æå‡ç”Ÿæˆè´¨é‡çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡æ–¹æ³•éœ€åœ¨å¤§è§„æ¨¡åå¥½æ•°æ®é›†ä¸Šè®­ç»ƒç‹¬ç«‹å¥–åŠ±æ¨¡å‹ï¼Œéš¾ä»¥çµæ´»é€‚é…æ–°åå¥½ï¼›è€Œä¾èµ–å¤§æ¨¡å‹è‡ªèº«åšå¥–åŠ±è¯„ä¼°çš„æ–¹æ³•ï¼ˆå¦‚ LLM-as-a-Judgeï¼‰åœ¨ç‰¹å®šåœºæ™¯ï¼ˆå¦‚æŠµå¾¡ prompt æ”»å‡»ã€é˜²èŒƒå¥–åŠ±é»‘å®¢è¡Œä¸ºï¼‰ä¸‹è¡¨ç°ä¸è¶³ã€‚åŒæ—¶ï¼Œç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨åº”å¯¹æ–°ä»»åŠ¡ã€æ–°å®‰å…¨å¨èƒä¸åè§æ—¶ï¼Œç¼ºä¹é«˜æ•ˆçµæ´»çš„é€‚é…èƒ½åŠ›ï¼Œè¿™äº›ç—›ç‚¹æ¨åŠ¨äº†å¯¹å°æ ·æœ¬ä¸‹é«˜æ•ˆå¥–åŠ±å»ºæ¨¡æ–¹æ³•çš„æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ¿€æ´»å¥–åŠ±æ¨¡å‹ï¼ˆActivation RMsï¼‰  
Activation RMs æ˜¯ä¸€ç§å°æ ·æœ¬å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œç»“åˆ**æ¿€æ´»å¼•å¯¼ï¼ˆactivation steeringï¼‰**ä¸**ç”Ÿæˆå¼è¯„åˆ†**ã€‚ç»™å®šå°‘é‡å¸¦æ ‡ç­¾çš„åå¥½ç¤ºä¾‹ï¼Œä»å¤§æ³¨æ„åŠ›æ¨¡å‹ä¸­æå–ç¨€ç–çš„ã€åŒ…å«ä¿¡æ¯çš„æ³¨æ„åŠ›å¤´æ¿€æ´»ï¼Œæ¨ç†æ—¶ç”¨è¿™äº›æ¿€æ´»å¼•å¯¼æ¨¡å‹å†…éƒ¨è¡¨ç¤ºï¼ˆæ— éœ€å‚æ•°æ›´æ–°æˆ–é¢å¤–ä¸Šä¸‹æ–‡ï¼‰ï¼›å†é€šè¿‡ç®€å•ç”Ÿæˆå¼è¯„åˆ†ï¼ˆå¦‚å¯¹äºŒå…ƒåå¥½æŸ¥è¯¢è¾“å‡º â€œYesâ€  token çš„æ¦‚ç‡ï¼‰å¾—åˆ°å¥–åŠ±åˆ†æ•°ï¼Œå®ç°å¯¹æ–°ä»»åŠ¡å’Œåå¥½çš„å¿«é€Ÿé€‚é…ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»º PreferenceHack åŸºå‡†  
ä¸ºæµ‹è¯•å¥–åŠ±æ¨¡å‹åœ¨å¥–åŠ±é»‘å®¢è¡Œä¸ºä¸‹çš„é²æ£’æ€§ï¼Œæå‡ºé¦–ä¸ªä»¥é…å¯¹åå¥½æ ¼å¼è¯„ä¼°å¥–åŠ±é»‘å®¢æ¼æ´çš„åŸºå‡† PreferenceHackï¼Œè¦†ç›–è¯­è¨€å’Œå¤šæ¨¡æ€åœºæ™¯ï¼Œé’ˆå¯¹é•¿åº¦ã€æ ¼å¼åå·®ç­‰æ¨¡å‹åè§è®¾è®¡ä»»åŠ¡ï¼Œå¡«è¡¥äº†å°æ ·æœ¬è®¾ç½®ä¸‹å¥–åŠ±æ¨¡å‹å®‰å…¨æµ‹è¯•çš„ç©ºç™½ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- åœ¨æ ‡å‡†å¥–åŠ±å»ºæ¨¡åŸºå‡†ï¼ˆå¦‚ RewardBenchã€MultimodalRewardBenchï¼‰ä¸Šï¼ŒActivation RMs è¶…è¶Šç°æœ‰å°æ ·æœ¬æ–¹æ³•ï¼ˆå¦‚åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„ LLM-as-a-Judgeã€æŠ•ç¥¨è¯„åˆ†ã€token æ¦‚ç‡è¯„åˆ†ç­‰ï¼‰ã€‚  
- åœ¨ PreferenceHack åŸºå‡†æµ‹è¯•å¥–åŠ±é»‘å®¢è¡Œä¸ºæ—¶ï¼ŒActivation RMs å±•ç°å‡ºæ›´å¼ºé²æ£’æ€§ï¼Œæ€§èƒ½è¶…è¶Š GPT-4o ç­‰å¼ºåŸºçº¿ï¼ŒéªŒè¯äº†å…¶åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å®ç”¨ä»·å€¼ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- **æ–¹æ³•å±‚é¢**ï¼šæ¿€æ´»å¼•å¯¼ä¸ç”Ÿæˆå¼è¯„åˆ†çš„ç»“åˆä¸ºå°æ ·æœ¬å¥–åŠ±å»ºæ¨¡æä¾›æ–°æ€è·¯ï¼Œæ— éœ€å…¨é‡å¾®è°ƒå³å¯è®©æ¨¡å‹å¿«é€Ÿå¯¹é½æ–°åå¥½ï¼Œå¯å¯å‘åç»­è½»é‡å‹ã€çµæ´»å‹å¥–åŠ±æ¨¡å‹çš„è®¾è®¡ã€‚  
- **åŸºå‡†å±‚é¢**ï¼šPreferenceHack é¦–æ¬¡èšç„¦å°æ ·æœ¬ä¸‹å¥–åŠ±é»‘å®¢é—®é¢˜çš„é…å¯¹åå¥½è¯„ä¼°ï¼Œä¸ºå¥–åŠ±æ¨¡å‹å®‰å…¨æ€§æµ‹è¯•æä¾›äº†æ ‡å‡†åŒ–å·¥å…·ï¼Œæ¨åŠ¨é¢†åŸŸå¯¹æ¨¡å‹åè§ä¸å®‰å…¨å¨èƒçš„ç ”ç©¶æ·±åº¦ã€‚  
- **åº”ç”¨å±‚é¢**ï¼šActivation RMs åœ¨å®‰å…¨å…³é”®åœºæ™¯çš„æœ‰æ•ˆæ€§ï¼Œè¯´æ˜è¯¥æ–¹æ³•å¯è¿ç§»åˆ°éœ€è¦å¿«é€Ÿå“åº”æ–°å®‰å…¨éœ€æ±‚ï¼ˆå¦‚é˜²èŒƒ hallucinationã€åè§æ»¥ç”¨ï¼‰çš„å®é™…ä¸šåŠ¡ä¸­ï¼Œä¸ºæ¨¡å‹éƒ¨ç½²çš„å®‰å…¨æ€§æä¾›æŠ€æœ¯å‚è€ƒã€‚  

## skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy
### Abstract
Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-Reward-V2ï¼šäººæœºååŒè§£é”åå¥½æ•°æ®è§„æ¨¡ä¸è´¨é‡æ–°é«˜åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­è‡³å…³é‡è¦ï¼Œä½†å½“å‰å¼€æºå¥–åŠ±æ¨¡å‹åœ¨å¤šæ•°è¯„ä¼°åŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ•æ‰å¤æ‚ç²¾å¦™çš„äººç±»åå¥½ã€‚ç©¶å…¶åŸå› ï¼Œç°æœ‰åå¥½æ•°æ®é›†å­˜åœ¨èŒƒå›´ç‹­çª„ã€æ ‡ç­¾åˆæˆæ€§å¼ºæˆ–è´¨é‡æ§åˆ¶ä¸ä¸¥è°¨ç­‰å±€é™ï¼Œå³ä¾¿é‡‡ç”¨å…ˆè¿›è®­ç»ƒæŠ€æœ¯ä¹Ÿéš¾æœ‰å®è´¨æ€§æå‡ã€‚å› æ­¤ï¼Œæå‡åå¥½æ•°æ®è´¨é‡ä»¥æ¨åŠ¨å¼€æºå¥–åŠ±æ¨¡å‹å‘å±•æˆä¸ºå…³é”®è¯‰æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºè¶…å¤§è§„æ¨¡åå¥½æ•°æ®é›†SynPref - 40M  
æ‰“é€ äº†åŒ…å«4000ä¸‡åå¥½å¯¹çš„å¤§è§„æ¨¡åå¥½æ•°æ®é›†SynPref - 40Mï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æ•°æ®åŸºç¡€ï¼Œè¿™ä¹Ÿæ˜¯ç›®å‰å·²çŸ¥è§„æ¨¡æœ€å¤§çš„ç²¾å¿ƒæ•´ç†åå¥½æ··åˆæ•°æ®é›†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äººæœºååŒä¸¤é˜¶æ®µæ•°æ®æ•´ç† pipeline  
ç¬¬ä¸€é˜¶æ®µå€ŸåŠ©ä¸¥æ ¼åè®®ä¸‹çš„äººå·¥éªŒè¯ä¿éšœæ•°æ®è´¨é‡ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨äººç±»åå¥½å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºâ€œè£åˆ¤â€å®ç°è§„æ¨¡åŒ–æ•´ç†ï¼ŒåŒæ—¶ç»“åˆå¥–åŠ±æ¨¡å‹çš„è¿­ä»£è®­ç»ƒï¼ŒæŒç»­çº³å…¥äººå·¥æ ‡ç­¾åé¦ˆå¹¶å¬å›æ¨¡å‹è¡¨ç°å·®çš„åå¥½æ•°æ®ä»¥ä¿ƒè¿›å­¦ä¹ ï¼Œæœ€ç»ˆå¾—åˆ°2600ä¸‡é«˜è´¨é‡åå¥½å¯¹ç”¨äºæ¨¡å‹è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨å‡ºSkywork - Reward - V2ç³»åˆ—å¥–åŠ±æ¨¡å‹  
åŸºäºSynPref - 40Mç­›é€‰å‡ºçš„åå¥½æ•°æ®ï¼Œè®­ç»ƒå‡ºåŒ…å«8ä¸ªä»0.6Båˆ°8Bå‚æ•°è§„æ¨¡çš„Skywork - Reward - V2ç³»åˆ—å¥–åŠ±æ¨¡å‹ï¼Œä»…ç”¨Bradley - Terryç›®æ ‡å‡½æ•°è®­ç»ƒå´èƒ½åœ¨å¤šåŸºå‡†å±•ç°å“è¶Šæ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨7ä¸ªä¸»è¦å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSkywork - Reward - V2ç³»åˆ—è¡¨ç°äº®çœ¼ï¼Œ8Bè§„æ¨¡æ¨¡å‹åœ¨æ‰€æœ‰7ä¸ªåŸºå‡†ä¸Šå¤§å¹…è¶…è¶Šç°æœ‰å¼€æºå¥–åŠ±æ¨¡å‹ï¼›åœ¨äººç±»åå¥½å¯¹é½ã€å®¢è§‚æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€æŠ—é£æ ¼åå·®ã€best - of - Nç¼©æ”¾ç­‰å¤šå…³é”®ç»´åº¦ä¹Ÿå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒSynPref - 40Mçš„æˆåŠŸæ—¢æºäºè§„æ¨¡ä¹Ÿå¾—ç›Šäºé«˜è´¨é‡ï¼ŒåŒæ—¶äººæœºååŒ pipeline ä¸­äººå·¥æ ‡æ³¨ã€äººç±»åå¥½å¼•å¯¼çš„LLMæ ‡æ³¨åŠä¸¥è°¨æ ‡æ³¨åè®®éƒ½è‡³å…³é‡è¦ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ•°æ®å±‚é¢ï¼Œå¤§è§„æ¨¡ä¸”é«˜è´¨é‡çš„åå¥½æ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½æå‡ä½œç”¨æ˜¾è‘—ï¼ŒSynPref - 40Mçš„æ„å»ºæ€è·¯ä¸ºæ•°æ®é©±åŠ¨çš„æ¨¡å‹ä¼˜åŒ–æä¾›èŒƒä¾‹ï¼›æ–¹æ³•å±‚é¢ï¼ŒäººæœºååŒçš„ä¸¤é˜¶æ®µæ•°æ®æ•´ç† pipeline æœ‰æ•ˆç»“åˆäººç±»æ ‡æ³¨è´¨é‡ä¸AIå¯æ‰©å±•æ€§ä¼˜åŠ¿ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®é«˜è´¨é‡æ•´ç†æä¾›äº†å¯å‚è€ƒçš„æµç¨‹æ¡†æ¶ï¼›æ¨¡å‹å±‚é¢ï¼ŒSkywork - Reward - V2ç³»åˆ—è¯æ˜åˆç†åˆ©ç”¨æ•°æ®ä¸è®­ç»ƒç­–ç•¥ï¼Œèƒ½åœ¨å¼€æºå¥–åŠ±æ¨¡å‹é¢†åŸŸå®ç°æ€§èƒ½çªç ´ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹ç ”å‘æŒ‡æ˜æ–¹å‘ï¼Œå‡¸æ˜¾äº†æŒ–æ˜ç°æœ‰åå¥½æ•°æ®æ½œåŠ›ä¸äººæœºååŒåœ¨æå‡æ•°æ®è´¨é‡ä¸Šçš„ä»·å€¼ã€‚

## safer--probing-safety-in-reward-models-with-sparse-autoencoder
### Abstract
Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}
### ğŸŒŸ è®ºæ–‡è§£è¯» | SAFERï¼šç”¨ç¨€ç–è‡ªç¼–ç å™¨é€è§†å¥–åŠ±æ¨¡å‹çš„å®‰å…¨å¥¥ç§˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨å‡¸æ˜¾äº†å®‰å…¨ä¸å¯é æ€§æ–¹é¢çš„å…³é”®æ‹…å¿§ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯è®©æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„ä¸»æµæ–¹æ³•ï¼Œè€Œå…¶æ ¸å¿ƒçš„å¥–åŠ±æ¨¡å‹å´åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¸é€æ˜ã€‚ä¸€æ–¹é¢ï¼Œå¥–åŠ±æ¨¡å‹ä»…è¾“å‡ºæ ‡é‡åˆ†æ•°ï¼Œæ©ç›–äº†èƒŒåçš„è¯­ä¹‰ç‰¹å¾ï¼Œé™ä½äº†é€æ˜åº¦ã€å¯é æ€§ä¸å®‰å…¨æ€§ï¼›å¦ä¸€æ–¹é¢ï¼Œå¥–åŠ±æ¨¡å‹å¯¹åå¥½æ•°æ®çš„æ ‡æ³¨ååˆ†æ•æ„Ÿï¼Œå¾®å°æ”¹åŠ¨å°±å¯èƒ½å¤§å¹…å½±å“æ€§èƒ½ï¼Œä½†å½“å‰æ£€æµ‹ã€è§£é‡Šå’Œä¿®æ­£æœ‰å™ªå£°æˆ–æœ‰é—®é¢˜æ ‡æ³¨çš„æ–¹æ³•è¿˜å¾ˆæœ‰é™ã€‚å› æ­¤ï¼Œç†è§£å¥–åŠ±æ¨¡å‹å†…éƒ¨æœºåˆ¶å’Œåå¥½æ•°æ®é›†è‡³å…³é‡è¦ï¼Œæœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³å¥–åŠ±æ¨¡å‹å¯è§£é‡Šæ€§ä»¥åŠåå¥½æ•°æ®å¯¹å…¶å½±å“çš„ç†è§£é—®é¢˜è€Œå±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰å®ç°å¥–åŠ±æ¨¡å‹çš„æœºåˆ¶æ€§è§£é‡Š  
åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨è¿™ä¸€æœºåˆ¶å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå°†å¥–åŠ±æ¨¡å‹çš„æ¿€æ´»åˆ†è§£ä¸ºç¨€ç–ä¸”å¯è§£é‡Šçš„ç‰¹å¾ï¼ŒæŒ–æ˜å‡ºé©±åŠ¨å¥–åŠ±é¢„æµ‹çš„æ˜ç¡®è¯­ä¹‰å› ç´ ï¼Œé€šè¿‡è¯†åˆ«ä¸å®‰å…¨ç›¸å…³çš„ç‰¹å¾å¤§å¹…æå‡äº†å¥–åŠ±æ¨¡å‹çš„é€æ˜åº¦ã€‚SAEsæ—¨åœ¨æŠŠè¯­è¨€æ¨¡å‹æ¿€æ´»è¡¨ç¤ºä¸ºè¿‡å®Œå¤‡åŸºå‘é‡çš„ç¨€ç–ç»„åˆï¼Œè®­ç»ƒæ—¶æœ€å°åŒ–é‡å»ºæŸå¤±å¹¶å¯¹æ½œåœ¨å‘é‡æ–½åŠ ç¨€ç–çº¦æŸï¼Œé‡‡ç”¨TopK SAEé€šè¿‡ä¿ç•™top Kæ¿€æ´»æ¥å¢å¼ºç¨€ç–æ€§æ§åˆ¶ä¸ç‰¹å¾å¯è§£é‡Šæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSAFERæ¡†æ¶å®ç°åå¥½æ•°æ®é›†çš„ç‰¹å¾çº§åˆ†æä¸å¢å¼º  
é’ˆå¯¹å®‰å…¨ç›¸å…³æ–¹é¢ï¼ˆå› å…¶å®é™…é‡è¦æ€§æ˜¾è‘—ï¼‰ï¼Œå…ˆåœ¨å®‰å…¨å¯¼å‘çš„åå¥½æ•°æ®é›†ï¼ˆSafeRLHFå’ŒWildGuardMixï¼‰ä¸Šè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå†åœ¨è¯¥å¥–åŠ±æ¨¡å‹çš„éšè—çŠ¶æ€æ¿€æ´»ä¸Šè®­ç»ƒSAEä»¥æå–ç¨€ç–å¯è§£é‡Šç‰¹å¾ï¼›é€šè¿‡é‡åŒ–é€‰ä¸­å’Œæ‹’ç»å“åº”é—´æ¿€æ´»å·®å¼‚æ¥ç¡®å®šç‰¹å¾æ˜¾è‘—æ€§ï¼Œè¿›è€Œå®ç°å¯¹å…³é”®å®‰å…¨ç›¸å…³ç‰¹å¾çš„éš”ç¦»ä¸è§£é‡Šã€‚åŸºäºè¿™äº›ç‰¹å¾çº§ä¿¡å·ï¼Œè®¾è®¡é’ˆå¯¹æ€§çš„æ•°æ®æŠ•æ¯’å’Œå»å™ªç­–ç•¥ï¼Œå›ç­”äº†åå¥½æ•°æ®å¯¹å¥–åŠ±æ¨¡å‹å½±å“çš„é—®é¢˜ï¼Œå®ç°äº†åŸºäºå®‰å…¨ç›¸å…³æ€§çš„é¶å‘æ•°æ®æ“ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°æ®æŠ•æ¯’å®éªŒä¸­ï¼Œåè½¬è¡¨ç°å‡ºæœ€å¤§å®‰å…¨ç›¸å…³ç‰¹å¾æ¿€æ´»å·®å¼‚çš„å­é›†å¯¹ï¼Œç»“æœæ˜¾ç¤ºæŠ•æ¯’æ“ä½œèƒ½åœ¨æå°æ•°æ®æ”¹åŠ¨ä¸‹æ˜¾è‘—é™ä½å®‰å…¨åˆ†æ•°ï¼Œä¸”å¯¹æ¨¡å‹é€šç”¨èƒ½åŠ›ï¼ˆå¦‚èŠå¤©èƒ½åŠ›ï¼‰å‡ ä¹æ— å½±å“ï¼›åœ¨æ•°æ®å»å™ªå®éªŒä¸­ï¼Œç§»é™¤ç‰¹å¾æ¿€æ´»å·®å¼‚æœ€å°çš„å¯¹ï¼Œå»å™ªæ–¹æ³•æå‡äº†å¥–åŠ±æ¨¡å‹åœ¨å®‰å…¨è¯„ä¼°ä¸Šçš„æ€§èƒ½ã€‚è¿™è¡¨æ˜SAFERèƒ½ç²¾å‡†åœ°é™ä½æˆ–å¢å¼ºå®‰å…¨å¯¹é½åº¦ï¼ŒåŒæ—¶ä¸ç‰ºç‰²é€šç”¨èŠå¤©æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢ï¼Œä¸ºå¥–åŠ±æ¨¡å‹çš„è§£é‡Šã€å®¡è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°é€”å¾„ï¼Œå€ŸåŠ©ç¨€ç–è‡ªç¼–ç å™¨å®ç°æœºåˆ¶æ€§è§£é‡Šçš„æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–æ¨¡å‹å¯è§£é‡Šæ€§ä»»åŠ¡ä¸­ï¼›ä»åº”ç”¨å±‚é¢ï¼Œé’ˆå¯¹åå¥½æ•°æ®é›†çš„ç‰¹å¾çº§æ¢æµ‹ç­–ç•¥ï¼Œä¸ºé«˜é£é™©LLMå¯¹é½ä»»åŠ¡ä¸­æ•°æ®çš„å¤„ç†ï¼ˆæŠ•æ¯’ã€å»å™ªç­‰ï¼‰æä¾›äº†é¶å‘æ“ä½œçš„èŒƒä¾‹ï¼Œèƒ½å¯å‘åç»­åœ¨æ•°æ®å±‚é¢ä¼˜åŒ–æ¨¡å‹å¯¹é½ä¸å®‰å…¨çš„å·¥ä½œï¼›ä»£ç å¼€æºä¹Ÿä¸ºç ”ç©¶è€…å¤ç°å’Œæ‹“å±•ç›¸å…³ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œæ¨åŠ¨è¯¥æ–¹å‘çš„å‘å±•ã€‚ 

## residual-reward-models-for-preference-based-reinforcement-learning
### Abstract
Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºåå¥½å¼ºåŒ–å­¦ä¹ çš„æ®‹å·®å¥–åŠ±æ¨¡å‹ï¼šé«˜æ•ˆåˆ©ç”¨å…ˆéªŒçŸ¥è¯†åŠ é€Ÿç­–ç•¥å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œå¥–åŠ±å‡½æ•°å¯¹æ™ºèƒ½ä½“å­¦ä¹ è¡Œä¸ºè‡³å…³é‡è¦ï¼Œä½†å¤æ‚ä»»åŠ¡ä¸‹è®¾è®¡åˆé€‚å¥–åŠ±å‡½æ•°å¾ˆå›°éš¾ï¼Œæ˜“å‡ºç°æ¼æ´æˆ–æ— æ³•é€‚é…åœºæ™¯ç­‰é—®é¢˜ã€‚åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡äººç±»åé¦ˆå­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œé¿å…äº†ç¹ççš„å¥–åŠ±è®¾è®¡ï¼Œç„¶è€Œå®ƒå­˜åœ¨æ”¶æ•›æ…¢ã€åé¦ˆæ•ˆç‡ä½ç­‰é—®é¢˜ã€‚æ­¤å‰ç”¨æ¼”ç¤ºé¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹å†ç”¨åå¥½å¾®è°ƒçš„æ–¹æ³•ï¼Œå› é¢„è®­ç»ƒå’Œå¾®è°ƒæŸå¤±å‡½æ•°ä¸åŒï¼Œä¼šç»™ç¥ç»ç½‘ç»œä¼˜åŒ–å¸¦æ¥ä¸ç¨³å®šé£é™©ã€‚åŒæ—¶ï¼Œé€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰è™½ç”¨ä¸“å®¶æ¼”ç¤ºä¼°è®¡å¥–åŠ±å‡½æ•°ï¼Œä½†æ”¶é›†ä¸“å®¶æ¼”ç¤ºæˆæœ¬é«˜ä¸”å¤æ‚è¡Œä¸ºä¸‹éš¾è·å–ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆç»“åˆå…ˆéªŒçŸ¥è¯†æå‡PbRLæ€§èƒ½æˆä¸ºå…³é”®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ®‹å·®å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰  
å‡è®¾ç¯å¢ƒçœŸå®å¥–åŠ±ç”±å…ˆéªŒå¥–åŠ±å’Œå­¦ä¹ åˆ°çš„å¥–åŠ±ä¸¤éƒ¨åˆ†ç›¸åŠ æ„æˆã€‚å…ˆéªŒå¥–åŠ±æ˜¯è®­ç»ƒå‰å¯å¾—çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ç”¨æˆ·â€œæœ€ä½³çŒœæµ‹â€çš„å¥–åŠ±å‡½æ•°ã€IRLå¾—åˆ°çš„å¥–åŠ±å‡½æ•°ç­‰ï¼›å­¦ä¹ åˆ°çš„å¥–åŠ±åˆ™é€šè¿‡åå¥½æ•°æ®è®­ç»ƒã€‚è¿™ç§ç»“æ„åƒæ®‹å·®ç½‘ç»œæ€è·¯ï¼Œè®©æ®‹å·®å¥–åŠ±åœ¨ä¸æ”¹å˜å…ˆéªŒå¥–åŠ±åŸºç¡€ä¸Šï¼Œå¯¹ä¸äººç±»åå¥½ä¸ä¸€è‡´å¤„è¿›è¡Œä¿®æ­£ï¼Œèƒ½ä»è®­ç»ƒåˆæœŸå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ç›®æ ‡ç›¸å…³ç­–ç•¥ï¼Œä¹Ÿèƒ½å¼•å…¥æ›´ç»†ç²’åº¦ä»»åŠ¡ä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€‚é…å¤šåœºæ™¯çš„RRMç‰ˆæœ¬  
æå‡ºåŸºäºçŠ¶æ€ï¼ˆstate - basedï¼‰å’ŒåŸºäºå›¾åƒï¼ˆimage - basedï¼‰çš„RRMç‰ˆæœ¬ï¼Œè¦†ç›–ä¸åŒè¾“å…¥ç±»å‹çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡åœºæ™¯ï¼Œæ‹“å®½äº†æ–¹æ³•çš„é€‚ç”¨èŒƒå›´ï¼Œèƒ½åœ¨å¤šç§ç¯å¢ƒä¸‹åˆ©ç”¨å…ˆéªŒçŸ¥è¯†è¾…åŠ©åå¥½å¼ºåŒ–å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. åœ¨Meta - Worldç¯å¢ƒå¥—ä»¶å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°ï¼Œæ— è®ºæ˜¯åŸºäºçŠ¶æ€è¿˜æ˜¯å›¾åƒçš„ä»»åŠ¡ï¼ŒRRMéƒ½å¤§å¹…æå‡äº†å¸¸è§PbRLæ–¹æ³•ï¼ˆå¦‚PEBBLEç­‰ï¼‰çš„æ€§èƒ½ï¼Œåœ¨ä¸åŒç±»å‹å…ˆéªŒå¥–åŠ±ï¼ˆä»£ç†å¥–åŠ±ã€IRLå¾—åˆ°çš„å¥–åŠ±ç”šè‡³ä»£ç†å¥–åŠ±çš„å–åç‰ˆæœ¬ï¼‰ä¸‹éƒ½èƒ½å¸¦æ¥æ€§èƒ½æ”¹è¿›ã€‚  
2. åœ¨çœŸå®Franka Pandaæœºå™¨äººä¸Šå¼€å±•sim2realå®éªŒï¼ŒRRMèƒ½è®©æœºå™¨äººæ›´å¿«å­¦ä¹ åˆ°é«˜æˆåŠŸç‡ç­–ç•¥ï¼Œä¸åŒä»»åŠ¡ä¸­ç­–ç•¥å­¦ä¹ é€Ÿåº¦åŠ å¿«ï¼Œå®Œæˆä»»åŠ¡æ­¥éª¤æ•°æ¯”åŸºçº¿æ–¹æ³•æ›´å°‘ã€‚  
3. å³ä½¿åœ¨åé¦ˆå‡å°‘ï¼ˆæ€»åé¦ˆæœ‰é™æˆ–åé¦ˆé¢‘ç‡ä½ï¼‰ã€åé¦ˆé”™è¯¯ç­‰è®¾ç½®ä¸‹ï¼ŒRRMä¹Ÿèƒ½ä¿æŒç¨³å®šæ€§èƒ½ï¼Œå¯¹è¯­ä¹‰ç›¸åçš„å…ˆéªŒå¥–åŠ±ä¹Ÿæœ‰é²æ£’æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•è®¾è®¡å±‚é¢ï¼šæ®‹å·®ç»“æ„åœ¨ç»“åˆå…ˆéªŒçŸ¥è¯†æ–¹é¢çš„æ€è·¯å¯å€Ÿé‰´ï¼Œä¸ºå¤„ç†â€œå·²æœ‰éƒ¨åˆ†çŸ¥è¯†ä½†éœ€ç²¾ä¿®â€ç±»é—®é¢˜æä¾›äº†æ¨¡å‹ç»“æ„è®¾è®¡å‚è€ƒï¼Œåœ¨å¼ºåŒ–å­¦ä¹ å¥–åŠ±å­¦ä¹ ä¹‹å¤–çš„é¢†åŸŸï¼Œè‹¥æœ‰ç±»ä¼¼éœ€åˆ©ç”¨å…ˆéªŒå¹¶å¾®è°ƒä¿®æ­£çš„åœºæ™¯ï¼Œæ®‹å·®å¼çš„æ¨¡å—è®¾è®¡æˆ–è®¸èƒ½å¯å‘æ€è·¯ã€‚  
2. å®éªŒéªŒè¯å±‚é¢ï¼šå¤šåœºæ™¯ï¼ˆä»¿çœŸåˆ°çœŸå®æœºå™¨äººï¼‰ã€å¤šå…ˆéªŒç±»å‹ã€å¤šå¹²æ‰°æ¡ä»¶ï¼ˆå°‘åé¦ˆã€é”™è¯¯åé¦ˆç­‰ï¼‰çš„å®éªŒéªŒè¯æ–¹å¼ï¼Œä¸ºéªŒè¯æ–¹æ³•é²æ£’æ€§å’Œæ³›åŒ–æ€§æä¾›äº†èŒƒä¾‹ï¼Œåœ¨åç»­ç ”ç©¶ä¸­å¯å­¦ä¹ è¿™ç§å…¨é¢éªŒè¯çš„æ€è·¯ï¼Œæ›´å……åˆ†åœ°å±•ç¤ºæ–¹æ³•ä»·å€¼ã€‚  
3. åº”ç”¨æ‹“å±•å±‚é¢ï¼šRRMå¯¹ä¸åŒPbRLç®—æ³•çš„æ€§èƒ½æå‡è¡¨æ˜ï¼Œå®ƒå¯ä½œä¸ºä¸€ç§é€šç”¨çš„å¢å¼ºæ¨¡å—ï¼Œä¸ºç°æœ‰PbRLæŠ€æœ¯æ ˆå‡çº§æä¾›äº†æ–¹å‘ï¼Œåœ¨å·¥ä¸šç•Œæˆ–å­¦æœ¯ç•Œç°æœ‰åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿä¼˜åŒ–æ—¶ï¼Œå¯è€ƒè™‘å¼•å…¥è¯¥æ®‹å·®å¥–åŠ±æ¨¡å‹æ€è·¯æ¥åŠ é€Ÿç­–ç•¥å­¦ä¹ ä¸æå‡æ•ˆæœã€‚

## generalist-reward-models--found-inside-large-language-models
### Abstract
The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹ä¸­è—ç€é€šç”¨å¥–åŠ±æ¨¡å‹ï¼ŸLLMå¯¹é½æ–°èŒƒå¼æ¥äº†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»ä»·å€¼è§‚ï¼ˆå¦‚å¸®åŠ©æ€§ã€è¯šå®æ€§ï¼‰æ˜¯AIå‘å±•çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸»æµçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸¥é‡ä¾èµ–ç”¨æ˜‚è´µäººç±»åå¥½æ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ã€‚æ„å»ºä¼˜è´¨RMéœ€å¤§è§„æ¨¡é«˜è´¨é‡äººç±»åå¥½æ•°æ®é›†ï¼Œå­˜åœ¨æ…¢ã€è´µã€éš¾æ‰©å±•ç­‰é—®é¢˜ã€‚åç»­ç”¨AIåé¦ˆæ›¿ä»£äººç±»åé¦ˆçš„æ–¹æ³•ï¼ˆå¦‚RLAIFã€LLM-as-a-judgeï¼‰åˆç¼ºä¹ä¸¥è°¨ç†è®ºåŸºç¡€ï¼Œè¿˜æ˜“ç»§æ‰¿è£åˆ¤æ¨¡å‹çš„é£æ ¼åå·®ä¸åè§ã€‚é‚£ä¹ˆï¼Œé«˜è´¨é‡å¥–åŠ±ä¿¡å·æ˜¯å¦å¿…é¡»å¤–éƒ¨è·å–ï¼Ÿè¿™æˆä¸ºå…³é”®é—®é¢˜ï¼Œæœ¬æ–‡æ­£æ˜¯åŸºäºæ­¤å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°LLMä¸­ latent å­˜åœ¨é€šç”¨å¥–åŠ±æ¨¡å‹  
è®ºæ–‡å‘ç°ï¼Œä»»ä½•ç»æ ‡å‡†ä¸‹ä¸€ä¸ªtokené¢„æµ‹è®­ç»ƒçš„LLMä¸­ï¼Œå¤©ç„¶æ½œä¼ç€å¼ºå¤§çš„é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œå°†å…¶å‘½åä¸ºâ€œå†…ç”Ÿå¥–åŠ±ï¼ˆendogenous rewardï¼‰â€ã€‚æ— éœ€é¢å¤–è®­ç»ƒï¼Œå°±èƒ½ä»åŸºç¡€æ¨¡å‹ï¼ˆé¢„è®­ç»ƒæˆ–æœ‰ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼‰ä¸­ç›´æ¥æå–é«˜è´¨é‡å¥–åŠ±ä¿¡å·ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºå±‚é¢å»ºç«‹ä¸ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ çš„è”ç³»  
ä»ç†è®ºä¸Šè¯æ˜ï¼Œè¿™ç§å†…ç”Ÿå¥–åŠ±ç­‰ä»·äºé€šè¿‡ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰å­¦åˆ°çš„å¥–åŠ±å‡½æ•°ã€‚å…·ä½“è€Œè¨€ï¼ŒLLMçš„logitså¯ç›´æ¥è§£é‡Šä¸ºè½¯Qå‡½æ•°ï¼Œå€ŸåŠ©é€†è½¯Bellmanç®—å­èƒ½ä»ä¸­æ¢å¤å‡ºå¥–åŠ±å‡½æ•°ï¼Œä¸ºæå–å¥–åŠ±å‡½æ•°æä¾›äº†åŸç†æ€§æ–¹æ³•ï¼Œçªç ´äº†è¿‡å¾€å¯å‘å¼åšæ³•ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¯æ˜åŸºäºå†…ç”Ÿå¥–åŠ±çš„RLæœ‰æ•ˆæ€§  
è¯æ˜ç”¨è¯¥å†…ç”Ÿå¥–åŠ±è¿›è¡Œåç»­å¼ºåŒ–å­¦ä¹ åï¼Œå¾—åˆ°çš„ç­–ç•¥ç›¸æ¯”åŸºç¡€æ¨¡å‹æœ‰æ›´ä¼˜çš„è¯¯å·®ç•Œã€‚RLè¿‡ç¨‹èƒ½ä¿®æ­£æ ‡å‡†æ¨¡ä»¿å­¦ä¹ ï¼ˆä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼‰çš„å¤åˆè¯¯å·®ï¼ŒæŠŠæ€§èƒ½å·®è·ä»ä¸ä»»åŠ¡æ—¶é•¿ç›¸å…³çš„äºŒæ¬¡ä¾èµ–ï¼ˆO(HÂ²)ï¼‰é™åˆ°æ›´ä¼˜çš„çº¿æ€§ä¾èµ–ï¼ˆO(H)ï¼‰ã€‚è¿™æ˜¯é¦–æ¬¡ä»ç†è®ºä¸Šè¯æ˜LLMå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒéªŒè¯äº†ç†è®ºï¼šæå–å†…ç”Ÿå¥–åŠ±çš„æ–¹æ³•ä¸ä»…ä¼˜äºç°æœ‰LLM-as-a-judgeæ–¹æ³•ï¼Œè¿˜èƒ½è¶…è¶Šåœ¨æ˜‚è´µäººç±»æ ‡æ³¨æ•°æ®ä¸Šæ˜¾å¼è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡è¡¨æ˜å¥–åŠ±å»ºæ¨¡é˜¶æ®µå¯è¢«ä¸€ç§åŸç†æ€§æ–¹æ³•æ›¿ä»£â€”â€”æå–é¢„è®­ç»ƒé˜¶æ®µå·²æ•è·çš„çŸ¥è¯†ã€‚è¿™ä¸ºLLMå¯¹é½ä»¥åŠå¤šæ¨¡æ€æ¨¡å‹é¢†åŸŸï¼Œå¼€è¾Ÿäº†æ›´é«˜æ•ˆã€å¼ºå¤§ä¸”å¯æ‰©å±•çš„æ–°èŒƒå¼ï¼Œåç»­åœ¨æ¨¡å‹å¯¹é½ã€å¥–åŠ±å‡½æ•°è®¾è®¡ç­‰æ–¹å‘ï¼Œéƒ½å¯å€Ÿé‰´è¿™ç§â€œæŒ–æ˜æ¨¡å‹å†…åœ¨å·²æœ‰èƒ½åŠ›â€çš„æ€è·¯ï¼Œå‡å°‘å¯¹å¤–éƒ¨æ˜‚è´µæ•°æ®ä¸é¢å¤–è®­ç»ƒçš„ä¾èµ–ã€‚

## listener-rewarded-thinking-in-vlms-for-image-preferences
### Abstract
Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨Listenerå¢å¼ºçš„RLï¼Œè®©è§†è§‰è¯­è¨€æ¨¡å‹æ›´æ‡‚äººç±»å›¾åƒåå¥½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç”Ÿæˆå¼å»ºæ¨¡é¢†åŸŸï¼Œè®©è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç²¾å‡†æ•æ‰äººç±»è§†è§‰åå¥½æ˜¯å…³é”®éš¾é¢˜ã€‚ç°æœ‰å¥–åŠ±æ¨¡å‹å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜“å¯¼è‡´æ¨¡å‹è®°å¿†è®­ç»ƒæ•°æ®ï¼Œè¿˜éœ€è¦å¤æ‚æ ‡æ³¨æµç¨‹ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é‡Œçš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰è™½èƒ½æå‡æ³›åŒ–æ€§ï¼Œä½†ç ”ç©¶å‘ç°åŸºäºRLçš„åå¥½æ¨ç†æ¨¡å‹å­˜åœ¨â€œå¬ä¼—åˆ†æ­§ï¼ˆlistener disagreementï¼‰â€é—®é¢˜ï¼šå½“æ¨¡å‹æ¨ç†è½¨è¿¹å’Œç‹¬ç«‹å†»ç»“çš„è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆâ€œlistenerâ€ï¼‰å¯¹åŒä¸€è¾“å‡ºçš„è¯„ä¼°çŸ›ç›¾æ—¶ï¼Œæ¨ç†å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è§£å†³è¿™ç§æ¨ç†çŸ›ç›¾ã€æå‡æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½åº¦æ˜¯æœ¬æ–‡åŠ¨æœºæ‰€åœ¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡è®­ç»ƒæ€ç»´é“¾é£æ ¼æ¨ç†æ¨¡å‹é¢„æµ‹äººç±»å¯¹ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„è§†è§‰åå¥½
ä»¥å¾€å·¥ä½œè¾ƒå°‘é’ˆå¯¹ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„äººç±»è§†è§‰åå¥½æ¥è®­ç»ƒè¿™ç§æ€ç»´é“¾æ¨ç†æ¨¡å‹ï¼Œæœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œä¸ºè§†è§‰åå¥½é¢„æµ‹æä¾›æ–°çš„æ¨¡å‹è®­ç»ƒæ€è·¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯†åˆ«å¹¶é‡åŒ–â€œå¬ä¼—åˆ†æ­§â€è¿™ä¸€RLè§†è§‰åå¥½å»ºæ¨¡çš„ä¸»è¦å¤±æ•ˆæ¨¡å¼
é€šè¿‡åˆ†æå‘ç°ï¼Œå½“æ¨¡å‹é¢„æµ‹å’Œç‹¬ç«‹â€œlistenerâ€é¢„æµ‹å·®å¼‚å¢å¤§æ—¶ï¼ŒVLMå‡†ç¡®ç‡æŒç»­ä¸‹é™ï¼Œå°†è¿™ç§ç°è±¡æ˜ç¡®ä¸ºå…³é”®é—®é¢˜å¹¶é‡åŒ–ï¼Œä¸ºåç»­è§£å†³æ–¹æ³•æä¾›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡é¢å‘GRPOçš„listener - shapedè½¯å¥–åŠ±æœºåˆ¶
å¼•å…¥å†»ç»“çš„VLMâ€œlistenerâ€ï¼Œè®©å…¶ç‹¬ç«‹é‡æ–°å¤„ç†æ¨ç†æ¨¡å‹ï¼ˆreasonerï¼‰çš„æ€ç»´é“¾ï¼ˆæ’é™¤æœ€ç»ˆç­”æ¡ˆtokenï¼‰ï¼Œè¾“å‡ºå¯¹æ­£ç¡®é€‰æ‹©çš„æ ¡å‡†ç½®ä¿¡åˆ†æ•°ï¼Œå¹¶æ•´åˆåˆ°RLå¥–åŠ±ä¿¡å·ä¸­ã€‚è¿™æ ·æ—¢æƒ©ç½šæ— æ³•è¯´æœç‹¬ç«‹æ¨¡å‹çš„è§£é‡Šï¼Œåˆæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨å°±èƒ½æä¾›å¯†é›†ã€æ•°æ®é«˜æ•ˆçš„ç›‘ç£ï¼Œè®©æ¨ç†æ¨¡å‹ä¸ä»…ç­”æ¡ˆæ­£ç¡®ï¼Œæ¨ç†è¿‡ç¨‹ä¹Ÿèƒ½è¢«â€œlistenerâ€è®¤å¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ImageRewardæµ‹è¯•é›†ä¸Šï¼Œæ–¹æ³•è¾¾åˆ°67.4%çš„å½“å‰æœ€ä¼˜å‡†ç¡®ç‡ï¼›åœ¨å¤§è§„æ¨¡ï¼ˆ120ä¸‡æŠ•ç¥¨ï¼‰çš„Rapidata - HSPåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¤§å¹…è¶…è¶Šå¼ºGRPOå’ŒSFTåŸºçº¿ï¼›åŒæ—¶å‡å°‘äº†æ¨ç†çŸ›ç›¾æƒ…å†µï¼Œä¸”åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå³ä¾¿ç”¨å°‘é‡åå¥½æ•°æ®è®­ç»ƒï¼Œä¹Ÿèƒ½è®©è¾“å‡ºæ›´æ ¡å‡†ã€OODé²æ£’æ€§æ›´å¼ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼Œåˆ©ç”¨ç‹¬ç«‹æ¨¡å‹æ„å»ºå¥–åŠ±æœºåˆ¶æ¥å¯¹é½æ¨ç†è½¨è¿¹å’Œæœ€ç»ˆå†³ç­–çš„æ€è·¯ï¼Œä¸ºè§£å†³æ¨¡å‹æ¨ç†ä¸€è‡´æ€§é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼›ä»åº”ç”¨è§’åº¦ï¼Œè¯æ˜listenerå¢å¼ºçš„RLæ˜¯VLMsä¸­åå¥½å¯¹é½çš„æœ‰æ•ˆå®ç”¨å·¥å…·ï¼Œä¸ºä¸‹ä¸€ä»£æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„åå¥½å¯¹é½è§£å†³æ–¹æ¡ˆï¼Œåœ¨å·¥ä¸šç•Œå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹åå¥½è°ƒä¼˜åœºæ™¯æœ‰å€Ÿé‰´ä»·å€¼ï¼›ä»é—®é¢˜å‘ç°è§’åº¦ï¼Œå¯¹â€œå¬ä¼—åˆ†æ­§â€è¿™ç§å¤±æ•ˆæ¨¡å¼çš„è¯†åˆ«å’Œé‡åŒ–ï¼Œè®©åç»­ç ”ç©¶è€…èƒ½æ›´å…³æ³¨æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## trofi--trajectory-ranked-offline-inverse-reinforcement-learning
### Abstract
In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TROFIï¼šæ— é¢„å®šä¹‰å¥–åŠ±ä¸‹çš„ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆORLï¼‰ä¸­ï¼Œæ™ºèƒ½ä½“éœ€ä¾èµ–å¸¦å¥–åŠ±æ ‡ç­¾çš„å­˜å‚¨è½¬ç§»æ•°æ®è®­ç»ƒï¼Œä½†åœ¨æ¸¸æˆå¼€å‘ç­‰å®é™…åœºæ™¯ä¸­ï¼Œå¥–åŠ±å‡½æ•°å¾€å¾€éš¾ä»¥è·å–æˆ–å®šä¹‰ã€‚åŒæ—¶ï¼Œç°æœ‰é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰å’Œæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ–¹æ³•å¤šå‡è®¾æ¼”ç¤ºè½¨è¿¹æœ€ä¼˜ï¼Œä¸”é€‚é…åœ¨çº¿åœºæ™¯ï¼Œæ— æ³•ç›´æ¥ç”¨äºå«éä¸“å®¶ã€æ— å¥–åŠ±æ•°æ®çš„ç¦»çº¿åœºæ™¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºTROFIæ–¹æ³•ï¼Œæ—¨åœ¨æ— é¢„å®šä¹‰å¥–åŠ±å‡½æ•°å’Œæœ€ä¼˜ä¸“å®¶æ¼”ç¤ºä¸‹å­¦ä¹ ç¦»çº¿ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼±ç›‘ç£å¥–åŠ±å­¦ä¹ ä¸ç­–ç•¥è®­ç»ƒæµç¨‹  
TROFIåˆ†ä¸‰æ­¥å®ç°ç¦»çº¿ç­–ç•¥å­¦ä¹ ï¼šé¦–å…ˆé€šè¿‡T - REXï¼ˆTrajectory - ranked Reward EXtrapolationï¼‰åˆ©ç”¨äººç±»åå¥½è¿›è¡Œå¼±ç›‘ç£å­¦ä¹ ï¼Œå¾—åˆ°å¥–åŠ±æ¨¡å‹\(\hat{r}_\theta\)ï¼›æ¥ç€ç”¨è¯¥å¥–åŠ±æ¨¡å‹ä¸ºåŸå§‹æ— å¥–åŠ±ç¦»çº¿æ•°æ®é›†æ‰“æ ‡ç­¾ï¼›æœ€ååŸºäºæ‰“æ ‡åçš„æ•°æ®é›†ï¼Œç”¨TD3 + BCï¼ˆTwin Delayed Deep Deterministic plus Behavioral Cloningï¼‰è®­ç»ƒå‚æ•°åŒ–ç­–ç•¥\(\pi_\theta\)ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— éœ€æœ€ä¼˜è½¨è¿¹çš„å¥–åŠ±å­¦ä¹   
åŒºåˆ«äºå¤šæ•°IRL/ILæ–¹æ³•å¯¹æœ€ä¼˜æ¼”ç¤ºçš„å‡è®¾ï¼ŒTROFIå€ŸåŠ©T - REXä»…éœ€è½¨è¿¹çš„å®šæ€§æ’åºä¿¡æ¯æ¥å­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚T - REXé€šè¿‡ç¥ç»ç½‘è·¯è¿‘ä¼¼çŠ¶æ€ä¸‹çš„å¥–åŠ±ï¼Œåˆ©ç”¨è½¨è¿¹æ’åçº¦æŸï¼ˆå¦‚\(\tau_i \prec \tau_j\)æ—¶ï¼Œè½¨è¿¹\(\tau_j\)çš„ç´¯è®¡å¥–åŠ±é¢„æµ‹å€¼å¤§äº\(\tau_i\)ï¼‰æ„å»ºæŸå¤±å‡½æ•°è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€çŸ¥æ™“çœŸå®å¥–åŠ±å‡½æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTROFIæŒç»­è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½åª²ç¾ä½¿ç”¨çœŸå®å¥–åŠ±è®­ç»ƒç­–ç•¥çš„æ•ˆæœï¼›åŒæ—¶åœ¨3Dæ¸¸æˆç¯å¢ƒä¸­éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¯¹å¥–åŠ±æ¨¡å‹çš„ç ”ç©¶è¡¨æ˜ï¼Œè®¾è®¡è‰¯å¥½ã€æ˜“å­¦ä¹ çš„å¥–åŠ±å‡½æ•°å¯¹ä¿è¯å€¼å‡½æ•°ä¸å®é™…æœªæ¥æŠ˜æ‰£å¥–åŠ±å¯¹é½è‡³å…³é‡è¦ï¼Œå‡¸æ˜¾å¥–åŠ±å‡½æ•°åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœºæ™¯çš„å…³é”®ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•å±‚é¢ï¼šä¸ºæ— å¥–åŠ±å‡½æ•°æˆ–éš¾è·å–æœ€ä¼˜æ¼”ç¤ºçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœºæ™¯æä¾›äº†å¼±ç›‘ç£è§£å†³æ–¹æ¡ˆï¼Œç»“åˆT - REXå’ŒTD3 + BCçš„æµç¨‹å¯å¤ç”¨è‡³ç±»ä¼¼æ•°æ®é©±åŠ¨ä¸”å¥–åŠ±ç¨€ç¼ºçš„ä»»åŠ¡ï¼ˆå¦‚æ¸¸æˆNPCè¡Œä¸ºå­¦ä¹ ã€å¤æ‚å·¥ä¸šåœºæ™¯ç­–ç•¥ä¼˜åŒ–ï¼‰ã€‚  
2. å®è·µå±‚é¢ï¼šå±•ç¤ºäº†æ¸¸æˆå¼€å‘è€…å¦‚ä½•é«˜æ•ˆåˆ©ç”¨å¤§è§„æ¨¡ç©å®¶æ— å¥–åŠ±æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡å¥–åŠ±å‡½æ•°å°±èƒ½è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä¸ºäº§ä¸šç•Œåˆ©ç”¨ç©å®¶è¡Œä¸ºæ•°æ®æä¾›æ–°æ€è·¯ã€‚  
3. ç†è®ºä¸åˆ†æå±‚é¢ï¼šå¼ºè°ƒäº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡çš„é‡è¦æ€§ï¼Œä¸ºåç»­ç ”ç©¶ä¸­å¥–åŠ±å‡½æ•°çš„æ„å»ºä¸ä¼˜åŒ–æä¾›äº†ç»éªŒæ€§æŒ‡å¯¼ï¼Œæç¤ºéœ€å…³æ³¨å¥–åŠ±å‡½æ•°çš„å¯å­¦ä¹ æ€§ä¸å’Œå®é™…ä»»åŠ¡ç›®æ ‡çš„å¯¹é½æ€§ã€‚

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonFlux-PRMï¼šé¢å‘å¤§æ¨¡å‹é•¿æ€ç»´é“¾æ¨ç†çš„è½¨è¿¹æ„ŸçŸ¥å‹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ¨ç†åœºæ™¯ï¼ˆå¦‚æ•°å­¦è§£é¢˜ï¼‰ä¸­ï¼ŒProcess Reward Modelsï¼ˆPRMsï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰æ˜¯ç›‘ç£ä¸­é—´æ¨ç†æ­¥éª¤çš„æœ‰åŠ›å·¥å…·ã€‚ä¸è¿‡ç°æœ‰PRMså­˜åœ¨æ˜æ˜¾å±€é™ï¼šå®ƒä»¬ä¸»è¦åŸºäºæ¨¡å‹æœ€ç»ˆè¾“å‡ºè®­ç»ƒï¼Œéš¾ä»¥å¯¹**è½¨è¿¹ - å“åº”ï¼ˆtrajectory - responseï¼‰**è¿™ç±»æ–°å…´è¾“å‡ºå½¢å¼çš„ä¸­é—´æ¨ç†è½¨è¿¹è¿›è¡Œé²æ£’è¯„ä¼°ã€‚åƒDeepseek - R1ç­‰å‰æ²¿æ¨ç†æ¨¡å‹ä¼šç”Ÿæˆâ€œå†—é•¿ã€æ¬ è§„æ•´çš„ä¸­é—´æ€è€ƒè½¨è¿¹ + ç®€æ´æœ€ç»ˆå“åº”â€çš„è½¨è¿¹ - å“åº”å¯¹ï¼Œè¿™ç±»æ•°æ®å¸¸è¢«ç”¨äºå°æ¨¡å‹è’¸é¦ï¼Œä½†ç°æœ‰PRMså› ä¸ä¸­é—´è½¨è¿¹åœ¨ç»“æ„ã€æ ¼å¼ä¸Šä¸åŒ¹é…ï¼Œä¸”è®­ç»ƒæ—¶ç¼ºä¹å¸¦å¥–åŠ±çš„è½¨è¿¹ - å“åº”æ•°æ®ï¼Œåœ¨ç›‘ç£è¿™ç±»æ•°æ®æ—¶æ•ˆæœä¸ä½³ç”šè‡³ä¼šæŸå®³ä¸‹æ¸¸è®­ç»ƒã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®©PRMsæ—¢èƒ½ç›‘ç£æœ€ç»ˆå“åº”ï¼Œåˆèƒ½æœ‰æ•ˆè¯„ä¼°ä¸­é—´æ€è€ƒè½¨è¿¹ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè½¨è¿¹æ„ŸçŸ¥çš„PRMâ€”â€”ReasonFlux - PRM  
ReasonFlux - PRMä¸“ä¸ºè¯„ä¼°è½¨è¿¹ - å“åº”å‹æ¨ç†ç—•è¿¹è®¾è®¡ï¼Œèåˆäº†**æ­¥éª¤çº§ï¼ˆstep - levelï¼‰**å’Œ**è½¨è¿¹çº§ï¼ˆtrajectory - levelï¼‰**ç›‘ç£ã€‚å®ƒåœ¨æ¶µç›–æ•°å­¦å’Œç§‘å­¦æ¨ç†çš„10ké«˜è´¨é‡è½¨è¿¹ - å“åº”å¯¹ curated æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½ä¸ºæ€è€ƒè½¨è¿¹å†…çš„æ¯ä¸ªæ­¥éª¤æä¾›ç»†ç²’åº¦å¥–åŠ±ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®©æ¨¡å‹ä¸­é—´æ€è€ƒè½¨è¿¹ä¸æœ€ç»ˆå“åº”æ›´å¯¹é½ï¼Œè§£å†³äº†ç°æœ‰PRMså¯¹ä¸­é—´è½¨è¿¹ç›‘ç£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šåœºæ™¯é€‚é…çš„å¥–åŠ±ç›‘ç£  
ReasonFlux - PRMé€‚é…ç¦»çº¿å’Œåœ¨çº¿å¤šç§åœºæ™¯ï¼š  
- ç¦»çº¿åœºæ™¯ï¼šä¸ºè½¨è¿¹ - å“åº”å¯¹æ‰“åˆ†ï¼Œç­›é€‰é«˜è´¨é‡æ•°æ®ï¼ŒåŠ©åŠ›å°æ¨¡å‹ä¸‹æ¸¸æœ‰ç›‘ç£å¾®è°ƒçš„è®­ç»ƒæ•°æ®ç²¾é€‰ï¼›  
- åœ¨çº¿åœºæ™¯ï¼šèå…¥GRPOç­‰ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ç­–ç•¥ä¼˜åŒ–æä¾›ç»†ç²’åº¦è¿‡ç¨‹å¥–åŠ±ï¼›  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest - time scalingï¼‰ï¼šé€šè¿‡å¥–åŠ±å¼•å¯¼çš„Best - of - Nç­–ç•¥ï¼Œè¯„ä¼°å¤šä¸ªç”Ÿæˆå“åº”å¹¶é€‰æœ€ä¼˜ï¼Œæå‡æ¨ç†æ€§èƒ½ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AIMEã€MATH500ã€GPQA - Diamondç­‰æŒ‘æˆ˜æ€§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux - PRMå±•ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼š  
- æ•°æ®é€‰æ‹©æ–¹é¢ï¼šReasonFlux - PRM - 7Bæ¯”å¼ºåŸºçº¿ï¼ˆå¦‚Qwen2.5 - Math - PRM - 72Bï¼‰å’Œäººå·¥ç­–åˆ’åŸºçº¿é€‰å‡ºçš„æ•°æ®é›†è´¨é‡æ›´é«˜ï¼›  
- æ€§èƒ½æå‡æ–¹é¢ï¼šReasonFlux - PRM - 7Båœ¨æœ‰ç›‘ç£å¾®è°ƒä¸­å¹³å‡æå‡12.1%ï¼Œå¼ºåŒ–å­¦ä¹ ä¸­å¹³å‡æå‡4.5%ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ä¸­å¹³å‡æå‡6.3%ï¼›  
- èµ„æºå‹å¥½å‹å‘å¸ƒï¼šè¿˜å‘å¸ƒäº†ReasonFlux - PRM - 1.5Bï¼Œé€‚é…èµ„æºå—é™åœºæ™¯ä¸è¾¹ç¼˜éƒ¨ç½²ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰ä¸åˆ†æè§’åº¦ï¼šé’ˆå¯¹æ–°å…´çš„è½¨è¿¹ - å“åº”è’¸é¦æ•°æ®è¶‹åŠ¿ï¼Œæ·±å…¥åˆ†æç°æœ‰PRMsåœ¨ç›‘ç£ä¸­é—´è½¨è¿¹æ—¶çš„é—®é¢˜ï¼ˆç»“æ„æ ¼å¼ä¸åŒ¹é…ã€è®­ç»ƒæ•°æ®ç¼ºå¤±ï¼‰ï¼Œè¿™ç§ä»äº§ä¸šæ–°æ•°æ®å½¢æ€åæ¨æŠ€æœ¯ç—›ç‚¹çš„æ€è·¯ï¼Œä¸ºåç»­ç ”ç©¶é”šå®šæ–¹å‘æä¾›å‚è€ƒï¼›  
2. å¤šç²’åº¦ç›‘ç£èåˆï¼šå°†æ­¥éª¤çº§å’Œè½¨è¿¹çº§ç›‘ç£ç»“åˆï¼Œä¸ºå¤„ç†â€œé•¿é“¾æ¡ã€å¤šé˜¶æ®µâ€çš„æ¨ç†ç±»ä»»åŠ¡æä¾›äº†ç»†ç²’åº¦å¥–åŠ±è®¾è®¡çš„èŒƒä¾‹ï¼Œå¯è¿ç§»åˆ°ä»£ç ç”Ÿæˆã€å¤æ‚å†³ç­–ç­‰éœ€åˆ†æ­¥è¯„ä¼°çš„åœºæ™¯ï¼›  
3. å¤šåœºæ™¯å·¥ç¨‹è½åœ°ï¼šä»ç¦»çº¿æ•°æ®ç­›é€‰ã€åœ¨çº¿RLä¼˜åŒ–åˆ°æµ‹è¯•æ—¶å¢å¼ºï¼Œå®Œæ•´è¦†ç›–å¤§æ¨¡å‹è®­ç»ƒ - æ¨ç†å…¨æµç¨‹çš„å¥–åŠ±ç›‘ç£ï¼Œå±•ç¤ºäº†æŠ€æœ¯æ–¹æ¡ˆåœ¨äº§ä¸šçº§è½åœ°ä¸­çš„å¤šç»´åº¦ä»·å€¼ï¼Œä¸ºæ‰“é€ ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹æ¨ç†å¢å¼ºç®¡çº¿æä¾›äº†å®è·µæ¨¡æ¿ï¼›  
4. èµ„æºåˆ†å±‚å‘å¸ƒï¼šåŒæ—¶æä¾›7Bå’Œ1.5Bè§„æ¨¡æ¨¡å‹ï¼Œå…¼é¡¾é«˜æ€§èƒ½ä¸èµ„æºå—é™åœºæ™¯ï¼Œä½“ç°äº†æŠ€æœ¯æ™®æƒ æ€§ï¼Œåœ¨å®é™…ä¸šåŠ¡ä¸­å¯æ ¹æ®ç®—åŠ›ã€å»¶è¿Ÿç­‰éœ€æ±‚çµæ´»é€‰æ‹©ï¼Œå¹³è¡¡æ•ˆæœä¸æˆæœ¬ã€‚  

## longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning
### Abstract
Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B
### ğŸŒŸ è®ºæ–‡è§£è¯» | LongWriter-Zeroï¼šç”¨å¼ºåŒ–å­¦ä¹ çªç ´è¶…é•¿æ–‡æœ¬ç”Ÿæˆéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¶…é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚ä¸‡å­—çº§æŠ¥å‘Šã€å™äº‹åˆ›ä½œç­‰ï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…åœºæ™¯ä¸­è‡³å…³é‡è¦çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰æŠ€æœ¯é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯æ¨¡å‹ç”Ÿæˆé•¿åº¦å­˜åœ¨ä¸Šé™ï¼ŒäºŒæ˜¯éšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œå†…å®¹è´¨é‡ï¼ˆè¿è´¯æ€§ã€ä¸€è‡´æ€§ã€ç»“æ„åˆç†æ€§ç­‰ï¼‰ä¼šæ˜¾è‘—ä¸‹é™ã€‚  

æ­¤å‰ä¸»æµæ–¹æ¡ˆï¼ˆå¦‚LongWriterï¼‰ä¾èµ–**æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼Œå³åœ¨äººå·¥æ„é€ çš„â€œæŒ‡ä»¤ - é•¿æ–‡æœ¬è¾“å‡ºâ€é…å¯¹æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ä½†è¿™ç§æ–¹å¼å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š  
- æ„é€ é«˜è´¨é‡çš„åˆæˆSFTæ•°æ®æˆæœ¬æé«˜ã€éš¾åº¦å¤§ï¼›  
- åˆæˆæ•°æ®å¾€å¾€ç¼ºä¹è¿è´¯æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸”é£æ ¼å•ä¸€ã€è¿‡åº¦â€œäººå·¥åŒ–â€ï¼›  
- SFTçš„æœ€å¤§ä¼¼ç„¶ç›®æ ‡æ— æ³•æ˜¾å¼ä¼˜åŒ–å…¨å±€å±‚é¢çš„æ–‡æœ¬å±æ€§ï¼ˆå¦‚æ•´ä½“è¿è´¯æ€§ã€æ ¼å¼ä¸€è‡´æ€§ï¼‰ã€‚  

ä¸ºçªç ´è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡º**å®Œå…¨ä»é›¶å¼€å§‹ã€ä¸ä¾èµ–ä»»ä½•æ ‡æ³¨/åˆæˆæ•°æ®**çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ¡ˆï¼Œè®©LLMè‡ªä¸»â€œè¿›åŒ–â€å‡ºè¶…é•¿é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ¡†æ¶  
ä¼ ç»ŸSFTä¾èµ–å›ºå®šå‚è€ƒæ–‡æœ¬ï¼Œè€Œæœ¬æ–‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè®©æ¨¡å‹é€šè¿‡**å¥–åŠ±ä¿¡å·**ä¼˜åŒ–é•¿æ–‡æœ¬ç”Ÿæˆçš„å…¨å±€ç›®æ ‡ï¼ˆæ— éœ€äººå·¥æ„é€ SFTæ•°æ®é›†ï¼‰ã€‚å…·ä½“é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•è®­ç»ƒç­–ç•¥ç½‘ç»œï¼šä»åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Qwen2.5 - 32Bï¼‰å‡ºå‘ï¼Œè®©æ¨¡å‹åœ¨â€œå†™ä½œè¿‡ç¨‹ä¸­è‡ªä¸»è§„åˆ’ä¸è¿­ä»£â€ï¼Œé€æ­¥æŒæ¡è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç»´åº¦å¥–åŠ±æ¨¡å‹è®¾è®¡ï¼ˆReward Designï¼‰  
é’ˆå¯¹å¼€æ”¾åŸŸæ–‡æœ¬ç”Ÿæˆçš„å¤æ‚æ€§ï¼ˆä¸»è§‚æ€§ã€å¤šç»´åº¦ï¼‰ï¼Œè®¾è®¡**å¤åˆå¥–åŠ±å‡½æ•°**ï¼Œæ•´åˆå¤šä¸ªä¸“é¡¹å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œåˆ†åˆ«å¼•å¯¼æ¨¡å‹ä¼˜åŒ–ä»¥ä¸‹å…³é”®ç»´åº¦ï¼š  
- é•¿åº¦æ§åˆ¶ï¼ˆLength RMï¼‰ï¼šç¡®ä¿è¾“å‡ºæ»¡è¶³â€œè¶…é•¿â€éœ€æ±‚ï¼ŒåŒæ—¶é¿å…æ— æ„ä¹‰å†—ä½™ï¼›  
- å†™ä½œè´¨é‡ï¼ˆQuality RMï¼‰ï¼šè¯„ä¼°å†…å®¹æµç•…åº¦ã€é€»è¾‘æ€§ã€ä¸“ä¸šæ€§ç­‰ï¼›  
- ç»“æ„æ ¼å¼ï¼ˆStructure RMï¼‰ï¼šä¿éšœæ–‡æœ¬ç»“æ„åˆç†ï¼ˆå¦‚åˆ†ç« èŠ‚ã€å±‚æ¬¡æ¸…æ™°ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - time Scalingï¼‰ä¸æŒç»­é¢„è®­ç»ƒï¼ˆContinual Pretrainingï¼‰  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼šå€Ÿé‰´å¤§æ¨¡å‹åœ¨æ•°å­¦/ä»£ç ä»»åŠ¡ä¸­â€œé•¿æ€ç»´é“¾ï¼ˆCoTï¼‰â€çš„æˆåŠŸç»éªŒï¼Œæ¢ç´¢åœ¨è¶…é•¿æ–‡æœ¬ç”Ÿæˆä¸­å¼•å…¥é•¿CoTï¼Œå¢å¼ºæ¨¡å‹æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ï¼›  
- æŒç»­é¢„è®­ç»ƒï¼šåœ¨é•¿æ–‡æœ¬ç´ æä¸æ¨ç†æ•°æ®ä¸ŠæŒç»­é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡RLè®­ç»ƒåæ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- åŸºå‡†æµ‹è¯•ç¢¾å‹ä¼ ç»ŸSFTï¼šåŸºäºQwen2.5 - 32Bè®­ç»ƒçš„LongWriter - Zeroï¼Œåœ¨WritingBenchã€Arena - Writeç­‰é•¿æ–‡æœ¬å†™ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼Œ**å…¨é¢è¶…è¶Šä¼ ç»ŸSFTæ–¹æ³•**ï¼›  
- è¶…è¶Šåƒäº¿å‚æ•°æ¨¡å‹ï¼šåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡»è´¥DeepSeek R1ã€Qwen3 - 235Bç­‰ç™¾ billion + è§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œåˆ·æ–°SOTAï¼›  
- å¼€æºèµ„æºä¸°å¯Œï¼šæ¨¡å‹ checkpoint å’Œæ•°æ®å·²å¼€æºï¼ˆhttps://huggingface.co/THU - KEG/LongWriter - Zero - 32Bï¼‰ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¯å¤ç°ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°ï¼šè¯æ˜å¼ºåŒ–å­¦ä¹ å¯åœ¨â€œæ— æ ‡æ³¨/åˆæˆæ•°æ®â€åœºæ™¯ä¸‹ï¼Œæ¿€æ´»LLMçš„è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹èƒ½åŠ›è§£é”æä¾›äº†â€œéSFTâ€çš„æ–°èŒƒå¼ï¼›  
2. å¥–åŠ±å·¥ç¨‹ï¼šå¤šç»´åº¦å¤åˆå¥–åŠ±æ¨¡å‹çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å¼€æ”¾åŸŸç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚åˆ›æ„å†™ä½œã€å¤šè½®å¯¹è¯ï¼‰ï¼Œç”¨äºåˆ»ç”»â€œä¸»è§‚æ€§å¼ºã€æ— æ˜ç¡®ground - truthâ€åœºæ™¯ä¸‹çš„è´¨é‡è¯„ä¼°ï¼›  
3. è®­ç»ƒç­–ç•¥ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆé•¿CoTï¼‰ä¸æŒç»­é¢„è®­ç»ƒçš„ç»„åˆï¼Œä¸ºæå‡å¤§æ¨¡å‹é•¿æ–‡æœ¬æ¨ç†ã€ç”Ÿæˆçš„ä¸Šé™æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯è·¯çº¿ï¼›  
4. è½åœ°ä»·å€¼ï¼šé’ˆå¯¹çœŸå®ä¸–ç•Œâ€œè¶…é•¿æ–‡æœ¬éœ€æ±‚â€ï¼ˆå¦‚æŠ¥å‘Šæ’°å†™ã€æ³•å¾‹æ–‡ä¹¦ã€æ•™è‚²å†…å®¹åˆ›ä½œï¼‰ï¼Œæä¾›äº†æ›´ä¼˜è´¨çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæ¨åŠ¨LLMåœ¨ä¸“ä¸šé¢†åŸŸçš„è½åœ°ã€‚  


LongWriter - Zeroçš„å·¥ä½œä¸ä»…è§£å†³äº†è¶…é•¿æ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ç—›ç‚¹ï¼Œæ›´å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹èƒ½åŠ›è¿›åŒ–ä¸­çš„æ½œåŠ›â€”â€”æ— éœ€ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨ï¼Œä¹Ÿèƒ½è®©æ¨¡å‹â€œè‡ªä¸»å­¦ä¹ â€å¤æ‚ä»»åŠ¡çš„å®Œæˆèƒ½åŠ›ã€‚è¿™ä¸ºå¤§æ¨¡å‹ç ”å‘èŒƒå¼ã€å¥–åŠ±æœºåˆ¶è®¾è®¡ç­‰æ–¹å‘ï¼Œéƒ½å¸¦æ¥äº†æå…·å¯å‘æ€§çš„å‚è€ƒã€‚

## reflective-verbal-reward-design-for-pluralistic-alignment
### Abstract
AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸ªæ€§åŒ–ä»·å€¼å¯¹é½æ–°èŒƒå¼ï¼šåæ€å¼è¯­è¨€å¥–åŠ±è®¾è®¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨AIä¸äººç±»ä»·å€¼å¯¹é½çš„é¢†åŸŸï¼Œä¸»æµçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•å­˜åœ¨å±€é™ï¼šäººç±»ä»·å€¼å…·æœ‰é«˜åº¦å¼‚è´¨æ€§ï¼Œä¸åŒäººä»·å€¼è§‚å·®å¼‚ç”šè‡³å†²çªï¼Œä½†RLHFå¸¸å°†æ‰€æœ‰äººåé¦ˆèšåˆä¸ºå•ä¸€å¥–åŠ±æ¨¡å‹ï¼Œè¿™ä¼šå¯¼è‡´å°‘æ•°ç¾¤ä½“åå¥½è¢«è¿‡åº¦å‹åˆ¶ã€‚æ­¤å¤–ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹äººç±»åå¥½ä¸æ˜¯ç®€å•â€œæå–â€è€Œæ˜¯â€œæ„å»ºâ€çš„ï¼Œç°æœ‰è¢«åŠ¨æ”¶é›†åå¥½ã€ç›´æ¥æ ‡æ³¨çš„æ–¹å¼éš¾ä»¥è®©ç”¨æˆ·å……åˆ†åæ€ä»¥å½¢æˆæ˜ç¡®åå¥½ã€‚å› æ­¤ï¼Œå¦‚ä½•å…¼é¡¾ä»·å€¼å¤šæ ·æ€§ä¸åå¥½æ„å»ºè¿‡ç¨‹ï¼Œå­¦ä¹ ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹æˆä¸ºå…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºInteractive - Reflective Dialogue Alignmentï¼ˆIRDAï¼‰æ¡†æ¶  
IRDAå€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡äº¤äº’å¼å¯¹è¯å­¦ä¹ ä¸ªæ€§åŒ–å¥–åŠ±å‡½æ•°ï¼ŒåŒ…å«ä¸‰éƒ¨åˆ†ï¼šä¸€æ˜¯**åæ€å¼è¯­è¨€åå¥½å¼•å¯¼**ï¼Œå¼•å¯¼ç”¨æˆ·æ¸…æ™°è¡¨è¾¾è‡ªèº«ä»·å€¼è§‚ï¼›äºŒæ˜¯**ä¸»åŠ¨å­¦ä¹ **ï¼Œç­–ç•¥æ€§é€‰æ‹©ç¤ºä¾‹ä¾›äººç±»æ‰¹åˆ¤ï¼Œæå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼›ä¸‰æ˜¯**LLMé©±åŠ¨çš„è¯­è¨€å¥–åŠ±å»ºæ¨¡**ï¼Œè®©LLMåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä»ç¨€ç–ç”¨æˆ·åé¦ˆä¸­æ³›åŒ–ï¼Œç›´æ¥ä½œä¸ºå¥–åŠ±å‡½æ•°è¯„ä¼°AIè¡Œä¸ºè½¨è¿¹ã€‚ç”¨LLMå¼•å¯¼çš„å¯¹è¯æ›¿ä»£è¢«åŠ¨æ ‡æ³¨ï¼Œæ¿€å‘ç”¨æˆ·æ·±æ€ç†Ÿè™‘çš„åæ€ï¼ˆSystem 2è®¤çŸ¥ï¼‰æ¥è§£å†³åå¥½æ„å»ºéš¾é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèåˆå¤šå­¦ç§‘è§†è§’å®ç°ä¸ªæ€§åŒ–å¯¹é½  
ç»“åˆAIã€äººæœºäº¤äº’ï¼ˆHCIï¼‰å’Œç¤¾ä¼šç§‘å­¦çŸ¥è¯†ï¼Œä¸å†å‡è®¾ç”¨æˆ·èƒ½ç›´æ¥æ¸…æ™°è¡¨è¾¾æ–°åœºæ™¯ä¸‹åå¥½ï¼Œè€Œæ˜¯ä¸»åŠ¨å¼•å¯¼ç”¨æˆ·é€šè¿‡åæ€å°†æ½œåœ¨ä»·å€¼è§‚è½¬åŒ–ä¸ºå…·ä½“åå¥½ï¼Œå¼¥è¡¥äº†è¿‡å¾€æ–¹æ³•åœ¨åå¥½æ„å»ºç¯èŠ‚çš„ä¸è¶³ï¼Œä¸ºä¸ªæ€§åŒ–AIå¯¹é½æä¾›æ–° pipelineã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¼€å±•ä¸¤é¡¹æ¶‰åŠ30åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ï¼šç¬¬ä¸€é¡¹é’ˆå¯¹â€œå°Šé‡è¡Œä¸ºâ€çš„ä¸ªäººå®šä¹‰æ„å»ºå¥–åŠ±æ¨¡å‹ï¼ˆ21äººå‚ä¸ï¼‰ï¼Œç¬¬äºŒé¡¹æ¢ç´¢è‡ªåŠ¨é©¾é©¶ä¼¦ç†å†³ç­–ï¼ˆ9äººå‚ä¸ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…ä»·å€¼åˆ¤æ–­å·®å¼‚æ˜¾è‘—ï¼ŒIRDAç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼ˆå¦‚éåæ€å¼è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼‰èƒ½æ›´å‡†ç¡®æ•æ‰ä¸ªä½“å¯¹ä»·å€¼å¯¹é½è¡Œä¸ºçš„å®šä¹‰ï¼Œåœ¨å‡†ç¡®ç‡ä¸Šæ¯”éåæ€å¼è¯­è¨€å¥–åŠ±æ¨¡å‹æå‡9 - 12%ï¼Œä¸”æ¯”ä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•æ ·æœ¬æ•ˆç‡æ›´é«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šå­¦ç§‘èåˆæ€è·¯ï¼šå°†AIæŠ€æœ¯ä¸HCIã€ç¤¾ä¼šç§‘å­¦ä¸­å…³äºäººç±»åå¥½æ„å»ºã€åæ€çš„ç ”ç©¶ç»“åˆï¼Œä¸ºAIå¯¹é½é—®é¢˜æä¾›æ›´è´´åˆäººç±»è®¤çŸ¥è§„å¾‹çš„è§£æ³•ï¼Œå¯ç¤ºåç»­ç ”ç©¶è·¨å­¦ç§‘è§£å†³å¤æ‚AIä¼¦ç†ä¸å¯¹é½é—®é¢˜ã€‚  
2. ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹æ„å»ºï¼šè¯æ˜äº†é€šè¿‡å¼•å¯¼åæ€ã€äº¤äº’å¼å¯¹è¯å­¦ä¹ ä¸ªæ€§åŒ–å¥–åŠ±å‡½æ•°çš„å¯è¡Œæ€§ï¼Œä¸ºé¢å‘ä¸ªäººçš„AIåŠ©æ‰‹ï¼ˆå¦‚ä¸ªæ€§åŒ–æ™ºèƒ½åŠ©ç†ï¼‰æä¾›æŠ€æœ¯å‚è€ƒï¼Œè®©AIå¥–åŠ±æ›´è´´åˆç”¨æˆ·ä¸ªä½“è€Œéç¾¤ä½“å¹³å‡ã€‚  
3. æ•°æ®é«˜æ•ˆå­¦ä¹ ï¼šä¸»åŠ¨å­¦ä¹ ç­–ç•¥åœ¨ä¸ªæ€§åŒ–åœºæ™¯ä¸‹æå‡æ ·æœ¬æ•ˆç‡ï¼Œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„å¥–åŠ±æ¨¡å‹å­¦ä¹ æä¾›å€Ÿé‰´ï¼Œåç»­å¯æ¢ç´¢åœ¨æ›´å¤šèµ„æºå—é™åœºæ™¯çš„åº”ç”¨ã€‚  
4. åå¥½æ„å»ºè¿‡ç¨‹é‡è§†ï¼šå¼ºè°ƒäººç±»åå¥½â€œæ„å»ºâ€è€Œéâ€œæå–â€ï¼Œæé†’ç ”ç©¶è€…åœ¨è®¾è®¡AIå¯¹é½ç³»ç»Ÿæ—¶å…³æ³¨ç”¨æˆ·åæ€ã€ä»·å€¼å¤–æ˜¾çš„è¿‡ç¨‹ï¼Œæœªæ¥å¯æ·±å…¥æ¢ç´¢å¦‚ä½•ä¼˜åŒ–å¯¹è¯å¼•å¯¼ç­–ç•¥ä»¥æ›´å¥½åŠ©åŠ›ç”¨æˆ·åå¥½æ„å»ºã€‚

## reasongrm--enhancing-generative-reward-models-through-large-reasoning-models
### Abstract
Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonGRMï¼šå€Ÿå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›é©æ–°ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£ã€ç”Ÿæˆä¸å†³ç­–ä¸Šå–å¾—é•¿è¶³è¿›æ­¥ï¼Œä½†è¦è®©æ¨¡å‹è¾“å‡ºè´´åˆäººç±»ä»·å€¼è§‚ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯å…³é”®ã€‚ä¼ ç»Ÿæ ‡é‡å¥–åŠ±æ¨¡å‹ï¼ˆSRMsï¼‰æŠŠå¤æ‚äººç±»åå¥½å‹ç¼©æˆå•ä¸€æ ‡é‡ï¼Œæ˜“ä¿¡æ¯ä¸¢å¤±ã€æ³›åŒ–æ€§å¼±ï¼›æ–°å…´ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰è™½æ›´çµæ´»ï¼Œä½†æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå¸¸å‡ºç°æ¨ç†è·¯å¾„ä¸å®Œæ•´æˆ–è¿‡åº¦æ¨æµ‹ï¼Œå¯¼è‡´ä»»åŠ¡ä¸­â€œå¹»è§‰â€æˆ–å…³é”®ä¿¡æ¯ç¼ºå¤±ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡GRMsçš„æ¨ç†è´¨é‡ä»¥å®ç°å¯é åå¥½å»ºæ¨¡ï¼Œæˆäº†æ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReasonGRMä¸‰é˜¶æ®µæ¡†æ¶  
ReasonGRMåˆ†ä¸‰æ­¥æ‰“é€ æ›´ä¼˜ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼š  
- é˜¶æ®µä¸€ï¼ˆç”Ÿæˆæ¨ç†è·¯å¾„ï¼‰ï¼šç”¨Zero - RLç”Ÿæˆç®€æ´ã€ä»¥ç»“æœä¸ºå¯¼å‘çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘å…³é”®ä¿¡æ¯é—æ¼é£é™©ï¼›  
- é˜¶æ®µäºŒï¼ˆç­›é€‰ä¼˜è´¨è·¯å¾„ï¼‰ï¼šå¼•å…¥å…¨æ–°è¯„ä¼°æŒ‡æ ‡\( R^\star \)ï¼Œä¾æ®ç”Ÿæˆå¯èƒ½æ€§ä¸ºæ¨ç†è·¯å¾„æ‰“åˆ†ï¼Œåå¥½â€œç”¨æœ€å°‘æ¢ç´¢è¾¾æ­£ç¡®ç­”æ¡ˆâ€çš„è·¯å¾„ï¼Œå‰Šå‡è®­ç»ƒä¸­æ˜“å¼•å‘å¹»è§‰çš„æ•°æ®ï¼›  
- é˜¶æ®µä¸‰ï¼ˆå¼ºåŒ–æ¨¡å‹èƒ½åŠ›ï¼‰ï¼šé’ˆå¯¹é«˜éš¾åº¦ç¤ºä¾‹ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ç²¾è°ƒæ¨¡å‹ï¼Œå¢å¼ºå…¶åå¥½åŒºåˆ†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰\( R^\star \)è¯„ä¼°æŒ‡æ ‡è§£å†³æ•°æ®è´¨é‡éš¾é¢˜  
\( R^\star \)ç»“åˆâ€œæœ‰æ•ˆæ€§ï¼ˆValidityï¼Œæ¨ç†å¯¼å‘æ­£ç¡®ç»“æœï¼‰â€ä¸â€œè‡ªä¸€è‡´æ€§ï¼ˆSelf - Consistencyï¼Œæ¨ç†é€»è¾‘è¿è´¯æ— å†—ä½™ï¼‰â€ä¸¤å¤§å…³é”®å±æ€§ï¼Œé€šè¿‡ç”Ÿæˆå¯èƒ½æ€§æ¥è¯„ä¼°æ¨ç†è·¯å¾„ï¼Œèƒ½ä»å™ªå£°å€™é€‰é›†ä¸­è‡ªåŠ¨é€‰ä¼˜è´¨æ¨ç†è·¯å¾„ï¼Œç ´è§£å¤æ‚ä»»åŠ¡å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æ•°æ®è´¨é‡ç“¶é¢ˆã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨RM - Benchã€RewardBenchã€RMBä¸‰å¤§å…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonGRMè¡¨ç°äº®çœ¼ï¼šå¹³å‡è¶…è¶Šæ­¤å‰æœ€ä¼˜GRMs 1.8%ï¼Œåœ¨éƒ¨åˆ†åœºæ™¯ä¸‹æ¯”GPT - 4oç­‰é—­æºæ¨¡å‹é¢†å…ˆè¾¾5.6%ï¼Œè¿˜æ¯”ä¸»æµSRMså¹³å‡é«˜4.5%ã€‚å®éªŒä¸ä»…éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œæ¶ˆèå®éªŒä¹Ÿå‰–æäº†æ¨ç†è´¨é‡ã€\( R^\star \)è¿‡æ»¤æ•ˆæœã€å„è®­ç»ƒé˜¶æ®µå¯¹æœ€ç»ˆå¥–åŠ±æ¨¡å‹çš„å½±å“ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é‡è§†æ¨ç†è´¨é‡åœ¨å¥–åŠ±æ¨¡å‹ä¸­çš„ä»·å€¼ï¼šæ­ç¤ºäº†é«˜è´¨é‡æ¨ç†è·¯å¾„ï¼ˆå…¼é¡¾æœ‰æ•ˆæ€§ä¸è‡ªä¸€è‡´æ€§ï¼‰å¯¹åå¥½å»ºæ¨¡çš„å…³é”®ä½œç”¨ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹è®¾è®¡æŒ‡æ˜â€œæ¨ç†æ„ŸçŸ¥â€æ–¹å‘ï¼›  
2. åˆ›æ–°è¯„ä¼°ä¸è¿‡æ»¤æœºåˆ¶ï¼š\( R^\star \)å±•ç¤ºäº†å¦‚ä½•ç”¨ç”Ÿæˆå¯èƒ½æ€§é‡åŒ–æ¨ç†è´¨é‡ï¼Œä¸ºå¤„ç†å™ªå£°æ•°æ®ã€æ„å»ºä¼˜è´¨è®­ç»ƒé›†æä¾›äº†å¯å¤ç”¨æ€è·¯ï¼›  
3. å¤šé˜¶æ®µè®­ç»ƒPipelineï¼šä»ç”Ÿæˆåˆ°ç­›é€‰å†åˆ°å¼ºåŒ–å­¦ä¹ çš„æµç¨‹ï¼Œä¸ºé€šç”¨LLMå‘ä¸“ç²¾å¥–åŠ±æ¨¡å‹è½¬åŒ–æä¾›äº†å·¥ç¨‹åŒ–å‚è€ƒèŒƒå¼ï¼›  
4. å…¨é¢å®éªŒéªŒè¯ï¼šè·¨å¤šä¸ªæƒå¨åŸºå‡†çš„æµ‹è¯•+æ¶ˆèå®éªŒï¼Œæ˜¯å­¦æœ¯ç ”ç©¶ä¸­éªŒè¯æ–¹æ³•æ™®é€‚æ€§ä¸æ¨¡å—ä»·å€¼çš„å…¸èŒƒï¼Œå€¼å¾—å€Ÿé‰´ä»¥å¢å¼ºç ”ç©¶è¯´æœåŠ›ã€‚  
```

## autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning
### Abstract
Rule-based rewards offer a promising strategy for improving reinforcement
learning from human feedback (RLHF), but current approaches often rely on
manual rule engineering. We present AutoRule, a fully automated method for
extracting rules from preference feedback and formulating them into rule-based
rewards. AutoRule extraction operates in three stages: it leverages a reasoning
model to interpret user preferences, identifies candidate rules from the
reasoning chain of these interpretations, and synthesizes them into a unified
rule set. Leveraging the finalized rule set, we employ language-model verifiers
to compute the fraction of rules satisfied by each output, using this metric as
an auxiliary reward alongside the learned reward model during policy
optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\%
relative improvement in length-controlled win rate on AlpacaEval2.0, and a
6.1\% relative gain in second-turn performance on a held-out MT-Bench subset,
compared to a GRPO baseline trained with the same learned reward model but
without the rule-based auxiliary reward. Our analysis confirms that the
extracted rules exhibit good agreement with dataset preference. We find that
AutoRule demonstrates reduced reward hacking compared to a learned reward model
when run over two episodes. Finally, our case study suggests that the extracted
rules capture unique qualities valued in different datasets. The extracted
rules are provided in the appendix, and the code is open-sourced at
https://github.com/cxcscmu/AutoRule.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AutoRuleï¼šä»æ¨ç†æ€ç»´é“¾ä¸­æå–è§„åˆ™å¥–åŠ±ï¼Œé©æ–°åå¥½å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»ä»·å€¼è§‚å’Œéµå¾ªæŒ‡ä»¤çš„å…³é”®æŠ€æœ¯ï¼Œä½†ç°æœ‰åŸºäºå­¦ä¹ çš„å¥–åŠ±æ¨¡å‹å­˜åœ¨â€œå¥–åŠ±é»‘å®¢æ”»å‡»â€é—®é¢˜ï¼ˆæ¨¡å‹ä¸ºè¿½æ±‚é«˜å¥–åŠ±è€ŒæŠ•æœºå–å·§ï¼ŒæœªçœŸæ­£æå‡å“åº”è´¨é‡ï¼‰ã€‚åŒæ—¶ï¼Œè§„åˆ™é©±åŠ¨çš„å¥–åŠ±è™½èƒ½æœ‰æ•ˆç¼“è§£è¯¥é—®é¢˜ï¼Œä½†ä¼ ç»Ÿè§„åˆ™ä¾èµ–äººå·¥è®¾è®¡æˆ–å¤§è§„æ¨¡ä¼—åŒ…æ ‡æ³¨ï¼Œæˆæœ¬é«˜ä¸”éš¾æ‰©å±•ã€‚å› æ­¤ï¼Œå¦‚ä½•è‡ªåŠ¨ä»åå¥½æ•°æ®ä¸­æå–è§„åˆ™æ¥æ„å»ºå¥–åŠ±æœºåˆ¶ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAutoRuleè‡ªåŠ¨è§„åˆ™æå–æ¡†æ¶  
AutoRuleé€šè¿‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»åå¥½æ•°æ®ä¸­è‡ªåŠ¨æå–å¯¹é½è§„åˆ™ã€‚æµç¨‹åˆ†ä¸‰æ­¥ï¼šé¦–å…ˆç”¨å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMä¸ºåå¥½è¾“å‡ºç”Ÿæˆé€æ­¥æ¨ç†ä¾æ®ï¼›æ¥ç€ä»æ¨ç†è¿‡ç¨‹ä¸­æå–å€™é€‰è§„åˆ™ï¼›æœ€åèšåˆè®­ç»ƒé›†å€™é€‰è§„åˆ™å¹¶åˆæˆç»Ÿä¸€è§„åˆ™é›†ï¼Œå€ŸåŠ©æ¨ç†é“¾çš„é€»è¾‘ç»“æ„æ•æ‰æ›´ç²¾å‡†çš„åå¥½å‡†åˆ™ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§„åˆ™å¥–åŠ±ä¸ç­–ç•¥ä¼˜åŒ–ç»“åˆ  
åˆ©ç”¨æœ€ç»ˆè§„åˆ™é›†ï¼Œè®©è¯­è¨€æ¨¡å‹å……å½“â€œéªŒè¯å™¨â€ï¼Œè®¡ç®—æ¯ä¸ªè¾“å‡ºæ»¡è¶³è§„åˆ™çš„æ¯”ä¾‹ï¼Œå°†è¯¥æŒ‡æ ‡ä½œä¸ºè¾…åŠ©å¥–åŠ±ï¼Œä¸ä¼ ç»Ÿå­¦ä¹ åˆ°çš„å¥–åŠ±æ¨¡å‹åœ¨ç­–ç•¥ä¼˜åŒ–é˜¶æ®µé…åˆä½¿ç”¨ï¼Œå¼•å¯¼æ¨¡å‹è®­ç»ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒéªŒè¯ä¸­ï¼Œç”¨Llama 3 8B Instructä½œéªŒè¯å™¨æ—¶ï¼Œè§„åˆ™åˆ†æ•°ï¼ˆå•ä¸ªæˆ–ç´¯è®¡ï¼‰åœ¨UltraFeedbackå’ŒMT - Bench Human Judgmentæ•°æ®é›†ä¸Šä¸åå¥½é«˜åº¦ä¸€è‡´ï¼›åŸºäºUltraFeedbackæ•°æ®ç”¨GRPOç»“åˆAutoRuleè®­ç»ƒLlama - 3 - 8Bæ¨¡å‹ï¼Œå¯¹æ¯”ä»…ç”¨å­¦ä¹ å¥–åŠ±æ¨¡å‹çš„GRPOåŸºçº¿ï¼ŒAlpacaEval2.0é•¿åº¦æ§åˆ¶èƒœç‡ç›¸å¯¹æå‡28.6%ï¼ŒMT - Benchä¿ç•™å­é›†çš„ç¬¬äºŒè½®è¡¨ç°ç›¸å¯¹æå‡6.1%ï¼›å¥–åŠ±é»‘å®¢å®éªŒè¡¨æ˜AutoRuleèƒ½å‡è½»å¥–åŠ±æ¨¡å‹è¿‡åº¦ä¼˜åŒ–ï¼›æ¶ˆèå®éªŒè¯æ˜ä»æ¨ç†é“¾æå–è§„åˆ™æ¯”ä»…ä»ä¾æ®æå–æ›´æœ‰æ•ˆï¼›å®šæ€§åˆ†ææ˜¾ç¤ºä¸åŒæ•°æ®é›†æå–çš„è§„åˆ™å„æœ‰ä¾§é‡ï¼ˆå¦‚UltraFeedbackä¾§é‡å¯¹è¯è´¨é‡ï¼ŒMT - Benchä¾§é‡æŒ‡ä»¤éµå¾ªä¸å¤æ‚ä»»åŠ¡é²æ£’æ€§ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªåŠ¨è§„åˆ™æå–æ€è·¯ï¼šæ‘†è„±äººå·¥è§„åˆ™ä¾èµ–ï¼Œå€ŸåŠ©LLMæ¨ç†èƒ½åŠ›ä»æ•°æ®ä¸­è‡ªåŠ¨æŒ–æ˜è§„åˆ™ï¼Œä¸ºé¢†åŸŸé€‚é…è§„åˆ™æ„å»ºæä¾›é«˜æ•ˆæ–¹æ¡ˆã€‚  
2. å¤šå¥–åŠ±èåˆä¼˜åŒ–ï¼šå°†è§„åˆ™å¥–åŠ±ä¸å­¦ä¹ å¥–åŠ±ç»“åˆç”¨äºç­–ç•¥ä¼˜åŒ–ï¼Œä¸ºç¼“è§£å¥–åŠ±é»‘å®¢ã€æå‡æ¨¡å‹çœŸå®æ€§èƒ½æä¾›æ–°èŒƒå¼ã€‚  
3. å¯è§£é‡Šæ€§ä¸é€‚é…æ€§ï¼šæå–çš„è§„åˆ™å¯è§£é‡Šï¼Œä¸”èƒ½æ•æ‰ä¸åŒæ•°æ®é›†çš„ç‹¬ç‰¹ä»·å€¼ï¼Œä¸ºç†è§£æ¨¡å‹åå¥½å¯¹é½é€»è¾‘å’Œå®šåˆ¶åŒ–ä¼˜åŒ–æä¾›å‚è€ƒã€‚

## reward-models-in-deep-reinforcement-learning--a-survey
### Abstract
In reinforcement learning (RL), agents continually interact with the
environment and use the feedback to refine their behavior. To guide policy
optimization, reward models are introduced as proxies of the desired
objectives, such that when the agent maximizes the accumulated reward, it also
fulfills the task designer's intentions. Recently, significant attention from
both academic and industrial researchers has focused on developing reward
models that not only align closely with the true objectives but also facilitate
policy optimization. In this survey, we provide a comprehensive review of
reward modeling techniques within the deep RL literature. We begin by outlining
the background and preliminaries in reward modeling. Next, we present an
overview of recent reward modeling approaches, categorizing them based on the
source, the mechanism, and the learning paradigm. Building on this
understanding, we discuss various applications of these reward modeling
techniques and review methods for evaluating reward models. Finally, we
conclude by highlighting promising research directions in reward modeling.
Altogether, this survey includes both established and emerging methods, filling
the vacancy of a systematic review of reward models in current literature.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±æ¨¡å‹å…¨æ™¯å¼ç»¼è¿°ï¼šä»åŸºç¡€åˆ°å‰æ²¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œå¥–åŠ±æ˜¯å¼•å¯¼æ™ºèƒ½ä½“ä¼˜åŒ–è¡Œä¸ºçš„æ ¸å¿ƒï¼Œä½†ç°å®åœºæ™¯é‡Œå¥–åŠ±å¸¸ç¼ºå¤±æˆ–éš¾å®šä¹‰ã€‚ç°æœ‰ç»¼è¿°å¤šèšç„¦é€†å¼ºåŒ–å­¦ä¹ ã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ç­‰å­é¢†åŸŸï¼Œç¼ºä¹å¯¹å¥–åŠ±æ¨¡å‹æœ¬èº«çš„ç³»ç»Ÿæ€§æ¢³ç†ã€‚åŒæ—¶ï¼Œä»¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸ºä»£è¡¨çš„åŸºç¡€æ¨¡å‹åœ¨å¥–åŠ±å»ºæ¨¡ä¸­å…´èµ·å´æœªè¢«å……åˆ†ç»¼è¿°ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨å¡«è¡¥ç©ºç™½ï¼Œå…¨é¢å›é¡¾æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å»ºæ¨¡æŠ€æœ¯ï¼Œæ¶µç›–åŸºç¡€ã€æ–¹æ³•ã€åº”ç”¨ä¸æœªæ¥æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç³»ç»Ÿæ€§åˆ†ç±»æ¡†æ¶æ„å»º
ä»**æ¥æº**ã€**æœºåˆ¶**ã€**å­¦ä¹ èŒƒå¼**ä¸‰ç»´åº¦æ„å»ºå¥–åŠ±æ¨¡å‹åˆ†ç±»æ¡†æ¶ã€‚æ¥æºä¸ŠåŒºåˆ†äººç±»æä¾›å‹ï¼ˆå«æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹ã€äººåœ¨ç¯å¥–åŠ±å­¦ä¹ ï¼Œåè€…åˆç»†åˆ†ä¸ºä»æ¼”ç¤ºã€ç›®æ ‡ã€åå¥½ä¸­å­¦ä¹ ï¼‰ä¸AIç”Ÿæˆå‹ï¼ˆä¾æ‰˜LLMã€VLMç­‰åŸºç¡€æ¨¡å‹ï¼‰ï¼›æœºåˆ¶ä¸Šåˆ†ä¸ºå†…åœ¨å¥–åŠ±ï¼ˆé©±åŠ¨æ™ºèƒ½ä½“è‡ªä¸»æ¢ç´¢ï¼‰ä¸å¤–åœ¨å¥–åŠ±ï¼ˆç¯å¢ƒç›´æ¥åé¦ˆï¼‰ï¼›å­¦ä¹ èŒƒå¼å›´ç»•ä¸åŒåé¦ˆç±»å‹å±•å¼€ï¼Œæ¸…æ™°æ¢³ç†å¥–åŠ±æ¨¡å‹çš„å¤šæ ·å½¢æ€ä¸æ„å»ºé€»è¾‘ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢è¦†ç›–æ–°å…´æ–¹å‘
èšç„¦åŸºç¡€æ¨¡å‹èµ‹èƒ½çš„å¥–åŠ±å»ºæ¨¡ï¼Œå¦‚LLMè§£è¯»äººç±»æ„å›¾è‡ªä¸»å®šä¹‰å¥–åŠ±ã€VLMå¤„ç†å¤šæ¨¡æ€åœºæ™¯ä¸‹å¥–åŠ±ç”Ÿæˆç­‰ï¼Œå¼¥è¡¥è¿‡å¾€ç»¼è¿°å¯¹è¿™ç±»æ–°å…´æ–¹æ³•å…³æ³¨ä¸è¶³çš„é—®é¢˜ï¼Œä¸ºé¢†åŸŸæ³¨å…¥æ–°è§†è§’ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå®Œæ•´ç”Ÿæ€æ¢³ç†
ä¸ä»…å‰–æå¥–åŠ±æ¨¡å‹æŠ€æœ¯æœ¬èº«ï¼Œè¿˜å»¶ä¼¸è‡³åº”ç”¨åœºæ™¯ï¼ˆå¦‚å¤æ‚æ¸¸æˆã€å¤§æ¨¡å‹å¯¹é½äººç±»æ„å›¾ã€å¤§è§„æ¨¡æ™ºèƒ½ä½“è®­ç»ƒï¼‰ä¸è¯„ä¼°æ–¹æ³•ï¼Œæ­å»ºä»æŠ€æœ¯åˆ°è½åœ°å†åˆ°æ•ˆæœéªŒè¯çš„å®Œæ•´çŸ¥è¯†é“¾æ¡ï¼ŒåŠ©åŠ›è¯»è€…ç†è§£å¥–åŠ±æ¨¡å‹åœ¨RLç”Ÿæ€ä¸­è§’è‰²ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªä¾§é‡ä¼ ç»Ÿå®éªŒæŒ‡æ ‡å¯¹æ¯”ï¼ˆå› å±ç»¼è¿°ï¼‰ï¼Œä½†é€šè¿‡å¯¹å¤§é‡å‰æ²¿è®ºæ–‡ï¼ˆå¦‚åŸºäºLLMçš„å¥–åŠ±è®¾è®¡ã€äººåœ¨ç¯å¥–åŠ±å­¦ä¹ æ¡ˆä¾‹ï¼‰çš„å½’çº³ï¼Œå±•ç°å„åˆ†ç±»ä¸‹æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ï¼ˆå¦‚AlphaGoçš„å†³ç­–å¥–åŠ±ã€InstructGPTçš„å¯¹é½å¥–åŠ±ï¼‰çš„æœ‰æ•ˆæ€§ï¼šæ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹åœ¨ Gym - MuJoCo  walker ä»»åŠ¡ä¸­é€šè¿‡ç»„åˆç”Ÿå­˜ã€ç§»åŠ¨ç­‰å¥–åŠ±å¼•å¯¼æ™ºèƒ½ä½“ï¼›äººåœ¨ç¯å­¦ä¹ ä»äººç±»æ¼”ç¤º/åå¥½ä¸­å­¦ä¹ çš„æ–¹å¼é™ä½äººå·¥è®¾è®¡éš¾åº¦ä¸”æå‡å¯¹é½åº¦ï¼›åŸºç¡€æ¨¡å‹ç”Ÿæˆå¥–åŠ±åœ¨å¤§æ¨¡å‹æ™ºèƒ½ä½“è®­ç»ƒï¼ˆå¦‚OpenAI - o1ï¼‰ä¸­å±•ç°å¼ºå¤§æ¨ç†å¼•å¯¼èƒ½åŠ›ï¼ŒéªŒè¯ä¸åŒå¥–åŠ±å»ºæ¨¡è·¯å¾„åœ¨å„è‡ªé€‚ç”¨åœºæ™¯çš„ä»·å€¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **åˆ†ç±»æ€ç»´**ï¼šä¸‰ç»´åº¦åˆ†ç±»æ¡†æ¶ä¸ºé¢†åŸŸå†…ç ”ç©¶æä¾›æ¸…æ™°â€œåæ ‡â€ï¼Œåç»­ç ”ç©¶å¯å¿«é€Ÿå®šä½è‡ªèº«æ–¹æ³•æ‰€å±ç»´åº¦ï¼Œä¹Ÿä¾¿äºæ¢³ç†é¢†åŸŸå‘å±•è„‰ç»œï¼Œç±»ä¼¼æ€è·¯å¯è¿ç§»è‡³å…¶ä»–AIå­é¢†åŸŸæŠ€æœ¯ç»¼è¿°ä¸åˆ†ç±»ã€‚
2. **æ–°å…´æŠ€æœ¯èåˆè§†è§’**ï¼šå…³æ³¨åŸºç¡€æ¨¡å‹ä¸RLå¥–åŠ±å»ºæ¨¡çš„ç»“åˆï¼Œå¯ç¤ºç ”ç©¶è€…é‡è§†è·¨æ¨¡æ€ã€å¤§æ¨¡å‹ç­‰å‰æ²¿æŠ€æœ¯å¯¹ä¼ ç»ŸRLæ¨¡å—çš„é©æ–°ï¼Œæ¨åŠ¨å­¦ç§‘äº¤å‰ã€‚
3. **ç”Ÿæ€åŒ–æ¢³ç†**ï¼šä»æŠ€æœ¯åˆ°åº”ç”¨å†åˆ°è¯„ä¼°çš„å…¨æµç¨‹è¦†ç›–ï¼Œæé†’ä»ä¸šè€…åšç ”ç©¶æˆ–è½åœ°æ—¶éœ€å…¼é¡¾ä¸Šä¸‹æ¸¸ç¯èŠ‚ï¼Œå¦‚è®¾è®¡å¥–åŠ±æ¨¡å‹è¦è€ƒè™‘åº”ç”¨åœºæ™¯ç‰¹æ€§ã€è¯„ä¼°æ‰‹æ®µåˆç†æ€§ï¼Œä¸ºé¡¹ç›®å…¨å‘¨æœŸè§„åˆ’æä¾›å‚è€ƒã€‚ 
4. **å¡«è¡¥é¢†åŸŸç©ºç™½ä»·å€¼**ï¼šä½œä¸ºé¦–ç¯‡ç³»ç»Ÿèšç„¦æ·±åº¦RLå¥–åŠ±æ¨¡å‹çš„ç»¼è¿°ï¼Œä¸ºå­¦ç•Œæä¾›äº†è¯¥æ–¹å‘çŸ¥è¯†åº•åº§ï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤å±•å¼€æ›´ç»†åˆ†ã€æ·±å…¥çš„æ¢ç´¢ï¼Œå¦‚ç‰¹å®šæ¥æºå¥–åŠ±æ¨¡å‹çš„ä¼˜åŒ–ã€è·¨æœºåˆ¶å¥–åŠ±èåˆç­‰ã€‚

## adaptive-accompaniment-with-realchords
### Abstract
Jamming requires coordination, anticipation, and collaborative creativity
between musicians. Current generative models of music produce expressive output
but are not able to generate in an \emph{online} manner, meaning simultaneously
with other musicians (human or otherwise). We propose ReaLchords, an online
generative model for improvising chord accompaniment to user melody. We start
with an online model pretrained by maximum likelihood, and use reinforcement
learning to finetune the model for online use. The finetuning objective
leverages both a novel reward model that provides feedback on both harmonic and
temporal coherency between melody and chord, and a divergence term that
implements a novel type of distillation from a teacher model that can see the
future melody. Through quantitative experiments and listening tests, we
demonstrate that the resulting model adapts well to unfamiliar input and
produce fitting accompaniment. ReaLchords opens the door to live jamming, as
well as simultaneous co-creation in other modalities.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReaLchordsï¼šå¼€å¯å®æ—¶éŸ³ä¹å³å…´ä¼´å¥æ–°ç¯‡ç« 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éŸ³ä¹å³å…´æ¼”å¥ï¼ˆJammingï¼‰éœ€è¦ä¹æ‰‹é—´çš„åè°ƒã€é¢„åˆ¤ä¸åä½œåˆ›é€ åŠ›ï¼Œä½†å½“å‰éŸ³ä¹ç”Ÿæˆæ¨¡å‹è™½èƒ½äº§å‡ºå¯Œæœ‰è¡¨ç°åŠ›çš„å†…å®¹ï¼Œå´æ— æ³•â€œåœ¨çº¿â€ï¼ˆonlineï¼‰ç”Ÿæˆï¼Œå³æ— æ³•ä¸å…¶ä»–ä¹æ‰‹ï¼ˆäººç±»æˆ–å…¶ä»–ï¼‰åŒæ­¥åˆ›ä½œã€‚åœ¨éŸ³ä¹ä¼´å¥åœºæ™¯ä¸­ï¼Œç°æœ‰æ¨¡å‹å¤šä¸ºç¦»çº¿ç”Ÿæˆï¼ˆå¦‚å…ˆæ‹¿åˆ°å®Œæ•´æ—‹å¾‹å†ç”Ÿæˆä¼´å¥ï¼‰ï¼Œä¸é€‚åº”å®æ—¶äº’åŠ¨ã€åŠ¨æ€è°ƒæ•´çš„éœ€æ±‚ï¼›ä¸”åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰é¢„è®­ç»ƒçš„æ¨¡å‹å­˜åœ¨â€œæš´éœ²åå·®â€ï¼Œåœ¨é¢å¯¹å®æ—¶åœºæ™¯ä¸­å¸¸è§çš„é”™è¯¯ã€é£æ ¼å˜åŒ–ç­‰æƒ…å†µæ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥é€‚é…é™Œç”Ÿè¾“å…¥å¹¶ç”Ÿæˆå¥‘åˆçš„ä¼´å¥ã€‚å› æ­¤ï¼Œæ‰“é€ ä¸€æ¬¾èƒ½åœ¨çº¿é€‚é…ç”¨æˆ·æ—‹å¾‹ã€å³å…´ç”Ÿæˆå’Œå¼¦ä¼´å¥çš„æ¨¡å‹æˆä¸ºå…³é”®éœ€æ±‚ï¼ŒReaLchords åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º ReaLchords åœ¨çº¿ä¼´å¥ç”Ÿæˆæ¨¡å‹  
ReaLchords èšç„¦äºä¸ºç”¨æˆ·æ—‹å¾‹å®æ—¶å³å…´ç”Ÿæˆå’Œå¼¦ä¼´å¥ï¼Œå®ƒå…ˆåŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡å®Œæˆé¢„è®­ç»ƒï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹æ¨¡å‹è¿›è¡Œåœ¨çº¿åœºæ™¯çš„å¾®è°ƒã€‚è¿™ç§æ–¹å¼è®©æ¨¡å‹èƒ½åœ¨ä¸çŸ¥é“åç»­æ—‹å¾‹çš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€å“åº”æ­£åœ¨ç”Ÿæˆçš„éŸ³ä¹å™äº‹ï¼Œæ»¡è¶³å®æ—¶äº’åŠ¨ä¸­â€œæ¡ä»¶ç‹¬ç«‹å‡è®¾â€çš„çº¦æŸï¼Œä¸ºç°åœºå³å…´æ¼”å¥ç­‰åœºæ™¯æä¾›æŠ€æœ¯æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆå¼ºåŒ–å­¦ä¹ ä¸è‡ªç›‘ç£å¥–åŠ±æ¨¡å‹ä¼˜åŒ–  
å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥æ–°é¢–çš„è‡ªç›‘ç£å¥–åŠ±æ¨¡å‹ï¼Œä»å’Œå£°ä¸æ—¶é—´è¿è´¯æ€§ç­‰å¤šä¸ªè§’åº¦ï¼Œä¸ºæ—‹å¾‹å’Œå’Œå¼¦ä¹‹é—´çš„éŸ³ä¹åè°ƒæ€§æä¾›åé¦ˆã€‚è¯¥å¥–åŠ±æ¨¡å‹æ— éœ€äººç±»æ ‡æ³¨ï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒå´èƒ½åœ¨äººç±»å¬è§‰æµ‹è¯•ä¸­å±•ç°å‡ºä¸äººç±»åå¥½çš„é«˜åº¦å¯¹é½ï¼Œæœ‰æ•ˆå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å…·éŸ³ä¹æ€§çš„ä¼´å¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºâ€œæœªæ¥å¯è§â€æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è’¸é¦  
è®¾è®¡äº†ä¸€ç§ä»â€œèƒ½çœ‹åˆ°æœªæ¥æ—‹å¾‹â€çš„æ•™å¸ˆæ¨¡å‹å‘â€œçœ‹ä¸åˆ°æœªæ¥â€çš„å­¦ç”Ÿæ¨¡å‹ï¼ˆå³åœ¨çº¿ç”Ÿæˆæ¨¡å‹ï¼‰è’¸é¦çŸ¥è¯†çš„æ–°é¢–æ–¹å¼ã€‚æ•™å¸ˆæ¨¡å‹å¯åˆ©ç”¨å®Œæ•´æ—‹å¾‹ä¿¡æ¯ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ™æ¨¡æ‹Ÿå®æ—¶åœºæ™¯åªèƒ½ä¾èµ–å†å²ä¿¡æ¯ï¼Œé€šè¿‡è¿™ç§è’¸é¦è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¼šâ€œé¢„åˆ¤â€ï¼Œå¤§å¹…æå‡ç”Ÿæˆè´¨é‡ï¼Œäººç±»è¯„ä¼°ç»“æœä¹ŸéªŒè¯äº†è’¸é¦å¯¹æ¨¡å‹æ•ˆæœçš„å¢ç›Šã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- æŠ—æ‰°åŠ¨ä¸é€‚é…æ€§ï¼šå¯¹æ¯”ä»…ç”¨ MLE è®­ç»ƒçš„åœ¨çº¿æ¨¡å‹ï¼ŒReaLchords åœ¨é¢å¯¹æµ‹è¯•é›†ä¸­æ—‹å¾‹ä¸­é€”çš„çªç„¶è½¬è°ƒç­‰æ‰°åŠ¨æ—¶ï¼Œèƒ½æ›´å¿«é€‚åº”ã€‚å¦‚ç¤ºä¾‹ä¸­ MLE æ¨¡å‹é¢„æµ‹å‡ºä¸åˆé€‚å’Œå¼¦åæ— æ³•è°ƒæ•´ï¼Œè€Œ ReaLchords è™½åˆæœŸä¹Ÿä¼šé¢„æµ‹ä¸ä½³ä½†èƒ½å¿«é€Ÿä¿®æ­£ï¼Œä¸”å®¢è§‚å’Œå£°è´¨é‡æŒ‡æ ‡ä¹Ÿä½è¯äº†è¿™ä¸€ä¼˜åŠ¿ã€‚  
- äººç±»è¯„ä¼°ä¸åå¥½å¯¹é½ï¼šå¥–åŠ±æ¨¡å‹åœ¨æ— äººç±»åé¦ˆè®­ç»ƒä¸‹ï¼Œç»äººç±»å¬è§‰æµ‹è¯•è¯æ˜ä¸äººç±»åå¥½é«˜åº¦å¥‘åˆï¼›åŒæ—¶ç»“åˆè’¸é¦ä¸ RL å¾®è°ƒåçš„æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆä¼´å¥çš„èŠ‚å¥ã€å’Œå£°è´¨é‡ç­‰ç»´åº¦è¡¨ç°æ›´ä¼˜ï¼Œå„ç»„ä»¶å¯¹æœ€ç»ˆæ•ˆæœå‡æœ‰æå‡è´¡çŒ®ã€‚  
- å®šé‡æŒ‡æ ‡éªŒè¯ï¼šé€šè¿‡é¢†åŸŸç‰¹å®šæŒ‡æ ‡åˆ†æï¼ˆå¦‚å’Œå£°ã€èŠ‚å¥ç›¸å…³åº¦é‡ï¼‰ï¼Œè¯å® RL å¾®è°ƒä¸­å„ç»„ä»¶ï¼ˆå¥–åŠ±æ¨¡å‹ã€è’¸é¦ç­‰ï¼‰å¯¹ç”Ÿæˆä¼´å¥åœ¨èŠ‚å¥å’Œå’Œå£°è´¨é‡ä¸Šçš„æ”¹è¿›ä½œç”¨ï¼Œæ—  RL å¾®è°ƒçš„æ¨¡å‹åœ¨åº”å¯¹é”™è¯¯å’Œæ‰°åŠ¨æ—¶è¡¨ç°æ‹‰èƒ¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- å®æ—¶äº’åŠ¨åœºæ™¯å»ºæ¨¡ï¼šä¸ºéŸ³ä¹ä»¥å¤–çš„å®æ—¶ååŒåˆ›ä½œï¼ˆå¦‚è§†è§‰è‰ºæœ¯åŒæ­¥å…±åˆ›ã€äº’åŠ¨å™äº‹ç”Ÿæˆç­‰ï¼‰æä¾›äº†â€œåœ¨çº¿ç”Ÿæˆ + å¼ºåŒ–å­¦ä¹ é€‚é…â€çš„æ€è·¯å‚è€ƒï¼Œå±•ç¤ºäº†å¦‚ä½•è®©ç”Ÿæˆæ¨¡å‹é€‚åº”åŠ¨æ€ã€ä¸å¯è§åç»­è¾“å…¥çš„åœºæ™¯ã€‚  
- è‡ªç›‘ç£å¥–åŠ±æœºåˆ¶ï¼šè¯æ˜æ— éœ€äººç±»æ ‡æ³¨çš„è‡ªç›‘ç£æ–¹å¼ä¹Ÿèƒ½è®­ç»ƒå‡ºä¸äººç±»åå¥½å¯¹é½çš„å¥–åŠ±æ¨¡å‹ï¼Œåœ¨å…¶ä»–åˆ›æ„é¢†åŸŸï¼ˆå¦‚ç»˜ç”»é£æ ¼ä¸€è‡´æ€§è¯„ä¼°ã€æ–‡å­¦åˆ›ä½œè¿è´¯æ€§æ‰“åˆ†ç­‰ï¼‰å¯å€Ÿé‰´è¯¥æ€è·¯æ„å»ºè‡ªç›‘ç£åé¦ˆä¿¡å·ã€‚  
- çŸ¥è¯†è’¸é¦æ–°èŒƒå¼ï¼šâ€œä»èƒ½çœ‹æœªæ¥çš„æ•™å¸ˆåˆ°åªèƒ½çœ‹å†å²çš„å­¦ç”Ÿâ€çš„è’¸é¦æ¨¡å¼ï¼Œä¸ºéœ€è¦â€œé¢„åˆ¤â€èƒ½åŠ›çš„ä»»åŠ¡ï¼ˆå¦‚å®æ—¶å¯¹è¯å›å¤ç”Ÿæˆã€å®æ—¶äº¤é€šè°ƒåº¦ç­–ç•¥å­¦ä¹ ç­‰ï¼‰æä¾›äº†è’¸é¦æ–¹å‘ä¸ç›®æ ‡è®¾å®šçš„åˆ›æ–°èŒƒå¼ï¼Œæ‹“å±•äº†çŸ¥è¯†è’¸é¦çš„åº”ç”¨è¾¹ç•Œã€‚

## senior--efficient-query-selection-and-preference-guided-exploration-in-preference-based-reinforcement-learning
### Abstract
Preference-based Reinforcement Learning (PbRL) methods provide a solution to
avoid reward engineering by learning reward models based on human preferences.
However, poor feedback- and sample- efficiency still remain the problems that
hinder the application of PbRL. In this paper, we present a novel efficient
query selection and preference-guided exploration method, called SENIOR, which
could select the meaningful and easy-to-comparison behavior segment pairs to
improve human feedback-efficiency and accelerate policy learning with the
designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We
designed a Motion-Distinction-based Selection scheme (MDS). It selects segment
pairs with apparent motion and different directions through kernel density
estimation of states, which is more task-related and easy for human preference
labeling; (2) We proposed a novel preference-guided exploration method (PGE).
It encourages the exploration towards the states with high preference and low
visits and continuously guides the agent achieving the valuable samples. The
synergy between the two mechanisms could significantly accelerate the progress
of reward and policy learning. Our experiments show that SENIOR outperforms
other five existing methods in both human feedback-efficiency and policy
convergence speed on six complex robot manipulation tasks from simulation and
four real-worlds.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SENIORï¼šåŸºäºåå¥½å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆæŸ¥è¯¢é€‰æ‹©ä¸åå¥½å¼•å¯¼æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼—å¤šå†³ç­–ä»»åŠ¡ä¸­å–å¾—æˆåŠŸï¼Œä½†å¥–åŠ±å‡½æ•°è®¾è®¡éšä»»åŠ¡å¤æ‚åº¦æå‡å˜å¾—å›°éš¾ã€‚åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡å­¦ä¹ äººç±»åå¥½æ¥é¿å…å¥–åŠ±å·¥ç¨‹ï¼Œç„¶è€Œåé¦ˆæ•ˆç‡ä¸æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜é˜»ç¢äº†å…¶åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©æ˜“æ ‡æ³¨çš„æœ‰æ„ä¹‰ç‰‡æ®µåŠè®©æ¢ç´¢ä¸åå¥½å…³è”æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› æ­¤æœ¬æ–‡æå‡ºSENIORæ–¹æ³•æ¥æå‡PbRLçš„åé¦ˆä¸æ¢ç´¢æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºè¿åŠ¨åŒºåˆ†çš„é€‰æ‹©æ–¹æ¡ˆï¼ˆMDSï¼‰
é€šè¿‡å¯¹çŠ¶æ€è¿›è¡Œæ ¸å¯†åº¦ä¼°è®¡ï¼Œé€‰æ‹©å…·æœ‰æ˜æ˜¾è¿åŠ¨ä¸”æ–¹å‘ä¸åŒçš„è¡Œä¸ºç‰‡æ®µå¯¹ã€‚è¿™ç±»ç‰‡æ®µæ›´ä¸ä»»åŠ¡ç›¸å…³ï¼Œä¹Ÿä¾¿äºäººç±»è¿›è¡Œåå¥½æ ‡æ³¨ï¼Œèƒ½ä¸ºå¥–åŠ±å­¦ä¹ è·å–é«˜è´¨é‡æ ‡ç­¾ï¼Œè§£å†³äº†ä»¥å¾€æŸ¥è¯¢é€‰æ‹©ä¸­æ— æ„ä¹‰æ•°æ®å¯¹å¤šã€æ ‡æ³¨éš¾çš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåå¥½å¼•å¯¼çš„æ¢ç´¢æ–¹æ³•ï¼ˆPGEï¼‰
ä»¥å†…åœ¨å¥–åŠ±çš„å½¢å¼ï¼Œé¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢é«˜åå¥½ä¸”ä½è®¿é—®çš„çŠ¶æ€ï¼ŒæŒç»­å¼•å¯¼æ™ºèƒ½ä½“è·å–æœ‰ä»·å€¼æ ·æœ¬ã€‚è®©æ¢ç´¢å’Œäººç±»åå¥½å…³è”èµ·æ¥ï¼Œå¼¥è¡¥äº†ä»¥å¾€RLæ¢ç´¢ä¸­è¾ƒå°‘å…³æ³¨åå¥½ç›¸å…³æ€§çš„ç¼ºé™·ï¼ŒåŠ é€Ÿç­–ç•¥å­¦ä¹ ã€‚å¹¶ä¸”MDSä¸PGEååŒä½œç”¨ï¼ŒMDSä¸ºPGEæä¾›å‡†ç¡®å¥–åŠ±æŒ‡å¯¼ï¼ŒPGEä¸ºMDSæä¾›æ›´æœ‰ä»·å€¼çš„æŸ¥è¯¢æ ·æœ¬ï¼Œå…±åŒæ¨åŠ¨å¥–åŠ±å’Œç­–ç•¥å­¦ä¹ è¿›ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨6ä¸ªæ¨¡æ‹Ÿçš„å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡å’Œ4ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ï¼ŒSENIORåœ¨äººç±»åé¦ˆæ•ˆç‡å’Œç­–ç•¥æ”¶æ•›é€Ÿåº¦ä¸Šå‡ä¼˜äºå…¶ä»–äº”ç§ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨æå‡PbRLæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åœ¨æŸ¥è¯¢é€‰æ‹©å±‚é¢ï¼Œå…³æ³¨ä»»åŠ¡ç›¸å…³çš„è¿åŠ¨ç­‰ä¿¡æ¯æ¥æŒ‘é€‰æ˜“æ ‡æ³¨ç‰‡æ®µï¼Œä¸ºç±»ä¼¼ä¾èµ–äººç±»åé¦ˆçš„å­¦ä¹ åœºæ™¯æä¾›äº†ä»ä»»åŠ¡ç‰¹å¾è§’åº¦ä¼˜åŒ–æ•°æ®é€‰æ‹©çš„æ€è·¯ã€‚
2. åœ¨æ¢ç´¢æœºåˆ¶å±‚é¢ï¼Œå°†äººç±»åå¥½èå…¥æ¢ç´¢è®¾è®¡ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢ä¸ä»»åŠ¡ç›®æ ‡ã€äººç±»æ„å›¾ç»“åˆæä¾›äº†åˆ›æ–°æ–¹å‘ï¼Œå¯å¯å‘åç»­åœ¨æå‡æ¢ç´¢é’ˆå¯¹æ€§ä¸Šçš„ç ”ç©¶ã€‚
3. å¤šä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿ+çœŸå®ä¸–ç•Œï¼‰çš„å®éªŒéªŒè¯æ¨¡å¼ï¼Œä¸ºæ–¹æ³•æ³›åŒ–æ€§éªŒè¯æä¾›äº†å‚è€ƒèŒƒå¼ï¼Œèƒ½è®©åç»­ç ”ç©¶æ›´æ³¨é‡æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§æ£€éªŒã€‚ 

## tgdpo--harnessing-token-level-reward-guidance-for-enhancing-direct-preference-optimization
### Abstract
Recent advancements in reinforcement learning from human feedback have shown
that utilizing fine-grained token-level reward models can substantially enhance
the performance of Proximal Policy Optimization (PPO) in aligning large
language models. However, it is challenging to leverage such token-level reward
as guidance for Direct Preference Optimization (DPO), since DPO is formulated
as a sequence-level bandit problem. To address this challenge, this work
decomposes the sequence-level PPO into a sequence of token-level proximal
policy optimization problems and then frames the problem of token-level PPO
with token-level reward guidance, from which closed-form optimal token-level
policy and the corresponding token-level reward can be derived. Using the
obtained reward and Bradley-Terry model, this work establishes a framework of
computable loss functions with token-level reward guidance for DPO, and
proposes a practical reward guidance based on the induced DPO reward. This
formulation enables different tokens to exhibit varying degrees of deviation
from reference policy based on their respective rewards. Experiment results
demonstrate that our method achieves substantial performance improvements over
DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on
AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at
https://github.com/dvlab-research/TGDPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TGDPOï¼šç”¨Tokençº§å¥–åŠ±æŒ‡å¯¼å¢å¼ºç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæå‡å¤§æ¨¡å‹å¯¹é½èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰é¢†åŸŸï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•å¸¸è¢«ç”¨äºè®©æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ï¼Œä½†ä¼ ç»Ÿåºåˆ—çº§å¥–åŠ±å­˜åœ¨ç¨€ç–æ€§ï¼ˆå¦‚å»¶è¿Ÿåé¦ˆï¼‰ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ ·æœ¬åˆ©ç”¨ä½æ•ˆã€‚è€Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è™½ç®€åŒ–äº†æµç¨‹ã€æ— éœ€å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä½†å®ƒæ˜¯åºåˆ—çº§çš„â€œè€è™æœºé—®é¢˜â€å½¢å¼ï¼Œéš¾ä»¥ç›´æ¥åˆ©ç”¨ç»†ç²’åº¦çš„Tokençº§å¥–åŠ±æ¥æŒ‡å¯¼ä¼˜åŒ–ã€‚å› æ­¤ï¼Œå¦‚ä½•æŠŠå¯¹PPOæœ‰æ•ˆçš„Tokençº§å¥–åŠ±ä¼˜åŠ¿å¼•å…¥DPOï¼Œæˆä¸ºå¾…è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåºåˆ—çº§PPOçš„Tokençº§åˆ†è§£ä¸æœ€ä¼˜ç­–ç•¥æ¨å¯¼  
è®ºæ–‡å°†åºåˆ—çº§çš„PPOåˆ†è§£ä¸ºä¸€ç³»åˆ—Tokençº§çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å­é—®é¢˜ï¼Œé€šè¿‡ä¸Šç•Œæ–¹æ³•é‡æ–°æ„å»ºé—®é¢˜åï¼Œæ¨å¯¼å‡ºäº†**é—­å¼çš„æœ€ä¼˜Tokençº§ç­–ç•¥**ï¼Œå¹¶å¾—åˆ°ä¸ä¹‹å¯¹åº”çš„Tokençº§å¥–åŠ±è¡¨ç¤ºï¼Œä¸ºåç»­æŠŠTokençº§å¥–åŠ±èå…¥DPOæ‰“ä¸‹åŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢å‘DPOçš„Tokençº§å¥–åŠ±æŒ‡å¯¼æ¡†æ¶ï¼ˆTGDPOï¼‰  
åŸºäºæ¨å¯¼å¾—åˆ°çš„Tokençº§å¥–åŠ±ï¼Œç»“åˆBradley - Terryæ¨¡å‹ä»¥åŠâ€œæ¶ˆé™¤é…åˆ†å‡½æ•°â€çš„æ–°ç†è®ºç»“æœï¼Œæå‡ºäº†**TGDPOæ¡†æ¶**â€”â€”ä¸ºDPOèµ‹äºˆTokençº§å¥–åŠ±æŒ‡å¯¼èƒ½åŠ›çš„åå¥½ä¼˜åŒ–ç®—æ³•ã€‚åŒæ—¶ï¼Œè¿˜è®¾è®¡äº†åŸºäºè¯±å¯¼DPOå¥–åŠ±çš„**å®ç”¨Tokençº§å¥–åŠ±æŒ‡å¯¼æ–¹æ¡ˆ**ï¼Œè®©ä¸åŒTokenèƒ½ä¾æ®è‡ªèº«å¥–åŠ±ï¼Œå‘ˆç°å‡ºä¸å‚è€ƒç­–ç•¥ä¸åŒç¨‹åº¦çš„åç¦»ï¼Œæ›´ç²¾ç»†åœ°è°ƒæ§ç”Ÿæˆè¿‡ç¨‹ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨AlpacaEval 2ã€MT - Benchã€Arena - Hardä¸‰å¤§æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­éªŒè¯æ•ˆæœï¼šä¸æœ€ä¼˜åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTGDPOåœ¨MT - Benchä¸Šèƒœç‡æå‡é«˜è¾¾7.5ä¸ªç™¾åˆ†ç‚¹ï¼ŒAlpacaEval 2ä¸Šæå‡6.2ä¸ªç™¾åˆ†ç‚¹ï¼ŒArena - Hardä¸Šæå‡4.3ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å±•ç°å‡ºTGDPOåœ¨æŸå¤±æ”¶æ•›æ—¶èƒ½å¾—åˆ°æ›´ä¼˜ç­–ç•¥ï¼ˆè¿™åœ¨ä¼ ç»Ÿåå¥½ä¼˜åŒ–æ–¹æ³•ä¸­ä¸å¸¸è§ï¼‰ã€å¯æ§åˆ¶æ”¶æ•›é€Ÿåº¦ä¸”å¯¹Tokençº§å¥–åŠ±å˜åŒ–é²æ£’ç­‰ä¼˜åŠ¿ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†è§£ä¸é‡æ„æ€è·¯ï¼šæŠŠåºåˆ—çº§é—®é¢˜æ‹†åˆ†ä¸ºTokençº§å­é—®é¢˜å¹¶æ¨å¯¼é—­å¼è§£ï¼Œè¿™ç§â€œåŒ–æ•´ä¸ºé›¶ + ç†è®ºæ¨å¯¼â€çš„æ€è·¯ï¼Œä¸ºå¤„ç†å¤§ç²’åº¦å¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†Tokençº§ç²¾ç»†è°ƒæ§çš„æ–°è§†è§’ã€‚  
2. è·¨ç®—æ³•èåˆåˆ›æ–°ï¼šæˆåŠŸå°†å¯¹PPOæœ‰æ•ˆçš„Tokençº§å¥–åŠ±èƒ½åŠ›å¼•å…¥DPOï¼Œè¯æ˜äº†ä¸åŒRLHFç®—æ³•é—´â€œä¼˜åŠ¿ç‰¹æ€§è¿ç§»â€çš„å¯è¡Œæ€§ï¼Œå¯å‘åç»­ç®—æ³•èåˆåˆ›æ–°ã€‚  
3. å®ç”¨å¥–åŠ±è®¾è®¡ï¼šæå‡ºçš„åŸºäºè¯±å¯¼å¥–åŠ±çš„Tokençº§æŒ‡å¯¼æ–¹æ¡ˆï¼Œä¸ºå®é™…è½åœ°æ—¶å¦‚ä½•åˆ©ç”¨ç»†ç²’åº¦åé¦ˆä¼˜åŒ–å¤§æ¨¡å‹ï¼Œæä¾›äº†å¯å‚è€ƒçš„å·¥ç¨‹åŒ–æ€è·¯ã€‚  

## gram--a-generative-foundation-reward-model-for-reward-generalization
### Abstract
In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GRAMï¼šé¢å‘å¥–åŠ±æ³›åŒ–çš„ç”Ÿæˆå¼åŸºç¡€å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½å·¥ä½œä¸­ï¼Œå¥–åŠ±æ¨¡å‹æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¤šä»¥åˆ¤åˆ«å¼æ¨¡å‹å½¢å¼è®­ç»ƒï¼Œä¸”é«˜åº¦ä¾èµ–å¸¦æ ‡ç­¾çš„äººç±»åå¥½æ•°æ®ã€‚è¿™å¸¦æ¥äº†ä¸¤æ–¹é¢é—®é¢˜ï¼šä¸€æ˜¯å¼ºåŒ–å­¦ä¹ ç®—æ³•å¤æ‚åº¦ä¸æ ‡æ³¨æ•°æ®è·å–éš¾åº¦å¯¼è‡´å¥–åŠ±æ¨¡å‹åº”ç”¨æˆæœ¬é«˜æ˜‚ï¼›äºŒæ˜¯ç°æœ‰è®­ç»ƒæ–¹å¼å¯¹å¤§é‡æ— æ ‡ç­¾æ•°æ®åˆ©ç”¨ä¸è¶³ï¼Œéš¾ä»¥å¾—åˆ°èƒ½çµæ´»é€‚é…å¤šä»»åŠ¡çš„é€šç”¨å¥–åŠ±æ¨¡å‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢åˆ©ç”¨æ— æ ‡ç­¾å’Œæœ‰æ ‡ç­¾æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼Œæ‰“é€ å¯æ³›åŒ–çš„åŸºç¡€å¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µè®­ç»ƒçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¶æ„  
åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆå¼èƒ½åŠ›ï¼Œæ„å»ºç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRAMï¼‰ï¼Œåˆ†ä¸¤é˜¶æ®µè®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µåœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾çš„è¾“å…¥ - å“åº”æ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ è¾“å…¥ä¸å“åº”é—´çš„å¯¹åº”å…³ç³»ï¼Œæ— éœ€åå¥½æ ‡æ³¨æ•°æ®ï¼Œå¯è§„æ¨¡åŒ–è·å–å“åº”æ¯”è¾ƒçš„é€šç”¨çŸ¥è¯†ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨äººç±»åå¥½æ•°æ®è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šé¢„æµ‹ä¸¤ä¸ªå“åº”é—´çš„åå¥½å…³ç³»ã€‚æœ€ç»ˆå¾—åˆ°çš„åŸºç¡€å¥–åŠ±æ¨¡å‹èƒ½ç›´æ¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡æˆ–ä»…ç”¨å°‘é‡ä»»åŠ¡ç‰¹å®šæ•°æ®å¾®è°ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ ‡ç­¾å¹³æ»‘ä¸‹çš„æŸå¤±å‡½æ•°ç»Ÿä¸€è§†è§’  
å¼•å…¥æ ‡ç­¾å¹³æ»‘æŠ€æœ¯åˆ°å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¯æ˜æ­¤æ—¶è®­ç»ƒç›®æ ‡å¯è½¬åŒ–ä¸ºæ­£åˆ™åŒ–çš„ pairwise ranking lossï¼ˆBradley - Terry æŸå¤±ï¼‰å½¢å¼ã€‚è¿™ä¸€æˆæœåœ¨ä¸€å®šç¨‹åº¦ä¸Šç»Ÿä¸€äº†ç”Ÿæˆå¼æ¨¡å‹ä¸åˆ¤åˆ«å¼æ¨¡å‹çš„è®­ç»ƒç›®æ ‡è§†è§’ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°è®¤çŸ¥ï¼Œä¸”æ ‡ç­¾å¹³æ»‘å¯¹è®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ååˆ†æœ‰ç›Šï¼Œæå‡äº†æ¨¡å‹æ³›åŒ–æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å“åº”æ’åºã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€ä»»åŠ¡é€‚é…ç­‰å¤šä»»åŠ¡åœºæ™¯ä¸‹å¼€å±•å¤§é‡å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒGRAM åœ¨å‡ ä¹æ— éœ€æˆ–ä»…éœ€å°‘é‡å¾®è°ƒæ—¶ï¼Œåœ¨å„ä»»åŠ¡ä¸Šæ³›åŒ–æ€§å‡ºè‰²ã€‚ä¾‹å¦‚ï¼ŒåŸºäº LLaMA - 3.1 - 8B - Instruct æ¨¡å‹è®­ç»ƒå¥–åŠ±æ¨¡å‹æ—¶ï¼Œåœ¨ RewardBench å¹³å‡å‡†ç¡®ç‡ä¸Šï¼Œç›¸æ¯”æ™®é€šåˆ¤åˆ«å¼å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åˆ†åˆ«æå‡ 11.0 å’Œ 5.1 ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—è¶…è¶Šå¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆ©ç”¨æ€è·¯ï¼šæ‰“ç ´ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¯¹æœ‰æ ‡ç­¾æ•°æ®çš„å¼ºä¾èµ–ï¼Œç¤ºèŒƒäº†æ— æ ‡ç­¾æ•°æ®åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸ºæ¨¡å‹æ³¨å…¥é€šç”¨çŸ¥è¯†çš„ä»·å€¼ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹ç”šè‡³å…¶ä»–æ¨¡å‹è®­ç»ƒåœ¨æ•°æ®åˆ©ç”¨ä¸Šå¼€è¾Ÿäº†â€œæ— æ ‡ç­¾ + æœ‰æ ‡ç­¾â€ç»“åˆçš„æ€è·¯ã€‚
2. æ¨¡å‹æ¶æ„åˆ›æ–°ï¼šä¸¤é˜¶æ®µçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¶æ„ä¸ºæ‰“é€ é€šç”¨åŸºç¡€å¥–åŠ±æ¨¡å‹æä¾›äº†å¯è¡ŒèŒƒå¼ï¼Œå±•ç¤ºäº†å…ˆæ— ç›‘ç£é¢„è®­ç»ƒå†ç›‘ç£å¾®è°ƒåœ¨å¥–åŠ±æ¨¡å‹é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œå¯å¯å‘åç»­å¤šæ¨¡æ€ã€å…¶ä»–ä»»åŠ¡å¯¼å‘æ¨¡å‹çš„æ¶æ„è®¾è®¡ã€‚
3. æŸå¤±å‡½æ•°ä¸æ­£åˆ™åŒ–ï¼šæ ‡ç­¾å¹³æ»‘ç»“åˆåå¯¹æŸå¤±å‡½æ•°çš„åˆ†æä¸è½¬åŒ–ï¼Œä¸ºç†è§£ç”Ÿæˆå¼å’Œåˆ¤åˆ«å¼æ¨¡å‹åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„è”ç³»æä¾›æ–°è§’åº¦ï¼Œä¹Ÿæé†’å¼€å‘è€…åœ¨æ¨¡å‹è®­ç»ƒä¸­é‡è§†æ­£åˆ™åŒ–æŠ€æœ¯å¯¹æ¨¡å‹æ³›åŒ–ç­‰èƒ½åŠ›çš„æå‡ä½œç”¨ã€‚

## vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training
### Abstract
Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large
language models but remains underexplored for Vision-Language (VL) models. The
Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing
structured feedback, yet training effective VL-RMs faces two major challenges.
First, the bootstrapping dilemma arises as high-quality training data depends
on already strong VL models, creating a cycle where self-generated supervision
reinforces existing biases. Second, modality bias and negative example
amplification occur when VL models hallucinate incorrect visual attributes,
leading to flawed preference data that further misguides training. To address
these issues, we propose an iterative training framework leveraging vision
experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection
Sampling. Our approach refines preference datasets, enhances structured
critiques, and iteratively improves reasoning. Experiments across VL-RM
benchmarks demonstrate superior performance in hallucination detection and
multimodal reasoning, advancing VL model alignment with reinforcement learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VL-GenRMï¼šå€Ÿè§†è§‰ä¸“å®¶ä¸è¿­ä»£è®­ç»ƒï¼Œçªç ´å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è®­ç»ƒå›°å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å·²å–å¾—è¿›å±•ï¼Œä½†åœ¨è§†è§‰-è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹ä¸­ä»å¾…æ·±å…¥æ¢ç´¢ã€‚è§†è§‰-è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼ˆVL-RMï¼‰æ˜¯å¯¹é½VLæ¨¡å‹çš„å…³é”®ï¼Œèƒ½æä¾›ç»“æ„åŒ–åé¦ˆï¼Œç„¶è€Œè®­ç»ƒé«˜æ•ˆVL-RMé¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯â€œè‡ªä¸¾å›°å¢ƒâ€ï¼Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ä¾èµ–å¼ºVLæ¨¡å‹ç”Ÿæˆï¼Œæ˜“å¼ºåŒ–æ¨¡å‹å›ºæœ‰åå·®ï¼›äºŒæ˜¯â€œæ¨¡æ€åå·®ä¸è´Ÿä¾‹æ”¾å¤§â€ï¼ŒVLæ¨¡å‹å¯¹è§†è§‰å±æ€§çš„é”™è¯¯å¹»æƒ³ä¼šäº§ç”Ÿæœ‰ç¼ºé™·çš„åå¥½æ•°æ®ï¼Œè¯¯å¯¼è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºåˆ›æ–°è®­ç»ƒæ¡†æ¶VL-GenRMã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå€Ÿè§†è§‰ä¸“å®¶è‡ªåŠ¨æ„å»ºåå¥½æ•°æ®é›†  
åˆ©ç”¨è§†è§‰ä¸“å®¶ï¼ˆå¦‚æ“…é•¿ç›®æ ‡æ£€æµ‹ã€æ·±åº¦ä¼°è®¡ç­‰çš„æ¨¡å‹ï¼‰ç”Ÿæˆå¤§è§„æ¨¡åå¥½æ•°æ®é›†ï¼Œæå‡VL-GenRMè®­ç»ƒæ—¶çš„ç›‘ç£è´¨é‡ï¼Œæ‰“ç ´â€œè‡ªä¸¾å›°å¢ƒâ€ä¸­ä¾èµ–è‡ªç”Ÿæˆæ•°æ®å¼ºåŒ–åå·®çš„é—®é¢˜ï¼Œä¸ºæ¨¡å‹æä¾›æ›´å¯é çš„è®­ç»ƒä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCoTå¢å¼ºVL-GenRMè®­ç»ƒ  
å¼•å…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºVL-GenRMè®­ç»ƒæä¾›ç³»ç»Ÿæ€§æŒ‡å¯¼ã€‚é€šè¿‡ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¢åŠ æ•°æ®ä¸­æœ‰æ•ˆæ­£ç¡®æè¿°å æ¯”ï¼Œç¼“è§£è‡ªç”Ÿæˆæ•°æ®çš„å±€é™æ€§ï¼Œå¼ºåŒ–å¥–åŠ±å»ºæ¨¡çš„è¿è´¯æ€§ï¼Œè®©æ¨¡å‹å­¦ä¹ æ›´åˆç†çš„è¯„ä¼°é€»è¾‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºè¾¹é™…æ‹’ç»é‡‡æ ·çš„è¿­ä»£è‡ªä¸¾  
é€šè¿‡å¯¹â€œæ­£ä¾‹ä¸è´Ÿä¾‹å¥–åŠ±ä¿¡å·è¾¹é™…â€ç­›é€‰å‡ºçš„ä¼˜è´¨æ¨ç†ä¾æ®ï¼Œè¿›è¡Œè¿­ä»£å¾®è°ƒï¼ŒæŒç»­ä¼˜åŒ–VL-GenRMçš„æ¨ç†èƒ½åŠ›ã€‚è®©æ¨¡å‹åœ¨å¤šè½®è®­ç»ƒä¸­é€æ­¥å‘æ›´ä¼˜è¾“å‡ºé€‚é…ï¼Œä¸æ–­æå‡å¯¹è§†è§‰-è¯­è¨€åœºæ™¯çš„è¯„ä¼°ä¸æ¨ç†æ°´å‡†ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨VL-RMåŸºå‡†æµ‹è¯•ä¸Best-of-Né‡‡æ ·ç­‰å®éªŒä¸­éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œåœ¨å¹»è§‰æ£€æµ‹ï¼ˆè¯†åˆ«VLæ¨¡å‹é”™è¯¯å¹»æƒ³è§†è§‰å±æ€§ç­‰é—®é¢˜ï¼‰ä¸å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´ä¼˜æ€§èƒ½ï¼Œæ¨åŠ¨äº†VLæ¨¡å‹å€ŸåŠ©å¼ºåŒ–å­¦ä¹ å®ç°æ›´å¥½å¯¹é½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨æ¨¡æ€é¢†åŸŸæ•°æ®å¢å¼ºæ€è·¯ï¼šå¼•å…¥é¢†åŸŸä¸“å®¶ï¼ˆå¦‚è§†è§‰ä¸“å®¶ï¼‰è¾…åŠ©æ„å»ºè®­ç»ƒæ•°æ®ï¼Œä¸ºçªç ´â€œè‡ªä¸¾å¾ªç¯â€æä¾›äº†æ–°èŒƒå¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€å¤šæ¨¡æ€åä½œã€ä¾èµ–æ•°æ®è´¨é‡çš„ä»»åŠ¡ã€‚  
2. ç»“æ„åŒ–æ¨ç†èå…¥è®­ç»ƒï¼šå€ŸåŠ©CoTå°†æ¨¡ç³Šçš„è¯„ä¼°è½¬åŒ–ä¸ºå¯è§£é‡Šçš„æ¨ç†æ­¥éª¤ï¼Œä¸ºæå‡æ¨¡å‹å¯è§£é‡Šæ€§ä¸è®­ç»ƒæœ‰æ•ˆæ€§æä¾›äº†å‚è€ƒï¼Œåœ¨å¤æ‚ä»»åŠ¡å‹æ¨¡å‹è®­ç»ƒä¸­å…·å€Ÿé‰´ä»·å€¼ã€‚  
3. è¿­ä»£å¼è®­ç»ƒç­–ç•¥ï¼šåŸºäºå¥–åŠ±è¾¹é™…çš„é‡‡æ ·ä¸è¿­ä»£å¾®è°ƒï¼Œè®©æ¨¡å‹èƒ½åŠ›é€æ­¥è¿­ä»£æå‡ï¼Œè¿™ç§â€œå°æ­¥å¿«è·‘ã€æ•°æ®æ‹©ä¼˜â€çš„è®­ç»ƒé€»è¾‘ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸å¤šè½®ä¼˜åŒ–åœºæ™¯ä¸­å€¼å¾—å¤ç”¨ã€‚  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å‘Šåˆ«ç¹çæ ‡æ³¨ï¼šGAN - RM è®©å¥–åŠ±å»ºæ¨¡â€œä»¥å‡ä¹±çœŸâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒåå¢å¼ºä¸­ï¼Œå¥–åŠ±æ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œå½“å‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•å­˜åœ¨è¯¸å¤šéš¾é¢˜ï¼šä¸€æ˜¯æ„å»ºå¥–åŠ±æ¨¡å‹éœ€å¤§é‡äººå·¥æ ‡æ³¨åå¥½æ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œä¸”åŸºäºç‰¹å®šç”Ÿæˆæ¨¡å‹è¾“å‡ºåŸŸæ ‡æ³¨çš„æ•°æ®ï¼Œåœ¨åº”ç”¨åˆ°ä¸åŒè¾“å‡ºåŸŸæ¨¡å‹æ—¶å­˜åœ¨åŸŸå·®è·ï¼›äºŒæ˜¯ä¸ºå…¨é¢è¯„ä¼°ç”Ÿæˆå†…å®¹è´¨é‡ï¼Œéœ€äººå·¥è®¾è®¡å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¢å¢åŠ å·¥ç¨‹æˆæœ¬ï¼Œåˆéš¾åœ¨ä¸åŒç»´åº¦é—´å–å¾—æœ€ä¼˜å¹³è¡¡ï¼Œè¿˜éš¾ä¿è¯ä¸äººç±»æ™®éåå¥½å¥‘åˆã€‚å› æ­¤ï¼Œæœ¬æ–‡å—ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸­å¯¹æŠ—è®­ç»ƒå¯å‘ï¼Œæå‡º GAN - RM æ¡†æ¶ï¼Œæ—¨åœ¨æ‘†è„±æ‰‹åŠ¨åå¥½æ ‡æ³¨å’Œæ˜¾å¼è´¨é‡ç»´åº¦è®¾è®¡ï¼Œé«˜æ•ˆæ„å»ºå¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— éœ€æ‰‹åŠ¨åå¥½æ ‡æ³¨ï¼Œåˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®
GAN - RM ä»…éœ€å°‘é‡ï¼ˆå‡ ç™¾ä¸ªï¼‰æ— æ ‡æ³¨çš„ä»£è¡¨æ€§æ ·æœ¬ï¼ˆå³åå¥½ä»£ç†æ•°æ®ï¼ŒPreference Proxy Dataï¼‰ä½œä¸ºå¤–éƒ¨æ•°æ®ã€‚é€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹åŒºåˆ†åå¥½ä»£ç†æ•°æ®å’Œç”Ÿæˆæ¨¡å‹è¾“å‡ºï¼Œè®©æ¨¡å‹å­¦ä¹ è¯„ä¼°ç”Ÿæˆæ ·æœ¬ã€‚åŒæ—¶é‡‡ç”¨åŸºäºæ’åçš„è‡ªä¸¾ç­–ç•¥ï¼Œå°† GAN - RM åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„ç½®ä¿¡åˆ†æ•°ä½œä¸ºè½¯æ ‡ç­¾ï¼Œåˆ©ç”¨é¢å¤–æ•°æ®å†è®­ç»ƒ GAN - RMï¼Œä½¿å…¶æ›´å¥½æ•æ‰æ½œåœ¨äººç±»åå¥½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šè½®è®­ç»ƒï¼Œè¿­ä»£å¯¹é½åå¥½
GAN - RM æ”¯æŒå¤šè½®è®­ç»ƒåä¼˜åŒ–ã€‚æ¯ä¸€è½®ä¸­ï¼Œå°†è¢«è¯†åˆ«ä¸ºæ¥è¿‘åå¥½ä»£ç†æ•°æ®çš„æ ·æœ¬ç”¨äºç”Ÿæˆå™¨çš„è®­ç»ƒåä¼˜åŒ–ï¼Œåè¿‡æ¥å†è®­ç»ƒåˆ¤åˆ«å™¨ä»¥åŒºåˆ†è¿™äº›æ›´éš¾çš„æ ·æœ¬ã€‚è¿™ç§è¿­ä»£çš„â€œä»¥å‡ä¹±çœŸâ€è¿‡ç¨‹èƒ½é€æ­¥è®©ç”Ÿæˆè´¨é‡ä¸åå¥½ä»£ç†æ•°æ®ä¸­çš„æ½œåœ¨äººç±»åå¥½å¯¹é½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒåŸºäº GAN - RM çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯ä¸ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼ˆå¦‚ Pickapic çš„ 100 ä¸‡æ ‡æ³¨äººç±»åå¥½æ•°æ®ï¼‰çš„æ–¹æ³•ï¼ˆå¦‚ç›¸å…³å¯¹æ¯”æ–¹æ³•ï¼‰ç›¸å½“ç”šè‡³è¶…è¶Šã€‚åœ¨å›¾åƒè´¨é‡å®éªŒè®¾ç½®ä¸­ï¼ŒGAN - RM ä»…éœ€ 500 ä¸ªåå¥½ä»£ç†æ•°æ®æ ·æœ¬ã€‚é™¤å›¾åƒè´¨é‡æå‡å®éªŒå¤–ï¼Œåœ¨å›¾åƒå®‰å…¨å’Œè§†é¢‘è´¨é‡å¢å¼ºåœºæ™¯ä¸‹çš„å®éªŒä¹Ÿå‡¸æ˜¾äº† GAN - RM æ¡†æ¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆå¦‚ Best - of - N æ ·æœ¬è¿‡æ»¤ï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰è®­ç»ƒåæ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼ŒGAN - RM ä¸ºè§£å†³å¥–åŠ±å»ºæ¨¡ä¸­æ•°æ®è·å–éš¾ã€ä¾èµ–ç‰¹å®šåŸŸã€äººå·¥è®¾è®¡ç»´åº¦éš¾å¥‘åˆäººç±»åå¥½ç­‰é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå…¶åˆ©ç”¨å¯¹æŠ—è®­ç»ƒå’Œå°‘é‡ä»£ç†æ•°æ®çš„æ–¹å¼ï¼Œå‡å°‘äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œé™ä½å·¥ç¨‹æˆæœ¬ï¼›ä»åº”ç”¨æ‹“å±•è§’åº¦ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒã€è§†é¢‘ç­‰å¤šåœºæ™¯çš„æœ‰æ•ˆå®éªŒï¼Œä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è®­ç»ƒåå¢å¼ºæä¾›äº†å¯å¤ç”¨çš„å¥–åŠ±å»ºæ¨¡èŒƒå¼ï¼Œåç»­åœ¨è§†è§‰ç”Ÿæˆç›¸å…³ä»»åŠ¡ä¸­ï¼Œè‹¥éœ€æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œå¯å€Ÿé‰´å…¶åˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®å’Œå¯¹æŠ—è®­ç»ƒçš„æ€è·¯æ¥é™ä½æˆæœ¬ä¸éš¾åº¦ã€‚

## pb$^2$--preference-space-exploration-via-population-based-methods-in-preference-based-reinforcement-learning
### Abstract
Preference-based reinforcement learning (PbRL) has emerged as a promising
approach for learning behaviors from human feedback without predefined reward
functions. However, current PbRL methods face a critical challenge in
effectively exploring the preference space, often converging prematurely to
suboptimal policies that satisfy only a narrow subset of human preferences. In
this work, we identify and address this preference exploration problem through
population-based methods. We demonstrate that maintaining a diverse population
of agents enables more comprehensive exploration of the preference landscape
compared to single-agent approaches. Crucially, this diversity improves reward
model learning by generating preference queries with clearly distinguishable
behaviors, a key factor in real-world scenarios where humans must easily
differentiate between options to provide meaningful feedback. Our experiments
reveal that current methods may fail by getting stuck in local optima,
requiring excessive feedback, or degrading significantly when human evaluators
make errors on similar trajectories, a realistic scenario often overlooked by
methods relying on perfect oracle teachers. Our population-based approach
demonstrates robust performance when teachers mislabel similar trajectory
segments and shows significantly enhanced preference exploration
capabilities,particularly in environments with complex reward landscapes.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PBÂ²ï¼šåŸºäºç¾¤ä½“æ–¹æ³•ç ´è§£åå¥½å¼ºåŒ–å­¦ä¹ ä¸­çš„åå¥½ç©ºé—´æ¢ç´¢éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼—å¤šé¢†åŸŸå–å¾—æˆåŠŸï¼Œä½†ä¼ ç»ŸRLä¾èµ–ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå¤æ‚ä»»åŠ¡ä¸­å› æ¶‰åŠä¸»è§‚ç»“æœæˆ–äººç±»åå¥½ï¼Œå¥–åŠ±å‡½æ•°éš¾ä»¥æŒ‡å®šã€‚åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡äººç±»å¯¹è½¨è¿¹å¯¹çš„åå¥½åé¦ˆå­¦ä¹ è¡Œä¸ºï¼Œæ— éœ€æ‰‹å·¥å¥–åŠ±å‡½æ•°ï¼Œä½†ç°æœ‰PbRLæ–¹æ³•å­˜åœ¨å…³é”®æŒ‘æˆ˜ï¼šæœ‰æ•ˆæ¢ç´¢åå¥½ç©ºé—´ä¸è¶³ï¼Œå¸¸è¿‡æ—©æ”¶æ•›åˆ°ä»…æ»¡è¶³ç‹­çª„äººç±»åå¥½å­é›†çš„æ¬¡ä¼˜ç­–ç•¥ã€‚å•æ™ºèƒ½ä½“æ–¹æ³•æ˜“ç”Ÿæˆç›¸ä¼¼è½¨è¿¹å¯¹ç”¨äºåå¥½ elicitationï¼Œé™åˆ¶åé¦ˆä¿¡æ¯å¤šæ ·æ€§ï¼Œå¥–åŠ±æ¨¡å‹å­¦ä¹ æ ·æœ¬å•ä¸€ï¼›ä¸”äººç±»è¯„ä¼°ç›¸ä¼¼è½¨è¿¹æ—¶åé¦ˆä¸ä¸€è‡´ä¼šä¸¥é‡é™ä½å­¦ä¹ æ€§èƒ½ï¼Œä¾èµ–â€œå®Œç¾å…ˆçŸ¥æ•™å¸ˆâ€çš„æ–¹æ³•å¸¸å¿½ç•¥è¿™ä¸€ç°å®åœºæ™¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯†åˆ«åå¥½æ¢ç´¢é—®é¢˜  
æ˜ç¡®PbRLä¸­åå¥½æ¢ç´¢éš¾é¢˜ï¼Œè®ºè¯å•æ™ºèƒ½ä½“æ–¹æ³•æ˜“åœ¨åå¥½ç©ºé—´æ”¶æ•›åˆ°æ¬¡ä¼˜å±€éƒ¨æå°å€¼ï¼Œå¯¼è‡´æ¢ç´¢ä¸è¶³ä¸ç­–ç•¥ä¸ä½³ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºç¾¤ä½“åŒ–PbRLæ¡†æ¶ï¼ˆPBÂ²ï¼‰  
é‡‡ç”¨ç¾¤ä½“æ–¹æ³•åŒæ—¶è®­ç»ƒå¤šä¸ªä¸åŒç­–ç•¥ï¼Œç›¸æ¯”å•ç­–ç•¥æ–¹æ³•æ›´å…¨é¢æ¢ç´¢åå¥½ landscapeã€‚é€šè¿‡ä¸åŒç­–ç•¥æ”¶é›†ç»éªŒæ„å»ºæ¯”è¾ƒå¯¹ï¼Œæå‡è¯„ä¼°è¡Œä¸ºçš„å¤šæ ·æ€§ï¼Œä¸”ä¸äººç±»åå¥½å¯¹é½ã€‚å€ŸåŠ©åé¦ˆå¾ªç¯å®ç°ï¼šå¤šæ ·ç­–ç•¥ç”Ÿæˆç‹¬ç‰¹è½¨è¿¹â†’äººç±»å¯¹è½¨è¿¹çš„åå¥½è®­ç»ƒå¥–åŠ±æ¨¡å‹â†’åˆ¤åˆ«å™¨ç»´æŒç¾¤ä½“å¤šæ ·æ€§å¹¶é¼“åŠ±ç¬¦åˆå½“å‰åå¥½çš„è¡Œä¸ºï¼Œè®©åå¥½æŸ¥è¯¢æ›´æ˜“åŒºåˆ†ï¼Œè§£å†³ç°æœ‰æ–¹æ³•éšå«å‡è®¾äººç±»èƒ½å¯é è¯„ä¼°ç»†å¾®å·®å¼‚è¡Œä¸ºçš„ä¸è¶³ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡ä¸‰ç±»å®éªŒéªŒè¯ï¼šï¼ˆ1ï¼‰åœ¨DMControl locomotionä»»åŠ¡ä¸Šè®¾ç½®ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼Ïµæ¨¡æ‹Ÿäººç±»åˆ¤æ–­ä¸ä¸€è‡´ï¼Œç³»ç»Ÿè¯„ä¼°æ–¹æ³•é²æ£’æ€§ï¼›ï¼ˆ2ï¼‰å®šæ€§å±•ç¤ºPBÂ²åœ¨å¤æ‚åå¥½ landscape ä¸­æ‘†è„±å±€éƒ¨æœ€ä¼˜ï¼Œè€Œå•æ™ºèƒ½ä½“æ–¹æ³•é™·å…¥å…¶ä¸­ï¼›ï¼ˆ3ï¼‰åœ¨åé¦ˆææœ‰é™çš„å¯¼èˆªä»»åŠ¡ä¸­å¯¹æ¯”åˆ†æï¼Œä½“ç°PBÂ²çš„åé¦ˆæ•ˆç‡ã€‚å®éªŒè¡¨æ˜PBÂ²ç”Ÿæˆæ›´æ˜“åŒºåˆ†çš„æŸ¥è¯¢æå‡å¥–åŠ±å­¦ä¹ æ•ˆç‡ï¼Œåœ¨äººç±»åé¦ˆä¸ä¸€è‡´æ—¶æ›´é²æ£’ï¼Œå°‘åé¦ˆä¸‹æ€§èƒ½æ›´ä¼˜ï¼›é¢å¯¹æ•™å¸ˆå¯¹ç›¸ä¼¼è½¨è¿¹æ®µè¯¯æ ‡åœºæ™¯ï¼Œä¹Ÿå±•ç°ç¨³å¥æ€§èƒ½ä¸æ›´å¼ºåå¥½æ¢ç´¢èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç¾¤ä½“åŒ–æ€è·¯æ‹“å±•ï¼šå°†ç¾¤ä½“æ–¹æ³•å¼•å…¥PbRLï¼Œä¸ºè§£å†³å•æ™ºèƒ½ä½“æ¢ç´¢å±€é™æä¾›æ–°èŒƒå¼ï¼Œå¯ç¤ºåœ¨ä¾èµ–äººç±»åé¦ˆã€éœ€è¡Œä¸ºå¤šæ ·æ€§çš„ä»»åŠ¡ä¸­ï¼Œå¯è€ƒè™‘å¤šæ™ºèƒ½ä½“å¹¶è¡Œæ¢ç´¢æ¨¡å¼ã€‚  
2. ç°å®åœºæ™¯é€‚é…ï¼šå…³æ³¨äººç±»åé¦ˆä¸ä¸€è‡´çš„ç°å®æƒ…å†µï¼Œè®¾è®¡é²æ£’æ–¹æ³•ï¼Œä¸ºå®é™…åº”ç”¨ä¸­å¤„ç†å™ªå£°åé¦ˆæä¾›å‚è€ƒï¼Œå¼ºè°ƒæ–¹æ³•è¦é€‚åº”çœŸå®ä¸–ç•Œéç†æƒ³æ•™å¸ˆçš„åœºæ™¯ã€‚  
3. åå¥½æ¢ç´¢ä¸å¥–åŠ±æ¨¡å‹ååŒï¼šé€šè¿‡è¡Œä¸ºå¤šæ ·æ€§è®©åå¥½æŸ¥è¯¢æ›´æ˜“åŒºåˆ†ï¼Œä¼˜åŒ–å¥–åŠ±æ¨¡å‹å­¦ä¹ ï¼Œæç¤ºåœ¨æ¶‰åŠäººç±»äº¤äº’çš„å­¦ä¹ ä»»åŠ¡é‡Œï¼Œé‡è§†äº¤äº’å†…å®¹çš„â€œå¯åŒºåˆ†æ€§â€ä»¥æå‡åé¦ˆä»·å€¼ã€‚

## $\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts
### Abstract
Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SPECSï¼šç”¨â€œæ¨æµ‹è‰ç¨¿â€åŠ é€Ÿå¤§æ¨¡å‹æ¨ç†ï¼Œå¹³è¡¡å»¶è¿Ÿä¸ç²¾åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æå‡å¸¸ä¾èµ–â€œæµ‹è¯•æ—¶ç®—åŠ›æ‰©å®¹â€ï¼Œæ¯”å¦‚åˆ†é…æ›´å¤šè®¡ç®—èµ„æºåšæ›´å……åˆ†çš„æ¢ç´¢ã€‚ä½†ç®—åŠ›å¢åŠ å¾€å¾€å¯¼è‡´ç”¨æˆ·ä¾§å»¶è¿Ÿå‡é«˜ï¼Œç›´æ¥å½±å“ä½“éªŒã€‚ç°æœ‰æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•å¤šèšç„¦ç®—åŠ›ï¼ˆFLOPSï¼‰ä¼˜åŒ–ç²¾åº¦ï¼Œå´å¿½ç•¥å»¶è¿Ÿçº¦æŸã€‚æ­¤å¤–ï¼ŒåŸºäºTransformerçš„LLMè‡ªå›å½’é‡‡æ ·å»¶è¿Ÿå¸¸å—é™äºå†…å­˜åŠ è½½è€Œéæ€»ç®—åŠ›ï¼Œè€Œæ¨æµ‹è§£ç è™½èƒ½å€Ÿå°æ¨¡å‹æå€™é€‰ token é™å»¶è¿Ÿï¼Œå´ä¼šå¢åŠ æ€»è®¡ç®—é‡ã€‚äºæ˜¯ï¼Œè®ºæ–‡è¯•å›¾å›ç­”ï¼š**èƒ½å¦è®¾è®¡é«˜æ•ˆæµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ï¼Œä¼˜åŒ–å»¶è¿Ÿ - æ•ˆç”¨æƒè¡¡ï¼Ÿ**

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º SPECS ç®—æ³•æ¡†æ¶  
SPECS å—æ¨æµ‹è§£ç å¯å‘ï¼Œæ˜¯ä¸€ç§â€œå»¶è¿Ÿæ„ŸçŸ¥å‹â€æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ã€‚å®ƒç”¨**æ›´å°æ›´å¿«çš„è‰ç¨¿æ¨¡å‹**é«˜æ•ˆç”Ÿæˆå€™é€‰åºåˆ—ï¼Œå†ç»“åˆ**æ›´å¤§çš„ç›®æ ‡æ¨¡å‹**ä¸**ä¸“ç”¨å¥–åŠ±æ¨¡å‹**è¯„ä¼°å€™é€‰ã€‚æ•´ä½“éµå¾ªâ€œè‰ç¨¿ - é€‰æ‹©â€æµç¨‹ï¼šè¿­ä»£ç”Ÿæˆå“åº”å—ï¼Œæ¯è½®ç”¨è‰ç¨¿æ¨¡å‹ç”Ÿæˆå€™é€‰å—ï¼Œç»æ‰“åˆ†é€‰æ‹©åæ‹¼æ¥ï¼Œè¿›å…¥ä¸‹ä¸€è½®ï¼›è‹¥è‰ç¨¿å…¨è¢«æ‹’ï¼Œåˆ™åˆ‡æ¢ç›®æ ‡æ¨¡å‹ç”Ÿæˆå€™é€‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¥–åŠ±å¼•å¯¼çš„è½¯éªŒè¯ä¸å»¶è¿Ÿæœºåˆ¶  
- å¥–åŠ±å¼•å¯¼è½¯éªŒè¯ï¼ˆSUBSAMPLE å­ä¾‹ç¨‹ï¼‰ï¼šåŸºäºè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡å‹è®¡ç®—çš„â€œåˆ†æ•°â€é€‰å€™é€‰å—ï¼Œæ—¢ä¼˜åŒ–æ•ˆç”¨ - å»¶è¿Ÿæƒè¡¡ï¼Œä¹Ÿé¿å…ç®€å•ä¸¢å¼ƒé«˜å¥–åŠ±ä½†å¯èƒ½è¢« naive æ¨æµ‹è§£ç æ¼æ‰çš„è½¨è¿¹ã€‚  
- å¥–åŠ±æ„ŸçŸ¥å»¶è¿Ÿè§„åˆ™ï¼ˆCASCADE å­ä¾‹ç¨‹ï¼‰ï¼šè‡ªé€‚åº”å†³å®šä¸‹ä¸€è½®ç”¨è‰ç¨¿è¿˜æ˜¯ç›®æ ‡æ¨¡å‹ç”Ÿæˆå€™é€‰â€”â€”è®©å¤§æ¨¡å‹å¤„ç†éš¾é¢˜æ­¥éª¤ï¼Œå°æ¨¡å‹å¤„ç†ç®€å•æ­¥éª¤ï¼ŒåŠ¨æ€å¹³è¡¡ç®—åŠ›ä¸å»¶è¿Ÿã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†è®ºåˆ†æä¿éšœæ”¶æ•›æ€§  
ä»ç†è®ºä¸Šåˆ†æï¼ŒSPECS åœ¨ç»“åˆè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡å‹ä¼˜åŒ–â€œKL æ­£åˆ™åŒ–å¥–åŠ±æœ€å¤§åŒ–â€ç›®æ ‡æ—¶ï¼Œå…¶è½¯éªŒè¯æ–¹æ³•éš beam å®½åº¦å¢å¤§ï¼Œèƒ½ä¼˜é›…æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ MATH500ã€AMC23ã€OlympiadBench æ•°æ®é›†æµ‹è¯•ï¼Œç”¨ Qwen - 1.5B - Instructï¼ˆè‰ç¨¿æ¨¡å‹ï¼‰ã€Qwen - 7B - Instructï¼ˆç›®æ ‡æ¨¡å‹ï¼‰ä¸ Qwen - 7B - Math - PRMï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰éªŒè¯ï¼š  
- ç²¾åº¦å±‚é¢ï¼šSPECS åŒ¹é…ç”šè‡³è¶…è¶Š beam search ç²¾åº¦ï¼›  
- å»¶è¿Ÿå±‚é¢ï¼šå»¶è¿Ÿæœ€å¤šé™ä½çº¦ 19.1%ï¼Œåœ¨ç²¾åº¦ä¸å»¶è¿Ÿé—´å®ç°æ›´ä¼˜æƒè¡¡ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å»¶è¿Ÿ - ç²¾åº¦æƒè¡¡æ€è·¯**ï¼šè·³å‡ºâ€œåªçœ‹ç®—åŠ›/ç²¾åº¦â€çš„æ€ç»´å®šå¼ï¼ŒæŠŠå»¶è¿Ÿä½œä¸ºæ ¸å¿ƒçº¦æŸï¼Œä¸ºå¤§æ¨¡å‹è½åœ°ä½å»¶è¿Ÿåœºæ™¯ï¼ˆå¦‚ä¸ªæ€§åŒ–äº¤äº’ï¼‰æä¾›æ–°æ€è·¯ï¼›  
2. **å¤šæ¨¡å‹åä½œèŒƒå¼**ï¼šç”¨â€œå°è‰ç¨¿æ¨¡å‹ + å¤§ç›®æ ‡æ¨¡å‹ + å¥–åŠ±æ¨¡å‹â€åˆ†å±‚åä½œï¼Œæ—¢åˆ©ç”¨å°æ¨¡å‹æé€Ÿï¼Œåˆé å¤§æ¨¡å‹ä¿ç²¾åº¦ï¼Œè¿˜å€Ÿå¥–åŠ±æ¨¡å‹åšçµæ´»é€‰æ‹©ï¼Œè¿™ç§â€œåˆ†å·¥â€æ¨¡å¼å¯è¿ç§»åˆ°å…¶ä»–éœ€å¹³è¡¡èµ„æºä¸æ•ˆæœçš„ä»»åŠ¡ï¼›  
3. **ç†è®º + å®éªŒåŒéªŒè¯**ï¼šä»ç†è®ºè¯æ˜æ”¶æ•›æ€§ï¼Œå†ç”¨çœŸå®æ•°æ®é›†éªŒè¯ï¼Œä¸ºæ–¹æ³•å¯é æ€§èƒŒä¹¦ï¼Œä¹Ÿç¤ºèŒƒäº†å­¦æœ¯ç ”ç©¶ä¸­â€œæ–¹æ³• - ç†è®º - å®éªŒâ€é—­ç¯çš„é‡è¦æ€§ã€‚  


SPECS ä¸ºå¤§æ¨¡å‹æ¨ç†çš„â€œå»¶è¿Ÿ - ç²¾åº¦â€éš¾é¢˜æä¾›äº†ä¸€å¥—å…¼å…·åˆ›æ–°æ€§ä¸å®ç”¨æ€§çš„è§£æ³•ï¼Œæ— è®ºæ˜¯å·¥ä¸šç•Œè½åœ°ä½å»¶è¿Ÿ LLM åº”ç”¨ï¼Œè¿˜æ˜¯å­¦æœ¯ç•Œæ¢ç´¢æµ‹è¯•æ—¶ä¼˜åŒ–æ–°æ–¹å‘ï¼Œéƒ½æœ‰ä¸å°‘å¯å€Ÿé‰´çš„é—ªå…‰ç‚¹~

## theoretical-tensions-in-rlhf--reconciling-empirical-success-with-inconsistencies-in-social-choice-theory
### Abstract
Despite its empirical success, Reinforcement Learning from Human Feedback
(RLHF) has been shown to violate almost all the fundamental axioms in social
choice theory -- such as majority consistency, pairwise majority consistency,
and Condorcet consistency. This raises a foundational question: why does RLHF
perform so well in practice if it fails these seemingly essential properties?
In this paper, we resolve this paradox by showing that under mild and
empirically plausible assumptions on the preference profile, RLHF does satisfy
pairwise majority and Condorcet consistency. These assumptions are frequently
satisfied in real-world alignment tasks, offering a theoretical explanation for
RLHF's strong practical performance. Furthermore, we show that a slight
modification to the reward modeling objective can ensure pairwise majority or
Condorcet consistency even under general preference profiles, thereby improving
the alignment process. Finally, we go beyond classical axioms in economic and
social choice theory and introduce new alignment criteria -- preference
matching, preference equivalence, and group preference matching -- that better
reflect the goal of learning distributions over responses. We show that while
RLHF satisfies the first two properties, it fails to satisfy the third. We
conclude by discussing how future alignment methods may be designed to satisfy
all three.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RLHFå®è·µæˆåŠŸèƒŒåï¼šè°ƒå’Œç¤¾ä¼šé€‰æ‹©ç†è®ºçŸ›ç›¾çš„ç†è®ºæ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯è®©æ¨¡å‹è¡Œä¸ºä¸äººç±»æœŸæœ›å¯¹é½çš„å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼ŒRLHFåœ¨ç†è®ºå±‚é¢å´ä¸ç¤¾ä¼šé€‰æ‹©ç†è®ºçš„è¯¸å¤šåŸºç¡€å…¬ç†ï¼ˆå¦‚å¤šæ•°ä¸€è‡´æ€§ã€æˆå¯¹å¤šæ•°ä¸€è‡´æ€§ã€å­”å¤šå¡ä¸€è‡´æ€§ï¼‰ç›¸è¿èƒŒã€‚è¿™å°±äº§ç”Ÿäº†ä¸€ä¸ªæ ¸å¿ƒç–‘é—®ï¼šRLHFåœ¨å®è·µä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯ä¸ºä½•åœ¨ç†è®ºä¸Šè¿èƒŒè¿™äº›çœ‹ä¼¼å…³é”®çš„å…¬ç†ï¼Ÿæœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³è¿™ä¸€çŸ›ç›¾ã€ä»ç†è®ºå±‚é¢è§£é‡ŠRLHFå®è·µæˆåŠŸåŸå› å¹¶æ¢ç´¢æ”¹è¿›æ–¹å‘è€Œå±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºRLHFæ»¡è¶³å…¬ç†çš„å‰ææ¡ä»¶  
åœ¨å¯¹åå¥½åˆ†å¸ƒåšæ¸©å’Œä¸”ç¬¦åˆå®è¯çš„å‡è®¾ä¸‹ï¼Œè¯æ˜RLHFèƒ½æ»¡è¶³æˆå¯¹å¤šæ•°ä¸€è‡´æ€§ä¸å­”å¤šå¡ä¸€è‡´æ€§ã€‚ç°å®ä¸­å¯¹é½ä»»åŠ¡é‡Œå¸¸è§çš„åå¥½æ•°æ®ç»“æ„ï¼ˆæ¯”å¦‚æ¯ä¸ªæ¯”è¾ƒæœ€å¤šç”±ä¸€ä¸ªæ ‡æ³¨è€…æ ‡æ³¨ã€å…è®¸å¾ªç¯åå¥½ç­‰æƒ…å†µæ„æˆçš„åå¥½åˆ†å¸ƒï¼‰ï¼Œè®©RLHFå¯ä»¥æ­£ç¡®è¯†åˆ«å­”å¤šå¡èƒœè€…ï¼ˆè‹¥å­˜åœ¨ï¼‰å¹¶èµ‹äºˆæœ€é«˜åˆ†æ•°ï¼Œä»ç†è®ºä¸Šè§£é‡Šäº†RLHFå®è·µæ•ˆæœå¥½çš„åŸå› ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¹è¿›å¥–åŠ±æ¨¡å‹ç›®æ ‡ä»¥ä¿éšœå…¬ç†æ»¡è¶³  
é’ˆå¯¹å¤šä¸ªæ ‡æ³¨è€…å‚ä¸åŒä¸€æ¯”è¾ƒçš„åœºæ™¯ï¼Œå¯¹å¥–åŠ±æ¨¡å‹ç›®æ ‡åšå¾®å°ä¿®æ”¹â€”â€”åŸºäºå¤šæ•°æŠ•ç¥¨èšåˆåå¥½å¹¶èµ‹äºˆäºŒå…ƒåå¥½ã€‚è¿™ç§ä¿®æ”¹è®©å¥–åŠ±æ¨¡å‹èƒ½æ»¡è¶³å¤šæ•°ä¸€è‡´æ€§ã€æˆå¯¹å¤šæ•°ä¸€è‡´æ€§å’Œå­”å¤šå¡ä¸€è‡´æ€§ï¼Œå…¶æœ¬è´¨æ˜¯è®©å¥–åŠ±å»ºæ¨¡éšå¼å®ç°äº†ç¤¾ä¼šé€‰æ‹©ç†è®ºä¸­æ»¡è¶³ä¼˜è‰¯å…¬ç†çš„Copelandè§„åˆ™ï¼Œä¸ºç°æœ‰å®è·µæä¾›ç†è®ºæ”¯æ’‘åŒæ—¶ï¼Œä¹Ÿä¸ºæœªæ¥å¯¹é½ç­–ç•¥æŒ‡æ˜æ–¹å‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºé€‚é…LLMå¯¹é½çš„æ–°å…¬ç†  
çªç ´ç»æµä¸ç¤¾ä¼šé€‰æ‹©ç†è®ºçš„ç»å…¸å…¬ç†æ¡†æ¶ï¼Œæå‡ºåå¥½åŒ¹é…ã€åå¥½ç­‰ä»·ã€ç¾¤ä½“åå¥½åŒ¹é…è¿™ä¸‰ä¸ªæ›´è´´åˆç”Ÿæˆæ¨¡å‹å­¦ä¹ å“åº”åˆ†å¸ƒç›®æ ‡çš„æ–°å¯¹é½å‡†åˆ™ã€‚è¯æ˜ç›®æ ‡åˆ†å¸ƒæ˜¯å®šä¹‰è‰¯å¥½ã€å­˜åœ¨ä¸”å”¯ä¸€çš„ï¼Œè¿˜åˆ†æå‡ºRLHFæ»¡è¶³å‰ä¸¤ä¸ªå‡†åˆ™ä½†ä¸æ»¡è¶³ç¬¬ä¸‰ä¸ªï¼Œä¸ºæœªæ¥æ–¹æ³•å¦‚ä½•æ»¡è¶³å…¨éƒ¨å‡†åˆ™æä¾›æ€è·¯ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªå¼€å±•ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®éªŒéªŒè¯ï¼ˆä¾§é‡ç†è®ºåˆ†ææ¨å¯¼ï¼‰ï¼Œé€šè¿‡ä¸¥è°¨çš„ç†è®ºæ¨å¯¼ä¸è®ºè¯ï¼Œé˜æ˜äº†åœ¨ç‰¹å®šåå¥½åˆ†å¸ƒå‡è®¾ä¸‹RLHFæ»¡è¶³ç»å…¸ç¤¾ä¼šé€‰æ‹©å…¬ç†çš„æƒ…å†µã€ä¿®æ”¹å¥–åŠ±æ¨¡å‹ç›®æ ‡åçš„å…¬ç†æ»¡è¶³æ€§ï¼Œä»¥åŠæ–°æå‡ºçš„ä¸‰ä¸ªåˆ†å¸ƒå±‚é¢å…¬ç†ä¸RLHFçš„å¥‘åˆæƒ…å†µç­‰å…³é”®ç»“è®ºï¼Œä»ç†è®ºè§’åº¦æ”¯æ’‘äº†å„åˆ›æ–°ç‚¹çš„åˆç†æ€§ä¸ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç†è®ºè§£é‡Šå®è·µï¼šä¸ºRLHFåœ¨å®è·µä¸­è¡¨ç°å¥½å´è¿èƒŒç»å…¸å…¬ç†è¿™ä¸€çŸ›ç›¾æä¾›äº†ç†è®ºå±‚é¢çš„è§£é‡Šï¼Œè®©ä»ä¸šè€…ç†è§£å…¶æˆåŠŸèƒŒåçš„åå¥½åˆ†å¸ƒå‡è®¾ç­‰å…³é”®å› ç´ ï¼Œåœ¨å®é™…æ„å»ºåå¥½æ•°æ®ã€è®¾è®¡å¯¹é½æµç¨‹æ—¶æ›´æœ‰ç†è®ºä¾æ®ã€‚  
2. æ”¹è¿›å¥–åŠ±æ¨¡å‹æ€è·¯ï¼šæå‡ºçš„å¥–åŠ±æ¨¡å‹ç›®æ ‡ä¿®æ”¹æ–¹å¼ï¼Œä¸ºå¤„ç†å¤šæ ‡æ³¨è€…åœºæ™¯ä¸‹çš„åå¥½èšåˆæä¾›äº†æ›´ä¼˜è·¯å¾„ï¼Œèƒ½æŒ‡å¯¼åç»­ä¼˜åŒ–å¥–åŠ±å»ºæ¨¡ç¯èŠ‚ä»¥ä¿éšœå¯¹é½è´¨é‡ä¸å…¬ç†æ»¡è¶³æ€§ã€‚  
3. æ–°å…¬ç†æ‹“å±•æ–¹å‘ï¼šæ–°æå‡ºçš„ä¸‰ä¸ªé€‚é…LLMå¯¹é½çš„å…¬ç†ï¼Œä¸ºè¯¥é¢†åŸŸåç»­è¯„ä¼°æ–¹æ³•ã€è®¾è®¡æ–°å¯¹é½ç®—æ³•æä¾›äº†å…¨æ–°çš„è¡¡é‡ç»´åº¦ä¸ç›®æ ‡å¯¼å‘ï¼Œæ¨åŠ¨é¢†åŸŸä»ç»å…¸å…¬ç†æ¡†æ¶å‘æ›´è´´åˆç”Ÿæˆæ¨¡å‹ç‰¹æ€§çš„æ–¹å‘å‘å±•ã€‚

## treerl--llm-reinforcement-learning-with-on-policy-tree-search
### Abstract
Reinforcement learning (RL) with tree search has demonstrated superior
performance in traditional reasoning tasks. Compared to conventional
independent chain sampling strategies with outcome supervision, tree search
enables better exploration of the reasoning space and provides dense, on-policy
process rewards during RL training but remains under-explored in On-Policy LLM
RL. We propose TreeRL, a reinforcement learning framework that directly
incorporates on-policy tree search for RL training. Our approach includes
intermediate supervision and eliminates the need for a separate reward model
training. Existing approaches typically train a separate process reward model,
which can suffer from distribution mismatch and reward hacking. We also
introduce a cost-effective tree search approach that achieves higher search
efficiency under the same generation token budget by strategically branching
from high-uncertainty intermediate steps rather than using random branching.
Experiments on challenging math and code reasoning benchmarks demonstrate that
TreeRL achieves superior performance compared to traditional ChainRL,
highlighting the potential of tree search for LLM. TreeRL is open-sourced at
https://github.com/THUDM/TreeRL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TreeRLï¼šåŸºäºOn - Policyæ ‘æœç´¢çš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯æå‡å…¶æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚å½“å‰LLMçš„RLæ–¹æ³•å¤šç‹¬ç«‹é‡‡æ ·è½¨è¿¹å¹¶åŸºäºæœ€ç»ˆç­”æ¡ˆè·å–å¥–åŠ±ï¼Œè€Œåœ¨å…¶ä»–é¢†åŸŸæˆåŠŸçš„æ ‘æœç´¢åœ¨LLMæ¨ç†çš„å¼ºåŒ–å­¦ä¹ ä¸­å‘å±•ä¸è¶³ã€‚ä¸€æ–¹é¢ï¼Œç»å…¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰åœ¨ç›¸åŒæ¨ç†æˆæœ¬ä¸‹æ•ˆæœå’Œæ•ˆç‡ä¸å¦‚ç‹¬ç«‹é‡‡æ ·å¤šå“åº”ï¼›å¦ä¸€æ–¹é¢ï¼Œæ ‘æœç´¢è™½èƒ½æä¾›ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£ï¼Œä½†ç¦»çº¿è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¯¹RLè®­ç»ƒæ€§èƒ½æå‡è´¡çŒ®å°ã€‚å› æ­¤ï¼Œæ¢ç´¢ç»“åˆæ ‘æœç´¢çš„On - Policy RLè®­ç»ƒä»¥æå‡LLMæ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºTreeRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶
TreeRLæ˜¯ä¸€ç§å°†On - Policyæ ‘æœç´¢ç›´æ¥çº³å…¥RLè®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸­é—´ç›‘ç£ï¼Œæ— éœ€å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•ä¸­å•ç‹¬è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¯èƒ½å‡ºç°çš„åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±é»‘å®¢é—®é¢˜ã€‚é€šè¿‡æ ‘æœç´¢ä¸ºRLè®­ç»ƒæä¾›å¯†é›†çš„ã€On - Policyçš„è¿‡ç¨‹å¥–åŠ±ï¼Œæ›´å¥½åœ°æ¢ç´¢æ¨ç†ç©ºé—´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºé«˜æ•ˆæ ‘æœç´¢ç­–ç•¥EPTree
ä¸åŒäºMCTSå°†ç­”æ¡ˆåˆ†è§£ä¸ºå°éƒ¨åˆ†è®©æ¨¡å‹é€æ­¥æ¢ç´¢ï¼ŒEPTreeåŸºäºç†µä»ç°æœ‰æ ‘ä¸­æœ€ä¸ç¡®å®šçš„ä¸­é—´tokenåˆ†å‰ç”Ÿæˆæ–°å“åº”ï¼Œç›´åˆ°å¾—åˆ°æœ€ç»ˆç­”æ¡ˆã€‚è¿™ç§æ–¹å¼åœ¨ç›¸åŒç”Ÿæˆtokené¢„ç®—ä¸‹ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§ä¸­é—´æ­¥éª¤ç­–ç•¥æ€§åˆ†æ”¯è€Œééšæœºåˆ†æ”¯ï¼Œå®ç°æ›´é«˜æœç´¢æ•ˆç‡ï¼Œä¸”é€šå¸¸åªéœ€çº¦ä¸¤æ¬¡è¿­ä»£å°±èƒ½å½¢æˆç”Ÿæˆæ ‘ï¼Œèƒ½ç”Ÿæˆæ›´å¤šæ ·æœ‰æ•ˆçš„å“åº”ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºæ ‘æœç´¢çš„è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ 
åœ¨æ ‘çš„æ¯ä¸ªæ­¥éª¤åŸºäºä¼˜åŠ¿åˆ†é…ä¿¡ç”¨ï¼Œè®¡ç®—ç»™å®šæ¨ç†æ­¥éª¤çš„è¿‡ç¨‹ä¿¡å·ä¸ºå…¨å±€ä¼˜åŠ¿å’Œå±€éƒ¨ä¼˜åŠ¿çš„åŠ æƒå’Œã€‚å…¨å±€ä¼˜åŠ¿åæ˜ è¯¥æ­¥éª¤å¯¹é—®é¢˜æ•´ä½“æ­£ç¡®ç‡çš„æ½œåŠ›ï¼Œå±€éƒ¨ä¼˜åŠ¿é‡åŒ–è¯¥æ­¥éª¤ä¸å…¶åœ¨æ ‘ä¸­çˆ¶æ­¥éª¤ç›¸æ¯”çš„æ”¹è¿›ã€‚è¿™äº›ä¼˜åŠ¿ä¿¡å·ç›´æ¥ä»On - Policyç”Ÿæˆçš„æ ‘ä¸­æ¨å¯¼ï¼Œèƒ½æŠµæŠ—å¥–åŠ±é»‘å®¢ä¸”ä¸ä¾èµ–é¢å¤–å¥–åŠ±æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆåŸºäºQwenå’ŒGLMæ¨¡å‹ï¼‰ä¸Šè¯„ä¼°TreeRLï¼Œç»“æœè¡¨æ˜TreeRLæ¯”ä¼ ç»ŸChainRLæ€§èƒ½æ›´ä¼˜ï¼Œå±•ç°å‡ºæ ‘æœç´¢å¯¹LLMçš„æ½œåŠ›ã€‚EPTreeåœ¨ä¸åŒæ¨ç†é¢„ç®—ä¸‹æŒç»­ä¼˜äºç‹¬ç«‹åŒåˆ†å¸ƒå¤šé“¾é‡‡æ ·å’ŒMCTSï¼ŒTreeRLç»“åˆEPTreeæ¯”é‡‡ç”¨ç‹¬ç«‹åŒåˆ†å¸ƒå¤šé“¾é‡‡æ ·çš„ChainRLè¡¨ç°æ›´å¥½ï¼Œæ€§èƒ½æå‡å—ç›ŠäºEPTreeè‰¯å¥½çš„PassRateè¡¨ç°å’Œè¿‡ç¨‹ç›‘ç£ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ ‘æœç´¢ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆçš„æ€è·¯ï¼šä¸ºæå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†æ–°æ–¹å‘ï¼Œä¸å†å±€é™äºä¼ ç»Ÿçš„ç‹¬ç«‹è½¨è¿¹é‡‡æ ·å’Œä»…åŸºäºæœ€ç»ˆç»“æœçš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨æ ‘æœç´¢æ›´å¥½æ¢ç´¢æ¨ç†ç©ºé—´ã€‚
2. é«˜æ•ˆæ ‘æœç´¢ç­–ç•¥è®¾è®¡ï¼šEPTreeåŸºäºç†µçš„é«˜ä¸ç¡®å®šæ€§ä¸­é—´æ­¥éª¤åˆ†æ”¯æ–¹å¼ï¼Œä¸ºåœ¨æœ‰é™tokené¢„ç®—ä¸‹æå‡æœç´¢æ•ˆç‡æä¾›äº†å‚è€ƒï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦é«˜æ•ˆæ¢ç´¢çš„ç”Ÿæˆå¼ä»»åŠ¡åœºæ™¯ã€‚
3. è¿‡ç¨‹ç›‘ç£ä¿¡å·æ„å»ºï¼šåŸºäºæ ‘ä¸­æ­¥éª¤ä¼˜åŠ¿è®¡ç®—è¿‡ç¨‹ä¿¡å·ï¼Œæ— éœ€é¢å¤–å¥–åŠ±æ¨¡å‹ä¸”æŠ—å¥–åŠ±é»‘å®¢çš„æ–¹å¼ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±ä¿¡å·è®¾è®¡æä¾›äº†åˆ›æ–°æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°éœ€è¦ç»†ç²’åº¦ç›‘ç£çš„RLä»»åŠ¡ä¸­ã€‚

## agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards
### Abstract
Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too sparse for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-RLVRï¼šè®©å¤§æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­â€œæ‹œå¸ˆå­¦è‰ºâ€çš„RLæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦ã€ç«èµ›ç¼–ç¨‹ç­‰å¯éªŒè¯é¢†åŸŸæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ™ºèƒ½ä½“ç¯å¢ƒï¼ˆå¤šæ­¥éª¤ã€å¤æ‚é—®é¢˜æ±‚è§£åœºæ™¯ï¼‰ä¸­æ•ˆæœéª¤é™ã€‚è¿™ç±»åœºæ™¯å¥–åŠ±ç¨€ç–ï¼Œå‰æ²¿LLMä¹Ÿæ˜“é«˜å¤±è´¥ç‡ï¼Œä¼ ç»ŸRLVRéš¾ä»¥æœ‰æ•ˆè®­ç»ƒã€‚åŒæ—¶ï¼Œæ™ºèƒ½ä½“ç¯å¢ƒéœ€å¤šè½®æ¨ç†ã€ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ï¼Œè®­ç»ƒå¤æ‚åº¦é«˜ï¼Œä¸ºè®©RLVRåœ¨å¤æ‚çœŸå®åœºæ™¯ï¼ˆå¦‚è½¯ä»¶å·¥ç¨‹ï¼‰ç”Ÿæ•ˆï¼Œå‚¬ç”Ÿäº†Agent - RLVRæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAgent - RLVRæ¡†æ¶é€‚é…æ™ºèƒ½ä½“åœºæ™¯  
å€Ÿé‰´äººç±»æ•™å­¦æ³•å¼•å…¥â€œagent guidanceï¼ˆæ™ºèƒ½ä½“æŒ‡å¯¼ï¼‰â€æœºåˆ¶ï¼Œåˆ©ç”¨ä»é«˜å±‚æˆ˜ç•¥è§„åˆ’åˆ°é”™è¯¯ä¸ç¯å¢ƒäº¤äº’åŠ¨æ€åé¦ˆç­‰å¤šæ ·ä¿¡æ¯çº¿ç´¢ï¼Œå¼•å¯¼æ™ºèƒ½ä½“èµ°å‘æˆåŠŸè½¨è¿¹ï¼Œåƒè€å¸ˆæŒ‡å¯¼æ–°äººä¸€æ ·å¸®æ™ºèƒ½ä½“åœ¨å¤æ‚è§£ç©ºé—´å¯¼èˆªï¼Œè¿˜èƒ½é€šè¿‡ç¯å¢ƒæ¢ç´¢ä¿ƒè¿›è‡ªæˆ‘æå‡ã€‚è®­ç»ƒå¾ªç¯åˆ†ä¸‰æ­¥ï¼šå…ˆè®©æ™ºèƒ½ä½“æ— æŒ‡å¯¼å°è¯•ç”Ÿæˆåˆå§‹è½¨è¿¹ï¼Œç”¨å•å…ƒæµ‹è¯•éªŒè¯å¹¶è¡¥å……æŒ‡å¯¼ï¼›å†è®©æ™ºèƒ½ä½“å¸¦æŒ‡å¯¼é‡è¯•ï¼›æœ€ååŸºäºæŒ‡å¯¼åè½¨è¿¹å¥–åŠ±ç”¨RLVRæ›´æ–°ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºè½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸“å±æ•°æ®é›†  
ç²¾å¿ƒæ•´ç†å«817ä¸ªè®­ç»ƒç¯å¢ƒçš„æ•°æ®é›†ï¼Œæ¶µç›–é—®é¢˜é™ˆè¿°ã€ç¯å¢ƒå’ŒæŒ‡å¯¼ä¿¡æ¯ï¼Œè¶…è¶Šä¼ ç»Ÿè¾“å…¥ - è¾“å‡ºå¯¹ï¼Œæ•æ‰å¸¦é›†æˆæŒ‡å¯¼ä¿¡å·çš„å®Œæ•´ç¼–ç ç¯å¢ƒï¼Œä¸ºè®­ç»ƒè½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“æä¾›ä¸°å¯Œèµ„æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨SWE - Bench VerifiedåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgent - RLVRå°†Qwen - 2.5 - 72B - Instructçš„pass@1æ€§èƒ½ä»9.4%æå‡è‡³22.4%ï¼›æŒ‡å¯¼å¢å¼ºçš„RLVRæ•°æ®ç”¨äºæµ‹è¯•æ—¶å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œèƒ½è¿›ä¸€æ­¥æŠŠpass@1æ¨è‡³27.8%ï¼›æŒ‡å¯¼æ¨¡å‹åœ¨pass@1ï¼ˆ19.8%â†’22.4%ï¼‰å’Œpass@32ï¼ˆ34.2%â†’38.4%ï¼‰ä¸Šéƒ½æœ‰æå‡ï¼ŒéªŒè¯æŒ‡å¯¼æ˜¯å…³é”®ç»„ä»¶ï¼Œä¹Ÿä½“ç°æ–¹æ³•åœ¨å°æ•°æ®é›†ä¸‹æå‡æ™ºèƒ½ä½“å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åº”å¯¹ç¨€ç–å¥–åŠ±åœºæ™¯æ—¶ï¼Œå¼•å…¥ç±»äººç±»æ•™å­¦çš„æŒ‡å¯¼æœºåˆ¶æ˜¯æœ‰æ•ˆæ€è·¯ï¼Œä¸ºå¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­æ¨¡å‹è®­ç»ƒæä¾›æ–°èŒƒå¼å‚è€ƒã€‚
2. æ„å»ºé¢†åŸŸä¸“å±ã€å«ä¸°å¯Œç¯å¢ƒä¸æŒ‡å¯¼ä¿¡æ¯çš„æ•°æ®é›†ï¼Œèƒ½ä¸ºç‰¹å®šé¢†åŸŸæ™ºèƒ½ä½“è®­ç»ƒç­‘ç‰¢æ•°æ®åŸºç¡€ï¼Œè¿™ç§â€œå®šåˆ¶åŒ– + åœºæ™¯åŒ–â€æ•°æ®æ„å»ºæ€ç»´å€¼å¾—å€Ÿé‰´ã€‚
3. å±•ç¤ºäº†RLVRæ•°æ®åœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒç­‰æ–¹é¢çš„é¢å¤–ä»·å€¼ï¼Œå¯å‘åç»­æ¢ç´¢ä¸åŒæ¨¡å—é—´æ•°æ®å¤ç”¨ä¸ååŒå¢æ•ˆï¼Œæ‹“å±•æŠ€æœ¯åº”ç”¨è¾¹ç•Œã€‚

## reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning
### Abstract
This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°æ¨¡å‹ä¹Ÿèƒ½æ‰“ï¼å¼ºåŒ–å­¦ä¹ å¾®è°ƒè®©è½»é‡çº§è¯­è¨€æ¨¡å‹ç©è½¬æŒ‡ä»¤éµå¾ªä¸æ•°å­¦æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆé¢†åŸŸå–å¾—äº†äº®çœ¼æˆæœï¼Œä½†å¦‚ä½•è®©å°è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªã€æ•°å­¦æ¨ç†ç­‰ä¸åŒæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ä»æ˜¯éš¾é¢˜ã€‚åŒæ—¶ï¼Œä¸åŒå¾®è°ƒæŠ€æœ¯ï¼ˆå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ ç±»æŠ€æœ¯ï¼‰é—´çš„æ€§èƒ½å¯¹æ¯”ä¹Ÿæœ‰å¾…æ·±å…¥æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶èšç„¦äºè½»é‡çº§å¼€æºæ¨¡å‹Qwen2.5 - 0.5B Baseï¼Œæ¢ç´¢åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•åœ¨åå¥½å¯¹é½ä¸ç‰¹å®šé¢†åŸŸé€‚é…æ–¹é¢çš„è¡¨ç°ï¼Œä»¥æ˜ç¡®è½»é‡çº§æ¨¡å‹åœ¨ç±»äººå¯¹é½å­¦ä¹ åœºæ™¯ä¸‹çš„èƒ½åŠ›ä¸å±€é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šå¾®è°ƒæŠ€æœ¯å¯¹æ¯”ä¸RLOO reward modelæ¢ç´¢  
å¯¹æ¯”äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€Reinforce Leave - One - Outï¼ˆRLOOï¼‰ä¸‰ç§å¾®è°ƒæŠ€æœ¯ã€‚åœ¨RLOOä¸­ï¼Œè¿˜è¯„ä¼°äº†DeBERTaã€DistilBERTã€Siamese DistilBERTç­‰ä¸åŒå¥–åŠ±æ¨¡å‹ï¼Œä»¥æ­¤æ¢ç©¶å¥–åŠ±æ¨¡å‹å¯¹æœ€ç»ˆç­–ç•¥æ€§èƒ½çš„å½±å“ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡çš„æ•°æ®å¢å¼ºä¸æ¨ç†å·¥å…·ç»“åˆ  
ä¸ºæå‡æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŸºäºCountdownæ•°æ®é›†æ„é€ å«1600ä¸ªæ ·æœ¬çš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼ˆå€ŸåŠ©GPT - 4oå®Œæˆé—®é¢˜ç”Ÿæˆä¸ç­”æ¡ˆéªŒè¯ï¼‰ï¼›åŒæ—¶é‡‡ç”¨best - of - Né‡‡æ ·ç­–ç•¥ï¼Œç»“åˆå¤–éƒ¨éªŒè¯å™¨æ¥æå‡æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ä¸æ­£ç¡®æ€§ï¼Œæ¢ç´¢å¾®è°ƒä¸æ¨ç†æ—¶å·¥å…·ç»“åˆçš„æ½œåŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šï¼ŒDPOç›¸æ¯”SFTåœ¨å…¨å‚æ•°ä¸LoRAé…ç½®ä¸‹å‡èƒ½è¿›ä¸€æ­¥æå‡æ•ˆæœï¼›RLOOå˜ä½“é‡Œï¼Œä»¥DeBERTaä¸ºå¥–åŠ±æ¨¡å‹çš„ç‰ˆæœ¬å¯¹é½åˆ†æ•°æœ€é«˜ã€‚æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œåˆæˆæ•°æ®èƒ½å°å¹…æå‡æ€§èƒ½ï¼Œè€Œç»“åˆå¤–éƒ¨éªŒè¯å™¨çš„best - of - Né‡‡æ ·å¸¦æ¥æ˜¾è‘—å¢ç›Šï¼Œå‡†ç¡®ç‡è¶…0.81ï¼Œæ˜¯SFTçš„ä¸¤å€å¤šã€‚æ•´ä½“è¡¨æ˜è½»é‡çº§æ¨¡å‹ç»æœ‰æ•ˆå¾®è°ƒä¸å·¥å…·è¾…åŠ©å¯å®ç°ä¸é”™æ€§èƒ½ï¼Œå¥–åŠ±æ¨¡å‹è´¨é‡ã€é‡‡æ ·å“åº”å¤šæ ·æ€§å¯¹RLOOå¾ˆå…³é”®ï¼Œå¤–éƒ¨éªŒè¯å™¨ + best - of - Né‡‡æ ·ä¸ºæ•°å­¦æ¨ç†æå‡†ç¡®ç‡æä¾›äº†ä½æˆæœ¬æ–¹æ¡ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºæƒ³ä¼˜åŒ–å°æ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„ç ”ç©¶è€…ä¸å¼€å‘è€…ï¼Œå¯å€Ÿé‰´å¤šå¼ºåŒ–å­¦ä¹ å¾®è°ƒæŠ€æœ¯å¯¹æ¯”æ€è·¯ï¼Œæ˜ç¡®ä¸åŒæŠ€æœ¯åœ¨åå¥½å¯¹é½ç­‰åœºæ™¯çš„ä¼˜åŠ£ï¼›åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰ä»»åŠ¡ä¸­ï¼Œå°è¯•åˆæˆæ•°æ®å¢å¼ºä¸æ¨ç†æ—¶å·¥å…·ï¼ˆå¦‚å¤–éƒ¨éªŒè¯å™¨ + best - of - Né‡‡æ ·ï¼‰ç»“åˆçš„æ–¹å¼ï¼Œåœ¨è®¡ç®—èµ„æºå—é™ä¸‹æå‡å°æ¨¡å‹è¡¨ç°ï¼›åŒæ—¶é‡è§†å¥–åŠ±æ¨¡å‹é€‰å‹ä¸é‡‡æ ·å¤šæ ·æ€§ç­‰å› ç´ å¯¹RLOOç±»æ–¹æ³•çš„å½±å“ï¼Œä¸ºè½»é‡çº§è¯­è¨€æ¨¡å‹é€‚é…å¤šä»»åŠ¡æä¾›å®è·µå‚è€ƒã€‚

## unsupervised-elicitation-of-language-models
### Abstract
To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— ç›‘ç£æ¿€å‘è¯­è¨€æ¨¡å‹æ½œåŠ›ï¼šICMç®—æ³•çªç ´äººç±»ç›‘ç£é™åˆ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“ä¸‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„åè®­ç»ƒèŒƒå¼ï¼Œä¾æ—§ä¾èµ–äººç±»æ¥æŒ‡å®šæœŸæœ›è¡Œä¸ºï¼Œåƒé€šè¿‡æ¼”ç¤ºæˆ–è€…åå¥½åé¦ˆç­‰æ–¹å¼ã€‚ç„¶è€Œï¼Œéšç€ä»»åŠ¡å’Œæ¨¡å‹è¡Œä¸ºæ„ˆå‘å¤æ‚ï¼Œäººç±»ç›‘ç£å˜å¾—è¶Šæ¥è¶Šä¸å¯é ï¼Œè¯­è¨€æ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ æ¨¡ä»¿æ¼”ç¤ºé‡Œçš„é”™è¯¯ï¼Œæˆ–è€…åˆ©ç”¨åé¦ˆä¸­çš„ç¼ºé™·ã€‚å¹¶ä¸”ï¼Œå¯¹äºå…·å¤‡è¶…äººç±»èƒ½åŠ›çš„æ¨¡å‹ï¼Œè·å–é«˜è´¨é‡çš„äººç±»ç›‘ç£å­˜åœ¨å›°éš¾ç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®­ç»ƒè¯­è¨€æ¨¡å‹å»å®Œæˆé‚£äº›äººç±»éš¾ä»¥å¯é æ¼”ç¤ºæˆ–è¯„ä¼°çš„ä»»åŠ¡ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ï¼Œæœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³è¯¥æŒ‘æˆ˜è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ— ç›‘ç£ç®—æ³•ICM
å¼•å…¥Internal Coherence Maximizationï¼ˆICMï¼‰è¿™ä¸€æ— ç›‘ç£ç®—æ³•ï¼Œåœ¨æ²¡æœ‰å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ ‡ç­¾å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å…¶ç›®æ ‡æ˜¯åœ¨ç»™å®šç”±å¸¦æ ‡ç­¾è¾“å…¥æŒ‡å®šçš„ä»»åŠ¡æ—¶ï¼Œè®©é¢„è®­ç»ƒæ¨¡å‹åŸºäºè‡ªèº«ç”Ÿæˆçš„æ ‡ç­¾åœ¨è¯¥ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•æä¾›çš„å¤–éƒ¨æ ‡ç­¾ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ scoring function è¡¡é‡æ ‡ç­¾è´¨é‡
ç”¨ç”±ä¸¤éƒ¨åˆ†ç»„æˆçš„è¯„åˆ†å‡½æ•°è¡¡é‡æ¨¡å‹ç”Ÿæˆæ ‡ç­¾é›†çš„è´¨é‡ï¼Œä¸€æ˜¯â€œç›¸äº’å¯é¢„æµ‹æ€§ï¼ˆMutual Predictabilityï¼‰â€ï¼Œè®¡ç®—æ¨¡å‹åœ¨ä»¥æ‰€æœ‰å…¶ä»–æ ‡ç­¾ä¸ºæ¡ä»¶æ—¶æ¨æ–­æ¯ä¸ªæ ‡ç­¾çš„å¯èƒ½æ€§ï¼Œå°†æ‰€æœ‰ç¤ºä¾‹çš„å¯¹æ•°æ¦‚ç‡æ±‚å’Œï¼›äºŒæ˜¯â€œé€»è¾‘ä¸€è‡´æ€§ï¼ˆLogical Consistencyï¼‰â€ï¼Œé€šè¿‡é€»è¾‘ä¸€è‡´æ€§å‡½æ•°æ£€æŸ¥æ ‡ç­¾é›†é‡Œæ•°æ®ç‚¹çš„æ ‡ç­¾ä¹‹é—´æ˜¯å¦é€»è¾‘ä¸€è‡´ï¼Œä»¥æ­¤è¡¡é‡æ ‡ç­¾ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚æœ€ç»ˆç»“åˆè¿™ä¸¤ä¸ªéƒ¨åˆ†å¾—åˆ°æ•´ä½“è¯„åˆ†å‡½æ•° \( U(D) = \alpha Â· P_Î¸(D) âˆ’ I(D) \) ï¼Œå…¶ä¸­ \( \alpha \) æ˜¯å¹³è¡¡ç›¸äº’å¯é¢„æµ‹æ€§å’Œé€»è¾‘ä¸€è‡´æ€§å¼ºåº¦çš„è¶…å‚æ•°ã€‚ 
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡æ‹Ÿé€€ç«å¯å‘çš„è¿‘ä¼¼æœç´¢ç®—æ³•
ç”±äºæ‰¾åˆ°æœ€å¤§åŒ–è¯„åˆ†å‡½æ•°çš„æœ€ä¼˜æ ‡ç­¾é›†åœ¨è®¡ç®—ä¸Šä¸å¯è¡Œï¼ˆç°å®æ•°æ®é›†è§„æ¨¡ä¸‹ï¼‰ï¼ŒICMé‡‡ç”¨å—æ¨¡æ‹Ÿé€€ç«å¯å‘çš„é«˜æ•ˆè¿‘ä¼¼ç®—æ³•ã€‚ä»ç©ºçš„æ ‡è®°é›†å¼€å§‹ï¼Œç”¨Kä¸ªéšæœºæ ‡è®°çš„ç¤ºä¾‹åˆå§‹åŒ–æœç´¢è¿‡ç¨‹ï¼Œç„¶åè¿­ä»£æ·»åŠ æ ‡ç­¾ï¼Œæ¯æ¬¡æ·»åŠ æ ‡ç­¾æ—¶æ‰§è¡Œé‡‡æ ·æ–°ç¤ºä¾‹ã€ç¡®å®šæ ‡ç­¾åŒæ—¶ä¿®å¤å¼•å…¥çš„ä¸ä¸€è‡´ã€åŸºäºè¯„åˆ†å‡½æ•°å†³å®šæ˜¯å¦æ¥å—æ–°æ ‡ç­¾è¿™ä¸‰ä¸ªæ­¥éª¤ï¼Œä»¥æ­¤å¢é‡å¼æ‰©å±•æ ‡ç­¾é›†å¹¶æé«˜åˆ†æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GSM8k - verificationã€TruthfulQAå’ŒAlpaca reward modelingä»»åŠ¡ä¸Šï¼ŒICMæ–¹æ³•åŒ¹é…äº†åŸºäºâ€œé»„é‡‘ç›‘ç£ï¼ˆgolden supervisionï¼‰â€è®­ç»ƒçš„æ€§èƒ½ï¼Œä¸”è¶…è¿‡äº†åŸºäºä¼—åŒ…äººç±»ç›‘ç£è®­ç»ƒçš„æ€§èƒ½ï¼›åœ¨è¯­è¨€æ¨¡å‹èƒ½åŠ›è¿œè¶…äººç±»çš„ä»»åŠ¡ï¼ˆå¦‚ä»å†™ä½œæ ·æœ¬è¯†åˆ«ä½œè€…æ€§åˆ«ï¼‰ä¸Šï¼ŒICMæ¯”åŸºäºäººç±»æ ‡ç­¾çš„è®­ç»ƒèƒ½æ˜¾è‘—æ›´å¥½åœ°æ¿€å‘æ¨¡å‹èƒ½åŠ›ï¼›åœ¨å‰æ²¿æ¨¡å‹è®­ç»ƒæ–¹é¢ï¼Œç”¨ICMè®­ç»ƒæ— ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŸºäºClaude 3.5 Haikuçš„åŠ©æ‰‹ï¼Œç»“æœæ˜¾ç¤ºæ— ç›‘ç£å¥–åŠ±æ¨¡å‹åœ¨Rewardbenchè¯„ä¼°ä¸­è¶…è¿‡åŸºäºç”Ÿäº§çº§é«˜è´¨é‡äººç±»ç›‘ç£è®­ç»ƒçš„å¯¹åº”æ¨¡å‹ï¼Œä¸”æ— ç›‘ç£åŠ©æ‰‹ç­–ç•¥åœ¨ä¸åŸºäºäººç±»ç›‘ç£å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„ç­–ç•¥å¤´å¯¹å¤´æ¯”è¾ƒä¸­ï¼Œèµ¢å¾—60%çš„å¯¹æ¯”ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ICMç®—æ³•ä¸ºçªç ´äººç±»ç›‘ç£é™åˆ¶è®­ç»ƒè¯­è¨€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ï¼Œè¯æ˜åœ¨ç°å®ç”Ÿäº§è§„æ¨¡åœºæ™¯ä¸­æ— ç›‘ç£æ¿€å‘èƒ½è¶…è¶Šäººç±»ç›‘ç£ï¼Œä¸ºåè®­ç»ƒå‰æ²¿æ¨¡å‹æˆä¸ºé€šç”¨åŠ©æ‰‹æä¾›äº†å®ç”¨æ–¹æ³•ï¼›å…¶è®¾è®¡çš„è¡¡é‡æ ‡ç­¾è´¨é‡çš„è¯„åˆ†å‡½æ•°æ€è·¯ï¼Œä»¥åŠå—æ¨¡æ‹Ÿé€€ç«å¯å‘çš„è¿‘ä¼¼æœç´¢ç®—æ³•ï¼Œåœ¨å¤„ç†éœ€æ¨¡å‹è‡ªèº«ç”Ÿæˆæ ‡ç­¾ä¼˜åŒ–ä»»åŠ¡ã€è§£å†³è®¡ç®—ä¸å¯è¡Œçš„ä¼˜åŒ–é—®é¢˜ç­‰åœºæ™¯ä¸­ï¼Œéƒ½æœ‰ä¸€å®šçš„å€Ÿé‰´æ„ä¹‰ï¼Œä¸ºåç»­ç›¸å…³ç ”ç©¶åœ¨æ–¹æ³•è®¾è®¡å’Œç®—æ³•é€‰æ‹©ä¸Šæä¾›äº†å‚è€ƒæ–¹å‘ã€‚

## learning-to-reason-across-parallel-samples-for-llm-reasoning
### Abstract
Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.
### ğŸŒŸ è®ºæ–‡è§£è¯» | èåˆå¹¶è¡Œä¸é¡ºåºæ¨ç†ï¼ŒSSAè®©å¤§æ¨¡å‹æ¨ç†æ›´é«˜æ•ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šèƒ½åŠ›ä¸æ–­æå‡ï¼Œè€Œæµ‹è¯•æ—¶è®¡ç®—èµ„æºçš„åˆ†é…ï¼ˆå³æµ‹è¯•æ—¶ç¼©æ”¾ï¼‰æ˜¯ä¼˜åŒ–æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹å‘ã€‚ç°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åˆ†å¹¶è¡Œå’Œé¡ºåºä¸¤ç±»ï¼šå¹¶è¡Œç¼©æ”¾æ˜¯ç‹¬ç«‹ç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„å†èšåˆï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨ï¼‰ï¼›é¡ºåºç¼©æ”¾åˆ™è¿­ä»£ä¼˜åŒ–å•ä¸ªè§£ï¼ˆå¦‚åŸºäºæç¤ºçš„è‡ªæˆ‘åæ€ï¼‰ã€‚ä½†å¹¶è¡Œæ–¹æ³•å¸¸å­¤ç«‹çœ‹å¾…æ ·æœ¬ï¼Œé¡ºåºæ–¹æ³•è®¡ç®—æˆæœ¬æˆ–é€‚é…æ€§å—é™ã€‚æœ¬æ–‡æ—¨åœ¨æå‡ºæ–°æ–¹æ³•ï¼ŒèåˆäºŒè€…ä¼˜åŠ¿ï¼Œæ›´é«˜æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºæå‡æ¨ç†æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSample Set Aggregatorï¼ˆSSAï¼‰æ¨¡å‹æ¶æ„  
è®¾è®¡è½»é‡çº§çš„SSAæ¨¡å‹ï¼Œå°†å…¶ä¸ç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ï¼ˆLMansï¼‰è§£è€¦ã€‚å…ˆç”±LManså¹¶è¡Œç”ŸæˆKä¸ªå€™é€‰ç­”æ¡ˆï¼Œå†æŠŠè¿™äº›å€™é€‰ç­”æ¡ˆæ‹¼æ¥æˆåºåˆ—è¾“å…¥SSAï¼ŒSSAé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä»¥è¾“å‡ºæœ€ç»ˆæ­£ç¡®ç­”æ¡ˆã€‚è¿™ç§è®¾è®¡è®©SSAèƒ½åŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºçš„åˆ†å¸ƒç‰¹æ€§ï¼Œç›´æ¥ä¼˜åŒ–ç­”æ¡ˆåˆæˆè¿‡ç¨‹ï¼Œè€Œéå­¤ç«‹è¯„ä¼°å•ä¸ªæ ·æœ¬ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºè¾“å‡ºåˆ†å¸ƒæ¨ç†ï¼Œè§£è€¦è®­ç»ƒä¸æ¨ç†  
SSAä¸ç›´æ¥è®­ç»ƒç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ï¼ˆLManså¯è§†ä¸ºé»‘ç›’ï¼‰ï¼Œè€Œæ˜¯é’ˆå¯¹å…¶é‡‡æ ·è¾“å‡ºè¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§â€œæ¨ç†è¾“å‡ºåˆ†å¸ƒè€Œéè°ƒæ•´æ¨¡å‹å†…éƒ¨â€çš„æ€è·¯ï¼Œè®©æ–¹æ³•æ›´çµæ´»â€”â€”å¯é€‚é…ä¸åŒåŸºç¡€æ¨¡å‹ï¼ˆç”šè‡³æ˜¯åªèƒ½é€šè¿‡APIè°ƒç”¨çš„é»‘ç›’å¤§æ¨¡å‹ï¼‰ï¼Œåªéœ€ç”¨å…¶é‡‡æ ·ç­”æ¡ˆè®­ç»ƒSSAå³å¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»Ÿä¸€å¹¶è¡Œä¸é¡ºåºç¼©æ”¾ä¼˜åŠ¿  
å¹¶è¡Œç¼©æ”¾èƒ½å¿«é€Ÿè·å–å¤šè§†è§’ç­”æ¡ˆï¼Œé¡ºåºç¼©æ”¾å¯è¿­ä»£ä¼˜åŒ–æ¨ç†ï¼›SSAé€šè¿‡â€œå¹¶è¡Œé‡‡æ ·+å•æ­¥é¡ºåºRLèšåˆâ€çš„æ–¹å¼ï¼Œåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç»“åˆäºŒè€…é•¿å¤„ï¼šç”¨å¹¶è¡Œè·å–å¤šæ ·æ€§ï¼Œç”¨SSAçš„é¡ºåºæ¨ç†å®ç°ç²¾å‡†èšåˆï¼Œä¸”ä»…éœ€è®­ç»ƒå°æ¨¡å‹å°±èƒ½å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ€§èƒ½è¶…è¶Šå¼ºåŸºçº¿ï¼šåœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒSSAç›¸æ¯”åŸºäºå¥–åŠ±æ¨¡å‹é‡æ’åºç­‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œå¤§å¹…ç¼©å°äº†æ¨¡å‹å®é™…æ€§èƒ½ä¸â€œç†è®ºæœ€ä¼˜ï¼ˆoracle - bestï¼‰â€ç²¾åº¦çš„å·®è·ã€‚  
2. æ³›åŒ–èƒ½åŠ›çªå‡ºï¼šè·¨æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—ï¼ˆå¦‚Qwen 2.5ã€Llama 3.1ï¼‰ã€æ¨¡å‹è§„æ¨¡ï¼ˆ7B/14B/32Bï¼‰å’Œä»»åŠ¡ï¼ŒSSAéƒ½å±•ç°å‡ºè‰¯å¥½æ³›åŒ–æ€§ã€‚æ¯”å¦‚åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šä¸ºç‰¹å®šæ¨¡å‹è®­ç»ƒçš„SSAï¼Œèƒ½æˆåŠŸèšåˆä¸åŒæ¨¡å‹å®¶æ—ã€è§„æ¨¡åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¾“å‡ºã€‚  
3. è½»é‡åŒ–ä¼˜åŠ¿ï¼šç´§å‡‘çš„SSAæ¨¡å‹èƒ½åŒ¹é…é¡ºåºç¼©æ”¾ä¸­ç»å¼ºåŒ–è®­ç»ƒçš„å¤§æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜å…¶ä½œä¸ºè½»é‡é¡ºåºç¼©æ”¾æ–¹å¼çš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¶æ„è§£è€¦æ€è·¯ï¼šå°†â€œç­”æ¡ˆç”Ÿæˆâ€ä¸â€œç­”æ¡ˆèšåˆåˆ†æâ€è§£è€¦ï¼Œä¸ºåˆ©ç”¨é»‘ç›’å¤§æ¨¡å‹ï¼ˆå¦‚è°ƒç”¨APIçš„å•†ç”¨å¤§æ¨¡å‹ï¼‰æä¾›äº†å¯è¡Œè·¯å¾„â€”â€”åªéœ€è·å–å…¶è¾“å‡ºï¼Œç”¨SSAåšåå¤„ç†å³å¯ï¼Œæ— éœ€æ”¹åŠ¨é»‘ç›’æ¨¡å‹æœ¬èº«ã€‚  
2. æµ‹è¯•æ—¶ç¼©æ”¾æ–°èŒƒå¼ï¼šå±•ç¤ºäº†â€œå¹¶è¡Œé‡‡æ · + é’ˆå¯¹æ€§å°æ¨¡å‹èšåˆâ€åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ½œåŠ›ï¼Œä¸ºåç»­ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—æ•ˆç‡ã€å¹³è¡¡èµ„æºä¸æ€§èƒ½æä¾›äº†æ–°æ–¹å‘ã€‚  
3. å¼ºåŒ–å­¦ä¹ åº”ç”¨å¯å‘ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–èšåˆæ¨¡å‹ï¼ˆSSAï¼‰æ¥æå‡æœ€ç»ˆç­”æ¡ˆç²¾åº¦ï¼ŒéªŒè¯äº†åœ¨â€œè¾“å‡ºåˆ†å¸ƒå±‚é¢åšæ¨ç†ä¼˜åŒ–â€çš„ä»·å€¼ï¼Œå¯å¯å‘æ›´å¤šå›´ç»•æ¨¡å‹è¾“å‡ºåå¤„ç†çš„ç ”ç©¶ã€‚

## gfriend--generative-few-shot-reward-inference-through-efficient-dpo
### Abstract
The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GFRIENDï¼šå°æ ·æœ¬ä¸‹é«˜æ•ˆDPOå®ç°ç”Ÿæˆå¼å¥–åŠ±æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆæœï¼Œä½†è®©æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½ä»æ˜¯æŒ‘æˆ˜ã€‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯å…³é”®æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ç”¨äººç±»æ ‡æ³¨çš„åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚ç„¶è€Œç°æœ‰æ–¹æ³•å­˜åœ¨æ ·æœ¬é…å¯¹ä½æ•ˆã€åå¥½æ•°æ®å¤šæ ·æ€§æœ‰é™ç­‰é—®é¢˜ï¼Œåœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸï¼Œå¤§è§„æ¨¡åå¥½æ•°æ®æ”¶é›†å—é™ï¼Œä½æ•°æ®ç¯å¢ƒä¸‹è®­ç»ƒå¥–åŠ±æ¨¡å‹æˆéš¾é¢˜ï¼Œå› æ­¤éœ€è¦é«˜æ•ˆåˆ©ç”¨æœ‰é™åå¥½æ•°æ®çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜æ•ˆæ•°æ®å¢å¼º  
å¼•å…¥æ€ç»´é“¾ï¼ˆChain - of - Thought, CoTï¼‰é‡‡æ ·æœºåˆ¶ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„åå¥½æ•°æ®ï¼Œç¼“è§£æ•°æ®ç¨€ç–æ€§é—®é¢˜ï¼Œæå‡ä½èµ„æºæ¡ä»¶ä¸‹å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§ã€‚é€šè¿‡CoTé‡‡æ ·æŒ–æ˜ä¸°å¯Œä¼˜è´¨çš„åå¥½å…³ç³»ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›æ›´å……è¶³ä¸”ä¼˜è´¨çš„æ•°æ®åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ°´å¹³åå¥½å»ºæ¨¡  
æå‡ºåŸºäºå›°æƒ‘åº¦çš„è¯„åˆ†æ–¹æ³•æ¥åˆ†é…ç»†è‡´çš„åå¥½ç­‰çº§ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„äºŒå…ƒé…å¯¹æ–¹æ³•ï¼Œèƒ½å®ç°æ›´ç»†ç²’åº¦çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚åˆ©ç”¨è¯¥è¯„åˆ†æœºåˆ¶é‡åŒ–åå¥½ç¨‹åº¦ï¼Œè®©æ¨¡å‹æ•æ‰æ ·æœ¬é—´æ›´ç»†å¾®çš„åå¥½å·®å¼‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¼˜åŒ–åå¥½å­¦ä¹   
é€šè¿‡ç»“åˆåŸºäºåå¥½å·®å¼‚çš„åŠ æƒæ ·æœ¬å¯¹æ¥å¢å¼ºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æŸå¤±å‡½æ•°ï¼Œåœ¨ä½æ•°æ®è®¾ç½®ä¸‹æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚ä¾æ®åå¥½å·®å¼‚å¯¹æ ·æœ¬å¯¹åŠ æƒï¼Œç¡®ä¿è®­ç»ƒæ—¶æ›´å…³æ³¨æœ‰ä»£è¡¨æ€§çš„æ•°æ®ï¼Œä¼˜åŒ–å¥–åŠ±æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°è¯¥æ¡†æ¶ï¼Œä¸ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡æ–¹æ³•å¯¹æ¯”ã€‚å®éªŒè¯å®æ•°æ®å¢å¼ºæé«˜äº†åå¥½å»ºæ¨¡å‡†ç¡®æ€§ï¼Œå¤šæ°´å¹³åå¥½è¯„åˆ†æœ‰æ•ˆã€‚æ¶ˆèå®éªŒæ­ç¤ºå„ç»„ä»¶çš„è´¡çŒ®ï¼Œç»“æœè¡¨æ˜åœ¨ä½èµ„æºè®¾ç½®ä¸‹æ€§èƒ½æå‡æ˜¾è‘—ï¼Œèƒ½ä¸åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ç›¸å½“ï¼Œå‡¸æ˜¾äº†æ•°æ®é«˜æ•ˆç­–ç•¥åœ¨æ¨è¿›RLHFä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æ•°æ®åˆ©ç”¨å±‚é¢ï¼Œå…¶é«˜æ•ˆæ•°æ®å¢å¼ºæ€è·¯ä¸ºä½èµ„æºåœºæ™¯ä¸‹çš„æ•°æ®æ‰©å……æä¾›äº†æ–°æ–¹å¼ï¼Œå€ŸåŠ©CoTé‡‡æ ·ç”Ÿæˆä¼˜è´¨æ•°æ®çš„æ–¹æ³•å¯è¢«å€Ÿé‰´åˆ°å…¶ä»–éœ€è¦æ•°æ®å¢å¼ºçš„ä»»åŠ¡ä¸­ï¼›åœ¨æ¨¡å‹è®­ç»ƒå±‚é¢ï¼Œå¤šæ°´å¹³åå¥½å»ºæ¨¡å’Œä¼˜åŒ–æŸå¤±å‡½æ•°çš„æ€è·¯ï¼Œä¸ºæå‡æ¨¡å‹å¯¹ç»†ç²’åº¦å·®å¼‚çš„æ•æ‰èƒ½åŠ›ã€å¢å¼ºæ¨¡å‹åœ¨ä½æ•°æ®ä¸‹çš„æ³›åŒ–æ€§æä¾›äº†å‚è€ƒï¼Œå¯¹äºèµ„æºå—é™ä½†éœ€é«˜ç²¾åº¦æ¨¡å‹è®­ç»ƒçš„åœºæ™¯æœ‰å¾ˆå¥½çš„å€Ÿé‰´ä»·å€¼ï¼Œæ¯”å¦‚å°ä¼—é¢†åŸŸçš„AIåº”ç”¨å¼€å‘ç­‰ã€‚

## intra-trajectory-consistency-for-reward-modeling
### Abstract
Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¥–åŠ±å»ºæ¨¡ä¸­çš„è½¨è¿¹å†…ä¸€è‡´æ€§ï¼šè®©å¥–åŠ±ä¿¡å·æ›´ç²¾ç»†åœ°æµåŠ¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ä¸­è‡³å…³é‡è¦ï¼Œä¸ç®¡æ˜¯åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰è¿˜æ˜¯æ¨ç†æ—¶éªŒè¯åœºæ™¯ï¼Œéƒ½ä¾èµ–å®ƒå¯¹å“åº”è´¨é‡åšé‡åŒ–è¯„ä¼°ã€‚ä½†**ç°æœ‰å¥–åŠ±å»ºæ¨¡å­˜åœ¨â€œç›‘ç£ä¿¡å·é¢—ç²’åº¦è¿‡ç²—â€**çš„é—®é¢˜ï¼šå½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–â€œæ•´ä¸ªå“åº”çš„æ‰“åˆ†â€æ¥å­¦ä¹ å¥–åŠ±ï¼Œè¿™ç§å“åº”çº§åˆ«çš„ç›‘ç£ä¿¡å·å¤ªç²—ç³™ï¼Œå¯¼è‡´å¥–åŠ±æ¨¡å‹å¾ˆéš¾ç²¾å‡†å®šä½å“åº”ç”Ÿæˆè½¨è¿¹é‡ŒçœŸæ­£å’Œåˆ†æ•°ç›¸å…³çš„å…·ä½“æˆåˆ†ï¼Œæœ€ç»ˆåœ¨æœªè§è¿‡çš„å“åº”ä¸Šæ³›åŒ–èƒ½åŠ›å·®ï¼Œç”šè‡³å¯èƒ½å»æ‹Ÿåˆåƒâ€œå“åº”é•¿åº¦â€è¿™ç±»æ— å…³ç´§è¦çš„è™šå‡ç‰¹å¾ï¼Œè€Œéå…³æ³¨å“åº”è½¨è¿¹é‡Œå’Œæ ‡ç­¾çœŸæ­£ç›¸å…³çš„å†…å®¹ã€‚è€Œä¸”ï¼Œæƒ³è·å–â€œè¿‡ç¨‹çº§ï¼ˆæ¯”å¦‚ç”Ÿæˆåˆ°ç¬¬kä¸ªtokenæ—¶çš„ä¸­é—´çŠ¶æ€ï¼‰â€çš„ç²¾ç»†æ ‡æ³¨æˆæœ¬åˆæé«˜ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†æ¨¡å‹å¯¹è¿‡ç¨‹ä¾èµ–å…³ç³»çš„æ•æ‰ã€‚æ‰€ä»¥ï¼Œè®ºæ–‡å¸Œæœ›æ‰¾åˆ°ä¸€ç§æ–¹å¼ï¼Œè®©â€œå“åº”çº§çš„ç›‘ç£ä¿¡å·â€èƒ½åœ¨ç”Ÿæˆè½¨è¿¹çš„å„ä¸ªè¿‡ç¨‹é—´ä¼ æ’­ï¼Œç»™å¥–åŠ±å­¦ä¹ è¡¥å……æ›´ç»†ç²’åº¦çš„ä¿¡å·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»è´å¶æ–¯æ¡†æ¶åˆ†æâ€œç”Ÿæˆæ¦‚ç‡â€ä¸â€œå¥–åŠ±ä¸€è‡´æ€§â€çš„å…³è”  
è®ºæ–‡å…ˆä»ç†è®ºå±‚é¢å‰–æï¼šç”Ÿæˆæ¦‚ç‡ï¼ˆå³ç”Ÿæˆå™¨ç”Ÿæˆåç»­åºåˆ—çš„å¯èƒ½æ€§ï¼‰å’Œå“åº”è½¨è¿¹å†…ä¸åŒè¿‡ç¨‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§å­˜åœ¨è”ç³»ã€‚é€šè¿‡è´å¶æ–¯åˆ†è§£ç­‰åˆ†æï¼Œå½¢å¼åŒ–åœ°é˜è¿°äº†â€”â€”å½“ç”Ÿæˆå™¨ç»™â€œè¿ç»­ç”Ÿæˆçš„è¿‡ç¨‹â€åˆ†é…æ›´é«˜æ¦‚ç‡æ—¶ï¼Œè¿™äº›è¿‡ç¨‹åœ¨åŒä¸€æ¡å“åº”é‡Œçš„å¥–åŠ±æ›´å¯èƒ½ç›¸è¿‘ã€‚ç®€å•è¯´ï¼Œç”Ÿæˆæ¦‚ç‡é«˜çš„ç›¸é‚»ç”Ÿæˆè¿‡ç¨‹ï¼Œè¯­ä¹‰ä¸Šæ›´è¿ç»­ï¼Œå¥–åŠ±ä¹Ÿè¯¥æ›´ä¸€è‡´ã€‚è¿™ä¸ºåç»­è®¾è®¡æ­£åˆ™é¡¹æä¾›äº†ç†è®ºä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºè½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆICRMï¼‰  
åŸºäºä¸Šè¿°åˆ†æï¼Œè®ºæ–‡è®¾è®¡äº†**Intra - Trajectory Consistency Regularization for Reward Modelingï¼ˆICRMï¼‰** ã€‚æ•´ä¸ªæ¡†æ¶åŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€æ˜¯å›ºå®šä½çš„ç”Ÿæˆå™¨ï¼ˆç”¨æ¥æä¾›ç”Ÿæˆæ¦‚ç‡ï¼Œç»™æ­£åˆ™åŒ–åšä¾æ®ï¼‰ï¼›äºŒæ˜¯è¦è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆé¢„æµ‹æœ€ç»ˆçš„ç»“æœå¥–åŠ±ï¼‰ã€‚æ­£åˆ™åŒ–çš„æ ¸å¿ƒé€»è¾‘æ˜¯ï¼šè®©å¥–åŠ±æ¨¡å‹åœ¨â€œä¸‹ä¸€ä¸ªtokenç”Ÿæˆæ¦‚ç‡æ›´é«˜çš„ç›¸é‚»ç”Ÿæˆè¿‡ç¨‹â€ä¸Šï¼Œè¾“å‡ºæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚è¿™æ ·ä¸€æ¥ï¼ŒåŸæœ¬åªåœ¨â€œæ•´ä¸ªå“åº”çº§åˆ«â€çš„ç›‘ç£ä¿¡å·ï¼Œå°±èƒ½æ²¿ç€ç”Ÿæˆè½¨è¿¹çš„å„ä¸ªè¿‡ç¨‹ä¼ æ’­å¼€ï¼Œè®©å¥–åŠ±æ¨¡å‹å­¦ä¹ åˆ°æ›´ç»†ç²’åº¦ã€å’Œç”Ÿæˆé€»è¾‘æ›´å¥‘åˆçš„ç›‘ç£ä¿¡æ¯ï¼Œè¿›è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åšäº†å¤šç»´åº¦å®éªŒéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼š  
- åœ¨RewardBenchç­‰æ ‡å‡†å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŠ å…¥ICRMæ­£åˆ™åŒ–åçš„å¥–åŠ±æ¨¡å‹æ€§èƒ½æ›´ä¼˜ï¼›  
- åœ¨RLHFåœºæ™¯ä¸‹ï¼Œç”¨è¯¥æ­£åˆ™åŒ–è®­ç»ƒå‡ºçš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½è¯±å¯¼å‡ºæ›´ä¼˜çš„ã€å’ŒDPOï¼ˆDirect Preference Optimizationï¼‰æ›´å¯¹é½çš„ç­–ç•¥ï¼›  
- åœ¨æ¨ç†æ—¶éªŒè¯ä»»åŠ¡ï¼ˆæ¯”å¦‚best - of - Né€‰æ‹©ï¼‰ä¸­ï¼Œä¹Ÿå–å¾—äº†æ›´å¥½çš„ç»“æœã€‚  

è¿™äº›å®éªŒä»ä¸åŒåº”ç”¨è§’åº¦ï¼Œè¯æ˜äº†ICRMèƒ½è®©å¥–åŠ±æ¨¡å‹åœ¨å®é™…ä»»åŠ¡é‡Œè¡¨ç°æ›´å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§£å†³â€œç²—ç²’åº¦ç›‘ç£ä¿¡å·â€é—®é¢˜çš„æ€è·¯ï¼šå½“ç›´æ¥è·å–ç»†ç²’åº¦æ ‡æ³¨æˆæœ¬å¤ªé«˜æ—¶ï¼Œè®ºæ–‡å·§å¦™åˆ©ç”¨ç”Ÿæˆå™¨æœ¬èº«çš„â€œç”Ÿæˆæ¦‚ç‡â€æ¥é—´æ¥æŒ–æ˜è¿‡ç¨‹é—´çš„å…³è”ï¼ŒæŠŠç²—ç²’åº¦ä¿¡å·â€œæ‹†è§£â€åˆ°è¿‡ç¨‹çº§åˆ«ï¼Œç»™å¥–åŠ±å­¦ä¹ è¡¥å……ä¿¡æ¯ã€‚è¿™ç§â€œå€ŸåŠ›å·²æœ‰ç”Ÿæˆå™¨èµ„æºï¼Œç»†åŒ–ç›‘ç£ä¿¡å·â€çš„æ€è·¯ï¼Œåœ¨å…¶ä»–éœ€è¦ç»†ç²’åº¦ç›‘ç£ä½†æ ‡æ³¨å—é™çš„åœºæ™¯ï¼ˆæ¯”å¦‚å…¶ä»–åºåˆ—ç”Ÿæˆä»»åŠ¡çš„å¥–åŠ±å»ºæ¨¡ã€åå¥½å­¦ä¹ ç­‰ï¼‰ä¸­å¾ˆæœ‰å‚è€ƒä»·å€¼ã€‚  
2. æ­£åˆ™åŒ–è®¾è®¡çš„å¯å‘ï¼šé€šè¿‡åˆ†æç”Ÿæˆæ¦‚ç‡å’Œå¥–åŠ±ä¸€è‡´æ€§çš„å…³è”ï¼Œå°†å…¶è½¬åŒ–ä¸ºæ­£åˆ™é¡¹çº¦æŸæ¨¡å‹ï¼Œè¿™ç§â€œä»ç”Ÿæˆè¿‡ç¨‹å†…åœ¨é€»è¾‘é‡Œæ‰¾æ­£åˆ™åŒ–ä¾æ®â€çš„æ–¹å¼ï¼Œèƒ½ä¸ºåç»­è®¾è®¡æ›´è´´åˆä»»åŠ¡ç‰¹æ€§çš„æ­£åˆ™æ–¹æ³•æä¾›æ€è·¯ï¼Œå¸®åŠ©æ¨¡å‹åœ¨åˆ©ç”¨å…¨å±€ç›‘ç£çš„åŒæ—¶ï¼Œæ•æ‰å±€éƒ¨è¿‡ç¨‹çš„åˆç†ä¾èµ–ã€‚  
3. å¤šåœºæ™¯éªŒè¯çš„ç¤ºèŒƒï¼šè®ºæ–‡åœ¨â€œå¥–åŠ±å»ºæ¨¡åŸºå‡†ã€RLHFã€æ¨ç†æ—¶éªŒè¯â€ç­‰å¤šä¸ªLLMå…³é”®åº”ç”¨åœºæ™¯åšå®éªŒï¼Œè¿™ç§å…¨é¢éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§çš„æ–¹å¼ï¼Œä¹Ÿæé†’æˆ‘ä»¬åœ¨åšæ–¹æ³•åˆ›æ–°æ—¶ï¼Œè¦å°½å¯èƒ½è¦†ç›–æŠ€æœ¯è½åœ°çš„ä¸åŒç¯èŠ‚ï¼Œæ›´å……åˆ†åœ°å±•ç¤ºæ–¹æ³•ä»·å€¼ã€‚

## reinforcement-learning-from-human-feedback-with-high-confidence-safety-constraints
### Abstract
Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜ç½®ä¿¡åº¦å®‰å…¨çº¦æŸä¸‹çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼šå¹³è¡¡è¯­è¨€æ¨¡å‹çš„å¸®åŠ©æ€§ä¸å®‰å…¨æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—å’¨è¯¢ã€æ³•å¾‹æ¨ç†ã€æ•™è‚²æ”¯æŒç­‰çœŸå®åœºæ™¯ä¸­å¹¿æ³›åº”ç”¨ï¼Œè¿™è¦æ±‚æ¨¡å‹è¾“å‡ºæ—¢â€œæœ‰å¸®åŠ©ï¼ˆhelpfulï¼‰â€åˆâ€œå®‰å…¨æ— å®³ï¼ˆharmlessï¼‰â€ã€‚ç„¶è€Œç°æœ‰è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•å¸¸å°†â€œå®‰å…¨æ€§â€ä¸â€œå¸®åŠ©æ€§â€è§†ä¸º trade-offï¼ˆæƒè¡¡ï¼‰ï¼Œåœ¨æ•æ„Ÿé¢†åŸŸæ˜“äº§ç”Ÿä¸å¯æ¥å—çš„å“åº”ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æœªæ˜¾å¼åˆ†ç¦»è¿™ä¸¤ä¸ªç›®æ ‡ï¼Œè¦ä¹ˆè®­ç»ƒå•ä¸€å¥–åŠ±æ¨¡å‹å…¼é¡¾äºŒè€…ï¼Œè¦ä¹ˆå¯å‘å¼ç»“åˆå¤šä¸ªå¥–åŠ±æ¨¡å‹è¾“å‡ºï¼Œå¯¼è‡´æå‡å®‰å…¨æ€§æ—¶å¯èƒ½ç‰ºç‰²å¸®åŠ©æ€§ï¼ˆå¦‚æ¨¡å‹è¿‡åº¦ä¿å®ˆæ‹’ç­”ï¼‰ï¼Œæˆ–æå‡å¸®åŠ©æ€§æ—¶äº§ç”Ÿä¸å®‰å…¨è¾“å‡ºã€‚è™½ç„¶åç»­ Safe RLHF æ–¹æ³•å°è¯•è§£è€¦å¹¶å°†æ— å®³æ€§ä½œä¸ºå®‰å…¨çº¦æŸï¼Œä½†ç¼ºä¹å®‰å…¨çš„æ¦‚ç‡æ€§ä¿éšœï¼Œåœ¨é«˜é£é™©åº”ç”¨ä¸­å­˜åœ¨éšæ‚£ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡º **é«˜ç½®ä¿¡åº¦å®‰å…¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆHC - RLHFï¼‰**ï¼Œæ—¨åœ¨ä¸ºæ— å®³æ€§æä¾›æ¦‚ç‡ä¿è¯çš„åŒæ—¶æœ€å¤§åŒ–å¸®åŠ©æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§£è€¦äººç±»åå¥½ä¸åŒæ¨¡å‹è®­ç»ƒ  
å’Œ Safe RLHF ç±»ä¼¼ï¼ŒHC - RLHF æ˜¾å¼åœ°å°†äººç±»åå¥½è§£è€¦ä¸ºâ€œå¸®åŠ©æ€§ï¼ˆhelpfulnessï¼‰â€å’Œâ€œæ— å®³æ€§ï¼ˆharmlessnessï¼Œå³å®‰å…¨æ€§ï¼‰â€ï¼Œå¹¶åˆ†åˆ«è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆreward modelï¼‰æ•æ‰å¸®åŠ©æ€§ã€æˆæœ¬æ¨¡å‹ï¼ˆcost modelï¼‰æ•æ‰æ— å®³æ€§ã€‚è¿™ç§è§£è€¦è®©ä¸¤ä¸ªç›®æ ‡çš„ä¼˜åŒ–æ›´å…·é’ˆå¯¹æ€§ï¼Œé¿å…ä¼ ç»Ÿæ–¹æ³•ä¸­ç›®æ ‡æ··æ·†å¯¼è‡´çš„æƒè¡¡å¤±è¡¡é—®é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸¤æ­¥å¼å®‰å…¨æ±‚è§£æµç¨‹  
HC - RLHF é‡‡ç”¨ä¸¤æ­¥æµç¨‹å¯»æ‰¾å®‰å…¨è§£ï¼š  
- ç¬¬ä¸€æ­¥ï¼Œåœ¨**æ•…æ„æ‚²è§‚ç‰ˆçš„æˆæœ¬çº¦æŸ**ä¸‹ä¼˜åŒ–å¥–åŠ±å‡½æ•°ã€‚è¿™é‡Œâ€œæ‚²è§‚ç‰ˆæˆæœ¬çº¦æŸâ€æ˜¯ä¸ºäº†è®©è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨åç»­å®‰å…¨æµ‹è¯•ä¸­æ›´æ˜“é€šè¿‡ï¼Œæå‰ä¸ºå®‰å…¨æ€§â€œç•™æœ‰ä½™åœ°â€ï¼›  
- ç¬¬äºŒæ­¥ï¼Œå¯¹è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œ**å®‰å…¨æµ‹è¯•**ï¼ŒéªŒè¯å…¶æ€§èƒ½æ˜¯å¦å¤„äºå®é™…æˆæœ¬çº¦æŸçš„ç½®ä¿¡ä¸Šç•Œå†…ã€‚åªæœ‰é€šè¿‡è¯¥æµ‹è¯•çš„æ¨¡å‹æ‰ä¼šè¢«æœ€ç»ˆé‡‡ç”¨ï¼Œä»¥æ­¤ç¡®ä¿è¾“å‡ºå®‰å…¨çš„æ¦‚ç‡æ»¡è¶³ç”¨æˆ·æŒ‡å®šé˜ˆå€¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†è®ºä¿éšœä¸æ¦‚ç‡å®‰å…¨ä¿è¯  
è®ºæ–‡å¯¹ HC - RLHF è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œè¯æ˜äº†è¯¥ç®—æ³•è¾“å‡ºä¸å®‰å…¨è§£çš„æ¦‚ç‡ä¸ä¼šè¶…è¿‡ç”¨æˆ·æŒ‡å®šçš„é˜ˆå€¼ï¼Œä¸ºé«˜é£é™©åœºæ™¯ä¸‹æ¨¡å‹çš„å®‰å…¨æ€§æä¾›äº†ç†è®ºå±‚é¢çš„å¯é æ€§æ”¯æ’‘ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡å°† HC - RLHF åº”ç”¨äºå¯¹é½ä¸‰ä¸ªä¸åŒè¯­è¨€æ¨¡å‹ï¼ˆQwen2 - 1.5Bã€Qwen2.5 - 3Bã€LLaMa3.2 - 3Bï¼‰ä¸äººç±»åå¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼š  
- HC - RLHF èƒ½ä»¥é«˜æ¦‚ç‡ç”Ÿæˆå®‰å…¨æ¨¡å‹ï¼Œåœ¨å®‰å…¨æ€§ä¿éšœä¸Šè¡¨ç°ä¼˜å¼‚ï¼›  
- ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒHC - RLHF åœ¨æå‡æ— å®³æ€§çš„åŒæ—¶ï¼Œè¿˜èƒ½æ”¹å–„å¸®åŠ©æ€§ï¼Œå®ç°äº†ä¸¤ä¸ªç›®æ ‡é—´æ›´ä¼˜çš„å¹³è¡¡ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç›®æ ‡è§£è€¦æ€è·¯ï¼šåœ¨å¤šç›®æ ‡ä¼˜åŒ–åœºæ™¯ï¼ˆå°¤å…¶æ˜¯å­˜åœ¨å†²çªçš„ç›®æ ‡ï¼Œå¦‚å¸®åŠ©æ€§ä¸å®‰å…¨æ€§ï¼‰ä¸‹ï¼Œæ˜¾å¼è§£è€¦ç›®æ ‡å¹¶åˆ†åˆ«å»ºæ¨¡ï¼ˆå¦‚å¥–åŠ±ã€æˆæœ¬åŒæ¨¡å‹ï¼‰æ˜¯æ§åˆ¶æƒè¡¡å…³ç³»çš„æœ‰æ•ˆæ–¹å¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€å¹³è¡¡å¤šä¸ªçŸ›ç›¾ç›®æ ‡çš„ AI ä»»åŠ¡ï¼›  
2. å®‰å…¨çº¦æŸçš„â€œæ‚²è§‚ + æµ‹è¯•â€æœºåˆ¶ï¼šé€šè¿‡è®­ç»ƒæ—¶å¼•å…¥ä¿å®ˆçº¦æŸ + è®­ç»ƒåä¸¥æ ¼æµ‹è¯•çš„ä¸¤æ­¥æ³•ï¼Œä¸ºé«˜ç½®ä¿¡åº¦å®‰å…¨ä¿éšœæä¾›äº†å¯è½åœ°çš„æŠ€æœ¯è·¯å¾„ï¼Œé€‚åˆåŒ»ç–—ã€æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸçš„ AI ç³»ç»Ÿå¼€å‘ï¼›  
3. ç†è®ºä¸å®éªŒç»“åˆï¼šæ—¢ä»ç†è®ºä¸Šè¯æ˜å®‰å…¨æ¦‚ç‡ä¿è¯ï¼Œåˆé€šè¿‡å¤šæ¨¡å‹å®éªŒéªŒè¯æ•ˆæœï¼Œè¿™ç§â€œç†è®ºæ”¯æ’‘ + å®è¯æ£€éªŒâ€çš„ç ”ç©¶èŒƒå¼ï¼Œä¸ºç®—æ³•ç±»ç ”ç©¶æä¾›äº†ç¤ºèŒƒï¼Œæœ‰åŠ©äºæå‡æ–¹æ³•çš„å¯ä¿¡åº¦ä¸è¯´æœåŠ›ã€‚  

## explicit-preference-optimization--no-need-for-an-implicit-reward-model
### Abstract
The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— éœ€éšå¼å¥–åŠ±æ¨¡å‹çš„æ˜¾å¼åå¥½ä¼˜åŒ–ï¼šEXPO æ¡†æ¶é©æ–°å¤§æ¨¡å‹åå¥½å¯¹é½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸é€šè¿‡**åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**æ¥å¯¹é½äººç±»åå¥½ï¼Œä½† RLHF å­˜åœ¨å¤šæ­¥éª¤è®­ç»ƒçš„å¤æ‚æ€§ï¼šéœ€å…ˆè®­ç»ƒç‹¬ç«‹å¥–åŠ±æ¨¡å‹ï¼Œå†ç”¨å…¶æ›´æ–° LLM ç­–ç•¥ï¼Œè¿‡ç¨‹æ˜“ä¸ç¨³å®šã€‚ä¸ºç®€åŒ–æµç¨‹ï¼Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åŠè¡ç”Ÿæ–¹æ³•é€šè¿‡â€œé‡æ–°å‚æ•°åŒ–æŠ€å·§â€å¼•å…¥éšå¼å¥–åŠ±ï¼Œå°†å­¦ä¹ å‹ç¼©ä¸ºå•æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚ç„¶è€Œï¼ŒDPO ç±»æ–¹æ³•å­˜åœ¨æœªè¢«å……åˆ†é‡è§†çš„ç¼ºé™·ï¼šæ­£åˆ™åŒ–æ•ˆæœæ¬ ä½³ã€æ’å€¼è¡Œä¸ºåç›´è§‰ç­‰ï¼Œæ ¹æºæ˜¯å…¶ä¾èµ–çš„é‡æ–°å‚æ•°åŒ–æœºåˆ¶ã€‚æœ¬æ–‡æ—¨åœ¨æå‡ºæ›´ä¼˜æ¡†æ¶è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰å…¨æ–°è¯„ä¼°å‡†åˆ™ï¼Œæ­éœ² DPO ç±»æ–¹æ³•ç¼ºé™·  
è®ºæ–‡æå‡ºç¬¦åˆç›´è§‰çš„åå¥½æ¨¡å‹ç†æƒ³è¡Œä¸ºè¯„ä¼°å‡†åˆ™ï¼Œè¯æ˜å¤§é‡ DPO ç±»æ–¹æ³•æ— æ³•æ»¡è¶³è¿™äº›å‡†åˆ™ã€‚ä¾‹å¦‚ï¼ŒDPO å¸¸ç”¨ç›®æ ‡çš„æå°å€¼ç‚¹ï¼Œéš¾ä»¥åœ¨â€œå‚è€ƒæ¨¡å‹å¼ºçš„åŒºåŸŸä¿æŒæ€§èƒ½ï¼ŒåŒæ—¶åœ¨å…¶ä»–åŒºåŸŸæ”¹è¿›â€ï¼›ä¸”æ¨¡å‹æƒè¡¡å‚æ•°å˜åŒ–æ—¶ï¼Œåœ¨ç†æƒ³ç«¯ç‚¹é—´çš„æ’å€¼èƒ½åŠ›ä¹Ÿå­˜åœ¨å±€é™ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡º EXPO æ˜¾å¼åå¥½ä¼˜åŒ–æ¡†æ¶  
æ— éœ€ä¾èµ–éšå¼å¥–åŠ±æˆ–é‡æ–°å‚æ•°åŒ–ï¼Œä»å¤´è®¾è®¡ç›´è§‚æ­£åˆ™åŒ–å› å­ï¼Œè§„é¿ DPO å…³é”®å˜ç§çš„æ½œåœ¨ç¼ºé™·ï¼Œä¸”ç†è®ºä¸Šæ»¡è¶³æ­¤å‰æ–¹æ³•æœªè¾¾çš„æ­£åˆ™åŒ–éœ€æ±‚ã€‚è™½ EXPO æŸå¤±å‡½æ•°ä¾èµ–ä¸å¯è§‚æµ‹çš„çœŸå®åå¥½åˆ†å¸ƒï¼Œä½†å¯ç›´æ¥è®¡ç®—æ¢¯åº¦æ— åä¼°è®¡ï¼Œæ”¯æŒé«˜æ•ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¼˜åŒ–ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. å—æ§ç¯å¢ƒå®éªŒï¼šåœ¨å·²çŸ¥çœŸå®åå¥½çš„åœºæ™¯ä¸‹ï¼ŒéªŒè¯ DPO ç±»æ–¹æ³•æ˜“æ”¶æ•›åˆ°â€œé€€åŒ–æå°å€¼â€ï¼Œè€Œ EXPO ä¸ä¼šï¼›  
2. çœŸå®å¯¹é½æ•°æ®å®éªŒï¼šEXPO åœ¨å“åº”èƒœç‡ï¼ˆresponse win ratesï¼‰æŒ‡æ ‡ä¸Šè¶…è¶Š DPO ç±»æ¨¡å‹ï¼Œè¯æ˜å®é™…æ•ˆæœæ›´ä¼˜ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¯„ä¼°è§†è§’é©æ–°ï¼šä¸ºåå¥½ä¼˜åŒ–æ–¹æ³•è®¾è®¡â€œç†æƒ³è¡Œä¸ºå‡†åˆ™â€ï¼Œæä¾›äº†æ›´æ¸…æ™°çš„æ€§èƒ½è¡¡é‡ä¸ç¼ºé™·è¯Šæ–­æ¡†æ¶ï¼Œåç»­ç ”ç©¶å¯å‚è€ƒè¿™ç±»å‡†åˆ™å®šä¹‰æ€è·¯ï¼›  
2. æ˜¾å¼æ­£åˆ™åŒ–è®¾è®¡ï¼šEXPO è·³å‡ºâ€œéšå¼å¥–åŠ± + é‡æ–°å‚æ•°åŒ–â€çš„ä¼ ç»Ÿè·¯å¾„ï¼Œé€šè¿‡ç›´è§‚æ­£åˆ™åŒ–å®ç°åå¥½ä¼˜åŒ–ï¼Œä¸ºç®€åŒ–å¤§æ¨¡å‹å¯¹é½æµç¨‹æä¾›äº†æ–°èŒƒå¼ï¼›  
3. ç†è®ºä¸å®éªŒç»“åˆï¼šä»ç†è®ºåˆ†æç¼ºé™·ã€è®¾è®¡æ–¹æ³•åˆ°å—æ§ + çœŸå®åœºæ™¯å®éªŒéªŒè¯ï¼Œæ•´å¥—é€»è¾‘ä¸¥è°¨ï¼Œä¸ºç®—æ³•ç±»è®ºæ–‡çš„è®ºè¯æä¾›äº†ç¤ºèŒƒã€‚  


æœ¬æ–‡é’ˆå¯¹ DPO ç±»æ–¹æ³•çš„å›ºæœ‰ç¼ºé™·ï¼Œä»¥å…¨æ–°è¯„ä¼°å‡†åˆ™ä¸ºé”šç‚¹ï¼Œæå‡º EXPO æ¡†æ¶ï¼Œåœ¨ç†è®ºå’Œå®éªŒå±‚é¢éƒ½å±•ç°äº†å¯¹å¤§æ¨¡å‹åå¥½å¯¹é½çš„é©æ–°ä»·å€¼ï¼Œä¸ºåç»­ç®€åŒ– RLHF æµç¨‹ã€æå‡å¯¹é½ç¨³å®šæ€§æä¾›äº†å…³é”®æ€è·¯ã€‚

## proteinzero--self-improving-protein-generation-via-online-reinforcement-learning
### Abstract
Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ProteinZeroï¼šåœ¨çº¿å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è›‹ç™½è´¨ç”Ÿæˆè‡ªæˆ‘è¿›åŒ–æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è›‹ç™½è´¨è®¾è®¡åœ¨è®¡ç®—ç”Ÿç‰©å­¦é¢†åŸŸæå…·å‰æ™¯ï¼Œç„¶è€Œç°æœ‰è›‹ç™½è´¨ç”Ÿæˆæ¨¡å‹å—é™äºé«˜è´¨é‡ç›‘ç£é¢„è®­ç»ƒæ•°æ®é›†ç¨€ç¼ºï¼Œåœ¨æˆåŠŸç‡ç­‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸»æµæ–¹æ³•ä¾èµ–Protein Data Bankï¼ˆPDBï¼‰çš„åºåˆ— - ç»“æ„é…å¯¹æ•°æ®ï¼Œè¿™ç±»æ•°æ®ä»…è¦†ç›–è›‹ç™½è´¨åºåˆ—ç©ºé—´çš„æå°éƒ¨åˆ†ï¼Œä¸”å­˜åœ¨å¤šæ ·æ€§æœ‰é™ä¸å¤©ç„¶åå·®é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ä¸å¯¹æ–°è›‹ç™½è´¨è®¾è®¡çš„æ¢ç´¢ã€‚åŒæ—¶ï¼Œé€†æŠ˜å çš„æœ‰ç›‘ç£å­¦ä¹ ä»»åŠ¡å’Œå®é™…è›‹ç™½è´¨è®¾è®¡ç›®æ ‡ï¼ˆå¦‚é«˜è®¾è®¡æ€§ã€çƒ­ç¨³å®šæ€§ã€å¤šæ ·æ€§ï¼‰å­˜åœ¨é”™ä½ï¼Œç°æœ‰å·¥ä½œåœ¨åˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆè¾“å‡ºæ¥å®ç°è‡ªæˆ‘è¿›åŒ–è®¾è®¡æ¨¡å‹æ–¹é¢è¿›å±•æœ‰é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–åˆ›åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ProteinZero
ProteinZeroæ˜¯é¦–ä¸ªæ— éœ€ä¾èµ–ç²¾å¿ƒæ„å»ºçš„åå¥½æ•°æ®é›†ï¼Œå®ç°è›‹ç™½è´¨åºåˆ—è®¾è®¡æ¨¡å‹è‡ªåŠ¨åŒ–è‡ªæˆ‘æ”¹è¿›çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½è®©é€†æŠ˜å æ¨¡å‹å®ç°å¯æ‰©å±•ã€è‡ªåŠ¨åŒ–ä¸”æŒç»­çš„è‡ªæˆ‘æå‡ï¼Œåœ¨å¤šè½®ä¼˜åŒ–ä¸­æœ‰æ•ˆä¼˜åŒ–è›‹ç™½è´¨åºåˆ—è®¾è®¡çš„å¤šä¸ªå…³é”®ç›®æ ‡ï¼ˆè®¾è®¡æ€§ã€çƒ­ç¨³å®šæ€§ã€åºåˆ—å¤šæ ·æ€§ç­‰ï¼‰åŒæ—¶æå‡é€†æŠ˜å ç²¾åº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆå¥–åŠ±æ¨¡å‹ä¸å¿«é€ŸddGé¢„æµ‹å™¨
æå‡ºåŸºäºé€†æŠ˜å åºåˆ—ä¼¼ç„¶å’Œæ— æ¡ä»¶åºåˆ—å…ˆéªŒçš„å¿«é€Ÿçƒ­åŠ›å­¦ç¨³å®šæ€§ddGä¼°è®¡å™¨ï¼Œç»“åˆESMFoldæä¾›çš„è½»é‡çº§è®¾è®¡æ€§å¥–åŠ±ï¼Œå¤§å¹…é™ä½è¯„ä¼°æˆæœ¬ï¼Œé¦–æ¬¡è®©è›‹ç™½è´¨è®¾è®¡çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒå…·å¤‡å¯è¡Œæ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œåœ¨è®¡ç®—åé¦ˆæ—¶æ›´é«˜æ•ˆï¼ŒåŠ é€Ÿæ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºè›‹ç™½è´¨åµŒå…¥çš„å¤šæ ·æ€§æ­£åˆ™åŒ–
å¼€å‘äº†åŸºäºè›‹ç™½è´¨åµŒå…¥çš„æ–°é¢–å¤šæ ·æ€§ä¿ƒè¿›æ­£åˆ™åŒ–æ–¹æ³•ï¼Œåœ¨æ¨¡å‹è¡¨ç¤ºç©ºé—´è€Œéç›´æ¥åœ¨åºåˆ—å±‚é¢æ“ä½œï¼Œæœ‰æ•ˆé˜²æ­¢æ¨¡å¼å´©æºƒï¼Œæå‡ç”Ÿæˆåºåˆ—å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿è¯è›‹ç™½è´¨ç»“æ„å®Œæ•´æ€§ä¸”ä¸ä¼šå¼•å‘è®­ç»ƒä¸ç¨³å®šï¼Œä¸ºè›‹ç™½è´¨è®¾è®¡æä¾›æ›´å…·åŠŸèƒ½æ€§å¤šæ ·æ€§çš„ç»“æœã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæ¢ç´¢åœ¨çº¿RLå¾®è°ƒæ¡†æ¶è®¾è®¡ç©ºé—´
é€šè¿‡ç ”ç©¶ä¸åŒç®—æ³•ï¼ˆGRPOã€RAFTã€DPOï¼‰ã€å¥–åŠ±ã€ divergenceï¼ˆKLæ•£åº¦ï¼‰å’Œå¤šæ ·æ€§æ­£åˆ™åŒ–ç­‰ï¼Œé˜æ˜åœ¨çº¿RLå¾®è°ƒæ¡†æ¶çš„è®¾è®¡ç©ºé—´ï¼Œæ‰¾åˆ°èƒ½ç¨³å®šä¼˜åŒ–å¤šç›®æ ‡ä¸”é¿å…æ¨¡å¼å´©æºƒçš„æœ€ä½³ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨é€†æŠ˜å ä»»åŠ¡çš„å¹¿æ³›å®éªŒä¸­ï¼ŒProteinZeroåœ¨å„å…³é”®æŒ‡æ ‡ä¸Šè¿œè¶…ç°æœ‰æ–¹æ³•ã€‚åœ¨ç»“æ„å‡†ç¡®æ€§ã€çƒ­åŠ›å­¦ç¨³å®šæ€§ã€åºåˆ—å¤šæ ·æ€§æ–¹é¢æ˜¾è‘—æå‡ï¼›ä¸ProteinMPNNã€ESM - IFå’ŒInstructPLMç­‰å¸¸ç”¨æ–¹æ³•ç›¸æ¯”ï¼Œè®¾è®¡å¤±è´¥ç‡é™ä½çº¦36% - 48%ï¼Œåœ¨å¤šæ ·å¤æ‚çš„è›‹ç™½è´¨æŠ˜å ä¸­æˆåŠŸç‡æŒç»­è¶…90%ï¼›ä¸”èƒ½åœ¨ä¸åŒå¤æ‚è›‹ç™½è´¨æŠ˜å ä¸Šè®¾è®¡æ›´ä¼˜åºåˆ—ï¼Œæ˜¾è‘—æå‡é•¿é“¾è›‹ç™½è´¨è®¾è®¡æˆåŠŸç‡ã€‚åœ¨è®¡ç®—æ•ˆç‡ä¸Šï¼Œåœ¨CATH - 4.3ä¸Šè¿è¡Œæ•´ä¸ªRLè¿‡ç¨‹ï¼ˆå«å¥–åŠ±è®¡ç®—ï¼‰ï¼Œå•ä¸ª8å¡GPUèŠ‚ç‚¹å¯åœ¨3å¤©å†…å®Œæˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶åˆ›æ–°è§’åº¦ï¼šProteinZeroå¼€åˆ›çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ è‡ªæˆ‘æ”¹è¿›æ¡†æ¶æ¨¡å¼ï¼Œä¸ºé¢†åŸŸå†…æ¨¡å‹æ‘†è„±å¯¹å›ºå®šç¦»çº¿æ•°æ®é›†ä¾èµ–ã€å®ç°è‡ªæˆ‘è¿›åŒ–æä¾›äº†èŒƒä¾‹ï¼Œåç»­å·¥ä½œå¯å€Ÿé‰´è¿™ç§åˆ©ç”¨æ¨¡å‹è‡ªèº«è¾“å‡ºè¿­ä»£ä¼˜åŒ–çš„æ€è·¯æ‹“å±•ç ”ç©¶ã€‚
2. æ•ˆç‡ä¼˜åŒ–è§’åº¦ï¼šå…¶ç»“åˆé«˜æ•ˆä»£ç†å¥–åŠ±æ¨¡å‹ä¸å¿«é€Ÿé¢„æµ‹å™¨é™ä½è¯„ä¼°æˆæœ¬çš„æ€è·¯ï¼Œä¸ºè®¡ç®—èµ„æºå—é™ä½†éœ€é«˜æ•ˆåé¦ˆçš„å¼ºåŒ–å­¦ä¹ åº”ç”¨åœºæ™¯ï¼ˆå¦‚å…¶ä»–ç”Ÿç‰©åˆ†å­è®¾è®¡ç­‰ï¼‰æä¾›äº†æ•ˆç‡ä¼˜åŒ–å‚è€ƒã€‚
3. æ­£åˆ™åŒ–è®¾è®¡è§’åº¦ï¼šåŸºäºè›‹ç™½è´¨åµŒå…¥çš„å¤šæ ·æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä¸ºè§£å†³ç”Ÿæˆæ¨¡å‹æ¨¡å¼å´©æºƒä¸å¤šæ ·æ€§ä¸è¶³é—®é¢˜æä¾›äº†æ–°çš„æŠ€æœ¯è·¯çº¿ï¼Œå¯å¯å‘å…¶ä»–ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å°åˆ†å­ç”Ÿæˆã€å›¾åƒç”Ÿæˆç­‰ï¼‰åœ¨æ­£åˆ™åŒ–è®¾è®¡ä¸Šçš„åˆ›æ–°ã€‚
4. å¤šç›®æ ‡ä¼˜åŒ–è§’åº¦ï¼šå¯¹åœ¨çº¿RLå¾®è°ƒæ¡†æ¶è®¾è®¡ç©ºé—´çš„æ¢ç´¢ä»¥åŠå¤šç›®æ ‡ç¨³å®šä¼˜åŒ–ç­–ç•¥çš„ç¡®å®šï¼Œä¸ºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ä»»åŠ¡ï¼ˆä¸ä»…é™äºè›‹ç™½è´¨è®¾è®¡ï¼‰ä¸­çš„åº”ç”¨æä¾›äº†æ–¹æ³•è®ºå±‚é¢çš„å€Ÿé‰´ï¼ŒæŒ‡å¯¼åç»­å¦‚ä½•å¹³è¡¡å¤šç›®æ ‡ã€é€‰æ‹©ç®—æ³•ä¸æ­£åˆ™åŒ–æ‰‹æ®µç­‰ã€‚

## efficient-online-rft-with-plug-and-play-llm-judges--unlocking-state-of-the-art-performance
### Abstract
Reward-model training is the cost bottleneck in modern Reinforcement Learning
Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters
and an offline preference-tuning phase. In the proposed method, a frozen,
instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a
rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling
it to serve as a complete substitute for the previously used heavyweight
evaluation models. The plug-and-play judge achieves 96.2% accuracy on
RewardBench, outperforming specialized reward networks ranging from 27B to 70B
parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO
baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K
utilizing online PPO. Thorough ablations indicate that (i) six in context
demonstrations deliver the majority of the zero-to-few-shot improvements
(+2pp), and (ii) the LoRA effectively addresses the remaining disparity,
particularly in the safety and adversarial Chat-Hard segments. The proposed
model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic
HH-RLHF, to examine interpretability, accompanied by human generated
justifications. GPT-4 scoring indicates that our LoRA judge attains
approximately = 9/10 in similarity to human explanations, while zero-shot
judges score around =5/10. These results indicate that the combination of
prompt engineering and tiny LoRA produces a cost effective, transparent, and
easily adjustable reward function, removing the offline phase while achieving
new state-of-the-art outcomes for both static evaluation and online RLHF.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è½»é‡LLMè£åˆ¤ï¼šç”¨å³æ’å³ç”¨æ–¹æ¡ˆè§£é”RLHFæ–°æ€§èƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°ä»£å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰æµç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹è®­ç»ƒæ˜¯æˆæœ¬ç“¶é¢ˆï¼Œå¸¸éœ€æ•°ç™¾äº¿å‚æ•°å’Œç¦»çº¿åå¥½è°ƒä¼˜é˜¶æ®µã€‚åŒæ—¶ï¼Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å‡†ç¡®è¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ¨¡å‹è¾“å‡ºã€ä¿æŒå¯¹è¯æˆ–æ–‡æ¡£è¿è´¯æ€§ä¸äº‹å®ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼›ä¼ ç»ŸåŒé˜¶æ®µå¯¹é½ç­–ç•¥ï¼ˆç»“åˆç¦»çº¿ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼‰å¤æ‚åº¦é«˜ï¼Œéœ€å¤šè®­ç»ƒé˜¶æ®µã€å¤§é‡æ•°æ®é›†æ ‡æ³¨ã€å•ç‹¬å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œåºå¤§è®¡ç®—èµ„æºã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆã€çµæ´»çš„æ–¹æ¡ˆæ¥æ›¿ä»£ä¼ ç»Ÿé‡é‡çº§å¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­çš„å¥–åŠ±è¯„ä¼°ç¯èŠ‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå³æ’å³ç”¨çš„è½»é‡å¥–åŠ±è¯„ä¼°å™¨æ¶æ„  
ä½¿ç”¨å†»ç»“çš„ã€ç»æŒ‡ä»¤è°ƒä¼˜çš„7Bå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»…æ·»åŠ ä¸€è¡ŒJSONè§„åˆ™å’Œä¸€ä¸ªç§©ä¸º16çš„LoRAé€‚é…å™¨ï¼ˆä»…å½±å“æ¨¡å‹0.8%å‚æ•°ï¼‰ï¼Œä½¿å…¶å®Œå…¨æ›¿ä»£ä¹‹å‰çš„é‡é‡çº§è¯„ä¼°æ¨¡å‹ï¼Œå°†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¼˜åŠ¿èå…¥åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ— éœ€å•ç‹¬ç¦»çº¿å¯¹é½é˜¶æ®µã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆæç¤ºå·¥ç¨‹ä¸å¾®å°LoRAå®ç°é«˜æ•ˆå¥–åŠ±å‡½æ•°  
é€šè¿‡æç¤ºå·¥ç¨‹ï¼ˆå¦‚æä¾›ä¸Šä¸‹æ–‡æ¼”ç¤ºï¼‰å’Œå¾®å°LoRAé€‚é…å™¨çš„ç»“åˆï¼Œæ‰“é€ ç»æµé«˜æ•ˆã€é€æ˜ä¸”æ˜“è°ƒæ•´çš„å¥–åŠ±å‡½æ•°ã€‚å…­æ¬¡ä¸Šä¸‹æ–‡æ¼”ç¤ºå¸¦æ¥é›¶æ ·æœ¬åˆ°å°‘æ ·æœ¬çš„å¤§éƒ¨åˆ†æ€§èƒ½æå‡ï¼ˆ+2ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼ŒLoRAåˆ™æœ‰æ•ˆè§£å†³å‰©ä½™å·®è·ï¼Œå°¤å…¶åœ¨å®‰å…¨å’Œå¯¹æŠ—æ€§Chat - Hardç¯èŠ‚ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥HH - Rationalesæå‡å¯è§£é‡Šæ€§  
æå‡ºHH - Rationalesï¼ˆæ¥è‡ªAnthropic HH - RLHFçš„10000å¯¹å­é›†ï¼‰æ¥æ£€éªŒå¯è§£é‡Šæ€§ï¼Œé™„å¸¦äººç±»ç”Ÿæˆçš„ç†ç”±ã€‚åˆ©ç”¨GPT - 4è¯„åˆ†è¡¨æ˜ï¼ŒLoRAè£åˆ¤ä¸äººç±»è§£é‡Šçš„ç›¸ä¼¼åº¦çº¦è¾¾9/10ï¼Œè¿œé«˜äºé›¶æ ·æœ¬è£åˆ¤çš„çº¦5/10ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- åœ¨RewardBenchä¸Šï¼Œå³æ’å³ç”¨è£åˆ¤è¾¾åˆ°96.2%å‡†ç¡®ç‡ï¼Œè¶…è¶Š27Båˆ°70Bå‚æ•°çš„ä¸“ç”¨å¥–åŠ±ç½‘ç»œã€‚  
- å€ŸåŠ©åœ¨çº¿PPOï¼Œ7Bçš„actoræ¨¡å‹åœ¨GSM - 8Kä¸Šå®ç°92%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼Œè¶…è¶Šå¾—åˆ†61.8%çš„é¡¶çº§70B DPOåŸºçº¿ã€‚  
- æ¶ˆèå®éªŒéªŒè¯ä¸Šä¸‹æ–‡æ¼”ç¤ºå’ŒLoRAçš„ä½œç”¨ï¼šå…­æ¬¡ä¸Šä¸‹æ–‡æ¼”ç¤ºå¸¦æ¥ä¸»è¦çš„é›¶åˆ°å°‘æ ·æœ¬æå‡ï¼›LoRAæœ‰æ•ˆå¼¥è¡¥å®‰å…¨ã€å¯¹æŠ—æ€§ç­‰ç¯èŠ‚çš„æ€§èƒ½å·®è·ã€‚  
- GPT - 4å¯¹è§£é‡Šè´¨é‡è¯„åˆ†æ˜¾ç¤ºï¼ŒLoRAè£åˆ¤ä¸äººç±»è§£é‡Šç›¸ä¼¼åº¦â‰ˆ9/10ï¼Œé›¶æ ·æœ¬è£åˆ¤ä»…â‰ˆ5/10ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ¶æ„è®¾è®¡å±‚é¢ï¼šå±•ç¤ºäº†è½»é‡çº§æ¨¡å‹é€šè¿‡å°‘é‡é€‚é…å™¨å’Œæç¤ºå·¥ç¨‹å®ç°é‡é‡çº§æ¨¡å‹åŠŸèƒ½çš„å¯è¡Œæ€§ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹çš„å¥–åŠ±æ¨¡å‹è®¾è®¡æä¾›æ€è·¯ï¼Œè¯æ˜æ— éœ€è¶…å¤§å‚æ•°æ¨¡å‹ä¹Ÿèƒ½å®ç°é«˜æ€§èƒ½è¯„ä¼°ã€‚  
- æ–¹æ³•èåˆå±‚é¢ï¼šå°†ç¦»çº¿RLä¼˜åŠ¿ï¼ˆæ˜ç¡®åˆ¤åˆ«èƒ½åŠ›ã€äººç±»æ ‡æ³¨æ ‡å‡†ï¼‰èå…¥åœ¨çº¿RLåœºæ™¯ï¼Œä¸ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¼˜åŒ–æä¾›æ–°èŒƒå¼ï¼Œç®€åŒ–ä¼ ç»ŸåŒé˜¶æ®µå¯¹é½çš„å¤æ‚æµç¨‹ã€‚  
- å¯è§£é‡Šæ€§ä¼˜åŒ–å±‚é¢ï¼šHH - Rationalesçš„å¼•å…¥ä»¥åŠå¯¹è§£é‡Šç›¸ä¼¼åº¦çš„é‡åŒ–è¯„ä¼°ï¼Œä¸ºæå‡AIç³»ç»Ÿå¯è§£é‡Šæ€§æä¾›äº†å¯å‚è€ƒçš„æ•°æ®é›†æ„å»ºå’Œè¯„ä¼°æ–¹æ³•ï¼ŒåŠ©åŠ›æ‰“é€ æ›´é€æ˜çš„AIç³»ç»Ÿã€‚  
- æ•ˆç‡æå‡å±‚é¢ï¼šå¤§å¹…é™ä½ä¼ ç»Ÿä¸¤é˜¶æ®µå¯¹é½æµç¨‹çš„è®¡ç®—å¼€é”€å’Œè®­ç»ƒæ—¶é—´ï¼Œæ— éœ€ä¸“é—¨å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œé‡å¤å¯¹é½è½®æ¬¡ï¼Œåœ¨å·¥ä¸šç•Œå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–ä¸­å…·æœ‰æˆæœ¬æ§åˆ¶å’Œæ•ˆç‡æå‡çš„å€Ÿé‰´ä»·å€¼ã€‚

## confidence-is-all-you-need--few-shot-rl-fine-tuning-of-language-models
### Abstract
Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,
RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on
Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a
simple, scalable post-training method for inference models, requiring only a
small number of samples and unlabelled supervision.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»…é ç½®ä¿¡åº¦å°±èƒ½å¾®è°ƒå¤§æ¨¡å‹ï¼ŸRLSC ç”¨è‡ªç½®ä¿¡å¼€å¯å°‘æ ·æœ¬å¼ºåŒ–å­¦ä¹ æ–°æ€è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†åè®­ç»ƒé˜¶æ®µå¯¹è®©æ¨¡å‹è¡Œä¸ºä¸ä»»åŠ¡ç›®æ ‡å¯¹é½è‡³å…³é‡è¦ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å¸¸ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œæ¯”å¦‚ RLHF éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ï¼ŒTTRL é æ¯ä¸ªé—®é¢˜ç”Ÿæˆ 64 ä¸ªå“åº”å†å¤šæ•°æŠ•ç¥¨ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè®¡ç®—å¼€é”€æå¤§ã€‚ä¸ºè§£å†³è¿™äº›ä¾èµ–å¤–éƒ¨ç›‘ç£ã€æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºç”¨æ¨¡å‹è‡ªèº«ç½®ä¿¡åº¦ä½œå¥–åŠ±ä¿¡å·çš„å¼ºåŒ–å­¦ä¹ æ–°èŒƒå¼ RLSCï¼Œæ— éœ€æ ‡ç­¾ã€åå¥½æ¨¡å‹ä¸å¥–åŠ±å·¥ç¨‹ï¼Œè®©æ¨¡å‹èƒ½åŸºäºè‡ªèº«è¾“å‡ºåé¦ˆä¼˜åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º RLSC æ¡†æ¶ï¼Œä»¥è‡ªç½®ä¿¡ä¸ºå¥–åŠ±ä¿¡å·  
å— TTRL å¤šæ•°æŠ•ç¥¨é€‰è¾“å‡ºæ¨¡å¼çš„å¯å‘ï¼Œæœ¬æ–‡å‘ç°å¤šæ•°æŠ•ç¥¨æœ¬è´¨æ˜¯é€‰è¾“å‡ºåˆ†å¸ƒçš„ä¼—æ•°ï¼Œä¼˜åŒ–é‡‡æ ·å®Œæˆåº¦é—´çš„ä¸€è‡´æ€§èƒ½â€œé”åŒ–â€åˆ†å¸ƒï¼ˆè®©æ¦‚ç‡é›†ä¸­åœ¨æœ€å¯èƒ½ç­”æ¡ˆï¼‰ã€‚åŸºäºæ­¤ï¼Œæå‡ºç›´æ¥æœ€å¤§åŒ–è‡ªç½®ä¿¡ç›®æ ‡ï¼š$F(p_Î¸) = \mathbb{E}_{y \sim p_Î¸(y|x)} [p_Î¸(y | x)]$ ï¼Œè®©æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒå‘æ›´â€œç¡®ä¿¡â€çš„å•ä¸€å“åº”åç¼©ï¼Œæ—¢ä¿ç•™ TTRL ç¨³å®šè¾“å‡ºçš„ä¼˜åŠ¿ï¼Œåˆæ— éœ€ä¼ªæ ‡ç­¾æå–æˆ–å¤šæ•°æŠ•ç¥¨ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªç½®ä¿¡æŸå¤±ä¸æ¢¯åº¦è®¾è®¡  
ä¸ºä¼˜åŒ–è‡ªç½®ä¿¡ç›®æ ‡ï¼Œæ¨å¯¼å…¶å¯¹æ¨¡å‹å‚æ•° $Î¸$ çš„æ¢¯åº¦ï¼Œå¾—åˆ°è®­ç»ƒæŸå¤±ã€‚åŸºç¡€æŸå¤± $L_1 = -\sum_y p_{\text{old}}(y | x) \cdot \log p_Î¸(y | x)$ ï¼Œè®©æ›´æ–°åçš„æ¨¡å‹å¯¹æ—§æ¨¡å‹é«˜ç½®ä¿¡çš„å“åº”åˆ†é…æ›´é«˜å¯¹æ•°æ¦‚ç‡ï¼›è¿˜æ¨å¹¿å‡ºå¸¦å¹³æ»‘é¡¹çš„æŸå¤± $L_2 = -\sum_y (p_{\text{old}}(y | x) + \alpha) \cdot \log p_Î¸(y | x)$ ï¼ˆ$\alpha>0$ ç”¨äºç¨³å®šä¼˜åŒ–ï¼Œå° $\alpha$ å°±èƒ½æå‡æ”¶æ•›ä¸æ³›åŒ–ï¼‰ï¼Œä»…ç”¨æ¨¡å‹è‡ªèº«ä¿¡å¿µåˆ†å¸ƒä½œåé¦ˆï¼Œæ— éœ€å¤–éƒ¨å¥–åŠ±æ¨¡å‹ä¸æ ‡æ³¨æ•°æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè½»é‡è®­ç»ƒé…ç½®è½åœ°  
åœ¨ Qwen2.5-Math-7B ä¸Šåº”ç”¨è‡ªç½®ä¿¡ç›®æ ‡å¾®è°ƒï¼šæ¯ä¸ªé—®é¢˜ç”Ÿæˆ 16 ä¸ªå€™é€‰å®Œæˆï¼ˆå›ºå®šæ¸©åº¦é‡‡æ ·ï¼Œè§†ä¸ºæ¥è‡ª $p_{\text{old}}$ çš„ç‹¬ç«‹åŒåˆ†å¸ƒæ ·æœ¬ï¼‰ï¼›è®¡ç®—æ›´æ–°åæ¨¡å‹ä¸‹æ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡ï¼Œç”¨åŸºç¡€æˆ–å¹³æ»‘ç‰ˆè‡ªç½®ä¿¡æŸå¤±è¯„ä¼°ï¼›ä»…ç”¨ AIME2024 æ•°æ®é›†è®­ç»ƒ 10 æˆ– 20 æ­¥ï¼Œ8 å¼  A100 GPUã€AdamW ä¼˜åŒ–å™¨ï¼ˆå­¦ä¹ ç‡ $1Ã—10^{-5}$ ï¼‰ç­‰è½»é‡é…ç½®ï¼Œå…¨ç¨‹æ— è¾…åŠ©æ•°æ®é›†ã€æŒ‡ä»¤å¾®è°ƒä¸åå¥½æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆé›¶æ ‡ç­¾å¼ºåŒ–å­¦ä¹ ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLSC å±•ç°æ˜¾è‘—æå‡ï¼šAIME2024 å‡†ç¡®ç‡ +13.4%ã€MATH500 +21.2%ã€Minerva Math +21.7%ã€Olympiadbench +20.8%ã€AMC23 +9.7% ã€‚å³ä¾¿è®­ç»ƒæ•°æ®å°‘ã€è®¡ç®—æˆæœ¬ä½ï¼Œä¹Ÿèƒ½è®©å¼ºé¢„è®­ç»ƒæ¨¡å‹ç»“åˆ RLSC æ¡†æ¶ï¼Œåœ¨çŸ­è®­ç»ƒé˜¶æ®µæå‡ç½®ä¿¡åº¦ä¸æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨æ— ç‰¹å®šè¾…åŠ©æ•°æ®ã€äººå·¥åé¦ˆæ ‡æ³¨ä¸æ‰‹å·¥å¥–åŠ±å‡½æ•°ä¸‹çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°ï¼šRLSC å¼€è¾Ÿäº†â€œè‡ªç½®ä¿¡é©±åŠ¨å¼ºåŒ–å­¦ä¹ â€è·¯å¾„ï¼Œæ‘†è„±å¯¹å¤–éƒ¨ç›‘ç£ä¸å¤æ‚å¥–åŠ±å·¥ç¨‹çš„ä¾èµ–ï¼Œä¸ºå¤§æ¨¡å‹åè®­ç»ƒæä¾›è½»é‡ä¸”æ™®é€‚çš„æ–°æ€è·¯ï¼Œå°¤å…¶é€‚åˆèµ„æºå—é™åœºæ™¯ã€‚  
2. æŸå¤±è®¾è®¡ï¼šè‡ªç½®ä¿¡æŸå¤±çš„æ¨å¯¼ä¸å¹³æ»‘å˜ä½“çš„å®è·µï¼Œå±•ç¤ºäº†å¦‚ä½•ä»æ¨¡å‹å†…åœ¨è¾“å‡ºåˆ†å¸ƒä¸­æŒ–æ˜åé¦ˆä¿¡å·ï¼Œä¸ºåç»­æ— ç›‘ç£/è‡ªç›‘ç£å¾®è°ƒçš„æŸå¤±å‡½æ•°è®¾è®¡æä¾›å‚è€ƒã€‚  
3. è½åœ°å¯è¡Œæ€§ï¼šä»…éœ€å°‘é‡æ ·æœ¬ï¼ˆæ¯é—®é¢˜ 16 ä¸ªï¼‰ã€çŸ­è®­ç»ƒæ­¥æ•°ï¼ˆ10 - 20 æ­¥ï¼‰å°±èƒ½å®ç°æ€§èƒ½è·ƒå‡ï¼Œè¯æ˜è½»é‡è®­ç»ƒåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ½œåŠ›ï¼Œä¸ºå·¥ä¸šç•Œå¿«é€Ÿè¿­ä»£æ¨¡å‹æä¾›å€Ÿé‰´æ–¹å‘ã€‚  
```

## customizing-speech-recognition-model-with-large-language-model-feedback
### Abstract
Automatic speech recognition (ASR) systems have achieved strong performance
on general transcription tasks. However, they continue to struggle with
recognizing rare named entities and adapting to domain mismatches. In contrast,
large language models (LLMs), trained on massive internet-scale datasets, are
often more effective across a wide range of domains. In this work, we propose a
reinforcement learning based approach for unsupervised domain adaptation,
leveraging unlabeled data to enhance transcription quality, particularly the
named entities affected by domain mismatch, through feedback from a LLM. Given
contextual information, our framework employs a LLM as the reward model to
score the hypotheses from the ASR model. These scores serve as reward signals
to fine-tune the ASR model via reinforcement learning. Our method achieves a
21\% improvement on entity word error rate over conventional self-training
methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¤§è¯­è¨€æ¨¡å‹åé¦ˆå®šåˆ¶è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼šé¢†åŸŸé€‚é…æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é€šç”¨è½¬å½•ä»»åŠ¡ä¸Šå·²å–å¾—ä¸é”™æˆç»©ï¼Œä½†é¢å¯¹ç½•è§å‘½åå®ä½“è¯†åˆ«ä¸é¢†åŸŸä¸åŒ¹é…åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ã€‚ä¼ ç»Ÿè§£å†³é¢†åŸŸé€‚é…çš„è‡ªè®­ç»ƒæ–¹æ³•ä¾èµ–ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦åˆ†æ•°ï¼Œè€Œè¿™äº›åˆ†æ•°åœ¨æ–°é¢†åŸŸä¸­å¯é æ€§ä¸è¶³ã€‚åŒæ—¶ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®è®­ç»ƒï¼Œå…·å¤‡è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºASRåœ¨é¢†åŸŸé€‚é…éš¾é¢˜ä¸Šæä¾›äº†æ–°çªç ´å£ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨åˆ©ç”¨LLMåé¦ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°ASRçš„æ— ç›‘ç£é¢†åŸŸé€‚é…ï¼Œæå‡é¢†åŸŸä¸åŒ¹é…åœºæ™¯ä¸‹çš„è½¬å½•è´¨é‡ï¼Œå°¤å…¶æ˜¯å‘½åå®ä½“çš„è¯†åˆ«æ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£é¢†åŸŸé€‚é…æ¡†æ¶  
æå‡ºä¸€å¥—ç»“åˆLLMåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æµç¨‹æ¥å®ç°ASRæ— ç›‘ç£é¢†åŸŸé€‚é…ã€‚æ¡†æ¶åˆ†ä¸ºæ•°æ®æ”¶é›†ã€ç”Ÿæˆå¥–åŠ±ã€æ¨¡å‹å¾®è°ƒä¸‰æ­¥ï¼šå…ˆè®©é¢„è®­ç»ƒASRæ¨¡å‹ä¸ºç›®æ ‡åŸŸéŸ³é¢‘ç”Ÿæˆå€™é€‰å‡è®¾ï¼›å†ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚é¢†åŸŸå…ƒæ•°æ®ç­‰ï¼‰ï¼Œç”¨LLMè®¡ç®—å„å‡è®¾çš„å¥–åŠ±åˆ†æ•°ï¼›æœ€åç”¨RLç®—æ³•ä¾æ®å¥–åŠ±å¾®è°ƒASRæ¨¡å‹ï¼Œæ‘†è„±äº†ä¼ ç»Ÿè‡ªè®­ç»ƒå¯¹ä¸å¯é ç½®ä¿¡åº¦åˆ†æ•°çš„ä¾èµ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨LLMæ„å»ºå¥–åŠ±å‡½æ•°  
ä¸å•ç‹¬è®­ç»ƒASRä¸“ç”¨å¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯å€ŸåŠ©LLMå¯¹è¯­è¨€å’Œä¸Šä¸‹æ–‡çš„éšå¼ç†è§£æ¥ç”Ÿæˆå¥–åŠ±ã€‚å°†é¢†åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯æ•´åˆä¸ºæç¤ºè¯è¾“å…¥LLMï¼Œé€šè¿‡è®¡ç®—å‡è®¾åœ¨LLMä¸‹çš„å¯¹æ•°æ¦‚ç‡å’Œï¼ˆç»“åˆASRæ¨¡å‹è‡ªèº«åˆ†æ•°ï¼Œé€šè¿‡å‚æ•°Î»æ§åˆ¶æƒé‡ï¼‰ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè®©LLMçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ä¸ºASRé¢†åŸŸé€‚é…æä¾›æŒ‡å¯¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRLç®—æ³•é©±åŠ¨ASRå¾®è°ƒ  
æ¢ç´¢å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ç”¨äºASRæ¨¡å‹å¾®è°ƒï¼Œåˆ©ç”¨LLMç»™å‡ºçš„å¥–åŠ±ä¿¡å·ï¼Œè®©ASRæ¨¡å‹åœ¨æ— ç›‘ç£åœºæ™¯ä¸‹æœç€æ›´å¥‘åˆç›®æ ‡é¢†åŸŸè½¬å½•è´¨é‡çš„æ–¹å‘æ›´æ–°ï¼ŒåŒºåˆ«äºä¼ ç»Ÿä»…ä¾èµ–è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æˆ–äººå·¥åé¦ˆè®¾è®¡å¥–åŠ±çš„æ€è·¯ï¼ŒæŠŠLLMåé¦ˆæ·±åº¦èå…¥è®­ç»ƒè¿‡ç¨‹ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒèšç„¦é¢†åŸŸä¸åŒ¹é…åœºæ™¯ä¸‹å‘½åå®ä½“ç­‰è½¬å½•è´¨é‡æå‡ã€‚å¯¹æ¯”ä¼ ç»Ÿè‡ªè®­ç»ƒé¢†åŸŸé€‚é…æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å®ä½“è¯é”™è¯¯ç‡ï¼ˆEWERï¼‰ä¸Šå®ç°äº†é«˜è¾¾21%çš„ç›¸å¯¹æå‡ï¼Œæœ‰åŠ›è¯æ˜äº†LLMåé¦ˆåœ¨å¼•å¯¼ASRæ¨¡å‹é€‚é…è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†åŸºäºRLå’ŒLLMåé¦ˆçš„æ— ç›‘ç£é¢†åŸŸé€‚é…æ¡†æ¶çš„ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨æ¨¡æ€åä½œæ€è·¯ï¼šå±•ç¤ºäº†ASRä¸LLMç»“åˆçš„æ–°è·¯å¾„ï¼Œä¸å†å±€é™äºLLMåœ¨ASRåå¤„ç†ï¼ˆé‡æ’åºã€çº é”™ï¼‰ç¯èŠ‚ï¼Œè€Œæ˜¯è®©LLMæ·±åº¦å‚ä¸è®­ç»ƒé˜¶æ®µï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹åä½œæä¾›äº†æ–°èŒƒå¼å‚è€ƒã€‚  
2. æ— ç›‘ç£é€‚é…æ–¹æ¡ˆï¼šé’ˆå¯¹é¢†åŸŸé€‚é…ä¸­æ ‡ç­¾æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæä¾›äº†åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®+LLMåé¦ˆ+å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆæ— ç›‘ç£é€‚é…æ–¹æ¡ˆï¼Œåœ¨ä½èµ„æºã€é¢†åŸŸå·®å¼‚å¤§çš„åœºæ™¯ä¸‹æœ‰æ¨å¹¿æ½œåŠ›ã€‚  
3. å¥–åŠ±è®¾è®¡åˆ›æ–°ï¼šå€ŸåŠ©LLMå¤©ç„¶çš„è¯­è¨€ç†è§£å’Œè·¨åŸŸèƒ½åŠ›æ„å»ºå¥–åŠ±å‡½æ•°ï¼Œé¿å…äº†ä¼ ç»Ÿè‡ªè®­ç»ƒç½®ä¿¡åº¦åˆ†æ•°ä¸å¯é çš„ç¼ºé™·ï¼Œä¸ºåç»­æ¨¡å‹è¯„ä¼°ä¸å¥–åŠ±æœºåˆ¶è®¾è®¡æ‰“å¼€æ–°æ€è·¯ã€‚

## treerpo--tree-relative-policy-optimization
### Abstract
Large Language Models (LLMs) have shown remarkable reasoning capabilities
through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,
a key limitation of existing approaches is that rewards defined at the full
trajectory level provide insufficient guidance for optimizing the intermediate
steps of a reasoning process. To address this, we introduce \textbf{\name}, a
novel method that estimates the mathematical expectations of rewards at various
reasoning steps using tree sampling. Unlike prior methods that rely on a
separate step reward model, \name directly estimates these rewards through this
sampling process. Building on the group-relative reward training mechanism of
GRPO, \name innovatively computes rewards based on step-level groups generated
during tree sampling. This advancement allows \name to produce fine-grained and
dense reward signals, significantly enhancing the learning process and overall
performance of LLMs. Experimental results demonstrate that our \name algorithm
substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test
benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name
significantly outperforms GRPO by 2.9\% in performance while simultaneously
reducing the average response length by 18.1\%, showcasing its effectiveness
and efficiency. Our code will be available at
\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | TreeRPOï¼šç”¨æ ‘é‡‡æ ·é‡å¡‘å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–çš„å¥–åŠ±ä¿¡å·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å…³é”®å±€é™ï¼š**è½¨è¿¹çº§å¥–åŠ±å¯¹æ¨ç†ä¸­é—´æ­¥éª¤çš„ä¼˜åŒ–æŒ‡å¯¼ä¸è¶³**ã€‚åœ¨åŸºäºå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ä¸­ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è™½èƒ½æä¾›ç»†ç²’åº¦å¥–åŠ±ï¼Œä½†é«˜è´¨é‡æ ‡æ³¨æ•°æ®è·å–éš¾ï¼›è€Œæ— å¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ä»…æä¾›è½¨è¿¹çº§å¥–åŠ±ï¼Œå¯¹ä¸­é—´æ­¥éª¤ä¼˜åŒ–åŠ©åŠ›æœ‰é™ã€‚å¦‚ä½•åœ¨ä¸ä¾èµ–å¥–åŠ±æ¨¡å‹çš„å‰æä¸‹ï¼Œä¸ºæ¨ç†è¿‡ç¨‹æä¾›å¯†é›†ã€ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ‘é‡‡æ ·å®ç°æ— å¥–åŠ±æ¨¡å‹çš„è¿‡ç¨‹å¥–åŠ±ä¼°è®¡  
TreeRPO æå‡º**æ ‘é‡‡æ ·æœºåˆ¶**ï¼Œæ— éœ€æ˜¾å¼çš„æ­¥éª¤å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥é€šè¿‡æ ‘é‡‡æ ·æ¥ä¼°è®¡å„æ¨ç†æ­¥éª¤å¥–åŠ±çš„æ•°å­¦æœŸæœ›ã€‚å®ƒçªç ´äº†ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å•ç‹¬æ­¥éª¤å¥–åŠ±æ¨¡å‹çš„é™åˆ¶ï¼Œç”¨é‡‡æ ·è¿‡ç¨‹æœ¬èº«å®Œæˆå¥–åŠ±ä¼°è®¡ï¼Œä¸ºä¸­é—´æ¨ç†æ­¥éª¤æä¾›æ›´ç›´æ¥çš„åé¦ˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºGRPOæ¡†æ¶çš„æ­¥éª¤çº§ç»„ç›¸å¯¹å¥–åŠ±è®¡ç®—  
åœ¨GRPOçš„ç»„ç›¸å¯¹å¥–åŠ±è®­ç»ƒæœºåˆ¶åŸºç¡€ä¸Šï¼ŒTreeRPOåˆ›æ–°æ€§åœ°**åŸºäºæ ‘é‡‡æ ·ç”Ÿæˆçš„æ­¥éª¤çº§åˆ†ç»„æ¥è®¡ç®—å¥–åŠ±**ã€‚è¿™ä¸€è®¾è®¡è®©æ¨¡å‹èƒ½äº§ç”Ÿç»†ç²’åº¦ã€å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œåœ¨ä¿ç•™å¯éªŒè¯å¥–åŠ±å‡½æ•°æ‰©å±•æ€§ä¼˜åŠ¿çš„åŒæ—¶ï¼Œæ›´é«˜æ•ˆåœ°å¼•å¯¼æ¨ç†è¿‡ç¨‹ä¼˜åŒ–ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ¨ç†å‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼šåœ¨Qwen - 2.5 - Mathçš„æµ‹è¯•åŸºå‡†ä¸­ï¼ŒPass@1å‡†ç¡®ç‡ä»19.0%å¤§å¹…æå‡è‡³35.5%ï¼›  
2. æ€§èƒ½ä¸æ•ˆç‡åŒä¼˜ï¼šå¯¹æ¯”GRPOï¼ŒTreeRPOæ€§èƒ½é¢†å…ˆ2.9%ï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘18.1%ï¼Œè¯æ˜åœ¨æå‡æ¨ç†ç²¾åº¦çš„åŒæ—¶ï¼Œè¿˜èƒ½æ›´é«˜æ•ˆåœ°åˆ©ç”¨tokenèµ„æºï¼›  
3. å¯è§†åŒ–å¯¹æ¯”ï¼ˆå›¾1ï¼‰ï¼šåœ¨MATH - 500ã€OlympiadBenchç­‰å››ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTreeRPOçš„Pass@1å‡†ç¡®ç‡éšè®­ç»ƒæ­¥éª¤æ¨è¿›ï¼Œå§‹ç»ˆä¼˜äºGRPOï¼Œå±•ç°æŒç»­ä¼˜åŒ–èƒ½åŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ— å¥–åŠ±æ¨¡å‹çš„ç»†ç²’åº¦å¥–åŠ±è®¾è®¡æ€è·¯ï¼šTreeRPOè¯æ˜äº†åœ¨ä¸ä¾èµ–æ˜‚è´µæ ‡æ³¨æ•°æ®æ„å»ºå¥–åŠ±æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡é‡‡æ ·ä¸åˆ†ç»„æœºåˆ¶ä¹Ÿèƒ½ç”Ÿæˆè¿‡ç¨‹çº§å¥–åŠ±ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹çš„RLä¸LLMç»“åˆæä¾›æ–°è·¯å¾„ï¼›  
2. å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä¼˜åŒ–çš„åˆ›æ–°åº”ç”¨ï¼šå°†æ ‘ç»“æ„é‡‡æ ·ä¸ç»„ç›¸å¯¹å¥–åŠ±ç»“åˆï¼Œä¸ºå¤æ‚æ¨ç†ä»»åŠ¡ä¸­â€œä¸­é—´æ­¥éª¤ä¼˜åŒ–â€è¿™ä¸€éš¾é¢˜æä¾›äº†å¯è½åœ°çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œå¯å‘åç»­åœ¨RLä¸LLMæ¨ç†ç»“åˆæ–¹å‘çš„ç ”ç©¶ï¼›  
3. æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ï¼šå®éªŒä¸­åŒæ—¶å®ç°å‡†ç¡®ç‡æå‡ä¸å“åº”é•¿åº¦ç¼©çŸ­ï¼Œä½“ç°äº†æ–¹æ³•åœ¨å®é™…éƒ¨ç½²ä¸­çš„ä»·å€¼ï¼Œä¸ºè¿½æ±‚â€œé«˜æ•ˆæ¨ç†â€çš„å·¥ä¸šçº§åº”ç”¨æä¾›å‚è€ƒã€‚  
```

## rival--reinforcement-learning-with-iterative-and-adversarial-optimization-for-machine-translation
### Abstract
Large language models (LLMs) possess strong multilingual capabilities, and
combining Reinforcement Learning from Human Feedback (RLHF) with translation
tasks has shown great potential. However, we observe that this paradigm
performs unexpectedly poorly when applied to colloquial subtitle translation
tasks. In this work, we investigate this issue and find that the offline reward
model (RM) gradually diverges from the online LLM due to distributional shift,
ultimately leading to undesirable training outcomes. To address this, we
propose RIVAL, an adversarial training framework that formulates the process as
a min-max game between the RM and the LLM. RIVAL iteratively updates the both
models, with the RM trained to distinguish strong from weak translations
(qualitative preference reward), and the LLM trained to enhance its translation
for closing this gap. To stabilize training and improve generalizability, we
also incorporate quantitative preference reward (e.g., BLEU) into the RM,
enabling reference-free quality modeling aligned with human evaluation. Through
extensive experiments, we demonstrate that the proposed adversarial training
framework significantly improves upon translation baselines.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RIVALï¼šå¯¹æŠ—è¿­ä»£ä¼˜åŒ–ä¸‹çš„æœºå™¨ç¿»è¯‘å¼ºåŒ–å­¦ä¹ æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡å¼ºå¤§çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œå°†åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸ç¿»è¯‘ä»»åŠ¡ç»“åˆä¹Ÿå±•ç°å‡ºæ½œåŠ›ã€‚ä½†åœ¨å£è¯­åŒ–å­—å¹•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä¼ ç»ŸRLHFèŒƒå¼è¡¨ç°ä¸ä½³ã€‚ç ”ç©¶å‘ç°ï¼Œç¦»çº¿å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä¼šå› åˆ†å¸ƒåç§»ä¸åœ¨çº¿LLMé€æ¸è„±èŠ‚ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœå·®ï¼Œæ¯”å¦‚æ¨¡å‹ä¼šç”Ÿæˆæºæ–‡æœ¬æ²¡æœ‰çš„å†…å®¹â€œæ¬ºéª—â€RMè·å–é«˜åˆ†ï¼ˆå¥–åŠ±é»‘å®¢é—®é¢˜ï¼‰ã€‚åŒæ—¶ç°æœ‰NMTç³»ç»Ÿå¤šå…³æ³¨æ­£å¼ä¹¦é¢è¯­ï¼Œé’ˆå¯¹æ¾æ•£ç»“æ„ã€å£è¯­åŒ–å­—å¹•çš„ç ”ç©¶è¾ƒå°‘ï¼Œä¸ºå¡«è¡¥ç©ºç™½ï¼Œå›¢é˜Ÿå…ˆæ„å»ºä¸“ç”¨æ•°æ®é›†å¹¶å°è¯•RLHFä¼˜åŒ–ï¼Œå´å‘ç° vanilla RLHF æ•ˆæœæ¬ ä½³ï¼Œç”±æ­¤å‚¬ç”Ÿäº†è§£å†³åˆ†å¸ƒåç§»ç­‰é—®é¢˜çš„ç ”ç©¶åŠ¨æœºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRIVALå¯¹æŠ—è®­ç»ƒæ¡†æ¶  
å°†ä¼ ç»ŸRLHFè®­ç»ƒé‡æ„ä¸ºRMä¸LLMé—´çš„æ˜¾å¼ minimax åšå¼ˆã€‚RMè¢«è®­ç»ƒæ¥æœ€å¤§åŒ–å¼ºç¿»è¯‘ä¸å¼±ç¿»è¯‘å¯¹ä¹‹é—´çš„åˆ†æ•°å·®è·ï¼ˆå®šæ€§åå¥½å¥–åŠ±ï¼‰ï¼›LLMåˆ™è¢«è®­ç»ƒæ¥ç¼©å°è‡ªèº«ä¸å¼ºç¿»è¯‘åœ¨ç¿»è¯‘è´¨é‡ä¸Šçš„å·®è·ã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–ä¸¤ä¸ªæ¨¡å‹ï¼Œè®©RMæŒç»­é€‚åº”LLMè®­ç»ƒä¸­å¼•å…¥çš„åˆ†å¸ƒåç§»ï¼Œä»å¼±åˆ°å¼ºé€æ­¥æå‡æ€§èƒ½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥å®šé‡åå¥½å¥–åŠ±ç¨³å®šè®­ç»ƒ  
å‘ç°ä»…å®šæ€§åå¥½å¥–åŠ±ä¸‹ï¼Œéšè®­ç»ƒæ¨è¿›RMå¯èƒ½åç¦»çœŸå®å¥–åŠ±ä¿¡å·ï¼ŒåŸå› æ˜¯ç¿»è¯‘ä»»åŠ¡æ¢ç´¢ç©ºé—´å¤§ã€‚äºæ˜¯å¼•å…¥å¦‚BLEUç­‰å®šé‡åå¥½å¥–åŠ±åˆ°å¯¹æŠ—æ¡†æ¶ï¼Œæ‰©å±•RMä½¿å…¶åŒæ—¶é¢„æµ‹è¿™ç±»å®šé‡å¥–åŠ±ï¼Œå°†å…¶èå…¥æ¡†æ¶ã€‚æ—¢ç¨³å®šè¿­ä»£è®­ç»ƒè¿‡ç¨‹ï¼Œåˆèƒ½å®ç°æ›´å…·æ³›åŒ–æ€§çš„æ— å‚è€ƒç¿»è¯‘è´¨é‡å»ºæ¨¡ï¼Œè¿˜èƒ½é€šè¿‡é—´æ¥å‚è€ƒæŒ‡å¯¼ä¸äººç±»è¯„ä¼°æ ‡å‡†å¯¹é½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºå¹¶å¼€æºå£è¯­åŒ–å­—å¹•ç¿»è¯‘æ•°æ®é›†å¤„ç† pipeline  
æä¾›äº†å¤„ç†å£è¯­åŒ–ä¸­è‹±å¹³è¡Œå­—å¹•ç¿»è¯‘æ•°æ®é›†çš„ç®€æ´æœ‰æ•ˆæµç¨‹ï¼Œå¹¶å°†å¤„ç†åæ•°æ®ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä¸ºè¯¥é¢†åŸŸç ”ç©¶æä¾›æ•°æ®åŸºç¡€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œæ‰€æå‡ºçš„RIVALå¯¹æŠ—è®­ç»ƒæ¡†æ¶åœ¨ç¿»è¯‘åŸºçº¿ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œæœ‰æ•ˆæ”¹è¿›äº†åŸŸå†…ç¿»è¯‘è´¨é‡ï¼ŒåŒæ—¶æ²¡æœ‰æŸå®³æ¨¡å‹çš„åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰æ€§èƒ½ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨æå‡ç¿»è¯‘æ•ˆæœä¸ä¿æŒæ³›åŒ–èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹æŠ—è®­ç»ƒæ€è·¯è¿ç§»ï¼šæŠŠGANä¸­å¯¹æŠ—è®­ç»ƒèŒƒå¼è¿ç§»åˆ°RLHFä¸æœºå™¨ç¿»è¯‘ç»“åˆåœºæ™¯ï¼Œä¸ºè§£å†³åˆ†å¸ƒåç§»ç­‰é—®é¢˜æä¾›äº†æ–°è§†è§’ï¼Œè¿™ç§è·¨èŒƒå¼èåˆæ€è·¯å¯å€Ÿé‰´åˆ°å…¶ä»–å­˜åœ¨åˆ†å¸ƒå˜åŒ–ã€éœ€è¦åŠ¨æ€é€‚é…çš„ä»»åŠ¡ä¸­ã€‚  
2. å¤šç»´åº¦å¥–åŠ±èåˆï¼šç»“åˆå®šæ€§ä¸å®šé‡å¥–åŠ±æ¥ç¨³å®šè®­ç»ƒã€æå‡æ³›åŒ–æ€§ï¼Œåœ¨å¤„ç†å¤æ‚ç”Ÿæˆç±»ä»»åŠ¡ï¼ˆå¦‚å¯¹è¯ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦ç­‰ï¼‰æ—¶ï¼Œå¯å‚è€ƒè¿™ç§å¤šç»´åº¦ä¿¡å·ç»“åˆçš„æ–¹å¼ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚  
3. é¢†åŸŸæ•°æ®å»ºè®¾ï¼šé’ˆå¯¹å£è¯­åŒ–å­—å¹•è¿™ç±»ç ”ç©¶è¾ƒå°‘çš„é¢†åŸŸæ„å»ºæ•°æ®é›†å¹¶å¼€æºï¼Œä¸ºé¢†åŸŸå†…åç»­ç ”ç©¶é“ºç –ï¼Œå¯ç¤ºç ”ç©¶è€…å…³æ³¨å°ä¼—ä½†å®ç”¨çš„é¢†åŸŸæ•°æ®å»ºè®¾ï¼Œæ¨åŠ¨ç‰¹å®šåœºæ™¯ä¸‹æŠ€æœ¯å‘å±•ã€‚

## logicpuzzlerl--cultivating-robust-mathematical-reasoning-in-llms-via-reinforcement-learning
### Abstract
Large language models (LLMs) excel at many supervised tasks but often
struggle with structured reasoning in unfamiliar settings. This discrepancy
suggests that standard fine-tuning pipelines may instill narrow,
domain-specific heuristics rather than fostering general-purpose thinking
strategies. In this work, we propose a "play to learn" framework that
fine-tunes LLMs through reinforcement learning on a suite of seven custom logic
puzzles, each designed to cultivate distinct reasoning skills such as
constraint propagation, spatial consistency, and symbolic deduction. Using a
reinforcement learning setup with verifiable rewards, models receive binary
feedback based on puzzle correctness, encouraging iterative, hypothesis-driven
problem solving. We demonstrate that this training approach significantly
improves out-of-distribution performance on a range of mathematical benchmarks,
especially for mid-difficulty problems that require multi-step reasoning.
Analyses across problem categories and difficulty levels reveal that puzzle
training promotes transferable reasoning routines, strengthening algebraic
manipulation, geometric inference, and combinatorial logic, while offering
limited gains on rote or highly specialized tasks. These findings show that
reinforcement learning over logic puzzles reshapes the internal reasoning of
LLMs, enabling more robust and compositional generalization without relying on
task-specific symbolic tools.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LogicPuzzleRLï¼šç”¨å¼ºåŒ–å­¦ä¹ åŸ¹å…»å¤§æ¨¡å‹é²æ£’æ•°å­¦æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¸å¤šæœ‰ç›‘ç£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸ç†Ÿæ‚‰åœºæ™¯ä¸‹è¿›è¡Œç»“æ„åŒ–æ¨ç†æ—¶å´å¸¸é‡å›°å¢ƒã€‚è¿™ä¸€çŸ›ç›¾è¡¨æ˜ï¼Œæ ‡å‡†å¾®è°ƒæµç¨‹å¯èƒ½è®©æ¨¡å‹ä¹ å¾—ç‹­çª„ã€ç‰¹å®šé¢†åŸŸçš„å¯å‘å¼ç­–ç•¥ï¼Œè€Œéé€šç”¨çš„æ€ç»´æ–¹å¼ã€‚è€Œäººç±»å­¦ä¹ è€…èƒ½å‘å±•å‡ºå‡è®¾æ£€éªŒã€é€æ­¥æ¨ç†ã€çº¦æŸæ»¡è¶³ç­‰æŠ½è±¡æ€ç»´ç­–ç•¥æ¥åº”å¯¹å„ç±»é—®é¢˜ï¼Œå—æ­¤å¯å‘ï¼Œæœ¬æ–‡å¸Œæœ›æ‰“é€ ä¸€å¥—â€œä»¥ç©ä¿ƒå­¦â€æ¡†æ¶ï¼ŒåŠ©åŠ›LLMsåŸ¹å…»é€šç”¨æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡â€œä»¥ç©ä¿ƒå­¦â€æ¡†æ¶ä¸å®šåˆ¶åŒ–é€»è¾‘è°œé¢˜é›†  
æ„å»ºäº†åŒ…å«ä¸ƒä¸ªå®šåˆ¶é€»è¾‘è°œé¢˜çš„é›†åˆï¼Œæ¯ä¸ªè°œé¢˜é’ˆå¯¹ä¸åŒæ¨ç†æŠ€èƒ½ï¼ˆå¦‚çº¦æŸä¼ æ’­ã€ç©ºé—´ä¸€è‡´æ€§ã€ç¬¦å·æ¼”ç»ç­‰ï¼‰ï¼Œä¸”æ— èƒŒæ™¯çŸ¥è¯†æˆ–è®°å¿†æ¨¡æ¿èƒ½ç›´æ¥è§£å†³ï¼Œè¿«ä½¿æ¨¡å‹é€æ­¥æ¨ç†ã€‚å€ŸåŠ©å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œæ¨¡å‹ç”Ÿæˆå€™é€‰è§£åå¾—åˆ°æ­£ç¡®/é”™è¯¯çš„äºŒå…ƒåé¦ˆï¼Œæ¿€åŠ±å…¶è¿­ä»£å¼ã€å‡è®¾é©±åŠ¨å¼è§£é¢˜ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèšç„¦æ¨ç†èƒ½åŠ›è´¨æ€§è½¬å˜çš„è¯„ä¼°ä¸åˆ†æ  
ä¸æ»¡è¶³äºä¸‹æ¸¸ä»»åŠ¡å®šé‡æå‡ï¼Œè¿˜é€šè¿‡æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆæ¶µç›–ç®—æœ¯ã€ä»£æ•°ã€ç»„åˆæ•°å­¦ç­‰ç±»åˆ«ä¸ä¸åŒéš¾åº¦ï¼‰ï¼Œç»“åˆä¸­é—´æ¨ç†è½¨è¿¹åˆ†æï¼ˆå¦‚è§£çš„é•¿åº¦ç»“æ„ã€è‡ªæˆ‘ä¿®æ­£é¢‘ç‡ã€é”™è¯¯ä¼ æ’­ç­‰æŒ‡æ ‡ï¼‰ï¼Œæ¢ç©¶æ¨¡å‹å†…éƒ¨æ¨ç†çš„è´¨æ€§å˜åŒ–ï¼ŒéªŒè¯æ˜¯æ¨ç†èƒ½åŠ›æå‡è€Œéç‰¹å®šé¢†åŸŸå¯å‘å¼ç­–ç•¥ç§¯ç´¯å¸¦æ¥æ€§èƒ½æ”¹è¿›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šç±»æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»ä¸ƒè°œé¢˜è¯¾ç¨‹è®­ç»ƒçš„LLMsåœ¨å¤šæ•°ç±»åˆ«å’Œéš¾åº¦å±‚çº§ä¸Šæœ‰ç»Ÿè®¡æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨éœ€å¤šæ­¥æ¨ç†çš„ä¸­ç­‰éš¾åº¦é—®é¢˜ä¸Šè¡¨ç°çªå‡ºï¼›è·¨é—®é¢˜ç±»åˆ«ä¸éš¾åº¦çš„åˆ†ææ˜¾ç¤ºï¼Œè°œé¢˜è®­ç»ƒä¿ƒè¿›äº†å¯è¿ç§»æ¨ç†æµç¨‹ï¼Œå¢å¼ºäº†ä»£æ•°æ“ä½œã€å‡ ä½•æ¨ç†ã€ç»„åˆé€»è¾‘ç­‰èƒ½åŠ›ï¼Œä¸è¿‡åœ¨æ­»è®°ç¡¬èƒŒæˆ–é«˜åº¦ä¸“ä¸šåŒ–ä»»åŠ¡ä¸Šå¢ç›Šæœ‰é™ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç ”ç©¶æ€è·¯çœ‹ï¼Œå°†æ¸¸æˆåŒ–ã€è°œé¢˜åŒ–ä»»åŠ¡ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆæ¥å¡‘é€ æ¨¡å‹æ¨ç†æœºåˆ¶ï¼Œä¸ºæå‡å¤§æ¨¡å‹é€šç”¨æ¨ç†èƒ½åŠ›æä¾›äº†æ–°èŒƒå¼ï¼Œè·³å‡ºä»…ä¾èµ–ç‰¹å®šé¢†åŸŸå·¥å…·æˆ–æ¨¡æ¿çš„ä¼ ç»Ÿè·¯å¾„ï¼›åœ¨è¯„ä¼°ç»´åº¦ä¸Šï¼Œä¸ä»…å…³æ³¨ä»»åŠ¡è¡¨ç°è¿˜æ·±å…¥å‰–æå†…éƒ¨æ¨ç†è¿‡ç¨‹çš„è´¨æ€§å˜åŒ–ï¼Œè¿™ç§å…¨é¢è¯„ä¼°æ–¹å¼å¯ä¸ºåç»­å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ç ”ç©¶æä¾›å‚è€ƒï¼›ä»åº”ç”¨ä»·å€¼è®²ï¼Œè¯æ˜äº†é€šè¿‡å°è€Œå¯éªŒè¯çš„è°œé¢˜è®­ç»ƒèƒ½é‡æ„å¤§æ¨¡å‹å†…éƒ¨æ¨ç†æµç¨‹ï¼ŒåŠ©åŠ›å…¶åœ¨æ›´å¹¿æ³›æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›è½åœ°åˆ°å¤æ‚åœºæ™¯æä¾›äº†å®è·µæ–¹å‘ã€‚ 

