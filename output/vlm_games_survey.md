# Paper List of Terms(VLM+Games)
- [25/05] **DSADF: Thinking Fast and Slow for Decision Making**  
[[Paper](http://arxiv.org/pdf/2505.08189v1)] [[Code/Page]()] [[TLDR/Notes](#dsadf--thinking-fast-and-slow-for-decision-making)]

- [25/05] **Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.03792v1)] [[Code/Page](https://github.com/langfengQ/CoSo.)] [[TLDR/Notes](#towards-efficient-online-tuning-of-vlm-agents-via-counterfactual-soft-reinforcement-learning)]

- [25/04] **mrCAD: Multimodal Refinement of Computer-aided Designs**  
[[Paper](http://arxiv.org/pdf/2504.20294v1)] [[Code/Page]()] [[TLDR/Notes](#mrcad--multimodal-refinement-of-computer-aided-designs)]

- [25/04] **Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference**  
[[Paper](http://arxiv.org/pdf/2504.09620v1)] [[Code/Page]()] [[TLDR/Notes](#metropolis-hastings-captioning-game--knowledge-fusion-of-vision-language-models-via-decentralized-bayesian-inference)]

- [25/04] **BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing**  
[[Paper](http://arxiv.org/pdf/2504.01786v1)] [[Code/Page]()] [[TLDR/Notes](#blendergym--benchmarking-foundational-model-systems-for-graphics-editing)]

- [25/03] **Cultivating Game Sense for Yourself: Making VLMs Gaming Experts**  
[[Paper](http://arxiv.org/pdf/2503.21263v1)] [[Code/Page]()] [[TLDR/Notes](#cultivating-game-sense-for-yourself--making-vlms-gaming-experts)]

- [25/03] **MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse**  
[[Paper](http://arxiv.org/pdf/2503.18470v1)] [[Code/Page](https://github.com/PzySeere/MetaSpatial.)] [[TLDR/Notes](#metaspatial--reinforcing-3d-spatial-reasoning-in-vlms-for-the-metaverse)]

- [25/03] **GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training**  
[[Paper](http://arxiv.org/pdf/2503.08525v1)] [[Code/Page]()] [[TLDR/Notes](#gtr--guided-thought-reinforcement-prevents-thought-collapse-in-rl-based-vlm-agent-training)]

- [25/03] **GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks**  
[[Paper](http://arxiv.org/pdf/2503.06514v2)] [[Code/Page]()] [[TLDR/Notes](#gflowvlm--enhancing-multi-step-reasoning-in-vision-language-models-with-generative-flow-networks)]

- [25/03] **AVA: Attentive VLM Agent for Mastering StarCraft II**  
[[Paper](http://arxiv.org/pdf/2503.05383v4)] [[Code/Page](https://github.com/camel-ai/VLM-Play-StarCraft2.)] [[TLDR/Notes](#ava--attentive-vlm-agent-for-mastering-starcraft-ii)]



# TLDR/Notes
## dsadf--thinking-fast-and-slow-for-decision-making
### Abstract
Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼šå¿«æ€ä¸æ…¢æ€çš„å†³ç­–è‰ºæœ¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†åœ¨æ˜ç¡®å®šä¹‰çš„ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€è®¾ç½®ä¸­ï¼Œç”±äºä¾èµ–è¯•é”™äº¤äº’ï¼Œå®ƒä»¬å¾€å¾€éš¾ä»¥æ³›åŒ–å…¶å­¦åˆ°çš„ç­–ç•¥ã€‚è¿‘æœŸç ”ç©¶å°è¯•åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥æå‡RLä»£ç†çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ç¼ºä¹RLä»£ç†ä¸åŸºç¡€æ¨¡å‹ä¹‹é—´çš„æ— ç¼åè°ƒï¼Œå¯¼è‡´åœ¨ä¸ç†Ÿæ‚‰çš„ç¯å¢ƒä¸­åšå‡ºä¸åˆç†çš„å†³ç­–å’Œæ•ˆç‡ç“¶é¢ˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å†³ç­–å’Œæ³›åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å—åˆ°Kahnemançš„å¿«æ€ï¼ˆç³»ç»Ÿ1ï¼‰å’Œæ…¢æ€ï¼ˆç³»ç»Ÿ2ï¼‰ç†è®ºçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šç³»ç»Ÿ1ç”±RLä»£ç†å’Œå†…å­˜ç©ºé—´ç»„æˆï¼Œè´Ÿè´£å¿«é€Ÿç›´è§‚çš„å†³ç­–ï¼›ç³»ç»Ÿ2ç”±VLMé©±åŠ¨ï¼Œè´Ÿè´£æ·±å…¥åˆ†ææ€§æ¨ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
DSADFé€šè¿‡ç»“åˆç³»ç»Ÿ1å’Œç³»ç»Ÿ2çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜æ•ˆè‡ªé€‚åº”çš„å†³ç­–ã€‚ç³»ç»Ÿ2åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œä»æŒ‡ä»¤æç¤ºä¸­æå–çº¿ç´¢ï¼Œå°†é•¿æœŸä»»åŠ¡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å•æ­¥ä»»åŠ¡ï¼Œä½¿RLä»£ç†èƒ½å¤Ÿä¸“æ³¨äºè§£å†³æ¯ä¸ªå•æ­¥ä»»åŠ¡ï¼Œæ˜¾è‘—æé«˜å…¶å­¦ä¹ å’Œæ‰§è¡Œæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸¤ä¸ªè§†é¢‘æ¸¸æˆç¯å¢ƒCrafterå’ŒHousekeepä¸­è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œè¿™äº›ç¯å¢ƒå…·æœ‰é•¿æœŸä»»åŠ¡å’ŒRLä»£ç†æœªè§è¿‡çš„ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿ1å’Œç³»ç»Ÿ2ååŒå·¥ä½œï¼Œè¡¨ç°å‡ºè‰²ï¼Œæ— è®ºæ˜¯åœ¨æ•ˆç‡è¿˜æ˜¯æ³›åŒ–èƒ½åŠ›ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„DSADFæ¡†æ¶ä¸ºRLä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å†³ç­–æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ç»“åˆå¿«é€Ÿååº”èƒ½åŠ›å’Œæ·±å…¥æ¨ç†èƒ½åŠ›ï¼Œå¯¹äºæé«˜RLä»£ç†çš„æ³›åŒ–èƒ½åŠ›å’Œå†³ç­–æ•ˆç‡å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„å®è¯ç ”ç©¶ä¸ºåŒç³»ç»Ÿæ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§æä¾›äº†è¯æ®ã€‚

## towards-efficient-online-tuning-of-vlm-agents-via-counterfactual-soft-reinforcement-learning
### Abstract
Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œè¿ˆå‘é«˜æ•ˆåœ¨çº¿å¾®è°ƒï¼šVLMæ™ºèƒ½ä½“åœ¨Counterfactual Soft RLæ¡†æ¶ä¸‹çš„æ¢ç´¢ä¼˜åŒ–â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å†³ç­–åˆ¶å®šæ™ºèƒ½ä½“ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¦‚ä½•å°†è¿™äº›æ¨¡å‹é€šè¿‡åœ¨çº¿å¾®è°ƒé€‚åº”åŠ¨æ€ç¯å¢ƒï¼Œæˆä¸ºå½“å‰ç ”ç©¶çš„çƒ­ç‚¹ã€‚ç„¶è€Œï¼ŒVLMæ™ºèƒ½ä½“åœ¨é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œåœ¨çº¿å¾®è°ƒæ—¶ï¼Œé¢ä¸´ç€æ–‡æœ¬åŠ¨ä½œç©ºé—´çš„å¼€æ”¾æ€§å’ŒåŠ¨ä½œç”Ÿæˆéç«¯åˆ°ç«¯æ€§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¢ç´¢ç©ºé—´çˆ†ç‚¸å’Œæ¢ç´¢æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿å¾®è°ƒæ–¹æ³•ï¼Œä»¥æå‡VLMæ™ºèƒ½ä½“çš„æ¢ç´¢æ•ˆç‡å’Œæ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†Counterfactual Soft Reinforcement Learningï¼ˆCoSoï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨åäº‹å®æ¨ç†åŠ¨æ€è¯„ä¼°å•ä¸ªä»¤ç‰Œå¯¹åå¤„ç†åŠ¨ä½œçš„å› æœå½±å“ï¼Œä»è€Œåœ¨åŠ¨ä½œå…³é”®çš„ä»¤ç‰Œä¸Šä¼˜å…ˆè¿›è¡Œæ¢ç´¢ï¼Œå‡å°‘è¯­ä¹‰å†—ä½™æˆ–ä½å½±å“ä»¤ç‰Œçš„å½±å“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCoSoæ–¹æ³•åœ¨è½¯RLåŸºç¡€ä¸Šå¼•å…¥äº†å› æœåŠ æƒçš„ç†µæ­£åˆ™åŒ–ä¼˜åŒ–ï¼Œè¿™ä¸ä»…å‡å°‘äº†å†—ä½™çš„æ¢ç´¢ç©ºé—´ï¼Œè¿˜ä½¿å¾—VLMæ™ºèƒ½ä½“çš„æ–‡æœ¬è¾“å‡ºæ¢ç´¢ä¸æœ€ç»ˆè§£æåŠ¨ä½œçš„æ¢ç´¢æ›´åŠ ä¸€è‡´ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªæ™ºèƒ½ä½“ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼ŒåŒ…æ‹¬Androidè®¾å¤‡æ§åˆ¶ã€å¡ç‰Œæ¸¸æˆå’Œå…·èº«AIä»»åŠ¡ã€‚ç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨CoSoæ–¹æ³•çš„æ™ºèƒ½ä½“åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡æ€§èƒ½å¢ç›Šåˆ†åˆ«ä¸ºAndroidè®¾å¤‡æ§åˆ¶12.3%ï¼Œå¡ç‰Œæ¸¸æˆ9.3%ï¼Œå…·èº«AIä»»åŠ¡16.7%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºVLMæ™ºèƒ½ä½“çš„åœ¨çº¿å¾®è°ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¼€æ”¾æ€§æ–‡æœ¬åŠ¨ä½œç©ºé—´æ—¶ï¼Œå¦‚ä½•é€šè¿‡åäº‹å®æ¨ç†å’Œç†µæ­£åˆ™åŒ–ä¼˜åŒ–æ¢ç´¢è¿‡ç¨‹ï¼Œå¯¹äºæå‡æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ•ˆç‡å…·æœ‰é‡è¦å€Ÿé‰´æ„ä¹‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æä¾›çš„ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œä¹Ÿä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶è€…æä¾›äº†å®è´µçš„å‚è€ƒèµ„æºã€‚

## mrcad--multimodal-refinement-of-computer-aided-designs
### Abstract
A key feature of human collaboration is the ability to iteratively refine the
concepts we have communicated. In contrast, while generative AI excels at the
\textit{generation} of content, it often struggles to make specific
language-guided \textit{modifications} of its prior outputs. To bridge the gap
between how humans and machines perform edits, we present mrCAD, a dataset of
multimodal instructions in a communication game. In each game, players created
computer aided designs (CADs) and refined them over several rounds to match
specific target designs. Only one player, the Designer, could see the target,
and they must instruct the other player, the Maker, using text, drawing, or a
combination of modalities. mrCAD consists of 6,082 communication games, 15,163
instruction-execution rounds, played between 1,092 pairs of human players. We
analyze the dataset and find that generation and refinement instructions differ
in their composition of drawing and text. Using the mrCAD task as a benchmark,
we find that state-of-the-art VLMs are better at following generation
instructions than refinement instructions. These results lay a foundation for
analyzing and modeling a multimodal language of refinement that is not
represented in previous datasets.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "mrCADï¼šè¿ˆå‘å¤šæ¨¡æ€ååŒè®¾è®¡æ–°ç¯‡ç« "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œç”Ÿæˆå¼AIåœ¨å†…å®¹åˆ›é€ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ ¹æ®ç”¨æˆ·æŒ‡ç¤ºè¿›è¡Œå…·ä½“ä¿®æ”¹æ—¶å´å¾€å¾€åŠ›ä¸ä»å¿ƒã€‚ä¸ºäº†å¼¥è¡¥äººç±»ä¸æœºå™¨åœ¨ç¼–è¾‘æ–¹å¼ä¸Šçš„å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†mrCADæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€æŒ‡ä»¤äº¤æµæ¸¸æˆã€‚é€šè¿‡åˆ†æäººç±»ç©å®¶ä¹‹é—´çš„äº¤æµï¼Œè¯¥ç ”ç©¶æ—¨åœ¨æ¢ç´¢å’Œå»ºæ¨¡ä¸€ç§åœ¨å…ˆå‰æ•°æ®é›†ä¸­æœªå¾—åˆ°ä½“ç°çš„å¤šæ¨¡æ€ refinement è¯­è¨€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªç‹¬ç‰¹çš„mrCADä»»åŠ¡ï¼Œå…¶ä¸­ç©å®¶é€šè¿‡å¤šè½®äº¤æµï¼Œä½¿ç”¨æ–‡æœ¬ã€ç»˜å›¾æˆ–ä¸¤è€…çš„ç»„åˆæ¥æŒ‡å¯¼å¯¹æ–¹åˆ›å»ºå’Œä¿®æ”¹CADè®¾è®¡ã€‚è¿™ä¸ªè®¾ç½®ä¸ä»…äº§ç”Ÿäº†è‡ªç„¶çš„å¤šæ¨¡æ€äº¤äº’ï¼Œè¿˜æ¶‰åŠåˆ°ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œå³ refinement æŒ‡ä»¤éœ€è¦åœ¨å‰ä¸€è½®åˆ›å»ºçš„å†…å®¹çš„èƒŒæ™¯ä¸‹è¿›è¡Œè§£é‡Šã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
mrCADæ•°æ®é›†çš„æ„å»ºé‡‡ç”¨äº†ä¼—åŒ…æ–¹æ³•ï¼Œæ”¶é›†äº†6082ä¸ªäº¤æµæ¸¸æˆï¼Œå…±15163ä¸ªæŒ‡ä»¤æ‰§è¡Œè½®æ¬¡ã€‚æ•°æ®é›†ä¸­çš„CADè®¾è®¡æ¥æºäºSketchGraphsæ•°æ®é›†ï¼Œè¿™äº›è®¾è®¡æ˜¯é€šè¿‡ç®€å•çš„æ›²çº¿ï¼ˆå¦‚ç›´çº¿ã€åœ†å’Œå¼§ï¼‰ç»„åˆè€Œæˆçš„ä¸°å¯Œè¯­ä¹‰å¯¹è±¡ï¼Œéœ€è¦ç©å®¶åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œè§£æã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡ä½¿ç”¨mrCADä»»åŠ¡ä½œä¸ºåŸºå‡†ï¼Œç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨éµå¾ªç”ŸæˆæŒ‡ä»¤æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨éµå¾ªrefinementæŒ‡ä»¤æ–¹é¢å­˜åœ¨æ˜æ˜¾å·®è·ã€‚è¿™è¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤„ç†éœ€è¦ä¸Šä¸‹æ–‡ç†è§£å’Œå¤šæ¨¡æ€äº¤æµçš„ä»»åŠ¡æ—¶ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
mrCADæ•°æ®é›†å’ŒåŸºå‡†ä¸ºç†è§£å’Œå»ºæ¨¡äººç±»åœ¨å¤šæ¨¡æ€ refinement äº¤æµä¸­çš„è¡Œä¸ºæä¾›äº†ä¸€ä¸ªå®è´µçš„èµ„æºã€‚å®ƒä¸ä»…æ­ç¤ºäº†äººç±»åœ¨ç”Ÿæˆå’ŒrefinementæŒ‡ä»¤ä¸­ä½¿ç”¨å¤šæ¨¡æ€è¯­è¨€çš„ä¸åŒä¹‹å¤„ï¼Œè¿˜ä¸ºå¼€å‘èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œå¤æ‚ä¿®æ”¹æŒ‡ä»¤çš„äººå·¥æ™ºèƒ½æ¨¡å‹æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚æ­¤å¤–ï¼ŒmrCADçš„ä¼—åŒ…æ•°æ®æ”¶é›†æ–¹æ³•å’Œè¯„ä¼°ç¯å¢ƒä¹Ÿä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†å¯å€Ÿé‰´çš„èŒƒä¾‹ã€‚

## metropolis-hastings-captioning-game--knowledge-fusion-of-vision-language-models-via-decentralized-bayesian-inference
### Abstract
We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse
knowledge of multiple vision-language models (VLMs) by learning from each
other. Although existing methods that combine multiple models suffer from
inference costs and architectural constraints, MHCG avoids these problems by
performing decentralized Bayesian inference through a process resembling a
language game. The knowledge fusion process establishes communication between
two VLM agents alternately captioning images and learning from each other. We
conduct two image-captioning experiments with two VLMs, each pre-trained on a
different dataset. The first experiment demonstrates that MHCG achieves
consistent improvement in reference-free evaluation metrics. The second
experiment investigates how MHCG contributes to sharing VLMs' category-level
vocabulary by observing the occurrence of the vocabulary in the generated
captions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œMHCGï¼šå›¾åƒå­—å¹•æ–°ç­–ç•¥ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çŸ¥è¯†èåˆçš„è‰ºæœ¯â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å½“å‰çš„å›¾åƒå­—å¹•ç”Ÿæˆé¢†åŸŸï¼Œè™½ç„¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å•ä¸ªæ¨¡å‹å¾€å¾€å—é™äºå…¶è®­ç»ƒæ•°æ®å’Œæ¶æ„ï¼Œéš¾ä»¥å…¨é¢æ•æ‰å›¾åƒçš„ä¸°å¯Œä¿¡æ¯ã€‚ä¼ ç»Ÿçš„å¤šæ¨¡å‹èåˆæ–¹æ³•å¾€å¾€é¢ä¸´æ¨ç†æˆæœ¬é«˜å’Œæ¶æ„é™åˆ¶çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Metropolis-Hastings Captioning Gameï¼ˆMHCGï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ ç›¸äº’ä¹‹é—´çš„çŸ¥è¯†æ¥èåˆå¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä»è€Œæé«˜å›¾åƒå­—å¹•ç”Ÿæˆçš„è´¨é‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
MHCGé‡‡ç”¨äº†ä¸€ç§ç±»ä¼¼è¯­è¨€æ¸¸æˆçš„æ–¹å¼ï¼Œé€šè¿‡ä¸¤ä¸ªVLMä»£ç†äº¤æ›¿åœ°ä¸ºå›¾åƒç”Ÿæˆå­—å¹•ï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­ç›¸äº’å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å»ä¸­å¿ƒåŒ–çš„è´å¶æ–¯æ¨ç†ï¼Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿå¤šæ¨¡å‹èåˆæ–¹æ³•ä¸­çš„æ¨ç†æˆæœ¬å’Œæ¶æ„é™åˆ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
çŸ¥è¯†èåˆè¿‡ç¨‹ä¸ä»…æé«˜äº†å›¾åƒå­—å¹•çš„ç”Ÿæˆè´¨é‡ï¼Œè¿˜ä¿ƒè¿›äº†VLMä»£ç†ä¹‹é—´åœ¨ç±»åˆ«çº§åˆ«è¯æ±‡è¡¨çš„å…±äº«ã€‚é€šè¿‡è§‚å¯Ÿç”Ÿæˆå­—å¹•ä¸­è¯æ±‡çš„å‡ºç°æƒ…å†µï¼Œå®éªŒéªŒè¯äº†MHCGåœ¨ä¿ƒè¿›æ¨¡å‹é—´çŸ¥è¯†å…±äº«æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡ä¸¤ä¸ªå›¾åƒå­—å¹•ç”Ÿæˆå®éªŒæ¥éªŒè¯MHCGçš„æœ‰æ•ˆæ€§ã€‚ç¬¬ä¸€ä¸ªå®éªŒè¡¨æ˜ï¼ŒMHCGåœ¨æ— éœ€å‚è€ƒçš„è¯„ä»·æŒ‡æ ‡ä¸Šå®ç°äº†æŒç»­çš„æ”¹è¿›ã€‚ç¬¬äºŒä¸ªå®éªŒåˆ™é€šè¿‡è§‚å¯Ÿç”Ÿæˆå­—å¹•ä¸­è¯æ±‡çš„å‡ºç°ï¼Œå±•ç¤ºäº†MHCGå¦‚ä½•å¸®åŠ©VLMä»£ç†å…±äº«ç±»åˆ«çº§åˆ«çš„è¯æ±‡è¡¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MHCGæ–¹æ³•ä¸ºå›¾åƒå­—å¹•ç”Ÿæˆé¢†åŸŸæä¾›äº†ä¸€ç§æ–°é¢–çš„çŸ¥è¯†èåˆç­–ç•¥ï¼Œä¸ä»…æé«˜äº†ç”Ÿæˆå­—å¹•çš„è´¨é‡ï¼Œè¿˜é€šè¿‡æ¨¡å‹é—´çš„äº¤äº’å­¦ä¹ ä¿ƒè¿›äº†çŸ¥è¯†å…±äº«ã€‚è¿™ç§æ–¹æ³•å¯¹äºå¼€å‘æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„å¤šæ¨¡å‹èåˆç³»ç»Ÿå…·æœ‰å¯ç¤ºæ„ä¹‰ï¼Œå€¼å¾—ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è€…å’Œå¼€å‘è€…å…³æ³¨å’Œå€Ÿé‰´ã€‚

## blendergym--benchmarking-foundational-model-systems-for-graphics-editing
### Abstract
3D graphics editing is crucial in applications like movie production and game
design, yet it remains a time-consuming process that demands highly specialized
domain expertise. Automating this process is challenging because graphical
editing requires performing a variety of tasks, each requiring distinct skill
sets. Recently, vision-language models (VLMs) have emerged as a powerful
framework for automating the editing process, but their development and
evaluation are bottlenecked by the lack of a comprehensive benchmark that
requires human-level perception and presents real-world editing complexity. In
this work, we present BlenderGym, the first comprehensive VLM system benchmark
for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D
reconstruction tasks. We evaluate closed- and open-source VLM systems and
observe that even the state-of-the-art VLM system struggles with tasks
relatively easy for human Blender users. Enabled by BlenderGym, we study how
inference scaling techniques impact VLM's performance on graphics editing
tasks. Notably, our findings reveal that the verifier used to guide the scaling
of generation can itself be improved through inference scaling, complementing
recent insights on inference scaling of LLM generation in coding and math
tasks. We further show that inference compute is not uniformly effective and
can be optimized by strategically distributing it between generation and
verification.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "BlenderGymï¼šæ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3Då›¾å½¢ç¼–è¾‘é¢†åŸŸçš„çªç ´"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
3Då›¾å½¢ç¼–è¾‘åœ¨ç”µå½±åˆ¶ä½œå’Œæ¸¸æˆè®¾è®¡ç­‰é¢†åŸŸè‡³å…³é‡è¦ï¼Œä½†è¿™ä¸€è¿‡ç¨‹è€—æ—¶ä¸”éœ€è¦é«˜åº¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ã€‚å°½ç®¡è¿‘å¹´æ¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨åŒ–ç¼–è¾‘è¿‡ç¨‹ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬çš„å‘å±•å’Œè¯„ä¼°å´å—åˆ°ç¼ºä¹ä¸€ä¸ªå…¨é¢è¯„ä¼°æ ‡å‡†çš„é™åˆ¶ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•è¦ä¹ˆè¦†ç›–èŒƒå›´æœ‰é™ï¼Œè¦ä¹ˆç¼ºä¹å¯é ã€ç»æµçš„é‡åŒ–æŒ‡æ ‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†BlenderGymï¼Œä¸€ä¸ªé’ˆå¯¹3Då›¾å½¢ç¼–è¾‘çš„VLMç³»ç»Ÿå…¨é¢è¯„ä¼°åŸºå‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†BlenderGymï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–ä¸ªé’ˆå¯¹3Då›¾å½¢ç¼–è¾‘çš„VLMç³»ç»Ÿè¯„ä¼°åŸºå‡†ã€‚å®ƒé€šè¿‡ä»£ç åŸºç¡€çš„3Dåœºæ™¯é‡å»ºä»»åŠ¡æ¥è¯„ä¼°VLMç³»ç»Ÿï¼Œæ¶µç›–äº†å¯¹è±¡æ”¾ç½®ã€å…‰ç…§è°ƒæ•´ã€ç¨‹åºæ€§ææ–™ç¼–è¾‘ã€æ··åˆå½¢çŠ¶æ“ä½œå’Œç¨‹åºæ€§å‡ ä½•ç¼–è¾‘äº”å¤§å…³é”®ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åˆ©ç”¨BlenderGymï¼Œæœ¬æ–‡ç ”ç©¶äº†æ¨ç†æ—¶é—´æ‰©å±•æŠ€æœ¯å¦‚ä½•å½±å“VLMåœ¨å›¾å½¢ç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œä¸ä»…VLMä½œä¸ºç”Ÿæˆå™¨æ—¶å¯ä»¥ä»æ¨ç†æ—¶é—´æ‰©å±•ä¸­å—ç›Šï¼Œä½œä¸ºéªŒè¯å™¨å¼•å¯¼ç”Ÿæˆçš„VLMåŒæ ·å¯ä»¥ä»æ¨ç†æ—¶é—´æ‰©å±•ä¸­è·å¾—æå‡ã€‚æ­¤å¤–ï¼Œæ¨ç†è®¡ç®—çš„æ•ˆæœå¹¶ä¸æ˜¯å‡åŒ€çš„ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°åœ¨ç”Ÿæˆå’ŒéªŒè¯ä¹‹é—´åˆ†é…è®¡ç®—èµ„æºï¼Œå¯ä»¥ä¼˜åŒ–æ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯„ä¼°äº†é—­æºå’Œå¼€æºçš„å…ˆè¿›VLMç³»ç»Ÿåœ¨BlenderGymä¸Šçš„è¡¨ç°ï¼Œå¹¶å‘ç°å³ä½¿æ˜¯å…ˆè¿›çš„VLMç³»ç»Ÿï¼Œåœ¨é¢å¯¹å¯¹äºäººç±»Blenderç”¨æˆ·ç›¸å¯¹ç®€å•çš„ä»»åŠ¡æ—¶ä¹Ÿæ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚å®éªŒç»“æœè¿˜æ­ç¤ºäº†ï¼Œå½“è®¡ç®—èµ„æºå¢åŠ æ—¶ï¼Œå°†æ›´å¤šçš„èµ„æºåˆ†é…ç»™éªŒè¯å™¨å¯ä»¥å¸¦æ¥æ›´å¤§çš„æ”¶ç›Šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
BlenderGymä¸º3Då›¾å½¢ç¼–è¾‘é¢†åŸŸæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚å®ƒä¸ä»…æä¾›äº†ä¸€ä¸ªå›ºå®šçš„èµ·å§‹-ç›®æ ‡åœºæ™¯å¯¹æ¥é‡åŒ–ç¼–è¾‘ç»“æœï¼Œè¿˜å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ¨ç†æ—¶é—´æ‰©å±•æŠ€æœ¯æ¥ä¼˜åŒ–VLMçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°å¯¹äº3Då›¾å½¢ç¼–è¾‘çš„è‡ªåŠ¨åŒ–ä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹çš„åº”ç”¨éƒ½å…·æœ‰é‡è¦ä»·å€¼ã€‚

## cultivating-game-sense-for-yourself--making-vlms-gaming-experts
### Abstract
Developing agents capable of fluid gameplay in first/third-person games
without API access remains a critical challenge in Artificial General
Intelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) as
direct controllers, frequently pausing the game to analyze screens and plan
action through language reasoning. However, this inefficient paradigm
fundamentally restricts agents to basic and non-fluent interactions: relying on
isolated VLM reasoning for each action makes it impossible to handle tasks
requiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,
ACT combat). To handle this, we propose a paradigm shift in gameplay agent
design: instead of directly controlling gameplay, VLM develops specialized
execution modules tailored for tasks like shooting and combat. These modules
handle real-time game interactions, elevating VLM to a high-level developer.
Building upon this paradigm, we introduce GameSense, a gameplay agent framework
where VLM develops task-specific game sense modules by observing task execution
and leveraging vision tools and neural network training pipelines. These
modules encapsulate action-feedback logic, ranging from direct action rules to
neural network-based decisions. Experiments demonstrate that our framework is
the first to achieve fluent gameplay in diverse genres, including ACT, FPS, and
Flappy Bird, setting a new benchmark for game-playing agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©VLMæˆä¸ºæ¸¸æˆé«˜æ‰‹ï¼šè‡ªä¸»åŸ¹å…»æ¸¸æˆæ„Ÿçš„æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¼€å‘èƒ½å¤Ÿåœ¨æ²¡æœ‰APIè®¿é—®æƒé™çš„æƒ…å†µä¸‹æµç•…è¿›è¡Œç¬¬ä¸€äººç§°æˆ–ç¬¬ä¸‰äººç§°æ¸¸æˆçš„æ¸¸æˆæ™ºèƒ½ä½“ï¼Œä¸€ç›´æ˜¯äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•è™½ç„¶èƒ½å¤Ÿé€šè¿‡è§†è§‰ç†è§£æ¸¸æˆå±å¹•æ¥è¿›è¡Œäº¤äº’ï¼Œä½†å…¶æš‚åœ-è®¡åˆ’çš„èŒƒå¼é™åˆ¶äº†æ™ºèƒ½ä½“è¿›è¡Œé«˜ååº”æ€§æˆ–åŠ¨æ€é€‚åº”æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„æ¸¸æˆæ™ºèƒ½ä½“è®¾è®¡èŒƒå¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªèŒƒå¼è½¬å˜ï¼šVLMä¸å†ç›´æ¥æ§åˆ¶æ¸¸æˆï¼Œè€Œæ˜¯å¼€å‘ä¸“é—¨é’ˆå¯¹ä»»åŠ¡ï¼ˆå¦‚å°„å‡»å’Œæˆ˜æ–—ï¼‰çš„æ‰§è¡Œæ¨¡å—ã€‚è¿™äº›æ¨¡å—èƒ½å¤Ÿå¤„ç†å®æ—¶æ¸¸æˆäº¤äº’ï¼Œå°†VLMæå‡ä¸ºé«˜çº§å¼€å‘è€…ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åŸºäºè¿™ä¸€æ–°èŒƒå¼ï¼Œæœ¬æ–‡å¼•å…¥äº†GameSenseæ¡†æ¶ï¼ŒVLMé€šè¿‡è§‚å¯Ÿä»»åŠ¡æ‰§è¡Œå¹¶åˆ©ç”¨è§†è§‰å·¥å…·å’Œç¥ç»ç½‘ç»œè®­ç»ƒç®¡é“æ¥å¼€å‘ä»»åŠ¡ç‰¹å®šçš„æ¸¸æˆæ„Ÿæ¨¡å—ï¼ˆGSMï¼‰ã€‚è¿™äº›æ¨¡å—å°è£…äº†åŠ¨ä½œ-åé¦ˆé€»è¾‘ï¼Œä»ç›´æ¥çš„åŠ¨ä½œè§„åˆ™åˆ°åŸºäºç¥ç»ç½‘ç»œçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGameSenseæ¡†æ¶æ˜¯ç¬¬ä¸€ä¸ªåœ¨å¤šç§æ¸¸æˆç±»å‹ï¼ˆåŒ…æ‹¬ACTã€FPSå’ŒFlappy Birdï¼‰ä¸­å®ç°æµç•…æ¸¸æˆç©çš„æ™ºèƒ½ä½“ï¼Œä¸ºæ¸¸æˆç©æ³•çš„æ™ºèƒ½ä½“è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ä»…æŒ‡å‡ºäº†ç°æœ‰VLMæ¸¸æˆæ™ºèƒ½ä½“æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå³é€šè¿‡VLMåŸ¹å…»ä¸“é—¨çš„æ¸¸æˆæ„Ÿæ¨¡å—æ¥æå‡æ™ºèƒ½ä½“çš„å®æ—¶æ€§èƒ½å’Œé€‚åº”èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•å¯¹äºå¼€å‘æ— éœ€APIè®¿é—®å³å¯åœ¨å¤šæ ·åŒ–æ¸¸æˆç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚çš„æ™ºèƒ½ä½“å…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ã€‚

## metaspatial--reinforcing-3d-spatial-reasoning-in-vlms-for-the-metaverse
### Abstract
We present MetaSpatial, the first reinforcement learning (RL)-based framework
designed to enhance 3D spatial reasoning in vision-language models (VLMs),
enabling real-time 3D scene generation without the need for hard-coded
optimizations. MetaSpatial addresses two core challenges: (i) the lack of
internalized 3D spatial reasoning in VLMs, which limits their ability to
generate realistic layouts, and (ii) the inefficiency of traditional supervised
fine-tuning (SFT) for layout generation tasks, as perfect ground truth
annotations are unavailable. Our key innovation is a multi-turn RL-based
optimization mechanism that integrates physics-aware constraints and rendered
image evaluations, ensuring generated 3D layouts are coherent, physically
plausible, and aesthetically consistent. Methodologically, MetaSpatial
introduces an adaptive, iterative reasoning process, where the VLM refines
spatial arrangements over multiple turns by analyzing rendered outputs,
improving scene coherence progressively. Empirical evaluations demonstrate that
MetaSpatial significantly enhances the spatial consistency and formatting
stability of various scale models. Post-training, object placements are more
realistic, aligned, and functionally coherent, validating the effectiveness of
RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game
development applications. Our code, data, and training pipeline are publicly
available at https://github.com/PzySeere/MetaSpatial.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "MetaSpatialï¼šèµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹ä»¥3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¼€å¯å®æ—¶ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–°ç¯‡ç« "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œä¸‰ç»´åœºæ™¯ç”Ÿæˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”ŸæˆçœŸå®å¸ƒå±€æ–¹é¢å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¼ºä¹å†…éƒ¨åŒ–çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆçš„åœºæ™¯å¸ƒå±€ä¸å¤ŸçœŸå®ï¼›äºŒæ˜¯ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨å¸ƒå±€ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºå®Œç¾çš„åœ°é¢çœŸå®æ ‡æ³¨æ˜¯ä¸å¯ç”¨çš„ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶MetaSpatialï¼Œä»¥å¢å¼ºVLMsçš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå®ç°æ— éœ€ç¡¬ç¼–ç ä¼˜åŒ–çš„å®æ—¶3Dåœºæ™¯ç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
MetaSpatialå¼•å…¥äº†ä¸€ç§å¤šè½®æ¬¡çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ•´åˆäº†ç‰©ç†æ„ŸçŸ¥çº¦æŸå’Œæ¸²æŸ“å›¾åƒè¯„ä¼°ï¼Œç¡®ä¿ç”Ÿæˆçš„3Då¸ƒå±€å…·æœ‰è¿è´¯æ€§ã€ç‰©ç†å¯è¡Œæ€§å’Œå®¡ç¾ä¸€è‡´æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
MetaSpatialé‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”çš„è¿­ä»£æ¨ç†è¿‡ç¨‹ï¼Œå…¶ä¸­VLMé€šè¿‡åˆ†ææ¸²æŸ“è¾“å‡ºæ¥é€æ­¥æ”¹è¿›ç©ºé—´å¸ƒå±€ï¼Œä»è€Œæé«˜åœºæ™¯çš„è¿è´¯æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMetaSpatialæ˜¾è‘—æé«˜äº†å„ç§è§„æ¨¡æ¨¡å‹çš„ç©ºé—»ä¸€è‡´æ€§å’Œæ ¼å¼ç¨³å®šæ€§ã€‚ç»è¿‡è®­ç»ƒåï¼Œç‰©ä½“çš„æ”¾ç½®æ›´åŠ çœŸå®ã€å¯¹é½å’ŒåŠŸèƒ½ä¸Šè¿è´¯ï¼ŒéªŒè¯äº†RLåœ¨3Dç©ºé—´æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸º3Dåœºæ™¯ç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå³é€šè¿‡å¼ºåŒ–å­¦ä¹ è€Œéå›ºå®šçš„æ ‡æ³¨æ¥ä¼˜åŒ–ç©ºé—´å¸ƒå±€ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†åœºæ™¯ç”Ÿæˆçš„è´¨é‡ï¼Œè¿˜å‡å°‘äº†åå¤„ç†çš„è´Ÿæ‹…ï¼Œå¯¹äºå…ƒå®‡å®™ã€AR/VRã€æ•°å­—å­ªç”Ÿå’Œæ¸¸æˆå¼€å‘ç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä¸­æå‡ºçš„å¤šè½®æ¬¡æ¨ç†ç­–ç•¥å’Œä¸‰çº§çš„è¯„ä¼°æœºåˆ¶ä¸ºç±»ä¼¼ä»»åŠ¡æä¾›äº†å¯å€Ÿé‰´çš„æ€è·¯ã€‚

## gtr--guided-thought-reinforcement-prevents-thought-collapse-in-rl-based-vlm-agent-training
### Abstract
Reinforcement learning with verifiable outcome rewards (RLVR) has effectively
scaled up chain-of-thought (CoT) reasoning in large language models (LLMs).
Yet, its efficacy in training vision-language model (VLM) agents for
goal-directed action reasoning in visual environments is less established. This
work investigates this problem through extensive experiments on complex card
games, such as 24 points, and embodied tasks from ALFWorld. We find that when
rewards are based solely on action outcomes, RL fails to incentivize CoT
reasoning in VLMs, instead leading to a phenomenon we termed thought collapse,
characterized by a rapid loss of diversity in the agent's thoughts,
state-irrelevant and incomplete reasoning, and subsequent invalid actions,
resulting in negative rewards. To counteract thought collapse, we highlight the
necessity of process guidance and propose an automated corrector that evaluates
and refines the agent's reasoning at each RL step. This simple and scalable GTR
(Guided Thought Reinforcement) framework trains reasoning and action
simultaneously without the need for dense, per-step human labeling. Our
experiments demonstrate that GTR significantly enhances the performance and
generalization of the LLaVA-7b model across various visual environments,
achieving 3-5 times higher task success rates compared to SoTA models with
notably smaller model sizes.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "GTRï¼šå¼•å¯¼æ€ç»´å¼ºåŒ–å­¦ä¹ ï¼Œé˜²æ­¢è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„æ€ç»´å´©æºƒ"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæœºå™¨ç†è§£å’Œå¤„ç†æ–‡æœ¬ä¸å›¾åƒçš„èƒ½åŠ›æ˜¾è‘—å¢å¼ºã€‚ç„¶è€Œï¼Œå¦‚ä½•è®­ç»ƒVLMä»£ç†åœ¨åŠ¨æ€è§†è§‰ç¯å¢ƒä¸­æ ¹æ®æ„ŸçŸ¥ä¿¡æ¯æ¨æ–­æ­£ç¡®åŠ¨ä½œåºåˆ—ï¼Œä»¥å®Œæˆç‰¹å®šç›®æ ‡ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒä¸­VLMä»£ç†é¢ä¸´çš„â€œæ€ç»´å´©æºƒâ€é—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å‘ç°ï¼Œå½“ä»…åŸºäºåŠ¨ä½œç»“æœæ¥è®¾è®¡å¥–åŠ±æ—¶ï¼ŒRLæ— æ³•æ¿€åŠ±VLMä¸­çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œåè€Œä¼šå¯¼è‡´â€œæ€ç»´å´©æºƒâ€ç°è±¡ã€‚è¿™ç§ç°è±¡è¡¨ç°ä¸ºä»£ç†æ€ç»´çš„å¤šæ ·æ€§è¿…é€Ÿä¸§å¤±ï¼Œæ¨ç†ä¸çŠ¶æ€æ— å…³ä¸”ä¸å®Œæ•´ï¼Œè¿›è€Œå¯¼è‡´æ— æ•ˆåŠ¨ä½œå’Œè´Ÿé¢å¥–åŠ±ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†å¯¹æŠ—æ€ç»´å´©æºƒï¼Œæœ¬æ–‡å¼ºè°ƒäº†è¿‡ç¨‹æŒ‡å¯¼çš„å¿…è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç®€å•å¯æ‰©å±•çš„GTRï¼ˆå¼•å¯¼æ€ç»´å¼ºåŒ–ï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨è¯„ä¼°å’Œç»†åŒ–ä»£ç†åœ¨æ¯ä¸ªRLæ­¥éª¤ä¸­çš„æ¨ç†ï¼ŒåŒæ—¶ä¼˜åŒ–ä»£ç†çš„æ€ç»´å’ŒåŠ¨ä½œã€‚GTRæ¡†æ¶ä¸ä¾èµ–äºç»†è‡´çš„äººå·¥æ ‡æ³¨æˆ–é¢å¤–è®­ç»ƒï¼Œæä¾›äº†æ›´ä¸°å¯Œçš„è¿‡ç¨‹ç›‘ç£ï¼ŒåŒæ—¶ä¿æŒäº†RLVRçš„çµæ´»æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å°†GTRåº”ç”¨äºè®­ç»ƒLLaVA-7bæ¨¡å‹ï¼Œå¹¶åœ¨24ç‚¹çº¸ç‰Œæ¸¸æˆå’ŒALFWorldçš„å…·èº«ä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒGTRæ˜¾è‘—æé«˜äº†LLaVA-7bæ¨¡å‹åœ¨å„ç§è§†è§‰ç¯å¢ƒä¸‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†3-5å€ï¼Œä¸”æ¨¡å‹å°ºå¯¸æ›´å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GTRæ¡†æ¶ä¸ºè®­ç»ƒVLMä»£ç†æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦å¤æ‚æ¨ç†çš„åŠ¨æ€è§†è§‰ç¯å¢ƒã€‚GTRæ¡†æ¶çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯¹äºå…¶ä»–ç±»ä¼¼é—®é¢˜çš„è§£å†³å…·æœ‰å€Ÿé‰´æ„ä¹‰ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè‡ªåŠ¨æ€ç»´æ ¡æ­£ã€æ ¼å¼å¥–åŠ±å’Œé‡å¤æƒ©ç½šã€å·¥å…·ä½¿ç”¨çš„å‡†ç¡®æ€§ä»¥åŠé€šè¿‡æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å‡è½»åˆ†å¸ƒåç§»ç­‰ã€‚

## gflowvlm--enhancing-multi-step-reasoning-in-vision-language-models-with-generative-flow-networks
### Abstract
Vision-Language Models (VLMs) have recently shown promising advancements in
sequential decision-making tasks through task-specific fine-tuning. However,
common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),
present notable limitations: SFT assumes Independent and Identically
Distributed (IID) data, while PPO focuses on maximizing cumulative rewards.
These limitations often restrict solution diversity and hinder generalization
in multi-step reasoning tasks. To address these challenges, we introduce a
novel framework, GFlowVLM, a framework that fine-tune VLMs using Generative
Flow Networks (GFlowNets) to promote generation of diverse solutions for
complex reasoning tasks. GFlowVLM models the environment as a non-Markovian
decision process, allowing it to capture long-term dependencies essential for
real-world applications. It takes observations and task descriptions as inputs
to prompt chain-of-thought (CoT) reasoning which subsequently guides action
selection. We use task based rewards to fine-tune VLM with GFlowNets. This
approach enables VLMs to outperform prior fine-tuning methods, including SFT
and RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex
tasks such as card games (NumberLine, BlackJack) and embodied planning tasks
(ALFWorld), showing enhanced training efficiency, solution diversity, and
stronger generalization capabilities across both in-distribution and
out-of-distribution scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GFlowVLMï¼šåˆ©ç”¨ç”Ÿæˆæµç½‘ç»œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é¡ºåºå†³ç­–ä»»åŠ¡ä¸­é€šè¿‡ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ˜¾ç¤ºäº†æœ‰å¸Œæœ›çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå¸¸è§çš„å¾®è°ƒæ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–PPOï¼‰ï¼Œå­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚SFTå‡è®¾æ•°æ®æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œè€ŒPPOä¸“æ³¨äºæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚è¿™äº›é™åˆ¶é€šå¸¸é™åˆ¶äº†è§£å†³æ–¹æ¡ˆçš„å¤šæ ·æ€§ï¼Œé˜»ç¢äº†å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶GFlowVLMï¼Œå®ƒä½¿ç”¨ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰æ¥å¾®è°ƒVLMsï¼Œä»¥ä¿ƒè¿›å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆçš„ç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
GFlowVLMæ¡†æ¶å°†ç¯å¢ƒå»ºæ¨¡ä¸ºéé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œè¿™ä½¿å…¶èƒ½å¤Ÿæ•æ‰åˆ°å¯¹ç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚å®ƒå°†è§‚å¯Ÿå’Œä»»åŠ¡æè¿°ä½œä¸ºè¾“å…¥ï¼Œä»¥æ¿€å‘é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œè¿›è€ŒæŒ‡å¯¼åŠ¨ä½œé€‰æ‹©ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
GFlowVLMä½¿ç”¨åŸºäºä»»åŠ¡çš„å¥–åŠ±æ¥å¾®è°ƒVLMä¸GFlowNetsã€‚è¿™ç§æ–¹æ³•ä½¿VLMsåœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­è¶…è¶Šä¹‹å‰çš„å¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬SFTå’ŒRLã€‚GFlowVLMé€šè¿‡éšå¼åœ°å°†æ¨ç†è¡¨ç¤ºä¸ºæ ‘ç»“æ„ï¼Œå¢å¼ºäº†å¤šæ ·åŒ–å¤æ‚æ¨ç†åºåˆ—çš„å­¦ä¹ æ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®è¯ç»“æœæ˜¾ç¤ºï¼ŒGFlowVLMåœ¨è¯¸å¦‚çº¸ç‰Œæ¸¸æˆï¼ˆNumberLineã€BlackJackï¼‰å’Œå…·èº«è§„åˆ’ä»»åŠ¡ï¼ˆALFWorldï¼‰ç­‰å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå¢å¼ºçš„è®­ç»ƒæ•ˆç‡ã€è§£å†³æ–¹æ¡ˆå¤šæ ·æ€§å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºåˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–åœºæ™¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GFlowVLMæ¡†æ¶ä¸ºå¤„ç†å¤šæ¨¡æ€ã€é¡ºåºæ¨ç†ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›éœ€è¦é•¿æœŸä¾èµ–å…³ç³»æ•æ‰çš„å¤æ‚è§„åˆ’ç¯å¢ƒã€‚GFlowNetsä¸VLMçš„é›†æˆä¸ä»…æé«˜äº†æ¨¡å‹åœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè¿˜å±•ç¤ºäº†åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¹‹å¤–çš„åº”ç”¨æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒGFlowVLMåœ¨å®éªŒä¸­å±•ç°å‡ºçš„é«˜æ•ˆè®­ç»ƒå’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ava--attentive-vlm-agent-for-mastering-starcraft-ii
### Abstract
We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that
aligns artificial agent perception with the human gameplay experience.
Traditional frameworks such as SMAC rely on abstract state representations that
diverge significantly from human perception, limiting the ecological validity
of agent behavior. Our agent addresses this limitation by incorporating RGB
visual inputs and natural language observations that more closely simulate
human cognitive processes during gameplay. The AVA architecture consists of
three integrated components: (1) a vision-language model enhanced with
specialized self-attention mechanisms for strategic unit targeting and
battlefield assessment, (2) a retrieval-augmented generation system that
leverages domain-specific StarCraft II knowledge to inform tactical decisions,
and (3) a dynamic role-based task distribution system that enables coordinated
multi-agent behavior. The experimental evaluation in our proposed AVACraft
environment, which contains 21 multimodal StarCraft II scenarios, demonstrates
that AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can
execute complex tactical maneuvers without explicit training, achieving
comparable performance to traditional MARL methods that require substantial
training iterations. This work establishes a foundation for developing
human-aligned StarCraft II agents and advances the broader research agenda of
multimodal game AI. Our implementation is available at
https://github.com/camel-ai/VLM-Play-StarCraft2.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "AVAï¼šå¼•é¢†æ˜Ÿé™…äº‰éœ¸IIæ™ºèƒ½ä½“æ–°çºªå…ƒ"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰é¢†åŸŸï¼Œæ˜Ÿé™…äº‰éœ¸IIå·²æˆä¸ºä¸€ä¸ªé‡è¦çš„åŸºå‡†æµ‹è¯•ç¯å¢ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶æ¡†æ¶å¦‚SMACç­‰ï¼Œé€šå¸¸ä¾èµ–äºä¸äººç±»æ„ŸçŸ¥æœ‰è¾ƒå¤§å·®å¼‚çš„æŠ½è±¡çŠ¶æ€è¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†æ™ºèƒ½ä½“è¡Œä¸ºçš„ç”Ÿæ€æ•ˆåº¦ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“AVAï¼Œä½¿æ™ºèƒ½ä½“çš„æ„ŸçŸ¥æ–¹å¼æ›´æ¥è¿‘äºäººç±»çš„æ¸¸æˆä½“éªŒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†AVACraftç¯å¢ƒï¼Œå®ƒé‡æ–°è®¾è®¡äº†è§‚å¯Ÿç©ºé—´ï¼Œä½¿å…¶ä¸äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ä¿æŒä¸€è‡´ã€‚è¿™ä¸ªç¯å¢ƒåŒ…å«äº†RGBè§†è§‰è¾“å…¥å’Œè‡ªç„¶è¯­è¨€æè¿°ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿä»¥ç±»ä¼¼äºäººç±»ç©å®¶çš„æ–¹å¼å¤„ç†æˆ˜åœºä¿¡æ¯ï¼Œä»è€Œä¿ƒè¿›æ›´ç›´è§‚çš„æˆ˜ç•¥å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
AVAæ¶æ„ç”±ä¸‰ä¸ªé›†æˆçš„ç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰ä¸€ç§å¢å¼ºçš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæˆ˜ç•¥å•ä½å®šä½å’Œæˆ˜åœºè¯„ä¼°ï¼›ï¼ˆ2ï¼‰ä¸€ç§æ£€ç´¢å¢å¼ºçš„ç”Ÿæˆç³»ç»Ÿï¼Œåˆ©ç”¨é¢†åŸŸç‰¹å®šçš„æ˜Ÿé™…äº‰éœ¸IIçŸ¥è¯†æ¥æŒ‡å¯¼æˆ˜æœ¯å†³ç­–ï¼›ï¼ˆ3ï¼‰ä¸€ç§åŠ¨æ€è§’è‰²åˆ†é…ç³»ç»Ÿï¼Œå®ç°åè°ƒçš„å¤šæ™ºèƒ½ä½“è¡Œä¸ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AVACraftç¯å¢ƒä¸­ï¼Œé€šè¿‡21ä¸ªå¤šæ¨¡æ€æ˜Ÿé™…äº‰éœ¸IIåœºæ™¯çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒAVAåœ¨æœªç»è¿‡æ˜¾å¼è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æˆ˜æœ¯æœºåŠ¨ï¼Œå…¶æ€§èƒ½ä¸éœ€è¦å¤§é‡è®­ç»ƒè¿­ä»£çš„ä¼ ç»ŸMARLæ–¹æ³•ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºå¼€å‘ä¸äººç±»æ„ŸçŸ¥å¯¹é½çš„æ˜Ÿé™…äº‰éœ¸IIæ™ºèƒ½ä½“å¥ å®šäº†åŸºç¡€ï¼Œå¹¶æ¨åŠ¨äº†å¤šæ¨¡æ€æ¸¸æˆAIç ”ç©¶çš„å¹¿æ³›è®®ç¨‹ã€‚ç ”ç©¶ä¸­çš„æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œå¦‚å¤šæ¨¡æ€æ„ŸçŸ¥ã€è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶ä»¥åŠåŠ¨æ€è§’è‰²åˆ†é…ç³»ç»Ÿï¼Œå¯¹äºæœªæ¥æ™ºèƒ½ä½“è®¾è®¡å’Œæ¸¸æˆAIç ”ç©¶å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

