# Paper List of Terms(Medical+RL)
- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.13973v1)] [[Code/Page]()] [[TLDR/Notes](#toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models)]

- [25/05] **Disentangling Reasoning and Knowledge in Medical Large Language Models**  
[[Paper](http://arxiv.org/pdf/2505.11462v1)] [[Code/Page]()] [[TLDR/Notes](#disentangling-reasoning-and-knowledge-in-medical-large-language-models)]

- [25/05] **Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner**  
[[Paper](http://arxiv.org/pdf/2505.11404v1)] [[Code/Page](https://github.com/Wenchuan-Zhang/Patho-R1.)] [[TLDR/Notes](#patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner)]

- [25/04] **Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation**  
[[Paper](http://arxiv.org/pdf/2504.18453v1)] [[Code/Page]()] [[TLDR/Notes](#reason-like-a-radiologist--chain-of-thought-and-reinforcement-learning-for-verifiable-report-generation)]



# TLDR/Notes
## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œå°‘é‡æ•°æ®è®­ç»ƒæœç´¢ä»£ç†ï¼Œs3æ¡†æ¶å¼•é¢†RLæ–°ç¯‡ç« â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰ç³»ç»Ÿçš„å…´èµ·ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¿é—®å¤–éƒ¨çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨ä»…å…³æ³¨æœç´¢çš„æŒ‡æ ‡ï¼ˆå¦‚NDCGï¼‰æ¥ä¼˜åŒ–æ£€ç´¢ï¼Œå¿½ç•¥ä¸‹æ¸¸ä»»åŠ¡çš„å®é™…æ•ˆç”¨ï¼Œè¦ä¹ˆé€šè¿‡å¾®è°ƒæ•´ä¸ªLLMæ¥åŒæ—¶è¿›è¡Œæ¨ç†å’Œæ£€ç´¢ï¼Œå¯¼è‡´æ£€ç´¢ä¸ç”Ÿæˆç´§å¯†è€¦åˆï¼Œé™åˆ¶äº†æœç´¢çš„å®é™…æ•ˆç”¨å’Œä¸å†»ç»“æˆ–ä¸“æœ‰æ¨¡å‹çš„å…¼å®¹æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„æ¡†æ¶s3ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒæœç´¢ä»£ç†ï¼Œè€Œæ— éœ€å¾®è°ƒç”Ÿæˆæ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†s3æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æœç´¢ä»£ç†ä¸ç”Ÿæˆæ¨¡å‹è§£è€¦ï¼Œä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæœç´¢ä»£ç†ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æ–°é¢–çš„å¥–åŠ±ä¿¡å·â€”â€”è¶…è¿‡RAGçš„å¢ç›Šï¼ˆGain Beyond RAG, GBRï¼‰ï¼Œè¯¥ä¿¡å·é‡åŒ–äº†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å¯¹ç”Ÿæˆå‡†ç¡®æ€§çš„æ”¹å–„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
s3æ¡†æ¶åœ¨ä»…æœ‰2.4kè®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå°±èƒ½è¶…è¿‡åŸºäºè¶…è¿‡70å€æ•°æ®è®­ç»ƒçš„åŸºçº¿ï¼Œè¿™åœ¨å…­ä¸ªé€šç”¨é—®ç­”å’Œäº”ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´è¡¨ç°å‡ºæ›´å¼ºå¤§çš„ä¸‹æ¸¸æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œs3æ¡†æ¶åœ¨å…­ä¸ªé€šç”¨é—®ç­”å’Œäº”ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨æ¯”åŸºçº¿å°‘70å€çš„æ•°æ®ï¼Œå¹³å‡å¾—åˆ†æ˜¾è‘—æé«˜ã€‚è¿™è¯æ˜äº†s3æ¡†æ¶åœ¨å°‘é‡æ•°æ®ä¸‹è®­ç»ƒæœç´¢ä»£ç†çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„s3æ¡†æ¶ä¸ºå¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆè®­ç»ƒæœç´¢ä»£ç†æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯å…¶åˆ›æ–°çš„GBRå¥–åŠ±ä¿¡å·å’Œæ¨¡å‹è§£è€¦çš„è®­ç»ƒæ–¹æ³•ï¼Œå¯¹äºæé«˜æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½å…·æœ‰é‡è¦ä½œç”¨ã€‚æ­¤å¤–ï¼Œs3æ¡†æ¶åœ¨å°‘é‡æ•°æ®ä¸Šçš„é«˜æ•ˆæ€§èƒ½ï¼Œå¯¹äºèµ„æºå—é™çš„åœºæ™¯å°¤å…¶æœ‰ä»·å€¼ã€‚

## toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models
### Abstract
Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "å¼ºåŒ–å­¦ä¹ å¾®è°ƒåŠ©åŠ›åŒ»å­¦è§†è§‰é—®ç­”ï¼šè¿ˆå‘ä¸´åºŠç²¾å‡†çš„AIä¹‹è·¯"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼•å…¥ï¼ŒAIé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†è¿™ä¸€æŠ€æœ¯åº”ç”¨äºåŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡æ—¶ï¼Œå¦‚ä½•ç¡®ä¿æ¨¡å‹çš„è¾“å‡ºç¬¦åˆä¸´åºŠæœŸæœ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•é€šè¿‡RLå¾®è°ƒç­–ç•¥ï¼Œä½¿åŒ»å­¦MLLMsçš„å“åº”æ›´è´´è¿‘ä¸´åºŠå®é™…éœ€æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆå§‹åŒ–ç­–ç•¥çš„é€‰æ‹©
æœ¬æ–‡å¯¹æ¯”äº†ä»é›¶å¼€å§‹è®­ç»ƒä¸åŸºäºæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹åœ¨åŒ»å­¦VQAä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä»é›¶å¼€å§‹è®­ç»ƒå¯ä»¥é¼“åŠ±æ›´å¤šçš„æ¨ç†æ¢ç´¢ï¼Œä½†ç¼ºä¹æŒ‡ä»¤å¾®è°ƒå¸¦æ¥çš„åŒ»å­¦çŸ¥è¯†å’Œè¯­è¨€æµç•…æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒ»å­¦è¯­ä¹‰å¯¹é½
ä¸ºäº†æé«˜æ¨¡å‹æ¨ç†è·¯å¾„ä¸ç›®æ ‡ä»»åŠ¡çš„åŒ¹é…åº¦ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŒ»å­¦è¯­ä¹‰å¯¹é½å¥–åŠ±ï¼Œé€šè¿‡å‚è€ƒé¢„å…ˆå®šä¹‰çš„ä¸“å®¶LLMsçš„åˆ¤æ–­ï¼Œé¼“åŠ±æ¨¡å‹å“åº”ä¸è¿™äº›åˆ¤æ–­ç›¸åŒ¹é…ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé•¿åº¦åŸºå¥–åŠ±å¯¹é•¿é“¾æ¨ç†çš„å½±å“
æœ¬æ–‡æ¢è®¨äº†ä»…ä¾èµ–é•¿åº¦åŸºå¥–åŠ±ï¼ˆå¦‚æ‰©å±•é“¾å¥–åŠ±ï¼ˆECRï¼‰å’Œæ­£ç¡®æ€§åŠ æƒé•¿åº¦å¥–åŠ±ï¼ˆCWRï¼‰ï¼‰æ˜¯å¦æœ‰åŠ©äºåŒ»å­¦VQAä¸­çš„æœ‰æ„ä¹‰çš„é•¿å½¢å¼æ¨ç†ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•å¾€å¾€å¯¼è‡´å›ç­”å†—é•¿ä¸”å‡†ç¡®æ€§è¾ƒä½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåŒ»å­¦MLLMsä¸­çš„åå·®
æœ¬æ–‡é€šè¿‡å®æ–½Dr.GRPOæ–¹æ³•ï¼Œè¯„ä¼°äº†é—®é¢˜çº§å½’ä¸€åŒ–å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼Œå‘ç°è¿™ç§æ–¹æ³•å¯ä»¥å‡å°‘å› å½’ä¸€åŒ–å¯¼è‡´çš„åå·®ï¼Œæé«˜å›ç­”çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šSFTä¸GRPO-based RLå¾®è°ƒçš„æ¯”è¾ƒ
æœ¬æ–‡æ¯”è¾ƒäº†æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸GRPO-based RLå¾®è°ƒåœ¨åŒ»å­¦VQAä»»åŠ¡ä¸­çš„æ•ˆæœã€‚ç»“æœæ˜¾ç¤ºï¼ŒGRPO-based RLå¾®è°ƒåœ¨å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ä¸Šå‡ä¼˜äºSFTã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨PMC-VQAåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜GRPO-based RLå¾®è°ƒåœ¨åŒ»å­¦VQAä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼Œå¦‚SFTã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æä¾›äº†å¯¹åŒ»å­¦MLLMsè¿›è¡ŒRLå¾®è°ƒçš„ç³»ç»Ÿåˆ†æï¼Œé‡ç‚¹å…³æ³¨åˆå§‹åŒ–ç­–ç•¥ã€åŒ»å­¦è¯­ä¹‰å¯¹é½ã€é•¿åº¦åŸºå¥–åŠ±çš„å½±å“ä»¥åŠåå·®ç›¸å…³è¡Œä¸ºã€‚è¿™äº›å‘ç°ä¸ºå¦‚ä½•åœ¨åŒ»å­¦VQAä»»åŠ¡ä¸­å®ç°RLå¾®è°ƒæä¾›äº†å®è´µçš„å®è·µè§è§£ï¼Œå¹¶çªæ˜¾äº†GRPO-based RLå¾®è°ƒåœ¨å¼€å‘æ›´é«˜æ•ˆã€æ›´ç¬¦åˆä¸´åºŠéœ€æ±‚çš„åŒ»å­¦MLLMsæ–¹é¢çš„æ½œåŠ›ã€‚

## disentangling-reasoning-and-knowledge-in-medical-large-language-models
### Abstract
Medical reasoning in large language models (LLMs) aims to emulate clinicians'
diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and
PubMedQA often mix reasoning with factual recall. We address this by separating
11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using
a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human
performance. Our analysis shows that only 32.8 percent of questions require
complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)
and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent
gaps between knowledge and reasoning performance. For example, m1 scores 60.5
on knowledge but only 47.1 on reasoning. In adversarial tests where models are
misled with incorrect initial reasoning, biomedical models degrade sharply,
while larger or RL-trained general models show more robustness. To address
this, we train BioMed-R1 using fine-tuning and reinforcement learning on
reasoning-heavy examples. It achieves the strongest performance among similarly
sized models. Further gains may come from incorporating clinical case reports
and training with adversarial and backtracking scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "ç²¾å‡†åŒºåˆ†åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†ä¸çŸ¥è¯†ï¼šè¿ˆå‘æ›´å¯é çš„åŒ»ç–—AI"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŒ»å­¦é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯•å›¾æ¨¡æ‹Ÿä¸´åºŠåŒ»ç”Ÿåœ¨è§£è¯»ç—…äººæ•°æ®å’Œåšå‡ºè¯Šæ–­å†³ç­–æ—¶çš„è®¤çŸ¥è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ ‡å‡†å¦‚MedQA-USMLEã€MedMCQAå’ŒPubMedQAç­‰ï¼Œå¾€å¾€å°†éœ€è¦åŒ»å­¦æ¨ç†çš„é—®é¢˜ä¸ä»…é€šè¿‡äº‹å®å›å¿†å³å¯è§£å†³çš„é—®é¢˜æ··åˆåœ¨ä¸€èµ·ã€‚è¿™ä½¿å¾—è¯„ä¼°LLMçš„çœŸæ­£æ¨ç†èƒ½åŠ›å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ†ç¦»å‡ºä¸“æ³¨äºæ¨ç†å’Œä¸“æ³¨äºçŸ¥è¯†çš„é—®é¢˜ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°LLMåœ¨åŒ»å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡ä½¿ç”¨PubMedBERTåˆ†ç±»å™¨ï¼Œå°†11ä¸ªç”Ÿç‰©åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­çš„é—®é¢˜åˆ†ä¸ºæ¨ç†å‹å’ŒçŸ¥è¯†å‹ã€‚è¯¥åˆ†ç±»å™¨è¾¾åˆ°äº†ä¸äººç±»è¡¨ç°ç›¸å½“çš„æ°´å¹³ï¼ˆçº¦81%çš„å‡†ç¡®ç‡ï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
é€šè¿‡å¯¹ç”Ÿç‰©åŒ»å­¦æ¨¡å‹ï¼ˆå¦‚HuatuoGPT-o1ã€MedReasonã€m1ï¼‰å’Œé€šç”¨é¢†åŸŸæ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ã€o4-miniã€Qwen3ï¼‰çš„è¯„ä¼°ï¼Œå‘ç°æ¨¡å‹åœ¨çŸ¥è¯†å’Œæ¨ç†æ–¹é¢çš„è¡¨ç°å­˜åœ¨ä¸€è‡´æ€§å·®è·ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡è®­ç»ƒäº†BioMed-R1æ¨¡å‹ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´å’ŒåŸºäºæ¨ç†é‡ç¤ºä¾‹çš„å¼ºåŒ–å­¦ä¹ ï¼Œä½¿å…¶åœ¨åŒç­‰è§„æ¨¡æ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€å¼ºçš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œåªæœ‰32.8%çš„é—®é¢˜éœ€è¦å¤æ‚çš„æ¨ç†ï¼Œè€Œå¤§å¤šæ•°é—®é¢˜é›†ä¸­åœ¨äº‹å®ç†è§£ä¸Šã€‚åœ¨å¯¹æŠ—æ€§æµ‹è¯•ä¸­ï¼Œç”Ÿç‰©åŒ»å­¦æ¨¡å‹åœ¨æ¥æ”¶åˆ°é”™è¯¯åˆå§‹æ¨ç†çš„æƒ…å†µä¸‹è¡¨ç°æ€¥å‰§ä¸‹é™ï¼Œè€Œè¾ƒå¤§æˆ–åŸºäºå¼ºåŒ–å­¦ä¹ çš„é€šç”¨æ¨¡å‹åˆ™æ˜¾ç¤ºå‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚ç»è¿‡è®­ç»ƒçš„BioMed-R1æ¨¡å‹åœ¨åŒç­‰è§„æ¨¡æ¨¡å‹ä¸­å–å¾—äº†æœ€å¼ºçš„ç»¼åˆå’Œå¯¹æŠ—æ€§è¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶å¯¹äºåŒ»å­¦LLMçš„å‘å±•å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ï¼Œå®ƒä¸ä»…æ­ç¤ºäº†ç°æœ‰è¯„ä¼°æ ‡å‡†çš„ä¸è¶³ï¼Œè¿˜æä¾›äº†é€šè¿‡åˆ†ç¦»æ¨ç†å’ŒçŸ¥è¯†é—®é¢˜æ¥è¯„ä¼°æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹æŠ—æ€§æµ‹è¯•å’Œå¼ºåŒ–å­¦ä¹ æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå¯é æ€§ï¼Œä¸ºæœªæ¥çš„åŒ»å­¦AIç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner
### Abstract
Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose PathoCLIP, trained on the same figure-caption corpus used for continued
pretraining. Comprehensive experimental results demonstrate that both PathoCLIP
and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è·¯å¾„å­¦è¯Šæ–­æ–°ç¯‡ç« ï¼šPatho-R1å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¨ç†ä¸“å®¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç—…ç†å­¦ä½œä¸ºç°ä»£ä¸´åºŠè¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œå…¶AIè¾…åŠ©ç³»ç»Ÿçš„æ„å»ºå´é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„ç—…ç†å­¦ç‰¹å®šVLMsåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†åˆç†æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸»è¦å› ä¸ºç°æœ‰çš„ç—…ç†å­¦æ•°æ®é›†ä¸»è¦ç”±ç¼ºä¹æ·±åº¦å’Œç»“æ„åŒ–è¯Šæ–­èŒƒå¼çš„å›¾åƒæè¿°å¯¹ç»„æˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¨ç†æ¡†æ¶ï¼Œä»¥æå‡ç—…ç†å­¦AIç³»ç»Ÿçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡åˆ©ç”¨ç—…ç†å­¦æ•™æå’ŒçœŸå®ä¸–ç•Œç—…ç†å­¦ä¸“å®¶çš„çŸ¥è¯†ï¼Œæ„å»ºäº†é«˜è´¨é‡ã€é¢å‘æ¨ç†çš„æ•°æ®é›†ã€‚è¿™ä¸€æ•°æ®é›†çš„æ„å»ºä¸ºè®­ç»ƒå…·æœ‰æ·±åº¦æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä»‹ç»äº†Patho-R1ï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ çš„ç—…ç†å­¦æ¨ç†å™¨ã€‚Patho-R1é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹è¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆæ˜¯ç»§ç»­åœ¨350ä¸‡å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥æ³¨å…¥çŸ¥è¯†ï¼›å…¶æ¬¡æ˜¯åœ¨50ä¸‡é«˜è´¨é‡é“¾å¼æ€ç»´æ ·æœ¬ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒä»¥æ¿€åŠ±æ¨ç†ï¼›æœ€åæ˜¯é‡‡ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’Œåˆ†ç¦»å‰ªè¾‘ä¸åŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥ç²¾ç‚¼å¤šæ¨¡æ€æ¨ç†è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒPathoCLIPå’ŒPatho-R1åœ¨ä¸€ç³»åˆ—ç—…ç†å­¦ç›¸å…³ä»»åŠ¡ä¸Šå‡å–å¾—äº†ç¨³å¥çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå¤šé¡¹é€‰æ‹©é¢˜ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ‰€æå‡ºçš„æ•°æ®é›†å’Œæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ç»¼åˆæ•°æ®é›†æ„å»ºæµç¨‹èƒ½å¤Ÿä»¥æœ€å°çš„äººå·¥åŠªåŠ›ç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„åŒ–å¾®è°ƒæ•°æ®ï¼Œè¿™å¯¹äºå…¶ä»–åŒ»å­¦é¢†åŸŸçš„AIç³»ç»Ÿå¼€å‘å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚æ­¤å¤–ï¼ŒPatho-R1æ¨¡å‹çš„æˆåŠŸå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥åŒ»å­¦AIç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚

## reason-like-a-radiologist--chain-of-thought-and-reinforcement-learning-for-verifiable-report-generation
### Abstract
Radiology report generation is critical for efficiency but current models
lack the structured reasoning of experts, hindering clinical trust and
explainability by failing to link visual findings to precise anatomical
locations. This paper introduces BoxMed-RL, a groundbreaking unified training
framework for generating spatially verifiable and explainable radiology
reports. Built on a large vision-language model, BoxMed-RL revolutionizes
report generation through two integrated phases: (1) In the Pretraining Phase,
we refine the model via medical concept learning, using Chain-of-Thought
supervision to internalize the radiologist-like workflow, followed by spatially
verifiable reinforcement, which applies reinforcement learning to align medical
findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze
the pretrained weights and train a downstream adapter to ensure fluent and
clinically credible reports. This framework precisely mimics radiologists'
workflow, compelling the model to connect high-level medical concepts with
definitive anatomical evidence. Extensive experiments on public datasets
demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR
and ROUGE-L metrics compared to state-of-the-art methods. An average 5%
improvement in large language model-based metrics further underscores
BoxMed-RL's robustness in generating high-quality radiology reports.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œBoxMed-RLï¼šé©æ–°åŒ»å­¦æŠ¥å‘Šç”Ÿæˆï¼Œå®ç°åƒæ”¾å°„ç§‘åŒ»ç”Ÿä¸€æ ·çš„æ¨ç†â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€åŒ»å­¦å½±åƒåœ¨ä¸´åºŠå†³ç­–ä¸­çš„é‡è¦ä½œç”¨æ—¥ç›Šå‡¸æ˜¾ï¼Œè‡ªåŠ¨åŒ–åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæˆä¸ºæé«˜æ•ˆç‡çš„å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹å¾€å¾€ç¼ºä¹ä¸“å®¶å¼çš„ç»“æ„åŒ–æ¨ç†ï¼Œæœªèƒ½å°†è§†è§‰å‘ç°ä¸ç²¾ç¡®çš„è§£å‰–ä½ç½®è”ç³»èµ·æ¥ï¼Œå¯¼è‡´ä¸´åºŠä¿¡ä»»åº¦å’Œè§£é‡Šæ€§ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤Ÿç”Ÿæˆç©ºé—´å¯éªŒè¯ä¸”è§£é‡Šæ€§å¼ºçš„æ”¾å°„å­¦æŠ¥å‘Šçš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶BoxMed-RLã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
BoxMed-RLæ¡†æ¶çš„ç¬¬ä¸€é˜¶æ®µæ˜¯é¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡åŒ»å­¦æ¦‚å¿µå­¦ä¹ å’Œé“¾å¼æ€ç»´ç›‘ç£æ¥ç»†åŒ–æ¨¡å‹ï¼Œä½¿æ¨¡å‹å†…éƒ¨åŒ–æ”¾å°„ç§‘åŒ»ç”Ÿçš„æ¨ç†æµç¨‹ã€‚éšåï¼Œé€šè¿‡ç©ºé—´å¯éªŒè¯çš„å¼ºåŒ–å­¦ä¹ ï¼Œå°†åŒ»å­¦å‘ç°ä¸è¾¹ç•Œæ¡†å¯¹é½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åœ¨ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µï¼Œæ¨¡å‹é¢„è®­ç»ƒçš„æƒé‡è¢«å†»ç»“ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªä¸‹æ¸¸é€‚é…å™¨ä»¥ç¡®ä¿ç”Ÿæˆçš„æŠ¥å‘Šæµç•…ä¸”å…·æœ‰ä¸´åºŠå¯ä¿¡åº¦ã€‚è¿™ä¸€æ¡†æ¶ç²¾ç¡®æ¨¡æ‹Ÿäº†æ”¾å°„ç§‘åŒ»ç”Ÿçš„æµç¨‹ï¼Œè¿«ä½¿æ¨¡å‹å°†é«˜çº§åŒ»å­¦æ¦‚å¿µä¸ç¡®åˆ‡çš„è§£å‰–è¯æ®è”ç³»èµ·æ¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBoxMed-RLåœ¨METEORå’ŒROUGE-LæŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†7%ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†ä¸Šå¹³å‡æé«˜äº†5%ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†BoxMed-RLåœ¨ç”Ÿæˆé«˜è´¨é‡æ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„ç¨³å¥æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„BoxMed-RLæ¡†æ¶åœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆé¢†åŸŸå…·æœ‰æ˜¾è‘—çš„åˆ›æ–°æ€§ï¼Œä¸ä»…æé«˜äº†æŠ¥å‘Šçš„è§£é‡Šæ€§å’Œä¸´åºŠä¿¡ä»»åº¦ï¼Œè¿˜ä¸ºåŒ»å­¦å½±åƒåˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ç»“åˆçš„é¢†åŸŸæä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯å…¶ç©ºé—´å¯éªŒè¯çš„å­¦ä¹ ç­–ç•¥å’Œæ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿæ¨ç†æµç¨‹çš„è®¾è®¡æ€è·¯ï¼Œå¯¹äºå…¶ä»–åŒ»å­¦AIåº”ç”¨åŒæ ·å…·æœ‰å¯å‘æ„ä¹‰ã€‚

