# Paper List of Terms(Reward Model+Test Time Scaling)
- [25/07] **Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS**  
[[Paper](http://arxiv.org/pdf/2507.05557v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts)]

- [25/07] **Test-Time Scaling with Reflective Generative Model**  
[[Paper](http://arxiv.org/pdf/2507.01951v2)] [[Code/Page](https://github.com/MetaStone-AI/MetaStone-S1.)] [[TLDR/Notes](#test-time-scaling-with-reflective-generative-model)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts**  
[[Paper](http://arxiv.org/pdf/2506.15733v1)] [[Code/Page]()] [[TLDR/Notes](#$\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts)]

- [25/06] **EQA-RM: A Generative Embodied Reward Model with Test-time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.10389v1)] [[Code/Page](https://github.com/UNITES-Lab/EQA-RM.)] [[TLDR/Notes](#eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **Learning to Reason Across Parallel Samples for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.09014v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-across-parallel-samples-for-llm-reasoning)]

- [25/06] **Guided Speculative Inference for Efficient Test-Time Alignment of LLMs**  
[[Paper](http://arxiv.org/pdf/2506.04118v1)] [[Code/Page](https://github.com/j-geuter/GSI)] [[TLDR/Notes](#guided-speculative-inference-for-efficient-test-time-alignment-of-llms)]



# TLDR/Notes
## enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts
### Abstract
Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.
### ðŸŒŸ è®ºæ–‡è§£è¯» | åˆ†å±‚æ£€ç´¢å¢žå¼ºMCTSï¼Œè§£é”å¤§æ¨¡åž‹æŽ¨ç†æ—¶æ€§èƒ½æ–°é«˜åº¦

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰æŽ¨ç†èƒ½åŠ›æå‡å¸¸ä¾èµ–è®­ç»ƒæ—¶ç®—åŠ›ï¼Œä½†æµ‹è¯•æ—¶æ‰©å±•ï¼ˆTest - Time Scalingï¼ŒTTSï¼‰ä½œä¸ºæ–°èŒƒå¼ï¼Œé€šè¿‡æŽ¨ç†æ—¶é¢å¤–ç®—åŠ›å¢žå¼ºæ€§èƒ½ã€‚çŽ°æœ‰åŸºäºŽæœç´¢çš„TTSæ–¹æ³•ï¼ˆå¦‚MCTSç»“åˆPRMï¼‰å­˜åœ¨å±€é™ï¼šä¾èµ–é¢„è®­ç»ƒä¿¡æ¯æ˜“é™·å±€éƒ¨æœ€ä¼˜ã€PRMéš¾æ•æ‰å…¨å±€ç­–ç•¥è‡´å¥–åŠ±ä¿¡å·ç¨€ç–ï¼Œå¤æ‚æŽ¨ç†æ˜“å¤±è´¥ã€‚éœ€æ›´é«˜æ•ˆé€šç”¨çš„æŽ¨ç†æ‰©å±•æ–¹æ³•ï¼Œæ— éœ€å¤§é‡é¢å¤–è®­ç»ƒè¿˜èƒ½æå‡é²æ£’æ€§ä¸Žé€‚åº”æ€§ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒå±‚æ¬¡æ£€ç´¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ï¼ˆç²—ç²’åº¦+ç»†ç²’åº¦ï¼‰
ç²—ç²’åº¦å±‚é¢ï¼Œæå‡ºæ·±åº¦é€»è¾‘æ£€ç´¢ï¼ˆDeep Logical Retrievalï¼‰ã€‚ä»Žå¤æ‚æŽ¨ç†é—®é¢˜ä¸­æå–æŠ½è±¡æ¨¡æ¿ï¼Œæ£€ç´¢ç›¸ä¼¼é—®é¢˜ - ç­”æ¡ˆå¯¹ï¼Œä¸ºæ¨¡åž‹æä¾›å¤šæ ·ç¤ºä¾‹ï¼ŒåŠ©åŠ›æ•æ‰é—®é¢˜ç»“æž„æ¨¡å¼ä¸Žå˜å¼‚æ€§ï¼Œå¢žå¼ºå¯¹æœªè§è¿‡é—®é¢˜çš„é€‚åº”æ€§ï¼Œæå‡ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæžœã€‚
ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†å±‚å¢žå¼ºæŽ¨ç†MCTS
ç»†ç²’åº¦å±‚é¢ï¼Œåœ¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿‡ç¨‹ä¸­ï¼ŒR2 - LLMsä»Žå¤–éƒ¨æ•°å­¦é—®é¢˜æ•°æ®é›†åŠ¨æ€æ£€ç´¢ç›¸å…³ä¸­é—´è§£é¢˜æ­¥éª¤ï¼Œç”¨ç±»ä¼¼å…ˆéªŒçŸ¥è¯†ä¸°å¯ŒæŽ¨ç†è¿‡ç¨‹ã€‚ç»“åˆè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ï¼ˆPRMï¼‰ï¼Œè®©PRMåŸºäºŽæ£€ç´¢åˆ°çš„æ­¥éª¤ç»™å‡ºæ›´åˆç†ä¸”ä¸Šä¸‹æ–‡ä¸€è‡´çš„è¯„ä¼°ï¼Œé™ä½Žæ— æ•ˆæŽ¢ç´¢é£Žé™©ï¼ŒåŒæ—¶å®Œå–„å€™é€‰ç”Ÿæˆä¸Žå†³ç­–ä»¥æå‡æŽ¨ç†ç²¾åº¦ã€‚

### ðŸ“ˆ å®žéªŒç»“æžœ
åœ¨MATH500ã€GSM8Kå’ŒOlympiadBench - TOæ•°æ®é›†ä¸Šï¼Œç”¨LLaMA - 3.1 - 8Bæ¨¡åž‹å¯¹æ¯”åŸºçº¿ï¼ŒR2 - LLMså®žçŽ°æ˜¾è‘—ç›¸å¯¹æå‡ï¼Œæœ€é«˜æå‡è¾¾16%ï¼Œåœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸­å±•çŽ°æœ‰æ•ˆæ€§ï¼›ä¸”åœ¨LLaMA 3.1 - 8Bå’ŒQwen 2 - 7Bç­‰ç­–ç•¥æ¨¡åž‹ä¸Šè¯„ä¼°ï¼Œä¹Ÿä¼˜äºŽåŸºäºŽä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’ŒåŸºäºŽæ ‘æœç´¢çš„åŸºçº¿æ–¹æ³•ã€‚

### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†å±‚æ£€ç´¢å¢žå¼ºæ€è·¯ï¼šåœ¨ä¸åŒç²’åº¦ï¼ˆç²—ã€ç»†ï¼‰ç»“åˆæ£€ç´¢ä¸°å¯Œæ¨¡åž‹æŽ¨ç†æ—¶ä¿¡æ¯ï¼Œä¸ºæå‡æ¨¡åž‹åœ¨æœªçŸ¥åœºæ™¯é€‚åº”æ€§æä¾›æ–°èŒƒå¼ï¼Œå¯å¯å‘å¤šç²’åº¦ä¿¡æ¯èžåˆç±»æ–¹æ³•è®¾è®¡ã€‚
2. æ£€ç´¢ä¸ŽMCTSç»“åˆï¼šå°†å¤–éƒ¨æ£€ç´¢æ•°æ®èžå…¥MCTSè¿‡ç¨‹è¾…åŠ©PRMè¯„ä¼°ï¼Œä¸ºåŸºäºŽæœç´¢çš„TTSæ–¹æ³•ä¼˜åŒ–ä¸­é—´æ­¥éª¤æŒ‡å¯¼ã€ç¼“è§£å¥–åŠ±ç¨€ç–é—®é¢˜æä¾›å‚è€ƒï¼ŒåŽç»­å¯æŽ¢ç´¢æ›´å¤šæ£€ç´¢æºä¸Žæœç´¢ç®—æ³•ç»“åˆæ–¹å¼ã€‚
3. æ— CoTè’¸é¦æ•°æ®ä¾èµ–ï¼šæ— éœ€ä»Žæ›´å…ˆè¿›æ¨¡åž‹è’¸é¦èŽ·å–æ€ç»´é“¾ï¼ˆCoTï¼‰è®­ç»ƒæ•°æ®ï¼Œé™ä½Žæ–¹æ³•åº”ç”¨é—¨æ§›ï¼Œåœ¨æ•°æ®èŽ·å–å—é™åœºæ™¯æœ‰æŽ¨å¹¿ä»·å€¼ï¼Œä¸ºè½»é‡æ•°æ®ä¾èµ–çš„æŽ¨ç†å¢žå¼ºæ–¹æ¡ˆæä¾›æ€è·¯ã€‚

## test-time-scaling-with-reflective-generative-model
### Abstract
We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3-mini's performance via the new Reflective Generative Form.
The new form focuses on high-quality reasoning trajectory selection and
contains two novelties: 1) A unified interface for policy and process reward
model: we share the backbone network and use task-specific heads for reasoning
trajectory predicting and scoring respectively, introducing only 53M extra
parameters for trajectory scoring. 2) Eliminating the reliance on process-level
annotation: we provide a self-supervised process reward model, which can
directly learn the high-quality reasoning trajectory selection from the outcome
reward. Equipped with the reflective generative form, MetaStone-S1 is naturally
suitable for test-time scaling, and we provide three reasoning effort modes
(low, medium, and high) based on the controllable thinking length. Experiments
demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI
o3-mini's series with only 32B parameter size. To support the research
community, we have open-sourced MetaStone-S1 at
https://github.com/MetaStone-AI/MetaStone-S1.
### ðŸŒŸ è®ºæ–‡è§£è¯» | å…¨æ–°åå°„ç”Ÿæˆæ¨¡åž‹MetaStone - S1ï¼šå°å‚æ•°å®žçŽ°å¤§æ€§èƒ½ï¼Œè§£é”æµ‹è¯•æ—¶ç¼©æ”¾æ–°èŒƒå¼

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼ŒOpenAIçš„o3æ¨¡åž‹å‡­å€Ÿæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - Time Scalingï¼ŒTTSï¼‰æŠ€æœ¯å®žçŽ°äº†å…ˆè¿›çš„æŽ¨ç†å’Œç¼–ç èƒ½åŠ›ï¼Œå¦‚åœ¨æŽ¨ç†æ—¶è¿›è¡Œå¤§è§„æ¨¡é‡‡æ ·ã€å€™é€‰è¯„åˆ†å’Œå¤šæŽ¨ç†è·¯å¾„æœç´¢ç­‰ã€‚è€ŒçŽ°æœ‰TTSæ–¹æ³•åˆ†ä¸ºå†…éƒ¨TTSå’Œå¤–éƒ¨TTSï¼Œå†…éƒ¨TTSå­˜åœ¨è®­ç»ƒé˜¶æ®µç»“æžœå¥–åŠ±è¯¯åˆ†ç±»æ­£ç¡®ç­”æ¡ˆï¼ˆå³å‡é˜³æ€§æŽ¨ç†è¿‡ç¨‹ï¼‰çš„é—®é¢˜ï¼›å¤–éƒ¨TTSè™½èƒ½æ›´æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œä½†çŽ°æœ‰æ–¹æ³•åœ¨æ¨¡åž‹æž¶æž„å’Œè®­ç»ƒä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¦‚è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ï¼ˆPRMï¼‰è®­ç»ƒæˆæœ¬é«˜ã€ä¾èµ–ç‰¹å®šæ ‡æ³¨æ•°æ®ç­‰ã€‚åŒæ—¶ï¼Œä¸åŒå‚æ•°è§„æ¨¡çš„ç­–ç•¥æ¨¡åž‹é€‚é…çš„å¤–éƒ¨TTSæ–¹æ³•ä¹Ÿä¸åŒï¼Œå°äºŽ32Bæ—¶æœç´¢æ–¹æ³•æ›´ä¼˜ï¼Œå¤§äºŽç­‰äºŽ32Bæ—¶é‡‡æ ·æ–¹æ³•æ›´ä¼˜ã€‚æœ¬æ–‡èšç„¦å¤–éƒ¨TTSï¼Œæ—¨åœ¨æå‡ºæ–°çš„åå°„ç”Ÿæˆå½¢å¼æ¥å®žçŽ°é«˜è´¨é‡æŽ¨ç†è½¨è¿¹é€‰æ‹©ï¼Œæ‰“é€ æ€§èƒ½ä¼˜å¼‚ä¸”å‚æ•°è§„æ¨¡æ›´ä¼˜çš„æ¨¡åž‹ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå…¨æ–°åå°„ç”Ÿæˆå½¢å¼ï¼ˆReflective Generative Formï¼‰
ç³»ç»Ÿæ¢³ç†çŽ°æœ‰TTSèŒƒå¼åŽï¼Œå®šä¹‰äº†ç”¨äºŽé«˜è´¨é‡æŽ¨ç†è½¨è¿¹é€‰æ‹©çš„åå°„ç”Ÿæˆå½¢å¼ã€‚è¯¥å½¢å¼è®©å•ä¸ªç½‘ç»œå®žçŽ°æŽ¨ç†è½¨è¿¹é¢„æµ‹ä¸Žé€‰æ‹©ï¼Œä¸”æ— éœ€è¿‡ç¨‹çº§æ ‡æ³¨ã€‚å…·ä½“ä¸ºç­–ç•¥å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹å…±äº«éª¨å¹²ç½‘ç»œï¼Œåˆ†åˆ«ç”¨ç‰¹å®šä»»åŠ¡å¤´è¿›è¡ŒæŽ¨ç†è½¨è¿¹é¢„æµ‹å’Œè¯„åˆ†ï¼Œä»…ä¸ºè½¨è¿¹è¯„åˆ†å¼•å…¥53Mé¢å¤–å‚æ•°ï¼Œå®žçŽ°äº†æ¨¡åž‹æž¶æž„ä¸Šçš„é«˜æ•ˆå¤ç”¨ä¸Žåˆ›æ–°è®¾è®¡ã€‚
ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªç›‘ç£è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹æ¶ˆé™¤è¿‡ç¨‹çº§æ ‡æ³¨ä¾èµ–
æä¾›è‡ªç›‘ç£çš„è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ï¼Œèƒ½å¤Ÿä»Žç»“æžœå¥–åŠ±ä¸­ç›´æŽ¥å­¦ä¹ é«˜è´¨é‡æŽ¨ç†è½¨è¿¹é€‰æ‹©ï¼Œæ‘†è„±äº†å¯¹è¿‡ç¨‹çº§æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œé™ä½Žäº†æ•°æ®èŽ·å–ä¸Žæ ‡æ³¨æˆæœ¬ï¼ŒåŒæ—¶ä¹Ÿè§£å†³äº†çŽ°æœ‰PRMä¾èµ–ç‰¹å®šæ ‡æ³¨æ•°æ®çš„ç—›ç‚¹ï¼Œè®©æ¨¡åž‹è®­ç»ƒæ›´å…·é€šç”¨æ€§ä¸Žé«˜æ•ˆæ€§ã€‚
ðŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¯æŽ§æ€è€ƒé•¿åº¦çš„å¤šæŽ¨ç†åŠªåŠ›æ¨¡å¼
åŸºäºŽåå°„ç”Ÿæˆå½¢å¼ï¼ŒMetaStone - S1è®¾ç½®äº†é«˜ã€ä¸­ã€ä½Žä¸‰ç§æŽ¨ç†åŠªåŠ›æ¨¡å¼ï¼ˆreasoning effort modesï¼‰ï¼Œä¾æ®å¯æŽ§çš„æ€è€ƒé•¿åº¦æ¥é€‚é…ä¸åŒåœºæ™¯éœ€æ±‚ï¼Œå¤©ç„¶é€‚ç”¨äºŽæµ‹è¯•æ—¶ç¼©æ”¾ï¼Œä¸ºæ¨¡åž‹åœ¨ä¸åŒæŽ¨ç†å¤æ‚åº¦ä»»åŠ¡ä¸‹çš„åº”ç”¨æä¾›äº†çµæ´»é€‰æ‹©ã€‚

### ðŸ“ˆ å®žéªŒç»“æžœ
å®žéªŒè¡¨æ˜Žï¼Œä»…32Bå‚æ•°è§„æ¨¡çš„MetaStone - S1åœ¨æ€§èƒ½ä¸Šèƒ½ä¸ŽOpenAI o3 - miniç³»åˆ—åª²ç¾Žã€‚å…¶ä¸­MetaStone - S1 - lowåœ¨æ•°å­¦ï¼ˆAIME24&25ï¼‰ã€ç¼–ç ï¼ˆLiveCodeBenchï¼‰å’Œä¸­æ–‡æŽ¨ç†ï¼ˆC - Evalï¼‰ä»»åŠ¡ä¸Šåˆ†åˆ«è¶…è¿‡OpenAI o3 - mini - lowï¼›MetaStone - S1 - mediumä¸ŽOpenAI o3 - mini - mediumç»“æžœç›¸è¿‘ï¼›MetaStone - S1 - highè¿›ä¸€æ­¥æå‡äº†æ™ºèƒ½ä¸Šé™ï¼Œåœ¨ä¸€ç³»åˆ—å¼€æºå’Œé—­æºæ¨¡åž‹ä¸­å–å¾—äº†SOTAç»“æžœã€‚

### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡åž‹æž¶æž„å¤ç”¨ä¸Žåˆ›æ–°ï¼šåå°„ç”Ÿæˆå½¢å¼ä¸­å…±äº«éª¨å¹²ç½‘ç»œã€ä»»åŠ¡å¤´åˆ†å·¥çš„è®¾è®¡æ€è·¯ï¼Œä¸ºåŽç»­å¤šä»»åŠ¡æ¨¡åž‹æˆ–éœ€è¦åŒæ—¶è¿›è¡Œç”Ÿæˆä¸Žè¯„ä¼°ç±»ä»»åŠ¡çš„æ¨¡åž‹æž¶æž„è®¾è®¡æä¾›äº†å‚è€ƒï¼Œåœ¨å‚æ•°é«˜æ•ˆåˆ©ç”¨ä¸Šæœ‰å¾ˆå¥½çš„ç¤ºèŒƒã€‚
2. è‡ªç›‘ç£å­¦ä¹ åœ¨å¥–åŠ±æ¨¡åž‹çš„åº”ç”¨ï¼šå…¶è‡ªç›‘ç£è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹æ‘†è„±è¿‡ç¨‹çº§æ ‡æ³¨çš„æ€è·¯ï¼Œä¸ºè§£å†³å¥–åŠ±æ¨¡åž‹æ•°æ®æ ‡æ³¨éš¾é¢˜ã€é™ä½Žè®­ç»ƒæˆæœ¬æä¾›äº†æ–°æ–¹å‘ï¼ŒåŽç»­åœ¨æž„å»ºå„ç±»å¥–åŠ±æ¨¡åž‹ï¼ˆä¸æ­¢äºŽLLMsé¢†åŸŸï¼‰æ—¶å¯å€Ÿé‰´è¯¥è‡ªç›‘ç£å­¦ä¹ æ€è·¯ã€‚
3. å¤šæ¨¡å¼æŽ¨ç†è®¾è®¡ï¼šä¾æ®å¯æŽ§æ€è€ƒé•¿åº¦è®¾ç½®å¤šæŽ¨ç†åŠªåŠ›æ¨¡å¼ï¼Œä¸ºæ¨¡åž‹åœ¨å®žé™…åº”ç”¨ä¸­æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ã€è®¡ç®—èµ„æºç­‰å› ç´ çµæ´»è°ƒæ•´æŽ¨ç†ç­–ç•¥æä¾›äº†å®žè·µèŒƒä¾‹ï¼Œæœ‰åŠ©äºŽæŽ¨åŠ¨æ¨¡åž‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è½åœ°åº”ç”¨ã€‚
4. å°å‚æ•°å®žçŽ°é«˜æ€§èƒ½ï¼šåœ¨å¤§æ¨¡åž‹è¿½æ±‚å‚æ•°è§„æ¨¡ç«žèµ›çš„å½“ä¸‹ï¼ŒMetaStone - S1ä»¥32Bå‚æ•°è¾¾åˆ°å¯¹æ ‡OpenAI o3 - miniç³»åˆ—çš„æ€§èƒ½ï¼Œè¯æ˜Žäº†ä¼˜åŒ–æ¨¡åž‹æž¶æž„ä¸Žè®­ç»ƒèŒƒå¼åœ¨æå‡æ€§èƒ½ä¸Šçš„é‡è¦æ€§ï¼Œä¸ºä¸­å°è§„æ¨¡å‚æ•°æ¨¡åž‹çš„å‘å±•æä¾›äº†ä¿¡å¿ƒä¸Žæ–¹å‘ï¼ŒåŽç»­ç ”ç©¶å¯æ›´å…³æ³¨æ¨¡åž‹æ•ˆçŽ‡ä¸Žæž¶æž„åˆ›æ–°è€Œéžå•çº¯å‚æ•°å †ç Œã€‚

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### ðŸŒŸ è®ºæ–‡è§£è¯» | ç”¨çŸ¥è¯†å¢žå¼ºæ ‘æœç´¢æŽ¨ç†åŠ©åŠ›å¤§æ¨¡åž‹æ”»å…‹åˆ†å­ç»“æž„è§£æžéš¾é¢˜

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åˆ†å­ç»“æž„è§£æžæ˜¯åŒ–å­¦å®žéªŒåˆ†æžé‡Œçš„å…³é”®ä»»åŠ¡ï¼Œè¦ä»Žæ ¸ç£ã€çº¢å¤–ç­‰å…‰è°±æ•°æ®æŽ¨å¯¼åˆ†å­ç»“æž„ï¼Œä¸“ä¸šäººå‘˜éƒ½å¾—èŠ±10 - 15åˆ†é’Ÿåˆ†æžå•ä¸ªåˆ†å­ï¼Œæ‰€ä»¥ç”¨å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–è§£æžå¾ˆæœ‰å¿…è¦ã€‚ä½†LLMåœ¨è¿™ä»»åŠ¡ä¸Šæœ‰æŒ‘æˆ˜ï¼šä¸€æ˜¯å¯¹åŒ–å­¦åˆ†å­ç»“æž„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œåƒå™»å©è¿™ç±»ç‰¹æ®Šæ‚çŽ¯ç»“æž„ï¼ŒLLMå¸¸å› ç¼ºä¹å­ç»“æž„çŸ¥è¯†è¯¯åˆ¤ï¼›äºŒæ˜¯æ²¡æ³•å‡†ç¡®è¯„ä¼°å’Œä¿®æ­£æŽ¨ç†è¿‡ç¨‹ï¼Œæ ‘æœç´¢æŽ¨ç†éœ€è¦æœ‰æ•ˆè¯„ä¼°åé¦ˆï¼Œå¯LLMç¼ºé¢†åŸŸçŸ¥è¯†ï¼Œåšä¸å¥½ reward model è§’è‰²ã€‚äºŽæ˜¯è®ºæ–‡è¦è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæå‡LLMåœ¨åˆ†å­ç»“æž„è§£æžçš„èƒ½åŠ›ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºK - MSEæ¡†æž¶  
æž„å»ºçŸ¥è¯†å¢žå¼ºçš„åˆ†å­ç»“æž„è§£æžæŽ¨ç†æ¡†æž¶K - MSEï¼ŒæŠŠè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºæ’ä»¶å®žçŽ°æµ‹è¯•æ—¶çš„èƒ½åŠ›æ‰©å±•ï¼Œèƒ½é€‚é…ä»»æ„LLMã€‚å€ŸåŠ©MCTSå¹³è¡¡æ–°è§£æŽ¢ç´¢å’Œå·²æœ‰è§£åˆ©ç”¨ï¼Œè¿˜ç»“åˆSelf - Refineè®©LLMåŠæ—¶ä¼˜åŒ–ä¹‹å‰çš„è§£ã€‚

ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤–éƒ¨åˆ†å­å­ç»“æž„çŸ¥è¯†åº“  
ä¸ºå¼¥è¡¥LLMåŒ–å­¦ç»“æž„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œæž„å»ºå¤–éƒ¨åˆ†å­å­ç»“æž„çŸ¥è¯†åº“ã€‚å­ç»“æž„æ˜¯åŒ–å­¦ç©ºé—´åŸºç¡€å…ƒç´ ï¼ŒçŸ¥è¯†åº“é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ•´åˆå­ç»“æž„å’Œç»“æž„æè¿°ï¼Œç»™LLMè¡¥å……é¢†åŸŸçŸ¥è¯†ï¼Œæå‡ç‰¹æ®Šç»“æž„æŽ¨ç†å‡†ç¡®æ€§ï¼Œå‡å°‘ atypical æ¡ˆä¾‹é”™è¯¯ã€‚

ðŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸“å±žåˆ†å­ - å…‰è°±è¯„åˆ†å™¨  
è®¾è®¡åˆ†å­ - å…‰è°±è¯„åˆ†å™¨å½“ reward modelï¼Œè§£å†³LLMè§£è¯„ä¼°ä¸å‡†é—®é¢˜ã€‚è¯„åˆ†å™¨æœ‰åˆ†å­ç¼–ç å™¨å’Œå…‰è°±ç¼–ç å™¨ï¼Œè¯„ä¼°åˆ†å­ç»“æž„å’Œå…‰è°±æ•°æ®åŒ¹é…åº¦ç»™å¥–åŠ±åˆ†ã€‚å®ƒè¿˜ä½œä¸ºLLMå’ŒçŸ¥è¯†åº“é—´çš„æ£€ç´¢å™¨ï¼Œç”¨è¾“å…¥å…‰è°±æŸ¥æœ€ç›¸å…³å­ç»“æž„ï¼Œå‡å°‘å­ç»“æž„æ£€ç´¢è¯¯å·®ï¼Œå¢žå¼ºæŽ¨ç†ç¨³å®šæ€§ã€‚

### ðŸ“ˆ å®žéªŒç»“æžœ
åœ¨MolPuzzleåŸºå‡†æµ‹è¯•ä¸Šï¼ŒK - MSEæ–¹æ³•æ•ˆæžœæ˜¾è‘—ï¼Œå¯¹GPT - 4o - miniå’ŒGPT - 4oéƒ½å¸¦æ¥è¶…20%çš„æ€§èƒ½æå‡ï¼Œè¯æ˜Žäº†æ¡†æž¶åœ¨å¢žå¼ºLLMåˆ†å­ç»“æž„è§£æžèƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸçŸ¥è¯†å¢žå¼ºæ€è·¯ï¼šé¢å¯¹ä¸“ä¸šé¢†åŸŸä»»åŠ¡ï¼ŒLLMé€šç”¨çŸ¥è¯†ä¸è¶³æ—¶ï¼Œæž„å»ºé¢†åŸŸå­ç»“æž„çŸ¥è¯†åº“è¡¥å……ï¼Œè¿™ç§â€œå¤–éƒ¨çŸ¥è¯† + LLMâ€æ¨¡å¼å¯å¤ç”¨åœ¨å…¶ä»–ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©ã€ææ–™ï¼‰ä»»åŠ¡ã€‚  
2. æŽ¨ç†è¿‡ç¨‹è¯„ä¼°ä¼˜åŒ–ï¼šè®¾è®¡é¢†åŸŸä¸“å±žè¯„åˆ†å™¨åš reward modelï¼Œç»“åˆæ ‘æœç´¢æ¡†æž¶ä¼˜åŒ–æŽ¨ç†ï¼Œä¸ºéœ€è¦æ·±åº¦æŽ¨ç†ã€éœ€è¯„ä¼°åé¦ˆçš„å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è¯æ˜Žã€ä»£ç è°ƒè¯•ï¼‰æä¾›äº†â€œè¯„åˆ†å™¨ + æ ‘æœç´¢â€çš„æŽ¨ç†å¢žå¼ºèŒƒå¼ã€‚  
3. æ’ä»¶åŒ–æ¡†æž¶è®¾è®¡ï¼šK - MSEä½œä¸ºæ’ä»¶é€‚é…ä»»æ„LLMï¼Œè¿™ç§è§£è€¦å¼è®¾è®¡æ–¹ä¾¿æŠ€æœ¯è½åœ°ï¼Œä¸åŒåœºæ™¯ä¸‹å¯å¿«é€Ÿé›†æˆåˆ°çŽ°æœ‰LLMå·¥ä½œæµé‡Œï¼Œé™ä½ŽæŠ€æœ¯è¿ç§»æˆæœ¬ã€‚

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### ðŸŒŸ è®ºæ–‡è§£è¯» | ReasonFlux-PRMï¼šé¢å‘å¤§æ¨¡åž‹é•¿æ€ç»´é“¾æŽ¨ç†çš„è½¨è¿¹æ„ŸçŸ¥åž‹è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„å¤æ‚æŽ¨ç†åœºæ™¯ï¼ˆå¦‚æ•°å­¦è§£é¢˜ï¼‰ä¸­ï¼ŒProcess Reward Modelsï¼ˆPRMsï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ï¼‰æ˜¯ç›‘ç£ä¸­é—´æŽ¨ç†æ­¥éª¤çš„æœ‰åŠ›å·¥å…·ã€‚ä¸è¿‡çŽ°æœ‰PRMså­˜åœ¨æ˜Žæ˜¾å±€é™ï¼šå®ƒä»¬ä¸»è¦åŸºäºŽæ¨¡åž‹æœ€ç»ˆè¾“å‡ºè®­ç»ƒï¼Œéš¾ä»¥å¯¹**è½¨è¿¹ - å“åº”ï¼ˆtrajectory - responseï¼‰**è¿™ç±»æ–°å…´è¾“å‡ºå½¢å¼çš„ä¸­é—´æŽ¨ç†è½¨è¿¹è¿›è¡Œé²æ£’è¯„ä¼°ã€‚åƒDeepseek - R1ç­‰å‰æ²¿æŽ¨ç†æ¨¡åž‹ä¼šç”Ÿæˆâ€œå†—é•¿ã€æ¬ è§„æ•´çš„ä¸­é—´æ€è€ƒè½¨è¿¹ + ç®€æ´æœ€ç»ˆå“åº”â€çš„è½¨è¿¹ - å“åº”å¯¹ï¼Œè¿™ç±»æ•°æ®å¸¸è¢«ç”¨äºŽå°æ¨¡åž‹è’¸é¦ï¼Œä½†çŽ°æœ‰PRMså› ä¸Žä¸­é—´è½¨è¿¹åœ¨ç»“æž„ã€æ ¼å¼ä¸Šä¸åŒ¹é…ï¼Œä¸”è®­ç»ƒæ—¶ç¼ºä¹å¸¦å¥–åŠ±çš„è½¨è¿¹ - å“åº”æ•°æ®ï¼Œåœ¨ç›‘ç£è¿™ç±»æ•°æ®æ—¶æ•ˆæžœä¸ä½³ç”šè‡³ä¼šæŸå®³ä¸‹æ¸¸è®­ç»ƒã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®©PRMsæ—¢èƒ½ç›‘ç£æœ€ç»ˆå“åº”ï¼Œåˆèƒ½æœ‰æ•ˆè¯„ä¼°ä¸­é—´æ€è€ƒè½¨è¿¹ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè½¨è¿¹æ„ŸçŸ¥çš„PRMâ€”â€”ReasonFlux - PRM  
ReasonFlux - PRMä¸“ä¸ºè¯„ä¼°è½¨è¿¹ - å“åº”åž‹æŽ¨ç†ç—•è¿¹è®¾è®¡ï¼Œèžåˆäº†**æ­¥éª¤çº§ï¼ˆstep - levelï¼‰**å’Œ**è½¨è¿¹çº§ï¼ˆtrajectory - levelï¼‰**ç›‘ç£ã€‚å®ƒåœ¨æ¶µç›–æ•°å­¦å’Œç§‘å­¦æŽ¨ç†çš„10ké«˜è´¨é‡è½¨è¿¹ - å“åº”å¯¹ curated æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½ä¸ºæ€è€ƒè½¨è¿¹å†…çš„æ¯ä¸ªæ­¥éª¤æä¾›ç»†ç²’åº¦å¥–åŠ±ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®©æ¨¡åž‹ä¸­é—´æ€è€ƒè½¨è¿¹ä¸Žæœ€ç»ˆå“åº”æ›´å¯¹é½ï¼Œè§£å†³äº†çŽ°æœ‰PRMså¯¹ä¸­é—´è½¨è¿¹ç›‘ç£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚  

ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šåœºæ™¯é€‚é…çš„å¥–åŠ±ç›‘ç£  
ReasonFlux - PRMé€‚é…ç¦»çº¿å’Œåœ¨çº¿å¤šç§åœºæ™¯ï¼š  
- ç¦»çº¿åœºæ™¯ï¼šä¸ºè½¨è¿¹ - å“åº”å¯¹æ‰“åˆ†ï¼Œç­›é€‰é«˜è´¨é‡æ•°æ®ï¼ŒåŠ©åŠ›å°æ¨¡åž‹ä¸‹æ¸¸æœ‰ç›‘ç£å¾®è°ƒçš„è®­ç»ƒæ•°æ®ç²¾é€‰ï¼›  
- åœ¨çº¿åœºæ™¯ï¼šèžå…¥GRPOç­‰ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ç­–ç•¥ä¼˜åŒ–æä¾›ç»†ç²’åº¦è¿‡ç¨‹å¥–åŠ±ï¼›  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest - time scalingï¼‰ï¼šé€šè¿‡å¥–åŠ±å¼•å¯¼çš„Best - of - Nç­–ç•¥ï¼Œè¯„ä¼°å¤šä¸ªç”Ÿæˆå“åº”å¹¶é€‰æœ€ä¼˜ï¼Œæå‡æŽ¨ç†æ€§èƒ½ã€‚  


### ðŸ“ˆ å®žéªŒç»“æžœ
åœ¨AIMEã€MATH500ã€GPQA - Diamondç­‰æŒ‘æˆ˜æ€§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux - PRMå±•çŽ°å‡ºä¼˜å¼‚æ€§èƒ½ï¼š  
- æ•°æ®é€‰æ‹©æ–¹é¢ï¼šReasonFlux - PRM - 7Bæ¯”å¼ºåŸºçº¿ï¼ˆå¦‚Qwen2.5 - Math - PRM - 72Bï¼‰å’Œäººå·¥ç­–åˆ’åŸºçº¿é€‰å‡ºçš„æ•°æ®é›†è´¨é‡æ›´é«˜ï¼›  
- æ€§èƒ½æå‡æ–¹é¢ï¼šReasonFlux - PRM - 7Båœ¨æœ‰ç›‘ç£å¾®è°ƒä¸­å¹³å‡æå‡12.1%ï¼Œå¼ºåŒ–å­¦ä¹ ä¸­å¹³å‡æå‡4.5%ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ä¸­å¹³å‡æå‡6.3%ï¼›  
- èµ„æºå‹å¥½åž‹å‘å¸ƒï¼šè¿˜å‘å¸ƒäº†ReasonFlux - PRM - 1.5Bï¼Œé€‚é…èµ„æºå—é™åœºæ™¯ä¸Žè¾¹ç¼˜éƒ¨ç½²ã€‚  


### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰ä¸Žåˆ†æžè§’åº¦ï¼šé’ˆå¯¹æ–°å…´çš„è½¨è¿¹ - å“åº”è’¸é¦æ•°æ®è¶‹åŠ¿ï¼Œæ·±å…¥åˆ†æžçŽ°æœ‰PRMsåœ¨ç›‘ç£ä¸­é—´è½¨è¿¹æ—¶çš„é—®é¢˜ï¼ˆç»“æž„æ ¼å¼ä¸åŒ¹é…ã€è®­ç»ƒæ•°æ®ç¼ºå¤±ï¼‰ï¼Œè¿™ç§ä»Žäº§ä¸šæ–°æ•°æ®å½¢æ€åæŽ¨æŠ€æœ¯ç—›ç‚¹çš„æ€è·¯ï¼Œä¸ºåŽç»­ç ”ç©¶é”šå®šæ–¹å‘æä¾›å‚è€ƒï¼›  
2. å¤šç²’åº¦ç›‘ç£èžåˆï¼šå°†æ­¥éª¤çº§å’Œè½¨è¿¹çº§ç›‘ç£ç»“åˆï¼Œä¸ºå¤„ç†â€œé•¿é“¾æ¡ã€å¤šé˜¶æ®µâ€çš„æŽ¨ç†ç±»ä»»åŠ¡æä¾›äº†ç»†ç²’åº¦å¥–åŠ±è®¾è®¡çš„èŒƒä¾‹ï¼Œå¯è¿ç§»åˆ°ä»£ç ç”Ÿæˆã€å¤æ‚å†³ç­–ç­‰éœ€åˆ†æ­¥è¯„ä¼°çš„åœºæ™¯ï¼›  
3. å¤šåœºæ™¯å·¥ç¨‹è½åœ°ï¼šä»Žç¦»çº¿æ•°æ®ç­›é€‰ã€åœ¨çº¿RLä¼˜åŒ–åˆ°æµ‹è¯•æ—¶å¢žå¼ºï¼Œå®Œæ•´è¦†ç›–å¤§æ¨¡åž‹è®­ç»ƒ - æŽ¨ç†å…¨æµç¨‹çš„å¥–åŠ±ç›‘ç£ï¼Œå±•ç¤ºäº†æŠ€æœ¯æ–¹æ¡ˆåœ¨äº§ä¸šçº§è½åœ°ä¸­çš„å¤šç»´åº¦ä»·å€¼ï¼Œä¸ºæ‰“é€ ç«¯åˆ°ç«¯çš„å¤§æ¨¡åž‹æŽ¨ç†å¢žå¼ºç®¡çº¿æä¾›äº†å®žè·µæ¨¡æ¿ï¼›  
4. èµ„æºåˆ†å±‚å‘å¸ƒï¼šåŒæ—¶æä¾›7Bå’Œ1.5Bè§„æ¨¡æ¨¡åž‹ï¼Œå…¼é¡¾é«˜æ€§èƒ½ä¸Žèµ„æºå—é™åœºæ™¯ï¼Œä½“çŽ°äº†æŠ€æœ¯æ™®æƒ æ€§ï¼Œåœ¨å®žé™…ä¸šåŠ¡ä¸­å¯æ ¹æ®ç®—åŠ›ã€å»¶è¿Ÿç­‰éœ€æ±‚çµæ´»é€‰æ‹©ï¼Œå¹³è¡¡æ•ˆæžœä¸Žæˆæœ¬ã€‚  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### ðŸŒŸ è®ºæ–‡è§£è¯» | å‘Šåˆ«ç¹çæ ‡æ³¨ï¼šGAN - RM è®©å¥–åŠ±å»ºæ¨¡â€œä»¥å‡ä¹±çœŸâ€

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§†è§‰ç”Ÿæˆæ¨¡åž‹çš„è®­ç»ƒåŽå¢žå¼ºä¸­ï¼Œå¥–åŠ±æ¨¡åž‹è‡³å…³é‡è¦ã€‚ç„¶è€Œå½“å‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•å­˜åœ¨è¯¸å¤šéš¾é¢˜ï¼šä¸€æ˜¯æž„å»ºå¥–åŠ±æ¨¡åž‹éœ€å¤§é‡äººå·¥æ ‡æ³¨åå¥½æ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œä¸”åŸºäºŽç‰¹å®šç”Ÿæˆæ¨¡åž‹è¾“å‡ºåŸŸæ ‡æ³¨çš„æ•°æ®ï¼Œåœ¨åº”ç”¨åˆ°ä¸åŒè¾“å‡ºåŸŸæ¨¡åž‹æ—¶å­˜åœ¨åŸŸå·®è·ï¼›äºŒæ˜¯ä¸ºå…¨é¢è¯„ä¼°ç”Ÿæˆå†…å®¹è´¨é‡ï¼Œéœ€äººå·¥è®¾è®¡å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¢å¢žåŠ å·¥ç¨‹æˆæœ¬ï¼Œåˆéš¾åœ¨ä¸åŒç»´åº¦é—´å–å¾—æœ€ä¼˜å¹³è¡¡ï¼Œè¿˜éš¾ä¿è¯ä¸Žäººç±»æ™®éåå¥½å¥‘åˆã€‚å› æ­¤ï¼Œæœ¬æ–‡å—ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸­å¯¹æŠ—è®­ç»ƒå¯å‘ï¼Œæå‡º GAN - RM æ¡†æž¶ï¼Œæ—¨åœ¨æ‘†è„±æ‰‹åŠ¨åå¥½æ ‡æ³¨å’Œæ˜¾å¼è´¨é‡ç»´åº¦è®¾è®¡ï¼Œé«˜æ•ˆæž„å»ºå¥–åŠ±æ¨¡åž‹ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— éœ€æ‰‹åŠ¨åå¥½æ ‡æ³¨ï¼Œåˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®
GAN - RM ä»…éœ€å°‘é‡ï¼ˆå‡ ç™¾ä¸ªï¼‰æ— æ ‡æ³¨çš„ä»£è¡¨æ€§æ ·æœ¬ï¼ˆå³åå¥½ä»£ç†æ•°æ®ï¼ŒPreference Proxy Dataï¼‰ä½œä¸ºå¤–éƒ¨æ•°æ®ã€‚é€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡åž‹åŒºåˆ†åå¥½ä»£ç†æ•°æ®å’Œç”Ÿæˆæ¨¡åž‹è¾“å‡ºï¼Œè®©æ¨¡åž‹å­¦ä¹ è¯„ä¼°ç”Ÿæˆæ ·æœ¬ã€‚åŒæ—¶é‡‡ç”¨åŸºäºŽæŽ’åçš„è‡ªä¸¾ç­–ç•¥ï¼Œå°† GAN - RM åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„ç½®ä¿¡åˆ†æ•°ä½œä¸ºè½¯æ ‡ç­¾ï¼Œåˆ©ç”¨é¢å¤–æ•°æ®å†è®­ç»ƒ GAN - RMï¼Œä½¿å…¶æ›´å¥½æ•æ‰æ½œåœ¨äººç±»åå¥½ã€‚
ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šè½®è®­ç»ƒï¼Œè¿­ä»£å¯¹é½åå¥½
GAN - RM æ”¯æŒå¤šè½®è®­ç»ƒåŽä¼˜åŒ–ã€‚æ¯ä¸€è½®ä¸­ï¼Œå°†è¢«è¯†åˆ«ä¸ºæŽ¥è¿‘åå¥½ä»£ç†æ•°æ®çš„æ ·æœ¬ç”¨äºŽç”Ÿæˆå™¨çš„è®­ç»ƒåŽä¼˜åŒ–ï¼Œåè¿‡æ¥å†è®­ç»ƒåˆ¤åˆ«å™¨ä»¥åŒºåˆ†è¿™äº›æ›´éš¾çš„æ ·æœ¬ã€‚è¿™ç§è¿­ä»£çš„â€œä»¥å‡ä¹±çœŸâ€è¿‡ç¨‹èƒ½é€æ­¥è®©ç”Ÿæˆè´¨é‡ä¸Žåå¥½ä»£ç†æ•°æ®ä¸­çš„æ½œåœ¨äººç±»åå¥½å¯¹é½ã€‚

### ðŸ“ˆ å®žéªŒç»“æžœ
å®žéªŒè¡¨æ˜Žï¼ŒåŸºäºŽ GAN - RM çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯ä¸Žä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼ˆå¦‚ Pickapic çš„ 100 ä¸‡æ ‡æ³¨äººç±»åå¥½æ•°æ®ï¼‰çš„æ–¹æ³•ï¼ˆå¦‚ç›¸å…³å¯¹æ¯”æ–¹æ³•ï¼‰ç›¸å½“ç”šè‡³è¶…è¶Šã€‚åœ¨å›¾åƒè´¨é‡å®žéªŒè®¾ç½®ä¸­ï¼ŒGAN - RM ä»…éœ€ 500 ä¸ªåå¥½ä»£ç†æ•°æ®æ ·æœ¬ã€‚é™¤å›¾åƒè´¨é‡æå‡å®žéªŒå¤–ï¼Œåœ¨å›¾åƒå®‰å…¨å’Œè§†é¢‘è´¨é‡å¢žå¼ºåœºæ™¯ä¸‹çš„å®žéªŒä¹Ÿå‡¸æ˜¾äº† GAN - RM æ¡†æž¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆå¦‚ Best - of - N æ ·æœ¬è¿‡æ»¤ï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æŽ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰è®­ç»ƒåŽæ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»Žæ–¹æ³•åˆ›æ–°è§’åº¦ï¼ŒGAN - RM ä¸ºè§£å†³å¥–åŠ±å»ºæ¨¡ä¸­æ•°æ®èŽ·å–éš¾ã€ä¾èµ–ç‰¹å®šåŸŸã€äººå·¥è®¾è®¡ç»´åº¦éš¾å¥‘åˆäººç±»åå¥½ç­‰é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå…¶åˆ©ç”¨å¯¹æŠ—è®­ç»ƒå’Œå°‘é‡ä»£ç†æ•°æ®çš„æ–¹å¼ï¼Œå‡å°‘äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œé™ä½Žå·¥ç¨‹æˆæœ¬ï¼›ä»Žåº”ç”¨æ‹“å±•è§’åº¦ï¼Œè¯¥æ¡†æž¶åœ¨å›¾åƒã€è§†é¢‘ç­‰å¤šåœºæ™¯çš„æœ‰æ•ˆå®žéªŒï¼Œä¸ºè§†è§‰ç”Ÿæˆæ¨¡åž‹åœ¨ä¸åŒé¢†åŸŸçš„è®­ç»ƒåŽå¢žå¼ºæä¾›äº†å¯å¤ç”¨çš„å¥–åŠ±å»ºæ¨¡èŒƒå¼ï¼ŒåŽç»­åœ¨è§†è§‰ç”Ÿæˆç›¸å…³ä»»åŠ¡ä¸­ï¼Œè‹¥éœ€æž„å»ºå¥–åŠ±æ¨¡åž‹ï¼Œå¯å€Ÿé‰´å…¶åˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®å’Œå¯¹æŠ—è®­ç»ƒçš„æ€è·¯æ¥é™ä½Žæˆæœ¬ä¸Žéš¾åº¦ã€‚

## $\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts
### Abstract
Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.
### ðŸŒŸ è®ºæ–‡è§£è¯» | SPECSï¼šç”¨â€œæŽ¨æµ‹è‰ç¨¿â€åŠ é€Ÿå¤§æ¨¡åž‹æŽ¨ç†ï¼Œå¹³è¡¡å»¶è¿Ÿä¸Žç²¾åº¦

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æŽ¨ç†èƒ½åŠ›çš„æå‡å¸¸ä¾èµ–â€œæµ‹è¯•æ—¶ç®—åŠ›æ‰©å®¹â€ï¼Œæ¯”å¦‚åˆ†é…æ›´å¤šè®¡ç®—èµ„æºåšæ›´å……åˆ†çš„æŽ¢ç´¢ã€‚ä½†ç®—åŠ›å¢žåŠ å¾€å¾€å¯¼è‡´ç”¨æˆ·ä¾§å»¶è¿Ÿå‡é«˜ï¼Œç›´æŽ¥å½±å“ä½“éªŒã€‚çŽ°æœ‰æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•å¤šèšç„¦ç®—åŠ›ï¼ˆFLOPSï¼‰ä¼˜åŒ–ç²¾åº¦ï¼Œå´å¿½ç•¥å»¶è¿Ÿçº¦æŸã€‚æ­¤å¤–ï¼ŒåŸºäºŽTransformerçš„LLMè‡ªå›žå½’é‡‡æ ·å»¶è¿Ÿå¸¸å—é™äºŽå†…å­˜åŠ è½½è€Œéžæ€»ç®—åŠ›ï¼Œè€ŒæŽ¨æµ‹è§£ç è™½èƒ½å€Ÿå°æ¨¡åž‹æå€™é€‰ token é™å»¶è¿Ÿï¼Œå´ä¼šå¢žåŠ æ€»è®¡ç®—é‡ã€‚äºŽæ˜¯ï¼Œè®ºæ–‡è¯•å›¾å›žç­”ï¼š**èƒ½å¦è®¾è®¡é«˜æ•ˆæµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ï¼Œä¼˜åŒ–å»¶è¿Ÿ - æ•ˆç”¨æƒè¡¡ï¼Ÿ**

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º SPECS ç®—æ³•æ¡†æž¶  
SPECS å—æŽ¨æµ‹è§£ç å¯å‘ï¼Œæ˜¯ä¸€ç§â€œå»¶è¿Ÿæ„ŸçŸ¥åž‹â€æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ã€‚å®ƒç”¨**æ›´å°æ›´å¿«çš„è‰ç¨¿æ¨¡åž‹**é«˜æ•ˆç”Ÿæˆå€™é€‰åºåˆ—ï¼Œå†ç»“åˆ**æ›´å¤§çš„ç›®æ ‡æ¨¡åž‹**ä¸Ž**ä¸“ç”¨å¥–åŠ±æ¨¡åž‹**è¯„ä¼°å€™é€‰ã€‚æ•´ä½“éµå¾ªâ€œè‰ç¨¿ - é€‰æ‹©â€æµç¨‹ï¼šè¿­ä»£ç”Ÿæˆå“åº”å—ï¼Œæ¯è½®ç”¨è‰ç¨¿æ¨¡åž‹ç”Ÿæˆå€™é€‰å—ï¼Œç»æ‰“åˆ†é€‰æ‹©åŽæ‹¼æŽ¥ï¼Œè¿›å…¥ä¸‹ä¸€è½®ï¼›è‹¥è‰ç¨¿å…¨è¢«æ‹’ï¼Œåˆ™åˆ‡æ¢ç›®æ ‡æ¨¡åž‹ç”Ÿæˆå€™é€‰ã€‚

ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¥–åŠ±å¼•å¯¼çš„è½¯éªŒè¯ä¸Žå»¶è¿Ÿæœºåˆ¶  
- å¥–åŠ±å¼•å¯¼è½¯éªŒè¯ï¼ˆSUBSAMPLE å­ä¾‹ç¨‹ï¼‰ï¼šåŸºäºŽè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡åž‹è®¡ç®—çš„â€œåˆ†æ•°â€é€‰å€™é€‰å—ï¼Œæ—¢ä¼˜åŒ–æ•ˆç”¨ - å»¶è¿Ÿæƒè¡¡ï¼Œä¹Ÿé¿å…ç®€å•ä¸¢å¼ƒé«˜å¥–åŠ±ä½†å¯èƒ½è¢« naive æŽ¨æµ‹è§£ç æ¼æŽ‰çš„è½¨è¿¹ã€‚  
- å¥–åŠ±æ„ŸçŸ¥å»¶è¿Ÿè§„åˆ™ï¼ˆCASCADE å­ä¾‹ç¨‹ï¼‰ï¼šè‡ªé€‚åº”å†³å®šä¸‹ä¸€è½®ç”¨è‰ç¨¿è¿˜æ˜¯ç›®æ ‡æ¨¡åž‹ç”Ÿæˆå€™é€‰â€”â€”è®©å¤§æ¨¡åž‹å¤„ç†éš¾é¢˜æ­¥éª¤ï¼Œå°æ¨¡åž‹å¤„ç†ç®€å•æ­¥éª¤ï¼ŒåŠ¨æ€å¹³è¡¡ç®—åŠ›ä¸Žå»¶è¿Ÿã€‚  

ðŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†è®ºåˆ†æžä¿éšœæ”¶æ•›æ€§  
ä»Žç†è®ºä¸Šåˆ†æžï¼ŒSPECS åœ¨ç»“åˆè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡åž‹ä¼˜åŒ–â€œKL æ­£åˆ™åŒ–å¥–åŠ±æœ€å¤§åŒ–â€ç›®æ ‡æ—¶ï¼Œå…¶è½¯éªŒè¯æ–¹æ³•éš beam å®½åº¦å¢žå¤§ï¼Œèƒ½ä¼˜é›…æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚


### ðŸ“ˆ å®žéªŒç»“æžœ
è®ºæ–‡åœ¨ MATH500ã€AMC23ã€OlympiadBench æ•°æ®é›†æµ‹è¯•ï¼Œç”¨ Qwen - 1.5B - Instructï¼ˆè‰ç¨¿æ¨¡åž‹ï¼‰ã€Qwen - 7B - Instructï¼ˆç›®æ ‡æ¨¡åž‹ï¼‰ä¸Ž Qwen - 7B - Math - PRMï¼ˆå¥–åŠ±æ¨¡åž‹ï¼‰éªŒè¯ï¼š  
- ç²¾åº¦å±‚é¢ï¼šSPECS åŒ¹é…ç”šè‡³è¶…è¶Š beam search ç²¾åº¦ï¼›  
- å»¶è¿Ÿå±‚é¢ï¼šå»¶è¿Ÿæœ€å¤šé™ä½Žçº¦ 19.1%ï¼Œåœ¨ç²¾åº¦ä¸Žå»¶è¿Ÿé—´å®žçŽ°æ›´ä¼˜æƒè¡¡ã€‚  


### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å»¶è¿Ÿ - ç²¾åº¦æƒè¡¡æ€è·¯**ï¼šè·³å‡ºâ€œåªçœ‹ç®—åŠ›/ç²¾åº¦â€çš„æ€ç»´å®šå¼ï¼ŒæŠŠå»¶è¿Ÿä½œä¸ºæ ¸å¿ƒçº¦æŸï¼Œä¸ºå¤§æ¨¡åž‹è½åœ°ä½Žå»¶è¿Ÿåœºæ™¯ï¼ˆå¦‚ä¸ªæ€§åŒ–äº¤äº’ï¼‰æä¾›æ–°æ€è·¯ï¼›  
2. **å¤šæ¨¡åž‹åä½œèŒƒå¼**ï¼šç”¨â€œå°è‰ç¨¿æ¨¡åž‹ + å¤§ç›®æ ‡æ¨¡åž‹ + å¥–åŠ±æ¨¡åž‹â€åˆ†å±‚åä½œï¼Œæ—¢åˆ©ç”¨å°æ¨¡åž‹æé€Ÿï¼Œåˆé å¤§æ¨¡åž‹ä¿ç²¾åº¦ï¼Œè¿˜å€Ÿå¥–åŠ±æ¨¡åž‹åšçµæ´»é€‰æ‹©ï¼Œè¿™ç§â€œåˆ†å·¥â€æ¨¡å¼å¯è¿ç§»åˆ°å…¶ä»–éœ€å¹³è¡¡èµ„æºä¸Žæ•ˆæžœçš„ä»»åŠ¡ï¼›  
3. **ç†è®º + å®žéªŒåŒéªŒè¯**ï¼šä»Žç†è®ºè¯æ˜Žæ”¶æ•›æ€§ï¼Œå†ç”¨çœŸå®žæ•°æ®é›†éªŒè¯ï¼Œä¸ºæ–¹æ³•å¯é æ€§èƒŒä¹¦ï¼Œä¹Ÿç¤ºèŒƒäº†å­¦æœ¯ç ”ç©¶ä¸­â€œæ–¹æ³• - ç†è®º - å®žéªŒâ€é—­çŽ¯çš„é‡è¦æ€§ã€‚  


SPECS ä¸ºå¤§æ¨¡åž‹æŽ¨ç†çš„â€œå»¶è¿Ÿ - ç²¾åº¦â€éš¾é¢˜æä¾›äº†ä¸€å¥—å…¼å…·åˆ›æ–°æ€§ä¸Žå®žç”¨æ€§çš„è§£æ³•ï¼Œæ— è®ºæ˜¯å·¥ä¸šç•Œè½åœ°ä½Žå»¶è¿Ÿ LLM åº”ç”¨ï¼Œè¿˜æ˜¯å­¦æœ¯ç•ŒæŽ¢ç´¢æµ‹è¯•æ—¶ä¼˜åŒ–æ–°æ–¹å‘ï¼Œéƒ½æœ‰ä¸å°‘å¯å€Ÿé‰´çš„é—ªå…‰ç‚¹~

## eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling
### Abstract
Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.
### ðŸŒŸ è®ºæ–‡è§£è¯» | EQA - RMï¼šä¸ºå…·èº«é—®ç­”é‡èº«å®šåˆ¶çš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡åž‹ï¼Œå®žçŽ°æµ‹è¯•æ—¶å¯æ‰©å±•è¯„ä¼°

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡åž‹ï¼ˆRMsï¼‰åœ¨å¤§æ¨¡åž‹å¯¹é½ä¸­è‡³å…³é‡è¦ï¼Œä½†åœ¨å¤æ‚çš„å…·èº«ä»»åŠ¡ï¼ˆå¦‚å…·èº«é—®ç­”ï¼ŒEQAï¼‰ä¸­å´æœªå¾—åˆ°å……åˆ†æŽ¢ç´¢ã€‚EQAéœ€è¦æ™ºèƒ½ä½“åœ¨3DçŽ¯å¢ƒä¸­é€šè¿‡å¤šæ¨¡æ€è§‚å¯Ÿå’ŒåŠ¨ä½œåºåˆ—æ¥æ„ŸçŸ¥ã€äº¤äº’å’ŒæŽ¨ç†ä»¥å›žç­”é—®é¢˜ï¼Œå¯¹æ™ºèƒ½ä½“çš„ç©ºé—´ã€æ—¶é—´å’Œé€»è¾‘ç†è§£è¿›è¡Œç»†è‡´è¯„ä¼°è‡³å…³é‡è¦ï¼Œè€Œé€šç”¨çš„å¥–åŠ±æ¨¡åž‹æ–¹æ³•æ— æ³•æ»¡è¶³è¿™ä¸€éœ€æ±‚ã€‚çŽ°æœ‰é€šç”¨å¥–åŠ±æ¨¡åž‹å¤šä¸ºé™æ€è¾“å…¥æˆ–ç®€å•ç»“æžœè®¾è®¡ï¼Œéš¾ä»¥æ•æ‰å…·èº«ä»»åŠ¡ä¸­å›ºæœ‰çš„æ—¶ç©ºå’Œé€»è¾‘ä¾èµ–å…³ç³»ï¼Œå› æ­¤è¿«åˆ‡éœ€è¦ä¸“é—¨çš„æœºåˆ¶æ¥å‡†ç¡®è¯„ä¼°EQAçš„å¤šæ–¹é¢æˆåŠŸæŒ‡æ ‡ã€‚åŒæ—¶ï¼ŒEQAé¢†åŸŸç¼ºä¹ç”¨äºŽä¸¥æ ¼è¯„ä¼°å’Œæ¯”è¾ƒå¥–åŠ±æ¨¡åž‹çš„æ ‡å‡†åŒ–åŸºå‡†ï¼Œå½“å‰EQAä»»åŠ¡åŸºå‡†ä¾§é‡äºŽç²—ç•¥çš„æˆåŠŸæŒ‡æ ‡ï¼Œè€Œéžå¯¹å¥–åŠ±æ¨¡åž‹å‘å±•è‡³å…³é‡è¦çš„ç»†ç²’åº¦è½¨è¿¹è´¨é‡è¯„ä¼°ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEQA - RMç”Ÿæˆå¼å¤šæ¨¡æ€å¥–åŠ±æ¨¡åž‹
EQA - RMæ˜¯ä¸“ä¸ºè¯„ä¼°EQAè½¨è¿¹è€Œè®¾è®¡çš„æ–°åž‹å¤šæ¨¡æ€å¥–åŠ±æ¨¡åž‹ï¼Œä½œä¸ºç”Ÿæˆå¼å¥–åŠ±æ¨¡åž‹ï¼ˆGenRMï¼‰ï¼Œä¸ä»…èƒ½äº§ç”Ÿæ ‡é‡å¥–åŠ±ï¼Œè¿˜èƒ½ä¸ºè¯„ä¼°æä¾›æ˜Žç¡®çš„æŽ¨ç†è¿‡ç¨‹ã€‚å…¶å…·æœ‰å¢žå¼ºçš„ç©ºé—´ã€æ—¶é—´å’ŒæŽ¨ç†å¤„ç†èƒ½åŠ›ï¼Œä»¥å¤„ç†EQAä»»åŠ¡ä¸­å›ºæœ‰çš„ç‹¬ç‰¹å¤šæ¨¡æ€æ•°æ®æµã€‚é€šè¿‡é«˜æ•ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µç”¨æ ‡å‡†çš„Rejective Finetuningï¼ˆRFTï¼‰æ•™ä¼šæ¨¡åž‹æœŸæœ›çš„è¾“å‡ºæ ¼å¼ï¼ˆåŒ…å«æ–‡æœ¬æ‰¹è¯„å’Œæ ‡é‡åˆ†æ•°ï¼‰ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åˆ›æ–°çš„Contrastive Group Relative Policy Optimizationï¼ˆC - GRPOï¼‰å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³ä»…RFTå¯èƒ½åªå­¦æ ¼å¼ä¸å­¦å†…å®¹çš„é—®é¢˜ï¼Œåˆ©ç”¨åŸºäºŽè§„åˆ™çš„å¯¹æ¯”å¥–åŠ±ï¼ˆæºäºŽé’ˆå¯¹æ€§çš„æ•°æ®å¢žå¼ºï¼Œå¦‚è§†é¢‘å¸§æ‰“ä¹±ã€ç©ºé—´åŒºåŸŸéšæœºæŽ©ç ã€æŽ¨ç†æ­¥éª¤æ··ä¹±ç­‰æ‰°åŠ¨æ–¹å¼ï¼‰ï¼Œè®©æ¨¡åž‹åŒºåˆ†åŽŸå§‹è¿žè´¯ä¸Šä¸‹æ–‡å’Œåˆæˆæ‰°åŠ¨ä¸Šä¸‹æ–‡ä¸‹çš„ç­–ç•¥è¾“å‡ºï¼Œä»Žè€Œå†…åŒ–æ—¶é—´é¡ºåºã€ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚å’Œè¿žè´¯é€»è¾‘æµçš„é‡è¦æ€§ï¼ŒåŸ¹å…»å¯¹å…·èº«ä»»åŠ¡å¼ºå¤§ä¸”æ•é”çš„è¯„ä¼°èƒ½åŠ›ã€‚

ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šæž„å»ºEQARewardBenchåŸºå‡†
ä¸ºè§£å†³EQAé¢†åŸŸå¥–åŠ±æ¨¡åž‹è¯„ä¼°åŸºå‡†ç¼ºå¤±çš„é—®é¢˜ï¼ŒåŸºäºŽOpenEQAæž„å»ºäº†EQARewardBenchã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªHM3Då’ŒScanNetä¸¤ç§å®¶åº­çŽ¯å¢ƒçš„å…·èº«æƒ…èŠ‚è®°å¿†è§†é¢‘ï¼Œä»ŽåŽŸå§‹é—®ç­”å¯¹æž„å»ºæ›´å…¨é¢çš„é—®é¢˜ - å“åº” - æŽ¨ç†è½¨è¿¹ä¸‰å…ƒç»„ï¼Œæœ‰1546ä¸ªæµ‹è¯•å®žä¾‹ï¼Œç”¨äºŽè¯„ä¼°å¥–åŠ±æ¨¡åž‹åœ¨è½¨è¿¹è´¨é‡çš„å…«ä¸ªä¸åŒæ–¹é¢ï¼ˆå¦‚æ­£ç¡®æ€§ã€æŽ¥åœ°æ€§ã€æ•ˆçŽ‡ç­‰ï¼‰ï¼Œä¸ºEQAä»»åŠ¡ä¸Šçš„å¥–åŠ±æ¨¡åž‹æä¾›äº†æ ‡å‡†åŒ–ã€å¯æ¯”è¾ƒçš„è¯„ä¼°å¹³å°ã€‚

### ðŸ“ˆ å®žéªŒç»“æžœ
ä»¥Qwen2 - VL - 2B - Instructä¸ºåŸºç¡€è¿›è¡Œå¾®è°ƒçš„EQA - RMå±•çŽ°å‡ºé«˜æ ·æœ¬æ•ˆçŽ‡ï¼Œä»…ç”¨700ä¸ªæ ·æœ¬åœ¨EQA - RM - Benchä¸Šè¾¾åˆ°61.9%çš„å‡†ç¡®çŽ‡ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„ä¸“æœ‰åŸºçº¿ï¼ˆå¦‚Gemini - 2.5 - Flashã€GPT - 4oã€Claude - 3.5 - Haikuï¼‰å’Œå¼€æºçš„æœ€å…ˆè¿›æ¨¡åž‹ï¼ˆå¦‚RoVRMå’ŒVisualPRMï¼‰ã€‚åŒæ—¶ï¼ŒEQA - RMå±•ç¤ºäº†æµ‹è¯•æ—¶å¯æ‰©å±•æ€§ï¼Œåœ¨æŽ¨ç†æ—¶å¢žåŠ è¯„ä¼°è®¡ç®—é‡ï¼Œå…¶åœ¨EQARewardBenchä¸Šçš„å‡†ç¡®çŽ‡ä»Ž42.47%æå‡åˆ°61.86%ï¼Œæ€§èƒ½æå‡åŽåœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢†å…ˆçš„å¤§åž‹å•†ä¸šæ¨¡åž‹ã€‚

### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é’ˆå¯¹ç‰¹å®šå¤æ‚ä»»åŠ¡è®¾è®¡ä¸“ç”¨å¥–åŠ±æ¨¡åž‹ï¼šå½“é€šç”¨æ¨¡åž‹æ— æ³•æ»¡è¶³å¤æ‚ä»»åŠ¡ï¼ˆå¦‚å…·èº«ä»»åŠ¡ï¼‰çš„è¯„ä¼°éœ€æ±‚æ—¶ï¼Œå¯åƒEQA - RMä¸€æ ·é’ˆå¯¹ä»»åŠ¡ç‰¹æ€§ï¼Œè®¾è®¡å…·å¤‡ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚ç©ºé—´ã€æ—¶é—´ã€æŽ¨ç†å¤„ç†èƒ½åŠ›ï¼‰çš„ä¸“ç”¨æ¨¡åž‹ï¼Œè§£å†³é€šç”¨æ¨¡åž‹çš„å±€é™æ€§ã€‚
2. åˆ›æ–°çš„è®­ç»ƒç­–ç•¥ï¼šä¸¤é˜¶æ®µè®­ç»ƒï¼ˆRFT + C - GRPOï¼‰ä»¥åŠåˆ©ç”¨æ•°æ®å¢žå¼ºçš„å¯¹æ¯”å¥–åŠ±ç­–ç•¥ï¼Œä¸ºè§£å†³æ¨¡åž‹åªå­¦å½¢å¼ä¸å­¦å†…å®¹ã€æå‡æ¨¡åž‹å¯¹ä»»åŠ¡å…³é”®è¦ç´ çš„ç†è§£æä¾›äº†æ€è·¯ï¼Œå¯å€Ÿé‰´äºŽå…¶ä»–éœ€è¦æ¨¡åž‹æ·±å…¥ç†è§£ä»»åŠ¡ç»†èŠ‚çš„è®­ç»ƒåœºæ™¯ã€‚
3. æž„å»ºé¢†åŸŸåŸºå‡†ï¼šå¯¹äºŽç¼ºä¹è¯„ä¼°åŸºå‡†çš„é¢†åŸŸï¼Œå¯åƒæž„å»ºEQARewardBenchä¸€æ ·ï¼ŒåŸºäºŽçŽ°æœ‰æ•°æ®é›†æž„å»ºä¸“é—¨çš„åŸºå‡†ï¼ŒæŽ¨åŠ¨é¢†åŸŸå†…æ¨¡åž‹çš„è¯„ä¼°å’Œå‘å±•ï¼Œä¸ºæ¨¡åž‹æ€§èƒ½æ¯”è¾ƒå’Œæ”¹è¿›æä¾›æ ‡å‡†å¹³å°ã€‚

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### ðŸŒŸ è®ºæ–‡è§£è¯» | Athenaï¼šç”¨æ•°æ®é«˜æ•ˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹æå‡å¤šæ¨¡æ€æŽ¨ç†èƒ½åŠ›

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§£å†³å¤æ‚æŽ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œå¤šæ­¥éª¤æŽ¨ç†ï¼‰ä»å…·æŒ‘æˆ˜ã€‚ä¸ºå¢žå¼ºæŽ¨ç†èƒ½åŠ›ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ç­‰æ–¹æ³•è¢«æŽ¢ç´¢ï¼Œå…¶ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ï¼ˆPRMsï¼‰èƒ½ä¸ºä¸­é—´æŽ¨ç†æ­¥éª¤æä¾›ç»†ç²’åº¦åé¦ˆï¼Œæ€§èƒ½æ›´ä¼˜ä¸”æ³›åŒ–æ€§å¼ºã€‚ç„¶è€Œï¼ŒPRMs å‘å±•é¢ä¸´ä¸¤å¤§éš¾é¢˜ï¼šä¸€æ˜¯èŽ·å–å¸¦è¿‡ç¨‹æ ‡ç­¾çš„é«˜è´¨é‡æ•°æ®æˆæœ¬é«˜ï¼ˆéœ€å¤§é‡äººå·¥æ ‡æ³¨æˆ–è®¡ç®—æ˜‚è´µçš„è‡ªåŠ¨åŒ–æ ‡æ³¨ï¼‰ï¼›äºŒæ˜¯ä¼ ç»Ÿè‡ªåŠ¨åŒ–æ ‡æ³¨ï¼ˆå¦‚è’™ç‰¹å¡æ´›ä¼°è®¡ï¼‰æ˜“äº§ç”Ÿå™ªå£°æ ‡ç­¾ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé™ä½Žè®¡ç®—æˆæœ¬å¹¶å‡è½»æ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œæå‡ PRMs æ€§èƒ½ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¼ºå¼±å®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç”Ÿæˆé«˜è´¨é‡è¿‡ç¨‹æ ‡ç­¾  
ä¼ ç»Ÿè’™ç‰¹å¡æ´›ç­‰è‡ªåŠ¨åŒ–æ ‡æ³¨æ–¹æ³•æ˜“å—å®Œæˆå™¨æŽ¨ç†èƒ½åŠ›å½±å“ï¼Œæ ‡ç­¾æœ‰å™ªå£°ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡å‘çŽ°ï¼Œå¼ºå®Œæˆå™¨å³ä¾¿ä¸­é—´æ­¥éª¤é”™è¯¯ä»èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œå¼±å®Œæˆå™¨åˆ™å¯èƒ½åœ¨ä¸­é—´æ­¥éª¤æ­£ç¡®æ—¶ä¹Ÿå¤±è´¥ã€‚åŸºäºŽæ­¤ï¼Œæå‡ºç”¨å¼±ã€å¼ºå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ä½œä¸ºç­›é€‰å¯é è¿‡ç¨‹æ ‡ç­¾çš„æ ‡å‡†ï¼Œä¿ç•™ä¸¤è€…æ ‡ç­¾ä¸€è‡´çš„æ­¥éª¤ï¼Œå‡å°‘å®Œæˆå™¨å¸¦æ¥çš„åå·®ï¼Œæå‡æ ‡ç­¾è´¨é‡ã€‚å®žéªŒè¡¨æ˜Žï¼Œçº¦ 5000 æ¡é«˜è´¨é‡æ ‡ç­¾å°±èƒ½æ¯”ä¼ ç»Ÿæ–¹æ³•çº¦ 30 ä¸‡æ¡å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¡¨çŽ°æ›´ä¼˜ï¼Œä¸”å¤§å¹…é™ä½Žæ•°æ®åˆæˆå’Œæ¨¡åž‹è®­ç»ƒçš„è®¡ç®—æˆæœ¬ã€‚

ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ PRMs æ€§èƒ½çš„ä¸¤å¤§ç­–ç•¥  
 - ORM åˆå§‹åŒ–ï¼šPRMs é€šå¸¸åŸºäºŽé¢„è®­ç»ƒåŸºç¡€æ¨¡åž‹å¾®è°ƒï¼Œè€Œç»“æžœå¥–åŠ±æ¨¡åž‹ï¼ˆORMsï¼‰åœ¨å¤§è§„æ¨¡å“åº”çº§æ•°æ®ä¸Šè®­ç»ƒï¼Œå…·å¤‡å¼±ç›‘ç£ä¸‹è¯„ä¼°ä¸­é—´æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚å› æ­¤ç”¨ ORMs åˆå§‹åŒ– PRMsï¼Œå°† ORMs ä½œä¸ºå¼±ç›‘ç£é¢„è®­ç»ƒï¼ŒPRMs å†åœ¨é«˜è´¨é‡ç»†ç²’åº¦æ­¥éª¤æ•°æ®ä¸Šå¾®è°ƒï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚  
 - è´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ï¼šè¿‡ç¨‹æ ‡ç­¾æ•°æ®å­˜åœ¨æ ‡ç­¾ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡å¯¹å«è´Ÿæ­¥éª¤æ ‡ç­¾çš„æ•°æ®è¿›è¡Œä¸Šé‡‡æ ·ï¼Œè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡é—®é¢˜ï¼Œä¼˜åŒ–æ¨¡åž‹è®­ç»ƒã€‚  

ðŸ’¡ åˆ›æ–°ç‚¹3ï¼šæž„å»º Athena ç³»åˆ—æ¨¡åž‹å¹¶å¤šåœºæ™¯éªŒè¯  
åŸºäºŽä¸Šè¿°æ–¹æ³•æž„å»ºç»“æžœå¥–åŠ±æ¨¡åž‹ Athena - ORM å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ Athena - PRMï¼Œå†åˆ©ç”¨ Athena - PRM é€šè¿‡å¥–åŠ±æŽ’åºå¾®è°ƒå¾—åˆ° Athena - 7Bã€‚å¹¶åœ¨ä¸‰ä¸ªåœºæ™¯éªŒè¯ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­å¯¹ç­–ç•¥æ¨¡åž‹ç”Ÿæˆçš„å¤šä¸ªè¾“å‡ºæŽ’åºï¼›ç›´æŽ¥è¯„ä¼°æŽ¨ç†æ­¥éª¤æ­£ç¡®æ€§ï¼›å¥–åŠ±æŽ’åºå¾®è°ƒï¼ˆç”¨é«˜å¥–åŠ±å“åº”å¾®è°ƒç­–ç•¥æ¨¡åž‹ï¼‰ã€‚

### ðŸ“ˆ å®žéªŒç»“æžœ
- æµ‹è¯•æ—¶ç¼©æ”¾åœºæ™¯ï¼šåœ¨ 7 ä¸ªå¤šæ¨¡æ€æ•°å­¦å’ŒæŽ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç”¨ Athena - PRM é…åˆä¸åŒè§„æ¨¡ï¼ˆ7B åˆ° 72Bï¼‰ç­–ç•¥æ¨¡åž‹ï¼ŒæŽ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ã€‚å¦‚ç”¨ Qwen2.5 - VL - 7B ä½œä¸ºç­–ç•¥æ¨¡åž‹æ—¶ï¼Œåœ¨ WeMath åŸºå‡†ä¸Šé›¶æ ·æœ¬åŸºçº¿æå‡ 10.2 åˆ†ï¼Œåœ¨ MathVista æå‡ 7.1 åˆ†ï¼›åœ¨æ–‡æœ¬-only æ•°å­¦åŸºå‡†ç”¨ Mistral - 8B æ—¶æå‡ 8.9 åˆ†ã€‚  
- æŽ¨ç†æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°åœºæ™¯ï¼šåœ¨ VisualProcessBench åŸºå‡†ä¸Šï¼ŒAthena - PRM è¡¨çŽ°å¼ºåŠ²ï¼Œè¶…è¶Šå¼€æºçš„ VisualPRM - 8B ç­‰æ¨¡åž‹ï¼ŒF1 åˆ†æ•°æ¯”ä¹‹å‰æœ€ä¼˜ç»“æžœé«˜ 3.9ï¼Œå±•çŽ°å‡†ç¡®è¯„ä¼°æŽ¨ç†æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚  
- å¥–åŠ±æŽ’åºå¾®è°ƒåœºæ™¯ï¼šåŸºäºŽ Qwen2.5 - VL - 7B å¾®è°ƒå¾—åˆ°çš„ Athena - 7Bï¼Œåœ¨ 7 ä¸ªæ•°å­¦å’ŒæŽ¨ç†åŸºå‡†ä¸Šå¤§å¹…æå‡ç­–ç•¥æ¨¡åž‹æŽ¨ç†èƒ½åŠ›ã€‚  

### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ•°æ®é«˜æ•ˆæ ‡æ³¨æ€è·¯ï¼šåˆ©ç”¨å¤šå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç­›é€‰æ ‡ç­¾ï¼Œä¸ºè§£å†³éœ€ç»†ç²’åº¦æ ‡æ³¨ä¸”æ ‡æ³¨æˆæœ¬é«˜çš„ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ï¼Œåœ¨å‡å°‘æ•°æ®é‡åŒæ—¶æå‡æ•°æ®è´¨é‡ï¼Œå®žçŽ°æ•°æ®é«˜æ•ˆåˆ©ç”¨ã€‚  
- æ¨¡åž‹è®­ç»ƒç­–ç•¥ï¼šORM åˆå§‹åŒ–å’Œè´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ç­–ç•¥ï¼Œä¸ºæå‡å¥–åŠ±æ¨¡åž‹æ€§èƒ½æä¾›äº†å¯å¤ç”¨æ–¹æ³•ï¼Œå¯å¯å‘å…¶ä»–å¥–åŠ±æ¨¡åž‹æˆ–éœ€ç»†ç²’åº¦åé¦ˆæ¨¡åž‹çš„è®­ç»ƒä¼˜åŒ–ã€‚  
- å¤šåœºæ™¯éªŒè¯æ¨¡å¼ï¼šåœ¨æµ‹è¯•æ—¶ç¼©æ”¾ã€æ­¥éª¤è¯„ä¼°ã€æ¨¡åž‹å¾®è°ƒç­‰å¤šåœºæ™¯éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢éªŒè¯æ€è·¯æœ‰åŠ©äºŽæ›´å……åˆ†å±•ç¤ºæ–¹æ³•ä»·å€¼ï¼Œä¸ºåŽç»­ç ”ç©¶æä¾›éªŒè¯èŒƒå¼å‚è€ƒã€‚

## learning-to-reason-across-parallel-samples-for-llm-reasoning
### Abstract
Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.
### ðŸŒŸ è®ºæ–‡è§£è¯» | èžåˆå¹¶è¡Œä¸Žé¡ºåºæŽ¨ç†ï¼ŒSSAè®©å¤§æ¨¡åž‹æŽ¨ç†æ›´é«˜æ•ˆ

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸Šèƒ½åŠ›ä¸æ–­æå‡ï¼Œè€Œæµ‹è¯•æ—¶è®¡ç®—èµ„æºçš„åˆ†é…ï¼ˆå³æµ‹è¯•æ—¶ç¼©æ”¾ï¼‰æ˜¯ä¼˜åŒ–æ¨¡åž‹æ€§èƒ½çš„æ–°æ–¹å‘ã€‚çŽ°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åˆ†å¹¶è¡Œå’Œé¡ºåºä¸¤ç±»ï¼šå¹¶è¡Œç¼©æ”¾æ˜¯ç‹¬ç«‹ç”Ÿæˆå¤šæ¡æŽ¨ç†è·¯å¾„å†èšåˆï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨ï¼‰ï¼›é¡ºåºç¼©æ”¾åˆ™è¿­ä»£ä¼˜åŒ–å•ä¸ªè§£ï¼ˆå¦‚åŸºäºŽæç¤ºçš„è‡ªæˆ‘åæ€ï¼‰ã€‚ä½†å¹¶è¡Œæ–¹æ³•å¸¸å­¤ç«‹çœ‹å¾…æ ·æœ¬ï¼Œé¡ºåºæ–¹æ³•è®¡ç®—æˆæœ¬æˆ–é€‚é…æ€§å—é™ã€‚æœ¬æ–‡æ—¨åœ¨æå‡ºæ–°æ–¹æ³•ï¼ŒèžåˆäºŒè€…ä¼˜åŠ¿ï¼Œæ›´é«˜æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºæå‡æŽ¨ç†æ€§èƒ½ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSample Set Aggregatorï¼ˆSSAï¼‰æ¨¡åž‹æž¶æž„  
è®¾è®¡è½»é‡çº§çš„SSAæ¨¡åž‹ï¼Œå°†å…¶ä¸Žç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡åž‹ï¼ˆLMansï¼‰è§£è€¦ã€‚å…ˆç”±LManså¹¶è¡Œç”ŸæˆKä¸ªå€™é€‰ç­”æ¡ˆï¼Œå†æŠŠè¿™äº›å€™é€‰ç­”æ¡ˆæ‹¼æŽ¥æˆåºåˆ—è¾“å…¥SSAï¼ŒSSAé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä»¥è¾“å‡ºæœ€ç»ˆæ­£ç¡®ç­”æ¡ˆã€‚è¿™ç§è®¾è®¡è®©SSAèƒ½åŸºäºŽåŸºç¡€æ¨¡åž‹è¾“å‡ºçš„åˆ†å¸ƒç‰¹æ€§ï¼Œç›´æŽ¥ä¼˜åŒ–ç­”æ¡ˆåˆæˆè¿‡ç¨‹ï¼Œè€Œéžå­¤ç«‹è¯„ä¼°å•ä¸ªæ ·æœ¬ã€‚  

ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºŽè¾“å‡ºåˆ†å¸ƒæŽ¨ç†ï¼Œè§£è€¦è®­ç»ƒä¸ŽæŽ¨ç†  
SSAä¸ç›´æŽ¥è®­ç»ƒç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡åž‹ï¼ˆLManså¯è§†ä¸ºé»‘ç›’ï¼‰ï¼Œè€Œæ˜¯é’ˆå¯¹å…¶é‡‡æ ·è¾“å‡ºè¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§â€œæŽ¨ç†è¾“å‡ºåˆ†å¸ƒè€Œéžè°ƒæ•´æ¨¡åž‹å†…éƒ¨â€çš„æ€è·¯ï¼Œè®©æ–¹æ³•æ›´çµæ´»â€”â€”å¯é€‚é…ä¸åŒåŸºç¡€æ¨¡åž‹ï¼ˆç”šè‡³æ˜¯åªèƒ½é€šè¿‡APIè°ƒç”¨çš„é»‘ç›’å¤§æ¨¡åž‹ï¼‰ï¼Œåªéœ€ç”¨å…¶é‡‡æ ·ç­”æ¡ˆè®­ç»ƒSSAå³å¯ã€‚  

ðŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»Ÿä¸€å¹¶è¡Œä¸Žé¡ºåºç¼©æ”¾ä¼˜åŠ¿  
å¹¶è¡Œç¼©æ”¾èƒ½å¿«é€ŸèŽ·å–å¤šè§†è§’ç­”æ¡ˆï¼Œé¡ºåºç¼©æ”¾å¯è¿­ä»£ä¼˜åŒ–æŽ¨ç†ï¼›SSAé€šè¿‡â€œå¹¶è¡Œé‡‡æ ·+å•æ­¥é¡ºåºRLèšåˆâ€çš„æ–¹å¼ï¼Œåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç»“åˆäºŒè€…é•¿å¤„ï¼šç”¨å¹¶è¡ŒèŽ·å–å¤šæ ·æ€§ï¼Œç”¨SSAçš„é¡ºåºæŽ¨ç†å®žçŽ°ç²¾å‡†èšåˆï¼Œä¸”ä»…éœ€è®­ç»ƒå°æ¨¡åž‹å°±èƒ½å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚  


### ðŸ“ˆ å®žéªŒç»“æžœ
1. æ€§èƒ½è¶…è¶Šå¼ºåŸºçº¿ï¼šåœ¨å¤šä¸ªæ•°å­¦æŽ¨ç†æ•°æ®é›†ä¸Šï¼ŒSSAç›¸æ¯”åŸºäºŽå¥–åŠ±æ¨¡åž‹é‡æŽ’åºç­‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•è¡¨çŽ°æ›´ä¼˜ï¼Œå¤§å¹…ç¼©å°äº†æ¨¡åž‹å®žé™…æ€§èƒ½ä¸Žâ€œç†è®ºæœ€ä¼˜ï¼ˆoracle - bestï¼‰â€ç²¾åº¦çš„å·®è·ã€‚  
2. æ³›åŒ–èƒ½åŠ›çªå‡ºï¼šè·¨æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡åž‹å®¶æ—ï¼ˆå¦‚Qwen 2.5ã€Llama 3.1ï¼‰ã€æ¨¡åž‹è§„æ¨¡ï¼ˆ7B/14B/32Bï¼‰å’Œä»»åŠ¡ï¼ŒSSAéƒ½å±•çŽ°å‡ºè‰¯å¥½æ³›åŒ–æ€§ã€‚æ¯”å¦‚åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šä¸ºç‰¹å®šæ¨¡åž‹è®­ç»ƒçš„SSAï¼Œèƒ½æˆåŠŸèšåˆä¸åŒæ¨¡åž‹å®¶æ—ã€è§„æ¨¡åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¾“å‡ºã€‚  
3. è½»é‡åŒ–ä¼˜åŠ¿ï¼šç´§å‡‘çš„SSAæ¨¡åž‹èƒ½åŒ¹é…é¡ºåºç¼©æ”¾ä¸­ç»å¼ºåŒ–è®­ç»ƒçš„å¤§æ¨¡åž‹æ€§èƒ½ï¼Œè¯æ˜Žå…¶ä½œä¸ºè½»é‡é¡ºåºç¼©æ”¾æ–¹å¼çš„æœ‰æ•ˆæ€§ã€‚  


### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æž¶æž„è§£è€¦æ€è·¯ï¼šå°†â€œç­”æ¡ˆç”Ÿæˆâ€ä¸Žâ€œç­”æ¡ˆèšåˆåˆ†æžâ€è§£è€¦ï¼Œä¸ºåˆ©ç”¨é»‘ç›’å¤§æ¨¡åž‹ï¼ˆå¦‚è°ƒç”¨APIçš„å•†ç”¨å¤§æ¨¡åž‹ï¼‰æä¾›äº†å¯è¡Œè·¯å¾„â€”â€”åªéœ€èŽ·å–å…¶è¾“å‡ºï¼Œç”¨SSAåšåŽå¤„ç†å³å¯ï¼Œæ— éœ€æ”¹åŠ¨é»‘ç›’æ¨¡åž‹æœ¬èº«ã€‚  
2. æµ‹è¯•æ—¶ç¼©æ”¾æ–°èŒƒå¼ï¼šå±•ç¤ºäº†â€œå¹¶è¡Œé‡‡æ · + é’ˆå¯¹æ€§å°æ¨¡åž‹èšåˆâ€åœ¨æŽ¨ç†ä»»åŠ¡ä¸Šçš„æ½œåŠ›ï¼Œä¸ºåŽç»­ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—æ•ˆçŽ‡ã€å¹³è¡¡èµ„æºä¸Žæ€§èƒ½æä¾›äº†æ–°æ–¹å‘ã€‚  
3. å¼ºåŒ–å­¦ä¹ åº”ç”¨å¯å‘ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–èšåˆæ¨¡åž‹ï¼ˆSSAï¼‰æ¥æå‡æœ€ç»ˆç­”æ¡ˆç²¾åº¦ï¼ŒéªŒè¯äº†åœ¨â€œè¾“å‡ºåˆ†å¸ƒå±‚é¢åšæŽ¨ç†ä¼˜åŒ–â€çš„ä»·å€¼ï¼Œå¯å¯å‘æ›´å¤šå›´ç»•æ¨¡åž‹è¾“å‡ºåŽå¤„ç†çš„ç ”ç©¶ã€‚

## guided-speculative-inference-for-efficient-test-time-alignment-of-llms
### Abstract
We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .
### ðŸŒŸ è®ºæ–‡è§£è¯» | é«˜æ•ˆLLMæµ‹è¯•æ—¶å¯¹é½çš„å¼•å¯¼å¼æŽ¨æµ‹æŽ¨ç†ï¼ˆGSIï¼‰

### ðŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨å„ç±»ç”Ÿæˆä»»åŠ¡ä¸­è¡¨çŽ°å“è¶Šï¼Œä½†æ¨¡åž‹ä¸Žæ•°æ®è§„æ¨¡çš„æ‰©å¤§å¸¦æ¥äº†é«˜æ˜‚çš„è®¡ç®—ä¸Žç»æµŽæˆæœ¬ï¼Œå› æ­¤éœ€è¦æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚æµ‹è¯•æ—¶æ‰©å±•ï¼ˆtest - time scalingï¼‰èšç„¦äºŽæŽ¨ç†æ—¶çš„è®¡ç®—æ‰©å±•ï¼Œè€Œæ¨¡åž‹å¯¹é½åˆ™è‡´åŠ›äºŽè®©æ¨¡åž‹ä¼˜åŒ–ä»¥æœ€å¤§åŒ–ç»™å®šå¥–åŠ±æ¨¡åž‹çš„å›žæŠ¥ã€‚çŽ°æœ‰æ–¹æ³•å¦‚å¥–åŠ±å¼•å¯¼çš„æŽ¨æµ‹è§£ç ï¼ˆRSDï¼‰ç¼ºä¹åˆ†å¸ƒä¿çœŸåº¦çš„ç†è®ºä¿è¯ï¼Œè½¯æœ€ä½³né‡‡æ ·ï¼ˆsoft best - of - nï¼‰è™½æœ‰ä¸€å®šä½œç”¨ä½†ä¹Ÿå­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡æå‡ºå¼•å¯¼å¼æŽ¨æµ‹æŽ¨ç†ï¼ˆGSIï¼‰ç®—æ³•ï¼Œæ—¨åœ¨å®žçŽ°é«˜æ•ˆçš„å¥–åŠ±å¼•å¯¼è§£ç ã€‚

### ðŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ðŸ’¡ åˆ›æ–°ç‚¹1ï¼šç®—æ³•èžåˆä¸Žç›®æ ‡è¿‘ä¼¼
GSIç»“åˆäº†è½¯best - of - næµ‹è¯•æ—¶æ‰©å±•ã€å¥–åŠ±æ¨¡åž‹\( r(x,y) \)ä»¥åŠæ¥è‡ªå°åž‹è¾…åŠ©æ¨¡åž‹\( \pi_S(y\mid x) \)çš„æŽ¨æµ‹æ ·æœ¬ã€‚é€šè¿‡å¯¹\( \pi_B \)å’Œ\( \pi_S \)ä¸‹çš„å¯¹æ•°ä¼¼ç„¶è°ƒæ•´å¥–åŠ±ï¼ˆå€¾æ–œï¼‰ï¼ŒGSIå¯è¯æ˜Žåœ°è¿‘ä¼¼\( \pi_B \)ä¸‹è½¯best - of - nçš„æœ€ä¼˜å€¾æ–œç­–ç•¥\( \pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta\,r(x,y)) \)ã€‚å°†å€¾æ–œåˆ†å¸ƒé‡å†™ä¸ºåŸºäºŽ\( \pi_S \)çš„åˆ†å¸ƒå½¢å¼ï¼Œé€šè¿‡å¯¹\( \pi_S \)é‡‡æ ·å¹¶é‡æ–°åŠ æƒå€™é€‰æ¥è¿‘ä¼¼\( \pi_{\beta,B} \)ï¼Œå®šä¹‰äº†å¥–åŠ± - ä¼¼ç„¶å€¾æ–œçš„è½¯best - of - nï¼ˆReward - Likelihood Tilted S - BoNï¼‰ã€‚
ðŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºä¿è¯ä¸Žç®—æ³•æµç¨‹
æŽ¨å¯¼äº†è¯±å¯¼åˆ†å¸ƒä¸Žæœ€ä¼˜ç­–ç•¥ä¹‹é—´KLæ•£åº¦çš„ç†è®ºç•Œã€‚æå‡ºçš„GSIç®—æ³•åœ¨æ¯ä¸€æ­¥æŽ¨ç†æ—¶ï¼Œå…ˆä»Ž\( \pi_S \)é‡‡æ ·ï¼Œè®¡ç®—å¥–åŠ±ä¸Žä¼¼ç„¶è°ƒæ•´åŽçš„å€¾æ–œå€¼ï¼Œé€šè¿‡softmaxé‡‡æ ·å€™é€‰ï¼›è‹¥å€™é€‰æ»¡è¶³é˜ˆå€¼åˆ™æŽ¥å—ï¼Œå¦åˆ™å›žé€€åˆ°ä»Ž\( \pi_B \)è¿›è¡Œè½¯best - of - né‡‡æ ·ã€‚åŒæ—¶å‡è®¾è¦†ç›–æ¡ä»¶\( C_{\infty}(x) := \sup_{y\in Y:\pi_B(y|x)>0} \frac{\pi_B(y | x)}{\pi_S(y | x)} < \infty \)ï¼Œåœ¨æ­¤æ¡ä»¶ä¸‹è¯æ˜ŽReward - Likelihood Tilted S - BoNå¯¹å€¾æ–œåˆ†å¸ƒçš„è¿‘ä¼¼æ€§ã€‚

### ðŸ“ˆ å®žéªŒç»“æžœ
åœ¨æŽ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆMATH500ã€OlympiadBenchã€Minerva Mathï¼‰ä¸Šï¼ŒGSIæ–¹æ³•æ¯”ä½¿ç”¨\( \pi_S \)çš„æ ‡å‡†è½¯best - of - nå’Œå¥–åŠ±å¼•å¯¼çš„æŽ¨æµ‹è§£ç ï¼ˆLiao et al., 2025ï¼‰å®žçŽ°äº†æ›´é«˜çš„å‡†ç¡®çŽ‡ï¼Œåœ¨æŸäº›è®¾ç½®ä¸‹ç”šè‡³è¶…è¿‡äº†ä½¿ç”¨\( \pi_B \)çš„è½¯best - of - nã€‚

### ðŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»Žæ–¹æ³•åˆ›æ–°è§’åº¦ï¼ŒGSIèžåˆå¤šç§æŠ€æœ¯å¹¶æä¾›ç†è®ºä¿è¯çš„æ€è·¯ï¼Œä¸ºåŽç»­é«˜æ•ˆLLMæŽ¨ç†ä¸Žå¯¹é½æ–¹æ³•ç ”ç©¶æä¾›äº†å‚è€ƒï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç»“åˆä¸åŒç»„ä»¶ï¼ˆå°æ¨¡åž‹æŽ¨æµ‹ã€å¥–åŠ±æ¨¡åž‹ã€è½¯é‡‡æ ·ç­‰ï¼‰å¹¶è¿›è¡Œç†è®ºåˆ†æžæ¥æå‡æ€§èƒ½ï¼›ä»Žå®žéªŒè§’åº¦ï¼Œåœ¨å¤šä¸ªæŽ¨ç†åŸºå‡†ä¸Šçš„éªŒè¯ä¸ºç±»ä¼¼æ–¹æ³•çš„æ•ˆæžœè¯„ä¼°æä¾›äº†èŒƒä¾‹ï¼Œè¯æ˜Žäº†è¯¥æ–¹æ³•åœ¨å®žé™…ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŽç»­ç ”ç©¶å¯å€Ÿé‰´å…¶ä»»åŠ¡é€‰æ‹©ä¸Žå¯¹æ¯”å®žéªŒè®¾ç½®æ–¹å¼ã€‚

