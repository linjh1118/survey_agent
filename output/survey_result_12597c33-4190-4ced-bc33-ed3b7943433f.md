# Paper List of Terms(Reward Model+Test Time Scaling)
- [25/07] **Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS**  
[[Paper](http://arxiv.org/pdf/2507.05557v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts)]

- [25/07] **Test-Time Scaling with Reflective Generative Model**  
[[Paper](http://arxiv.org/pdf/2507.01951v2)] [[Code/Page](https://github.com/MetaStone-AI/MetaStone-S1.)] [[TLDR/Notes](#test-time-scaling-with-reflective-generative-model)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts**  
[[Paper](http://arxiv.org/pdf/2506.15733v1)] [[Code/Page]()] [[TLDR/Notes](#$\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts)]

- [25/06] **EQA-RM: A Generative Embodied Reward Model with Test-time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.10389v1)] [[Code/Page](https://github.com/UNITES-Lab/EQA-RM.)] [[TLDR/Notes](#eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **Learning to Reason Across Parallel Samples for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.09014v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-across-parallel-samples-for-llm-reasoning)]

- [25/06] **Guided Speculative Inference for Efficient Test-Time Alignment of LLMs**  
[[Paper](http://arxiv.org/pdf/2506.04118v1)] [[Code/Page](https://github.com/j-geuter/GSI)] [[TLDR/Notes](#guided-speculative-inference-for-efficient-test-time-alignment-of-llms)]



# TLDR/Notes
## enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts
### Abstract
Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.
### 🌟 论文解读 | 分层检索增强MCTS，解锁大模型推理时性能新高度

### 📌 背景痛点/本文动机
大语言模型（LLMs）推理能力提升常依赖训练时算力，但测试时扩展（Test - Time Scaling，TTS）作为新范式，通过推理时额外算力增强性能。现有基于搜索的TTS方法（如MCTS结合PRM）存在局限：依赖预训练信息易陷局部最优、PRM难捕捉全局策略致奖励信号稀疏，复杂推理易失败。需更高效通用的推理扩展方法，无需大量额外训练还能提升鲁棒性与适应性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：双层次检索的上下文学习机制（粗粒度+细粒度）
粗粒度层面，提出深度逻辑检索（Deep Logical Retrieval）。从复杂推理问题中提取抽象模板，检索相似问题 - 答案对，为模型提供多样示例，助力捕捉问题结构模式与变异性，增强对未见过问题的适应性，提升上下文学习效果。
💡 创新点2：分层增强推理MCTS
细粒度层面，在蒙特卡洛树搜索（MCTS）过程中，R2 - LLMs从外部数学问题数据集动态检索相关中间解题步骤，用类似先验知识丰富推理过程。结合过程奖励模型（PRM），让PRM基于检索到的步骤给出更合理且上下文一致的评估，降低无效探索风险，同时完善候选生成与决策以提升推理精度。

### 📈 实验结果
在MATH500、GSM8K和OlympiadBench - TO数据集上，用LLaMA - 3.1 - 8B模型对比基线，R2 - LLMs实现显著相对提升，最高提升达16%，在复杂推理任务中展现有效性；且在LLaMA 3.1 - 8B和Qwen 2 - 7B等策略模型上评估，也优于基于上下文学习（ICL）和基于树搜索的基线方法。

### 💬 可借鉴之处
1. 分层检索增强思路：在不同粒度（粗、细）结合检索丰富模型推理时信息，为提升模型在未知场景适应性提供新范式，可启发多粒度信息融合类方法设计。
2. 检索与MCTS结合：将外部检索数据融入MCTS过程辅助PRM评估，为基于搜索的TTS方法优化中间步骤指导、缓解奖励稀疏问题提供参考，后续可探索更多检索源与搜索算法结合方式。
3. 无CoT蒸馏数据依赖：无需从更先进模型蒸馏获取思维链（CoT）训练数据，降低方法应用门槛，在数据获取受限场景有推广价值，为轻量数据依赖的推理增强方案提供思路。

## test-time-scaling-with-reflective-generative-model
### Abstract
We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3-mini's performance via the new Reflective Generative Form.
The new form focuses on high-quality reasoning trajectory selection and
contains two novelties: 1) A unified interface for policy and process reward
model: we share the backbone network and use task-specific heads for reasoning
trajectory predicting and scoring respectively, introducing only 53M extra
parameters for trajectory scoring. 2) Eliminating the reliance on process-level
annotation: we provide a self-supervised process reward model, which can
directly learn the high-quality reasoning trajectory selection from the outcome
reward. Equipped with the reflective generative form, MetaStone-S1 is naturally
suitable for test-time scaling, and we provide three reasoning effort modes
(low, medium, and high) based on the controllable thinking length. Experiments
demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI
o3-mini's series with only 32B parameter size. To support the research
community, we have open-sourced MetaStone-S1 at
https://github.com/MetaStone-AI/MetaStone-S1.
### 🌟 论文解读 | 全新反射生成模型MetaStone - S1：小参数实现大性能，解锁测试时缩放新范式

### 📌 背景痛点/本文动机
在大语言模型（LLMs）领域，OpenAI的o3模型凭借测试时缩放（Test - Time Scaling，TTS）技术实现了先进的推理和编码能力，如在推理时进行大规模采样、候选评分和多推理路径搜索等。而现有TTS方法分为内部TTS和外部TTS，内部TTS存在训练阶段结果奖励误分类正确答案（即假阳性推理过程）的问题；外部TTS虽能更有效提升性能，但现有方法在模型架构和训练上存在不足，如过程奖励模型（PRM）训练成本高、依赖特定标注数据等。同时，不同参数规模的策略模型适配的外部TTS方法也不同，小于32B时搜索方法更优，大于等于32B时采样方法更优。本文聚焦外部TTS，旨在提出新的反射生成形式来实现高质量推理轨迹选择，打造性能优异且参数规模更优的模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出全新反射生成形式（Reflective Generative Form）
系统梳理现有TTS范式后，定义了用于高质量推理轨迹选择的反射生成形式。该形式让单个网络实现推理轨迹预测与选择，且无需过程级标注。具体为策略和过程奖励模型共享骨干网络，分别用特定任务头进行推理轨迹预测和评分，仅为轨迹评分引入53M额外参数，实现了模型架构上的高效复用与创新设计。
💡 创新点2：自监督过程奖励模型消除过程级标注依赖
提供自监督的过程奖励模型，能够从结果奖励中直接学习高质量推理轨迹选择，摆脱了对过程级标注数据的依赖，降低了数据获取与标注成本，同时也解决了现有PRM依赖特定标注数据的痛点，让模型训练更具通用性与高效性。
💡 创新点3：可控思考长度的多推理努力模式
基于反射生成形式，MetaStone - S1设置了高、中、低三种推理努力模式（reasoning effort modes），依据可控的思考长度来适配不同场景需求，天然适用于测试时缩放，为模型在不同推理复杂度任务下的应用提供了灵活选择。

### 📈 实验结果
实验表明，仅32B参数规模的MetaStone - S1在性能上能与OpenAI o3 - mini系列媲美。其中MetaStone - S1 - low在数学（AIME24&25）、编码（LiveCodeBench）和中文推理（C - Eval）任务上分别超过OpenAI o3 - mini - low；MetaStone - S1 - medium与OpenAI o3 - mini - medium结果相近；MetaStone - S1 - high进一步提升了智能上限，在一系列开源和闭源模型中取得了SOTA结果。

### 💬 可借鉴之处
1. 模型架构复用与创新：反射生成形式中共享骨干网络、任务头分工的设计思路，为后续多任务模型或需要同时进行生成与评估类任务的模型架构设计提供了参考，在参数高效利用上有很好的示范。
2. 自监督学习在奖励模型的应用：其自监督过程奖励模型摆脱过程级标注的思路，为解决奖励模型数据标注难题、降低训练成本提供了新方向，后续在构建各类奖励模型（不止于LLMs领域）时可借鉴该自监督学习思路。
3. 多模式推理设计：依据可控思考长度设置多推理努力模式，为模型在实际应用中根据任务复杂度、计算资源等因素灵活调整推理策略提供了实践范例，有助于推动模型在不同场景下的落地应用。
4. 小参数实现高性能：在大模型追求参数规模竞赛的当下，MetaStone - S1以32B参数达到对标OpenAI o3 - mini系列的性能，证明了优化模型架构与训练范式在提升性能上的重要性，为中小规模参数模型的发展提供了信心与方向，后续研究可更关注模型效率与架构创新而非单纯参数堆砌。

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### 🌟 论文解读 | 用知识增强树搜索推理助力大模型攻克分子结构解析难题

### 📌 背景痛点/本文动机
分子结构解析是化学实验分析里的关键任务，要从核磁、红外等光谱数据推导分子结构，专业人员都得花10 - 15分钟分析单个分子，所以用大语言模型（LLM）自动化解析很有必要。但LLM在这任务上有挑战：一是对化学分子结构空间覆盖不足，像噻吩这类特殊杂环结构，LLM常因缺乏子结构知识误判；二是没法准确评估和修正推理过程，树搜索推理需要有效评估反馈，可LLM缺领域知识，做不好 reward model 角色。于是论文要解决这两个问题，提升LLM在分子结构解析的能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出K - MSE框架  
构建知识增强的分子结构解析推理框架K - MSE，把蒙特卡洛树搜索（MCTS）作为插件实现测试时的能力扩展，能适配任意LLM。借助MCTS平衡新解探索和已有解利用，还结合Self - Refine让LLM及时优化之前的解。

💡 创新点2：外部分子子结构知识库  
为弥补LLM化学结构空间覆盖不足，构建外部分子子结构知识库。子结构是化学空间基础元素，知识库通过自动化流程整合子结构和结构描述，给LLM补充领域知识，提升特殊结构推理准确性，减少 atypical 案例错误。

💡 创新点3：专属分子 - 光谱评分器  
设计分子 - 光谱评分器当 reward model，解决LLM解评估不准问题。评分器有分子编码器和光谱编码器，评估分子结构和光谱数据匹配度给奖励分。它还作为LLM和知识库间的检索器，用输入光谱查最相关子结构，减少子结构检索误差，增强推理稳定性。

### 📈 实验结果
在MolPuzzle基准测试上，K - MSE方法效果显著，对GPT - 4o - mini和GPT - 4o都带来超20%的性能提升，证明了框架在增强LLM分子结构解析能力上的有效性。

### 💬 可借鉴之处
1. 领域知识增强思路：面对专业领域任务，LLM通用知识不足时，构建领域子结构知识库补充，这种“外部知识 + LLM”模式可复用在其他专业领域（如生物、材料）任务。  
2. 推理过程评估优化：设计领域专属评分器做 reward model，结合树搜索框架优化推理，为需要深度推理、需评估反馈的复杂任务（如数学证明、代码调试）提供了“评分器 + 树搜索”的推理增强范式。  
3. 插件化框架设计：K - MSE作为插件适配任意LLM，这种解耦式设计方便技术落地，不同场景下可快速集成到现有LLM工作流里，降低技术迁移成本。

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### 🌟 论文解读 | ReasonFlux-PRM：面向大模型长思维链推理的轨迹感知型过程奖励模型

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的复杂推理场景（如数学解题）中，Process Reward Models（PRMs，过程奖励模型）是监督中间推理步骤的有力工具。不过现有PRMs存在明显局限：它们主要基于模型最终输出训练，难以对**轨迹 - 响应（trajectory - response）**这类新兴输出形式的中间推理轨迹进行鲁棒评估。像Deepseek - R1等前沿推理模型会生成“冗长、欠规整的中间思考轨迹 + 简洁最终响应”的轨迹 - 响应对，这类数据常被用于小模型蒸馏，但现有PRMs因与中间轨迹在结构、格式上不匹配，且训练时缺乏带奖励的轨迹 - 响应数据，在监督这类数据时效果不佳甚至会损害下游训练。所以，如何让PRMs既能监督最终响应，又能有效评估中间思考轨迹，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出轨迹感知的PRM——ReasonFlux - PRM  
ReasonFlux - PRM专为评估轨迹 - 响应型推理痕迹设计，融合了**步骤级（step - level）**和**轨迹级（trajectory - level）**监督。它在涵盖数学和科学推理的10k高质量轨迹 - 响应对 curated 数据集上训练，能为思考轨迹内的每个步骤提供细粒度奖励作为监督信号，让模型中间思考轨迹与最终响应更对齐，解决了现有PRMs对中间轨迹监督能力不足的问题。  

💡 创新点2：多场景适配的奖励监督  
ReasonFlux - PRM适配离线和在线多种场景：  
- 离线场景：为轨迹 - 响应对打分，筛选高质量数据，助力小模型下游有监督微调的训练数据精选；  
- 在线场景：融入GRPO等策略优化过程，为强化学习（RL）中的策略优化提供细粒度过程奖励；  
- 测试时缩放（test - time scaling）：通过奖励引导的Best - of - N策略，评估多个生成响应并选最优，提升推理性能。  


### 📈 实验结果
在AIME、MATH500、GPQA - Diamond等挑战性下游基准测试中，ReasonFlux - PRM展现出优异性能：  
- 数据选择方面：ReasonFlux - PRM - 7B比强基线（如Qwen2.5 - Math - PRM - 72B）和人工策划基线选出的数据集质量更高；  
- 性能提升方面：ReasonFlux - PRM - 7B在有监督微调中平均提升12.1%，强化学习中平均提升4.5%，测试时缩放中平均提升6.3%；  
- 资源友好型发布：还发布了ReasonFlux - PRM - 1.5B，适配资源受限场景与边缘部署。  


### 💬 可借鉴之处
1. 问题定义与分析角度：针对新兴的轨迹 - 响应蒸馏数据趋势，深入分析现有PRMs在监督中间轨迹时的问题（结构格式不匹配、训练数据缺失），这种从产业新数据形态反推技术痛点的思路，为后续研究锚定方向提供参考；  
2. 多粒度监督融合：将步骤级和轨迹级监督结合，为处理“长链条、多阶段”的推理类任务提供了细粒度奖励设计的范例，可迁移到代码生成、复杂决策等需分步评估的场景；  
3. 多场景工程落地：从离线数据筛选、在线RL优化到测试时增强，完整覆盖大模型训练 - 推理全流程的奖励监督，展示了技术方案在产业级落地中的多维度价值，为打造端到端的大模型推理增强管线提供了实践模板；  
4. 资源分层发布：同时提供7B和1.5B规模模型，兼顾高性能与资源受限场景，体现了技术普惠性，在实际业务中可根据算力、延迟等需求灵活选择，平衡效果与成本。  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### 🌟 论文解读 | 告别繁琐标注：GAN - RM 让奖励建模“以假乱真”

### 📌 背景痛点/本文动机
在视觉生成模型的训练后增强中，奖励模型至关重要。然而当前奖励建模方法存在诸多难题：一是构建奖励模型需大量人工标注偏好数据，收集成本高昂，且基于特定生成模型输出域标注的数据，在应用到不同输出域模型时存在域差距；二是为全面评估生成内容质量，需人工设计多种评估指标，既增加工程成本，又难在不同维度间取得最优平衡，还难保证与人类普遍偏好契合。因此，本文受生成对抗网络（GAN）中对抗训练启发，提出 GAN - RM 框架，旨在摆脱手动偏好标注和显式质量维度设计，高效构建奖励模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无需手动偏好标注，利用少量代理数据
GAN - RM 仅需少量（几百个）无标注的代表性样本（即偏好代理数据，Preference Proxy Data）作为外部数据。通过训练奖励模型区分偏好代理数据和生成模型输出，让模型学习评估生成样本。同时采用基于排名的自举策略，将 GAN - RM 在这些样本上的置信分数作为软标签，利用额外数据再训练 GAN - RM，使其更好捕捉潜在人类偏好。
💡 创新点2：支持多轮训练，迭代对齐偏好
GAN - RM 支持多轮训练后优化。每一轮中，将被识别为接近偏好代理数据的样本用于生成器的训练后优化，反过来再训练判别器以区分这些更难的样本。这种迭代的“以假乱真”过程能逐步让生成质量与偏好代理数据中的潜在人类偏好对齐。

### 📈 实验结果
实验表明，基于 GAN - RM 的方法在性能上可与依赖大量标注数据（如 Pickapic 的 100 万标注人类偏好数据）的方法（如相关对比方法）相当甚至超越。在图像质量实验设置中，GAN - RM 仅需 500 个偏好代理数据样本。除图像质量提升实验外，在图像安全和视频质量增强场景下的实验也凸显了 GAN - RM 框架在不同场景下的泛化能力，验证了其在测试时缩放（如 Best - of - N 样本过滤）、监督微调（SFT）和直接偏好优化（DPO）等训练后方法中的有效性。

### 💬 可借鉴之处
从方法创新角度，GAN - RM 为解决奖励建模中数据获取难、依赖特定域、人工设计维度难契合人类偏好等问题提供了新思路，其利用对抗训练和少量代理数据的方式，减少了对大规模人工标注的依赖，降低工程成本；从应用拓展角度，该框架在图像、视频等多场景的有效实验，为视觉生成模型在不同领域的训练后增强提供了可复用的奖励建模范式，后续在视觉生成相关任务中，若需构建奖励模型，可借鉴其利用少量代理数据和对抗训练的思路来降低成本与难度。

## $\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts
### Abstract
Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.
### 🌟 论文解读 | SPECS：用“推测草稿”加速大模型推理，平衡延迟与精度

### 📌 背景痛点/本文动机
大语言模型（LLM）推理能力的提升常依赖“测试时算力扩容”，比如分配更多计算资源做更充分的探索。但算力增加往往导致用户侧延迟升高，直接影响体验。现有测试时扩容方法多聚焦算力（FLOPS）优化精度，却忽略延迟约束。此外，基于Transformer的LLM自回归采样延迟常受限于内存加载而非总算力，而推测解码虽能借小模型提候选 token 降延迟，却会增加总计算量。于是，论文试图回答：**能否设计高效测试时扩容方法，优化延迟 - 效用权衡？**

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 SPECS 算法框架  
SPECS 受推测解码启发，是一种“延迟感知型”测试时扩容方法。它用**更小更快的草稿模型**高效生成候选序列，再结合**更大的目标模型**与**专用奖励模型**评估候选。整体遵循“草稿 - 选择”流程：迭代生成响应块，每轮用草稿模型生成候选块，经打分选择后拼接，进入下一轮；若草稿全被拒，则切换目标模型生成候选。

💡 创新点2：奖励引导的软验证与延迟机制  
- 奖励引导软验证（SUBSAMPLE 子例程）：基于草稿、目标、奖励模型计算的“分数”选候选块，既优化效用 - 延迟权衡，也避免简单丢弃高奖励但可能被 naive 推测解码漏掉的轨迹。  
- 奖励感知延迟规则（CASCADE 子例程）：自适应决定下一轮用草稿还是目标模型生成候选——让大模型处理难题步骤，小模型处理简单步骤，动态平衡算力与延迟。  

💡 创新点3：理论分析保障收敛性  
从理论上分析，SPECS 在结合草稿、目标、奖励模型优化“KL 正则化奖励最大化”目标时，其软验证方法随 beam 宽度增大，能优雅收敛到最优解。


### 📈 实验结果
论文在 MATH500、AMC23、OlympiadBench 数据集测试，用 Qwen - 1.5B - Instruct（草稿模型）、Qwen - 7B - Instruct（目标模型）与 Qwen - 7B - Math - PRM（奖励模型）验证：  
- 精度层面：SPECS 匹配甚至超越 beam search 精度；  
- 延迟层面：延迟最多降低约 19.1%，在精度与延迟间实现更优权衡。  


### 💬 可借鉴之处
1. **延迟 - 精度权衡思路**：跳出“只看算力/精度”的思维定式，把延迟作为核心约束，为大模型落地低延迟场景（如个性化交互）提供新思路；  
2. **多模型协作范式**：用“小草稿模型 + 大目标模型 + 奖励模型”分层协作，既利用小模型提速，又靠大模型保精度，还借奖励模型做灵活选择，这种“分工”模式可迁移到其他需平衡资源与效果的任务；  
3. **理论 + 实验双验证**：从理论证明收敛性，再用真实数据集验证，为方法可靠性背书，也示范了学术研究中“方法 - 理论 - 实验”闭环的重要性。  


SPECS 为大模型推理的“延迟 - 精度”难题提供了一套兼具创新性与实用性的解法，无论是工业界落地低延迟 LLM 应用，还是学术界探索测试时优化新方向，都有不少可借鉴的闪光点~

## eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling
### Abstract
Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.
### 🌟 论文解读 | EQA - RM：为具身问答量身定制的生成式奖励模型，实现测试时可扩展评估

### 📌 背景痛点/本文动机
奖励模型（RMs）在大模型对齐中至关重要，但在复杂的具身任务（如具身问答，EQA）中却未得到充分探索。EQA需要智能体在3D环境中通过多模态观察和动作序列来感知、交互和推理以回答问题，对智能体的空间、时间和逻辑理解进行细致评估至关重要，而通用的奖励模型方法无法满足这一需求。现有通用奖励模型多为静态输入或简单结果设计，难以捕捉具身任务中固有的时空和逻辑依赖关系，因此迫切需要专门的机制来准确评估EQA的多方面成功指标。同时，EQA领域缺乏用于严格评估和比较奖励模型的标准化基准，当前EQA任务基准侧重于粗略的成功指标，而非对奖励模型发展至关重要的细粒度轨迹质量评估。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出EQA - RM生成式多模态奖励模型
EQA - RM是专为评估EQA轨迹而设计的新型多模态奖励模型，作为生成式奖励模型（GenRM），不仅能产生标量奖励，还能为评估提供明确的推理过程。其具有增强的空间、时间和推理处理能力，以处理EQA任务中固有的独特多模态数据流。通过高效的两阶段训练过程，第一阶段用标准的Rejective Finetuning（RFT）教会模型期望的输出格式（包含文本批评和标量分数）；第二阶段采用创新的Contrastive Group Relative Policy Optimization（C - GRPO）强化学习策略，解决仅RFT可能只学格式不学内容的问题，利用基于规则的对比奖励（源于针对性的数据增强，如视频帧打乱、空间区域随机掩码、推理步骤混乱等扰动方式），让模型区分原始连贯上下文和合成扰动上下文下的策略输出，从而内化时间顺序、细粒度空间细节和连贯逻辑流的重要性，培养对具身任务强大且敏锐的评估能力。

💡 创新点2：构建EQARewardBench基准
为解决EQA领域奖励模型评估基准缺失的问题，基于OpenEQA构建了EQARewardBench。该基准包含来自HM3D和ScanNet两种家庭环境的具身情节记忆视频，从原始问答对构建更全面的问题 - 响应 - 推理轨迹三元组，有1546个测试实例，用于评估奖励模型在轨迹质量的八个不同方面（如正确性、接地性、效率等），为EQA任务上的奖励模型提供了标准化、可比较的评估平台。

### 📈 实验结果
以Qwen2 - VL - 2B - Instruct为基础进行微调的EQA - RM展现出高样本效率，仅用700个样本在EQA - RM - Bench上达到61.9%的准确率，超越了强大的专有基线（如Gemini - 2.5 - Flash、GPT - 4o、Claude - 3.5 - Haiku）和开源的最先进模型（如RoVRM和VisualPRM）。同时，EQA - RM展示了测试时可扩展性，在推理时增加评估计算量，其在EQARewardBench上的准确率从42.47%提升到61.86%，性能提升后在基准测试中超越了领先的大型商业模型。

### 💬 可借鉴之处
1. 针对特定复杂任务设计专用奖励模型：当通用模型无法满足复杂任务（如具身任务）的评估需求时，可像EQA - RM一样针对任务特性，设计具备特定能力（如空间、时间、推理处理能力）的专用模型，解决通用模型的局限性。
2. 创新的训练策略：两阶段训练（RFT + C - GRPO）以及利用数据增强的对比奖励策略，为解决模型只学形式不学内容、提升模型对任务关键要素的理解提供了思路，可借鉴于其他需要模型深入理解任务细节的训练场景。
3. 构建领域基准：对于缺乏评估基准的领域，可像构建EQARewardBench一样，基于现有数据集构建专门的基准，推动领域内模型的评估和发展，为模型性能比较和改进提供标准平台。

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### 🌟 论文解读 | Athena：用数据高效的过程奖励模型提升多模态推理能力

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）和多模态大语言模型（MLLMs）在自然语言处理和多模态任务中取得显著进展，但解决复杂推理任务（如数学和多步骤推理）仍具挑战。为增强推理能力，测试时缩放（TTS）等方法被探索，其中过程奖励模型（PRMs）能为中间推理步骤提供细粒度反馈，性能更优且泛化性强。然而，PRMs 发展面临两大难题：一是获取带过程标签的高质量数据成本高（需大量人工标注或计算昂贵的自动化标注）；二是传统自动化标注（如蒙特卡洛估计）易产生噪声标签。本文旨在解决这些挑战，降低计算成本并减轻标签噪声问题，提升 PRMs 性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：利用强弱完成器预测一致性生成高质量过程标签  
传统蒙特卡洛等自动化标注方法易受完成器推理能力影响，标签有噪声且计算成本高。本文发现，强完成器即便中间步骤错误仍能得到正确答案，弱完成器则可能在中间步骤正确时也失败。基于此，提出用弱、强完成器预测一致性作为筛选可靠过程标签的标准，保留两者标签一致的步骤，减少完成器带来的偏差，提升标签质量。实验表明，约 5000 条高质量标签就能比传统方法约 30 万条大规模标注数据表现更优，且大幅降低数据合成和模型训练的计算成本。

💡 创新点2：提升 PRMs 性能的两大策略  
 - ORM 初始化：PRMs 通常基于预训练基础模型微调，而结果奖励模型（ORMs）在大规模响应级数据上训练，具备弱监督下评估中间步骤正确性的能力。因此用 ORMs 初始化 PRMs，将 ORMs 作为弱监督预训练，PRMs 再在高质量细粒度步骤数据上微调，显著提升性能。  
 - 负样本上采样：过程标签数据存在标签不平衡问题，通过对含负步骤标签的数据进行上采样，解决数据分布不均问题，优化模型训练。  

💡 创新点3：构建 Athena 系列模型并多场景验证  
基于上述方法构建结果奖励模型 Athena - ORM 和过程奖励模型 Athena - PRM，再利用 Athena - PRM 通过奖励排序微调得到 Athena - 7B。并在三个场景验证：测试时缩放（TTS）中对策略模型生成的多个输出排序；直接评估推理步骤正确性；奖励排序微调（用高奖励响应微调策略模型）。

### 📈 实验结果
- 测试时缩放场景：在 7 个多模态数学和推理基准测试中，用 Athena - PRM 配合不同规模（7B 到 72B）策略模型，推理能力显著提升。如用 Qwen2.5 - VL - 7B 作为策略模型时，在 WeMath 基准上零样本基线提升 10.2 分，在 MathVista 提升 7.1 分；在文本-only 数学基准用 Mistral - 8B 时提升 8.9 分。  
- 推理步骤正确性评估场景：在 VisualProcessBench 基准上，Athena - PRM 表现强劲，超越开源的 VisualPRM - 8B 等模型，F1 分数比之前最优结果高 3.9，展现准确评估推理步骤正确性的能力。  
- 奖励排序微调场景：基于 Qwen2.5 - VL - 7B 微调得到的 Athena - 7B，在 7 个数学和推理基准上大幅提升策略模型推理能力。  

### 💬 可借鉴之处
- 数据高效标注思路：利用多完成器预测一致性筛选标签，为解决需细粒度标注且标注成本高的任务提供了新范式，在减少数据量同时提升数据质量，实现数据高效利用。  
- 模型训练策略：ORM 初始化和负样本上采样策略，为提升奖励模型性能提供了可复用方法，可启发其他奖励模型或需细粒度反馈模型的训练优化。  
- 多场景验证模式：在测试时缩放、步骤评估、模型微调等多场景验证方法有效性，这种全面验证思路有助于更充分展示方法价值，为后续研究提供验证范式参考。

## learning-to-reason-across-parallel-samples-for-llm-reasoning
### Abstract
Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.
### 🌟 论文解读 | 融合并行与顺序推理，SSA让大模型推理更高效

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务上能力不断提升，而测试时计算资源的分配（即测试时缩放）是优化模型性能的新方向。现有测试时缩放方法分并行和顺序两类：并行缩放是独立生成多条推理路径再聚合（如多数投票）；顺序缩放则迭代优化单个解（如基于提示的自我反思）。但并行方法常孤立看待样本，顺序方法计算成本或适配性受限。本文旨在提出新方法，融合二者优势，更高效利用测试时计算资源提升推理性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Sample Set Aggregator（SSA）模型架构  
设计轻量级的SSA模型，将其与生成答案的基础模型（LMans）解耦。先由LMans并行生成K个候选答案，再把这些候选答案拼接成序列输入SSA，SSA通过强化学习优化以输出最终正确答案。这种设计让SSA能基于基础模型输出的分布特性，直接优化答案合成过程，而非孤立评估单个样本。  

💡 创新点2：基于输出分布推理，解耦训练与推理  
SSA不直接训练生成答案的基础模型（LMans可视为黑盒），而是针对其采样输出进行优化。这种“推理输出分布而非调整模型内部”的思路，让方法更灵活——可适配不同基础模型（甚至是只能通过API调用的黑盒大模型），只需用其采样答案训练SSA即可。  

💡 创新点3：统一并行与顺序缩放优势  
并行缩放能快速获取多视角答案，顺序缩放可迭代优化推理；SSA通过“并行采样+单步顺序RL聚合”的方式，在一次前向传递中结合二者长处：用并行获取多样性，用SSA的顺序推理实现精准聚合，且仅需训练小模型就能带来显著性能提升。  


### 📈 实验结果
1. 性能超越强基线：在多个数学推理数据集上，SSA相比基于奖励模型重排序等测试时缩放方法表现更优，大幅缩小了模型实际性能与“理论最优（oracle - best）”精度的差距。  
2. 泛化能力突出：跨样本集大小、基础模型家族（如Qwen 2.5、Llama 3.1）、模型规模（7B/14B/32B）和任务，SSA都展现出良好泛化性。比如在一个数据集上为特定模型训练的SSA，能成功聚合不同模型家族、规模在不同任务上的输出。  
3. 轻量化优势：紧凑的SSA模型能匹配顺序缩放中经强化训练的大模型性能，证明其作为轻量顺序缩放方式的有效性。  


### 💬 可借鉴之处
1. 架构解耦思路：将“答案生成”与“答案聚合分析”解耦，为利用黑盒大模型（如调用API的商用大模型）提供了可行路径——只需获取其输出，用SSA做后处理即可，无需改动黑盒模型本身。  
2. 测试时缩放新范式：展示了“并行采样 + 针对性小模型聚合”在推理任务上的潜力，为后续优化测试时计算效率、平衡资源与性能提供了新方向。  
3. 强化学习应用启发：通过强化学习优化聚合模型（SSA）来提升最终答案精度，验证了在“输出分布层面做推理优化”的价值，可启发更多围绕模型输出后处理的研究。

## guided-speculative-inference-for-efficient-test-time-alignment-of-llms
### Abstract
We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .
### 🌟 论文解读 | 高效LLM测试时对齐的引导式推测推理（GSI）

### 📌 背景痛点/本文动机
大语言模型（LLMs）在各类生成任务中表现卓越，但模型与数据规模的扩大带来了高昂的计算与经济成本，因此需要更高效的替代方案。测试时扩展（test - time scaling）聚焦于推理时的计算扩展，而模型对齐则致力于让模型优化以最大化给定奖励模型的回报。现有方法如奖励引导的推测解码（RSD）缺乏分布保真度的理论保证，软最佳n采样（soft best - of - n）虽有一定作用但也存在改进空间。在此背景下，本文提出引导式推测推理（GSI）算法，旨在实现高效的奖励引导解码。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：算法融合与目标近似
GSI结合了软best - of - n测试时扩展、奖励模型\( r(x,y) \)以及来自小型辅助模型\( \pi_S(y\mid x) \)的推测样本。通过对\( \pi_B \)和\( \pi_S \)下的对数似然调整奖励（倾斜），GSI可证明地近似\( \pi_B \)下软best - of - n的最优倾斜策略\( \pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta\,r(x,y)) \)。将倾斜分布重写为基于\( \pi_S \)的分布形式，通过对\( \pi_S \)采样并重新加权候选来近似\( \pi_{\beta,B} \)，定义了奖励 - 似然倾斜的软best - of - n（Reward - Likelihood Tilted S - BoN）。
💡 创新点2：理论保证与算法流程
推导了诱导分布与最优策略之间KL散度的理论界。提出的GSI算法在每一步推理时，先从\( \pi_S \)采样，计算奖励与似然调整后的倾斜值，通过softmax采样候选；若候选满足阈值则接受，否则回退到从\( \pi_B \)进行软best - of - n采样。同时假设覆盖条件\( C_{\infty}(x) := \sup_{y\in Y:\pi_B(y|x)>0} \frac{\pi_B(y | x)}{\pi_S(y | x)} < \infty \)，在此条件下证明Reward - Likelihood Tilted S - BoN对倾斜分布的近似性。

### 📈 实验结果
在推理基准测试（MATH500、OlympiadBench、Minerva Math）上，GSI方法比使用\( \pi_S \)的标准软best - of - n和奖励引导的推测解码（Liao et al., 2025）实现了更高的准确率，在某些设置下甚至超过了使用\( \pi_B \)的软best - of - n。

### 💬 可借鉴之处
从方法创新角度，GSI融合多种技术并提供理论保证的思路，为后续高效LLM推理与对齐方法研究提供了参考，展示了如何通过结合不同组件（小模型推测、奖励模型、软采样等）并进行理论分析来提升性能；从实验角度，在多个推理基准上的验证为类似方法的效果评估提供了范例，证明了该方法在实际任务中的有效性，后续研究可借鉴其任务选择与对比实验设置方式。

