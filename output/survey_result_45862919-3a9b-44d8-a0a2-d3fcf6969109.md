# Paper List of Terms(reward model+data)
- [25/07] **Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities**  
[[Paper](http://arxiv.org/pdf/2507.13158v1)] [[Code/Page]()] [[TLDR/Notes](#inverse-reinforcement-learning-meets-large-language-model-post-training--basics--advances--and-opportunities)]

- [25/07] **Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**  
[[Paper](http://arxiv.org/pdf/2507.11155v1)] [[Code/Page]()] [[TLDR/Notes](#bridging-the-gap-in-vision-language-models-in-identifying-unsafe-concepts-across-modalities)]

- [25/07] **EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique**  
[[Paper](http://arxiv.org/pdf/2507.09374v1)] [[Code/Page]()] [[TLDR/Notes](#eduflow--advancing-mllms--problem-solving-proficiency-through-multi-stage--multi-perspective-critique)]

- [25/07] **One Token to Fool LLM-as-a-Judge**  
[[Paper](http://arxiv.org/pdf/2507.08794v1)] [[Code/Page](https://huggingface.co/sarosavo/Master-RM)] [[TLDR/Notes](#one-token-to-fool-llm-as-a-judge)]

- [25/07] **Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions**  
[[Paper](http://arxiv.org/pdf/2507.08068v1)] [[Code/Page]()] [[TLDR/Notes](#quantile-reward-policy-optimization--alignment-with-pointwise-regression-and-exact-partition-functions)]

- [25/07] **Why is Your Language Model a Poor Implicit Reward Model?**  
[[Paper](http://arxiv.org/pdf/2507.07981v1)] [[Code/Page]()] [[TLDR/Notes](#why-is-your-language-model-a-poor-implicit-reward-model-)]

- [25/07] **Bradley-Terry and Multi-Objective Reward Modeling Are Complementary**  
[[Paper](http://arxiv.org/pdf/2507.07375v1)] [[Code/Page]()] [[TLDR/Notes](#bradley-terry-and-multi-objective-reward-modeling-are-complementary)]

- [25/07] **Perception-Aware Policy Optimization for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2507.06448v2)] [[Code/Page](https://mikewangwzhl.github.io/PAPO.)] [[TLDR/Notes](#perception-aware-policy-optimization-for-multimodal-reasoning)]

- [25/07] **Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling**  
[[Paper](http://arxiv.org/pdf/2507.06419v1)] [[Code/Page]()] [[TLDR/Notes](#reward-models-can-improve-themselves--reward-guided-adversarial-failure-mode-discovery-for-robust-reward-modeling)]

- [25/07] **Prompt-Free Conditional Diffusion for Multi-object Image Augmentation**  
[[Paper](http://arxiv.org/pdf/2507.06146v1)] [[Code/Page](https://github.com/00why00/PFCD}{here}.)] [[TLDR/Notes](#prompt-free-conditional-diffusion-for-multi-object-image-augmentation)]

- [25/07] **Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS**  
[[Paper](http://arxiv.org/pdf/2507.05557v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts)]

- [25/07] **ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization**  
[[Paper](http://arxiv.org/pdf/2507.03069v1)] [[Code/Page]()] [[TLDR/Notes](#arf-rlhf--adaptive-reward-following-for-rlhf-through-emotion-driven-self-supervision-and-trace-biased-dynamic-optimization)]

- [25/07] **Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy**  
[[Paper](http://arxiv.org/pdf/2507.01352v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy)]

- [25/07] **SAFER: Probing Safety in Reward Models with Sparse Autoencoder**  
[[Paper](http://arxiv.org/pdf/2507.00665v1)] [[Code/Page](https://github.com/xzy-101/SAFER-code.)] [[TLDR/Notes](#safer--probing-safety-in-reward-models-with-sparse-autoencoder)]

- [25/06] **Generalist Reward Models: Found Inside Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.23235v1)] [[Code/Page]()] [[TLDR/Notes](#generalist-reward-models--found-inside-large-language-models)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **Listener-Rewarded Thinking in VLMs for Image Preferences**  
[[Paper](http://arxiv.org/pdf/2506.22832v2)] [[Code/Page](https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.)] [[TLDR/Notes](#listener-rewarded-thinking-in-vlms-for-image-preferences)]

- [25/06] **Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents**  
[[Paper](http://arxiv.org/pdf/2506.21252v1)] [[Code/Page]()] [[TLDR/Notes](#agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents)]

- [25/06] **Off-Policy Evaluation and Learning for the Future under Non-Stationarity**  
[[Paper](http://arxiv.org/pdf/2506.20417v1)] [[Code/Page]()] [[TLDR/Notes](#off-policy-evaluation-and-learning-for-the-future-under-non-stationarity)]

- [25/06] **Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations**  
[[Paper](http://arxiv.org/pdf/2506.20294v1)] [[Code/Page]()] [[TLDR/Notes](#ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.18841v1)] [[Code/Page](https://huggingface.co/THU-KEG/LongWriter-Zero-32B)] [[TLDR/Notes](#longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning)]

- [25/06] **RDPO: Real Data Preference Optimization for Physics Consistency Video Generation**  
[[Paper](http://arxiv.org/pdf/2506.18655v1)] [[Code/Page](https://wwenxu.github.io/RDPO/)] [[TLDR/Notes](#rdpo--real-data-preference-optimization-for-physics-consistency-video-generation)]

- [25/07] **Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization**  
[[Paper](http://arxiv.org/pdf/2507.01050v2)] [[Code/Page](https://github.com/allacnobug/Detoxification-of-Text.)] [[TLDR/Notes](#text-detoxification--data-efficiency--semantic-preservation-and-model-generalization)]

- [25/06] **Shrinking the Generation-Verification Gap with Weak Verifiers**  
[[Paper](http://arxiv.org/pdf/2506.18203v1)] [[Code/Page]()] [[TLDR/Notes](#shrinking-the-generation-verification-gap-with-weak-verifiers)]

- [25/06] **ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2506.16712v1)] [[Code/Page]()] [[TLDR/Notes](#reasongrm--enhancing-generative-reward-models-through-large-reasoning-models)]

- [25/06] **Robust Reward Modeling via Causal Rubrics**  
[[Paper](http://arxiv.org/pdf/2506.16507v1)] [[Code/Page]()] [[TLDR/Notes](#robust-reward-modeling-via-causal-rubrics)]

- [25/06] **Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples**  
[[Paper](http://arxiv.org/pdf/2506.16502v1)] [[Code/Page]()] [[TLDR/Notes](#relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples)]

- [25/06] **GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks**  
[[Paper](http://arxiv.org/pdf/2506.16114v1)] [[Code/Page]()] [[TLDR/Notes](#gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks)]

- [25/06] **GRAM: A Generative Foundation Reward Model for Reward Generalization**  
[[Paper](http://arxiv.org/pdf/2506.14175v2)] [[Code/Page]()] [[TLDR/Notes](#gram--a-generative-foundation-reward-model-for-reward-generalization)]

- [25/06] **VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training**  
[[Paper](http://arxiv.org/pdf/2506.13888v1)] [[Code/Page]()] [[TLDR/Notes](#vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **Personalized LLM Decoding via Contrasting Personal Preference**  
[[Paper](http://arxiv.org/pdf/2506.12109v1)] [[Code/Page]()] [[TLDR/Notes](#personalized-llm-decoding-via-contrasting-personal-preference)]

- [25/06] **Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11474v1)] [[Code/Page](https://med-prm.github.io/)] [[TLDR/Notes](#med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards)]

- [25/06] **Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11425v2)] [[Code/Page]()] [[TLDR/Notes](#agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards)]

- [25/06] **ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems**  
[[Paper](http://arxiv.org/pdf/2506.10955v1)] [[Code/Page]()] [[TLDR/Notes](#reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems)]

- [25/06] **Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.21560v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning)]

- [25/06] **DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision**  
[[Paper](http://arxiv.org/pdf/2506.09814v1)] [[Code/Page]()] [[TLDR/Notes](#dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO**  
[[Paper](http://arxiv.org/pdf/2506.08965v1)] [[Code/Page]()] [[TLDR/Notes](#gfriend--generative-few-shot-reward-inference-through-efficient-dpo)]

- [25/06] **Saffron-1: Safety Inference Scaling**  
[[Paper](http://arxiv.org/pdf/2506.06444v2)] [[Code/Page](https://github.com/q-rz/saffron)] [[TLDR/Notes](#saffron-1--safety-inference-scaling)]

- [25/06] **Preference Learning for AI Alignment: a Causal Perspective**  
[[Paper](http://arxiv.org/pdf/2506.05967v1)] [[Code/Page]()] [[TLDR/Notes](#preference-learning-for-ai-alignment--a-causal-perspective)]

- [25/06] **Customizing Speech Recognition Model with Large Language Model Feedback**  
[[Paper](http://arxiv.org/pdf/2506.11091v1)] [[Code/Page]()] [[TLDR/Notes](#customizing-speech-recognition-model-with-large-language-model-feedback)]

- [25/06] **Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models**  
[[Paper](http://arxiv.org/pdf/2506.05339v2)] [[Code/Page]()] [[TLDR/Notes](#flattery--fluff--and-fog--diagnosing-and-mitigating-idiosyncratic-biases-in-preference-models)]

- [25/06] **A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation via Learning to Search**  
[[Paper](http://arxiv.org/pdf/2506.05294v1)] [[Code/Page](https://github.com/arnavkj1995/SAILOR)] [[TLDR/Notes](#a-smooth-sea-never-made-a-skilled-$\texttt{sailor}$--robust-imitation-via-learning-to-search)]

- [25/06] **RewardAnything: Generalizable Principle-Following Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.03637v2)] [[Code/Page]()] [[TLDR/Notes](#rewardanything--generalizable-principle-following-reward-models)]

- [25/06] **DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models**  
[[Paper](http://arxiv.org/pdf/2506.03517v1)] [[Code/Page]()] [[TLDR/Notes](#densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models)]

- [25/06] **AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation**  
[[Paper](http://arxiv.org/pdf/2506.03122v1)] [[Code/Page]()] [[TLDR/Notes](#autocircuit-rl--reinforcement-learning-driven-llm-for-automated-circuit-topology-generation)]

- [25/06] **BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF**  
[[Paper](http://arxiv.org/pdf/2506.03234v1)] [[Code/Page]()] [[TLDR/Notes](#badreward--clean-label-poisoning-of-reward-models-in-text-to-image-rlhf)]

- [25/06] **Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences**  
[[Paper](http://arxiv.org/pdf/2506.02698v2)] [[Code/Page](https://jaydenlyh.github.io/SmPO-project-page/.)] [[TLDR/Notes](#smoothed-preference-optimization-via-renoise-inversion-for-aligning-diffusion-models-with-varied-human-preferences)]



# TLDR/Notes
## inverse-reinforcement-learning-meets-large-language-model-post-training--basics--advances--and-opportunities
### Abstract
In the era of Large Language Models (LLMs), alignment has emerged as a
fundamental yet challenging problem in the pursuit of more reliable,
controllable, and capable machine intelligence. The recent success of reasoning
models and conversational AI systems has underscored the critical role of
reinforcement learning (RL) in enhancing these systems, driving increased
research interest at the intersection of RL and LLM alignment. This paper
provides a comprehensive review of recent advances in LLM alignment through the
lens of inverse reinforcement learning (IRL), emphasizing the distinctions
between RL techniques employed in LLM alignment and those in conventional RL
tasks. In particular, we highlight the necessity of constructing neural reward
models from human data and discuss the formal and practical implications of
this paradigm shift. We begin by introducing fundamental concepts in RL to
provide a foundation for readers unfamiliar with the field. We then examine
recent advances in this research agenda, discussing key challenges and
opportunities in conducting IRL for LLM alignment. Beyond methodological
considerations, we explore practical aspects, including datasets, benchmarks,
evaluation metrics, infrastructure, and computationally efficient training and
inference techniques. Finally, we draw insights from the literature on
sparse-reward RL to identify open questions and potential research directions.
By synthesizing findings from diverse studies, we aim to provide a structured
and critical overview of the field, highlight unresolved challenges, and
outline promising future directions for improving LLM alignment through RL and
IRL techniques.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹å¯¹é½ä¸­çš„é€†å¼ºåŒ–å­¦ä¹ ï¼šåŸºç¡€ã€è¿›å±•ä¸æœºé‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ä»£ï¼Œå¯¹é½ï¼ˆalignmentï¼‰å·²æˆä¸ºè¿½æ±‚æ›´å¯é ã€å¯æ§å’Œæœ‰èƒ½åŠ›çš„æœºå™¨æ™ºèƒ½è¿‡ç¨‹ä¸­ä¸€ä¸ªåŸºç¡€ä¸”å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œå¤§è§„æ¨¡æ•°æ®é©±åŠ¨æ¨¡å‹ï¼ˆå¦‚LLMï¼‰åœ¨å¤šé¢†åŸŸå–å¾—æˆåŠŸï¼Œä½†LLMå­˜åœ¨æ— æ³•è‡ªä¸»æŒç»­æ”¹è¿›ç­‰ä¸è¶³ï¼›å¦ä¸€æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼—å¤šé¢†åŸŸå±•ç°è¶…äººç±»æ€§èƒ½å´é¢ä¸´ç³»ç»Ÿé€æ˜æ€§ç­‰æŒ‘æˆ˜ã€‚é‰´äºRLå’ŒLLMåœ¨å„è‡ªé¢†åŸŸçš„æˆåŠŸï¼Œå°†äºŒè€…ç»“åˆæå…·å‰æ™¯ã€‚åŒæ—¶ï¼Œå½“å‰åœ¨å°†RLæ‰©å±•åˆ°æ›´å¹¿æ³›LLMä»»åŠ¡å’Œåº”ç”¨ä¸­å­˜åœ¨ç¼ºä¹å¥–åŠ±ä¿¡å·ã€è®¡ç®—æˆæœ¬é«˜ã€RLç®—æ³•éœ€é€‚é…LLMå¯¹é½ä»»åŠ¡ç‰¹æ€§ç­‰å…³é”®æŒ‘æˆ˜ï¼Œå› æ­¤æœ¬æ–‡ä»é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰è§†è§’å…¨é¢ç»¼è¿°LLMå¯¹é½çš„æœ€æ–°è¿›å±•ï¼Œè¯•å›¾å¼¥åˆIRLä¸LLMå¯¹é½é—´çš„å·®è·ä»¥åŠ©åŠ›æœªæ¥ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»¥é€†å¼ºåŒ–å­¦ä¹ è§†è§’ç»¼è¿°LLMå¯¹é½è¿›å±•  
æœ¬æ–‡èšç„¦äºä»é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰çš„è§’åº¦æ¥å®¡è§†å¤§è¯­è¨€æ¨¡å‹å¯¹é½çš„æœ€æ–°ç ”ç©¶æˆæœï¼Œçªå‡ºLLMå¯¹é½ä¸­æ‰€ç”¨RLæŠ€æœ¯ä¸ä¼ ç»ŸRLä»»åŠ¡ä¸­æŠ€æœ¯çš„åŒºåˆ«ï¼Œå¼ºè°ƒä»äººç±»æ•°æ®æ„å»ºç¥ç»å¥–åŠ±æ¨¡å‹çš„å¿…è¦æ€§ï¼Œå¹¶æ¢è®¨è¿™ç§èŒƒå¼è½¬å˜åœ¨å½¢å¼å’Œå®è·µä¸Šçš„å½±å“ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†å…¨æ–°çš„å®¡è§†ç»´åº¦ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢è¦†ç›–å¤šæ–¹é¢å†…å®¹æ„å»ºçŸ¥è¯†ä½“ç³»  
å…ˆä¸ºä¸ç†Ÿæ‚‰RLé¢†åŸŸçš„è¯»è€…ä»‹ç»RLåŸºæœ¬æ¦‚å¿µï¼ˆå¦‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç­‰ï¼‰å¥ å®šåŸºç¡€ï¼›æ¥ç€å®¡è§†è¯¥ç ”ç©¶è®®ç¨‹çš„æœ€æ–°è¿›å±•ï¼Œè®¨è®ºä¸ºLLMå¯¹é½è¿›è¡ŒIRLçš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ï¼›é™¤æ–¹æ³•è€ƒé‡å¤–ï¼Œè¿˜æ¢ç´¢æ•°æ®é›†ã€åŸºå‡†ã€è¯„ä¼°æŒ‡æ ‡ã€åŸºç¡€è®¾æ–½ä»¥åŠè®¡ç®—é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ç­‰å®è·µæ–¹é¢ï¼›æœ€åä»ç¨€ç–å¥–åŠ±RLæ–‡çŒ®ä¸­æ±²å–è§è§£ä»¥ç¡®å®šå¼€æ”¾æ€§é—®é¢˜å’Œæ½œåœ¨ç ”ç©¶æ–¹å‘ï¼Œå…¨æ–¹ä½æ„å»ºèµ·LLMå¯¹é½ä¸IRLç»“åˆé¢†åŸŸçš„çŸ¥è¯†ä½“ç³»ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªæ˜ç¡®æåŠä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®éªŒç»“æœå‘ˆç°ï¼ˆå¦‚å¯¹æ¯”å®éªŒæ•°æ®ç­‰ï¼‰ï¼Œä¸»è¦æ˜¯ä»ç»¼è¿°è§’åº¦æ¢³ç†é¢†åŸŸå†…ç›¸å…³å·¥ä½œã€æŒ‘æˆ˜ä¸æœºé‡ç­‰å†…å®¹ï¼Œé€šè¿‡å¯¹RLåœ¨å¯¹è¯AIï¼ˆå¦‚RLHFæå‡LLMèƒ½åŠ›ï¼‰ã€æ•°å­¦æ¨ç†ï¼ˆå¦‚AlphaProofç­‰åœ¨æ•°å­¦ç«èµ›è¡¨ç°ï¼‰ç­‰åœºæ™¯åº”ç”¨çš„åˆ†æï¼Œå±•ç°RLä¸LLMç»“åˆçš„æ½œåŠ›ä¸ç°çŠ¶ï¼Œä¸ºåç»­ç ”ç©¶æä¾›å‚è€ƒä¾æ®ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸäº¤å‰è§†è§’ï¼šå°†é€†å¼ºåŒ–å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹å¯¹é½ç»“åˆè¿›è¡Œç»¼è¿°ï¼Œä¸ºç ”ç©¶è€…æä¾›äº†è·¨é¢†åŸŸèåˆçš„æ€è€ƒè§’åº¦ï¼Œå¯å‘åœ¨AIä¸åŒå­é¢†åŸŸé—´å¯»æ‰¾å…³è”ä¸åˆ›æ–°ç‚¹ã€‚  
2. çŸ¥è¯†ä½“ç³»æ„å»ºï¼šä»åŸºç¡€æ¦‚å¿µåˆ°å‰æ²¿è¿›å±•ï¼Œå†åˆ°å®è·µå±‚é¢ï¼ˆæ•°æ®é›†ã€åŸºç¡€è®¾æ–½ç­‰ï¼‰å’Œæœªæ¥æ–¹å‘ï¼Œå®Œæ•´çš„çŸ¥è¯†è„‰ç»œæ¢³ç†æœ‰åŠ©äºæ–°æ‰‹å¿«é€Ÿå…¥é—¨è¯¥é¢†åŸŸï¼Œä¹Ÿè®©èµ„æ·±ç ”ç©¶è€…å…¨é¢æŠŠæ¡é¢†åŸŸç°çŠ¶ä¸è¶‹åŠ¿ã€‚  
3. æŒ‘æˆ˜ä¸æœºé‡åˆ†æï¼šå¯¹LLMå¯¹é½ä¸­RLåº”ç”¨é¢ä¸´çš„ç¼ºä¹å¥–åŠ±ä¿¡å·ã€è®¡ç®—æˆæœ¬ã€ç®—æ³•é€‚é…ç­‰æŒ‘æˆ˜çš„å‰–æï¼Œä»¥åŠå¯¹æ½œåœ¨è§£å†³æ–¹å‘çš„æ¢è®¨ï¼Œä¸ºåç»­ç ”ç©¶é€‰é¢˜å’ŒæŠ€æœ¯çªç ´æä¾›äº†æ¸…æ™°çš„é—®é¢˜å¯¼å‘ã€‚

## bridging-the-gap-in-vision-language-models-in-identifying-unsafe-concepts-across-modalities
### Abstract
Vision-language models (VLMs) are increasingly applied to identify unsafe or
inappropriate images due to their internal ethical standards and powerful
reasoning abilities. However, it is still unclear whether they can recognize
various unsafe concepts when presented in different modalities, such as text
and images. To address this, we first compile the UnsafeConcepts dataset,
featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and
``Assaults,'' along with associated 1.5K images. We then conduct a systematic
evaluation of VLMs' perception (concept recognition) and alignment (ethical
reasoning) capabilities. We assess eight popular VLMs and find that, although
most VLMs accurately perceive unsafe concepts, they sometimes mistakenly
classify these concepts as safe. We also identify a consistent modality gap
among open-source VLMs in distinguishing between visual and textual unsafe
concepts. To bridge this gap, we introduce a simplified reinforcement learning
(RL)-based approach using proximal policy optimization (PPO) to strengthen the
ability to identify unsafe concepts from images. Our approach uses reward
scores based directly on VLM responses, bypassing the need for collecting
human-annotated preference data to train a new reward model. Experimental
results show that our approach effectively enhances VLM alignment on images
while preserving general capabilities. It outperforms baselines such as
supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope
our dataset, evaluation findings, and proposed alignment solution contribute to
the community's efforts in advancing safe VLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è·¨æ¨¡æ€è¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µï¼šå¼¥åˆè§†è§‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›é¸¿æ²Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å†…å®¹å®¡æ ¸ç­‰ç°å®åœºæ™¯çš„å¹¿æ³›åº”ç”¨ï¼Œè¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µï¼ˆå¦‚ä»‡æ¨ç¬¦å·ã€æš´åŠ›å›¾åƒã€éœ²éª¨å†…å®¹ç­‰ï¼‰æˆä¸ºæ„å»ºè´Ÿè´£ä»»AIçš„å…³é”®ã€‚ç„¶è€Œï¼ŒVLMsåœ¨æ–‡æœ¬å’Œå›¾åƒç­‰ä¸åŒæ¨¡æ€ä¸‹è¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µæ—¶ï¼Œæ˜¯å¦å­˜åœ¨**æ¨¡æ€é¸¿æ²Ÿ**ï¼ˆå³å¯¹ç›¸åŒæœ‰å®³å†…å®¹ï¼Œæ–‡æœ¬è¾“å…¥å’Œå›¾åƒè¾“å…¥ä¸‹å“åº”ä¸ä¸€è‡´ï¼‰å°šä¸æ˜ç¡®ï¼›ä¸”è‹¥å­˜åœ¨è¯¥é¸¿æ²Ÿï¼Œå¦‚ä½•åœ¨ä¸å½±å“æ¨¡å‹é€šç”¨èƒ½åŠ›çš„å‰æä¸‹å¼¥åˆå®ƒï¼Œä¹Ÿæ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹VLMsè¯†åˆ«å„ç±»ä¸å®‰å…¨æ¦‚å¿µèƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºUnsafeConceptsæ•°æ®é›†  
é¦–æ¬¡æ•´åˆæ¶µç›–9å¤§ç±»åˆ«ã€75ä¸ªä¸å®‰å…¨æ¦‚å¿µï¼ˆå¦‚â€œçº³ç²¹ä¸‡å­—ç¬¦â€â€œæ€§éªšæ‰°â€â€œè¢­å‡»â€ï¼‰åŠ1.5Kå…³è”å›¾åƒçš„æ•°æ®é›†ï¼Œä¸ºè¯„ä¼°VLMsè¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µèƒ½åŠ›æä¾›äº†ç»†ç²’åº¦æ ‡æ³¨çš„åŸºå‡†èµ„æºã€‚åŒºåˆ†â€œè§†è§‰ä¸å®‰å…¨æ¦‚å¿µâ€ï¼ˆå‘ˆç°æ¦‚å¿µçš„å›¾åƒï¼‰ä¸â€œæ–‡æœ¬ä¸å®‰å…¨æ¦‚å¿µâ€ï¼ˆæè¿°æ¦‚å¿µçš„æ–‡æœ¬ï¼‰ï¼Œæ”¯æ’‘è·¨æ¨¡æ€å¯¹æ¯”åˆ†æã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç³»ç»Ÿè¯„ä¼°VLMsçš„åŒæ ¸å¿ƒèƒ½åŠ›  
ä»**æ„ŸçŸ¥ï¼ˆPerceptionï¼‰**å’Œ**å¯¹é½ï¼ˆAlignmentï¼‰**ä¸¤ä¸ªç»´åº¦è¯„ä¼°8æ¬¾ä¸»æµVLMsï¼š  
- æ„ŸçŸ¥èƒ½åŠ›ï¼šæµ‹è¯•æ¨¡å‹æ£€æµ‹å›¾åƒä¸­ä¸å®‰å…¨æ¦‚å¿µçš„å‡†ç¡®æ€§ï¼ˆä¸ºæ¯ä¸ªå›¾åƒè®¾è®¡å•é€‰æ‹©é¢˜ï¼Œå«1ä¸ªæ­£ç¡®é€‰é¡¹å’Œ3ä¸ªå¹²æ‰°é¡¹ï¼‰ï¼›  
- å¯¹é½èƒ½åŠ›ï¼šè¯„ä¼°æ¨¡å‹åˆ¤æ–­æ˜¯å¦ç¬¦åˆäººç±»ä¼¦ç†æ ‡å‡†ï¼ˆè®¾è®¡æç¤ºè¯ï¼Œè¯¢é—®æ¦‚å¿µåœ¨â€œç¤¾äº¤åª’ä½“å±•ç¤ºâ€ç­‰å®‰å…¨åœºæ™¯ä¸‹æ˜¯å¦å®‰å…¨ï¼Œå¹¶èå…¥æ¦‚å¿µç‰¹å®šä¸Šä¸‹æ–‡çº¿ç´¢åˆ†æ nuanced contexts çš„å½±å“ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç®€åŒ–RLHFæ–¹æ³•å¼¥åˆæ¨¡æ€é¸¿æ²Ÿ  
æå‡ºåŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„ç®€åŒ–å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€æ”¶é›†äººå·¥æ ‡æ³¨åå¥½æ•°æ®è®­ç»ƒæ–°å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ç”¨å“åº”åˆ†ç±»å™¨è¯„ä¼°VLMå›ç­”çš„æ­£ç¡®æ€§å¹¶åˆ†é…å¥–åŠ±åˆ†æ•°ï¼Œé€šè¿‡â€œç”Ÿæˆï¼ˆroll - outï¼‰- è¯„ä¼° - ä¼˜åŒ–â€ä¸‰é˜¶æ®µè¿­ä»£æ›´æ–°æ¨¡å‹ï¼š  
- Roll - outï¼šé’ˆå¯¹ä»£è¡¨ä¼¦ç†æ ‡å‡†çš„å®‰å…¨/ä¸å®‰å…¨æ¦‚å¿µï¼Œé‡‡æ ·VLMå“åº”ï¼›  
- è¯„ä¼°ï¼šç”¨åˆ†ç±»å™¨åˆ¤æ–­å“åº”æ­£ç¡®æ€§å¹¶æ‰“èµï¼›  
- ä¼˜åŒ–ï¼šç»“åˆå¥–åŠ±åˆ†æ•°ã€ç†µå¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰å’ŒKLæ•£åº¦ï¼ˆé˜²æ­¢åç¦»åŸè¡Œä¸ºï¼‰ï¼Œé€šè¿‡PPOä¼˜åŒ–VLMã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ„ŸçŸ¥ä¸å¯¹é½èƒ½åŠ›å­˜åœ¨è½å·®ï¼šå¤šæ•°VLMsèƒ½è¾ƒå‡†ç¡®æ„ŸçŸ¥å›¾åƒä¸­ä¸å®‰å…¨æ¦‚å¿µï¼ˆå¦‚LLaVA - 7Bæ„ŸçŸ¥å‡†ç¡®ç‡è¾¾0.93ï¼‰ï¼Œä½†åœ¨å®‰å…¨åœºæ™¯ä¸‹åˆ¤æ–­æ˜¯å¦â€œå®‰å…¨/åˆé€‚â€æ—¶ï¼Œå¯¹é½å¾—åˆ†æ˜¾è‘—é™ä½ï¼ˆLLaVA - 7Bä»…0.37ï¼‰ï¼Œå³å¸¸å¿½è§†å›¾åƒçš„ä¸å®‰å…¨å±æ€§ï¼›  
2. æ¨¡æ€é¸¿æ²Ÿæ™®éå­˜åœ¨ï¼š8æ¬¾VLMsåœ¨åŒºåˆ†è§†è§‰ä¸æ–‡æœ¬ä¸å®‰å…¨æ¦‚å¿µæ—¶ï¼Œä¸€è‡´è¡¨ç°å‡ºè·¨æ¨¡æ€å“åº”ä¸ä¸€è‡´çš„é—®é¢˜ï¼›  
3. ç®€åŒ–RLHFæ–¹æ³•æ›´ä¼˜ï¼šå¯¹æ¯”æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰åŸºçº¿ï¼Œè¯¥æ–¹æ³•åœ¨å¢å¼ºå›¾åƒä¸Šä¸å®‰å…¨æ¦‚å¿µå¯¹é½èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿ç•™äº†æ¨¡å‹é€šç”¨èƒ½åŠ›ï¼Œä¸”åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šæ³›åŒ–æ€§æ›´å¼ºã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†å»ºè®¾ï¼šUnsafeConceptsä¸ºå®‰å…¨AIé¢†åŸŸæä¾›äº†é¦–ä¸ªç»†ç²’åº¦ã€å¤šæ¨¡æ€çš„ä¸å®‰å…¨æ¦‚å¿µåŸºå‡†é›†ï¼Œå¯å‘åç»­ç ”ç©¶è€…æ„å»ºæ›´ä¸°å¯Œçš„å®‰å…¨è¯„ä¼°èµ„æºï¼›  
2. è¯„ä¼°ç»´åº¦ï¼šä»â€œæ„ŸçŸ¥ï¼ˆèƒ½ä¸èƒ½è¯†åˆ«ï¼‰â€åˆ°â€œå¯¹é½ï¼ˆæ˜¯å¦ç¬¦åˆä¼¦ç†ï¼‰â€çš„åŒç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œä¸ºåˆ†æAIæ¨¡å‹å®‰å…¨èƒ½åŠ›æä¾›äº†æ›´å…¨é¢çš„è§†è§’ï¼›  
3. è½»é‡åŒ–å¼ºåŒ–å­¦ä¹ ï¼šæ— éœ€äººå·¥åå¥½æ•°æ®çš„ç®€åŒ–RLHFæ€è·¯ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹ä¼˜åŒ–æ¨¡å‹å®‰å…¨å¯¹é½æä¾›äº†é«˜æ•ˆæ–¹æ¡ˆï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€ä¼¦ç†å¯¹é½ä½†æ ‡æ³¨æˆæœ¬é«˜çš„ä»»åŠ¡ã€‚

## eduflow--advancing-mllms--problem-solving-proficiency-through-multi-stage--multi-perspective-critique
### Abstract
Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EduFlowï¼šå¤šé˜¶æ®µå¤šè§†è§’æ‰¹åˆ¤åŠ©åŠ›MLLMsæ”»å…‹ç§‘å­¦æ¨ç†éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­è¨€ç±»å­¦ç§‘è¡¨ç°å°šå¯ï¼Œä½†åœ¨ç§‘å­¦ç±»ä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯å¤šæ­¥éª¤ã€å¯è§£é‡Šæ€§æ¨ç†ä»»åŠ¡ï¼‰ä¸­è¡¨ç°ç³Ÿç³•ã€‚å…¶å­˜åœ¨ä¸‰å¤§æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ˜¯ç§‘å­¦æ¨ç†æ¨¡å¼å‚¨å¤‡ä¸è¶³ï¼Œé¢„è®­ç»ƒé˜¶æ®µSTEMï¼ˆç§‘å­¦ã€æŠ€æœ¯ã€å·¥ç¨‹ã€æ•°å­¦ï¼‰ç±»ç‰¹å®šè¯­æ–™ï¼ˆå¦‚å…¬å¼ã€ç¬¦å·é€»è¾‘ã€ç»“æ„åŒ–è§£é¢˜æ­¥éª¤ç­‰ï¼‰è¿œå°‘äºå¼€æ”¾åŸŸæ–‡æœ¬ï¼›äºŒæ˜¯å¤šæ­¥éª¤æ¨ç†æ—¶ç¼ºä¹å…¨å±€è¿è´¯æ€§ï¼Œæ¨ç†é”™è¯¯ä¸æ–­ç´¯ç§¯ï¼›ä¸‰æ˜¯æ²¡æœ‰æœ‰æ•ˆçš„è‡ªæˆ‘åæ€å’Œçº é”™æœºåˆ¶ï¼Œéš¾ä»¥è·³å‡ºè®°å¿†çŸ¥è¯†ã€æ•´åˆæ–°è§è§£ã€‚2024å¹´ä¸­å›½é«˜è€ƒä¸­LLMsåœ¨æ•°å­¦ç§‘ç›®å…¨å†›è¦†æ²¡ç­‰æ¡ˆä¾‹ï¼Œä¹Ÿå‡¸æ˜¾äº†â€œå­¦ç§‘å¤±è¡¡â€é—®é¢˜åœ¨ç°ä»£åŸºå‡†æµ‹è¯•ä¸­æ™®éå­˜åœ¨ï¼Œè¿™äº›éƒ½ä¿ƒä½¿ç ”ç©¶è€…æ€è€ƒå¦‚ä½•æå‡MLLMsåœ¨ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼ŒEduFlowæ¡†æ¶åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºç«¯åˆ°ç«¯æ¡†æ¶EduFlow  
EduFlowæ˜¯é¦–ä¸ªè¦†ç›–æ•™è‚²ç§‘å­¦æ¨ç†å…¨æµç¨‹çš„æ¡†æ¶ï¼ŒåŒ…å«æ•°æ®é€‰æ‹©ã€åŸºäºMCTSï¼ˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼‰çš„æ¨ç†è½¨è¿¹æ„å»ºã€æ¨¡å‹è®­ç»ƒã€è¾“å‡ºä¼˜åŒ–ç­‰ç¯èŠ‚ã€‚å®ƒä»¥EduPRMä¸ºæ ¸å¿ƒï¼Œä¸²è”èµ·â€œPRMå¼•å¯¼çš„æ•°æ®è¿‡æ»¤ï¼ˆç­›é€‰éœ€é‡æ„çš„ä½è´¨é‡æ¨ç†è½¨è¿¹ï¼‰ã€PRMå¼•å¯¼çš„EduMCTSæ•°æ®æ„å»ºï¼ˆå€ŸåŠ©EduPRMç›‘ç£æœç´¢ç”Ÿæˆä¼˜è´¨è½¨è¿¹ï¼‰ã€åŸºäºPRMçš„Best - of - Né€‰æ‹©ï¼ˆä»å€™é€‰è¾“å‡ºé€‰æœ€ä¼˜ç­”æ¡ˆï¼‰â€ç­‰å…³é”®é˜¶æ®µï¼Œä¸ºæ•™è‚²é¢†åŸŸç»“æ„åŒ–æ¨ç†ç›‘ç£æä¾›äº†é€šç”¨ä¸”å¯æ‰©å±•çš„è“å›¾ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è¿‡ç¨‹æ„ŸçŸ¥å¥–åŠ±æ¨¡å‹EduPRM  
EduPRMçªç ´ä¼ ç»ŸPRMå±€é™ï¼Œèƒ½ä»¥åˆ†æ•°ã€æ ‡ç­¾ã€ç†ç”±å¯¹æ¨ç†æ­¥éª¤è¿›è¡Œæ‰¹åˆ¤ã€‚å®ƒé€šè¿‡è¯¾ç¨‹å­¦ä¹ ï¼Œåˆ©ç”¨ä¸‰ç§äº’è¡¥ç›‘ç£æºè®­ç»ƒï¼šMCTSå¼•å¯¼çš„æ¨ç†è½¨è¿¹ï¼ˆè‡ªåŠ¨æ¢ç´¢å¤šæ ·è§£é¢˜è·¯å¾„ï¼‰ã€æ³¨å…¥é”™è¯¯çš„æ‰¹åˆ¤ï¼ˆæ¨¡æ‹Ÿå­¦ç”Ÿå¸¸è§è¯¯è§£ï¼‰ã€å¸ˆç”Ÿå¯¹è¯ï¼ˆå®Œå–„æ¨¡ç³Šæ¨ç†æ­¥éª¤ï¼‰ï¼Œä»¥æ­¤æ•æ‰å¤šç²’åº¦æ•™è‚²æ¨ç†æ¨¡å¼ï¼Œè¿˜èƒ½åŠ¨æ€é€‚é…å¤šé˜¶æ®µè§£é¢˜å¹¶åœ¨æ¨ç†ä¸­è¿­ä»£ä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºé¢†åŸŸé€‚é…æœç´¢æ¡†æ¶EduMCTS  
é’ˆå¯¹MCTSåº”ç”¨äºMLLMsæ—¶çš„ä½æ•ˆæœç´¢ï¼ˆæ— æ­¥éª¤æŒ‡å¯¼æ˜“æ¢ç´¢åŒè´¨åŒ–ä½è´¨è·¯å¾„ï¼‰å’Œé«˜æˆæœ¬é—®é¢˜ï¼ŒEduMCTSç»“åˆactoræ¨¡å‹ç”Ÿæˆä¸EduPRMå¼•å¯¼çš„æ‰¹åˆ¤ï¼Œè¿­ä»£æ„å»ºé«˜è´¨é‡æ¨ç†è·¯å¾„ã€‚å®ƒèå…¥é€æ­¥å¼•å¯¼åŠ¨ä½œã€æ­¥éª¤çº§å¥–åŠ±åé¦ˆã€è‡ªæˆ‘åæ€æœºåˆ¶ï¼ˆä¿ƒè¿›åæ€æ€§çº é”™ï¼‰ï¼Œäº§å‡ºæ•™å­¦åˆç†æ€§å¼ºçš„æ¨ç†è½¨è¿¹ã€‚åŸºäºæ­¤æ„å»ºçš„EduMCTS - 160Kæ•°æ®é›†ï¼Œåœ¨ç§‘å­¦é¢†åŸŸæ¯”LLaVA - CoTæˆåŠŸç‡é«˜18%ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒéªŒè¯äº†EduFlowçš„æœ‰æ•ˆæ€§ï¼šåœ¨ä¸åŒè§„æ¨¡Qwenæ¨¡å‹å®¶æ—ä¸Šï¼Œå¯¹æ¯”åŸºçº¿ã€Edu - 1000Kã€CoT - 1000Kç­‰è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼ŒEduFlowç›¸å…³æ–¹æ³•ï¼ˆå¦‚EduMCTS - 160Kç»“åˆ+BoN(N = 8)ï¼‰åœ¨é€æ­¥æ¨ç†å¾—åˆ†ä¸Šæœ‰æ˜¾è‘—æå‡ï¼›åœ¨è¿‡ç¨‹å¯¼å‘å’Œç»“æœå¯¼å‘æŒ‡æ ‡ä¸Šå‡å®ç°+8.2%çš„ä¸€è‡´æ”¹è¿›ï¼Œå¤§å¹…ç¼©å°äº†LLMsä¸äººç±»æ°´å¹³åœ¨å¤æ‚æ•™è‚²ä»»åŠ¡æ¨ç†ä¸Šçš„å·®è·ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å…¨æµç¨‹æ¡†æ¶æ€è·¯ï¼šEduFlowæ•´åˆæ•°æ®åˆ°è¾“å‡ºå…¨ç¯èŠ‚çš„åšæ³•ï¼Œä¸ºç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•™è‚²ã€STEMï¼‰æ¨¡å‹èƒ½åŠ›æå‡æä¾›äº†â€œå…¨æ ˆå¼â€ä¼˜åŒ–å‚è€ƒï¼Œå¯å¯å‘ç ”ç©¶è€…é’ˆå¯¹å…¶ä»–ä¸“ä¸šé¢†åŸŸä»»åŠ¡è®¾è®¡ç«¯åˆ°ç«¯æ–¹æ¡ˆã€‚  
2. å¥–åŠ±æ¨¡å‹åˆ›æ–°ï¼šEduPRMç»“åˆå¤šç›‘ç£æºã€è¯¾ç¨‹å­¦ä¹ ä»¥åŠä¸°å¯Œåé¦ˆå½¢å¼ï¼ˆåˆ†æ•°ã€æ ‡ç­¾ã€ç†ç”±ï¼‰çš„è®¾è®¡ï¼Œä¸ºæ‰“é€ æ›´æ™ºèƒ½çš„â€œè¿‡ç¨‹è¯„ä¼°â€æ¨¡å‹æä¾›äº†èŒƒå¼ï¼Œå¯ç”¨äºéœ€æ­¥éª¤çº§åé¦ˆçš„æ¨ç†ã€å†³ç­–ç±»ä»»åŠ¡ã€‚  
3. æœç´¢æ¡†æ¶é€‚é…ï¼šEduMCTSé’ˆå¯¹é¢†åŸŸç‰¹æ€§æ”¹é€ MCTSï¼Œèå…¥å¼•å¯¼ä¸åæ€æœºåˆ¶ï¼Œè¯æ˜äº†ç»å…¸æœç´¢ç®—æ³•åœ¨å¤§æ¨¡å‹æ—¶ä»£ç»“åˆé¢†åŸŸçŸ¥è¯†å’Œå¥–åŠ±æ¨¡å‹èƒ½ç„•å‘æ–°æ´»åŠ›ï¼Œä¸ºå¤æ‚ä»»åŠ¡æ¨ç†è·¯å¾„æ¢ç´¢æä¾›äº†å€Ÿé‰´æ–¹å‘ã€‚  
4. æ•°æ®é›†æ„å»ºï¼šé€šè¿‡è‡ªä¸€è‡´æ€§å’Œæ‹’ç»é‡‡æ ·æ„å»ºEduMCTS - 160Ké«˜è´¨é‡æ•™è‚²æ¨ç†è½¨è¿¹æ•°æ®é›†ï¼Œä¸ºé¢†åŸŸå†…æ¨¡å‹è®­ç»ƒã€è¯„ä¼°æä¾›ä¼˜è´¨æ•°æ®èµ„æºçš„æ€è·¯å€¼å¾—å­¦ä¹ ï¼Œå°¤å…¶æ˜¯åœ¨ä¼˜è´¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸã€‚

## one-token-to-fool-llm-as-a-judge
### Abstract
Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸€ä¸ªTokenå°±èƒ½éª—è¿‡LLMè£åˆ¤ï¼Ÿæ­ç§˜ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„è„†å¼±æ€§ä¸åº”å¯¹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„èŒƒå¼ä¸­ï¼Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆä¹Ÿè¢«ç§°ä¸ºâ€œLLMä½œä¸ºè£åˆ¤â€ï¼‰å‡­å€Ÿå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„ç”Ÿæˆä¸æ³›åŒ–èƒ½åŠ›ï¼Œé€æ¸å–ä»£åƒµåŒ–çš„è§„åˆ™å¼æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ç­”æ¡ˆè´¨é‡ã€‚ä½†ç ”ç©¶å‘ç°ï¼Œè¿™ç±»ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹å­˜åœ¨æƒŠäººçš„è„†å¼±æ€§ï¼šä»…éœ€ä¸€äº›è¡¨é¢æ“ä½œï¼ˆæ¯”å¦‚éæ–‡å­—ç¬¦å·â€œ:â€â€œ.â€ï¼Œæˆ–è€…åƒâ€œThought process:â€â€œLetâ€™s solve this problem step by step.â€è¿™ç±»æ¨ç†å¼€å¤´ï¼‰ï¼Œå°±èƒ½è®©æ¨¡å‹ç»™å‡ºé”™è¯¯çš„æ­£å‘å¥–åŠ±ã€‚è¿™ç§å¼±ç‚¹åœ¨ä¸åŒLLMã€æ•°æ®é›†å’Œæç¤ºæ ¼å¼ä¸­å¹¿æ³›å­˜åœ¨ï¼Œå¯¹ä¾èµ–ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒç®—æ³•ï¼ˆå¦‚æ‹’ç»é‡‡æ ·ã€åå¥½ä¼˜åŒ–ã€RLVRç­‰ï¼‰æ„æˆä¸¥é‡å¨èƒã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨æ­ç¤ºè¯¥é—®é¢˜å¹¶æå‡ºæ”¹è¿›æ–¹æ³•ï¼Œæ¨åŠ¨æ›´å¯é çš„åŸºäºLLMçš„è¯„ä¼°æ–¹å¼å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°â€œä¸‡èƒ½é’¥åŒ™â€æ¼æ´  
è®ºæ–‡é¦–æ¬¡æ˜ç¡®æŒ‡å‡ºï¼Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åœ¨RLVRåœºæ™¯ä¸‹å­˜åœ¨ç³»ç»Ÿæ€§è„†å¼±æ€§â€”â€”ä»…å«éæ–‡å­—ç¬¦å·æˆ–æ¨ç†å¼€å¤´çš„â€œå€™é€‰ç­”æ¡ˆâ€ï¼Œå¸¸èƒ½éª—å–æ­£å‘å¥–åŠ±ã€‚è¿™ç±»èƒ½è§¦å‘é”™è¯¯å¥–åŠ±çš„å¯¹æŠ—å¼å“åº”è¢«ç§°ä¸ºâ€œmaster keysï¼ˆä¸‡èƒ½é’¥åŒ™ï¼‰â€ï¼Œä¸”è¯¥é—®é¢˜åœ¨å¤šæ¨¡å‹ã€å¤šæ•°æ®é›†ä¸Šæ™®éå­˜åœ¨ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç³»ç»Ÿæ€§è¯„ä¼°æ¼æ´çš„æ™®éæ€§  
å›¢é˜Ÿç”¨åç§â€œä¸‡èƒ½é’¥åŒ™â€å“åº”ï¼Œåœ¨å¤šæ¨¡å‹ï¼ˆé€šç”¨æ¨¡å‹å¦‚Qwen2.5 - 72Bã€GPT - 4oï¼Œä¸“ç”¨éªŒè¯å™¨å¦‚Omni - Judgeï¼‰å’Œå¤šæ•°æ®é›†ï¼ˆæ•°å­¦æ¨ç†ã€é€šç”¨é¢†åŸŸç­‰ï¼‰ä¸Šå±•å¼€æµ‹è¯•ï¼ŒéªŒè¯äº†æ¼æ´çš„å¹¿æ³›æ€§ï¼›è¿˜åˆ†æäº†è¯¥ç°è±¡çš„è§„æ¨¡æ•ˆåº”ä¸ç”Ÿæˆæ–°â€œä¸‡èƒ½é’¥åŒ™â€çš„æŠ€å·§ï¼ŒåŒæ—¶è¯æ˜æ¨ç†æ—¶ç­–ç•¥éš¾ä»¥å¯é é˜²å¾¡è¿™ç±»æ”»å‡»ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®å¢å¼º+é²æ£’å¥–åŠ±æ¨¡å‹  
ä¸ºç¼“è§£æ¼æ´ï¼Œæå‡ºç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼šæ„é€ â€œç±»å¯¹æŠ—â€å“åº”ï¼ˆæˆªæ–­æ¨¡å‹è¾“å‡ºï¼Œä¿ç•™ä»…åšæ³›åŒ–é“ºå«ã€æ— å®é™…è§£é¢˜çš„å¼€å¤´ç‰‡æ®µä½œä¸ºè´Ÿæ ·æœ¬ï¼‰æ¥æ‰©å……è®­ç»ƒæ•°æ®ã€‚åŸºäºæ­¤è®­ç»ƒå‡ºé€šç”¨é¢†åŸŸçš„Master - RMå¥–åŠ±æ¨¡å‹ï¼Œå¤§å¹…æå‡äº†å¯¹â€œä¸‡èƒ½é’¥åŒ™â€çš„é²æ£’æ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”å¤§æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå«GSM8Kã€MATHç­‰æ•°å­¦æ¨ç†æ•°æ®é›†ï¼ŒåŠMulti - subject RLVRç­‰é€šç”¨é¢†åŸŸæ•°æ®é›†ï¼‰ä¸­ï¼Œä¼ ç»ŸLLMè£åˆ¤é¢å¯¹â€œä¸‡èƒ½é’¥åŒ™â€æ”»å‡»æ—¶ï¼Œé”™è¯¯æ­£ä¾‹ç‡ï¼ˆFPRï¼‰æœ€é«˜è¾¾80%ï¼›è€Œæ–°è®­ç»ƒçš„Master - RMåœ¨æ‰€æœ‰åœºæ™¯ä¸‹é”™è¯¯æ­£ä¾‹ç‡æ¥è¿‘0ï¼Œé²æ£’æ€§æ˜¾è‘—è¶…è¶ŠåŒç±»æ¨¡å‹ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¼æ´å‘ç°è§†è§’ï¼šæé†’ç ”ç©¶è€…ä¸å¼€å‘è€…é‡æ–°å®¡è§†â€œLLMä½œä¸ºè£åˆ¤â€çš„é²æ£’æ€§å‡è®¾ï¼Œåœ¨ä¾èµ–ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„ç³»ç»Ÿè®¾è®¡ä¸­ï¼Œéœ€ä¼˜å…ˆè€ƒè™‘å¯¹æŠ—å¼æµ‹è¯•ä¸é²æ£’æ€§éªŒè¯ï¼›  
2. æ•°æ®å¢å¼ºæ€è·¯ï¼šè®ºæ–‡ç”¨â€œæ„é€ ç±»å¯¹æŠ—è´Ÿæ ·æœ¬â€æ¥å¢å¼ºæ¨¡å‹é²æ£’æ€§çš„æ–¹æ³•ï¼Œä¸ºè®­ç»ƒæ›´å¯é çš„å¥–åŠ±æ¨¡å‹æä¾›äº†è½»é‡å´æœ‰æ•ˆçš„èŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€æŠµå¾¡è¡¨é¢å¹²æ‰°çš„è¯„ä¼°ç±»æ¨¡å‹è®­ç»ƒä¸­ï¼›  
3. å¼€æºèµ„æºä»·å€¼ï¼šå‘å¸ƒçš„Master - RMæ¨¡å‹ä¸åˆæˆè®­ç»ƒæ•°æ®ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ç›´æ¥å¯ç”¨çš„é²æ£’å¥–åŠ±æ¨¡å‹åŸºçº¿ï¼Œé™ä½äº†é¢†åŸŸå†…é‡å¤é€ è½®å­çš„æˆæœ¬ï¼Œæ¨åŠ¨ç›¸å…³æ–¹å‘å¿«é€Ÿè¿­ä»£ã€‚

## quantile-reward-policy-optimization--alignment-with-pointwise-regression-and-exact-partition-functions
### Abstract
Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼¥åˆç»å¯¹å¥–åŠ±ä¸ç¦»çº¿ç­–ç•¥é¸¿æ²Ÿï¼šQRPOå¼€å¯å¤§æ¨¡å‹å¯¹é½æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½é¢†åŸŸä¸­ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾â€œå‰²è£‚â€ï¼šä¸€æ–¹é¢ï¼ŒåƒPPOã€GRPOè¿™ç±»**åœ¨çº¿ç­–ç•¥ï¼ˆon - policyï¼‰**ç®—æ³•è™½èƒ½åˆ©ç”¨â€œé€ç‚¹ç»å¯¹å¥–åŠ±â€ï¼ˆpointwise absolute rewardsï¼‰ä¼˜åŒ–æ¨¡å‹ï¼Œä½†éœ€è¦åœ¨çº¿é‡‡æ ·ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”å¤æ‚ï¼›å¦ä¸€æ–¹é¢ï¼ŒDPOã€REBELç­‰**ç­–ç•¥æ‹Ÿåˆï¼ˆpolicy fittingï¼‰**æ–¹æ³•è™½ç®€å•ä¸”æ”¯æŒç¦»çº¿æ•°æ®ï¼Œå´åªèƒ½ä¾èµ–â€œç›¸å¯¹å¥–åŠ±ä¿¡å·â€ï¼ˆå¦‚åå¥½å¯¹ï¼‰ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ›´æ˜“æ”¶é›†ï¼ˆå¦‚è¯„åˆ†é‡è¡¨å½¢å¼ï¼‰ã€è¡¨è¾¾èƒ½åŠ›æ›´å¼ºçš„ç»å¯¹å¥–åŠ±ï¼ˆå¦‚å¼ºå¤§çš„å¥–åŠ±æ¨¡å‹è¾“å‡ºã€å¯éªŒè¯å¥–åŠ±ï¼‰ã€‚æ­¤å¤–ï¼Œç›¸å¯¹å¥–åŠ±è¿˜å¯èƒ½å› å€™é€‰æ ·æœ¬â€œåŒä¼˜â€ç­‰æƒ…å†µæä¾›æ¬¡ä¼˜ä¿¡å·ï¼Œé™åˆ¶äº†æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚ä¸ºå¼¥åˆè¿™ä¸€æŠ€æœ¯é¸¿æ²Ÿï¼Œè®ºæ–‡æå‡º**åˆ†ä½æ•°å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆQuantile Reward Policy Optimizationï¼ŒQRPOï¼‰** ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ”»å…‹ç­–ç•¥æ‹Ÿåˆä¸­é…åˆ†å‡½æ•°ä¼°è®¡éš¾é¢˜ï¼Œå®ç°ç»å¯¹å¥–åŠ±ä¸‹çš„é—­å¼è§£æ‹Ÿåˆ  
ç­–ç•¥æ‹Ÿåˆæ–¹æ³•æ­¤å‰å› ç»å¯¹å¥–åŠ±ä¸‹é…åˆ†å‡½æ•°ï¼ˆpartition functionï¼‰éš¾ä»¥ä¼°è®¡è€Œå—é™ã€‚QRPOçš„å…³é”®æ´è§æ˜¯**é‡‡ç”¨åˆ†ä½æ•°å¥–åŠ±ï¼ˆquantile rewardsï¼‰** ï¼Œè®©é…åˆ†å‡½æ•°æœ‰äº†è§£æå¯è§£çš„è¡¨è¾¾å¼ï¼ˆæ¨å¯¼å¾—é…åˆ†å‡½æ•°ä¸ºÎ²(exp(1/Î²)âˆ’1)ï¼‰ã€‚è¿™ä½¿å¾—KLæ­£åˆ™åŒ–çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡èƒ½é€šè¿‡ç®€å•çš„ç›‘ç£å¼å›å½’ï¼Œåœ¨å•ä¸ªæ ·æœ¬çš„é€ç‚¹ç»å¯¹å¥–åŠ±ä¿¡å·ä¸‹ï¼Œæ‹Ÿåˆé—­å¼æœ€ä¼˜ç­–ç•¥ï¼Œæ— éœ€ç›¸å¯¹ä¿¡å·æ¥æŠµæ¶ˆé…åˆ†å‡½æ•°é¡¹ï¼Œä»ç†è®ºå±‚é¢çªç ´äº†ä¼ ç»Ÿç­–ç•¥æ‹Ÿåˆåªèƒ½ä¾èµ–ç›¸å¯¹å¥–åŠ±çš„é™åˆ¶ã€‚åŒæ—¶ï¼ŒQRPOè¿˜å¯æ‹“å±•ä¸ºä¸€ä¸ªæ¡†æ¶ï¼Œåœ¨åˆ†ä½æ•°å¥–åŠ±åŸºç¡€ä¸Šæ–½åŠ é¢å¤–å˜æ¢åï¼Œé…åˆ†å‡½æ•°ä»ä¿æŒå¯ tractableï¼ˆæ˜“å¤„ç†ï¼‰ï¼Œçµæ´»æ€§å¼ºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¡ç®—èµ„æºå¯æ‰©å±•çš„é¢„è®¡ç®—èŒƒå¼ï¼Œæå‡åˆ†ä½æ•°å¥–åŠ±ä¼°è®¡æ•ˆèƒ½  
QRPOæ”¯æŒé€šè¿‡**å¢åŠ é¢„è®¡ç®—é¢„ç®—**æ¥ç”Ÿæˆæ›´å¤šå‚è€ƒå¥–åŠ±ï¼Œä»¥æ­¤ä¼˜åŒ–åˆ†ä½æ•°å¥–åŠ±çš„ä¼°è®¡è´¨é‡ï¼Œè¿›è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ç§â€œé¢„è®¡ç®— - è®­ç»ƒâ€çš„åˆ†ç¦»æ¨¡å¼ï¼Œä¸ºè®¡ç®—èµ„æºçš„è§„æ¨¡åŒ–åˆ©ç”¨å¼€è¾Ÿäº†æ–°ç»´åº¦â€”â€”æ›´å¤šçš„é¢„è®¡ç®—å¯è½¬åŒ–ä¸ºæ›´ç²¾å‡†çš„å¥–åŠ±ä¼°è®¡ï¼Œè®©æ–¹æ³•åœ¨ä¸åŒç®—åŠ›è§„æ¨¡ä¸‹éƒ½æœ‰æ€§èƒ½æå‡ç©ºé—´ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
1. å¯¹è¯ä¸ç¼–ç ä»»åŠ¡å…¨é¢é¢†å…ˆï¼šåœ¨èŠå¤©ï¼ˆå¦‚åŸºäº reward model è¯„åˆ†ã€AlpacaEval 2 åŸºå‡†ï¼‰å’Œç¼–ç ï¼ˆå¦‚ LeetCode æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡ï¼‰ç­‰ä»»åŠ¡ä¸­ï¼ŒQRPO å¯¹æ¯” DPOã€REBELã€SimPO ç­‰æ–¹æ³•ï¼Œåœ¨å¤šæ•°æ®é›†ï¼ˆå¦‚ Magpie - Airã€UltraFeedbackï¼‰å’Œ 8B è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ Llama 8Bï¼‰çš„è¶… 200 ç»„è®¾ç½®ä¸‹ï¼ŒæŒç»­æ–©è·æœ€ä¼˜è¡¨ç°ã€‚ä»¥ LeetCode ä»»åŠ¡ä¸ºä¾‹ï¼ŒLlama 8B åŸºç¡€ä¸Šï¼ŒQRPO èƒ½å°†æµ‹è¯•ç”¨ä¾‹å¹³å‡é€šè¿‡ç‡æ¨è‡³ 32.7% Â± 1.0ï¼Œæ˜¾è‘—é«˜äº DPOï¼ˆ30.2% Â± 1.4ï¼‰ã€REBELï¼ˆ26.1% Â± 1.8ï¼‰ç­‰æ–¹æ³•ã€‚  
2. é•¿åº¦åå·®ç¼“è§£ä¼˜åŠ¿ï¼šç›¸è¾ƒäºéœ€å°†å¥–åŠ±è½¬åå¥½çš„æ–¹æ³•ï¼ˆå¦‚ DPOã€SimPOï¼‰ï¼ŒQRPO ç›´æ¥ä½¿ç”¨é²æ£’å¥–åŠ±è®­ç»ƒï¼Œå±•ç°å‡ºæ›´å¼±çš„é•¿åº¦åå·®ã€‚SimPO è™½å°è¯•ç”¨é•¿åº¦å½’ä¸€åŒ–ç¼“è§£ï¼Œä½†æ”¿ç­–ä»å­˜åœ¨å¼ºé•¿åº¦åå‘ï¼›è€Œ QRPO ä¸ REBEL åˆ™æ— æ­¤é—®é¢˜ï¼ŒéªŒè¯äº†ç»å¯¹å¥–åŠ±ç›´æ¥åˆ©ç”¨åœ¨åå·®æ§åˆ¶ä¸Šçš„ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æŠ€æœ¯èŒƒå¼åˆ›æ–°ï¼šQRPO ä¸ºâ€œç¦»çº¿ç­–ç•¥ + é€ç‚¹ç»å¯¹å¥–åŠ±â€çš„å¤§æ¨¡å‹å¯¹é½æä¾›äº†å…¨æ–°å¯è¡Œè·¯å¾„ï¼Œæ‰“ç ´äº†åœ¨çº¿ç­–ç•¥ç®—æ³•å¯¹ç»å¯¹å¥–åŠ±åˆ©ç”¨çš„å„æ–­ï¼Œè®©ç®€å•é«˜æ•ˆçš„ç­–ç•¥æ‹Ÿåˆæ–¹æ³•èƒ½æ‹¥æŠ±æ›´ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·æºï¼Œå¯å‘åç»­åœ¨å¥–åŠ±å»ºæ¨¡ä¸ç­–ç•¥ä¼˜åŒ–ç»“åˆä¸Šçš„æ¢ç´¢ã€‚  
2. é¢„è®¡ç®—è§„æ¨¡åŒ–æ€è·¯ï¼šå…¶â€œé¢„è®¡ç®—å‚è€ƒå¥–åŠ± - è®­ç»ƒæ—¶ç”¨åˆ†ä½æ•°å¥–åŠ±â€çš„æ¨¡å¼ï¼Œä¸ºå¤§æ¨¡å‹è®­ç»ƒä¸­è®¡ç®—èµ„æºçš„åˆ†å±‚åˆ†é…ã€é¢„è®¡ç®—æµæ°´çº¿æ„å»ºæä¾›äº†å®è·µå‚è€ƒï¼ŒåŠ©åŠ›è¡Œä¸šåœ¨ç®—åŠ›åˆ©ç”¨æ•ˆç‡ä¸Šçš„ä¼˜åŒ–ã€‚  
3. åå·®ä¸æ€§èƒ½å¹³è¡¡ï¼šæ­ç¤ºäº†ç›´æ¥åˆ©ç”¨é²æ£’ç»å¯¹å¥–åŠ±åœ¨ç¼“è§£é•¿åº¦åå·®ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸ºè¿½æ±‚æ¨¡å‹è¾“å‡ºè´¨é‡ï¼ˆå¦‚ç¼–ç ä»»åŠ¡æ­£ç¡®æ€§ï¼‰ä¸å…¬å¹³æ€§ï¼ˆæ— åå‘æ€§ï¼‰çš„åœºæ™¯ï¼Œæä¾›äº†æ–¹æ³•è®ºå±‚é¢çš„å€Ÿé‰´æ–¹å‘ã€‚

## why-is-your-language-model-a-poor-implicit-reward-model-
### Abstract
Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹ä¸ºä½•æˆäº†â€œå·®åŠ²â€çš„éšå¼å¥–åŠ±æ¨¡å‹ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹æ˜¯è¯­è¨€æ¨¡å‹åè®­ç»ƒä¸æ¨ç†æµç¨‹çš„å…³é”®ç»„ä»¶ã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œæ¯ä¸ªè¯­è¨€æ¨¡å‹éƒ½èƒ½å®šä¹‰éšå¼å¥–åŠ±æ¨¡å‹ï¼ˆIM - RMï¼‰ï¼Œä¸”æ— éœ€æ¶æ„æ”¹åŠ¨ï¼›åŒæ—¶è¿˜æœ‰æ˜¾å¼å¥–åŠ±æ¨¡å‹ï¼ˆEX - RMï¼‰ï¼Œå®ƒåœ¨è¯­è¨€æ¨¡å‹éšè—è¡¨ç¤ºä¸Šé™„åŠ ä¸“ç”¨çº¿æ€§å¤´ã€‚å°½ç®¡äºŒè€…è®­ç»ƒæ•°æ®ã€æŸå¤±å‡½æ•°å’ŒåŸºç¡€è¯­è¨€æ¨¡å‹å‡ ä¹ç›¸åŒï¼Œä»…å¥–åŠ±è®¡ç®—æ–¹å¼æœ‰åˆ«ï¼Œä½†IM - RMæ³›åŒ–æ€§ï¼ˆå°¤å…¶åˆ†å¸ƒå¤–åœºæ™¯ï¼‰è¿œå·®äºEX - RMï¼Œè¿™ç§æ³›åŒ–å·®è·æˆå› ä¸æ˜ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶ä¸åŒå¥–åŠ±æ¨¡å‹éšå¼åå·®ï¼Œæ­ç¤ºè¯¥å·®è·æ ¹æºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç©¶æ³›åŒ–å·®è·æ›¿ä»£å‡è®¾å¹¶è¯ä¼ª
é’ˆå¯¹â€œIM - RMå› å…¼å…·éªŒè¯ä¸ç”Ÿæˆè§’è‰²ï¼Œåœ¨ç”Ÿæˆæ¯”éªŒè¯éš¾çš„ä»»åŠ¡ä¸­æ³›åŒ–å·®â€è¿™ä¸€å‡è®¾ï¼Œé€šè¿‡ç†è®ºè¯æ˜å­¦ä¹ IM - RMçš„éªŒè¯èƒ½åŠ›æ— éœ€å­¦ä¹ ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åœ¨å“ˆå¯†é¡¿å›è·¯éªŒè¯ä»»åŠ¡å®éªŒä¸­ä½è¯è¯¥ç†è®ºï¼ŒæŒ‘æˆ˜äº†è¿™ä¸€ç›´è§‚è®¤çŸ¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†æå­¦ä¹ åŠ¨æ€æ­ç¤ºåå·®æœ¬è´¨
ä»ç†è®ºä¸Šåˆ»ç”»EX - RMä¸IM - RMçš„å­¦ä¹ åŠ¨æ€ï¼ˆæ¢¯åº¦è®­ç»ƒä¸­å¥–åŠ±çš„æ¼”å˜ï¼‰ï¼Œå‘ç°EX - RMå­¦ä¹ åŠ¨æ€ä¸»è¦é€šè¿‡å“åº”çš„éšè—è¡¨ç¤ºä¾èµ–å“åº”ï¼›è€ŒIM - RMå¯¹å“åº”ä¸­ç‰¹å®š tokens æ›´æ•æ„Ÿï¼Œæå‡æŸå“åº”å¥–åŠ±å¯èƒ½ä¸å½±å“ç”šè‡³é™ä½è¯­ä¹‰ç›¸ä¼¼ä½† tokens ä¸åŒå“åº”çš„å¥–åŠ±ï¼Œæ­ç¤ºå‡ºIM - RMæ›´ä¾èµ–è¡¨å±‚ token çº§çº¿ç´¢è¿™ä¸€å…³é”®å·®å¼‚ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†è®ºä¸å®éªŒç»“åˆéªŒè¯ç»“è®º
ç†è®ºå±‚é¢ï¼Œæ„é€ åœºæ™¯è¯æ˜å½“éšè—è¡¨ç¤ºç»“æ„è‰¯å¥½æ—¶ï¼ŒIM - RMå¯èƒ½æ— æ³•æ³›åŒ–åˆ°æœªè§ tokensï¼Œè€ŒEX - RMå¯æˆåŠŸæ³›åŒ–ï¼›å®éªŒå±‚é¢ï¼Œåœ¨å—æ§ä¸çœŸå®åœºæ™¯ä¸‹å¼€å±•å®éªŒï¼Œå¦‚åŸºäºUltraFeedbackè®­ç»ƒå¹¶åœ¨åˆ†å¸ƒå†…ã€tokençº§åˆ†å¸ƒåç§»ã€é¢†åŸŸåç§»åœºæ™¯è¯„ä¼°ï¼Œè¿›ä¸€æ­¥è¯å®IM - RMå› ä¾èµ–tokençº§çº¿ç´¢æ³›åŒ–æ€§å·®ç­‰ç»“è®ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨UltraFeedbackæ•°æ®é›†è®­ç»ƒEX - RMä¸IM - RMï¼Œä½¿ç”¨å¤šç§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Gemma - 2 - 2B - ITã€Qwenç³»åˆ—ã€Llamaç³»åˆ—ç­‰ï¼‰ï¼Œè¯„ä¼°åˆ†å¸ƒå†…ï¼ˆUltraFeedbackæµ‹è¯•é›†ï¼‰ã€tokençº§åˆ†å¸ƒåç§»ï¼ˆå“åº”æ”¹å†™æˆ–ç¿»è¯‘ç­‰å˜ä½“ï¼‰ã€é¢†åŸŸåç§»ï¼ˆæ•°å­¦ä¸ä»£ç æ•°æ®é›†ï¼‰åœºæ™¯ã€‚ç»“æœæ˜¾ç¤ºï¼šIM - RMåœ¨tokençº§åˆ†å¸ƒåç§»ä¸‹é²æ£’æ€§å¼±äºEX - RMï¼Œä½†é¢†åŸŸåç§»ä¸‹è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ï¼›ä¸”IM - RMåœ¨åˆ†å¸ƒå†…æ³›åŒ–ä¹Ÿå¸¸å·®äºEX - RM ï¼Œå®éªŒç»“æœæ”¯æ’‘äº†IM - RMæ›´ä¾èµ–tokençº§çº¿ç´¢å¯¼è‡´æ³›åŒ–å·®å¼‚çš„ç»“è®ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡èšç„¦å¥–åŠ±æ¨¡å‹è®¾è®¡ä¸­â€œå¥–åŠ±è®¡ç®—æ–¹å¼â€è¿™ä¸€çœ‹ä¼¼å¾®å°çš„é€‰æ‹©ï¼Œå´å‘ç°å…¶å¯¹æ³›åŒ–è¡Œä¸ºå½±å“é‡å¤§ã€‚è¿™å¯ç¤ºç ”ç©¶è€…å…³æ³¨æ¨¡å‹è®¾è®¡ç»†èŠ‚ä¸­éšå«çš„åå·®ï¼Œåç»­å¯ä»ç†è§£ä¸åŒå¥–åŠ±æ¨¡å‹éšå¼åå·®å…¥æ‰‹ï¼Œé’ˆå¯¹æ€§å¢å¼ºå¥–åŠ±æ¨¡å‹é²æ£’æ€§ï¼›åŒæ—¶æœ¬æ–‡ç†è®ºåˆ†æä¸å®éªŒç»“åˆçš„ç ”ç©¶æ€è·¯ï¼Œä¹Ÿä¸ºæ¢ç©¶æ¨¡å‹å†…åœ¨æœºåˆ¶ç±»é—®é¢˜æä¾›äº†èŒƒä¾‹ï¼Œæœ‰åŠ©äºæ¨åŠ¨å¥–åŠ±æ¨¡å‹ä¹ƒè‡³æ›´å¹¿æ³›çš„è¯­è¨€æ¨¡å‹ç›¸å…³ç ”ç©¶æœç€æ›´å¯è§£é‡Šã€æ›´é²æ£’æ–¹å‘å‘å±•ã€‚

## bradley-terry-and-multi-objective-reward-modeling-are-complementary
### Abstract
Reward models trained on human preference data have demonstrated strong
effectiveness in aligning Large Language Models (LLMs) with human intent under
the framework of Reinforcement Learning from Human Feedback (RLHF). However,
RLHF remains vulnerable to reward hacking, where the policy exploits
imperfections in the reward function rather than genuinely learning the
intended behavior. Although significant efforts have been made to mitigate
reward hacking, they predominantly focus on and evaluate in-distribution
scenarios, where the training and testing data for the reward model share the
same distribution. In this paper, we empirically show that state-of-the-art
methods struggle in more challenging out-of-distribution (OOD) settings. We
further demonstrate that incorporating fine-grained multi-attribute scores
helps address this challenge. However, the limited availability of high-quality
data often leads to weak performance of multi-objective reward functions, which
can negatively impact overall performance and become the bottleneck. To address
this issue, we propose a unified reward modeling framework that jointly trains
Bradley--Terry (BT) single-objective and multi-objective regression-based
reward functions using a shared embedding space. We theoretically establish a
connection between the BT loss and the regression objective and highlight their
complementary benefits. Specifically, the regression task enhances the
single-objective reward function's ability to mitigate reward hacking in
challenging OOD settings, while BT-based training improves the scoring
capability of the multi-objective reward function, enabling a 7B model to
outperform a 70B baseline. Extensive experimental results demonstrate that our
framework significantly improves both the robustness and the scoring
performance of reward models.
### ğŸŒŸ è®ºæ–‡è§£è¯» |  Bradley - Terryä¸å¤šç›®æ ‡å¥–åŠ±å»ºæ¨¡ï¼šä¼˜åŠ¿äº’è¡¥åº”å¯¹RLHFå¥–åŠ±é»‘å®¢éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ¡†æ¶ä¸‹ï¼ŒåŸºäºäººç±»åå¥½æ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹èƒ½æœ‰æ•ˆè®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»æ„å›¾å¯¹é½ï¼Œä½†RLHFæ˜“å—â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜å›°æ‰°ï¼Œå³ç­–ç•¥åˆ©ç”¨å¥–åŠ±å‡½æ•°ç¼ºé™·è€ŒéçœŸæ­£å­¦ä¹ é¢„æœŸè¡Œä¸ºã€‚è¿‡å¾€ç¼“è§£è¯¥é—®é¢˜çš„å·¥ä½œå¤šèšç„¦åˆ†å¸ƒå†…åœºæ™¯ï¼ˆå¥–åŠ±æ¨¡å‹è®­ç»ƒä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒç›¸åŒï¼‰ï¼Œè€Œåœ¨æ›´å…·æŒ‘æˆ˜çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚åŒæ—¶ï¼Œå¼•å…¥ç»†ç²’åº¦å¤šå±æ€§åˆ†æ•°è™½æœ‰åŠ©äºåº”å¯¹æŒ‘æˆ˜ï¼Œä½†é«˜è´¨é‡æ•°æ®æœ‰é™å¯¼è‡´å¤šç›®æ ‡å¥–åŠ±å‡½æ•°æ€§èƒ½å¼±ï¼Œæˆä¸ºç“¶é¢ˆã€‚æ­¤å¤–ï¼Œç°æœ‰ç ”ç©¶åœ¨OODåœºæ™¯ä¸‹å¥–åŠ±æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å±€é™æ€§æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå¤šç›®æ ‡å¥–åŠ±æ¨¡å‹ï¼ˆMORMsï¼‰å› æ•°æ®é™åˆ¶æ€§èƒ½ä¸å¦‚å•ç›®æ ‡å¥–åŠ±æ¨¡å‹ï¼ˆSORMsï¼‰ï¼Œä¸”ä¸¤è€…ç‹¬ç«‹ä½¿ç”¨å­˜åœ¨è®¡ç®—æ˜‚è´µå’Œæ€§èƒ½ç“¶é¢ˆç­‰é—®é¢˜ï¼Œè¿™äº›éƒ½æ„æˆäº†æœ¬æ–‡ç ”ç©¶çš„åŠ¨æœºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºSOTAå¥–åŠ±æ¨¡å‹åœ¨OODåœºæ™¯çš„ç¼ºé™·  
é€šè¿‡å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨PPOå’ŒBest - of - Né‡‡æ ·æ—¶ï¼Œè‹¥æ‰€ç”¨promptæ¥è‡ªä¸è®­ç»ƒæ•°æ®ä¸åŒåˆ†å¸ƒçš„OODåœºæ™¯ï¼Œä¼šå‡ºç°å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œå‡¸æ˜¾äº†ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨OODåœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›çš„å…³é”®å±€é™ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSMORMç»Ÿä¸€å¥–åŠ±å»ºæ¨¡æ¡†æ¶  
æå‡ºè”åˆå•ç›®æ ‡å’Œå¤šç›®æ ‡å¥–åŠ±æ¨¡å‹ï¼ˆSMORMï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å…±äº«åµŒå…¥ç©ºé—´è”åˆè®­ç»ƒåŸºäºBradley - Terryï¼ˆBTï¼‰çš„å•ç›®æ ‡å¥–åŠ±å‡½æ•°å’ŒåŸºäºå¤šç›®æ ‡å›å½’çš„å¥–åŠ±å‡½æ•°ã€‚ä»ç†è®ºä¸Šå»ºç«‹äº†BTæŸå¤±å’Œå›å½’ç›®æ ‡ä¹‹é—´çš„è”ç³»ï¼Œå®ç°äº’è¡¥ä¼˜åŠ¿ï¼šå¤šç›®æ ‡å¤´è®­ç»ƒä¼˜åŒ–åµŒå…¥ç©ºé—´ï¼Œä½¿è¡¨ç¤ºèƒ½æ•æ‰å¤šå±æ€§è´¨é‡å·®å¼‚ï¼Œå¢å¼ºå•ç›®æ ‡å¤´åœ¨OODåœºæ™¯æŠ—å¥–åŠ±é»‘å®¢çš„é²æ£’æ€§ï¼›å•ç›®æ ‡å¤´è®­ç»ƒèƒ½ä¿®æ­£åµŒå…¥ç©ºé—´ä¸­å“åº”çš„å®šä½ï¼Œè®©å¤šç›®æ ‡å¤´åœ¨æ•°æ®æœ‰é™æ—¶ä¹Ÿèƒ½æœ‰ç«äº‰åŠ›è¡¨ç°ã€‚ä¸”SMORMè®­ç»ƒçµæ´»ï¼Œè®­ç»ƒä¸¤ä¸ªå¤´çš„prompt - responseå¯¹æ— éœ€å®Œå…¨ç›¸åŒï¼Œåªéœ€å•æ¬¡å‰å‘ä¼ æ’­å…±äº«éª¨å¹²ç½‘ç»œï¼Œè§£å†³äº†è®¡ç®—æ˜‚è´µé—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§å’Œè¯„åˆ†æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ç›¸åŒå¤šç›®æ ‡æ•°æ®é›†æ—¶ï¼ŒåŸºäºSMORMçš„7Bæ¨¡å‹èƒ½è¶…è¶Š70Bçš„åŸºçº¿æ¨¡å‹ï¼›åŒæ—¶åœ¨åº”å¯¹OODåœºæ™¯å¥–åŠ±é»‘å®¢é—®é¢˜ä¸Šï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ï¼ŒéªŒè¯äº†å¤šç›®æ ‡å¤´å¯¹å•ç›®æ ‡å¤´æ³›åŒ–èƒ½åŠ›çš„å¢å¼ºï¼Œä»¥åŠå•ç›®æ ‡å¤´å¯¹å¤šç›®æ ‡å¤´åœ¨æœ‰é™æ•°æ®ä¸‹æ€§èƒ½çš„æå‡ç­‰ç†è®ºåˆ†æç»“è®ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç ”ç©¶è§†è§’ä¸Šï¼Œå…³æ³¨åˆ°OODåœºæ™¯ä¸‹å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½é—®é¢˜ï¼Œå¡«è¡¥äº†è¿‡å¾€ç ”ç©¶åœ¨è¯¥åœºæ™¯ä¸‹çš„ç©ºç™½ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹åœ¨æ›´å¤æ‚åœºæ™¯çš„ç ”ç©¶æä¾›äº†æ–¹å‘å‚è€ƒã€‚
2. æ–¹æ³•åˆ›æ–°ä¸Šï¼ŒSMORMæ¡†æ¶ä¸ºç»“åˆå•ç›®æ ‡å’Œå¤šç›®æ ‡å¥–åŠ±å»ºæ¨¡æä¾›äº†æœ‰æ•ˆèŒƒå¼ï¼Œå…¶ç†è®ºå±‚é¢å»ºç«‹çš„BTæŸå¤±ä¸å›å½’ç›®æ ‡çš„è”ç³»ï¼Œä¸ºå¥–åŠ±æ¨¡å‹çš„è”åˆè®­ç»ƒæä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œåç»­åœ¨å¤„ç†å¤šç±»å‹å¥–åŠ±å‡½æ•°ç»“åˆã€æå‡æ¨¡å‹é²æ£’æ€§ç­‰æ–¹é¢å¯å€Ÿé‰´è¯¥è”åˆè®­ç»ƒæ€è·¯ã€‚
3. æ•°æ®åˆ©ç”¨ä¸Šï¼Œåœ¨é«˜è´¨é‡å¤šå±æ€§æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´è”åˆè®­ç»ƒæå‡å¤šç›®æ ‡å¥–åŠ±å‡½æ•°æ€§èƒ½çš„æ–¹å¼ï¼Œä¸ºè§£å†³æ•°æ®ç“¶é¢ˆé—®é¢˜æä¾›äº†åˆ›æ–°æ€è·¯ï¼Œå¯åº”ç”¨äºå…¶ä»–å› æ•°æ®é™åˆ¶å¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™çš„åœºæ™¯ã€‚

## perception-aware-policy-optimization-for-multimodal-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€æ¨ç†çš„æ„ŸçŸ¥æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼šPAPO å¦‚ä½•çªç ´è§†è§‰æ„ŸçŸ¥ç“¶é¢ˆï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çº¯æ–‡æœ¬é¢†åŸŸçš„æ¨ç†èƒ½åŠ›å·²é€šè¿‡å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¾—åˆ°æœ‰æ•ˆå¢å¼ºï¼Œä½†ç°æœ‰ RLVR è®¾è®¡å’Œä¼˜åŒ–ä¸»è¦é’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ã€‚å½“å‰å¤šæ¨¡æ€æ¨ç†çš„ä¸»è¦è¯¯å·®æ¥æºæ˜¯å¯¹è§†è§‰è¾“å…¥çš„æ„ŸçŸ¥é—®é¢˜ï¼šæ¨¡å‹è™½èƒ½å®Œæˆé€»è¾‘æˆ–ä»£æ•°æ¨ç†ï¼Œä½†å¸¸å› æ— æ³•å‡†ç¡®ç†è§£è§†è§‰è¾“å…¥ï¼ˆå¦‚ç©ºé—´å…³ç³»ã€æ ‡ç­¾å…³è”ï¼‰å¯¼è‡´é”™è¯¯ã€‚æ­¤å‰é’ˆå¯¹æ„ŸçŸ¥ä¼˜åŒ–çš„æ–¹æ³•è¦ä¹ˆä¾èµ–é¢å¤–å¥–åŠ±æ¨¡å‹æˆ–æ•°æ®å¤„ç†ï¼Œè¦ä¹ˆå°†æ„ŸçŸ¥ä¸æ¨ç†ç”Ÿç¡¬åˆ†ç¦»ï¼Œå­˜åœ¨è®¡ç®—å¼€é”€å¤§æˆ–é€‚é…æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡æ›´é€‚é…å¤šæ¨¡æ€é¢†åŸŸçš„ RLVR ç®—æ³•ï¼Œæˆä¸ºå…³é”®ç ”ç©¶é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º Perception - Aware Policy Optimizationï¼ˆPAPOï¼‰ç®—æ³•  
PAPO æ˜¯å¯¹ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰çš„ç®€æ´ä¸”é«˜æ•ˆçš„æ‰©å±•ï¼Œæ—¨åœ¨è®©æ¨¡å‹åœ¨å­¦ä¹ æ¨ç†çš„åŒæ—¶å­¦ä¹ æ„ŸçŸ¥ï¼Œä¸”å®Œå…¨ä¾èµ–å†…éƒ¨ç›‘ç£ä¿¡å·ï¼Œæ— éœ€é¢å¤–æ•°æ®æ•´ç†ã€å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹ã€‚æ ¸å¿ƒæ˜¯å¼•å…¥**éšå¼æ„ŸçŸ¥æŸå¤±ï¼ˆImplicit Perception Lossï¼‰**ï¼Œä»¥ KL æ•£åº¦å½¢å¼åŠ å…¥ GRPO ç›®æ ‡å‡½æ•°ã€‚é€šè¿‡å¯¹æ¯”æ¨¡å‹åœ¨åŸå§‹å›¾åƒå’Œâ€œæŸåå›¾åƒï¼ˆå¦‚éšæœºé®ç›–å›¾åƒå—çš„ç‰ˆæœ¬ï¼‰â€ä¸‹ç”Ÿæˆè¾“å‡ºçš„æ¦‚ç‡å·®å¼‚ï¼Œå¼•å¯¼æ¨¡å‹æ›´ä¾èµ–æœ‰æ•ˆè§†è§‰ä¿¡æ¯æ¨ç†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£å†³æŸå¤±é»‘å®¢ï¼ˆLoss Hackingï¼‰é—®é¢˜  
ç”±äº KL ç›®æ ‡çš„æ— ç•Œæ€§ï¼Œè‹¥éšå¼æ„ŸçŸ¥æŸå¤±ç³»æ•°è¿‡é«˜ï¼ŒPAPO å¯èƒ½è¿‡åº¦ä¼˜åŒ– KL é¡¹ï¼Œå¯¼è‡´å¥–åŠ±å´©æºƒã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æ·±å…¥åˆ†æè¯¥é—®é¢˜å¹¶æå‡º**åŒç†µæŸå¤±ï¼ˆDouble Entropy Lossï¼‰**æ¥ç¼“è§£ï¼Œä¿éšœè®­ç»ƒç¨³å®šæ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼šåœ¨ 8 ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šï¼ŒPAPO ç›¸æ¯” GRPO å¹³å‡æå‡ 4.4%ï¼›åœ¨è§†è§‰ä¾èµ–åº¦é«˜çš„ä»»åŠ¡ä¸­ï¼Œæå‡å¹…åº¦æ¥è¿‘ 8.0%ã€‚  
- æ„ŸçŸ¥è¯¯å·®å‡å°‘ï¼šäººå·¥åˆ†æ 200 ä¸ªé”™è¯¯æ¡ˆä¾‹æ˜¾ç¤ºï¼ŒPAPO ä½¿æ„ŸçŸ¥ç›¸å…³é”™è¯¯é™ä½ 30.5%ï¼Œè¯æ˜æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›æå‡ã€‚  
- æ”¶æ•›é€Ÿåº¦ï¼šPAPO æ—©æœŸï¼ˆçº¦ 25 æ­¥ï¼‰å°±å±•ç°å¢ç›Šï¼Œæ”¶æ•›æ›´å¿«ï¼›ç»“åˆç§»é™¤å‚è€ƒ KL ç­‰ç­–ç•¥ï¼Œåœ¨ 3B è§„æ¨¡æ¨¡å‹ä¸Šæœ€é«˜å¯æå‡ 11.2%ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€ä»»åŠ¡ä¼˜åŒ–æ€è·¯ï¼šé’ˆå¯¹å¤šæ¨¡æ€åœºæ™¯ç‰¹æœ‰é—®é¢˜ï¼ˆå¦‚è§†è§‰æ„ŸçŸ¥ç“¶é¢ˆï¼‰ï¼Œä»ç®—æ³•å±‚é¢èåˆæ„ŸçŸ¥ä¸æ¨ç†ç›‘ç£ï¼Œä¸ºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æä¾›æ–°èŒƒå¼ã€‚  
2. è½»é‡é«˜æ•ˆè®¾è®¡ï¼šä¸ä¾èµ–é¢å¤–å¤æ‚ç»„ä»¶ï¼ˆå¦‚å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼‰ï¼Œä»…é€šè¿‡ä¿®æ”¹ç›®æ ‡å‡½æ•°å¼•å…¥æ„ŸçŸ¥ç›‘ç£ï¼Œåœ¨å·¥ç¨‹å®ç°ä¸Šæ›´å…·å¯æ“ä½œæ€§ã€‚  
3. é—®é¢˜å‘ç°ä¸è§£å†³ï¼šé€šè¿‡ error analysis å®šä½æ ¸å¿ƒç—›ç‚¹ï¼ˆæ„ŸçŸ¥è¯¯å·®ä¸»å¯¼å¤šæ¨¡æ€æ¨ç†é”™è¯¯ï¼‰ï¼Œå†é’ˆå¯¹æ€§è®¾è®¡æŸå¤±å‡½æ•°ï¼›åŒæ—¶å‘ç°å¹¶ç¼“è§£â€œæŸå¤±é»‘å®¢â€è¿™ç±»æ–°é—®é¢˜ï¼Œä½“ç°äº†ä»é—®é¢˜è¯Šæ–­åˆ°æ–¹æ³•è¿­ä»£çš„å®Œæ•´ç ”ç©¶æ€è·¯ï¼Œä¸ºåç»­ç®—æ³•ä¼˜åŒ–æä¾›å‚è€ƒèŒƒå¼ã€‚

## reward-models-can-improve-themselves--reward-guided-adversarial-failure-mode-discovery-for-robust-reward-modeling
### Abstract
Reward modeling (RM), which captures human preferences to align large
language models (LLMs), is increasingly employed in tasks such as model
finetuning, response filtering, and ranking. However, due to the inherent
complexity of human preferences and the limited coverage of available datasets,
reward models often fail under distributional shifts or adversarial
perturbations. Existing approaches for identifying such failure modes typically
rely on prior knowledge about preference distributions or failure attributes,
limiting their practicality in real-world settings where such information is
unavailable. In this work, we propose a tractable, preference-distribution
agnostic method for discovering reward model failure modes via reward guided
controlled decoding. Building on this, we introduce REFORM, a self-improving
reward modeling framework that enhances robustness by using the reward model
itself to guide the generation of falsely scored responses. These adversarial
examples are then used to augment the training data and patch the reward
model's misaligned behavior. We evaluate REFORM on two widely used preference
datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate
that it significantly improves robustness without sacrificing reward quality.
Notably, REFORM preserves performance both in direct evaluation and in
downstream policy training, and further improves alignment quality by removing
spurious correlations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¥–åŠ±æ¨¡å‹ä¹Ÿèƒ½è‡ªæˆ‘æå‡ï¼ŸREFORMæ¡†æ¶è®©å¥–åŠ±å»ºæ¨¡æ›´é²æ£’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±å»ºæ¨¡ï¼ˆRMï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½äººç±»åå¥½æ–¹é¢è‡³å…³é‡è¦ï¼Œå¹¿æ³›ç”¨äºæ¨¡å‹å¾®è°ƒã€å“åº”è¿‡æ»¤å’Œæ’åºç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œäººç±»åå¥½çš„å¤æ‚æ€§ä¸æ•°æ®é›†è¦†ç›–æœ‰é™ï¼Œå¯¼è‡´å¥–åŠ±æ¨¡å‹åœ¨åˆ†å¸ƒåç§»æˆ–å¯¹æŠ—æ‰°åŠ¨ä¸‹æ˜“å¤±æ•ˆï¼Œå‡ºç°é”™è¯¯æ‰“åˆ†ï¼ˆå¦‚ç»™åå¥½å“åº”ä½å¥–åŠ±ã€ç»™éåå¥½å“åº”é«˜å¥–åŠ±ï¼‰ï¼Œå¼•å‘è¿‡åº¦ä¼˜åŒ–æˆ–å¥–åŠ±é»‘å®¢æ”»å‡»ç­‰é—®é¢˜ã€‚ç°æœ‰è¯†åˆ«å¤±æ•ˆæ¨¡å¼çš„æ–¹æ³•ä¾èµ–åå¥½åˆ†å¸ƒæˆ–å¤±æ•ˆå±æ€§çš„å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨æ— æ­¤ç±»ä¿¡æ¯çš„çœŸå®åœºæ™¯ä¸­å®ç”¨æ€§å—é™ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢**æ— éœ€åå¥½åˆ†å¸ƒå…ˆéªŒã€å¯å¤„ç†åœ°è¯†åˆ«å¥–åŠ±æ¨¡å‹å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ¨¡å¼è®©å¥–åŠ±æ¨¡å‹è‡ªæˆ‘æå‡é²æ£’æ€§**çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— åå¥½å…ˆéªŒçš„å¤±æ•ˆæ¨¡å¼å‘ç°  
æå‡ºåŸºäºå¥–åŠ±å¼•å¯¼çš„å¯æ§è§£ç æ¡†æ¶ï¼Œæ— éœ€åå¥½åˆ†å¸ƒå…ˆéªŒå°±èƒ½æŒ–æ˜å¥–åŠ±æ¨¡å‹çš„é”™è¯¯æ‰“åˆ†ç¤ºä¾‹ï¼ˆå³å¤±æ•ˆæ¨¡å¼ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œå¤±æ•ˆæ¨¡å¼éœ€æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼šä¸€æ˜¯å“åº”åœ¨åå¥½åˆ†å¸ƒä¸‹æœ‰æ˜ç¡®ç±»åˆ«ï¼ˆå¦‚æ˜ç¡®æ˜¯åå¥½æˆ–éåå¥½å“åº”ï¼‰ï¼›äºŒæ˜¯å¥–åŠ±æ¨¡å‹æ‰“åˆ†ä¸è¯¥ç±»åˆ«çŸ›ç›¾ï¼ˆå¦‚åå¥½å“åº”å¾—ä½åˆ†ã€éåå¥½å“åº”å¾—é«˜åˆ†ï¼‰ã€‚ä¸ºæ»¡è¶³æ¡ä»¶ï¼Œå…ˆåˆ©ç”¨ä¸ç›®æ ‡åå¥½åˆ†å¸ƒå¯¹é½/æœªå¯¹é½çš„ç­–ç•¥ç”Ÿæˆåæ˜ å·²çŸ¥åå¥½ç±»åˆ«çš„ç¤ºä¾‹ï¼Œå†é€šè¿‡å¥–åŠ±å¼•å¯¼çš„å¯æ§è§£ç ï¼Œç”Ÿæˆè¢«å¥–åŠ±æ¨¡å‹é”™è¯¯æ‰“åˆ†çš„å“åº”ï¼Œå¾—åˆ°â€œç±»åˆ«ä¸€è‡´ä½†å¥–åŠ±ä¸ä¸€è‡´â€çš„å¤±æ•ˆæ¨¡å¼ç¤ºä¾‹ï¼Œä¸”å®è¯è¡¨æ˜è¯¥æ–¹æ³•æ¯”æ— æ¨¡å‹å…ˆéªŒçš„åäº‹å®åŸºçº¿æ›´é«˜æ•ˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå¤±æ•ˆæ¨¡å¼çš„è‡ªæˆ‘æ”¹è¿›æ¡†æ¶REFORM  
åˆ©ç”¨å‘ç°çš„å¤±æ•ˆæ¨¡å¼å¢å¼ºå¥–åŠ±æ¨¡å‹é²æ£’æ€§ã€‚å…·ä½“æ˜¯æ„å»ºå¢å¼ºåå¥½æ•°æ®é›†ï¼šç­›é€‰åŸå§‹è®­ç»ƒé›†ä¸­5%æœ€å…·å½±å“åŠ›çš„prompt - å“åº”å¯¹ï¼Œç”Ÿæˆå…¶å¤±æ•ˆå˜ä½“ï¼ˆå¦‚åå¥½å“åº”å˜ä½“å¾—åˆ†ä½äºå¯¹åº”éåå¥½å“åº”ã€éåå¥½å“åº”å˜ä½“å¾—åˆ†é«˜äºå¯¹åº”åå¥½å“åº”ï¼‰ï¼Œç”¨è¿™äº›é’ˆå¯¹æ€§ä¿®æ­£çš„å¤±æ•ˆç¤ºä¾‹å¾®è°ƒå¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€å¤§è§„æ¨¡æ•°æ®å¢å¼ºæˆ–æ‰‹åŠ¨é‡æ–°æ ‡æ³¨ï¼Œå°±èƒ½å¤§å¹…æå‡é²æ£’æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„åå¥½æ•°æ®é›†Anthropic Helpful - Harmlessï¼ˆHHï¼‰å’ŒPKU Beavertailsä¸Šè¯„ä¼°REFORMï¼š  
- é²æ£’æ€§æ˜¾è‘—æå‡ï¼šèƒ½æœ‰æ•ˆåº”å¯¹åˆ†å¸ƒåç§»ç­‰æ‰°åŠ¨ï¼Œä¸”ä¸ç‰ºç‰²å¥–åŠ±è´¨é‡ï¼›  
- æ€§èƒ½ä¿ç•™ä¸æå‡ï¼šåœ¨ç›´æ¥è¯„ä¼°å’Œä¸‹æ¸¸ç­–ç•¥è®­ç»ƒï¼ˆå¦‚Best - of - Né‡‡æ ·ã€PPOã€DPOç­‰å¯¹é½æ–¹æ³•ï¼‰ä¸­ä¿ç•™æ€§èƒ½ï¼Œè¿˜é€šè¿‡æ¶ˆé™¤è™šå‡ç›¸å…³æ€§æå‡äº†å¯¹é½è´¨é‡ï¼Œåœ¨å¤šæ ·æ€§ã€å¯è¯»æ€§å’Œç”¨æˆ·æ•ˆç”¨ç­‰ç”Ÿæˆè´¨é‡ç»´åº¦è¡¨ç°è‰¯å¥½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ–¹æ³•æ™®é€‚æ€§ï¼šæå‡ºçš„å¤±æ•ˆæ¨¡å¼å‘ç°æ–¹æ³•ä¸ä¾èµ–åå¥½åˆ†å¸ƒå…ˆéªŒï¼Œä¸ºä¸åŒåœºæ™¯ä¸‹å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§åˆ†ææä¾›äº†é€šç”¨æ€è·¯ï¼›  
- è‡ªæˆ‘æ”¹è¿›èŒƒå¼ï¼šå±•ç¤ºäº†åˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„å¯¹æŠ—ç¤ºä¾‹ï¼ˆå¤±æ•ˆæ¨¡å¼ï¼‰è¿›è¡Œæ•°æ®å¢å¼ºæ¥æ”¹è¿›æ¨¡å‹çš„èŒƒå¼ï¼Œå¯å¯å‘å…¶ä»–éœ€é²æ£’æ€§æå‡çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼›  
- ä¸‹æ¸¸å½±å“è€ƒé‡ï¼šå…³æ³¨å¥–åŠ±æ¨¡å‹é²æ£’æ€§å¯¹ä¸‹æ¸¸å¯¹é½ä»»åŠ¡çš„å½±å“ï¼ŒéªŒè¯äº†æ”¹è¿›åæ¨¡å‹åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œä¸ºå®é™…è½åœ°æä¾›å‚è€ƒï¼Œé¿å…é²æ£’æ€§æå‡ä»¥ç‰ºç‰²ä¸‹æ¸¸æ•ˆæœä¸ºä»£ä»·ã€‚

## prompt-free-conditional-diffusion-for-multi-object-image-augmentation
### Abstract
Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— éœ€Promptï¼å¤šç›®æ ‡å›¾åƒå¢å¼ºçš„æ¡ä»¶æ‰©æ•£æ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹æ¨åŠ¨äº†æ•°æ®é›†å¢å¼ºçš„å‘å±•ï¼Œä½†ç”Ÿæˆå¤šç›®æ ‡çœŸå®åœºæ™¯å›¾åƒæ—¶ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šè¦ä¹ˆå®Œå…¨ä¾èµ–æ–‡æœ¬æ¡ä»¶ï¼Œå¯¼è‡´ç”Ÿæˆå¯¹è±¡ä¸åŸå§‹æ•°æ®åå·®ï¼›è¦ä¹ˆè¿‡åº¦ä¾èµ–åŸå§‹å›¾åƒï¼Œç”Ÿæˆå›¾åƒç¼ºä¹å¤šæ ·æ€§ï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡å¸®åŠ©æœ‰é™ã€‚åŒæ—¶ï¼Œç°æœ‰å¤šç›®æ ‡å›¾åƒç”Ÿæˆæ–¹æ³•è¿˜å­˜åœ¨ç”Ÿæˆéš¾åº¦å¤§ã€æ ‡ç­¾è´¨é‡å·®ã€é£æ ¼å°ºå¯¸åå·®ã€ä¿¡æ¯å¢é‡æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºæ— Promptçš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ç”¨äºå¤šç›®æ ‡å›¾åƒå¢å¼ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— Promptçš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ä¸å±€éƒ¨-å…¨å±€è¯­ä¹‰èåˆç­–ç•¥
æå‡ºæ— Promptçš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œå°†æ–‡æœ¬æ¡ä»¶æ”¹ä¸ºå±€éƒ¨-å…¨å±€è¯­ä¹‰èåˆç­–ç•¥ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ä»æ¡ä»¶å›¾åƒä¸­åˆ†ç¦»æå–æ•´ä¸ªå›¾åƒåŠå…¶å±€éƒ¨è£å‰ªå†…çš„è¯­ä¹‰çŸ¥è¯†ï¼Œæ›¿ä»£æ–‡æœ¬æå–å¤šç›®æ ‡ä¿¡æ¯å¹¶æ³¨å…¥æ‰©æ•£æ¨¡å‹ï¼Œå‡å°‘æ–‡æœ¬æè¿°å¸¦æ¥çš„ç±»åˆ«åå·®ï¼Œå®ç°ä»æ¡ä»¶å›¾åƒä¸­æ°å½“æå–å¤šç›®æ ‡ä¿¡æ¯ç”¨äºå›¾åƒç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå¥–åŠ±æ¨¡å‹çš„è®¡æ•°æŸå¤±è®¾è®¡
è®¾è®¡åŸºäºå¥–åŠ±æ¨¡å‹çš„è®¡æ•°æŸå¤±è¾…åŠ©æ¨¡å‹è®­ç»ƒã€‚ä¸åŒäºé€åƒç´ çº¦æŸï¼Œè¯¥æŸå¤±æ˜¾å¼é™åˆ¶ç”Ÿæˆå›¾åƒä¸­æ¯ä¸ªç±»åˆ«å¯¹è±¡çš„æ•°é‡ï¼Œä¸å¯¹ç©ºé—´å¸ƒå±€åšçº¦æŸï¼Œåœ¨å¼¥åˆç”Ÿæˆæ•°æ®ä¸åŸå§‹æ•°æ®æ•°é‡åå·®çš„åŒæ—¶ï¼Œæå‡ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ï¼Œè®©æ¨¡å‹èƒ½ç”ŸæˆåŒç±»åˆ«å¯¹è±¡æ•°é‡ç›¸å½“ç”šè‡³æ›´å¤šä¸”å¸ƒå±€ä¸åŒçš„é«˜è´¨é‡å›¾åƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªä»£è¡¨æ€§çš„å‰æ²¿åŸºçº¿æ–¹æ³•ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§ï¼Œåœ¨MS - COCOæ•°æ®é›†ä¸Šçš„å¤šç›®æ ‡å›¾åƒå¢å¼ºä»»åŠ¡ä¸­ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½è¾¾åˆ°äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ï¼ŒåŒæ—¶è¿˜å±•ç¤ºäº†å¼ºå¤§çš„ä¸‹æ¸¸ä»»åŠ¡å¢ç›Šå’ŒåŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¯­ä¹‰æå–ä¸æ³¨å…¥æ–¹å¼ï¼šå±€éƒ¨-å…¨å±€è¯­ä¹‰èåˆç­–ç•¥ä¸ºä»å›¾åƒä¸­æå–è¯­ä¹‰æ›¿ä»£æ–‡æœ¬æ¡ä»¶æä¾›äº†æ–°æ€è·¯ï¼Œåœ¨ä¸éœ€è¦æ–‡æœ¬æè¿°æ—¶ï¼Œå¯å€Ÿé‰´è¿™ç§ä»å›¾åƒè‡ªèº«æå–å¤šç»´åº¦è¯­ä¹‰ä¿¡æ¯çš„æ–¹å¼ç”¨äºç”Ÿæˆä»»åŠ¡ã€‚
2. æŸå¤±å‡½æ•°è®¾è®¡ï¼šåŸºäºå¥–åŠ±æ¨¡å‹çš„è®¡æ•°æŸå¤±è·³å‡ºé€åƒç´ çº¦æŸçš„ä¼ ç»Ÿæ€è·¯ï¼Œä»å¯¹è±¡æ•°é‡ç»´åº¦çº¦æŸç”Ÿæˆï¼Œä¸ºæå‡ç”Ÿæˆå¤šæ ·æ€§å’Œæ§åˆ¶ç”Ÿæˆæ•°æ®ä¸åŸå§‹æ•°æ®æ•°é‡å…³ç³»æä¾›äº†æ–°çš„æŸå¤±å‡½æ•°è®¾è®¡æ–¹å‘ï¼Œåœ¨éœ€è¦æ§åˆ¶ç”Ÿæˆå¯¹è±¡æ•°é‡æˆ–æå‡å¤šæ ·æ€§çš„ç”Ÿæˆä»»åŠ¡ä¸­å¯å‚è€ƒã€‚
3. æ¡†æ¶åˆ›æ–°ï¼šæ— Promptçš„æ¡ä»¶æ‰©æ•£æ¡†æ¶é’ˆå¯¹å¤šç›®æ ‡å›¾åƒå¢å¼ºåœºæ™¯ï¼Œä¸ºè§£å†³æ–‡æœ¬æ¡ä»¶å¸¦æ¥çš„åå·®é—®é¢˜æä¾›äº†åˆ›æ–°æ¶æ„ï¼Œåœ¨ç±»ä¼¼çš„ä¾èµ–ç”Ÿæˆæ¨¡å‹åšæ•°æ®å¢å¼ºä¸”å¯¹ç”Ÿæˆä¸åŸå§‹æ•°æ®åå·®æ•æ„Ÿçš„ä»»åŠ¡ä¸­ï¼Œè¿™ç§æ”¹å˜æ¡ä»¶è¾“å…¥æ–¹å¼çš„æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚
```

## enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts
### Abstract
Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ†å±‚æ£€ç´¢å¢å¼ºMCTSï¼Œè§£é”å¤§æ¨¡å‹æ¨ç†æ—¶ç¼©æ”¾æ–°å§¿åŠ¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æå‡ä¼ ç»Ÿä¸Šä¾èµ–è®­ç»ƒæ—¶å¤§è§„æ¨¡è®¡ç®—ï¼Œè€Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - Time Scalingï¼ŒTTSï¼‰ä½œä¸ºäº’è¡¥èŒƒå¼ï¼Œé€šè¿‡æ¨ç†æ—¶åˆ†é…é¢å¤–è®¡ç®—èµ„æºå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰åŸºäºæœç´¢çš„TTSæ–¹æ³•ï¼ˆå¦‚MCTSç»“åˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹PRMï¼‰å­˜åœ¨å±€é™ï¼šä¾èµ–é¢„è®­ç»ƒä¿¡æ¯æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜æˆ–æ¢ç´¢ç›²åŒºï¼Œä¸”ä»…é PRMè¯„ä¼°æ­¥éª¤éš¾ä»¥æ•æ‰å…¨å±€ç­–ç•¥å’Œè¯­ä¹‰å…³ç³»ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ç¨€ç–æˆ–æ¬¡ä¼˜ï¼Œå½±å“å¤æ‚æ¨ç†ä»»åŠ¡æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚å› æ­¤éœ€è¦æ›´æœ‰æ•ˆé€šç”¨çš„æ¨ç†ç¼©æ”¾æ–¹æ³•ï¼Œåœ¨æ— éœ€å¤§é‡é¢å¤–è®­ç»ƒä¸‹å¢å¼ºæ¨ç†èƒ½åŠ›å¹¶æå‡é²æ£’æ€§ä¸é€‚åº”æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒå±‚æ¬¡æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆç²—ç²’åº¦å±‚é¢ï¼‰
æå‡ºæ·±åº¦é€»è¾‘æ£€ç´¢ï¼ˆDeep Logical Retrievalï¼‰ï¼Œä»å¤æ‚æ¨ç†é—®é¢˜ä¸­æå–æŠ½è±¡æ¨¡æ¿ï¼Œæ£€ç´¢ç›¸ä¼¼é—®é¢˜ - ç­”æ¡ˆå¯¹ã€‚è¿™äº›ç›¸ä¼¼å¯¹ä¸ºæ¨¡å‹æä¾›å¤šæ ·ç¤ºä¾‹ï¼ŒåŠ©åŠ›æ¨¡å‹æ•æ‰é—®é¢˜ç»“æ„çš„æ½œåœ¨æ¨¡å¼ä¸å˜å¼‚æ€§ï¼Œè¿›è€Œæå‡ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœï¼Œå¢å¼ºå¯¹æœªè§è¿‡é—®é¢˜çš„é€‚åº”æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†å±‚å¢å¼ºæ¨ç†MCTSï¼ˆç»†ç²’åº¦å±‚é¢ï¼‰
åœ¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿‡ç¨‹ä¸­ï¼ŒR2 - LLMsä»å¤–éƒ¨æ•°å­¦é—®é¢˜æ•°æ®é›†åŠ¨æ€æ£€ç´¢ç›¸å…³ä¸­é—´è§£å†³æ­¥éª¤ï¼Œç”¨ç›¸ä¼¼å…ˆéªŒçŸ¥è¯†ä¸°å¯Œæ¨ç†è¿‡ç¨‹ã€‚ç»“åˆè¿™äº›æ£€ç´¢æ­¥éª¤åï¼ŒPRMèƒ½åšå‡ºæ›´å…·ä¿¡æ¯æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§çš„è¯„ä¼°ï¼Œé™ä½æ— æ•ˆæ¢ç´¢é£é™©ï¼ŒåŒæ—¶è¯¥æ–¹æ³•æ— ç¼æ•´åˆä¸Šä¸‹æ–‡çº§æ¨ç†å¢å¼ºä¸æ­¥éª¤çº§æ ‘æœç´¢æ–¹æ³•ï¼Œåˆ©ç”¨PRMä¼˜åŒ–å€™é€‰ç”Ÿæˆä¸å†³ç­–ä»¥æå‡æ¨ç†å‡†ç¡®æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MATH500ã€GSM8Kå’ŒOlympiadBench - TOæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨LLaMA - 3.1 - 8Bæ¨¡å‹æ—¶ï¼Œä¸åŸºçº¿ç›¸æ¯”R2 - LLMså®ç°äº†æ˜¾è‘—ç›¸å¯¹æå‡ï¼Œæå‡å¹…åº¦æœ€é«˜è¾¾16%ï¼›åœ¨LLaMA 3.1 - 8Bå’ŒQwen 2 - 7Bç­‰ç­–ç•¥æ¨¡å‹ä¸Šè¯„ä¼°ï¼Œä¹Ÿä¼˜äºåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’ŒåŸºäºæ ‘æœç´¢çš„åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†å±‚æ£€ç´¢å¢å¼ºæ€è·¯ï¼šå°†æ£€ç´¢åœ¨æ¨ç†æ—¶çš„ä½œç”¨åˆ†å±‚è®¾è®¡ï¼Œç²—ç²’åº¦æŠ“é—®é¢˜ç»“æ„æ¨¡å¼ã€ç»†ç²’åº¦è¡¥ä¸­é—´æ­¥éª¤çŸ¥è¯†ï¼Œä¸ºå¤šç²’åº¦åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†è¾…åŠ©æ¨ç†æä¾›äº†å‚è€ƒèŒƒå¼ã€‚
2. æ£€ç´¢ä¸MCTSç»“åˆï¼šæŠŠå¤–éƒ¨æ£€ç´¢å¼•å…¥MCTSè¿‡ç¨‹æ¥è¾…åŠ©PRMè¯„ä¼°ï¼Œä¸ºæ”¹è¿›åŸºäºæœç´¢çš„TTSæ–¹æ³•ä¸­å¥–åŠ±ä¿¡å·ä¸è¶³ã€æ¢ç´¢ä½æ•ˆç­‰é—®é¢˜æä¾›äº†åˆ›æ–°è§£æ³•ï¼Œåç»­å¯å€Ÿé‰´è¿™ç§å¤–éƒ¨çŸ¥è¯†èµ‹èƒ½æœç´¢è¿‡ç¨‹çš„æ€è·¯æ‹“å±•æ›´å¤šæ¨ç†åœºæ™¯ã€‚
3. æ— CoTè’¸é¦ä¾èµ–ï¼šæ— éœ€ä»æ›´å…ˆè¿›æ¨¡å‹è’¸é¦è·å–æ€ç»´é“¾è®­ç»ƒæ•°æ®ï¼Œé™ä½äº†æ–¹æ³•åº”ç”¨é—¨æ§›ï¼Œåœ¨èµ„æºæœ‰é™æˆ–éš¾è·å–é«˜çº§æ¨¡å‹è’¸é¦æ•°æ®æ—¶ï¼Œè¯¥è½»é‡ï¼ˆç›¸å¯¹ï¼‰å¢å¼ºæ¨ç†çš„æ–¹å¼å€¼å¾—å‚è€ƒã€‚

## arf-rlhf--adaptive-reward-following-for-rlhf-through-emotion-driven-self-supervision-and-trace-biased-dynamic-optimization
### Abstract
With the rapid advancement of Reinforcement Learning from Human Feedback
(RLHF) and autoregressive transformers, state-of-the-art models such as
GPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and
personalization. However, most existing RLHF approaches (e.g., PPO, DPO) still
rely on a binary-preference (BT) paradigm, which, while reducing annotation
costs, still requires substantial human effort and captures only group-level
tendencies rather than individual preferences. To overcome these limitations,
we propose Adaptive Reward-Following (ARF), a self-assessment framework that
leverages a high-precision emotion analyzer achieving over 70% accuracy on
GoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback
into continuous preference scores. We further enrich and debias these signals
through lightweight data augmentations, including synonym replacement, random
trace truncation, and score bias annotation algorithm. A Dynamic Adapter
Preference Tracker continuously models evolving user tastes in real time,
enabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly
on these tracked rewards instead of coarse binary labels. Experiments on
Qwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate
that ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,
TB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF
presents a scalable, personalized, and cost-effective approach to RLHF LLMs
through autonomous reward modeling.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ARF-RLHFï¼šç”¨æƒ…æ„Ÿé©±åŠ¨è‡ªç›‘ç£ä¸è½¨è¿¹åç½®åŠ¨æ€ä¼˜åŒ–é©æ–°RLHF

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œè‡ªå›å½’Transformerçš„å¿«é€Ÿå‘å±•ï¼ŒåƒGPT - 4.0ã€DeepSeek R1ã€Llama 3.3ç­‰å‰æ²¿æ¨¡å‹æ„ˆå‘é‡è§†å›ç­”æ·±åº¦ä¸ä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰å¤šæ•°RLHFæ–¹æ³•ï¼ˆå¦‚PPOã€DPOï¼‰ä¾èµ–äºŒå…ƒåå¥½ï¼ˆBTï¼‰èŒƒå¼ï¼Œè™½é™ä½äº†æ ‡æ³¨æˆæœ¬ï¼Œä½†ä»éœ€å¤§é‡äººåŠ›ï¼Œä¸”ä»…èƒ½æ•æ‰ç¾¤ä½“å±‚é¢è¶‹åŠ¿è€Œéä¸ªä½“åå¥½ï¼Œè¿˜å­˜åœ¨æ ‡æ³¨åå·®ã€æ›´æ–°æ»åç­‰é—®é¢˜ã€‚ä¸ºå…‹æœè¿™äº›å±€é™ï¼Œè®ºæ–‡æå‡ºARF - RLHFæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªé€‚åº”å¥–åŠ±è·Ÿéšï¼ˆARFï¼‰è¯„åˆ†å™¨
åŸºäºäººç±»äº¤æµä¸­éšå«æ»¡æ„åº¦ä¿¡å·è¿™ä¸€è§‚å¯Ÿï¼Œåˆ©ç”¨åœ¨GoEmotionsã€Sentiment140ã€DailyDialogç­‰æ•°æ®é›†ä¸Šå‡†ç¡®ç‡è¶…70%çš„é«˜ç²¾åº¦æƒ…æ„Ÿåˆ†æå™¨ï¼Œå°†è‡ªç”±å½¢å¼çš„ç”¨æˆ·åé¦ˆè½¬åŒ–ä¸ºè¿ç»­åå¥½åˆ†æ•°ã€‚è¯„åˆ†å™¨åŸºäºè½»é‡çš„RoBERTa - miniæ¶æ„æ„å»ºï¼Œå¹³è¡¡ä½å»¶è¿Ÿä¸å¼ºè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå®ç°ä»QAå¯¹çš„åŠ¨æ€äº¤äº’åˆ†æä¸­è‡ªåŠ¨è¿›è¡Œåå¥½è¯„åˆ†ï¼Œæ›¿ä»£ä¼ ç»ŸBT - RLHFçš„äºŒå…ƒæ¯”è¾ƒåˆ†æ•°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¢å¼ºå‹æ•°æ®å¤„ç†ä¸åŠ¨æ€åå¥½è·Ÿè¸ª
é€šè¿‡è½»é‡æ•°æ®å¢å¼ºï¼ˆåŒä¹‰è¯æ›¿æ¢ã€éšæœºè½¨è¿¹æˆªæ–­ã€åˆ†æ•°åå·®æ ‡æ³¨ç®—æ³•ï¼‰æ¥ä¸°å¯Œå’Œå»åä¿¡å·ï¼›å€ŸåŠ©å¸¦ç»éªŒå›æ”¾ï¼ˆERï¼‰æœºåˆ¶çš„åŠ¨æ€é€‚é…å™¨åå¥½è·Ÿè¸ªå™¨ï¼Œç»“åˆè½¯æ ‡ç­¾å­¦ä¹ å‘¨æœŸæ€§æ›´æ–°è¯„åˆ†å™¨ï¼Œå®æ—¶å»ºæ¨¡ç”¨æˆ·å˜åŒ–çš„åå¥½ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼Œä¸ºåç»­ä¼˜åŒ–æä¾›æ›´ä¼˜çš„å¥–åŠ±ä¿¡å·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šTrace Biasï¼ˆTBï¼‰å¾®è°ƒç®—æ³•
æ— éœ€ä¾èµ–BTå¯¹æ•°æ®ï¼ŒåŸºäºåŠ¨æ€è¯„åˆ†å™¨åé¦ˆã€éšæœºè·¯å¾„æˆªæ–­å’Œè·¯å¾„åå·®æ ¡æ­£æå‡ºæ–°é¢–ä¼˜åŒ–ç­–ç•¥ï¼Œç›´æ¥åœ¨è·Ÿè¸ªåˆ°çš„å¥–åŠ±ä¸Šä¼˜åŒ–ï¼Œè€Œéç²—ç³™çš„äºŒå…ƒæ ‡ç­¾ï¼Œä¸”åœ¨ç†è®ºä¸Šä¸PPOå’ŒDPOç›®æ ‡ä¿æŒä¸€è‡´ï¼Œå®ç°ç¨³å®šä¸”æœ‰ç†è®ºä¾æ®çš„å¾®è°ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Qwen - 2/2.5ã€Gemma - 2ã€Llama - 3.2ç­‰æ¨¡å‹åŠå››ä¸ªåå¥½é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒARFç›¸å¯¹PPOæå‡3.3%ï¼Œç›¸å¯¹DPOæå‡7.6%ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æƒ…æ„Ÿé©±åŠ¨çš„è‡ªç›‘ç£æ€è·¯ï¼šåˆ©ç”¨ç”¨æˆ·äº¤äº’ä¸­éšå«çš„æƒ…æ„Ÿã€æ»¡æ„åº¦ç­‰ä¿¡å·æ¥æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œä¸ºå‡å°‘äººå·¥æ ‡æ³¨ä¾èµ–æä¾›äº†æ–°æ–¹å‘ã€‚
2. è½»é‡æ•°æ®å¢å¼ºä¸åŠ¨æ€è·Ÿè¸ªç»“åˆï¼šé€šè¿‡ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ‰‹æ®µä¸°å¯Œæ•°æ®å¹¶å»åï¼Œæ­é…åŠ¨æ€è·Ÿè¸ªæœºåˆ¶é€‚åº”ç”¨æˆ·åå¥½å˜åŒ–ï¼Œåœ¨æ•°æ®å¤„ç†å’Œæ¨¡å‹é€‚åº”æ€§æå‡ä¸Šæœ‰å€Ÿé‰´ä»·å€¼ã€‚
3. æ— äºŒå…ƒæ ‡ç­¾ä¾èµ–çš„ä¼˜åŒ–ç®—æ³•ï¼šTrace Biasç®—æ³•è·³å‡ºä¼ ç»ŸRLHFä¾èµ–äºŒå…ƒæ¯”è¾ƒçš„å±€é™ï¼Œä¸ºLLMçš„RLHFä¼˜åŒ–å¼€è¾Ÿäº†æ›´è‡ªä¸»ã€å¯æ‰©å±•çš„è·¯å¾„ï¼Œåœ¨ç®—æ³•åˆ›æ–°å±‚é¢æä¾›äº†å‚è€ƒã€‚

## skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy
### Abstract
Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-Reward-V2ï¼šäººæœºååŒè§£é”åå¥½æ•°æ®è§„æ¨¡ä¸è´¨é‡æ–°é«˜åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­è‡³å…³é‡è¦ï¼Œä½†å½“å‰å¼€æºå¥–åŠ±æ¨¡å‹åœ¨å¤šæ•°è¯„ä¼°åŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ•æ‰å¤æ‚ç²¾å¦™çš„äººç±»åå¥½ã€‚ç©¶å…¶åŸå› ï¼Œç°æœ‰åå¥½æ•°æ®é›†å­˜åœ¨èŒƒå›´ç‹­çª„ã€æ ‡ç­¾åˆæˆæ€§å¼ºæˆ–è´¨é‡æ§åˆ¶ä¸ä¸¥è°¨ç­‰å±€é™ï¼Œå³ä¾¿é‡‡ç”¨å…ˆè¿›è®­ç»ƒæŠ€æœ¯ä¹Ÿéš¾æœ‰å®è´¨æ€§æå‡ã€‚å› æ­¤ï¼Œæå‡åå¥½æ•°æ®è´¨é‡ä»¥æ¨åŠ¨å¼€æºå¥–åŠ±æ¨¡å‹å‘å±•æˆä¸ºå…³é”®è¯‰æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºè¶…å¤§è§„æ¨¡åå¥½æ•°æ®é›†SynPref - 40M  
æ‰“é€ äº†åŒ…å«4000ä¸‡åå¥½å¯¹çš„å¤§è§„æ¨¡åå¥½æ•°æ®é›†SynPref - 40Mï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æ•°æ®åŸºç¡€ï¼Œè¿™ä¹Ÿæ˜¯ç›®å‰å·²çŸ¥è§„æ¨¡æœ€å¤§çš„ç²¾å¿ƒæ•´ç†åå¥½æ··åˆæ•°æ®é›†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äººæœºååŒä¸¤é˜¶æ®µæ•°æ®æ•´ç† pipeline  
ç¬¬ä¸€é˜¶æ®µå€ŸåŠ©ä¸¥æ ¼åè®®ä¸‹çš„äººå·¥éªŒè¯ä¿éšœæ•°æ®è´¨é‡ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨äººç±»åå¥½å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºâ€œè£åˆ¤â€å®ç°è§„æ¨¡åŒ–æ•´ç†ï¼ŒåŒæ—¶ç»“åˆå¥–åŠ±æ¨¡å‹çš„è¿­ä»£è®­ç»ƒï¼ŒæŒç»­çº³å…¥äººå·¥æ ‡ç­¾åé¦ˆå¹¶å¬å›æ¨¡å‹è¡¨ç°å·®çš„åå¥½æ•°æ®ä»¥ä¿ƒè¿›å­¦ä¹ ï¼Œæœ€ç»ˆå¾—åˆ°2600ä¸‡é«˜è´¨é‡åå¥½å¯¹ç”¨äºæ¨¡å‹è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨å‡ºSkywork - Reward - V2ç³»åˆ—å¥–åŠ±æ¨¡å‹  
åŸºäºSynPref - 40Mç­›é€‰å‡ºçš„åå¥½æ•°æ®ï¼Œè®­ç»ƒå‡ºåŒ…å«8ä¸ªä»0.6Båˆ°8Bå‚æ•°è§„æ¨¡çš„Skywork - Reward - V2ç³»åˆ—å¥–åŠ±æ¨¡å‹ï¼Œä»…ç”¨Bradley - Terryç›®æ ‡å‡½æ•°è®­ç»ƒå´èƒ½åœ¨å¤šåŸºå‡†å±•ç°å“è¶Šæ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨7ä¸ªä¸»è¦å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSkywork - Reward - V2ç³»åˆ—è¡¨ç°äº®çœ¼ï¼Œ8Bè§„æ¨¡æ¨¡å‹åœ¨æ‰€æœ‰7ä¸ªåŸºå‡†ä¸Šå¤§å¹…è¶…è¶Šç°æœ‰å¼€æºå¥–åŠ±æ¨¡å‹ï¼›åœ¨äººç±»åå¥½å¯¹é½ã€å®¢è§‚æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€æŠ—é£æ ¼åå·®ã€best - of - Nç¼©æ”¾ç­‰å¤šå…³é”®ç»´åº¦ä¹Ÿå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒSynPref - 40Mçš„æˆåŠŸæ—¢æºäºè§„æ¨¡ä¹Ÿå¾—ç›Šäºé«˜è´¨é‡ï¼ŒåŒæ—¶äººæœºååŒ pipeline ä¸­äººå·¥æ ‡æ³¨ã€äººç±»åå¥½å¼•å¯¼çš„LLMæ ‡æ³¨åŠä¸¥è°¨æ ‡æ³¨åè®®éƒ½è‡³å…³é‡è¦ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ•°æ®å±‚é¢ï¼Œå¤§è§„æ¨¡ä¸”é«˜è´¨é‡çš„åå¥½æ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½æå‡ä½œç”¨æ˜¾è‘—ï¼ŒSynPref - 40Mçš„æ„å»ºæ€è·¯ä¸ºæ•°æ®é©±åŠ¨çš„æ¨¡å‹ä¼˜åŒ–æä¾›èŒƒä¾‹ï¼›æ–¹æ³•å±‚é¢ï¼ŒäººæœºååŒçš„ä¸¤é˜¶æ®µæ•°æ®æ•´ç† pipeline æœ‰æ•ˆç»“åˆäººç±»æ ‡æ³¨è´¨é‡ä¸AIå¯æ‰©å±•æ€§ä¼˜åŠ¿ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®é«˜è´¨é‡æ•´ç†æä¾›äº†å¯å‚è€ƒçš„æµç¨‹æ¡†æ¶ï¼›æ¨¡å‹å±‚é¢ï¼ŒSkywork - Reward - V2ç³»åˆ—è¯æ˜åˆç†åˆ©ç”¨æ•°æ®ä¸è®­ç»ƒç­–ç•¥ï¼Œèƒ½åœ¨å¼€æºå¥–åŠ±æ¨¡å‹é¢†åŸŸå®ç°æ€§èƒ½çªç ´ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹ç ”å‘æŒ‡æ˜æ–¹å‘ï¼Œå‡¸æ˜¾äº†æŒ–æ˜ç°æœ‰åå¥½æ•°æ®æ½œåŠ›ä¸äººæœºååŒåœ¨æå‡æ•°æ®è´¨é‡ä¸Šçš„ä»·å€¼ã€‚

## safer--probing-safety-in-reward-models-with-sparse-autoencoder
### Abstract
Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}
### ğŸŒŸ è®ºæ–‡è§£è¯» | SAFERï¼šç”¨ç¨€ç–è‡ªç¼–ç å™¨é€è§†å¥–åŠ±æ¨¡å‹çš„å®‰å…¨å¥¥ç§˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨å‡¸æ˜¾äº†å®‰å…¨ä¸å¯é æ€§æ–¹é¢çš„å…³é”®æ‹…å¿§ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯è®©æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„ä¸»æµæ–¹æ³•ï¼Œè€Œå…¶æ ¸å¿ƒçš„å¥–åŠ±æ¨¡å‹å´åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¸é€æ˜ã€‚ä¸€æ–¹é¢ï¼Œå¥–åŠ±æ¨¡å‹ä»…è¾“å‡ºæ ‡é‡åˆ†æ•°ï¼Œæ©ç›–äº†èƒŒåçš„è¯­ä¹‰ç‰¹å¾ï¼Œé™ä½äº†é€æ˜åº¦ã€å¯é æ€§ä¸å®‰å…¨æ€§ï¼›å¦ä¸€æ–¹é¢ï¼Œå¥–åŠ±æ¨¡å‹å¯¹åå¥½æ•°æ®çš„æ ‡æ³¨ååˆ†æ•æ„Ÿï¼Œå¾®å°æ”¹åŠ¨å°±å¯èƒ½å¤§å¹…å½±å“æ€§èƒ½ï¼Œä½†å½“å‰æ£€æµ‹ã€è§£é‡Šå’Œä¿®æ­£æœ‰å™ªå£°æˆ–æœ‰é—®é¢˜æ ‡æ³¨çš„æ–¹æ³•è¿˜å¾ˆæœ‰é™ã€‚å› æ­¤ï¼Œç†è§£å¥–åŠ±æ¨¡å‹å†…éƒ¨æœºåˆ¶å’Œåå¥½æ•°æ®é›†è‡³å…³é‡è¦ï¼Œæœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³å¥–åŠ±æ¨¡å‹å¯è§£é‡Šæ€§ä»¥åŠåå¥½æ•°æ®å¯¹å…¶å½±å“çš„ç†è§£é—®é¢˜è€Œå±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰å®ç°å¥–åŠ±æ¨¡å‹çš„æœºåˆ¶æ€§è§£é‡Š  
åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨è¿™ä¸€æœºåˆ¶å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå°†å¥–åŠ±æ¨¡å‹çš„æ¿€æ´»åˆ†è§£ä¸ºç¨€ç–ä¸”å¯è§£é‡Šçš„ç‰¹å¾ï¼ŒæŒ–æ˜å‡ºé©±åŠ¨å¥–åŠ±é¢„æµ‹çš„æ˜ç¡®è¯­ä¹‰å› ç´ ï¼Œé€šè¿‡è¯†åˆ«ä¸å®‰å…¨ç›¸å…³çš„ç‰¹å¾å¤§å¹…æå‡äº†å¥–åŠ±æ¨¡å‹çš„é€æ˜åº¦ã€‚SAEsæ—¨åœ¨æŠŠè¯­è¨€æ¨¡å‹æ¿€æ´»è¡¨ç¤ºä¸ºè¿‡å®Œå¤‡åŸºå‘é‡çš„ç¨€ç–ç»„åˆï¼Œè®­ç»ƒæ—¶æœ€å°åŒ–é‡å»ºæŸå¤±å¹¶å¯¹æ½œåœ¨å‘é‡æ–½åŠ ç¨€ç–çº¦æŸï¼Œé‡‡ç”¨TopK SAEé€šè¿‡ä¿ç•™top Kæ¿€æ´»æ¥å¢å¼ºç¨€ç–æ€§æ§åˆ¶ä¸ç‰¹å¾å¯è§£é‡Šæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSAFERæ¡†æ¶å®ç°åå¥½æ•°æ®é›†çš„ç‰¹å¾çº§åˆ†æä¸å¢å¼º  
é’ˆå¯¹å®‰å…¨ç›¸å…³æ–¹é¢ï¼ˆå› å…¶å®é™…é‡è¦æ€§æ˜¾è‘—ï¼‰ï¼Œå…ˆåœ¨å®‰å…¨å¯¼å‘çš„åå¥½æ•°æ®é›†ï¼ˆSafeRLHFå’ŒWildGuardMixï¼‰ä¸Šè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå†åœ¨è¯¥å¥–åŠ±æ¨¡å‹çš„éšè—çŠ¶æ€æ¿€æ´»ä¸Šè®­ç»ƒSAEä»¥æå–ç¨€ç–å¯è§£é‡Šç‰¹å¾ï¼›é€šè¿‡é‡åŒ–é€‰ä¸­å’Œæ‹’ç»å“åº”é—´æ¿€æ´»å·®å¼‚æ¥ç¡®å®šç‰¹å¾æ˜¾è‘—æ€§ï¼Œè¿›è€Œå®ç°å¯¹å…³é”®å®‰å…¨ç›¸å…³ç‰¹å¾çš„éš”ç¦»ä¸è§£é‡Šã€‚åŸºäºè¿™äº›ç‰¹å¾çº§ä¿¡å·ï¼Œè®¾è®¡é’ˆå¯¹æ€§çš„æ•°æ®æŠ•æ¯’å’Œå»å™ªç­–ç•¥ï¼Œå›ç­”äº†åå¥½æ•°æ®å¯¹å¥–åŠ±æ¨¡å‹å½±å“çš„é—®é¢˜ï¼Œå®ç°äº†åŸºäºå®‰å…¨ç›¸å…³æ€§çš„é¶å‘æ•°æ®æ“ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°æ®æŠ•æ¯’å®éªŒä¸­ï¼Œåè½¬è¡¨ç°å‡ºæœ€å¤§å®‰å…¨ç›¸å…³ç‰¹å¾æ¿€æ´»å·®å¼‚çš„å­é›†å¯¹ï¼Œç»“æœæ˜¾ç¤ºæŠ•æ¯’æ“ä½œèƒ½åœ¨æå°æ•°æ®æ”¹åŠ¨ä¸‹æ˜¾è‘—é™ä½å®‰å…¨åˆ†æ•°ï¼Œä¸”å¯¹æ¨¡å‹é€šç”¨èƒ½åŠ›ï¼ˆå¦‚èŠå¤©èƒ½åŠ›ï¼‰å‡ ä¹æ— å½±å“ï¼›åœ¨æ•°æ®å»å™ªå®éªŒä¸­ï¼Œç§»é™¤ç‰¹å¾æ¿€æ´»å·®å¼‚æœ€å°çš„å¯¹ï¼Œå»å™ªæ–¹æ³•æå‡äº†å¥–åŠ±æ¨¡å‹åœ¨å®‰å…¨è¯„ä¼°ä¸Šçš„æ€§èƒ½ã€‚è¿™è¡¨æ˜SAFERèƒ½ç²¾å‡†åœ°é™ä½æˆ–å¢å¼ºå®‰å…¨å¯¹é½åº¦ï¼ŒåŒæ—¶ä¸ç‰ºç‰²é€šç”¨èŠå¤©æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢ï¼Œä¸ºå¥–åŠ±æ¨¡å‹çš„è§£é‡Šã€å®¡è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°é€”å¾„ï¼Œå€ŸåŠ©ç¨€ç–è‡ªç¼–ç å™¨å®ç°æœºåˆ¶æ€§è§£é‡Šçš„æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–æ¨¡å‹å¯è§£é‡Šæ€§ä»»åŠ¡ä¸­ï¼›ä»åº”ç”¨å±‚é¢ï¼Œé’ˆå¯¹åå¥½æ•°æ®é›†çš„ç‰¹å¾çº§æ¢æµ‹ç­–ç•¥ï¼Œä¸ºé«˜é£é™©LLMå¯¹é½ä»»åŠ¡ä¸­æ•°æ®çš„å¤„ç†ï¼ˆæŠ•æ¯’ã€å»å™ªç­‰ï¼‰æä¾›äº†é¶å‘æ“ä½œçš„èŒƒä¾‹ï¼Œèƒ½å¯å‘åç»­åœ¨æ•°æ®å±‚é¢ä¼˜åŒ–æ¨¡å‹å¯¹é½ä¸å®‰å…¨çš„å·¥ä½œï¼›ä»£ç å¼€æºä¹Ÿä¸ºç ”ç©¶è€…å¤ç°å’Œæ‹“å±•ç›¸å…³ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œæ¨åŠ¨è¯¥æ–¹å‘çš„å‘å±•ã€‚ 

## generalist-reward-models--found-inside-large-language-models
### Abstract
The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹ä¸­è—ç€é€šç”¨å¥–åŠ±æ¨¡å‹ï¼ŸLLMå¯¹é½æ–°èŒƒå¼æ¥äº†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»ä»·å€¼è§‚ï¼ˆå¦‚å¸®åŠ©æ€§ã€è¯šå®æ€§ï¼‰æ˜¯AIå‘å±•çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸»æµçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸¥é‡ä¾èµ–ç”¨æ˜‚è´µäººç±»åå¥½æ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ã€‚æ„å»ºä¼˜è´¨RMéœ€å¤§è§„æ¨¡é«˜è´¨é‡äººç±»åå¥½æ•°æ®é›†ï¼Œå­˜åœ¨æ…¢ã€è´µã€éš¾æ‰©å±•ç­‰é—®é¢˜ã€‚åç»­ç”¨AIåé¦ˆæ›¿ä»£äººç±»åé¦ˆçš„æ–¹æ³•ï¼ˆå¦‚RLAIFã€LLM-as-a-judgeï¼‰åˆç¼ºä¹ä¸¥è°¨ç†è®ºåŸºç¡€ï¼Œè¿˜æ˜“ç»§æ‰¿è£åˆ¤æ¨¡å‹çš„é£æ ¼åå·®ä¸åè§ã€‚é‚£ä¹ˆï¼Œé«˜è´¨é‡å¥–åŠ±ä¿¡å·æ˜¯å¦å¿…é¡»å¤–éƒ¨è·å–ï¼Ÿè¿™æˆä¸ºå…³é”®é—®é¢˜ï¼Œæœ¬æ–‡æ­£æ˜¯åŸºäºæ­¤å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°LLMä¸­ latent å­˜åœ¨é€šç”¨å¥–åŠ±æ¨¡å‹  
è®ºæ–‡å‘ç°ï¼Œä»»ä½•ç»æ ‡å‡†ä¸‹ä¸€ä¸ªtokené¢„æµ‹è®­ç»ƒçš„LLMä¸­ï¼Œå¤©ç„¶æ½œä¼ç€å¼ºå¤§çš„é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œå°†å…¶å‘½åä¸ºâ€œå†…ç”Ÿå¥–åŠ±ï¼ˆendogenous rewardï¼‰â€ã€‚æ— éœ€é¢å¤–è®­ç»ƒï¼Œå°±èƒ½ä»åŸºç¡€æ¨¡å‹ï¼ˆé¢„è®­ç»ƒæˆ–æœ‰ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼‰ä¸­ç›´æ¥æå–é«˜è´¨é‡å¥–åŠ±ä¿¡å·ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºå±‚é¢å»ºç«‹ä¸ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ çš„è”ç³»  
ä»ç†è®ºä¸Šè¯æ˜ï¼Œè¿™ç§å†…ç”Ÿå¥–åŠ±ç­‰ä»·äºé€šè¿‡ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰å­¦åˆ°çš„å¥–åŠ±å‡½æ•°ã€‚å…·ä½“è€Œè¨€ï¼ŒLLMçš„logitså¯ç›´æ¥è§£é‡Šä¸ºè½¯Qå‡½æ•°ï¼Œå€ŸåŠ©é€†è½¯Bellmanç®—å­èƒ½ä»ä¸­æ¢å¤å‡ºå¥–åŠ±å‡½æ•°ï¼Œä¸ºæå–å¥–åŠ±å‡½æ•°æä¾›äº†åŸç†æ€§æ–¹æ³•ï¼Œçªç ´äº†è¿‡å¾€å¯å‘å¼åšæ³•ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¯æ˜åŸºäºå†…ç”Ÿå¥–åŠ±çš„RLæœ‰æ•ˆæ€§  
è¯æ˜ç”¨è¯¥å†…ç”Ÿå¥–åŠ±è¿›è¡Œåç»­å¼ºåŒ–å­¦ä¹ åï¼Œå¾—åˆ°çš„ç­–ç•¥ç›¸æ¯”åŸºç¡€æ¨¡å‹æœ‰æ›´ä¼˜çš„è¯¯å·®ç•Œã€‚RLè¿‡ç¨‹èƒ½ä¿®æ­£æ ‡å‡†æ¨¡ä»¿å­¦ä¹ ï¼ˆä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼‰çš„å¤åˆè¯¯å·®ï¼ŒæŠŠæ€§èƒ½å·®è·ä»ä¸ä»»åŠ¡æ—¶é•¿ç›¸å…³çš„äºŒæ¬¡ä¾èµ–ï¼ˆO(HÂ²)ï¼‰é™åˆ°æ›´ä¼˜çš„çº¿æ€§ä¾èµ–ï¼ˆO(H)ï¼‰ã€‚è¿™æ˜¯é¦–æ¬¡ä»ç†è®ºä¸Šè¯æ˜LLMå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒéªŒè¯äº†ç†è®ºï¼šæå–å†…ç”Ÿå¥–åŠ±çš„æ–¹æ³•ä¸ä»…ä¼˜äºç°æœ‰LLM-as-a-judgeæ–¹æ³•ï¼Œè¿˜èƒ½è¶…è¶Šåœ¨æ˜‚è´µäººç±»æ ‡æ³¨æ•°æ®ä¸Šæ˜¾å¼è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡è¡¨æ˜å¥–åŠ±å»ºæ¨¡é˜¶æ®µå¯è¢«ä¸€ç§åŸç†æ€§æ–¹æ³•æ›¿ä»£â€”â€”æå–é¢„è®­ç»ƒé˜¶æ®µå·²æ•è·çš„çŸ¥è¯†ã€‚è¿™ä¸ºLLMå¯¹é½ä»¥åŠå¤šæ¨¡æ€æ¨¡å‹é¢†åŸŸï¼Œå¼€è¾Ÿäº†æ›´é«˜æ•ˆã€å¼ºå¤§ä¸”å¯æ‰©å±•çš„æ–°èŒƒå¼ï¼Œåç»­åœ¨æ¨¡å‹å¯¹é½ã€å¥–åŠ±å‡½æ•°è®¾è®¡ç­‰æ–¹å‘ï¼Œéƒ½å¯å€Ÿé‰´è¿™ç§â€œæŒ–æ˜æ¨¡å‹å†…åœ¨å·²æœ‰èƒ½åŠ›â€çš„æ€è·¯ï¼Œå‡å°‘å¯¹å¤–éƒ¨æ˜‚è´µæ•°æ®ä¸é¢å¤–è®­ç»ƒçš„ä¾èµ–ã€‚

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨çŸ¥è¯†å¢å¼ºæ ‘æœç´¢æ¨ç†åŠ©åŠ›å¤§æ¨¡å‹æ”»å…‹åˆ†å­ç»“æ„è§£æéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åˆ†å­ç»“æ„è§£ææ˜¯åŒ–å­¦å®éªŒåˆ†æé‡Œçš„å…³é”®ä»»åŠ¡ï¼Œè¦ä»æ ¸ç£ã€çº¢å¤–ç­‰å…‰è°±æ•°æ®æ¨å¯¼åˆ†å­ç»“æ„ï¼Œä¸“ä¸šäººå‘˜éƒ½å¾—èŠ±10 - 15åˆ†é’Ÿåˆ†æå•ä¸ªåˆ†å­ï¼Œæ‰€ä»¥ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–è§£æå¾ˆæœ‰å¿…è¦ã€‚ä½†LLMåœ¨è¿™ä»»åŠ¡ä¸Šæœ‰æŒ‘æˆ˜ï¼šä¸€æ˜¯å¯¹åŒ–å­¦åˆ†å­ç»“æ„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œåƒå™»å©è¿™ç±»ç‰¹æ®Šæ‚ç¯ç»“æ„ï¼ŒLLMå¸¸å› ç¼ºä¹å­ç»“æ„çŸ¥è¯†è¯¯åˆ¤ï¼›äºŒæ˜¯æ²¡æ³•å‡†ç¡®è¯„ä¼°å’Œä¿®æ­£æ¨ç†è¿‡ç¨‹ï¼Œæ ‘æœç´¢æ¨ç†éœ€è¦æœ‰æ•ˆè¯„ä¼°åé¦ˆï¼Œå¯LLMç¼ºé¢†åŸŸçŸ¥è¯†ï¼Œåšä¸å¥½ reward model è§’è‰²ã€‚äºæ˜¯è®ºæ–‡è¦è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæå‡LLMåœ¨åˆ†å­ç»“æ„è§£æçš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºK - MSEæ¡†æ¶  
æ„å»ºçŸ¥è¯†å¢å¼ºçš„åˆ†å­ç»“æ„è§£ææ¨ç†æ¡†æ¶K - MSEï¼ŒæŠŠè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºæ’ä»¶å®ç°æµ‹è¯•æ—¶çš„èƒ½åŠ›æ‰©å±•ï¼Œèƒ½é€‚é…ä»»æ„LLMã€‚å€ŸåŠ©MCTSå¹³è¡¡æ–°è§£æ¢ç´¢å’Œå·²æœ‰è§£åˆ©ç”¨ï¼Œè¿˜ç»“åˆSelf - Refineè®©LLMåŠæ—¶ä¼˜åŒ–ä¹‹å‰çš„è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“  
ä¸ºå¼¥è¡¥LLMåŒ–å­¦ç»“æ„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œæ„å»ºå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“ã€‚å­ç»“æ„æ˜¯åŒ–å­¦ç©ºé—´åŸºç¡€å…ƒç´ ï¼ŒçŸ¥è¯†åº“é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ•´åˆå­ç»“æ„å’Œç»“æ„æè¿°ï¼Œç»™LLMè¡¥å……é¢†åŸŸçŸ¥è¯†ï¼Œæå‡ç‰¹æ®Šç»“æ„æ¨ç†å‡†ç¡®æ€§ï¼Œå‡å°‘ atypical æ¡ˆä¾‹é”™è¯¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸“å±åˆ†å­ - å…‰è°±è¯„åˆ†å™¨  
è®¾è®¡åˆ†å­ - å…‰è°±è¯„åˆ†å™¨å½“ reward modelï¼Œè§£å†³LLMè§£è¯„ä¼°ä¸å‡†é—®é¢˜ã€‚è¯„åˆ†å™¨æœ‰åˆ†å­ç¼–ç å™¨å’Œå…‰è°±ç¼–ç å™¨ï¼Œè¯„ä¼°åˆ†å­ç»“æ„å’Œå…‰è°±æ•°æ®åŒ¹é…åº¦ç»™å¥–åŠ±åˆ†ã€‚å®ƒè¿˜ä½œä¸ºLLMå’ŒçŸ¥è¯†åº“é—´çš„æ£€ç´¢å™¨ï¼Œç”¨è¾“å…¥å…‰è°±æŸ¥æœ€ç›¸å…³å­ç»“æ„ï¼Œå‡å°‘å­ç»“æ„æ£€ç´¢è¯¯å·®ï¼Œå¢å¼ºæ¨ç†ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MolPuzzleåŸºå‡†æµ‹è¯•ä¸Šï¼ŒK - MSEæ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œå¯¹GPT - 4o - miniå’ŒGPT - 4oéƒ½å¸¦æ¥è¶…20%çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¡†æ¶åœ¨å¢å¼ºLLMåˆ†å­ç»“æ„è§£æèƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸçŸ¥è¯†å¢å¼ºæ€è·¯ï¼šé¢å¯¹ä¸“ä¸šé¢†åŸŸä»»åŠ¡ï¼ŒLLMé€šç”¨çŸ¥è¯†ä¸è¶³æ—¶ï¼Œæ„å»ºé¢†åŸŸå­ç»“æ„çŸ¥è¯†åº“è¡¥å……ï¼Œè¿™ç§â€œå¤–éƒ¨çŸ¥è¯† + LLMâ€æ¨¡å¼å¯å¤ç”¨åœ¨å…¶ä»–ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©ã€ææ–™ï¼‰ä»»åŠ¡ã€‚  
2. æ¨ç†è¿‡ç¨‹è¯„ä¼°ä¼˜åŒ–ï¼šè®¾è®¡é¢†åŸŸä¸“å±è¯„åˆ†å™¨åš reward modelï¼Œç»“åˆæ ‘æœç´¢æ¡†æ¶ä¼˜åŒ–æ¨ç†ï¼Œä¸ºéœ€è¦æ·±åº¦æ¨ç†ã€éœ€è¯„ä¼°åé¦ˆçš„å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è¯æ˜ã€ä»£ç è°ƒè¯•ï¼‰æä¾›äº†â€œè¯„åˆ†å™¨ + æ ‘æœç´¢â€çš„æ¨ç†å¢å¼ºèŒƒå¼ã€‚  
3. æ’ä»¶åŒ–æ¡†æ¶è®¾è®¡ï¼šK - MSEä½œä¸ºæ’ä»¶é€‚é…ä»»æ„LLMï¼Œè¿™ç§è§£è€¦å¼è®¾è®¡æ–¹ä¾¿æŠ€æœ¯è½åœ°ï¼Œä¸åŒåœºæ™¯ä¸‹å¯å¿«é€Ÿé›†æˆåˆ°ç°æœ‰LLMå·¥ä½œæµé‡Œï¼Œé™ä½æŠ€æœ¯è¿ç§»æˆæœ¬ã€‚

## listener-rewarded-thinking-in-vlms-for-image-preferences
### Abstract
Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨Listenerå¢å¼ºçš„RLï¼Œè®©è§†è§‰è¯­è¨€æ¨¡å‹æ›´æ‡‚äººç±»å›¾åƒåå¥½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç”Ÿæˆå¼å»ºæ¨¡é¢†åŸŸï¼Œè®©è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç²¾å‡†æ•æ‰äººç±»è§†è§‰åå¥½æ˜¯å…³é”®éš¾é¢˜ã€‚ç°æœ‰å¥–åŠ±æ¨¡å‹å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜“å¯¼è‡´æ¨¡å‹è®°å¿†è®­ç»ƒæ•°æ®ï¼Œè¿˜éœ€è¦å¤æ‚æ ‡æ³¨æµç¨‹ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é‡Œçš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰è™½èƒ½æå‡æ³›åŒ–æ€§ï¼Œä½†ç ”ç©¶å‘ç°åŸºäºRLçš„åå¥½æ¨ç†æ¨¡å‹å­˜åœ¨â€œå¬ä¼—åˆ†æ­§ï¼ˆlistener disagreementï¼‰â€é—®é¢˜ï¼šå½“æ¨¡å‹æ¨ç†è½¨è¿¹å’Œç‹¬ç«‹å†»ç»“çš„è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆâ€œlistenerâ€ï¼‰å¯¹åŒä¸€è¾“å‡ºçš„è¯„ä¼°çŸ›ç›¾æ—¶ï¼Œæ¨ç†å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è§£å†³è¿™ç§æ¨ç†çŸ›ç›¾ã€æå‡æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½åº¦æ˜¯æœ¬æ–‡åŠ¨æœºæ‰€åœ¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡è®­ç»ƒæ€ç»´é“¾é£æ ¼æ¨ç†æ¨¡å‹é¢„æµ‹äººç±»å¯¹ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„è§†è§‰åå¥½
ä»¥å¾€å·¥ä½œè¾ƒå°‘é’ˆå¯¹ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„äººç±»è§†è§‰åå¥½æ¥è®­ç»ƒè¿™ç§æ€ç»´é“¾æ¨ç†æ¨¡å‹ï¼Œæœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œä¸ºè§†è§‰åå¥½é¢„æµ‹æä¾›æ–°çš„æ¨¡å‹è®­ç»ƒæ€è·¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯†åˆ«å¹¶é‡åŒ–â€œå¬ä¼—åˆ†æ­§â€è¿™ä¸€RLè§†è§‰åå¥½å»ºæ¨¡çš„ä¸»è¦å¤±æ•ˆæ¨¡å¼
é€šè¿‡åˆ†æå‘ç°ï¼Œå½“æ¨¡å‹é¢„æµ‹å’Œç‹¬ç«‹â€œlistenerâ€é¢„æµ‹å·®å¼‚å¢å¤§æ—¶ï¼ŒVLMå‡†ç¡®ç‡æŒç»­ä¸‹é™ï¼Œå°†è¿™ç§ç°è±¡æ˜ç¡®ä¸ºå…³é”®é—®é¢˜å¹¶é‡åŒ–ï¼Œä¸ºåç»­è§£å†³æ–¹æ³•æä¾›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡é¢å‘GRPOçš„listener - shapedè½¯å¥–åŠ±æœºåˆ¶
å¼•å…¥å†»ç»“çš„VLMâ€œlistenerâ€ï¼Œè®©å…¶ç‹¬ç«‹é‡æ–°å¤„ç†æ¨ç†æ¨¡å‹ï¼ˆreasonerï¼‰çš„æ€ç»´é“¾ï¼ˆæ’é™¤æœ€ç»ˆç­”æ¡ˆtokenï¼‰ï¼Œè¾“å‡ºå¯¹æ­£ç¡®é€‰æ‹©çš„æ ¡å‡†ç½®ä¿¡åˆ†æ•°ï¼Œå¹¶æ•´åˆåˆ°RLå¥–åŠ±ä¿¡å·ä¸­ã€‚è¿™æ ·æ—¢æƒ©ç½šæ— æ³•è¯´æœç‹¬ç«‹æ¨¡å‹çš„è§£é‡Šï¼Œåˆæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨å°±èƒ½æä¾›å¯†é›†ã€æ•°æ®é«˜æ•ˆçš„ç›‘ç£ï¼Œè®©æ¨ç†æ¨¡å‹ä¸ä»…ç­”æ¡ˆæ­£ç¡®ï¼Œæ¨ç†è¿‡ç¨‹ä¹Ÿèƒ½è¢«â€œlistenerâ€è®¤å¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ImageRewardæµ‹è¯•é›†ä¸Šï¼Œæ–¹æ³•è¾¾åˆ°67.4%çš„å½“å‰æœ€ä¼˜å‡†ç¡®ç‡ï¼›åœ¨å¤§è§„æ¨¡ï¼ˆ120ä¸‡æŠ•ç¥¨ï¼‰çš„Rapidata - HSPåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¤§å¹…è¶…è¶Šå¼ºGRPOå’ŒSFTåŸºçº¿ï¼›åŒæ—¶å‡å°‘äº†æ¨ç†çŸ›ç›¾æƒ…å†µï¼Œä¸”åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå³ä¾¿ç”¨å°‘é‡åå¥½æ•°æ®è®­ç»ƒï¼Œä¹Ÿèƒ½è®©è¾“å‡ºæ›´æ ¡å‡†ã€OODé²æ£’æ€§æ›´å¼ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼Œåˆ©ç”¨ç‹¬ç«‹æ¨¡å‹æ„å»ºå¥–åŠ±æœºåˆ¶æ¥å¯¹é½æ¨ç†è½¨è¿¹å’Œæœ€ç»ˆå†³ç­–çš„æ€è·¯ï¼Œä¸ºè§£å†³æ¨¡å‹æ¨ç†ä¸€è‡´æ€§é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼›ä»åº”ç”¨è§’åº¦ï¼Œè¯æ˜listenerå¢å¼ºçš„RLæ˜¯VLMsä¸­åå¥½å¯¹é½çš„æœ‰æ•ˆå®ç”¨å·¥å…·ï¼Œä¸ºä¸‹ä¸€ä»£æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„åå¥½å¯¹é½è§£å†³æ–¹æ¡ˆï¼Œåœ¨å·¥ä¸šç•Œå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹åå¥½è°ƒä¼˜åœºæ™¯æœ‰å€Ÿé‰´ä»·å€¼ï¼›ä»é—®é¢˜å‘ç°è§’åº¦ï¼Œå¯¹â€œå¬ä¼—åˆ†æ­§â€è¿™ç§å¤±æ•ˆæ¨¡å¼çš„è¯†åˆ«å’Œé‡åŒ–ï¼Œè®©åç»­ç ”ç©¶è€…èƒ½æ›´å…³æ³¨æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents
### Abstract
As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-RewardBenchï¼šé¢å‘çœŸå®ä¸–ç•Œå¤šæ¨¡æ€æ™ºèƒ½ä½“çš„å¥–åŠ±å»ºæ¨¡ç»Ÿä¸€åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨ç½‘é¡µå¯¼èˆªã€å…·èº«æ™ºèƒ½ç­‰çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ä½†è¿™äº›æ™ºèƒ½ä½“å› ç¼ºä¹å¤–éƒ¨åé¦ˆï¼Œåœ¨è‡ªæˆ‘ä¿®æ­£ä¸æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ã€‚åˆ©ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºå¤–éƒ¨åé¦ˆæ˜¯æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œç„¶è€Œç›®å‰ç¼ºä¹é’ˆå¯¹æ™ºèƒ½ä½“å¥–åŠ±æ¨¡å‹é€‰æ‹©çš„æ˜ç¡®æŒ‡å¯¼ï¼Œä¹Ÿç¼ºå°‘ä¸“é—¨é¢å‘æ™ºèƒ½ä½“çš„å¥–åŠ±åŸºå‡†ã€‚å› æ­¤ï¼Œæ„å»ºé’ˆå¯¹æ™ºèƒ½ä½“çš„å¥–åŠ±åŸºå‡†è¿«åœ¨çœ‰ç«ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç»´åº¦ä¸çœŸå®åœºæ™¯è¦†ç›–  
æå‡ºçš„Agent - RewardBenchåŸºå‡†æ¶µç›–æ„ŸçŸ¥ã€è§„åˆ’ã€å®‰å…¨3ä¸ªè¯„ä¼°ç»´åº¦ä¸7ä¸ªçœŸå®ä¸–ç•Œæ™ºèƒ½ä½“åœºæ™¯ï¼ˆå¦‚ç§»åŠ¨ã€ç½‘é¡µã€è‡ªåŠ¨é©¾é©¶ã€Minecraftç­‰ï¼‰ã€‚åœ¨æ„ŸçŸ¥ç»´åº¦è¯„ä¼°è§†è§‰ç†è§£ä¸ grounding çš„å¥–åŠ±èƒ½åŠ›ï¼›è§„åˆ’ç»´åº¦èšç„¦å¥–åŠ±æ¨¡å‹å¯¹åºåˆ—å†³ç­–å’Œä»»åŠ¡åˆ†è§£çš„è¯„ä¼°èƒ½åŠ›ï¼›å®‰å…¨ç»´åº¦è€ƒå¯Ÿåœ¨æ”»å‡»å’Œä¸å®‰å…¨ç¯å¢ƒåœºæ™¯ä¸‹çš„å¥–åŠ±èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ­¥éª¤çº§å¥–åŠ±è¯„ä¼°  
ä¸åŒäºä»…å…³æ³¨æœ€ç»ˆç»“æœçš„è¯„ä¼°ï¼Œè¯¥åŸºå‡†æ”¯æŒåœ¨ä»»åŠ¡çš„å•ä¸ªæ­¥éª¤å±‚é¢è¯„ä¼°æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œä¸ºè§„åˆ’è¿‡ç¨‹ä¸­çš„æ€§èƒ½æä¾›æ›´ç»†è‡´çš„è§†è§’ï¼Œèƒ½æ›´è¯¦ç»†åœ°åé¦ˆæ¨¡å‹åœ¨æ¯ä¸€æ­¥çš„å¥–åŠ±èƒ½åŠ›è¡¨ç°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šéš¾åº¦é€‚é…ä¸é«˜è´¨é‡æ•°æ®ä¿éšœ  
é€šè¿‡ä¸‰ç§ç­–ç•¥ä¿éšœéš¾åº¦ä¸æ•°æ®è´¨é‡ï¼šä»10ç§ä¸åŒçš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆæ¶µç›–é»‘ç›’ä¸ç™½ç›’æ¨¡å‹ï¼‰é‡‡æ ·ä»¥ä¿è¯å¤šæ ·æ€§ï¼›ç”¨å°æ¨¡å‹è¿‡æ»¤æ•°æ®æ¥æ§åˆ¶ä»»åŠ¡éš¾åº¦ï¼Œé¿å…è¿‡æ˜“æˆ–è¿‡éš¾ï¼›äººå·¥éªŒè¯æ•°æ®ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§ä¸é«˜è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ç°æœ‰å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šè¡¨ç°æœ‰é™ï¼šé»‘ç›’æ¨¡å‹gemini - 1.5 - proå‡†ç¡®ç‡ä»…61.6%ï¼ŒGPT - 4oä¸º61.4%ï¼ŒClaude - 3.5 - Sonnetä¸º57.9%ï¼Œä½“ç°åŸºå‡†æŒ‘æˆ˜æ€§ï¼›åƒGPT - 4oè¿™æ ·è¾ƒå¼ºçš„æ¨¡å‹åœ¨å®‰å…¨å¥–åŠ±å»ºæ¨¡ä¸Šå‡†ç¡®ç‡ä»…39.2%ï¼Œè¯´æ˜æ™ºèƒ½ä½“é¢†åŸŸå®‰å…¨å¥–åŠ±å»ºæ¨¡ä»ä¸è¶³ï¼›å¼€æºæ¨¡å‹å¦‚Llama - 3.2 - 11B - Vision - Instructåœ¨æ„ŸçŸ¥å’Œè§„åˆ’ä¸Šåˆ†åˆ«ä»…å¾—53.5%å’Œ50.6%ï¼Œå‡¸æ˜¾æ™ºèƒ½ä½“å¥–åŠ±æ¨¡å‹ä¸“é¡¹è®­ç»ƒçš„å¿…è¦æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¡«è¡¥é¢†åŸŸç©ºç™½ï¼šAgent - RewardBenchæ˜¯é¦–ä¸ªè¯„ä¼°å¤šæ­¥éª¤æ™ºèƒ½ä½“ä»»åŠ¡ä¸­æ¨¡å‹å¥–åŠ±å»ºæ¨¡èƒ½åŠ›çš„åŸºå‡†ï¼Œä¸ºä»æ¨¡ä»¿å­¦ä¹ å‘å¸¦åé¦ˆå­¦ä¹ çš„è¿‡æ¸¡æä¾›å…³é”®è¯„ä¼°æ‰‹æ®µã€‚ 
2. ä¸°å¯Œè¯„ä¼°ç»´åº¦ä¸åœºæ™¯ï¼šæ¶µç›–å¤šç»´åº¦ã€å¤šçœŸå®åœºæ™¯ä¸”é‡‡ç”¨å¤šæ¨¡å‹çœŸå®æ ·æœ¬ä¸äººå·¥éªŒè¯ä¿éšœæ•°æ®è´¨é‡ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹è¯„ä¼°æä¾›äº†å…¨é¢ä¸”é«˜è´¨é‡çš„å‚è€ƒèŒƒå¼ã€‚ 
3. æŒ‡å¯¼ä¸‹æ¸¸ä»»åŠ¡ï¼šå±•ç°å‡ºä¸ä¸‹æ¸¸ä»»åŠ¡çš„å¼ºå…³è”æ€§ï¼Œè¯´æ˜å‡†ç¡®çš„å¥–åŠ±å»ºæ¨¡å¯¹æå‡å®é™…åº”ç”¨ä¸­æœç´¢æ€§èƒ½è‡³å…³é‡è¦ï¼Œä¸ºåç»­ä¼˜åŒ–æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ˜æ–¹å‘ã€‚

## off-policy-evaluation-and-learning-for-the-future-under-non-stationarity
### Abstract
We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.
### ğŸŒŸ è®ºæ–‡è§£è¯» | éå¹³ç¨³ç¯å¢ƒä¸‹é¢å‘æœªæ¥çš„ç¦»ç­–ç•¥è¯„ä¼°ä¸å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ¨èç³»ç»Ÿã€ç²¾å‡†åŒ»ç–—ç­‰å†³ç­–é—®é¢˜ä¸­ï¼Œç¦»ç­–ç•¥è¯„ä¼°ï¼ˆOPEï¼‰ç”¨äºåˆ©ç”¨å†å²æ—¥å¿—æ•°æ®è¯„ä¼°æ–°ç­–ç•¥æ•ˆæœï¼Œä½†å¤šæ•°å®é™…åœºæ™¯å¤„äº**éå¹³ç¨³ç¯å¢ƒ**ï¼ˆå¦‚ç”µå•†æ¨èä¸­ç”¨æˆ·åå¥½ã€å¥–åŠ±éšæ—¶é—´å˜åŒ–ï¼‰ã€‚ç°æœ‰æ–¹æ³•å‡è®¾ç¯å¢ƒå¹³ç¨³æˆ–ä¾èµ–ä¸¥æ ¼å¥–åŠ±å»ºæ¨¡å‡è®¾ï¼Œéš¾ä»¥å‡†ç¡®ä¼°è®¡æœªæ¥ç­–ç•¥ä»·å€¼â€”â€”å› ä¸ºå†å²æ•°æ®ä¸­æ²¡æœ‰æœªæ¥ç¯å¢ƒçš„è§‚æµ‹ï¼Œç›´æ¥å¥—ç”¨ä¼ ç»ŸOPEæ–¹æ³•ä¼šå¼•å…¥æ˜¾è‘—åå·®ã€‚ä¾‹å¦‚ç”µå•†æ¨èéœ€ç”¨ä¸Šä¸ªæœˆæ—§ç­–ç•¥æ•°æ®ä¼°è®¡ä¸‹ä¸ªæœˆæ–°ç­–ç•¥ä»·å€¼ï¼Œè€Œç”¨æˆ·è¡Œä¸ºçš„å‘¨åº¦ã€å­£èŠ‚ç­‰æ—¶é—´æ¨¡å¼åœ¨å†å²ä¸æœªæ¥å­˜åœ¨ä¸€è‡´æ€§å´æœªè¢«å……åˆ†åˆ©ç”¨ï¼Œè¿™æˆä¸ºå…³é”®ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰æœªæ¥ç¦»ç­–ç•¥è¯„ä¼°ï¼ˆF - OPEï¼‰é—®é¢˜å¹¶æå‡ºOPFV estimator  
é¦–æ¬¡å½¢å¼åŒ–**æœªæ¥ç¦»ç­–ç•¥è¯„ä¼°ï¼ˆF - OPEï¼‰**é—®é¢˜ï¼šåœ¨éå¹³ç¨³ç¯å¢ƒä¸‹ï¼Œä»…ç”¨å†å²æ—¥å¿—æ•°æ®ä¼°è®¡æœªæ¥ä»»æ„æ—¶é—´ç‚¹çš„ç­–ç•¥ä»·å€¼ã€‚æå‡º**Off - Policy Estimator for the Future Value (OPFV)**ï¼Œå…¶æ ¸å¿ƒæ˜¯åˆ©ç”¨æ—¶é—´åºåˆ—ä¸­ç¨³å®šçš„ç»“æ„ï¼ˆå¦‚å­£èŠ‚ã€å‘¨åº¦ã€èŠ‚å‡æ—¥æ•ˆåº”ï¼‰â€”â€”è¿™äº›æ•ˆåº”åœ¨å†å²å’Œæœªæ¥æ•°æ®ä¸­å…·æœ‰ä¸€è‡´æ€§ã€‚é€šè¿‡**æ–°å‹é‡è¦æ€§åŠ æƒ**ï¼ŒOPFVèƒ½æ— åä¼°è®¡æ—¶é—´åºåˆ—ç»“æ„åˆ»ç”»çš„æ•ˆåº”ï¼ˆç§°ä¸ºæ—¶é—´åºåˆ—ç‰¹å¾ï¼‰ï¼ŒåŒæ—¶ç”¨å›å½’æ¨¡å‹å¤„ç†å‰©ä½™æ•ˆåº”ä»¥å¹³è¡¡åå·®å’Œæ–¹å·®ã€‚ç†è®ºåˆ†æè¯æ˜äº†OPFVåœ¨ç‰¹å®šæ¡ä»¶ä¸‹å®ç°ä½åå·®ä¼°è®¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‰©å±•OPFVå®ç°æœªæ¥ç¦»ç­–ç•¥å­¦ä¹ ï¼ˆF - OPLï¼‰  
åŸºäºOPFV estimatorï¼Œè¿›ä¸€æ­¥æå‡º**æ–°çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•**ï¼Œä»…ç”¨å†å²æ•°æ®ä¸»åŠ¨å­¦ä¹ æœªæ¥çš„ä¼˜è´¨ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ‰©å±•OPFVä»¥ä¼°è®¡æœªæ¥ç­–ç•¥ä»·å€¼çš„ç­–ç•¥æ¢¯åº¦ï¼Œä»è€ŒæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œè§£å†³äº†éå¹³ç¨³ç¯å¢ƒä¸‹â€œç”¨å†å²æ•°æ®å­¦æœªæ¥ç­–ç•¥â€çš„éš¾é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é©±åŠ¨çš„æ—¶é—´åºåˆ—ç‰¹å¾é€‰æ‹©  
åˆ†ææ—¶é—´åºåˆ—ç‰¹å¾é€‰æ‹©å¯¹OPFVåå·® - æ–¹å·®æƒè¡¡çš„å½±å“ï¼Œæå‡º**ç®€å•çš„æ•°æ®é©±åŠ¨æµç¨‹**é€‰æ‹©æ—¶é—´åºåˆ—ç‰¹å¾ï¼Œä»¥ä¼˜åŒ–OPFVçš„ä¼°è®¡ç²¾åº¦ï¼Œè®©æ–¹æ³•æ›´å…·å®ç”¨æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åˆæˆæ•°æ®ä¸çœŸå®æ¨èç³»ç»Ÿéå¹³ç¨³æ•°æ®çš„å®éªŒä¸­ï¼ŒOPFVåœ¨**ä¼°è®¡æœªæ¥ç­–ç•¥ä»·å€¼**å’Œ**ä¼˜åŒ–æœªæ¥ç­–ç•¥**ä¸¤æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®éªŒè¦†ç›–å¤šç§éå¹³ç¨³åœºæ™¯ï¼ˆå¦‚åˆ†å¸ƒçš„å¹³æ»‘/çªå˜å˜åŒ–ï¼‰ï¼ŒéªŒè¯äº†OPFVåœ¨ä¸åŒè®¾å®šä¸‹çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰å±‚é¢ï¼šå°†â€œæ—¶é—´æˆ³â€ä½œä¸ºéšæœºå˜é‡çº³å…¥å»ºæ¨¡ï¼Œç»Ÿä¸€å¤„ç†å¹³æ»‘ä¸çªå˜çš„éå¹³ç¨³æ€§ï¼Œä¸ºéå¹³ç¨³ç¯å¢ƒä¸‹çš„ç­–ç•¥è¯„ä¼°ä¸å­¦ä¹ æä¾›äº†æ›´æ™®é€‚çš„æ¡†æ¶æ€è·¯ã€‚  
2. æ–¹æ³•è®¾è®¡å±‚é¢ï¼šæŒ–æ˜æ—¶é—´åºåˆ—ä¸­è·¨å†å² - æœªæ¥çš„ç¨³å®šç»“æ„ï¼ˆå¦‚å‘¨æœŸæ•ˆåº”ï¼‰ï¼Œå¹¶ç”¨æ–°å‹é‡è¦æ€§åŠ æƒå’Œå›å½’ç»“åˆçš„æ–¹å¼åˆ©ç”¨è¿™äº›ç»“æ„ï¼Œä¸ºâ€œæ— æœªæ¥è§‚æµ‹æ—¶ä¼°è®¡æœªæ¥ä»·å€¼â€æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯èŒƒå¼ã€‚  
3. è½åœ°å®è·µå±‚é¢ï¼šæ•°æ®é©±åŠ¨çš„ç‰¹å¾é€‰æ‹©æµç¨‹ä¸åŸºäºOPFVçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œä¸ºå·¥ä¸šç•Œï¼ˆå¦‚æ¨èç³»ç»Ÿã€å¹¿å‘ŠæŠ•æ”¾ï¼‰åœ¨éå¹³ç¨³åœºæ™¯ä¸‹å¿«é€Ÿè¿­ä»£ç­–ç•¥æä¾›äº†è½åœ°è·¯å¾„ï¼Œå‡å°‘åœ¨çº¿å®éªŒæˆæœ¬ä¸ä¼¦ç†é£é™©ã€‚

## ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations
### Abstract
Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´å±€éƒ¨æœ€ä¼˜ï¼Ctrl - Z Samplingï¼šå¯æ§éšæœºä¹‹å­—å½¢æ¢ç´¢çš„æ‰©æ•£é‡‡æ ·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ‰©æ•£æ¨¡å‹åœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ƒé€šè¿‡è¿­ä»£å»å™ªå°†é«˜æ–¯å™ªå£°é€æ­¥è½¬åŒ–ä¸ºç›®æ ‡æ•°æ®åˆ†å¸ƒã€‚ä½†ç”±äºæ½œåœ¨ç©ºé—´å¤æ‚å’Œåˆå§‹åŒ–æ¬ ä½³ï¼Œæ‰©æ•£æ¨¡å‹å¸¸æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜â€”â€”ç”Ÿæˆç»“æœå±€éƒ¨è§†è§‰è¿è´¯ï¼Œå´å…¨å±€ä¸ä¸€è‡´æˆ–æ¡ä»¶é”™ä½ã€‚ä»¥å¾€æ–¹æ³•å¦‚å¼ºåŒ–å¼•å¯¼ä¿¡å·ã€æ“çºµåˆå§‹å™ªå£°åˆ†å¸ƒç­‰ï¼Œåœ¨å¼•å¯¼æ§åˆ¶æˆ–é€ƒé€¸å±€éƒ¨æœ€ä¼˜çš„çµæ´»æ€§ä¸Šå­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆå¼•å¯¼æ•ˆæœä¸ä½³ï¼Œè¦ä¹ˆéœ€å¤§é‡å€™é€‰çŠ¶æ€è¯„ä¼°ï¼Œå®ç”¨æ€§å—é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½æœ‰æ•ˆæ£€æµ‹å¹¶é€ƒé€¸å±€éƒ¨æœ€ä¼˜çš„é‡‡æ ·ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå¯æ§éšæœºä¹‹å­—å½¢é‡‡æ ·ï¼ˆCtrl - Z Samplingï¼‰
è¯¥æ–¹æ³•å°†æ¡ä»¶æ‰©æ•£ç”Ÿæˆè§†ä¸ºâ€œçˆ¬å±±â€è¿‡ç¨‹ï¼ŒåŸºäºå¥–åŠ±æ¨¡å‹æ£€æµ‹æ½œåœ¨å±€éƒ¨æœ€ä¼˜ï¼ˆä¾æ®é¢„æµ‹åˆ†æ•°çš„åœæ»æƒ…å†µï¼‰ã€‚ä¸€æ—¦æ£€æµ‹åˆ°ï¼Œæ³¨å…¥å™ªå£°å¹¶å›é€€åˆ°æ›´æ—©ã€æ›´å…·å™ªå£°çš„æ—¶é—´æ­¥ï¼Œä»¥æ­¤é€ƒç¦»å½“å‰ä¼˜åŒ–å¹³å°ï¼Œé¼“åŠ±åœ¨æ½œåœ¨ç©ºé—´æ¢ç´¢æ›´ä¼˜åŒºåŸŸã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€å¹³è¡¡ backward exploration ä¸ forward refinement
å¥–åŠ±æ¨¡å‹è¯„ä¼°å€™é€‰çŠ¶æ€ï¼Œä»…æ¥å—èƒ½å¸¦æ¥æ›´ä¼˜æœªæ¥é¢„æµ‹çš„çŠ¶æ€ï¼›è‹¥é™„è¿‘æ— æ”¹è¿›ï¼Œä¼šç»§ç»­å›é€€åˆ°å™ªå£°æ›´å¤§çš„æ°´å¹³ä»¥å¢åŠ é€ƒé€¸å¯èƒ½æ€§ã€‚è¿™ç§å¯æ§éšæœºä¹‹å­—å½¢è¿‡ç¨‹å®ç°äº†å‰å‘ä¼˜åŒ–ä¸åå‘æ¢ç´¢çš„åŠ¨æ€äº¤æ›¿ï¼Œæå‡ç”Ÿæˆç»“æœçš„æ¡ä»¶å¯¹é½åº¦ä¸è§†è§‰è´¨é‡ï¼Œä¸”æ¨¡å‹æ— å…³ï¼Œé€‚é…ç°æœ‰æ‰©æ•£æ¡†æ¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCtrl - Z Sampling åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°ç”Ÿæˆè´¨é‡çš„æ˜¾è‘—æå‡ï¼Œä»…éœ€çº¦ 7.6 å€å‡½æ•°è¯„ä¼°ï¼ˆNFEsï¼‰å¢åŠ ã€‚è¿™è¡¨æ˜å…¶åœ¨æ¨ç†æ—¶èƒ½ä»¥é€‚åº¦å¼€é”€å¤§å¹…æ”¹è¿›ç”Ÿæˆæ•ˆæœï¼Œæ˜¯æ˜‚è´µæµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶é€‚ç”¨äºæœ¬åœ°æ¨ç†åœºæ™¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»â€œçˆ¬å±±â€è§†è§’åˆ†ææ‰©æ•£æ¨¡å‹æ¡ä»¶ç”Ÿæˆï¼Œä¸ºç†è§£æ¨¡å‹æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜æä¾›æ–°è§’åº¦ï¼Œå¯å‘åç»­å¯¹ç”Ÿæˆè¿‡ç¨‹åŠ¨åŠ›å­¦çš„ç ”ç©¶ã€‚
2. Ctrl - Z Sampling çš„è®¾è®¡æ€è·¯â€”â€”é€šè¿‡å¥–åŠ±å¼•å¯¼çš„éšæœºæ¢ç´¢ã€è‡ªé€‚åº”å™ªå£°æ³¨å…¥å®ç°å¯æ§é€ƒé€¸å±€éƒ¨æœ€ä¼˜ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–éœ€åœ¨å¤æ‚ç©ºé—´ä¸­ä¼˜åŒ–ã€æ˜“é™·å±€éƒ¨æœ€ä¼˜çš„ç”Ÿæˆæˆ–ä¼˜åŒ–ä»»åŠ¡ä¸­ï¼Œå¦‚å…¶ä»–ç±»å‹ç”Ÿæˆæ¨¡å‹ï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ç­‰ï¼‰çš„é‡‡æ ·ç­–ç•¥æ”¹è¿›ã€‚
3. æ¨¡å‹æ— å…³æ€§ä½¿å…¶èƒ½æ— ç¼èå…¥ç°æœ‰æ‰©æ•£æ¡†æ¶ï¼Œä¸ºå·¥ç¨‹å®è·µä¸­æå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡æä¾›å³æ’å³ç”¨çš„æ€è·¯ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œé™ä½åº”ç”¨æˆæœ¬ã€‚

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonFlux-PRMï¼šé¢å‘å¤§æ¨¡å‹é•¿æ€ç»´é“¾æ¨ç†çš„è½¨è¿¹æ„ŸçŸ¥å‹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ¨ç†åœºæ™¯ï¼ˆå¦‚æ•°å­¦è§£é¢˜ï¼‰ä¸­ï¼ŒProcess Reward Modelsï¼ˆPRMsï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰æ˜¯ç›‘ç£ä¸­é—´æ¨ç†æ­¥éª¤çš„æœ‰åŠ›å·¥å…·ã€‚ä¸è¿‡ç°æœ‰PRMså­˜åœ¨æ˜æ˜¾å±€é™ï¼šå®ƒä»¬ä¸»è¦åŸºäºæ¨¡å‹æœ€ç»ˆè¾“å‡ºè®­ç»ƒï¼Œéš¾ä»¥å¯¹**è½¨è¿¹ - å“åº”ï¼ˆtrajectory - responseï¼‰**è¿™ç±»æ–°å…´è¾“å‡ºå½¢å¼çš„ä¸­é—´æ¨ç†è½¨è¿¹è¿›è¡Œé²æ£’è¯„ä¼°ã€‚åƒDeepseek - R1ç­‰å‰æ²¿æ¨ç†æ¨¡å‹ä¼šç”Ÿæˆâ€œå†—é•¿ã€æ¬ è§„æ•´çš„ä¸­é—´æ€è€ƒè½¨è¿¹ + ç®€æ´æœ€ç»ˆå“åº”â€çš„è½¨è¿¹ - å“åº”å¯¹ï¼Œè¿™ç±»æ•°æ®å¸¸è¢«ç”¨äºå°æ¨¡å‹è’¸é¦ï¼Œä½†ç°æœ‰PRMså› ä¸ä¸­é—´è½¨è¿¹åœ¨ç»“æ„ã€æ ¼å¼ä¸Šä¸åŒ¹é…ï¼Œä¸”è®­ç»ƒæ—¶ç¼ºä¹å¸¦å¥–åŠ±çš„è½¨è¿¹ - å“åº”æ•°æ®ï¼Œåœ¨ç›‘ç£è¿™ç±»æ•°æ®æ—¶æ•ˆæœä¸ä½³ç”šè‡³ä¼šæŸå®³ä¸‹æ¸¸è®­ç»ƒã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®©PRMsæ—¢èƒ½ç›‘ç£æœ€ç»ˆå“åº”ï¼Œåˆèƒ½æœ‰æ•ˆè¯„ä¼°ä¸­é—´æ€è€ƒè½¨è¿¹ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè½¨è¿¹æ„ŸçŸ¥çš„PRMâ€”â€”ReasonFlux - PRM  
ReasonFlux - PRMä¸“ä¸ºè¯„ä¼°è½¨è¿¹ - å“åº”å‹æ¨ç†ç—•è¿¹è®¾è®¡ï¼Œèåˆäº†**æ­¥éª¤çº§ï¼ˆstep - levelï¼‰**å’Œ**è½¨è¿¹çº§ï¼ˆtrajectory - levelï¼‰**ç›‘ç£ã€‚å®ƒåœ¨æ¶µç›–æ•°å­¦å’Œç§‘å­¦æ¨ç†çš„10ké«˜è´¨é‡è½¨è¿¹ - å“åº”å¯¹ curated æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½ä¸ºæ€è€ƒè½¨è¿¹å†…çš„æ¯ä¸ªæ­¥éª¤æä¾›ç»†ç²’åº¦å¥–åŠ±ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®©æ¨¡å‹ä¸­é—´æ€è€ƒè½¨è¿¹ä¸æœ€ç»ˆå“åº”æ›´å¯¹é½ï¼Œè§£å†³äº†ç°æœ‰PRMså¯¹ä¸­é—´è½¨è¿¹ç›‘ç£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šåœºæ™¯é€‚é…çš„å¥–åŠ±ç›‘ç£  
ReasonFlux - PRMé€‚é…ç¦»çº¿å’Œåœ¨çº¿å¤šç§åœºæ™¯ï¼š  
- ç¦»çº¿åœºæ™¯ï¼šä¸ºè½¨è¿¹ - å“åº”å¯¹æ‰“åˆ†ï¼Œç­›é€‰é«˜è´¨é‡æ•°æ®ï¼ŒåŠ©åŠ›å°æ¨¡å‹ä¸‹æ¸¸æœ‰ç›‘ç£å¾®è°ƒçš„è®­ç»ƒæ•°æ®ç²¾é€‰ï¼›  
- åœ¨çº¿åœºæ™¯ï¼šèå…¥GRPOç­‰ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ç­–ç•¥ä¼˜åŒ–æä¾›ç»†ç²’åº¦è¿‡ç¨‹å¥–åŠ±ï¼›  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest - time scalingï¼‰ï¼šé€šè¿‡å¥–åŠ±å¼•å¯¼çš„Best - of - Nç­–ç•¥ï¼Œè¯„ä¼°å¤šä¸ªç”Ÿæˆå“åº”å¹¶é€‰æœ€ä¼˜ï¼Œæå‡æ¨ç†æ€§èƒ½ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AIMEã€MATH500ã€GPQA - Diamondç­‰æŒ‘æˆ˜æ€§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux - PRMå±•ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼š  
- æ•°æ®é€‰æ‹©æ–¹é¢ï¼šReasonFlux - PRM - 7Bæ¯”å¼ºåŸºçº¿ï¼ˆå¦‚Qwen2.5 - Math - PRM - 72Bï¼‰å’Œäººå·¥ç­–åˆ’åŸºçº¿é€‰å‡ºçš„æ•°æ®é›†è´¨é‡æ›´é«˜ï¼›  
- æ€§èƒ½æå‡æ–¹é¢ï¼šReasonFlux - PRM - 7Båœ¨æœ‰ç›‘ç£å¾®è°ƒä¸­å¹³å‡æå‡12.1%ï¼Œå¼ºåŒ–å­¦ä¹ ä¸­å¹³å‡æå‡4.5%ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ä¸­å¹³å‡æå‡6.3%ï¼›  
- èµ„æºå‹å¥½å‹å‘å¸ƒï¼šè¿˜å‘å¸ƒäº†ReasonFlux - PRM - 1.5Bï¼Œé€‚é…èµ„æºå—é™åœºæ™¯ä¸è¾¹ç¼˜éƒ¨ç½²ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰ä¸åˆ†æè§’åº¦ï¼šé’ˆå¯¹æ–°å…´çš„è½¨è¿¹ - å“åº”è’¸é¦æ•°æ®è¶‹åŠ¿ï¼Œæ·±å…¥åˆ†æç°æœ‰PRMsåœ¨ç›‘ç£ä¸­é—´è½¨è¿¹æ—¶çš„é—®é¢˜ï¼ˆç»“æ„æ ¼å¼ä¸åŒ¹é…ã€è®­ç»ƒæ•°æ®ç¼ºå¤±ï¼‰ï¼Œè¿™ç§ä»äº§ä¸šæ–°æ•°æ®å½¢æ€åæ¨æŠ€æœ¯ç—›ç‚¹çš„æ€è·¯ï¼Œä¸ºåç»­ç ”ç©¶é”šå®šæ–¹å‘æä¾›å‚è€ƒï¼›  
2. å¤šç²’åº¦ç›‘ç£èåˆï¼šå°†æ­¥éª¤çº§å’Œè½¨è¿¹çº§ç›‘ç£ç»“åˆï¼Œä¸ºå¤„ç†â€œé•¿é“¾æ¡ã€å¤šé˜¶æ®µâ€çš„æ¨ç†ç±»ä»»åŠ¡æä¾›äº†ç»†ç²’åº¦å¥–åŠ±è®¾è®¡çš„èŒƒä¾‹ï¼Œå¯è¿ç§»åˆ°ä»£ç ç”Ÿæˆã€å¤æ‚å†³ç­–ç­‰éœ€åˆ†æ­¥è¯„ä¼°çš„åœºæ™¯ï¼›  
3. å¤šåœºæ™¯å·¥ç¨‹è½åœ°ï¼šä»ç¦»çº¿æ•°æ®ç­›é€‰ã€åœ¨çº¿RLä¼˜åŒ–åˆ°æµ‹è¯•æ—¶å¢å¼ºï¼Œå®Œæ•´è¦†ç›–å¤§æ¨¡å‹è®­ç»ƒ - æ¨ç†å…¨æµç¨‹çš„å¥–åŠ±ç›‘ç£ï¼Œå±•ç¤ºäº†æŠ€æœ¯æ–¹æ¡ˆåœ¨äº§ä¸šçº§è½åœ°ä¸­çš„å¤šç»´åº¦ä»·å€¼ï¼Œä¸ºæ‰“é€ ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹æ¨ç†å¢å¼ºç®¡çº¿æä¾›äº†å®è·µæ¨¡æ¿ï¼›  
4. èµ„æºåˆ†å±‚å‘å¸ƒï¼šåŒæ—¶æä¾›7Bå’Œ1.5Bè§„æ¨¡æ¨¡å‹ï¼Œå…¼é¡¾é«˜æ€§èƒ½ä¸èµ„æºå—é™åœºæ™¯ï¼Œä½“ç°äº†æŠ€æœ¯æ™®æƒ æ€§ï¼Œåœ¨å®é™…ä¸šåŠ¡ä¸­å¯æ ¹æ®ç®—åŠ›ã€å»¶è¿Ÿç­‰éœ€æ±‚çµæ´»é€‰æ‹©ï¼Œå¹³è¡¡æ•ˆæœä¸æˆæœ¬ã€‚  

## longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning
### Abstract
Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B
### ğŸŒŸ è®ºæ–‡è§£è¯» | LongWriter-Zeroï¼šç”¨å¼ºåŒ–å­¦ä¹ çªç ´è¶…é•¿æ–‡æœ¬ç”Ÿæˆéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¶…é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚ä¸‡å­—çº§æŠ¥å‘Šã€å™äº‹åˆ›ä½œç­‰ï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…åœºæ™¯ä¸­è‡³å…³é‡è¦çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰æŠ€æœ¯é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯æ¨¡å‹ç”Ÿæˆé•¿åº¦å­˜åœ¨ä¸Šé™ï¼ŒäºŒæ˜¯éšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œå†…å®¹è´¨é‡ï¼ˆè¿è´¯æ€§ã€ä¸€è‡´æ€§ã€ç»“æ„åˆç†æ€§ç­‰ï¼‰ä¼šæ˜¾è‘—ä¸‹é™ã€‚  

æ­¤å‰ä¸»æµæ–¹æ¡ˆï¼ˆå¦‚LongWriterï¼‰ä¾èµ–**æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼Œå³åœ¨äººå·¥æ„é€ çš„â€œæŒ‡ä»¤ - é•¿æ–‡æœ¬è¾“å‡ºâ€é…å¯¹æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ä½†è¿™ç§æ–¹å¼å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š  
- æ„é€ é«˜è´¨é‡çš„åˆæˆSFTæ•°æ®æˆæœ¬æé«˜ã€éš¾åº¦å¤§ï¼›  
- åˆæˆæ•°æ®å¾€å¾€ç¼ºä¹è¿è´¯æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸”é£æ ¼å•ä¸€ã€è¿‡åº¦â€œäººå·¥åŒ–â€ï¼›  
- SFTçš„æœ€å¤§ä¼¼ç„¶ç›®æ ‡æ— æ³•æ˜¾å¼ä¼˜åŒ–å…¨å±€å±‚é¢çš„æ–‡æœ¬å±æ€§ï¼ˆå¦‚æ•´ä½“è¿è´¯æ€§ã€æ ¼å¼ä¸€è‡´æ€§ï¼‰ã€‚  

ä¸ºçªç ´è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡º**å®Œå…¨ä»é›¶å¼€å§‹ã€ä¸ä¾èµ–ä»»ä½•æ ‡æ³¨/åˆæˆæ•°æ®**çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ¡ˆï¼Œè®©LLMè‡ªä¸»â€œè¿›åŒ–â€å‡ºè¶…é•¿é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ¡†æ¶  
ä¼ ç»ŸSFTä¾èµ–å›ºå®šå‚è€ƒæ–‡æœ¬ï¼Œè€Œæœ¬æ–‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè®©æ¨¡å‹é€šè¿‡**å¥–åŠ±ä¿¡å·**ä¼˜åŒ–é•¿æ–‡æœ¬ç”Ÿæˆçš„å…¨å±€ç›®æ ‡ï¼ˆæ— éœ€äººå·¥æ„é€ SFTæ•°æ®é›†ï¼‰ã€‚å…·ä½“é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•è®­ç»ƒç­–ç•¥ç½‘ç»œï¼šä»åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Qwen2.5 - 32Bï¼‰å‡ºå‘ï¼Œè®©æ¨¡å‹åœ¨â€œå†™ä½œè¿‡ç¨‹ä¸­è‡ªä¸»è§„åˆ’ä¸è¿­ä»£â€ï¼Œé€æ­¥æŒæ¡è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç»´åº¦å¥–åŠ±æ¨¡å‹è®¾è®¡ï¼ˆReward Designï¼‰  
é’ˆå¯¹å¼€æ”¾åŸŸæ–‡æœ¬ç”Ÿæˆçš„å¤æ‚æ€§ï¼ˆä¸»è§‚æ€§ã€å¤šç»´åº¦ï¼‰ï¼Œè®¾è®¡**å¤åˆå¥–åŠ±å‡½æ•°**ï¼Œæ•´åˆå¤šä¸ªä¸“é¡¹å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œåˆ†åˆ«å¼•å¯¼æ¨¡å‹ä¼˜åŒ–ä»¥ä¸‹å…³é”®ç»´åº¦ï¼š  
- é•¿åº¦æ§åˆ¶ï¼ˆLength RMï¼‰ï¼šç¡®ä¿è¾“å‡ºæ»¡è¶³â€œè¶…é•¿â€éœ€æ±‚ï¼ŒåŒæ—¶é¿å…æ— æ„ä¹‰å†—ä½™ï¼›  
- å†™ä½œè´¨é‡ï¼ˆQuality RMï¼‰ï¼šè¯„ä¼°å†…å®¹æµç•…åº¦ã€é€»è¾‘æ€§ã€ä¸“ä¸šæ€§ç­‰ï¼›  
- ç»“æ„æ ¼å¼ï¼ˆStructure RMï¼‰ï¼šä¿éšœæ–‡æœ¬ç»“æ„åˆç†ï¼ˆå¦‚åˆ†ç« èŠ‚ã€å±‚æ¬¡æ¸…æ™°ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - time Scalingï¼‰ä¸æŒç»­é¢„è®­ç»ƒï¼ˆContinual Pretrainingï¼‰  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼šå€Ÿé‰´å¤§æ¨¡å‹åœ¨æ•°å­¦/ä»£ç ä»»åŠ¡ä¸­â€œé•¿æ€ç»´é“¾ï¼ˆCoTï¼‰â€çš„æˆåŠŸç»éªŒï¼Œæ¢ç´¢åœ¨è¶…é•¿æ–‡æœ¬ç”Ÿæˆä¸­å¼•å…¥é•¿CoTï¼Œå¢å¼ºæ¨¡å‹æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ï¼›  
- æŒç»­é¢„è®­ç»ƒï¼šåœ¨é•¿æ–‡æœ¬ç´ æä¸æ¨ç†æ•°æ®ä¸ŠæŒç»­é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡RLè®­ç»ƒåæ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- åŸºå‡†æµ‹è¯•ç¢¾å‹ä¼ ç»ŸSFTï¼šåŸºäºQwen2.5 - 32Bè®­ç»ƒçš„LongWriter - Zeroï¼Œåœ¨WritingBenchã€Arena - Writeç­‰é•¿æ–‡æœ¬å†™ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼Œ**å…¨é¢è¶…è¶Šä¼ ç»ŸSFTæ–¹æ³•**ï¼›  
- è¶…è¶Šåƒäº¿å‚æ•°æ¨¡å‹ï¼šåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡»è´¥DeepSeek R1ã€Qwen3 - 235Bç­‰ç™¾ billion + è§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œåˆ·æ–°SOTAï¼›  
- å¼€æºèµ„æºä¸°å¯Œï¼šæ¨¡å‹ checkpoint å’Œæ•°æ®å·²å¼€æºï¼ˆhttps://huggingface.co/THU - KEG/LongWriter - Zero - 32Bï¼‰ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¯å¤ç°ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°ï¼šè¯æ˜å¼ºåŒ–å­¦ä¹ å¯åœ¨â€œæ— æ ‡æ³¨/åˆæˆæ•°æ®â€åœºæ™¯ä¸‹ï¼Œæ¿€æ´»LLMçš„è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹èƒ½åŠ›è§£é”æä¾›äº†â€œéSFTâ€çš„æ–°èŒƒå¼ï¼›  
2. å¥–åŠ±å·¥ç¨‹ï¼šå¤šç»´åº¦å¤åˆå¥–åŠ±æ¨¡å‹çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å¼€æ”¾åŸŸç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚åˆ›æ„å†™ä½œã€å¤šè½®å¯¹è¯ï¼‰ï¼Œç”¨äºåˆ»ç”»â€œä¸»è§‚æ€§å¼ºã€æ— æ˜ç¡®ground - truthâ€åœºæ™¯ä¸‹çš„è´¨é‡è¯„ä¼°ï¼›  
3. è®­ç»ƒç­–ç•¥ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆé•¿CoTï¼‰ä¸æŒç»­é¢„è®­ç»ƒçš„ç»„åˆï¼Œä¸ºæå‡å¤§æ¨¡å‹é•¿æ–‡æœ¬æ¨ç†ã€ç”Ÿæˆçš„ä¸Šé™æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯è·¯çº¿ï¼›  
4. è½åœ°ä»·å€¼ï¼šé’ˆå¯¹çœŸå®ä¸–ç•Œâ€œè¶…é•¿æ–‡æœ¬éœ€æ±‚â€ï¼ˆå¦‚æŠ¥å‘Šæ’°å†™ã€æ³•å¾‹æ–‡ä¹¦ã€æ•™è‚²å†…å®¹åˆ›ä½œï¼‰ï¼Œæä¾›äº†æ›´ä¼˜è´¨çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæ¨åŠ¨LLMåœ¨ä¸“ä¸šé¢†åŸŸçš„è½åœ°ã€‚  


LongWriter - Zeroçš„å·¥ä½œä¸ä»…è§£å†³äº†è¶…é•¿æ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ç—›ç‚¹ï¼Œæ›´å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹èƒ½åŠ›è¿›åŒ–ä¸­çš„æ½œåŠ›â€”â€”æ— éœ€ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨ï¼Œä¹Ÿèƒ½è®©æ¨¡å‹â€œè‡ªä¸»å­¦ä¹ â€å¤æ‚ä»»åŠ¡çš„å®Œæˆèƒ½åŠ›ã€‚è¿™ä¸ºå¤§æ¨¡å‹ç ”å‘èŒƒå¼ã€å¥–åŠ±æœºåˆ¶è®¾è®¡ç­‰æ–¹å‘ï¼Œéƒ½å¸¦æ¥äº†æå…·å¯å‘æ€§çš„å‚è€ƒã€‚

## rdpo--real-data-preference-optimization-for-physics-consistency-video-generation
### Abstract
Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/
### ğŸŒŸ è®ºæ–‡è§£è¯» | RDPOï¼šä»çœŸå®è§†é¢‘ä¸­æç‚¼ç‰©ç†å…ˆéªŒï¼Œé©æ–°è§†é¢‘ç”Ÿæˆçš„ç‰©ç†ä¸€è‡´æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†é¢‘ç”ŸæˆæŠ€æœ¯åœ¨è§†è§‰è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¦å¿ å®å¤ç°çœŸå®ä¸–ç•Œçš„ç‰©ç†è§„å¾‹ä»é¢‡å…·æŒ‘æˆ˜ã€‚åŸºäºåå¥½çš„æ¨¡å‹åè®­ç»ƒè™½æœ‰æœ›æå‡ç‰©ç†ä¸€è‡´æ€§ï¼Œå´ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨æ•°æ®é›†æˆ–å°šæœªæˆç†Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚ä¸€æ–¹é¢ï¼Œæ„å»ºèƒ½æ£€æµ‹ä»»æ„è§†é¢‘ç‰©ç†è¿è§„çš„å¥–åŠ±å‡½æ•°ä»æ˜¯å¼€æ”¾éš¾é¢˜ï¼›å¦ä¸€æ–¹é¢ï¼Œå¤§è§„æ¨¡äººå·¥åå¥½æ•°æ®é›†çš„åˆ¶ä½œæˆæœ¬é«˜ã€è€—æ—¶é•¿ï¼Œä¸”äººç±»åˆ¤æ–­å­˜åœ¨ä¸»è§‚æ€§å·®å¼‚ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§é«˜æ•ˆã€æ— éœ€æ ‡æ³¨çš„åå¥½ä¼˜åŒ–ç­–ç•¥æ¥é’ˆå¯¹æ€§æå‡ç‰©ç†çœŸå®æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ— æ ‡æ³¨æ¡†æ¶RDPO  
Real Data Preference Optimizationï¼ˆRDPOï¼‰æ˜¯ä¸€ä¸ªæ— éœ€äººå·¥æ ‡æ³¨çš„åå¥½ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥ä»çœŸå®ä¸–ç•Œè§†é¢‘ä¸­æç‚¼ç‰©ç†å…ˆéªŒã€‚å®ƒç»•å¼€äº†ä¼ ç»ŸDPOæˆ–RLHFå¯¹äººç±»è¾“å…¥çš„ä¾èµ–ï¼Œä»¥çœŸå®è§†é¢‘ç‰‡æ®µä½œä¸ºç‰©ç†åŠ¨æ€ä¿¡æ¯çš„â€œé»„é‡‘æ ‡å‡†â€ï¼Œæ¢ç´¢åˆ©ç”¨çœŸå®ä¸–ç•Œè§†é¢‘å›ºæœ‰ç‰©ç†å…ˆéªŒçš„é«˜æ•ˆè·¯å¾„ï¼Œè€Œéå¯¹è¿™äº›è§†é¢‘åå¤è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªåŠ¨æ„å»ºåå¥½å¯¹çš„æ–¹æ³•  
RDPOå€ŸåŠ©é¢„è®­ç»ƒç”Ÿæˆå™¨å¯¹çœŸå®è§†é¢‘åºåˆ—è¿›è¡Œåå‘é‡‡æ ·ï¼Œè‡ªåŠ¨æ„å»ºåœ¨ç‰©ç†æ­£ç¡®æ€§ä¸Šå…·æœ‰ç»Ÿè®¡åŒºåˆ†åº¦çš„åå¥½å¯¹ã€‚åœ¨åå‘é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡é€‰æ‹©æ€§åˆ©ç”¨ latent è¡¨ç¤ºæ¥æ„é€ è§†é¢‘ï¼Œè¿™äº› latent è¡¨ç¤ºæ—¢æºäºçœŸå®æ•°æ®ï¼ˆå¯Œå«çœŸå®ç‰©ç†ä¿¡æ¯ï¼‰ï¼Œåˆä¸æ¨¡å‹å›ºæœ‰ç”Ÿæˆåˆ†å¸ƒä¿æŒå¯¹é½ï¼ˆé¿å…è¿‡åº¦æ”¹å˜æ¨¡å‹è§†è§‰é£æ ¼æˆ–äº§ç”Ÿåˆ†å¸ƒå¤–è¾“å‡ºï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šé˜¶æ®µè¿­ä»£è®­ç»ƒæœºåˆ¶  
é‡‡ç”¨å¤šé˜¶æ®µè¿­ä»£è®­ç»ƒè°ƒåº¦ï¼Œå¼•å¯¼ç”Ÿæˆå™¨é€æ­¥æ›´å¥½åœ°éµå¾ªç‰©ç†å®šå¾‹ã€‚åˆ©ç”¨çœŸå®è§†é¢‘ä¸­æ¢ç´¢åˆ°çš„åŠ¨æ€ä¿¡æ¯ï¼Œè®©ç”Ÿæˆè§†é¢‘åœ¨åŠ¨ä½œè¿è´¯æ€§ä¸ç‰©ç†çœŸå®æ€§ä¸Šå¾—åˆ°æ˜¾è‘—æå‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸äººç±»è¯„ä¼°ä¸­éªŒè¯ï¼ŒRDPO åœ¨å¤šä¸ªç»´åº¦ï¼ˆå¦‚ç‰©ç†ä¸€è‡´æ€§ã€è§†é¢‘æ•´ä½“è´¨é‡ç­‰ï¼‰å®ç°äº†æå‡ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶åœ¨æ”¹è¿›ä¸åŒåŸºçº¿æ¨¡å‹ç‰©ç†ä¸€è‡´æ€§ä¸è§†é¢‘è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼›åŒæ—¶è¿˜å¯¹æ¯”åˆ†æäº† RDPO ä¸ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨çš„ä¼ ç»Ÿ DPO æ–¹æ³•ï¼Œæ¢ç´¢äº†äºŒè€…ç»“åˆçš„æ½œåœ¨ååŒæ•ˆç›Šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ— æ ‡æ³¨èŒƒå¼çš„åˆ›æ–°ï¼šä¸ºè§£å†³éœ€å¤§é‡äººå·¥æ ‡æ³¨çš„ä»»åŠ¡æä¾›äº†æ€è·¯ï¼Œå±•ç¤ºäº†å¦‚ä½•ä»çœŸå®æ•°æ®ä¸­è‡ªåŠ¨æŒ–æ˜ç›‘ç£ä¿¡å·ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚  
2. ç‰©ç†å…ˆéªŒçš„åˆ©ç”¨ï¼šåœ¨è§†é¢‘ç”Ÿæˆè¿™ç±»éœ€éµå¾ªç°å®ä¸–ç•Œè§„å¾‹çš„ä»»åŠ¡ä¸­ï¼Œæä¾›äº†ä»çœŸå®æ•°æ®æç‚¼é¢†åŸŸå…ˆéªŒï¼ˆå¦‚ç‰©ç†è§„å¾‹ï¼‰çš„èŒƒä¾‹ï¼Œå¯å¯å‘å…¶ä»–éœ€ç»“åˆç°å®çŸ¥è¯†çš„ç”Ÿæˆç±»ä»»åŠ¡ï¼ˆå¦‚æ¨¡æ‹Ÿä»¿çœŸã€è™šæ‹Ÿåœºæ™¯æ„å»ºç­‰ï¼‰ã€‚  
3. è¿­ä»£è®­ç»ƒä¸åˆ†å¸ƒå¯¹é½ï¼šå¤šé˜¶æ®µè¿­ä»£è®­ç»ƒè°ƒåº¦ä»¥åŠ latent ç©ºé—´å¯¹é½çš„æ€è·¯ï¼Œå¯¹å¹³è¡¡â€œå¼•å…¥æ–°å…ˆéªŒâ€ä¸â€œä¿æŒæ¨¡å‹åŸæœ‰èƒ½åŠ›/åˆ†å¸ƒâ€å…·æœ‰å‚è€ƒä»·å€¼ï¼Œåœ¨æ¨¡å‹å¾®è°ƒã€é¢†åŸŸé€‚é…ç­‰åœºæ™¯ä¸­æˆ–å¯å¤ç”¨ã€‚  

## text-detoxification--data-efficiency--semantic-preservation-and-model-generalization
### Abstract
The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at: https://github.com/allacnobug/Detoxification-of-Text.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ–‡æœ¬å»æ¯’ï¼šæ•°æ®é«˜æ•ˆã€è¯­ä¹‰ä¿ç•™ä¸æ¨¡å‹æ³›åŒ–çš„æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€åœ¨çº¿åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•ï¼Œç¤¾äº¤åª’ä½“ä¸Šæœ‰æ¯’å†…å®¹ï¼ˆå¦‚ä¾®è¾±ã€æ­§è§†ã€ä»‡æ¨è¨€è®ºç­‰ï¼‰çš„å¹¿æ³›ä¼ æ’­å¯¹ç½‘ç»œç¯å¢ƒå’Œå…¬å…±è¯è¯­æ„æˆä¸¥é‡å¨èƒï¼Œå› æ­¤æ€¥éœ€èƒ½åœ¨ä¿ç•™åŸå§‹è¯­ä¹‰çš„åŒæ—¶æœ‰æ•ˆå»é™¤æ¯’æ€§çš„å»æ¯’æ–¹æ³•ã€‚ç„¶è€Œç°æœ‰æ–¹æ³•å­˜åœ¨è¯¸å¤šä¸è¶³ï¼šä¸€æ˜¯éš¾ä»¥åŒæ—¶å®ç°å¼ºå»æ¯’æ€§èƒ½ã€è¯­ä¹‰ä¿ç•™å’Œå¯¹åˆ†å¸ƒå¤–æ•°æ®çš„é²æ£’æ€§ï¼›äºŒæ˜¯ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨å¹³è¡Œè¯­æ–™ä¸”æ•°æ®æ•ˆç‡ä½ã€‚æ­¤å¤–ï¼Œä»¥å¾€åŸºäºè¾ƒå°æ¨¡å‹ï¼ˆå¦‚T5ã€BARTï¼‰çš„æ–¹æ³•åœ¨å¤„ç†è¯­ä¹‰å¤æ‚ã€é£æ ¼å¤šå˜çš„æœ‰æ¯’å†…å®¹æ—¶ï¼Œè¯­ä¹‰ç†è§£å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œæ˜“å‡ºç°åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³çš„æƒ…å†µï¼Œä¸”åœ¨å»æ¯’å’Œè¯­ä¹‰ä¿ç•™é—´éš¾ä»¥å¹³è¡¡ï¼›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½æœ‰å¼ºè¯­ä¹‰ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å› ä¸äººç±»ä»·å€¼è§‚å¯¹é½è¿‡ç¨‹å¯¹æœ‰æ¯’å†…å®¹é«˜åº¦æ•æ„Ÿï¼Œç®€å•promptå·¥ç¨‹æˆ–few - shotsæ–¹æ³•æ˜“æ‹’ç”Ÿæˆæˆ–è¿‡åº¦æ”¹å˜åŸæ„ï¼Œè€Œäººå·¥æ ‡æ³¨æœ‰æ¯’å†…å®¹æˆæœ¬é«˜ä¸”æ•°æ®è´¨é‡å­˜ç–‘ï¼Œç›´æ¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ noisyæ•°æ®æ˜“æ•ˆæœå·®ä¸”é™åˆ¶æ¨¡å‹æ€§èƒ½ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æ¢ç´¢åˆ©ç”¨LLMè§£å†³æ–‡æœ¬å»æ¯’é—®é¢˜ï¼Œæå‡ºä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶åº”å¯¹æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶å®ç°æ•°æ®é«˜æ•ˆå»æ¯’
é¦–å…ˆä½¿ç”¨å°æ‰¹é‡é«˜è´¨é‡ã€ç»è¿‡ç­›é€‰çš„å¹³è¡Œæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å»ºç«‹å¼ºåˆå§‹åŒ–ï¼Œæ­¤æ­¥éª¤èƒ½åˆ©ç”¨ä¼˜è´¨æ•°æ®è®©æ¨¡å‹å…ˆå­¦ä¹ åˆ°å»æ¯’å’Œè¯­ä¹‰ä¿ç•™çš„åŸºç¡€æ¨¡å¼ï¼Œé¿å…åœ¨å™ªå£°æ•°æ®ä¸Šå¾®è°ƒå¸¦æ¥çš„â€œåƒåœ¾è¿›åƒåœ¾å‡ºâ€é—®é¢˜ï¼›ç„¶ååˆ©ç”¨æ— æ ‡æ³¨çš„æœ‰æ¯’è¾“å…¥å’Œè‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰è®­ç»ƒLLMã€‚è¯¥æ¡†æ¶ä»…ç”¨20%çš„æ ‡æ³¨æ•°æ®å°±èƒ½å®ç°ä¼˜å¼‚æ€§èƒ½ï¼Œå¤§å¹…é™ä½å¯¹æ˜‚è´µäººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¹³è¡¡å»æ¯’æ€§èƒ½ä¸è¯­ä¹‰ä¿ç•™
é¦–æ¬¡åŒæ—¶ä¼˜åŒ–å»æ¯’æœ‰æ•ˆæ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±å‡½æ•°è”åˆè€ƒè™‘è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå»æ¯’è´¨é‡ï¼Œå¼•å¯¼æ¨¡å‹åœ¨å»é™¤æ¯’æ€§çš„åŒæ—¶å°½å¯èƒ½ä¿ç•™åŸå§‹è¯­ä¹‰ï¼Œè§£å†³äº†ä»¥å¾€æ–¹æ³•è¦ä¹ˆå»æ¯’å¥½ä½†è¯­ä¹‰ä¸¢å¤±å¤šã€è¦ä¹ˆè¯­ä¹‰ä¿ç•™å¥½ä½†å»æ¯’ä¸å½»åº•çš„ä¸¤éš¾é—®é¢˜ï¼Œåœ¨å¤šåŸºçº¿å¯¹æ¯”ä¸­å®ç°äº†å½“å‰æœ€ä¼˜æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥GRPOå¢å¼ºOODæ€§èƒ½
é¦–æ¬¡å°†åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ å¼•å…¥æœ‰æ¯’å†…å®¹é‡å†™ä»»åŠ¡ã€‚åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä»¥æ— æ ‡æ³¨æ–¹å¼è®©LLMä¸å»æ¯’ç›®æ ‡å¯¹é½ï¼Œæå‡äº†æ¨¡å‹å¯¹å¤šæ ·ä¸”ä¸æ–­æ¼”å˜çš„æœ‰æ¯’è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œåº”å¯¹åˆ†å¸ƒå¤–æ•°æ®æ—¶è¡¨ç°æ›´å¼ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†å…ˆå‰å·¥ä½œé¢ä¸´çš„æƒè¡¡é—®é¢˜ï¼Œåœ¨æå‡æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå®ç°äº†å½“å‰æœ€ä¼˜æ€§èƒ½ã€‚ï¼ˆæ–‡ä¸­æœªè¯¦ç»†å±•å¼€å®éªŒæ•°æ®ç»†èŠ‚ï¼Œä½†ä»æ‘˜è¦å’Œå¼•è¨€å¯æ¨æ–­åœ¨å»æ¯’æ•ˆæœã€è¯­ä¹‰ä¿ç•™åº¦ã€OODåœºæ™¯è¡¨ç°ç­‰æ–¹é¢å‡è¶…è¶Šç°æœ‰æ–¹æ³•ï¼‰

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆ©ç”¨æ€è·¯ï¼šåœ¨æ•°æ®ç¨€ç¼ºä¸”æ ‡æ³¨æ˜‚è´µçš„ä»»åŠ¡ä¸­ï¼Œå¯å€Ÿé‰´å…ˆç­›é€‰é«˜è´¨é‡å°æ•°æ®ç›‘ç£å¾®è°ƒåˆå§‹åŒ–ï¼Œå†ç»“åˆæ— æ ‡æ³¨æ•°æ®å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„æ€è·¯ï¼Œæå‡æ•°æ®æ•ˆç‡ã€‚
2. å¤šç›®æ ‡ä¼˜åŒ–ï¼šé¢å¯¹éœ€å¹³è¡¡å¤šä¸ªç›®æ ‡ï¼ˆå¦‚å»æ¯’ä¸è¯­ä¹‰ä¿ç•™ï¼‰çš„ä»»åŠ¡ï¼Œè®¾è®¡è”åˆè€ƒè™‘å„ç›®æ ‡çš„å¥–åŠ±å‡½æ•°æˆ–ä¼˜åŒ–ç›®æ ‡ï¼Œæ˜¯å®ç°å¤šç›®æ ‡æœ€ä¼˜çš„æœ‰æ•ˆæ–¹å¼ã€‚
3. å¼ºåŒ–å­¦ä¹ åœ¨LLMå¾®è°ƒçš„åº”ç”¨ï¼šå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥LLMé’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬å»æ¯’ï¼‰çš„å¾®è°ƒï¼Œä¸ºæå‡æ¨¡å‹æ³›åŒ–æ€§å’Œé²æ£’æ€§æä¾›äº†æ–°èŒƒå¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€æ¨¡å‹é€‚åº”å¤šå˜è¾“å…¥åœºæ™¯çš„NLPä»»åŠ¡ä¸­ã€‚

## shrinking-the-generation-verification-gap-with-weak-verifiers
### Abstract
Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¼±éªŒè¯å™¨ç¼©å°ç”Ÿæˆ-éªŒè¯å·®è·ï¼šWeaveræ¡†æ¶çš„åˆ›æ–°ä¹‹è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨éƒ¨ç½²è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ—¶ï¼ŒéªŒè¯æ¨¡å‹å“åº”çš„è´¨é‡æˆ–æ­£ç¡®æ€§æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¿™ä¸€é—®é¢˜åœ¨æ•°æ®é›†æ•´ç†ã€æ¨¡å‹å¯¹é½å’Œæ¨ç†æ—¶å†³ç­–ç­‰LM pipelineå„ç¯èŠ‚éƒ½å­˜åœ¨ã€‚å€ŸåŠ©å®Œç¾éªŒè¯å™¨ç»“åˆé‡å¤é‡‡æ ·ï¼ˆç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”åé€‰æœ€ä¼˜ï¼‰èƒ½å¤§å¹…æå‡æ¨¡å‹åœ¨æ•°å­¦ã€ä»£ç ã€æ¨ç†ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†å®Œç¾éªŒè¯å™¨è¦ä¹ˆä¸å¯æ‰©å±•ï¼ˆå¦‚äººå·¥éªŒè¯ï¼‰ï¼Œè¦ä¹ˆå®ç”¨æ€§æœ‰é™ï¼ˆå¦‚Leanè¿™ç±»å½¢å¼åŒ–è¯æ˜å·¥å…·ï¼‰ã€‚è€Œä½œä¸ºé€šç”¨éªŒè¯å™¨çš„LMè£åˆ¤å’Œå¥–åŠ±æ¨¡å‹ï¼Œä¸â€œ oracle verifiersï¼ˆå®Œç¾å‡†ç¡®çš„éªŒè¯å™¨ï¼‰â€ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå³â€œç”Ÿæˆ - éªŒè¯å·®è·â€â€”â€”æ¨¡å‹èƒ½ç”Ÿæˆæ­£ç¡®å“åº”ä½†æ— æ³•è¢«è¯†åˆ«ã€‚åŒæ—¶ï¼Œå¼±éªŒè¯å™¨ï¼ˆå¦‚LMè£åˆ¤ã€å¥–åŠ±æ¨¡å‹ï¼‰å­˜åœ¨åˆ†æ•°å™ªå£°å¤§ã€æ ¡å‡†å·®ã€å‡é˜³æ€§ç‡é«˜ç­‰é—®é¢˜ï¼Œä¸”æ•´åˆå¼±éªŒè¯å™¨è¿˜é¢ä¸´ naive èšåˆä¸è¶³ã€æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹æœ‰æ•ˆé›†æˆéš¾ã€æ¨ç†æ—¶éƒ¨ç½²éªŒè¯æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ï¼Œå› æ­¤æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•ç»“åˆå¤šä¸ªå¼±éªŒè¯å™¨æ¥æ”¹è¿›é‡å¤é‡‡æ ·ä¸‹çš„å“åº”é€‰æ‹©ï¼Œç¼©å°ç”Ÿæˆ - éªŒè¯å·®è·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºWeaveræ¡†æ¶èšåˆå¼±éªŒè¯å™¨
Weaveræ˜¯ä¸€ä¸ªæ— éœ€åœ¨çœŸå®æ ‡ç­¾ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒæ¥èšåˆå¼±éªŒè¯å™¨çš„æ¡†æ¶ã€‚é¦–å…ˆå‘ç°ï¼Œåœ¨æœ‰å¤§é‡å¸¦æ ‡ç­¾è®­ç»ƒæ•°æ®æ—¶ï¼Œå­¦ä¹ åŠ æƒéªŒè¯å™¨é›†åˆï¼ˆåˆ©ç”¨éªŒè¯å™¨å‡†ç¡®ç‡å·®å¼‚ï¼‰èƒ½æ¯” naive å¹³å‡ï¼ˆå‡è®¾éªŒè¯å™¨è´¨é‡ä¸€è‡´ï¼Œæ˜“è®©ä½è´¨é‡éªŒè¯å™¨ä¸»å¯¼è‡´ç²¾åº¦ä¸‹é™ï¼‰æœ€å¤šé«˜å‡º11.2ä¸ªç™¾åˆ†ç‚¹ï¼›å½“ç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®æ—¶ï¼Œå°†å¼±ç›‘ç£ï¼ˆWSï¼‰é€‚é…åˆ°éªŒè¯åœºæ™¯ï¼Œè§£å†³è¾“å‡ºä¸ä¸€è‡´å’Œä½ç²¾åº¦éªŒè¯å™¨é—®é¢˜ï¼Œé€šè¿‡è¿‡æ»¤æ— ä¿¡æ¯éªŒè¯å™¨ã€å½’ä¸€åŒ–éªŒè¯å™¨åˆ†æ•°ï¼Œå¹¶åŸºäºè¿™äº›åˆ†æ•°å’ŒæœªçŸ¥çœŸå®æ ‡ç­¾æ„å»º latent variable model æ¥ä¼°è®¡éªŒè¯å™¨å‡†ç¡®ç‡ä½œä¸ºé›†åˆæƒé‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£å†³å¼±éªŒè¯å™¨é›†æˆçš„å¤šæŒ‘æˆ˜
é’ˆå¯¹å¼±éªŒè¯å™¨é›†æˆçš„ä¸‰å¤§æŒ‘æˆ˜é€ä¸€åº”å¯¹ï¼šå¯¹äº naive èšåˆä¸è¶³ï¼Œç”¨åŠ æƒé›†åˆæ›¿ä»£ naive å¹³å‡ï¼Œåˆ©ç”¨éªŒè¯å™¨å‡†ç¡®ç‡å·®å¼‚æå‡æ€§èƒ½ï¼›å¯¹äºæœ‰é™æ ‡æ³¨æ•°æ®ä¸‹æœ‰æ•ˆé›†æˆéš¾ï¼Œé€‚é…å¼±ç›‘ç£æŠ€æœ¯ï¼Œå¤„ç†å¼±éªŒè¯å™¨è¾“å‡ºæ ¼å¼ä¸ä¸€è‡´ï¼ˆå¦‚logitsã€äºŒåˆ†ç±»åˆ†æ•°ã€Likertåˆ†æ•°ç­‰ï¼‰å’Œä½è´¨é‡é—®é¢˜ï¼Œå€ŸåŠ©æ•°æ®é›†ç»Ÿè®¡å½’ä¸€åŒ–è¾“å‡ºå’Œè¿‡æ»¤ç‰¹å®šéªŒè¯å™¨ï¼›å¯¹äºæ¨ç†æ—¶éƒ¨ç½²éªŒè¯æˆæœ¬é«˜ï¼Œç”¨Weaverçš„ç»„åˆè¾“å‡ºåˆ†æ•°è®­ç»ƒç´§å‡‘çš„400Mè·¨ç¼–ç å™¨ï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—æˆæœ¬åŒæ—¶ä¿ç•™é«˜å‡†ç¡®ç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æµ‹è¯•æ—¶é‡å¤é‡‡æ ·åœºæ™¯ï¼ˆæ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”åé€‰æ‹©å…¶ä¸€ï¼‰ä¸‹è¯„ä¼°ï¼ŒWeaverç›¸æ¯”éªŒè¯å™¨åˆ†æ•°æ— åŠ æƒå¹³å‡çš„é‡å¤é‡‡æ ·æ€§èƒ½æå‡17.1%ï¼Œç›¸æ¯”å¤šæ•°æŠ•ç¥¨æå‡13.5%ï¼›å¯¹æ¯”LMçš„Pass@1ï¼ˆé€‰ç¬¬ä¸€ä¸ªå€™é€‰å“åº”çš„æ€§èƒ½ï¼‰ï¼Œåœ¨æ¨ç†å’Œæ•°å­¦ä»»åŠ¡ä¸Šï¼Œå¯¹8Bæ¨¡å‹æ€§èƒ½æå‡17.9%ï¼Œå¯¹70Bæ¨¡å‹æå‡14.5%ï¼›ç”¨Llama 3.3 70B Instructä½œç”Ÿæˆå™¨ã€70Bæˆ–æ›´å°çš„è£åˆ¤å’Œå¥–åŠ±æ¨¡å‹ä½œéªŒè¯å™¨é›†åˆæ—¶ï¼Œèƒ½è¾¾åˆ°o3 - miniæ°´å¹³å‡†ç¡®ç‡ï¼ˆå¹³å‡87.7%ï¼‰ï¼Œè¯¥å¢ç›Šå ªæ¯”GPT - 4oåˆ°o3 - miniçš„æ€§èƒ½è·ƒå‡ï¼ˆ69.0% vs. 86.7%ï¼Œåè€…éœ€å¤§é‡å¾®è°ƒä¸åè®­ç»ƒï¼‰ï¼›è®­ç»ƒçš„400Mè·¨ç¼–ç å™¨è’¸é¦æ¨¡å‹ä¿ç•™äº†Weaverå…¨ç²¾åº¦çš„98.7%ï¼ŒåŒæ—¶å°†éªŒè¯è®¡ç®—é‡é™ä½è¾¾99.97%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šå¼±éªŒè¯å™¨èšåˆæ€è·¯ï¼šå½“é¢ä¸´å¤šä¸ªæœ‰ç¼ºé™·ä½†äº’è¡¥çš„å·¥å…·/æ¨¡å‹æ—¶ï¼Œå¯å€Ÿé‰´WeaveråŠ æƒèšåˆå¹¶ç»“åˆå¼±ç›‘ç£ä¼°è®¡æƒé‡çš„æ–¹å¼ï¼Œå……åˆ†åˆ©ç”¨å„å·¥å…·ä¼˜åŠ¿ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®ä¾èµ–ã€‚
2. æ€§èƒ½ - æˆæœ¬æƒè¡¡ï¼šåœ¨è¿½æ±‚æ¨¡å‹æ€§èƒ½æå‡åŒæ—¶å…³æ³¨æ¨ç†æˆæœ¬ï¼Œå¦‚Weaveré€šè¿‡è’¸é¦å¾—åˆ°ç´§å‡‘æ¨¡å‹é™ä½è®¡ç®—æˆæœ¬ï¼Œè¿™ç§åœ¨åº”ç”¨ä¸­å¹³è¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—çš„æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚
3. å¼±ç›‘ç£é€‚é…ç‰¹å®šåœºæ™¯ï¼šé’ˆå¯¹è‡ªèº«ä»»åŠ¡åœºæ™¯ä¸­ç±»ä¼¼â€œå¼±éªŒè¯å™¨è¾“å‡ºæ ¼å¼ä¸ä¸€ã€è´¨é‡å‚å·®â€ç­‰é—®é¢˜ï¼Œå¯å‚è€ƒWeaveråˆ©ç”¨æ•°æ®é›†ç»Ÿè®¡è¿›è¡Œå½’ä¸€åŒ–ã€è¿‡æ»¤ç­‰æ‰‹æ®µæ¥é€‚é…å¼±ç›‘ç£æŠ€æœ¯ï¼Œæ‹“å±•å¼±ç›‘ç£åº”ç”¨è¾¹ç•Œã€‚

## reasongrm--enhancing-generative-reward-models-through-large-reasoning-models
### Abstract
Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonGRMï¼šå€Ÿå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›é©æ–°ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£ã€ç”Ÿæˆä¸å†³ç­–ä¸Šå–å¾—é•¿è¶³è¿›æ­¥ï¼Œä½†è¦è®©æ¨¡å‹è¾“å‡ºè´´åˆäººç±»ä»·å€¼è§‚ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯å…³é”®ã€‚ä¼ ç»Ÿæ ‡é‡å¥–åŠ±æ¨¡å‹ï¼ˆSRMsï¼‰æŠŠå¤æ‚äººç±»åå¥½å‹ç¼©æˆå•ä¸€æ ‡é‡ï¼Œæ˜“ä¿¡æ¯ä¸¢å¤±ã€æ³›åŒ–æ€§å¼±ï¼›æ–°å…´ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰è™½æ›´çµæ´»ï¼Œä½†æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå¸¸å‡ºç°æ¨ç†è·¯å¾„ä¸å®Œæ•´æˆ–è¿‡åº¦æ¨æµ‹ï¼Œå¯¼è‡´ä»»åŠ¡ä¸­â€œå¹»è§‰â€æˆ–å…³é”®ä¿¡æ¯ç¼ºå¤±ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡GRMsçš„æ¨ç†è´¨é‡ä»¥å®ç°å¯é åå¥½å»ºæ¨¡ï¼Œæˆäº†æ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReasonGRMä¸‰é˜¶æ®µæ¡†æ¶  
ReasonGRMåˆ†ä¸‰æ­¥æ‰“é€ æ›´ä¼˜ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼š  
- é˜¶æ®µä¸€ï¼ˆç”Ÿæˆæ¨ç†è·¯å¾„ï¼‰ï¼šç”¨Zero - RLç”Ÿæˆç®€æ´ã€ä»¥ç»“æœä¸ºå¯¼å‘çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘å…³é”®ä¿¡æ¯é—æ¼é£é™©ï¼›  
- é˜¶æ®µäºŒï¼ˆç­›é€‰ä¼˜è´¨è·¯å¾„ï¼‰ï¼šå¼•å…¥å…¨æ–°è¯„ä¼°æŒ‡æ ‡\( R^\star \)ï¼Œä¾æ®ç”Ÿæˆå¯èƒ½æ€§ä¸ºæ¨ç†è·¯å¾„æ‰“åˆ†ï¼Œåå¥½â€œç”¨æœ€å°‘æ¢ç´¢è¾¾æ­£ç¡®ç­”æ¡ˆâ€çš„è·¯å¾„ï¼Œå‰Šå‡è®­ç»ƒä¸­æ˜“å¼•å‘å¹»è§‰çš„æ•°æ®ï¼›  
- é˜¶æ®µä¸‰ï¼ˆå¼ºåŒ–æ¨¡å‹èƒ½åŠ›ï¼‰ï¼šé’ˆå¯¹é«˜éš¾åº¦ç¤ºä¾‹ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ç²¾è°ƒæ¨¡å‹ï¼Œå¢å¼ºå…¶åå¥½åŒºåˆ†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰\( R^\star \)è¯„ä¼°æŒ‡æ ‡è§£å†³æ•°æ®è´¨é‡éš¾é¢˜  
\( R^\star \)ç»“åˆâ€œæœ‰æ•ˆæ€§ï¼ˆValidityï¼Œæ¨ç†å¯¼å‘æ­£ç¡®ç»“æœï¼‰â€ä¸â€œè‡ªä¸€è‡´æ€§ï¼ˆSelf - Consistencyï¼Œæ¨ç†é€»è¾‘è¿è´¯æ— å†—ä½™ï¼‰â€ä¸¤å¤§å…³é”®å±æ€§ï¼Œé€šè¿‡ç”Ÿæˆå¯èƒ½æ€§æ¥è¯„ä¼°æ¨ç†è·¯å¾„ï¼Œèƒ½ä»å™ªå£°å€™é€‰é›†ä¸­è‡ªåŠ¨é€‰ä¼˜è´¨æ¨ç†è·¯å¾„ï¼Œç ´è§£å¤æ‚ä»»åŠ¡å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æ•°æ®è´¨é‡ç“¶é¢ˆã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨RM - Benchã€RewardBenchã€RMBä¸‰å¤§å…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonGRMè¡¨ç°äº®çœ¼ï¼šå¹³å‡è¶…è¶Šæ­¤å‰æœ€ä¼˜GRMs 1.8%ï¼Œåœ¨éƒ¨åˆ†åœºæ™¯ä¸‹æ¯”GPT - 4oç­‰é—­æºæ¨¡å‹é¢†å…ˆè¾¾5.6%ï¼Œè¿˜æ¯”ä¸»æµSRMså¹³å‡é«˜4.5%ã€‚å®éªŒä¸ä»…éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œæ¶ˆèå®éªŒä¹Ÿå‰–æäº†æ¨ç†è´¨é‡ã€\( R^\star \)è¿‡æ»¤æ•ˆæœã€å„è®­ç»ƒé˜¶æ®µå¯¹æœ€ç»ˆå¥–åŠ±æ¨¡å‹çš„å½±å“ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é‡è§†æ¨ç†è´¨é‡åœ¨å¥–åŠ±æ¨¡å‹ä¸­çš„ä»·å€¼ï¼šæ­ç¤ºäº†é«˜è´¨é‡æ¨ç†è·¯å¾„ï¼ˆå…¼é¡¾æœ‰æ•ˆæ€§ä¸è‡ªä¸€è‡´æ€§ï¼‰å¯¹åå¥½å»ºæ¨¡çš„å…³é”®ä½œç”¨ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹è®¾è®¡æŒ‡æ˜â€œæ¨ç†æ„ŸçŸ¥â€æ–¹å‘ï¼›  
2. åˆ›æ–°è¯„ä¼°ä¸è¿‡æ»¤æœºåˆ¶ï¼š\( R^\star \)å±•ç¤ºäº†å¦‚ä½•ç”¨ç”Ÿæˆå¯èƒ½æ€§é‡åŒ–æ¨ç†è´¨é‡ï¼Œä¸ºå¤„ç†å™ªå£°æ•°æ®ã€æ„å»ºä¼˜è´¨è®­ç»ƒé›†æä¾›äº†å¯å¤ç”¨æ€è·¯ï¼›  
3. å¤šé˜¶æ®µè®­ç»ƒPipelineï¼šä»ç”Ÿæˆåˆ°ç­›é€‰å†åˆ°å¼ºåŒ–å­¦ä¹ çš„æµç¨‹ï¼Œä¸ºé€šç”¨LLMå‘ä¸“ç²¾å¥–åŠ±æ¨¡å‹è½¬åŒ–æä¾›äº†å·¥ç¨‹åŒ–å‚è€ƒèŒƒå¼ï¼›  
4. å…¨é¢å®éªŒéªŒè¯ï¼šè·¨å¤šä¸ªæƒå¨åŸºå‡†çš„æµ‹è¯•+æ¶ˆèå®éªŒï¼Œæ˜¯å­¦æœ¯ç ”ç©¶ä¸­éªŒè¯æ–¹æ³•æ™®é€‚æ€§ä¸æ¨¡å—ä»·å€¼çš„å…¸èŒƒï¼Œå€¼å¾—å€Ÿé‰´ä»¥å¢å¼ºç ”ç©¶è¯´æœåŠ›ã€‚  
```

## robust-reward-modeling-via-causal-rubrics
### Abstract
Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå› æœå‡†åˆ™æ‰“é€ é²æ£’å¥–åŠ±æ¨¡å‹ï¼šCromeæ¡†æ¶ç ´è§£å¥–åŠ±é»‘å®¢éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»åé¦ˆçš„è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯æ ¸å¿ƒç¯èŠ‚ï¼Œä½†å½“å‰å¥–åŠ±æ¨¡å‹æ™®éé¢ä¸´ â€œå¥–åŠ±é»‘å®¢â€ é—®é¢˜ã€‚ä¼ ç»ŸRMå¸¸æŠŠè®­ç»ƒæ•°æ®é‡Œçš„ç»Ÿè®¡ç›¸å…³æ€§é”™å½“æˆè´¨é‡çš„çœŸæ­£å› æœé©±åŠ¨å› ç´ ï¼Œæ¯”å¦‚ä¼šä¾æ®å›å¤é•¿åº¦ã€æ ¼å¼è¿™ç±»è¡¨é¢æˆ–è™šå‡å±æ€§æ¥æ‰“åˆ†ï¼Œè€Œéäº‹å®æ€§ã€ç›¸å…³æ€§ç­‰çœŸå®è´¨é‡ç»´åº¦ã€‚è¿™æ˜¯å› ä¸ºæ ‡å‡†è®­ç»ƒç›®æ ‡éš¾ä»¥åŒºåˆ†å› æœå› ç´ å’Œè™šå‡å…³è”ï¼Œæœ€ç»ˆå¯¼è‡´RMè„†å¼±ã€ç­–ç•¥å¯¹é½å¤±æ•ˆã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®©RMåœ¨æœªçŸ¥è™šå‡å±æ€§çš„æƒ…å†µä¸‹ï¼Œåªä¾æ‰˜èƒ½è·å–çš„çœŸå®å› æœè´¨é‡å±æ€§æ¥å®ç°é²æ£’è®­ç»ƒï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— è™šå‡æ„ŸçŸ¥çš„å› æœæ¡†æ¶  
æå‡ºåŸºäºå› æœæ¨¡å‹çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ¡†æ¶Cromeï¼Œæ— éœ€é¢„å…ˆæŒ‡å®šæˆ–å¹²é¢„ä»»ä½•è™šå‡å±æ€§ï¼Œä»…é€šè¿‡è°ƒç”¨ â€œ oracle LLM â€ è¯†åˆ«å‡ºçš„å› æœè´¨é‡å‡†åˆ™è¿›è¡Œå¹²é¢„ï¼Œå°±èƒ½å¼•å¯¼RMå­¦ä¹ ï¼Œä»æœºåˆ¶ä¸Šç»•å¼€äº†å¯¹è™šå‡å±æ€§å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå› æœå±æ€§çš„é¶å‘åäº‹å®å¢å¼º  
è®¾è®¡ä¸¤ç±»åˆæˆè®­ç»ƒæ ·æœ¬å¢å¼ºæ–¹å¼ï¼š  
- **å› æœå¢å¼ºï¼ˆCausal Augmentationsï¼‰**ï¼šç”Ÿæˆåœ¨ç‰¹å®šå› æœå±æ€§ï¼ˆå¦‚äº‹å®æ€§ï¼‰ä¸Šæœ‰å·®å¼‚çš„æ ·æœ¬å¯¹ï¼Œè®©RMå¯¹çœŸå®è´¨é‡å˜åŒ–çš„ç»´åº¦äº§ç”Ÿæ•æ„Ÿæ€§ï¼Œç²¾å‡†æ•æ‰å› æœç»´åº¦çš„å½±å“ï¼›  
- **ä¸­æ€§å¢å¼ºï¼ˆNeutral Augmentationsï¼‰**ï¼šåˆ©ç”¨å› æœå¢å¼ºåçš„æ•°æ®ä¸åŸå§‹åå¥½å¯¹ï¼Œç”Ÿæˆåœ¨è™šå‡ç‰¹å¾ï¼ˆå¦‚é£æ ¼ï¼‰ä¸Šå˜åŒ–ä½†å› æœå†…å®¹ä¿ç•™çš„æ ·æœ¬ï¼Œå¹¶æ­é…å¹³å±€æ ‡ç­¾ï¼Œå¼ºåˆ¶RMå¯¹è™šå‡å±æ€§ä¿æŒä¸å˜æ€§ã€‚ä¸”æ•´ä¸ªè¿‡ç¨‹æ— éœ€æ˜¾å¼çŸ¥æ™“è™šå‡å› ç´ ï¼Œä»…é€šè¿‡å› æœå‡†åˆ™å¹²é¢„å°±å¯ç¼“è§£å¯¹å¤§é‡è™šå‡å…³è”çš„æ•æ„Ÿã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨RewardBenchç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCromeè¡¨ç°è¿œè¶…æ ‡å‡†åŸºçº¿ï¼š  
- åœ¨RewardBenchä¸Šå¹³å‡å‡†ç¡®ç‡æå‡æœ€é«˜è¾¾5.4%ï¼Œå…¶ä¸­å®‰å…¨ï¼ˆSafetyï¼‰ç±»åˆ«æå‡13.18%ã€æ¨ç†ï¼ˆReasoningï¼‰ç±»åˆ«æå‡7.19%ï¼›  
- åœ¨Best - of - Næ¨ç†åœºæ™¯ä¸‹ï¼Œé¢å¯¹RewardBenchã€å®‰å…¨ä¸“é¡¹çš„WildGuardTestã€æ¨ç†ä¸“é¡¹çš„GSM8kç­‰åŸºå‡†ï¼Œéšç€Nå¢å¤§ï¼ŒCromeçš„RMåœ¨é€‰æ‹©æœ€ä¼˜ç»“æœæ—¶æŒç»­é¢†å…ˆåŸºçº¿ï¼Œè¯æ˜å…¶åœ¨åº”å¯¹ç¨€æœ‰ï¼ˆé•¿å°¾ï¼‰è™šå‡å› ç´ æ—¶ä¹Ÿå…·å¤‡é²æ£’æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å› æœè§†è§’çš„é²æ£’è®­ç»ƒæ€è·¯ï¼šå°†å› æœå»ºæ¨¡å¼•å…¥å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œä¸ºè§£å†³â€œè™šå‡å…³è”å¹²æ‰°æ¨¡å‹å­¦ä¹ â€è¿™ç±»æ™®éé—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘åç»­åœ¨å…¶ä»–éœ€åŒºåˆ†å› æœä¸ç›¸å…³åœºæ™¯çš„æ¨¡å‹è®­ç»ƒå·¥ä½œï¼›  
2. æ— å…ˆéªŒçš„å¢å¼ºç­–ç•¥ï¼šå±•ç¤ºäº†å¦‚ä½•åœ¨ä¸ä¾èµ–è™šå‡å±æ€§å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡å¯¹å› æœå±æ€§çš„å¹²é¢„æ¥é—´æ¥å‡å°‘è™šå‡å…³è”å½±å“ï¼Œè¿™ç§â€œç»•å¼€æœªçŸ¥ã€å¼ºåŒ–å·²çŸ¥å› æœâ€çš„æ€è·¯åœ¨æ•°æ®å¢å¼ºã€é²æ£’è®­ç»ƒæ–¹å‘æœ‰æ¨å¹¿ä»·å€¼ï¼›  
3. å¤šåœºæ™¯é²æ£’æ€§éªŒè¯ï¼šåœ¨èŠå¤©ã€å®‰å…¨ã€æ¨ç†ç­‰ä¸åŒä»»åŠ¡åŸºå‡†åŠBest - of - Nè¿™ç±»å®é™…æ¨ç†åœºæ™¯ä¸‹éªŒè¯æœ‰æ•ˆæ€§ï¼Œè¯æ˜æ–¹æ³•å…·å¤‡è·¨ä»»åŠ¡ã€è·¨åœºæ™¯çš„æ™®é€‚æ€§ï¼Œä¸ºå·¥ä¸šçº§å¤§æ¨¡å‹å¯¹é½æµç¨‹çš„é²æ£’æ€§ä¼˜åŒ–æä¾›äº†å¯è½åœ°å‚è€ƒã€‚

## relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples
### Abstract
Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RELICï¼šå°æ ·æœ¬ç¤ºä¾‹åŠ©åŠ›ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±æ¨¡å‹æ³›åŒ–èƒ½åŠ›æå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½çš„å…³é”®ï¼Œä½†ç°æœ‰å¼€æºå¤šè¯­è¨€å¥–åŠ±æ¨¡å‹å¤šåŸºäºé«˜èµ„æºè¯­è¨€çš„åå¥½æ•°æ®é›†è®­ç»ƒï¼Œåœ¨ä½èµ„æºå°åº¦è¯­è¨€ä¸Šå¥–åŠ±ä¿¡å·ä¸å¯é ã€‚æ”¶é›†ä½èµ„æºè¯­è¨€å¤§è§„æ¨¡é«˜è´¨é‡åå¥½æ•°æ®æˆæœ¬æé«˜ï¼ŒåŸºäºåå¥½çš„è®­ç»ƒæ–¹æ³•éš¾ä»¥å®æ–½ã€‚åŒæ—¶ç ”ç©¶å‘ç°å¼€æºå¤šè¯­è¨€å¥–åŠ±æ¨¡å‹åœ¨ä½èµ„æºå°åº¦è¯­è¨€ä¸­æ— æ³•å‡†ç¡®åŒºåˆ†å®‰å…¨ä¸ä¸å®‰å…¨å“åº”ï¼Œå¦‚å¯¹ä½èµ„æºè¯­è¨€Bodoï¼Œå¥–åŠ±æ¨¡å‹ç»™ä¸å®‰å…¨å“åº”çš„åˆ†æ•°åè€Œé«˜äºå®‰å…¨å“åº”ï¼Œè¿™å¯¹ä¾èµ–è¿™äº›ä½èµ„æºè¯­è¨€ç¤¾åŒºä½¿ç”¨å®‰å…¨å¯¹é½çš„LLMsæ„æˆæŒ‘æˆ˜ï¼Œå› æ­¤éœ€è¦æ–°æ–¹æ³•æå‡ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±æ¨¡å‹æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRELICæ¡†æ¶  
RELICæ˜¯é’ˆå¯¹ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±å»ºæ¨¡çš„æ–°å‹ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ã€‚å®ƒè®­ç»ƒæ£€ç´¢å™¨æ—¶é‡‡ç”¨ pairwise ranking ç›®æ ‡ï¼Œä»è¾…åŠ©é«˜èµ„æºè¯­è¨€ä¸­é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹èƒ½æœ€æœ‰æ•ˆåœ°çªå‡ºåå¥½å“åº”å’Œæ¬¡åå¥½å“åº”ä¹‹é—´çš„åŒºåˆ«ï¼Œä¸ºå¥–åŠ±æ¨¡å‹æä¾›æœ‰åˆ¤åˆ«æ€§çš„ä¸Šä¸‹æ–‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ£€ç´¢å™¨è®­ç»ƒä¸é€‚ç”¨æ€§  
RELICä½¿ç”¨ pairwise ranking lossï¼ˆåŸºäºThurstoneã€Bradley - Terry ç›¸å…³ç†è®ºï¼‰è®­ç»ƒæ£€ç´¢å™¨ï¼Œä¼˜åŒ–å…¶é€‰æ‹©æœ€èƒ½åŒºåˆ†æ­£å“åº”å’Œè´Ÿå“åº”çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚ä¸”ä¸éœ€è¦è®¿é—®é…å¯¹åå¥½æ•°æ®é›†ï¼Œå¯åº”ç”¨äºä»»ä½•åŒ…å«äºŒå…ƒè´¨é‡æ ‡ç­¾çš„ä½èµ„æºæ•°æ®é›†ï¼Œé€‚ç”¨æ€§å¹¿ï¼Œè¿˜ç»“åˆé«˜èµ„æºè¯­è¨€è¾…åŠ©ç¤ºä¾‹åº“ï¼Œæ¨ç†æ—¶èƒ½æä¾›æ›´ä¸°å¯Œæœ‰åˆ¤åˆ«æ€§çš„ä¸Šä¸‹æ–‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ PKU - SafeRLHFã€WebGPTã€HH - RLHF ä¸‰ä¸ªåå¥½æ•°æ®é›†ä¸Šï¼Œç”¨æœ€å…ˆè¿›çš„å¼€æºå¥–åŠ±æ¨¡å‹è¿›è¡Œå¤§é‡å®éªŒã€‚ç»“æœæ˜¾ç¤ºRELICæ˜¾è‘—æé«˜ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±æ¨¡å‹å‡†ç¡®ç‡ï¼ŒæŒç»­è¶…è¶Šç°æœ‰ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ã€‚å¦‚åœ¨ä½èµ„æºå°åº¦è¯­è¨€Bodoä¸Šï¼Œç”¨LLaMA - 3.2 - 3Bå¥–åŠ±æ¨¡å‹æ—¶ï¼Œç›¸æ¯”é›¶æ ·æœ¬æç¤ºå’Œç°æœ‰æœ€å…ˆè¿›ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡12.81%å’Œ10.13%ï¼›åœ¨Santaliè¯­è¨€ä¸Šï¼ŒåŸºäºLLAMA - 3.1 - 8Bçš„å¥–åŠ±æ¨¡å‹ï¼ŒRELICæ¯”é›¶æ ·æœ¬æç¤ºå‡†ç¡®ç‡æå‡24.16%ï¼Œæ¯”ç°æœ‰åŸºäºç›¸å…³æ€§çš„ç¤ºä¾‹é€‰æ‹©æ–¹æ³•æå‡21.26%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†æè§’åº¦å¯å€Ÿé‰´ï¼šå¯¹ä½èµ„æºè¯­è¨€ä¸Šå¼€æºå¤šè¯­è¨€å¥–åŠ±æ¨¡å‹æ³›åŒ–èƒ½åŠ›åˆ†æï¼Œæ­ç¤ºå“åº”è´¨é‡åŒºåˆ†èƒ½åŠ›çš„å…³é”®ç¼ºå£ï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜é—®é¢˜æ–¹å‘ã€‚
2. æ–¹æ³•åˆ›æ–°å¯å€Ÿé‰´ï¼šRELICæ¡†æ¶é’ˆå¯¹ä½èµ„æºåœºæ™¯ä¸‹æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ£€ç´¢å™¨ç»“åˆï¼Œä¸”åŸºäºå¥–åŠ±æ¨¡å‹æ’åºç›®æ ‡è®­ç»ƒæ£€ç´¢å™¨çš„æ€è·¯ï¼Œä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€ä»»åŠ¡æˆ–éœ€è¦åˆ©ç”¨é«˜èµ„æºè¾…åŠ©æ•°æ®çš„ä»»åŠ¡æä¾›äº†æ–¹æ³•å‚è€ƒã€‚
3. å®éªŒè®¾è®¡å¯å€Ÿé‰´ï¼šåœ¨å¤šä¸ªå…¸å‹åå¥½æ•°æ®é›†å’Œå¼€æºå¥–åŠ±æ¨¡å‹ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢çš„å®éªŒéªŒè¯æ–¹å¼èƒ½æœ‰åŠ›æ”¯æ’‘æ–¹æ³•ä»·å€¼ï¼Œå€¼å¾—ç›¸å…³ç ”ç©¶å®éªŒè®¾è®¡å­¦ä¹ ã€‚

## gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks
### Abstract
Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GFlowGRï¼šç”¨ç”Ÿæˆæµç½‘ç»œä¼˜åŒ–ç”Ÿæˆå¼æ¨èæ¡†æ¶çš„å¾®è°ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”Ÿæˆå¼æ¨èï¼ˆGRï¼‰æ¡†æ¶ç»“åˆç‰©å“ tokenizer å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨è¯¸å¤šåœºæ™¯å–å¾—æˆåŠŸï¼Œä½†ç›®å‰ç ”ç©¶å¤šèšç„¦äºä¼˜åŒ–ç‰©å“ tokenizer æˆ– LLM è§£ç ç­–ç•¥ï¼Œå¯¹ LLM é€‚é…æ¨èæ•°æ®çš„**å…³é”®å¾®è°ƒç¯èŠ‚**æ¢ç´¢ä¸è¶³ã€‚ç°æœ‰å¾®è°ƒæ–¹æ³•ï¼ˆå¦‚ç›‘ç£å¾®è°ƒ SFTã€ç›´æ¥åå¥½ä¼˜åŒ– DPOï¼‰å­˜åœ¨â€œæš´éœ²åå·®â€é—®é¢˜ï¼šSFT åªå…³æ³¨æ•°æ®é›†ä¸­æ­£æ ·æœ¬ï¼Œæ—¢æ²¡å¼•å…¥è´Ÿæ ·æœ¬å¯¹æ¯”ï¼Œä¹Ÿéš¾æ¢ç´¢æ½œåœ¨æ­£æ ·æœ¬ï¼›DPO ç­‰å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•è™½èƒ½åˆ©ç”¨è´Ÿæ ·æœ¬ï¼Œä½†ä¾èµ–å·²æ”¶é›†æ•°æ®ï¼Œæ— æ³•å……åˆ†åæ˜ çœŸå®ç”¨æˆ·åå¥½ï¼Œå¿½ç•¥äº†æœªè§‚æµ‹åˆ°çš„æ½œåœ¨æ­£æ ·æœ¬ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½æ¢ç´¢æ½œåœ¨æ­£æ ·æœ¬ã€ç¼“è§£æš´éœ²åå·®çš„ LLM å¾®è°ƒæ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†ç”Ÿæˆå¼æ¨èå»ºæ¨¡ä¸ºå¤šæ­¥ç”Ÿæˆä»»åŠ¡ï¼Œæå‡º GFlowGR æ¡†æ¶  
æŠŠ GR è§†ä¸ºå¤šæ­¥éª¤ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŸºäºç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰æ„å»ºå¾®è°ƒæ¡†æ¶ GFlowGRã€‚GFlowNets èƒ½è®© LLM æŒ‰ä¸å¥–åŠ±åˆ†å¸ƒæˆæ¯”ä¾‹çš„æ¦‚ç‡ç”Ÿæˆ token åºåˆ—ï¼Œå€Ÿæ­¤è¯†åˆ«æ½œåœ¨æ­£æ ·æœ¬ã€æ‰©å±•æ¨èå¤šæ ·æ€§ï¼Œä»ç”Ÿæˆæ¦‚ç‡å±‚é¢ç¼“è§£æš´éœ²åå·®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è‡ªé€‚åº”è½¨è¿¹é‡‡æ ·å™¨ä¸å¤šç»´åº¦å¥–åŠ±æ¨¡å‹  
- è‡ªé€‚åº”è½¨è¿¹é‡‡æ ·å™¨ï¼šèåˆä¼ ç»Ÿæ¨èç³»ç»Ÿçš„ååŒçŸ¥è¯†ï¼Œç”Ÿæˆâ€œä»æ˜“åˆ°éš¾â€çš„å¢å¼ºè½¨è¿¹ï¼Œç”¨è¯¾ç¨‹å¼è®­ç»ƒæ€è·¯æŒç»­æä¾›é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼›  
- å¤šç»´åº¦å¥–åŠ±æ¨¡å‹ï¼šç»¼åˆè€ƒé‡å¢å¼ºä¿¡å·ã€ååŒåˆ†æ•°ã€è¯­ä¹‰ç›¸ä¼¼åº¦ç­‰ï¼Œæœ‰æ•ˆåŒºåˆ†é‡‡æ ·åˆ°çš„æœªè§‚æµ‹è½¨è¿¹ä¸­â€œç”¨æˆ·å¯èƒ½åå¥½â€çš„ç‰©å“ï¼›  
- ç½®ä¿¡åŠ æƒæœºåˆ¶ï¼šä¾æ®å¢å¼ºä¿¡å·ä¸ååŒåˆ†æ•°çš„ä¸€è‡´æ€§ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ç½®ä¿¡æƒé‡ï¼Œè¿›ä¸€æ­¥ç¼“è§£æš´éœ²åå·®ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨**ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†**ã€åŸºäº**ä¸¤ç§ä¸åŒ GR éª¨å¹²æ¨¡å‹**å¼€å±•å¤§é‡å®éªŒã€‚ç»“æœéªŒè¯äº† GFlowGR åœ¨ç¼“è§£æš´éœ²åå·®ã€æå‡æ¨èæ•ˆæœä¸Šçš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ï¼Œè¯æ˜è¯¥æ¡†æ¶èƒ½åœ¨ä¸åŒæ•°æ®å’Œæ¨¡å‹åº•åº§ä¸‹ç¨³å®šå‘æŒ¥ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨é¢†åŸŸèåˆæ€è·¯ï¼šå°†ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰ä¸æ¨èç³»ç»Ÿç»“åˆï¼Œä¸ºè§£å†³æ¨èåœºæ™¯ä¸­â€œæš´éœ²åå·®â€è¿™ç±»ç‰¹æœ‰é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å‘åç»­è·¨é¢†åŸŸæŠ€æœ¯è¿ç§»ï¼›  
2. è¯¾ç¨‹å¼é‡‡æ ·ä¸å¤šç»´åº¦å¥–åŠ±ï¼šè‡ªé€‚åº”è½¨è¿¹é‡‡æ ·çš„â€œä»æ˜“åˆ°éš¾â€è¯¾ç¨‹è®­ç»ƒã€å¤šç»´åº¦å¥–åŠ±æ¨¡å‹çš„æ„å»ºæ–¹å¼ï¼Œä¸ºå¤„ç†â€œæ•°æ®ç¨€ç–+åå¥½éš¾å»ºæ¨¡â€åœºæ™¯æä¾›äº†å¯å¤ç”¨çš„è®¾è®¡æ€è·¯ï¼›  
3. é—®é¢˜å»ºæ¨¡è§†è§’ï¼šæŠŠæ¨èä»»åŠ¡æ‹†è§£ä¸ºå¤šæ­¥ç”Ÿæˆä»»åŠ¡ï¼Œé‡æ–°å®šä¹‰æ¨èç³»ç»Ÿä¸ç”Ÿæˆå¼æ¨¡å‹çš„äº¤äº’æ–¹å¼ï¼Œä¸ºæ¨èç³»ç»Ÿçš„â€œç”Ÿæˆå¼è½¬å‹â€æä¾›äº†æ–¹æ³•è®ºå‚è€ƒã€‚  

## gram--a-generative-foundation-reward-model-for-reward-generalization
### Abstract
In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GRAMï¼šé¢å‘å¥–åŠ±æ³›åŒ–çš„ç”Ÿæˆå¼åŸºç¡€å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½å·¥ä½œä¸­ï¼Œå¥–åŠ±æ¨¡å‹æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¤šä»¥åˆ¤åˆ«å¼æ¨¡å‹å½¢å¼è®­ç»ƒï¼Œä¸”é«˜åº¦ä¾èµ–å¸¦æ ‡ç­¾çš„äººç±»åå¥½æ•°æ®ã€‚è¿™å¸¦æ¥äº†ä¸¤æ–¹é¢é—®é¢˜ï¼šä¸€æ˜¯å¼ºåŒ–å­¦ä¹ ç®—æ³•å¤æ‚åº¦ä¸æ ‡æ³¨æ•°æ®è·å–éš¾åº¦å¯¼è‡´å¥–åŠ±æ¨¡å‹åº”ç”¨æˆæœ¬é«˜æ˜‚ï¼›äºŒæ˜¯ç°æœ‰è®­ç»ƒæ–¹å¼å¯¹å¤§é‡æ— æ ‡ç­¾æ•°æ®åˆ©ç”¨ä¸è¶³ï¼Œéš¾ä»¥å¾—åˆ°èƒ½çµæ´»é€‚é…å¤šä»»åŠ¡çš„é€šç”¨å¥–åŠ±æ¨¡å‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢åˆ©ç”¨æ— æ ‡ç­¾å’Œæœ‰æ ‡ç­¾æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼Œæ‰“é€ å¯æ³›åŒ–çš„åŸºç¡€å¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µè®­ç»ƒçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¶æ„  
åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆå¼èƒ½åŠ›ï¼Œæ„å»ºç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRAMï¼‰ï¼Œåˆ†ä¸¤é˜¶æ®µè®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µåœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾çš„è¾“å…¥ - å“åº”æ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ è¾“å…¥ä¸å“åº”é—´çš„å¯¹åº”å…³ç³»ï¼Œæ— éœ€åå¥½æ ‡æ³¨æ•°æ®ï¼Œå¯è§„æ¨¡åŒ–è·å–å“åº”æ¯”è¾ƒçš„é€šç”¨çŸ¥è¯†ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨äººç±»åå¥½æ•°æ®è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šé¢„æµ‹ä¸¤ä¸ªå“åº”é—´çš„åå¥½å…³ç³»ã€‚æœ€ç»ˆå¾—åˆ°çš„åŸºç¡€å¥–åŠ±æ¨¡å‹èƒ½ç›´æ¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡æˆ–ä»…ç”¨å°‘é‡ä»»åŠ¡ç‰¹å®šæ•°æ®å¾®è°ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ ‡ç­¾å¹³æ»‘ä¸‹çš„æŸå¤±å‡½æ•°ç»Ÿä¸€è§†è§’  
å¼•å…¥æ ‡ç­¾å¹³æ»‘æŠ€æœ¯åˆ°å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¯æ˜æ­¤æ—¶è®­ç»ƒç›®æ ‡å¯è½¬åŒ–ä¸ºæ­£åˆ™åŒ–çš„ pairwise ranking lossï¼ˆBradley - Terry æŸå¤±ï¼‰å½¢å¼ã€‚è¿™ä¸€æˆæœåœ¨ä¸€å®šç¨‹åº¦ä¸Šç»Ÿä¸€äº†ç”Ÿæˆå¼æ¨¡å‹ä¸åˆ¤åˆ«å¼æ¨¡å‹çš„è®­ç»ƒç›®æ ‡è§†è§’ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°è®¤çŸ¥ï¼Œä¸”æ ‡ç­¾å¹³æ»‘å¯¹è®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ååˆ†æœ‰ç›Šï¼Œæå‡äº†æ¨¡å‹æ³›åŒ–æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å“åº”æ’åºã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€ä»»åŠ¡é€‚é…ç­‰å¤šä»»åŠ¡åœºæ™¯ä¸‹å¼€å±•å¤§é‡å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒGRAM åœ¨å‡ ä¹æ— éœ€æˆ–ä»…éœ€å°‘é‡å¾®è°ƒæ—¶ï¼Œåœ¨å„ä»»åŠ¡ä¸Šæ³›åŒ–æ€§å‡ºè‰²ã€‚ä¾‹å¦‚ï¼ŒåŸºäº LLaMA - 3.1 - 8B - Instruct æ¨¡å‹è®­ç»ƒå¥–åŠ±æ¨¡å‹æ—¶ï¼Œåœ¨ RewardBench å¹³å‡å‡†ç¡®ç‡ä¸Šï¼Œç›¸æ¯”æ™®é€šåˆ¤åˆ«å¼å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åˆ†åˆ«æå‡ 11.0 å’Œ 5.1 ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—è¶…è¶Šå¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆ©ç”¨æ€è·¯ï¼šæ‰“ç ´ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¯¹æœ‰æ ‡ç­¾æ•°æ®çš„å¼ºä¾èµ–ï¼Œç¤ºèŒƒäº†æ— æ ‡ç­¾æ•°æ®åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸ºæ¨¡å‹æ³¨å…¥é€šç”¨çŸ¥è¯†çš„ä»·å€¼ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹ç”šè‡³å…¶ä»–æ¨¡å‹è®­ç»ƒåœ¨æ•°æ®åˆ©ç”¨ä¸Šå¼€è¾Ÿäº†â€œæ— æ ‡ç­¾ + æœ‰æ ‡ç­¾â€ç»“åˆçš„æ€è·¯ã€‚
2. æ¨¡å‹æ¶æ„åˆ›æ–°ï¼šä¸¤é˜¶æ®µçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¶æ„ä¸ºæ‰“é€ é€šç”¨åŸºç¡€å¥–åŠ±æ¨¡å‹æä¾›äº†å¯è¡ŒèŒƒå¼ï¼Œå±•ç¤ºäº†å…ˆæ— ç›‘ç£é¢„è®­ç»ƒå†ç›‘ç£å¾®è°ƒåœ¨å¥–åŠ±æ¨¡å‹é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œå¯å¯å‘åç»­å¤šæ¨¡æ€ã€å…¶ä»–ä»»åŠ¡å¯¼å‘æ¨¡å‹çš„æ¶æ„è®¾è®¡ã€‚
3. æŸå¤±å‡½æ•°ä¸æ­£åˆ™åŒ–ï¼šæ ‡ç­¾å¹³æ»‘ç»“åˆåå¯¹æŸå¤±å‡½æ•°çš„åˆ†æä¸è½¬åŒ–ï¼Œä¸ºç†è§£ç”Ÿæˆå¼å’Œåˆ¤åˆ«å¼æ¨¡å‹åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„è”ç³»æä¾›æ–°è§’åº¦ï¼Œä¹Ÿæé†’å¼€å‘è€…åœ¨æ¨¡å‹è®­ç»ƒä¸­é‡è§†æ­£åˆ™åŒ–æŠ€æœ¯å¯¹æ¨¡å‹æ³›åŒ–ç­‰èƒ½åŠ›çš„æå‡ä½œç”¨ã€‚

## vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training
### Abstract
Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large
language models but remains underexplored for Vision-Language (VL) models. The
Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing
structured feedback, yet training effective VL-RMs faces two major challenges.
First, the bootstrapping dilemma arises as high-quality training data depends
on already strong VL models, creating a cycle where self-generated supervision
reinforces existing biases. Second, modality bias and negative example
amplification occur when VL models hallucinate incorrect visual attributes,
leading to flawed preference data that further misguides training. To address
these issues, we propose an iterative training framework leveraging vision
experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection
Sampling. Our approach refines preference datasets, enhances structured
critiques, and iteratively improves reasoning. Experiments across VL-RM
benchmarks demonstrate superior performance in hallucination detection and
multimodal reasoning, advancing VL model alignment with reinforcement learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VL-GenRMï¼šå€Ÿè§†è§‰ä¸“å®¶ä¸è¿­ä»£è®­ç»ƒï¼Œçªç ´å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è®­ç»ƒå›°å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å·²å–å¾—è¿›å±•ï¼Œä½†åœ¨è§†è§‰-è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹ä¸­ä»å¾…æ·±å…¥æ¢ç´¢ã€‚è§†è§‰-è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼ˆVL-RMï¼‰æ˜¯å¯¹é½VLæ¨¡å‹çš„å…³é”®ï¼Œèƒ½æä¾›ç»“æ„åŒ–åé¦ˆï¼Œç„¶è€Œè®­ç»ƒé«˜æ•ˆVL-RMé¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯â€œè‡ªä¸¾å›°å¢ƒâ€ï¼Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ä¾èµ–å¼ºVLæ¨¡å‹ç”Ÿæˆï¼Œæ˜“å¼ºåŒ–æ¨¡å‹å›ºæœ‰åå·®ï¼›äºŒæ˜¯â€œæ¨¡æ€åå·®ä¸è´Ÿä¾‹æ”¾å¤§â€ï¼ŒVLæ¨¡å‹å¯¹è§†è§‰å±æ€§çš„é”™è¯¯å¹»æƒ³ä¼šäº§ç”Ÿæœ‰ç¼ºé™·çš„åå¥½æ•°æ®ï¼Œè¯¯å¯¼è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºåˆ›æ–°è®­ç»ƒæ¡†æ¶VL-GenRMã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå€Ÿè§†è§‰ä¸“å®¶è‡ªåŠ¨æ„å»ºåå¥½æ•°æ®é›†  
åˆ©ç”¨è§†è§‰ä¸“å®¶ï¼ˆå¦‚æ“…é•¿ç›®æ ‡æ£€æµ‹ã€æ·±åº¦ä¼°è®¡ç­‰çš„æ¨¡å‹ï¼‰ç”Ÿæˆå¤§è§„æ¨¡åå¥½æ•°æ®é›†ï¼Œæå‡VL-GenRMè®­ç»ƒæ—¶çš„ç›‘ç£è´¨é‡ï¼Œæ‰“ç ´â€œè‡ªä¸¾å›°å¢ƒâ€ä¸­ä¾èµ–è‡ªç”Ÿæˆæ•°æ®å¼ºåŒ–åå·®çš„é—®é¢˜ï¼Œä¸ºæ¨¡å‹æä¾›æ›´å¯é çš„è®­ç»ƒä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCoTå¢å¼ºVL-GenRMè®­ç»ƒ  
å¼•å…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºVL-GenRMè®­ç»ƒæä¾›ç³»ç»Ÿæ€§æŒ‡å¯¼ã€‚é€šè¿‡ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¢åŠ æ•°æ®ä¸­æœ‰æ•ˆæ­£ç¡®æè¿°å æ¯”ï¼Œç¼“è§£è‡ªç”Ÿæˆæ•°æ®çš„å±€é™æ€§ï¼Œå¼ºåŒ–å¥–åŠ±å»ºæ¨¡çš„è¿è´¯æ€§ï¼Œè®©æ¨¡å‹å­¦ä¹ æ›´åˆç†çš„è¯„ä¼°é€»è¾‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºè¾¹é™…æ‹’ç»é‡‡æ ·çš„è¿­ä»£è‡ªä¸¾  
é€šè¿‡å¯¹â€œæ­£ä¾‹ä¸è´Ÿä¾‹å¥–åŠ±ä¿¡å·è¾¹é™…â€ç­›é€‰å‡ºçš„ä¼˜è´¨æ¨ç†ä¾æ®ï¼Œè¿›è¡Œè¿­ä»£å¾®è°ƒï¼ŒæŒç»­ä¼˜åŒ–VL-GenRMçš„æ¨ç†èƒ½åŠ›ã€‚è®©æ¨¡å‹åœ¨å¤šè½®è®­ç»ƒä¸­é€æ­¥å‘æ›´ä¼˜è¾“å‡ºé€‚é…ï¼Œä¸æ–­æå‡å¯¹è§†è§‰-è¯­è¨€åœºæ™¯çš„è¯„ä¼°ä¸æ¨ç†æ°´å‡†ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨VL-RMåŸºå‡†æµ‹è¯•ä¸Best-of-Né‡‡æ ·ç­‰å®éªŒä¸­éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œåœ¨å¹»è§‰æ£€æµ‹ï¼ˆè¯†åˆ«VLæ¨¡å‹é”™è¯¯å¹»æƒ³è§†è§‰å±æ€§ç­‰é—®é¢˜ï¼‰ä¸å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´ä¼˜æ€§èƒ½ï¼Œæ¨åŠ¨äº†VLæ¨¡å‹å€ŸåŠ©å¼ºåŒ–å­¦ä¹ å®ç°æ›´å¥½å¯¹é½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨æ¨¡æ€é¢†åŸŸæ•°æ®å¢å¼ºæ€è·¯ï¼šå¼•å…¥é¢†åŸŸä¸“å®¶ï¼ˆå¦‚è§†è§‰ä¸“å®¶ï¼‰è¾…åŠ©æ„å»ºè®­ç»ƒæ•°æ®ï¼Œä¸ºçªç ´â€œè‡ªä¸¾å¾ªç¯â€æä¾›äº†æ–°èŒƒå¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€å¤šæ¨¡æ€åä½œã€ä¾èµ–æ•°æ®è´¨é‡çš„ä»»åŠ¡ã€‚  
2. ç»“æ„åŒ–æ¨ç†èå…¥è®­ç»ƒï¼šå€ŸåŠ©CoTå°†æ¨¡ç³Šçš„è¯„ä¼°è½¬åŒ–ä¸ºå¯è§£é‡Šçš„æ¨ç†æ­¥éª¤ï¼Œä¸ºæå‡æ¨¡å‹å¯è§£é‡Šæ€§ä¸è®­ç»ƒæœ‰æ•ˆæ€§æä¾›äº†å‚è€ƒï¼Œåœ¨å¤æ‚ä»»åŠ¡å‹æ¨¡å‹è®­ç»ƒä¸­å…·å€Ÿé‰´ä»·å€¼ã€‚  
3. è¿­ä»£å¼è®­ç»ƒç­–ç•¥ï¼šåŸºäºå¥–åŠ±è¾¹é™…çš„é‡‡æ ·ä¸è¿­ä»£å¾®è°ƒï¼Œè®©æ¨¡å‹èƒ½åŠ›é€æ­¥è¿­ä»£æå‡ï¼Œè¿™ç§â€œå°æ­¥å¿«è·‘ã€æ•°æ®æ‹©ä¼˜â€çš„è®­ç»ƒé€»è¾‘ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸å¤šè½®ä¼˜åŒ–åœºæ™¯ä¸­å€¼å¾—å¤ç”¨ã€‚  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å‘Šåˆ«ç¹çæ ‡æ³¨ï¼šGAN - RM è®©å¥–åŠ±å»ºæ¨¡â€œä»¥å‡ä¹±çœŸâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒåå¢å¼ºä¸­ï¼Œå¥–åŠ±æ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œå½“å‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•å­˜åœ¨è¯¸å¤šéš¾é¢˜ï¼šä¸€æ˜¯æ„å»ºå¥–åŠ±æ¨¡å‹éœ€å¤§é‡äººå·¥æ ‡æ³¨åå¥½æ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œä¸”åŸºäºç‰¹å®šç”Ÿæˆæ¨¡å‹è¾“å‡ºåŸŸæ ‡æ³¨çš„æ•°æ®ï¼Œåœ¨åº”ç”¨åˆ°ä¸åŒè¾“å‡ºåŸŸæ¨¡å‹æ—¶å­˜åœ¨åŸŸå·®è·ï¼›äºŒæ˜¯ä¸ºå…¨é¢è¯„ä¼°ç”Ÿæˆå†…å®¹è´¨é‡ï¼Œéœ€äººå·¥è®¾è®¡å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¢å¢åŠ å·¥ç¨‹æˆæœ¬ï¼Œåˆéš¾åœ¨ä¸åŒç»´åº¦é—´å–å¾—æœ€ä¼˜å¹³è¡¡ï¼Œè¿˜éš¾ä¿è¯ä¸äººç±»æ™®éåå¥½å¥‘åˆã€‚å› æ­¤ï¼Œæœ¬æ–‡å—ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸­å¯¹æŠ—è®­ç»ƒå¯å‘ï¼Œæå‡º GAN - RM æ¡†æ¶ï¼Œæ—¨åœ¨æ‘†è„±æ‰‹åŠ¨åå¥½æ ‡æ³¨å’Œæ˜¾å¼è´¨é‡ç»´åº¦è®¾è®¡ï¼Œé«˜æ•ˆæ„å»ºå¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— éœ€æ‰‹åŠ¨åå¥½æ ‡æ³¨ï¼Œåˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®
GAN - RM ä»…éœ€å°‘é‡ï¼ˆå‡ ç™¾ä¸ªï¼‰æ— æ ‡æ³¨çš„ä»£è¡¨æ€§æ ·æœ¬ï¼ˆå³åå¥½ä»£ç†æ•°æ®ï¼ŒPreference Proxy Dataï¼‰ä½œä¸ºå¤–éƒ¨æ•°æ®ã€‚é€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹åŒºåˆ†åå¥½ä»£ç†æ•°æ®å’Œç”Ÿæˆæ¨¡å‹è¾“å‡ºï¼Œè®©æ¨¡å‹å­¦ä¹ è¯„ä¼°ç”Ÿæˆæ ·æœ¬ã€‚åŒæ—¶é‡‡ç”¨åŸºäºæ’åçš„è‡ªä¸¾ç­–ç•¥ï¼Œå°† GAN - RM åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„ç½®ä¿¡åˆ†æ•°ä½œä¸ºè½¯æ ‡ç­¾ï¼Œåˆ©ç”¨é¢å¤–æ•°æ®å†è®­ç»ƒ GAN - RMï¼Œä½¿å…¶æ›´å¥½æ•æ‰æ½œåœ¨äººç±»åå¥½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šè½®è®­ç»ƒï¼Œè¿­ä»£å¯¹é½åå¥½
GAN - RM æ”¯æŒå¤šè½®è®­ç»ƒåä¼˜åŒ–ã€‚æ¯ä¸€è½®ä¸­ï¼Œå°†è¢«è¯†åˆ«ä¸ºæ¥è¿‘åå¥½ä»£ç†æ•°æ®çš„æ ·æœ¬ç”¨äºç”Ÿæˆå™¨çš„è®­ç»ƒåä¼˜åŒ–ï¼Œåè¿‡æ¥å†è®­ç»ƒåˆ¤åˆ«å™¨ä»¥åŒºåˆ†è¿™äº›æ›´éš¾çš„æ ·æœ¬ã€‚è¿™ç§è¿­ä»£çš„â€œä»¥å‡ä¹±çœŸâ€è¿‡ç¨‹èƒ½é€æ­¥è®©ç”Ÿæˆè´¨é‡ä¸åå¥½ä»£ç†æ•°æ®ä¸­çš„æ½œåœ¨äººç±»åå¥½å¯¹é½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒåŸºäº GAN - RM çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯ä¸ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼ˆå¦‚ Pickapic çš„ 100 ä¸‡æ ‡æ³¨äººç±»åå¥½æ•°æ®ï¼‰çš„æ–¹æ³•ï¼ˆå¦‚ç›¸å…³å¯¹æ¯”æ–¹æ³•ï¼‰ç›¸å½“ç”šè‡³è¶…è¶Šã€‚åœ¨å›¾åƒè´¨é‡å®éªŒè®¾ç½®ä¸­ï¼ŒGAN - RM ä»…éœ€ 500 ä¸ªåå¥½ä»£ç†æ•°æ®æ ·æœ¬ã€‚é™¤å›¾åƒè´¨é‡æå‡å®éªŒå¤–ï¼Œåœ¨å›¾åƒå®‰å…¨å’Œè§†é¢‘è´¨é‡å¢å¼ºåœºæ™¯ä¸‹çš„å®éªŒä¹Ÿå‡¸æ˜¾äº† GAN - RM æ¡†æ¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆå¦‚ Best - of - N æ ·æœ¬è¿‡æ»¤ï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰è®­ç»ƒåæ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼ŒGAN - RM ä¸ºè§£å†³å¥–åŠ±å»ºæ¨¡ä¸­æ•°æ®è·å–éš¾ã€ä¾èµ–ç‰¹å®šåŸŸã€äººå·¥è®¾è®¡ç»´åº¦éš¾å¥‘åˆäººç±»åå¥½ç­‰é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå…¶åˆ©ç”¨å¯¹æŠ—è®­ç»ƒå’Œå°‘é‡ä»£ç†æ•°æ®çš„æ–¹å¼ï¼Œå‡å°‘äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œé™ä½å·¥ç¨‹æˆæœ¬ï¼›ä»åº”ç”¨æ‹“å±•è§’åº¦ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒã€è§†é¢‘ç­‰å¤šåœºæ™¯çš„æœ‰æ•ˆå®éªŒï¼Œä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è®­ç»ƒåå¢å¼ºæä¾›äº†å¯å¤ç”¨çš„å¥–åŠ±å»ºæ¨¡èŒƒå¼ï¼Œåç»­åœ¨è§†è§‰ç”Ÿæˆç›¸å…³ä»»åŠ¡ä¸­ï¼Œè‹¥éœ€æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œå¯å€Ÿé‰´å…¶åˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®å’Œå¯¹æŠ—è®­ç»ƒçš„æ€è·¯æ¥é™ä½æˆæœ¬ä¸éš¾åº¦ã€‚

## personalized-llm-decoding-via-contrasting-personal-preference
### Abstract
As large language models (LLMs) are progressively deployed in various
real-world applications, personalization of LLMs has become increasingly
important. While various approaches to LLM personalization such as prompt-based
and training-based methods have been actively explored, the development of
effective decoding-time algorithms remains largely overlooked, despite their
demonstrated potential. In this paper, we propose CoPe (Contrasting Personal
Preference), a novel decoding-time approach applied after performing
parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is
to leverage reward-guided decoding specifically for personalization by
maximizing each user's implicit reward signal. We evaluate CoPe across five
open-ended personalized text generation tasks. Our empirical results
demonstrate that CoPe achieves strong performance, improving personalization by
an average of 10.57% in ROUGE-L, without relying on external reward models or
additional training procedures.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£ç é˜¶æ®µä¸ªæ€§åŒ–LLMæ–°èŒƒå¼ï¼šCoPeè®©æ¨¡å‹æ›´æ‡‚ä½ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®åº”ç”¨ä¸­å¹¿æ³›éƒ¨ç½²ï¼ŒLLMçš„ä¸ªæ€§åŒ–å˜å¾—æ„ˆå‘é‡è¦ã€‚ç›®å‰å·²æœ‰åŸºäºæç¤ºï¼ˆprompt - basedï¼‰å’ŒåŸºäºè®­ç»ƒï¼ˆtraining - basedï¼‰çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œä½†è§£ç é˜¶æ®µçš„æœ‰æ•ˆç®—æ³•å¼€å‘å´è¢«å¿½è§†ï¼Œå°½ç®¡å…¶æœ‰å¾ˆå¤§æ½œåŠ›ã€‚åŸºäºæç¤ºçš„æ–¹æ³•ç¼ºä¹å¯¹ç”¨æˆ·æ•°æ®çš„ç›´æ¥å­¦ä¹ ï¼Œæ•ˆæœå—é™ï¼›åŸºäºè®­ç»ƒçš„æ–¹æ³•è™½èƒ½æ›´å¥½æ•æ‰ç”¨æˆ·åå¥½ï¼Œä½†å­˜åœ¨ç¾éš¾æ€§é—å¿˜å’Œè®¡ç®—æˆæœ¬å¢åŠ ç­‰é—®é¢˜ã€‚æ‰€ä»¥æœ¬æ–‡æ—¨åœ¨ä»è§£ç é˜¶æ®µå…¥æ‰‹ï¼Œæå‡ºæ–°æ–¹æ³•å®ç°LLMçš„æœ‰æ•ˆä¸ªæ€§åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCoPeï¼ˆContrasting Personal Preferenceï¼‰è§£ç é˜¶æ®µæ–¹æ³•  
CoPeæ˜¯åœ¨å¯¹ç”¨æˆ·ç‰¹å®šæ•°æ®è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ååº”ç”¨çš„è§£ç é˜¶æ®µæ–°æ–¹æ³•ã€‚å®ƒå±äºå¥–åŠ±å¼•å¯¼è§£ç çš„ä¸€ç§ï¼Œä½†æ— éœ€å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨PEFTè°ƒä¼˜æ¨¡å‹å’ŒåŸå§‹åŸºç¡€æ¨¡å‹çš„ä¼¼ç„¶æ¥è¿‘ä¼¼éšå¼ç”¨æˆ·å¥–åŠ±ä¿¡å·ï¼Œå°†è¿™ç§éšå¼å¥–åŠ±ä¸å¯¹æ¯”è§£ç ç›®æ ‡ç›¸è”ç³»ï¼Œå®ç°å¥–åŠ±å¼•å¯¼è§£ç ç”¨äºä¸ªæ€§åŒ–ï¼Œè®©ç”Ÿæˆæ–‡æœ¬æ›´å¥½åœ°ä¸ç”¨æˆ·åå¥½å¯¹é½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¢å¼ºPEFTæ•æ‰éšå¼ç”¨æˆ·å¥–åŠ±  
é€šè¿‡Direct Preference Optimizationï¼ˆDPOï¼‰å¯¹æ¯”æ­£é¢å“åº”ï¼ˆç”¨æˆ·æä¾›ï¼‰å’Œè´Ÿé¢å“åº”ï¼ˆä¸å¤ªå¯èƒ½æ¥è‡ªç”¨æˆ·ï¼Œå¦‚å…¶ä»–ç”¨æˆ·æˆ–åˆæˆçš„ä½éšå¼å¥–åŠ±è¾“å‡ºï¼‰ä¹‹é—´çš„éšå¼å¥–åŠ±ã€‚ä¸ºé¿å…ä¾èµ–å…¶ä»–ç”¨æˆ·æ•°æ®çš„éšç§å’Œå®é™…æŒ‘æˆ˜ï¼Œç”¨Best - of - Né‡‡æ ·ç”Ÿæˆä½éšå¼å¥–åŠ±çš„åˆæˆè´Ÿé¢å“åº”ã€‚è¿™ç§è®­ç»ƒæ–¹æ³•ä¸ä»…æå‡äº†PEFTçš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºå¥–åŠ±å¼•å¯¼è§£ç æä¾›æ›´å‡†ç¡®çš„éšå¼ç”¨æˆ·å¥–åŠ±å»ºæ¨¡ï¼Œè¿›è€Œæå‡æ•´ä½“ä¸ªæ€§åŒ–æ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¥è‡ªLanguage Model Personalizationï¼ˆLaMPï¼‰å’ŒLongLaMPåŸºå‡†çš„äº”ä¸ªä¸åŒä¸ªæ€§åŒ–å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¯„ä¼°CoPeã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ä»»åŠ¡å¾®è°ƒæ¨¡å‹ç›¸æ¯”ï¼ŒCoPeåœ¨ROUGE - Lä¸Šå¹³å‡ç›¸å¯¹æå‡10.57%ï¼›ä¸ç¼ºä¹å¯¹æ¯”æœºåˆ¶çš„ç®€å•ä¸ªæ€§åŒ–æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨å„ä»»åŠ¡ä¸­ROUGE - Lå¹³å‡æå‡5.67%ã€‚è€Œä¸”CoPeåœ¨ä¸åŒè§„æ¨¡å’Œç±»å‹çš„æœ€å…ˆè¿›LLMsä¸Šæ³›åŒ–æ€§è‰¯å¥½ï¼Œè¯æ˜å…¶éšå¼å¥–åŠ±æœ€å¤§åŒ–èƒ½è¿›ä¸€æ­¥å¢å¼ºä¸ä¸ªä½“ç”¨æˆ·åå¥½çš„å¯¹é½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§£ç é˜¶æ®µä¸ªæ€§åŒ–æ€è·¯ï¼šå¼€æ‹“äº†LLMä¸ªæ€§åŒ–åœ¨è§£ç é˜¶æ®µçš„ç ”ç©¶æ–¹å‘ï¼Œå±•ç¤ºäº†æ— éœ€é¢å¤–è®­ç»ƒæµç¨‹å’Œå¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œä»…åœ¨è§£ç æ—¶åˆ©ç”¨æ¨¡å‹è‡ªèº«ä¼¼ç„¶å¯¹æ¯”å®ç°ä¸ªæ€§åŒ–çš„å¯è¡Œæ€§ï¼Œä¸ºåç»­è§£ç é˜¶æ®µä¸ªæ€§åŒ–æ–¹æ³•æä¾›äº†æ–°æ€è·¯ã€‚
2. PEFTå¢å¼ºæ–¹å¼ï¼šåˆ©ç”¨DPOå’Œåˆæˆè´Ÿé¢æ ·æœ¬å¢å¼ºPEFTæ•æ‰ç”¨æˆ·éšå¼å¥–åŠ±çš„æ–¹å¼ï¼Œä¸ºå‚æ•°é«˜æ•ˆå¾®è°ƒåœ¨ä¸ªæ€§åŒ–åœºæ™¯ä¸‹çš„ä¼˜åŒ–æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼Œåœ¨å¤„ç†ç”¨æˆ·æ•°æ®éšç§å’Œé¿å…ä¾èµ–å¤–éƒ¨æ•°æ®æ–¹é¢ç»™å‡ºäº†åˆ›æ–°è§£æ³•ã€‚
3. å¤šä»»åŠ¡å¤šæ¨¡å‹éªŒè¯ï¼šåœ¨å¤šä¸ªä»»åŠ¡å’Œä¸åŒLLMä¸ŠéªŒè¯æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢çš„å®éªŒè®¾è®¡æ€è·¯ä»¥åŠæ‰€å±•ç°å‡ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºç›¸å…³æ–¹æ³•çš„å®ç”¨æ€§éªŒè¯æä¾›äº†èŒƒä¾‹ï¼Œè®©è¯¥æ–¹æ³•åœ¨å®é™…è½åœ°åˆ°ä¸åŒåœºæ™¯æ—¶æ›´å…·å¯ä¿¡åº¦ã€‚

## med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards
### Abstract
Large language models have shown promise in clinical decision making, but
current approaches struggle to localize and correct errors at specific steps of
the reasoning process. This limitation is critical in medicine, where
identifying and addressing reasoning errors is essential for accurate diagnosis
and effective patient care. We introduce Med-PRM, a process reward modeling
framework that leverages retrieval-augmented generation to verify each
reasoning step against established medical knowledge bases. By verifying
intermediate reasoning steps with evidence retrieved from clinical guidelines
and literature, our model can precisely assess the reasoning quality in a
fine-grained manner. Evaluations on five medical QA benchmarks and two
open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art
performance, with improving the performance of base models by up to 13.50%
using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by
integrating it in a plug-and-play fashion with strong policy models such as
Meerkat, achieving over 80\% accuracy on MedQA for the first time using
small-scale models of 8 billion parameters. Our code and data are available at:
https://med-prm.github.io/
### ğŸŒŸ è®ºæ–‡è§£è¯» | Med - PRMï¼šåŒ»ç–—æ¨ç†æ¨¡å‹çš„â€œæ­¥æ­¥ä¸ºè¥â€æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¸´åºŠå†³ç­–åˆ¶å®šï¼ˆCDMï¼‰æ˜¯ä¸€ä¸ªå¤æ‚å¤šæ­¥éª¤è¿‡ç¨‹ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨åŒ»ç–—åº”ç”¨æœ‰è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾åœ¨æ¨ç†ç‰¹å®šæ­¥éª¤å®šä½å’Œçº æ­£é”™è¯¯ï¼Œè€ŒåŒ»ç–—é¢†åŸŸè¯†åˆ«ä¸è§£å†³æ¨ç†é”™è¯¯å¯¹å‡†ç¡®è¯Šæ–­å’Œæœ‰æ•ˆåŒ»ç–—è‡³å…³é‡è¦ã€‚åŒæ—¶ï¼Œè¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ï¼ˆPRMï¼‰åº”ç”¨äºåŒ»ç–—æœ‰æŒ‘æˆ˜ï¼šä¸€æ˜¯é«˜è´¨é‡æ­¥éª¤çº§ç›‘ç£è·å–æˆæœ¬é«˜ä¸”è´¹åŠ›ï¼Œç°æœ‰è‡ªåŠ¨æ ‡æ³¨ç­–ç•¥æ˜“ä½ä¼°åˆç†ä½†æ²¡å¯¼å‘æ­£ç¡®ç»“æœçš„æ—©æœŸæ­¥éª¤ï¼›äºŒæ˜¯åŒ»ç–—æ¨ç†éœ€å¤§é‡é¢†åŸŸçŸ¥è¯†ï¼Œä»…é è¯­è¨€æ¨¡å‹å‚æ•°éš¾å®Œå…¨æ¶µç›–ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ç¼ºåŒ»ç–—ä¸Šä¸‹æ–‡ä¹Ÿä¸å¤Ÿã€‚è¿™äº›ç—›ç‚¹æ¨åŠ¨äº†Med - PRMçš„æå‡ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMed - PRMæ¡†æ¶
Med - PRMæ˜¯æ£€ç´¢å¢å¼ºçš„è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œé‡‡ç”¨RAG - AS - A - JUDGEæ–¹å¼ï¼ŒåŸºäºä¸´åºŠé—®é¢˜å’Œæ£€ç´¢åˆ°çš„åŒ»ç–—æ–‡æ¡£å¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤åšé€æ­¥è¯„ä¼°ã€‚è¯¥è¯„ä¼°ç›¸æ¯”è®­ç»ƒæ—¶åŸºäºé‡‡æ ·çš„è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ï¼Œæ›´è´´è¿‘ä¸“å®¶åŒ»å¸ˆæ ‡æ³¨ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µèå…¥ä¸´åºŠçŸ¥è¯†ï¼Œèƒ½æ›´ç²¾å‡†è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¼å…·é€šç”¨æ€§ä¸é«˜æ•ˆæ€§
Med - PRMå±•ç°å‡ºå³æ’å³ç”¨çš„é€šç”¨æ€§ï¼Œå¯ä¸Meerkatç­‰å¼ºå¤§ç­–ç•¥æ¨¡å‹ç»“åˆã€‚ä¸”è®­ç»ƒæˆæœ¬é«˜æ•ˆï¼Œå¦‚é’ˆå¯¹èŠ±è´¹çº¦2ä¸‡ç¾å…ƒè®­ç»ƒæ•°æ®çš„UltraMedicalæ¨¡å‹ï¼ŒMed - PRMç”¨èŠ±è´¹ä¸åˆ°20ç¾å…ƒçš„ç²¾å¿ƒç­–åˆ’æ•°æ®é›†è®­ç»ƒï¼Œä»èƒ½æå‡æ€§èƒ½ï¼Œä½“ç°æˆæœ¬æ•ˆç›Šä¸æ‰©å±•æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªåŒ»ç–—QAåŸºå‡†å’Œä¸¤ä¸ªå¼€æ”¾å¼è¯Šæ–­ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒMed - PRMå®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œèƒ½å°†åŸºç¡€æ¨¡å‹æ€§èƒ½æå‡é«˜è¾¾13.50%ã€‚ä¸å¼ºç­–ç•¥æ¨¡å‹ç»“åˆåï¼Œç”¨80äº¿å‚æ•°å°è§„æ¨¡æ¨¡å‹åœ¨MedQAé¦–æ¬¡å®ç°è¶…80%å‡†ç¡®ç‡ï¼Œåœ¨ä¸ƒä¸ªåŒ»ç–—åŸºå‡†ä¸­å…­ä¸ªå®ç°SOTAï¼Œåœ¨MedQAï¼ˆ4é€‰é¡¹ï¼‰ä¸Šç”¨8Bå‚æ•°æ¨¡å‹è¾¾åˆ°80.35%å‡†ç¡®ç‡ï¼Œå¹³å‡æ¯”ç°æœ‰PRMåŸºçº¿åœ¨ä¸ƒä¸ªåŒ»ç–—åŸºå‡†ä¸Šé«˜3.44%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æŠ€æœ¯åˆ›æ–°çœ‹ï¼Œæ£€ç´¢å¢å¼ºç»“åˆè¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ç”¨äºå‚ç›´é¢†åŸŸæ¨ç†è¯„ä¼°æ˜¯å¾ˆå¥½æ€è·¯ï¼Œä¸ºé¢†åŸŸç‰¹å®šçš„LLMä¼˜åŒ–æä¾›æ–¹å‘ï¼›åœ¨å·¥ç¨‹å®è·µä¸Šï¼Œå±•ç¤ºäº†ä½æˆæœ¬æ•°æ®è®­ç»ƒé«˜æ•ˆæ¨¡å‹è¾…åŠ©å¼ºæ¨¡å‹æå‡æ€§èƒ½çš„è·¯å¾„ï¼Œä¸ºèµ„æºæœ‰é™ä½†éœ€æå‡æ¨¡å‹åŒ»ç–—æ¨ç†èƒ½åŠ›çš„åœºæ™¯æä¾›å‚è€ƒï¼›ä»åŒ»ç–—AIå‘å±•è§’åº¦ï¼Œå¼ºè°ƒæ­¥éª¤çº§éªŒè¯å’ŒåŒ»ç–—çŸ¥è¯†èå…¥ï¼Œè®©æ¨¡å‹æ¨ç†æ›´é€æ˜å¯é ï¼Œä¸ºåŒ»ç–—AIè´´è¿‘ä¸´åºŠå®é™…åº”ç”¨æ ‡å‡†æä¾›äº†å®è·µèŒƒå¼ï¼Œåç»­åŒ»ç–—æˆ–å…¶ä»–å‚ç›´é¢†åŸŸçš„æ¨ç†æ¨¡å‹ä¼˜åŒ–å¯å€Ÿé‰´å…¶æ­¥éª¤éªŒè¯ã€çŸ¥è¯†èåˆä¸æˆæœ¬æ§åˆ¶ç­‰æ€è·¯ã€‚

## agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards
### Abstract
Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too sparse for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-RLVRï¼šè®©å¤§æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­â€œæ‹œå¸ˆå­¦è‰ºâ€çš„RLæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦ã€ç«èµ›ç¼–ç¨‹ç­‰å¯éªŒè¯é¢†åŸŸæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ™ºèƒ½ä½“ç¯å¢ƒï¼ˆå¤šæ­¥éª¤ã€å¤æ‚é—®é¢˜æ±‚è§£åœºæ™¯ï¼‰ä¸­æ•ˆæœéª¤é™ã€‚è¿™ç±»åœºæ™¯å¥–åŠ±ç¨€ç–ï¼Œå‰æ²¿LLMä¹Ÿæ˜“é«˜å¤±è´¥ç‡ï¼Œä¼ ç»ŸRLVRéš¾ä»¥æœ‰æ•ˆè®­ç»ƒã€‚åŒæ—¶ï¼Œæ™ºèƒ½ä½“ç¯å¢ƒéœ€å¤šè½®æ¨ç†ã€ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ï¼Œè®­ç»ƒå¤æ‚åº¦é«˜ï¼Œä¸ºè®©RLVRåœ¨å¤æ‚çœŸå®åœºæ™¯ï¼ˆå¦‚è½¯ä»¶å·¥ç¨‹ï¼‰ç”Ÿæ•ˆï¼Œå‚¬ç”Ÿäº†Agent - RLVRæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAgent - RLVRæ¡†æ¶é€‚é…æ™ºèƒ½ä½“åœºæ™¯  
å€Ÿé‰´äººç±»æ•™å­¦æ³•å¼•å…¥â€œagent guidanceï¼ˆæ™ºèƒ½ä½“æŒ‡å¯¼ï¼‰â€æœºåˆ¶ï¼Œåˆ©ç”¨ä»é«˜å±‚æˆ˜ç•¥è§„åˆ’åˆ°é”™è¯¯ä¸ç¯å¢ƒäº¤äº’åŠ¨æ€åé¦ˆç­‰å¤šæ ·ä¿¡æ¯çº¿ç´¢ï¼Œå¼•å¯¼æ™ºèƒ½ä½“èµ°å‘æˆåŠŸè½¨è¿¹ï¼Œåƒè€å¸ˆæŒ‡å¯¼æ–°äººä¸€æ ·å¸®æ™ºèƒ½ä½“åœ¨å¤æ‚è§£ç©ºé—´å¯¼èˆªï¼Œè¿˜èƒ½é€šè¿‡ç¯å¢ƒæ¢ç´¢ä¿ƒè¿›è‡ªæˆ‘æå‡ã€‚è®­ç»ƒå¾ªç¯åˆ†ä¸‰æ­¥ï¼šå…ˆè®©æ™ºèƒ½ä½“æ— æŒ‡å¯¼å°è¯•ç”Ÿæˆåˆå§‹è½¨è¿¹ï¼Œç”¨å•å…ƒæµ‹è¯•éªŒè¯å¹¶è¡¥å……æŒ‡å¯¼ï¼›å†è®©æ™ºèƒ½ä½“å¸¦æŒ‡å¯¼é‡è¯•ï¼›æœ€ååŸºäºæŒ‡å¯¼åè½¨è¿¹å¥–åŠ±ç”¨RLVRæ›´æ–°ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºè½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸“å±æ•°æ®é›†  
ç²¾å¿ƒæ•´ç†å«817ä¸ªè®­ç»ƒç¯å¢ƒçš„æ•°æ®é›†ï¼Œæ¶µç›–é—®é¢˜é™ˆè¿°ã€ç¯å¢ƒå’ŒæŒ‡å¯¼ä¿¡æ¯ï¼Œè¶…è¶Šä¼ ç»Ÿè¾“å…¥ - è¾“å‡ºå¯¹ï¼Œæ•æ‰å¸¦é›†æˆæŒ‡å¯¼ä¿¡å·çš„å®Œæ•´ç¼–ç ç¯å¢ƒï¼Œä¸ºè®­ç»ƒè½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“æä¾›ä¸°å¯Œèµ„æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨SWE - Bench VerifiedåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgent - RLVRå°†Qwen - 2.5 - 72B - Instructçš„pass@1æ€§èƒ½ä»9.4%æå‡è‡³22.4%ï¼›æŒ‡å¯¼å¢å¼ºçš„RLVRæ•°æ®ç”¨äºæµ‹è¯•æ—¶å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œèƒ½è¿›ä¸€æ­¥æŠŠpass@1æ¨è‡³27.8%ï¼›æŒ‡å¯¼æ¨¡å‹åœ¨pass@1ï¼ˆ19.8%â†’22.4%ï¼‰å’Œpass@32ï¼ˆ34.2%â†’38.4%ï¼‰ä¸Šéƒ½æœ‰æå‡ï¼ŒéªŒè¯æŒ‡å¯¼æ˜¯å…³é”®ç»„ä»¶ï¼Œä¹Ÿä½“ç°æ–¹æ³•åœ¨å°æ•°æ®é›†ä¸‹æå‡æ™ºèƒ½ä½“å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åº”å¯¹ç¨€ç–å¥–åŠ±åœºæ™¯æ—¶ï¼Œå¼•å…¥ç±»äººç±»æ•™å­¦çš„æŒ‡å¯¼æœºåˆ¶æ˜¯æœ‰æ•ˆæ€è·¯ï¼Œä¸ºå¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­æ¨¡å‹è®­ç»ƒæä¾›æ–°èŒƒå¼å‚è€ƒã€‚
2. æ„å»ºé¢†åŸŸä¸“å±ã€å«ä¸°å¯Œç¯å¢ƒä¸æŒ‡å¯¼ä¿¡æ¯çš„æ•°æ®é›†ï¼Œèƒ½ä¸ºç‰¹å®šé¢†åŸŸæ™ºèƒ½ä½“è®­ç»ƒç­‘ç‰¢æ•°æ®åŸºç¡€ï¼Œè¿™ç§â€œå®šåˆ¶åŒ– + åœºæ™¯åŒ–â€æ•°æ®æ„å»ºæ€ç»´å€¼å¾—å€Ÿé‰´ã€‚
3. å±•ç¤ºäº†RLVRæ•°æ®åœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒç­‰æ–¹é¢çš„é¢å¤–ä»·å€¼ï¼Œå¯å‘åç»­æ¢ç´¢ä¸åŒæ¨¡å—é—´æ•°æ®å¤ç”¨ä¸ååŒå¢æ•ˆï¼Œæ‹“å±•æŠ€æœ¯åº”ç”¨è¾¹ç•Œã€‚

## reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems
### Abstract
There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReGuidanceï¼šä¸ºå›°éš¾é€†é—®é¢˜æå‡é‡‡æ ·è´¨é‡çš„ç®€æ´æ‰©æ•£åŒ…è£…å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒæ¥è§£å†³é€†é—®é¢˜ä»¥åŠé€šè¿‡å¥–åŠ±æ¨¡å‹å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„ç ”ç©¶ååˆ†æ´»è·ƒã€‚åƒæ‰©æ•£åéªŒé‡‡æ ·ï¼ˆDPSï¼‰è¿™ç±»æ— è®­ç»ƒæ–¹æ³•è™½æä¾›äº†çµæ´»çš„å¯å‘å¼ç®—æ³•ï¼Œä½†å½“å¥–åŠ±ä¿¡æ¯ä¸è¶³æ—¶ï¼ˆå¦‚ä½ä¿¡å™ªæ¯”çš„å›°éš¾é€†é—®é¢˜åœºæ™¯ï¼‰ï¼Œè¿™äº›æŠ€æœ¯ä¼šåç¦»æ•°æ®æµå½¢ï¼Œæ— æ³•ç”Ÿæˆé€¼çœŸè¾“å‡ºã€‚ä¾‹å¦‚åœ¨å¤§åŒºåŸŸå›¾åƒä¿®å¤ã€é«˜å€æ•°è¶…åˆ†è¾¨ç‡ç­‰ç¡¬é€†é—®é¢˜ä¸­ï¼Œç°æœ‰å…ˆè¿›åŸºçº¿æ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œæ—¢éš¾ä¿è¯é‡‡æ ·æ¥è‡ªåˆé€‚çš„å€¾æ–œå¯†åº¦ï¼ˆåéªŒåˆ†å¸ƒï¼‰ï¼Œç”Ÿæˆç»“æœçš„çœŸå®æ„Ÿä¹Ÿå¾ˆå·®ã€‚æœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºæå‡æ ·æœ¬çœŸå®æ„Ÿä¸å¥–åŠ±è¡¨ç°çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReGuidanceæ–¹æ³•  
ReGuidanceæ˜¯ä¸€ä¸ªç®€æ´çš„åŒ…è£…å™¨ç®—æ³•ï¼Œç”¨äºæå‡ç°æœ‰åŸºäºæ‰©æ•£çš„é€†é—®é¢˜æ±‚è§£å™¨çš„æ ·æœ¬è´¨é‡ã€‚å®ƒä»¥ç”¨æˆ·é€‰æ‹©çš„é€†é—®é¢˜æ±‚è§£å™¨ç”Ÿæˆçš„å€™é€‰è§£$\hat{x}$ä¸ºè¾“å…¥ï¼Œåˆ†ä¸¤æ­¥æ“ä½œï¼šç¬¬ä¸€æ­¥ï¼Œä»$\hat{x}$å‡ºå‘åå‘è¿è¡Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹é’ˆå¯¹åŸºç¡€å¯†åº¦$q$çš„ç¡®å®šæ€§é‡‡æ ·å™¨ï¼Œæå–ä¸ä¹‹å…³è”çš„æ½œåœ¨å˜é‡$x^*$ï¼›ç¬¬äºŒæ­¥ï¼Œä»¥$x^*$ä¸ºåˆå§‹åŒ–ï¼Œè¿è¡ŒDiffusion Posterior Samplingï¼ˆDPSï¼‰ç®—æ³•æ¥ç”Ÿæˆæ–°çš„é‡å»ºç»“æœ$x_{\text{DPS}}$ ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºä¿éšœ  
ä»ç†è®ºå±‚é¢è¯æ˜ï¼Œåœ¨æŸäº›å¤šæ¨¡æ€æ•°æ®åˆ†å¸ƒä¸‹ï¼ŒReGuidanceèƒ½åŒæ—¶æå‡å¥–åŠ±å¹¶ä½¿å€™é€‰è§£æ›´æ¥è¿‘æ•°æ®æµå½¢ã€‚è¿™æ˜¯é¦–æ¬¡ä¸ºDPSæä¾›ä¸¥æ ¼çš„ç®—æ³•ä¿éšœï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç†è®ºå±‚é¢çš„éƒ¨åˆ†ç©ºç™½ï¼Œä»æ•°å­¦è§’åº¦æ”¯æ’‘äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å›¾åƒä¿®å¤çš„ä¼ ç»Ÿå›°éš¾é€†ä»»åŠ¡ï¼ˆå¦‚å¤§åŒºåŸŸå›¾åƒä¿®å¤ã€é«˜å€æ•°è¶…åˆ†è¾¨ç‡ï¼‰ä¸­å±•å¼€è¯„ä¼°ï¼šå½“ç°æœ‰å…ˆè¿›åŸºçº¿æ–¹æ³•è¡¨ç°ä¸ä½³æ—¶ï¼Œåœ¨è¿™äº›åŸºçº¿ä¹‹ä¸Šåº”ç”¨ReGuidanceï¼Œèƒ½æ˜¾è‘—æå‡æ ·æœ¬è´¨é‡ä¸æµ‹é‡ä¸€è‡´æ€§ã€‚ä»¥å›¾åƒä¿®å¤ä»»åŠ¡ä¸ºä¾‹ï¼Œå¯¹ä¸åŒåŸºçº¿åº”ç”¨ReGuidanceåï¼Œå¥–åŠ±å’ŒçœŸå®æ„ŸæŒ‡æ ‡éƒ½æœ‰æŒç»­ä¸”æ˜¾è‘—çš„æå‡ï¼›å®šæ€§å±‚é¢è¿˜èƒ½å¾—åˆ°å¤šæ ·ä¸”é€¼çœŸã€ä¸åŸå§‹æµ‹é‡æ ·æœ¬æœ‰æ„ä¹‰åŒºåˆ«çš„é‡å»ºç»“æœã€‚ 

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•è®¾è®¡ç®€æ´æ¨¡å—åŒ–ï¼šReGuidanceçš„ä¸¤æ­¥æ“ä½œæ€è·¯æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œæ•´åˆåˆ°ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„é€†é—®é¢˜æ±‚è§£æµç¨‹ä¸­ï¼Œä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•æä¾›äº†è½»é‡ä½†æœ‰æ•ˆçš„æ€è·¯ã€‚  
2. å…¼é¡¾å®è¯ä¸ç†è®ºï¼šæ—¢é€šè¿‡å®éªŒéªŒè¯åœ¨ç¡¬é€†é—®é¢˜åœºæ™¯ä¸‹å¯¹æ ·æœ¬è´¨é‡çš„æå‡ï¼Œåˆç»™å‡ºç†è®ºè¯æ˜æ”¯æ’‘æ–¹æ³•ä¼˜åŠ¿ï¼Œè¿™ç§ä»å®è·µåˆ°ç†è®ºçš„å®Œæ•´ç ”ç©¶èŒƒå¼å€¼å¾—ç›¸å…³é¢†åŸŸç ”ç©¶å€Ÿé‰´ï¼Œå¸®åŠ©åç»­å·¥ä½œåœ¨æ–¹æ³•åˆ›æ–°æ—¶æ›´æ³¨é‡ç†è®ºä¸å®è¯çš„ç»“åˆã€‚ 
3. é’ˆå¯¹ç¡¬é€†é—®é¢˜åœºæ™¯ï¼šèšç„¦ä½ä¿¡å™ªæ¯”ç­‰ç¡¬é€†é—®é¢˜åœºæ™¯å±•å¼€ç ”ç©¶ï¼Œä¸ºè¿™ç±»é•¿æœŸå›°æ‰°çš„éš¾é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹å‘ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨æ›´å…·æŒ‘æˆ˜æ€§çš„é€†é—®é¢˜åœºæ™¯ä¸‹çš„æ–¹æ³•ä¼˜åŒ–ã€‚ 

## reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning
### Abstract
This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°æ¨¡å‹ä¹Ÿèƒ½æ‰“ï¼å¼ºåŒ–å­¦ä¹ å¾®è°ƒè®©è½»é‡çº§è¯­è¨€æ¨¡å‹ç©è½¬æŒ‡ä»¤éµå¾ªä¸æ•°å­¦æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆé¢†åŸŸå–å¾—äº†äº®çœ¼æˆæœï¼Œä½†å¦‚ä½•è®©å°è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªã€æ•°å­¦æ¨ç†ç­‰ä¸åŒæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ä»æ˜¯éš¾é¢˜ã€‚åŒæ—¶ï¼Œä¸åŒå¾®è°ƒæŠ€æœ¯ï¼ˆå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ ç±»æŠ€æœ¯ï¼‰é—´çš„æ€§èƒ½å¯¹æ¯”ä¹Ÿæœ‰å¾…æ·±å…¥æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶èšç„¦äºè½»é‡çº§å¼€æºæ¨¡å‹Qwen2.5 - 0.5B Baseï¼Œæ¢ç´¢åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•åœ¨åå¥½å¯¹é½ä¸ç‰¹å®šé¢†åŸŸé€‚é…æ–¹é¢çš„è¡¨ç°ï¼Œä»¥æ˜ç¡®è½»é‡çº§æ¨¡å‹åœ¨ç±»äººå¯¹é½å­¦ä¹ åœºæ™¯ä¸‹çš„èƒ½åŠ›ä¸å±€é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šå¾®è°ƒæŠ€æœ¯å¯¹æ¯”ä¸RLOO reward modelæ¢ç´¢  
å¯¹æ¯”äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€Reinforce Leave - One - Outï¼ˆRLOOï¼‰ä¸‰ç§å¾®è°ƒæŠ€æœ¯ã€‚åœ¨RLOOä¸­ï¼Œè¿˜è¯„ä¼°äº†DeBERTaã€DistilBERTã€Siamese DistilBERTç­‰ä¸åŒå¥–åŠ±æ¨¡å‹ï¼Œä»¥æ­¤æ¢ç©¶å¥–åŠ±æ¨¡å‹å¯¹æœ€ç»ˆç­–ç•¥æ€§èƒ½çš„å½±å“ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡çš„æ•°æ®å¢å¼ºä¸æ¨ç†å·¥å…·ç»“åˆ  
ä¸ºæå‡æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŸºäºCountdownæ•°æ®é›†æ„é€ å«1600ä¸ªæ ·æœ¬çš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼ˆå€ŸåŠ©GPT - 4oå®Œæˆé—®é¢˜ç”Ÿæˆä¸ç­”æ¡ˆéªŒè¯ï¼‰ï¼›åŒæ—¶é‡‡ç”¨best - of - Né‡‡æ ·ç­–ç•¥ï¼Œç»“åˆå¤–éƒ¨éªŒè¯å™¨æ¥æå‡æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ä¸æ­£ç¡®æ€§ï¼Œæ¢ç´¢å¾®è°ƒä¸æ¨ç†æ—¶å·¥å…·ç»“åˆçš„æ½œåŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šï¼ŒDPOç›¸æ¯”SFTåœ¨å…¨å‚æ•°ä¸LoRAé…ç½®ä¸‹å‡èƒ½è¿›ä¸€æ­¥æå‡æ•ˆæœï¼›RLOOå˜ä½“é‡Œï¼Œä»¥DeBERTaä¸ºå¥–åŠ±æ¨¡å‹çš„ç‰ˆæœ¬å¯¹é½åˆ†æ•°æœ€é«˜ã€‚æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œåˆæˆæ•°æ®èƒ½å°å¹…æå‡æ€§èƒ½ï¼Œè€Œç»“åˆå¤–éƒ¨éªŒè¯å™¨çš„best - of - Né‡‡æ ·å¸¦æ¥æ˜¾è‘—å¢ç›Šï¼Œå‡†ç¡®ç‡è¶…0.81ï¼Œæ˜¯SFTçš„ä¸¤å€å¤šã€‚æ•´ä½“è¡¨æ˜è½»é‡çº§æ¨¡å‹ç»æœ‰æ•ˆå¾®è°ƒä¸å·¥å…·è¾…åŠ©å¯å®ç°ä¸é”™æ€§èƒ½ï¼Œå¥–åŠ±æ¨¡å‹è´¨é‡ã€é‡‡æ ·å“åº”å¤šæ ·æ€§å¯¹RLOOå¾ˆå…³é”®ï¼Œå¤–éƒ¨éªŒè¯å™¨ + best - of - Né‡‡æ ·ä¸ºæ•°å­¦æ¨ç†æå‡†ç¡®ç‡æä¾›äº†ä½æˆæœ¬æ–¹æ¡ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºæƒ³ä¼˜åŒ–å°æ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„ç ”ç©¶è€…ä¸å¼€å‘è€…ï¼Œå¯å€Ÿé‰´å¤šå¼ºåŒ–å­¦ä¹ å¾®è°ƒæŠ€æœ¯å¯¹æ¯”æ€è·¯ï¼Œæ˜ç¡®ä¸åŒæŠ€æœ¯åœ¨åå¥½å¯¹é½ç­‰åœºæ™¯çš„ä¼˜åŠ£ï¼›åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰ä»»åŠ¡ä¸­ï¼Œå°è¯•åˆæˆæ•°æ®å¢å¼ºä¸æ¨ç†æ—¶å·¥å…·ï¼ˆå¦‚å¤–éƒ¨éªŒè¯å™¨ + best - of - Né‡‡æ ·ï¼‰ç»“åˆçš„æ–¹å¼ï¼Œåœ¨è®¡ç®—èµ„æºå—é™ä¸‹æå‡å°æ¨¡å‹è¡¨ç°ï¼›åŒæ—¶é‡è§†å¥–åŠ±æ¨¡å‹é€‰å‹ä¸é‡‡æ ·å¤šæ ·æ€§ç­‰å› ç´ å¯¹RLOOç±»æ–¹æ³•çš„å½±å“ï¼Œä¸ºè½»é‡çº§è¯­è¨€æ¨¡å‹é€‚é…å¤šä»»åŠ¡æä¾›å®è·µå‚è€ƒã€‚

## dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision
### Abstract
While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DreamCSï¼šæ— é…å¯¹3Då¥–åŠ±ç›‘ç£ä¸‹çš„å‡ ä½•æ„ŸçŸ¥æ–‡æœ¬åˆ°3Dç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬åˆ°3Dç”ŸæˆæŠ€æœ¯åœ¨æ•°å­—æ¼«ç”»ã€æ¸¸æˆã€ç”µå½±å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸå±•ç°å‡ºå…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•ç”Ÿæˆçš„3Dèµ„äº§å¾€å¾€éš¾ä»¥å¥‘åˆäººç±»åå¥½ã€‚å½“å‰3Då†…å®¹çš„åå¥½å¯¹é½æŠ€æœ¯ï¼Œé€šå¸¸ä¾èµ–éš¾æ”¶é›†çš„åå¥½é…å¯¹å¤šè§†è§’2Då›¾åƒæ¥è®­ç»ƒ2Då¥–åŠ±æ¨¡å‹ä»¥æŒ‡å¯¼3Dç”Ÿæˆï¼Œç„¶è€Œè¿™ç§æ–¹å¼å› å›ºæœ‰2Dåå·®æ˜“äº§ç”Ÿå‡ ä½•ä¼ªå½±ã€‚åŒæ—¶ï¼Œæ”¶é›†é…å¯¹çš„åå¥½æ ‡è®°æ•°æ®æˆæœ¬é«˜ã€è€—æ—¶ä¸”å›°éš¾ï¼Œä¸”2Dç©ºé—´çš„åé¦ˆä»…åŸºäºå›¾åƒæ¸²æŸ“è€Œé3Dç»“æ„ï¼Œä¼šå¯¼è‡´å¦‚è§†è§’ä¸ä¸€è‡´ã€Janusè„¸é—®é¢˜ç­‰å‡ ä½•ç¼ºé™·ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½æä¾›å‡ ä½•çº§åé¦ˆä¸”æ‘†è„±é…å¯¹æ•°æ®ä¾èµ–çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»º3D - MeshPrefæ•°æ®é›†
ä¸ºè§£å†³é…å¯¹åå¥½æ ‡è®°3Dæ•°æ®æ”¶é›†éš¾çš„é—®é¢˜ï¼Œæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ— é…å¯¹3Dåå¥½æ•°æ®é›†3D - MeshPrefï¼ŒåŒ…å«14000 + æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰æ–‡æœ¬æç¤ºã€3Dèµ„äº§åŠå…¶åå¥½å¥–åŠ±åˆ†æ•°ã€‚ä»Cap3Dç­›é€‰å¤šæ ·é«˜è´¨é‡ç½‘æ ¼ï¼Œç”¨Llama - Meshå¯¹å‡ ä½•ä¿çœŸåº¦ã€è¯­ä¹‰å¯¹é½å’Œç»“æ„åˆç†æ€§è¯„åˆ†ï¼Œå†ç»äººå·¥éªŒè¯ä¼˜åŒ–åˆ†æ•°ï¼Œå°†é«˜å¥–åŠ±3Dèµ„äº§é€‰ä¸ºåå¥½æ ·æœ¬ï¼Œä½å¥–åŠ±é€‰ä¸ºéåå¥½æ ·æœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºRewardCSå¥–åŠ±æ¨¡å‹
é’ˆå¯¹æ— é…å¯¹æ•°æ®è®­ç»ƒ3Då¥–åŠ±æ¨¡å‹çš„éš¾é¢˜ï¼Œæå‡ºé¦–ä¸ªåŸºäºæ— é…å¯¹æ•°æ®è®­ç»ƒçš„3Då‡ ä½•æ„ŸçŸ¥å¥–åŠ±æ¨¡å‹RewardCSã€‚å¼•å…¥åŸºäºæŸ¯è¥¿ - æ–½ç“¦èŒ¨ï¼ˆCSï¼‰æ•£åº¦çš„åˆ†å¸ƒçº§è®­ç»ƒç›®æ ‡ï¼Œå°†åå¥½å’Œéåå¥½èµ„äº§çš„åµŒå…¥è§†ä¸ºä¸¤ä¸ªåˆ†å¸ƒçš„æ ·æœ¬ï¼Œä¼˜åŒ–å®ƒä»¬ä¹‹é—´çš„CSæ•£åº¦ï¼Œä½¿æ¨¡å‹ç»™å‡ ä½•å’Œè¯­ä¹‰æ›´ä¼˜çš„3Dèµ„äº§æ›´é«˜å¥–åŠ±ã€‚è¿˜è¯æ˜äº†æ— é…å¯¹æ•°æ®ä¸Šçš„CSæ•£åº¦ä¸ä¼ ç»Ÿé…å¯¹åå¥½ç›‘ç£æ¸è¿‘ç­‰ä»·ï¼Œä¸ºæ— é…å¯¹å¥–åŠ±å­¦ä¹ èŒƒå¼æä¾›ç†è®ºä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºDreamCSæ¡†æ¶
ç°æœ‰æ–‡æœ¬åˆ°3Dæ¡†æ¶ç¼ºä¹å‡ ä½•çº§åé¦ˆåŸç”Ÿæ”¯æŒï¼Œä¸ºæ­¤å¼€å‘DreamCSï¼Œé¦–ä¸ªé›†æˆRewardCSåˆ°ç°æœ‰æ–‡æœ¬åˆ°3Dç®¡çº¿çš„3Då¥–åŠ±å¼•å¯¼æ¡†æ¶ã€‚é€šè¿‡ä¸‰é¡¹åˆ›æ–°å®ç°ä¸éšå¼å’Œæ˜¾å¼3Dè¡¨ç¤ºæ— ç¼åä½œï¼šå¯å¾®åˆ†ç½‘æ ¼åŒ–ä½¿éšå¼åœºåˆ°ç«¯åˆ°ç«¯æ¢¯åº¦æµæˆä¸ºå¯èƒ½ï¼›è‡ªé€‚åº”ç½‘æ ¼èåˆåœ¨ä¸ç‰ºç‰²å‡ ä½•ç»†èŠ‚çš„æƒ…å†µä¸‹ä¼˜åŒ–æ‹“æ‰‘ä»¥é€‚é…RewardCSï¼›æ¸è¿›å¼å¥–åŠ±å¼•å¯¼è‡ªåŠ¨å¹³è¡¡ç²—ç»“æ„ç»†åŒ–å’Œç»†ç²’åº¦ä¼˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GPTEval3DåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹äºMVDreamã€Dreamfusionã€Magic3Dç­‰å•é˜¶æ®µå’Œä¸¤é˜¶æ®µæ–‡æœ¬åˆ°3Dç”Ÿæˆç®¡çº¿ï¼Œé›†æˆDreamCSååœ¨å‡ ä½•å¯¹é½å’Œç½‘æ ¼è´¨é‡æ–¹é¢æ¯”ä¹‹å‰å¤šè§†è§’å¼•å¯¼æ–¹æ³•å–å¾—æ›´ä¼˜ç»“æœã€‚ä¸”3Då¥–åŠ±å¼•å¯¼ä¸ç°æœ‰2Dæ–¹æ³•äº’è¡¥ï¼Œç»“åˆåèƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºæ€è·¯ï¼šé¢å¯¹æ•°æ®æ”¶é›†éš¾é¢˜æ—¶ï¼Œå¯è€ƒè™‘æ„å»ºå¤§è§„æ¨¡æ— é…å¯¹ä¸”ç»“åˆå¤§è¯­è¨€æ¨¡å‹åˆç­›ä¸äººå·¥ç²¾ä¿®çš„æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›é«˜è´¨é‡æ•°æ®æ”¯æ’‘ã€‚
2. æ¨¡å‹è®­ç»ƒç›®æ ‡åˆ›æ–°ï¼šåœ¨å¤„ç†æ— é…å¯¹æ•°æ®åœºæ™¯æ—¶ï¼Œå€Ÿé‰´åŸºäºåˆ†å¸ƒçº§çš„è®­ç»ƒç›®æ ‡ï¼ˆå¦‚CSæ•£åº¦ï¼‰æ€è·¯ï¼Œæ‘†è„±é…å¯¹æ¯”è¾ƒä¾èµ–ï¼Œæ‹“å±•æ¨¡å‹åœ¨æ— é…å¯¹æ•°æ®ä¸‹çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. æ¡†æ¶é›†æˆåˆ›æ–°ï¼šé’ˆå¯¹é¢†åŸŸå†…ç°æœ‰æ¡†æ¶çŸ­æ¿ï¼ˆå¦‚ç¼ºä¹å‡ ä½•çº§åé¦ˆæ”¯æŒï¼‰ï¼Œé€šè¿‡å¤šé¡¹é’ˆå¯¹æ€§åˆ›æ–°ï¼ˆå¯å¾®åˆ†ç½‘æ ¼åŒ–ã€è‡ªé€‚åº”ç½‘æ ¼èåˆã€æ¸è¿›å¼å¥–åŠ±å¼•å¯¼ç­‰ï¼‰å®ç°æ–°ç»„ä»¶ä¸ç°æœ‰ç®¡çº¿çš„æ— ç¼é›†æˆï¼Œä¸ºç±»ä¼¼é¢†åŸŸå†…æ¡†æ¶å‡çº§æä¾›å‚è€ƒèŒƒå¼ã€‚

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Athenaï¼šç”¨æ•°æ®é«˜æ•ˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œå¤šæ­¥éª¤æ¨ç†ï¼‰ä»å…·æŒ‘æˆ˜ã€‚ä¸ºå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ç­‰æ–¹æ³•è¢«æ¢ç´¢ï¼Œå…¶ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰èƒ½ä¸ºä¸­é—´æ¨ç†æ­¥éª¤æä¾›ç»†ç²’åº¦åé¦ˆï¼Œæ€§èƒ½æ›´ä¼˜ä¸”æ³›åŒ–æ€§å¼ºã€‚ç„¶è€Œï¼ŒPRMs å‘å±•é¢ä¸´ä¸¤å¤§éš¾é¢˜ï¼šä¸€æ˜¯è·å–å¸¦è¿‡ç¨‹æ ‡ç­¾çš„é«˜è´¨é‡æ•°æ®æˆæœ¬é«˜ï¼ˆéœ€å¤§é‡äººå·¥æ ‡æ³¨æˆ–è®¡ç®—æ˜‚è´µçš„è‡ªåŠ¨åŒ–æ ‡æ³¨ï¼‰ï¼›äºŒæ˜¯ä¼ ç»Ÿè‡ªåŠ¨åŒ–æ ‡æ³¨ï¼ˆå¦‚è’™ç‰¹å¡æ´›ä¼°è®¡ï¼‰æ˜“äº§ç”Ÿå™ªå£°æ ‡ç­¾ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶å‡è½»æ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œæå‡ PRMs æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¼ºå¼±å®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç”Ÿæˆé«˜è´¨é‡è¿‡ç¨‹æ ‡ç­¾  
ä¼ ç»Ÿè’™ç‰¹å¡æ´›ç­‰è‡ªåŠ¨åŒ–æ ‡æ³¨æ–¹æ³•æ˜“å—å®Œæˆå™¨æ¨ç†èƒ½åŠ›å½±å“ï¼Œæ ‡ç­¾æœ‰å™ªå£°ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡å‘ç°ï¼Œå¼ºå®Œæˆå™¨å³ä¾¿ä¸­é—´æ­¥éª¤é”™è¯¯ä»èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œå¼±å®Œæˆå™¨åˆ™å¯èƒ½åœ¨ä¸­é—´æ­¥éª¤æ­£ç¡®æ—¶ä¹Ÿå¤±è´¥ã€‚åŸºäºæ­¤ï¼Œæå‡ºç”¨å¼±ã€å¼ºå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ä½œä¸ºç­›é€‰å¯é è¿‡ç¨‹æ ‡ç­¾çš„æ ‡å‡†ï¼Œä¿ç•™ä¸¤è€…æ ‡ç­¾ä¸€è‡´çš„æ­¥éª¤ï¼Œå‡å°‘å®Œæˆå™¨å¸¦æ¥çš„åå·®ï¼Œæå‡æ ‡ç­¾è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œçº¦ 5000 æ¡é«˜è´¨é‡æ ‡ç­¾å°±èƒ½æ¯”ä¼ ç»Ÿæ–¹æ³•çº¦ 30 ä¸‡æ¡å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¡¨ç°æ›´ä¼˜ï¼Œä¸”å¤§å¹…é™ä½æ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒçš„è®¡ç®—æˆæœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ PRMs æ€§èƒ½çš„ä¸¤å¤§ç­–ç•¥  
 - ORM åˆå§‹åŒ–ï¼šPRMs é€šå¸¸åŸºäºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¾®è°ƒï¼Œè€Œç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰åœ¨å¤§è§„æ¨¡å“åº”çº§æ•°æ®ä¸Šè®­ç»ƒï¼Œå…·å¤‡å¼±ç›‘ç£ä¸‹è¯„ä¼°ä¸­é—´æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚å› æ­¤ç”¨ ORMs åˆå§‹åŒ– PRMsï¼Œå°† ORMs ä½œä¸ºå¼±ç›‘ç£é¢„è®­ç»ƒï¼ŒPRMs å†åœ¨é«˜è´¨é‡ç»†ç²’åº¦æ­¥éª¤æ•°æ®ä¸Šå¾®è°ƒï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚  
 - è´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ï¼šè¿‡ç¨‹æ ‡ç­¾æ•°æ®å­˜åœ¨æ ‡ç­¾ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡å¯¹å«è´Ÿæ­¥éª¤æ ‡ç­¾çš„æ•°æ®è¿›è¡Œä¸Šé‡‡æ ·ï¼Œè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡é—®é¢˜ï¼Œä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»º Athena ç³»åˆ—æ¨¡å‹å¹¶å¤šåœºæ™¯éªŒè¯  
åŸºäºä¸Šè¿°æ–¹æ³•æ„å»ºç»“æœå¥–åŠ±æ¨¡å‹ Athena - ORM å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ Athena - PRMï¼Œå†åˆ©ç”¨ Athena - PRM é€šè¿‡å¥–åŠ±æ’åºå¾®è°ƒå¾—åˆ° Athena - 7Bã€‚å¹¶åœ¨ä¸‰ä¸ªåœºæ™¯éªŒè¯ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­å¯¹ç­–ç•¥æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªè¾“å‡ºæ’åºï¼›ç›´æ¥è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§ï¼›å¥–åŠ±æ’åºå¾®è°ƒï¼ˆç”¨é«˜å¥–åŠ±å“åº”å¾®è°ƒç­–ç•¥æ¨¡å‹ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- æµ‹è¯•æ—¶ç¼©æ”¾åœºæ™¯ï¼šåœ¨ 7 ä¸ªå¤šæ¨¡æ€æ•°å­¦å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç”¨ Athena - PRM é…åˆä¸åŒè§„æ¨¡ï¼ˆ7B åˆ° 72Bï¼‰ç­–ç•¥æ¨¡å‹ï¼Œæ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ã€‚å¦‚ç”¨ Qwen2.5 - VL - 7B ä½œä¸ºç­–ç•¥æ¨¡å‹æ—¶ï¼Œåœ¨ WeMath åŸºå‡†ä¸Šé›¶æ ·æœ¬åŸºçº¿æå‡ 10.2 åˆ†ï¼Œåœ¨ MathVista æå‡ 7.1 åˆ†ï¼›åœ¨æ–‡æœ¬-only æ•°å­¦åŸºå‡†ç”¨ Mistral - 8B æ—¶æå‡ 8.9 åˆ†ã€‚  
- æ¨ç†æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°åœºæ™¯ï¼šåœ¨ VisualProcessBench åŸºå‡†ä¸Šï¼ŒAthena - PRM è¡¨ç°å¼ºåŠ²ï¼Œè¶…è¶Šå¼€æºçš„ VisualPRM - 8B ç­‰æ¨¡å‹ï¼ŒF1 åˆ†æ•°æ¯”ä¹‹å‰æœ€ä¼˜ç»“æœé«˜ 3.9ï¼Œå±•ç°å‡†ç¡®è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚  
- å¥–åŠ±æ’åºå¾®è°ƒåœºæ™¯ï¼šåŸºäº Qwen2.5 - VL - 7B å¾®è°ƒå¾—åˆ°çš„ Athena - 7Bï¼Œåœ¨ 7 ä¸ªæ•°å­¦å’Œæ¨ç†åŸºå‡†ä¸Šå¤§å¹…æå‡ç­–ç•¥æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ•°æ®é«˜æ•ˆæ ‡æ³¨æ€è·¯ï¼šåˆ©ç”¨å¤šå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç­›é€‰æ ‡ç­¾ï¼Œä¸ºè§£å†³éœ€ç»†ç²’åº¦æ ‡æ³¨ä¸”æ ‡æ³¨æˆæœ¬é«˜çš„ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ï¼Œåœ¨å‡å°‘æ•°æ®é‡åŒæ—¶æå‡æ•°æ®è´¨é‡ï¼Œå®ç°æ•°æ®é«˜æ•ˆåˆ©ç”¨ã€‚  
- æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼šORM åˆå§‹åŒ–å’Œè´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ç­–ç•¥ï¼Œä¸ºæå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½æä¾›äº†å¯å¤ç”¨æ–¹æ³•ï¼Œå¯å¯å‘å…¶ä»–å¥–åŠ±æ¨¡å‹æˆ–éœ€ç»†ç²’åº¦åé¦ˆæ¨¡å‹çš„è®­ç»ƒä¼˜åŒ–ã€‚  
- å¤šåœºæ™¯éªŒè¯æ¨¡å¼ï¼šåœ¨æµ‹è¯•æ—¶ç¼©æ”¾ã€æ­¥éª¤è¯„ä¼°ã€æ¨¡å‹å¾®è°ƒç­‰å¤šåœºæ™¯éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢éªŒè¯æ€è·¯æœ‰åŠ©äºæ›´å……åˆ†å±•ç¤ºæ–¹æ³•ä»·å€¼ï¼Œä¸ºåç»­ç ”ç©¶æä¾›éªŒè¯èŒƒå¼å‚è€ƒã€‚

## gfriend--generative-few-shot-reward-inference-through-efficient-dpo
### Abstract
The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GFRIENDï¼šå°æ ·æœ¬ä¸‹é«˜æ•ˆDPOå®ç°ç”Ÿæˆå¼å¥–åŠ±æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆæœï¼Œä½†è®©æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½ä»æ˜¯æŒ‘æˆ˜ã€‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯å…³é”®æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ç”¨äººç±»æ ‡æ³¨çš„åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚ç„¶è€Œç°æœ‰æ–¹æ³•å­˜åœ¨æ ·æœ¬é…å¯¹ä½æ•ˆã€åå¥½æ•°æ®å¤šæ ·æ€§æœ‰é™ç­‰é—®é¢˜ï¼Œåœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸï¼Œå¤§è§„æ¨¡åå¥½æ•°æ®æ”¶é›†å—é™ï¼Œä½æ•°æ®ç¯å¢ƒä¸‹è®­ç»ƒå¥–åŠ±æ¨¡å‹æˆéš¾é¢˜ï¼Œå› æ­¤éœ€è¦é«˜æ•ˆåˆ©ç”¨æœ‰é™åå¥½æ•°æ®çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜æ•ˆæ•°æ®å¢å¼º  
å¼•å…¥æ€ç»´é“¾ï¼ˆChain - of - Thought, CoTï¼‰é‡‡æ ·æœºåˆ¶ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„åå¥½æ•°æ®ï¼Œç¼“è§£æ•°æ®ç¨€ç–æ€§é—®é¢˜ï¼Œæå‡ä½èµ„æºæ¡ä»¶ä¸‹å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§ã€‚é€šè¿‡CoTé‡‡æ ·æŒ–æ˜ä¸°å¯Œä¼˜è´¨çš„åå¥½å…³ç³»ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›æ›´å……è¶³ä¸”ä¼˜è´¨çš„æ•°æ®åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ°´å¹³åå¥½å»ºæ¨¡  
æå‡ºåŸºäºå›°æƒ‘åº¦çš„è¯„åˆ†æ–¹æ³•æ¥åˆ†é…ç»†è‡´çš„åå¥½ç­‰çº§ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„äºŒå…ƒé…å¯¹æ–¹æ³•ï¼Œèƒ½å®ç°æ›´ç»†ç²’åº¦çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚åˆ©ç”¨è¯¥è¯„åˆ†æœºåˆ¶é‡åŒ–åå¥½ç¨‹åº¦ï¼Œè®©æ¨¡å‹æ•æ‰æ ·æœ¬é—´æ›´ç»†å¾®çš„åå¥½å·®å¼‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¼˜åŒ–åå¥½å­¦ä¹   
é€šè¿‡ç»“åˆåŸºäºåå¥½å·®å¼‚çš„åŠ æƒæ ·æœ¬å¯¹æ¥å¢å¼ºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æŸå¤±å‡½æ•°ï¼Œåœ¨ä½æ•°æ®è®¾ç½®ä¸‹æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚ä¾æ®åå¥½å·®å¼‚å¯¹æ ·æœ¬å¯¹åŠ æƒï¼Œç¡®ä¿è®­ç»ƒæ—¶æ›´å…³æ³¨æœ‰ä»£è¡¨æ€§çš„æ•°æ®ï¼Œä¼˜åŒ–å¥–åŠ±æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°è¯¥æ¡†æ¶ï¼Œä¸ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡æ–¹æ³•å¯¹æ¯”ã€‚å®éªŒè¯å®æ•°æ®å¢å¼ºæé«˜äº†åå¥½å»ºæ¨¡å‡†ç¡®æ€§ï¼Œå¤šæ°´å¹³åå¥½è¯„åˆ†æœ‰æ•ˆã€‚æ¶ˆèå®éªŒæ­ç¤ºå„ç»„ä»¶çš„è´¡çŒ®ï¼Œç»“æœè¡¨æ˜åœ¨ä½èµ„æºè®¾ç½®ä¸‹æ€§èƒ½æå‡æ˜¾è‘—ï¼Œèƒ½ä¸åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ç›¸å½“ï¼Œå‡¸æ˜¾äº†æ•°æ®é«˜æ•ˆç­–ç•¥åœ¨æ¨è¿›RLHFä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æ•°æ®åˆ©ç”¨å±‚é¢ï¼Œå…¶é«˜æ•ˆæ•°æ®å¢å¼ºæ€è·¯ä¸ºä½èµ„æºåœºæ™¯ä¸‹çš„æ•°æ®æ‰©å……æä¾›äº†æ–°æ–¹å¼ï¼Œå€ŸåŠ©CoTé‡‡æ ·ç”Ÿæˆä¼˜è´¨æ•°æ®çš„æ–¹æ³•å¯è¢«å€Ÿé‰´åˆ°å…¶ä»–éœ€è¦æ•°æ®å¢å¼ºçš„ä»»åŠ¡ä¸­ï¼›åœ¨æ¨¡å‹è®­ç»ƒå±‚é¢ï¼Œå¤šæ°´å¹³åå¥½å»ºæ¨¡å’Œä¼˜åŒ–æŸå¤±å‡½æ•°çš„æ€è·¯ï¼Œä¸ºæå‡æ¨¡å‹å¯¹ç»†ç²’åº¦å·®å¼‚çš„æ•æ‰èƒ½åŠ›ã€å¢å¼ºæ¨¡å‹åœ¨ä½æ•°æ®ä¸‹çš„æ³›åŒ–æ€§æä¾›äº†å‚è€ƒï¼Œå¯¹äºèµ„æºå—é™ä½†éœ€é«˜ç²¾åº¦æ¨¡å‹è®­ç»ƒçš„åœºæ™¯æœ‰å¾ˆå¥½çš„å€Ÿé‰´ä»·å€¼ï¼Œæ¯”å¦‚å°ä¼—é¢†åŸŸçš„AIåº”ç”¨å¼€å‘ç­‰ã€‚

## saffron-1--safety-inference-scaling
### Abstract
Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .
### ğŸŒŸ è®ºæ–‡è§£è¯» | Saffron-1ï¼šå®‰å…¨æ¨ç†ç¼©æ”¾ï¼Œç­‘ç‰¢å¤§æ¨¡å‹å®‰å…¨é˜²çº¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¿«é€Ÿå‘å±•ä¸å¹¿æ³›åº”ç”¨å¸¦æ¥äº†æ–°å®‰å…¨é£é™©ï¼Œæœ‰å®³è¾“å‡ºåœ¨å®é™…åº”ç”¨ä¸­åæœä¸¥é‡ã€‚ç°æœ‰å®‰å…¨ä¿éšœç ”ç©¶å¤šèšç„¦è®­ç»ƒé˜¶æ®µå¯¹é½ä»¥æ¤å…¥å®‰å…¨è¡Œä¸ºï¼Œä½†è¿‘æœŸç ”ç©¶æš´éœ²å…¶æ˜“å—è¶Šç‹±æ”»å‡»çš„å¼±ç‚¹ã€‚åŒæ—¶ï¼Œæ¨ç†ç¼©æ”¾å¤§å¹…æå‡äº†LLMæ¨ç†èƒ½åŠ›ï¼Œå´åœ¨å®‰å…¨ä¿éšœé¢†åŸŸæœªè¢«æ¢ç´¢ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æ¢ç´¢æ¨ç†ç¼©æ”¾ä½œä¸ºåº”å¯¹æ–°å¨èƒã€å»ºç«‹æ›´å¼ºå®‰å…¨ä¿éšœçš„æ–°æ–¹å‘ï¼Œæ ¸å¿ƒé—®é¢˜æ˜¯â€œæ¨ç†ç¼©æ”¾åœ¨LLMå®‰å…¨ä¿éšœä¸­èƒ½æœ‰å¤šå¼ºï¼Ÿâ€ ä¸”ç°æœ‰å…ˆè¿›æ¨ç†ç¼©æ”¾æ–¹æ³•ç”¨äºå®‰å…¨ä¿éšœæ—¶ï¼Œç¼©æ”¾æ•ˆç‡ç”šè‡³ä¸å¦‚æœ€åŸºç¡€çš„Best - of - Né‡‡æ ·ï¼ŒåŸå› æ˜¯å­˜åœ¨â€œæ¢ç´¢ - æ•ˆç‡å›°å¢ƒâ€â€”â€”é¢‘ç¹è°ƒç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¸¦æ¥é«˜è®¡ç®—å¼€é”€ï¼Œæ¢ç´¢è¶Šå¤šå¥–åŠ±æ¨¡å‹è°ƒç”¨è¶Šå¤šï¼Œç¼©æ”¾æ•ˆç‡è¶Šå·®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSAFFRONæ¨ç†ç¼©æ”¾èŒƒå¼
ä¸ºè§£å†³â€œæ¢ç´¢ - æ•ˆç‡å›°å¢ƒâ€ï¼Œæå‡ºé¦–ä¸ªé’ˆå¯¹LLMå®‰å…¨ä¿éšœçš„æ¨ç†ç¼©æ”¾èŒƒå¼SAFFRONã€‚æ ¸å¿ƒæ˜¯å¼•å…¥å¤šåˆ†æ”¯å¥–åŠ±æ¨¡å‹ï¼ˆMRMï¼‰æ›¿ä»£è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œå¤§å¹…å‡å°‘å¥–åŠ±æ¨¡å‹è°ƒç”¨æ¬¡æ•°ã€‚å¦‚ä¼ ç»ŸPRMåœ¨æ ‘æœç´¢æ­¥éª¤ä¸­éœ€å¯¹æ¯ä¸ªå€™é€‰next - tokenå¯¹åº”çš„åºåˆ—å¤šæ¬¡è°ƒç”¨PRMï¼Œè€ŒMRMåªéœ€å¯¹å½“å‰åºåˆ—è°ƒç”¨ä¸€æ¬¡ï¼Œå°±èƒ½å¾—åˆ°å„next - tokenå¯¹åº”çš„å¥–åŠ±å€¼ï¼Œå‡å°‘è®¡ç®—å¼€é”€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMRMè®­ç»ƒä¸è¾…åŠ©ç­–ç•¥
 - æå‡ºMRMçš„éƒ¨åˆ†ç›‘ç£è®­ç»ƒç›®æ ‡ï¼Œç”¨äºè®­ç»ƒå¤šåˆ†æ”¯å¥–åŠ±æ¨¡å‹ï¼Œä½¿å…¶èƒ½æœ‰æ•ˆä¸ºå¤šä¸ªå€™é€‰åˆ†æ”¯æä¾›å¥–åŠ±è¯„ä¼°ã€‚
 - å¼•å…¥ä¿å®ˆæ¢ç´¢çº¦æŸï¼Œé˜²æ­¢åˆ†å¸ƒå¤–æ¢ç´¢ï¼Œç¡®ä¿æ¨¡å‹æ¢ç´¢åœ¨å®‰å…¨ç›¸å…³çš„åˆç†èŒƒå›´å†…ï¼Œé¿å…å› æ— æ„ä¹‰æ¢ç´¢å¢åŠ å¼€é”€ä¸é£é™©ã€‚
 - è®¾è®¡åŸºäºTrieçš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ç­–ç•¥ï¼Œåœ¨æ ‘æœç´¢è¿‡ç¨‹ä¸­å®ç°åºåˆ—é—´çš„KVç¼“å­˜å…±äº«ï¼Œæå‡è®¡ç®—æ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨å®‰å…¨ä¿éšœåœºæ™¯ä¸‹ï¼Œç°æœ‰å…ˆè¿›æ¨ç†ç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚DeALï¼ˆMCTSï¼‰ã€Rebaseï¼ˆBeam Searchï¼‰ï¼‰ç¼©æ”¾æ•ˆç‡ä¸å¦‚åŸºç¡€çš„Best - of - Né‡‡æ ·ï¼›è€Œæå‡ºçš„Saffron - 1æ–¹æ³•æ˜¾è‘—ä¼˜äºBest - of - Nï¼Œå³ä¾¿Best - of - Né…å¤‡äº†æœ¬æ–‡çš„Trie - based KVç¼“å­˜ï¼ŒSaffron - 1ä»æœ‰å¤§å¹…é¢†å…ˆï¼Œåœ¨åº”å¯¹é«˜éš¾åº¦è¶Šç‹±æ”»å‡»æ—¶è¡¨ç°å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - æ€è·¯åˆ›æ–°ï¼šå°†æ¨ç†ç¼©æ”¾å¼•å…¥å®‰å…¨ä¿éšœé¢†åŸŸï¼Œä¸ºLLMå®‰å…¨ç ”ç©¶å¼€è¾Ÿæ–°æ–¹å‘ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨æ¨ç†é˜¶æ®µçš„å®‰å…¨å¼ºåŒ–ã€‚
 - æŠ€æœ¯è´¡çŒ®ï¼šæå‡ºçš„SAFFRONèŒƒå¼åŠMRMã€è®­ç»ƒç›®æ ‡ã€æ¢ç´¢çº¦æŸã€ç¼“å­˜ç­–ç•¥ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œä¸ºè§£å†³å®‰å…¨æ¨ç†ä¸­çš„æ•ˆç‡ä¸æ¢ç´¢å¹³è¡¡é—®é¢˜æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œåç»­ç ”ç©¶å¯åœ¨æ­¤åŸºç¡€ä¸Šä¼˜åŒ–æˆ–æ‹“å±•åº”ç”¨ã€‚
 - èµ„æºå¼€æ”¾ï¼šå…¬å¼€äº†è®­ç»ƒå¥½çš„å¤šåˆ†æ”¯å¥–åŠ±æ¨¡å‹Saffron - 1å’Œtokençº§å®‰å…¨å¥–åŠ±æ•°æ®é›†Safety4Mï¼Œä¸ºè¯¥é¢†åŸŸåç»­ç ”ç©¶æä¾›äº†æ•°æ®å’Œæ¨¡å‹åŸºç¡€ï¼ŒåŠ é€ŸLLMå®‰å…¨ç ”ç©¶è¿›ç¨‹ã€‚

## preference-learning-for-ai-alignment--a-causal-perspective
### Abstract
Reward modelling from preference data is a crucial step in aligning large
language models (LLMs) with human values, requiring robust generalisation to
novel prompt-response pairs. In this work, we propose to frame this problem in
a causal paradigm, providing the rich toolbox of causality to identify the
persistent challenges, such as causal misidentification, preference
heterogeneity, and confounding due to user-specific factors. Inheriting from
the literature of causal inference, we identify key assumptions necessary for
reliable generalisation and contrast them with common data collection
practices. We illustrate failure modes of naive reward models and demonstrate
how causally-inspired approaches can improve model robustness. Finally, we
outline desiderata for future research and practices, advocating targeted
interventions to address inherent limitations of observational data.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»å› æœè§†è§’çœ‹AIå¯¹é½ä¸­çš„åå¥½å­¦ä¹ ï¼šæŒ‘æˆ˜ä¸æ–°æ–¹å‘

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½äººç±»ä»·å€¼è§‚çš„å…³é”®ç¯èŠ‚æ˜¯åŸºäºåå¥½æ•°æ®çš„å¥–åŠ±å»ºæ¨¡ï¼Œå…¶éœ€è¦å¯¹æ–°çš„æç¤º - å“åº”å¯¹å®ç°é²æ£’æ³›åŒ–ã€‚ä½†å½“å‰ç®€å•ä¾èµ–åŸºäºè§‚æµ‹æ•°æ®é›†æ‹Ÿåˆçš„å›å½’æ¨¡å‹ï¼Œå­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå› ä¸ºè¿™ç±»æ¨¡å‹æ˜“å­¦ä¹ è™šå‡ç›¸å…³æ€§è€Œéå½±å“ç”¨æˆ·åå¥½çš„çœŸå®å› ç´ ã€‚åŒæ—¶ï¼Œæˆå¯¹åå¥½æ•°æ®é›†æ”¶é›†å—LLMé‡‡æ ·åˆ†å¸ƒã€å“åº”æ½œåœ¨ç‰¹å¾å’Œç”¨æˆ·ç‰¹å®šä¸Šä¸‹æ–‡ç­‰å› ç´ å½±å“ï¼Œä½¿ç°æœ‰æ–¹æ³•çš„é²æ£’æ€§å’Œå¯é æ€§å­˜ç–‘ã€‚å› æ­¤ï¼Œè®ºæ–‡ä¸»å¼ ä»å› æœèŒƒå¼é‡æ–°å®¡è§†åå¥½å­¦ä¹ é—®é¢˜ï¼Œå€ŸåŠ©å› æœå·¥å…·è§£å†³å› æœè¯¯è¯†åˆ«ã€åå¥½å¼‚è´¨æ€§å’Œç”¨æˆ·ç‰¹å®šå› ç´ æ··æ·†ç­‰æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå› æœèŒƒå¼ä¸‹çš„åå¥½å­¦ä¹ æ¡†æ¶æ„å»º  
å°†åå¥½å­¦ä¹ ç½®äºå› æœèŒƒå¼ä¸­ï¼ŒæŠŠè§‚æµ‹åˆ°çš„ï¼ˆæç¤ºã€å“åº”ã€åå¥½æ ‡ç­¾ï¼‰å…ƒç»„è§†ä¸ºåˆ†é…ç»™äººç±»æ ‡æ³¨è€…çš„â€œå¤„ç†ï¼ˆtreatmentï¼‰â€ï¼Œåå¥½æ ‡ç­¾è§†ä¸ºâ€œç»“æœï¼ˆoutcomeï¼‰â€ï¼Œå¼•å…¥æ½œåœ¨ç»“æœæ¡†æ¶åˆ°Bradley - Terry - Luceï¼ˆBTLï¼‰å¥–åŠ±æ¨¡å‹ä¸­ï¼Œå®šä¹‰æ½œåœ¨å¥–åŠ±æ¦‚å¿µï¼Œå»ºç«‹æ½œåœ¨å¥–åŠ±å·®å¼‚ä¸é¢„æœŸæ½œåœ¨ç»“æœçš„ç›´æ¥è”ç³»ï¼Œä»¥æ­¤æ˜ç¡®åå¥½å­¦ä¹ ä¸­å› æœå…³ç³»çš„å½¢å¼åŒ–è¡¨è¾¾ï¼Œä¸ºåˆ†ææ³›åŒ–æ€§ç­‰é—®é¢˜æä¾›åŸºç¡€ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…³é”®å‡è®¾è¯†åˆ«ä¸å®è·µå¯¹æ¯”  
ä»å› æœæ¨æ–­æ–‡çŒ®å‡ºå‘ï¼Œè¯†åˆ«å¥–åŠ±æ¨¡å‹å¯é æ³›åŒ–æ‰€éœ€çš„å…³é”®å‡è®¾ï¼Œå¹¶å°†è¿™äº›å‡è®¾ä¸å¸¸è§çš„æ•°æ®æ”¶é›†å®è·µç›¸å¯¹æ¯”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ­ç¤ºç°æœ‰æ•°æ®æ”¶é›†å’Œæ¨¡å‹è®­ç»ƒä¸­å¯èƒ½å­˜åœ¨çš„ä¸å› æœå‡è®¾ç›¸æ‚–ä¹‹å¤„ï¼Œä¸ºæ”¹è¿›æ–¹æ³•æŒ‡æ˜æ–¹å‘ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå› æœå¯å‘æ–¹æ³•æå‡æ¨¡å‹é²æ£’æ€§  
é˜è¿°æœ´ç´ å¥–åŠ±æ¨¡å‹åœ¨å› æœå‡è®¾è¢«è¿åæ—¶çš„å¤±è´¥æ¨¡å¼ï¼Œå¦‚å› ç”¨æˆ·ç‰¹å®šç›®æ ‡å¯¼è‡´çš„æ··æ·†ç­‰ï¼ˆè®ºæ–‡é¦–æ¬¡æ˜ç¡®è¯†åˆ«å’Œè§£å†³è¯¥é—®é¢˜ï¼‰ã€‚åŒæ—¶å±•ç¤ºå› æœè¡¨ç¤ºå­¦ä¹ ç­‰å› æœå¯å‘æ–¹æ³•å¦‚ä½•æ”¹å–„æ¨¡å‹é²æ£’æ€§ï¼Œä¸ºæ„å»ºæ›´å¯é çš„å¥–åŠ±æ¨¡å‹æä¾›æŠ€æœ¯è·¯å¾„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡å®ä¾‹å’ŒçœŸå®ä¸–ç•Œå®éªŒï¼Œå±•ç°äº†æœ´ç´ æ¨¡å‹åœ¨å› æœå‡è®¾è¢«è¿åæ—¶ï¼ˆå¦‚ç”¨æˆ·ç‰¹å®šç›®æ ‡å¼•å‘çš„æ··æ·†æƒ…å†µï¼‰çš„å¤±æ•ˆæ¨¡å¼ï¼ŒåŒæ—¶éªŒè¯äº†å› æœå¯å‘æ–¹æ³•åœ¨æå‡æ¨¡å‹å¯¹æœªè§æ–‡æœ¬å’Œåœºæ™¯æ³›åŒ–èƒ½åŠ›ã€å¢å¼ºé²æ£’æ€§æ–¹é¢çš„æ•ˆæœï¼Œç›´è§‚åœ°å‘ˆç°å‡ºå› æœè§†è§’ä¸‹åå¥½å­¦ä¹ æ–¹æ³•æ”¹è¿›çš„å¿…è¦æ€§ä¸æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡ä¸ºAIå¯¹é½ä¸­åå¥½å­¦ä¹ çš„æœªæ¥ç ”ç©¶å’Œå®è·µå‹¾å‹’äº†æ„¿æ™¯ï¼Œå€¡å¯¼æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½ä»¥è§£å†³è§‚æµ‹æ•°æ®çš„å›ºæœ‰å±€é™æ€§ã€‚åœ¨æ–¹æ³•å±‚é¢ï¼Œå¼•å…¥å› æœæ¡†æ¶ä¸ºå¤„ç†åå¥½å­¦ä¹ ä¸­çš„å¤æ‚å› ç´ ï¼ˆå¦‚ç”¨æˆ·å¼‚è´¨æ€§ã€è™šå‡å…³è”ç­‰ï¼‰æä¾›äº†æ–°çš„åˆ†æç»´åº¦ä¸å·¥å…·ï¼›åœ¨æ•°æ®æ”¶é›†å’Œæ¨¡å‹è®¾è®¡å±‚é¢ï¼Œè¯†åˆ«å…³é”®å‡è®¾å¹¶å¯¹æ¯”å®è·µï¼Œèƒ½æŒ‡å¯¼ç ”ç©¶è€…ä¼˜åŒ–æ•°æ®é‡‡é›†æµç¨‹ã€æ”¹è¿›æ¨¡å‹ç»“æ„ä»¥æ»¡è¶³å¯é æ³›åŒ–è¦æ±‚ï¼›åœ¨é—®é¢˜è®¤çŸ¥å±‚é¢ï¼Œè®©å­¦ç•Œæ›´æ¸…æ™°è®¤è¯†åˆ°å½“å‰åå¥½å­¦ä¹ é¢ä¸´çš„æ ¹æœ¬æŒ‘æˆ˜ï¼ˆå¦‚å› æœè¯¯è¯†åˆ«ç­‰ï¼‰ï¼Œæ¨åŠ¨é¢†åŸŸå‘æ›´é²æ£’çš„æŠ€æœ¯æ–¹å‘å‘å±•ã€‚

## customizing-speech-recognition-model-with-large-language-model-feedback
### Abstract
Automatic speech recognition (ASR) systems have achieved strong performance
on general transcription tasks. However, they continue to struggle with
recognizing rare named entities and adapting to domain mismatches. In contrast,
large language models (LLMs), trained on massive internet-scale datasets, are
often more effective across a wide range of domains. In this work, we propose a
reinforcement learning based approach for unsupervised domain adaptation,
leveraging unlabeled data to enhance transcription quality, particularly the
named entities affected by domain mismatch, through feedback from a LLM. Given
contextual information, our framework employs a LLM as the reward model to
score the hypotheses from the ASR model. These scores serve as reward signals
to fine-tune the ASR model via reinforcement learning. Our method achieves a
21\% improvement on entity word error rate over conventional self-training
methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¤§è¯­è¨€æ¨¡å‹åé¦ˆå®šåˆ¶è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼šé¢†åŸŸé€‚é…æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é€šç”¨è½¬å½•ä»»åŠ¡ä¸Šå·²å–å¾—ä¸é”™æˆç»©ï¼Œä½†é¢å¯¹ç½•è§å‘½åå®ä½“è¯†åˆ«ä¸é¢†åŸŸä¸åŒ¹é…åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ã€‚ä¼ ç»Ÿè§£å†³é¢†åŸŸé€‚é…çš„è‡ªè®­ç»ƒæ–¹æ³•ä¾èµ–ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦åˆ†æ•°ï¼Œè€Œè¿™äº›åˆ†æ•°åœ¨æ–°é¢†åŸŸä¸­å¯é æ€§ä¸è¶³ã€‚åŒæ—¶ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®è®­ç»ƒï¼Œå…·å¤‡è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºASRåœ¨é¢†åŸŸé€‚é…éš¾é¢˜ä¸Šæä¾›äº†æ–°çªç ´å£ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨åˆ©ç”¨LLMåé¦ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°ASRçš„æ— ç›‘ç£é¢†åŸŸé€‚é…ï¼Œæå‡é¢†åŸŸä¸åŒ¹é…åœºæ™¯ä¸‹çš„è½¬å½•è´¨é‡ï¼Œå°¤å…¶æ˜¯å‘½åå®ä½“çš„è¯†åˆ«æ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£é¢†åŸŸé€‚é…æ¡†æ¶  
æå‡ºä¸€å¥—ç»“åˆLLMåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æµç¨‹æ¥å®ç°ASRæ— ç›‘ç£é¢†åŸŸé€‚é…ã€‚æ¡†æ¶åˆ†ä¸ºæ•°æ®æ”¶é›†ã€ç”Ÿæˆå¥–åŠ±ã€æ¨¡å‹å¾®è°ƒä¸‰æ­¥ï¼šå…ˆè®©é¢„è®­ç»ƒASRæ¨¡å‹ä¸ºç›®æ ‡åŸŸéŸ³é¢‘ç”Ÿæˆå€™é€‰å‡è®¾ï¼›å†ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚é¢†åŸŸå…ƒæ•°æ®ç­‰ï¼‰ï¼Œç”¨LLMè®¡ç®—å„å‡è®¾çš„å¥–åŠ±åˆ†æ•°ï¼›æœ€åç”¨RLç®—æ³•ä¾æ®å¥–åŠ±å¾®è°ƒASRæ¨¡å‹ï¼Œæ‘†è„±äº†ä¼ ç»Ÿè‡ªè®­ç»ƒå¯¹ä¸å¯é ç½®ä¿¡åº¦åˆ†æ•°çš„ä¾èµ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨LLMæ„å»ºå¥–åŠ±å‡½æ•°  
ä¸å•ç‹¬è®­ç»ƒASRä¸“ç”¨å¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯å€ŸåŠ©LLMå¯¹è¯­è¨€å’Œä¸Šä¸‹æ–‡çš„éšå¼ç†è§£æ¥ç”Ÿæˆå¥–åŠ±ã€‚å°†é¢†åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯æ•´åˆä¸ºæç¤ºè¯è¾“å…¥LLMï¼Œé€šè¿‡è®¡ç®—å‡è®¾åœ¨LLMä¸‹çš„å¯¹æ•°æ¦‚ç‡å’Œï¼ˆç»“åˆASRæ¨¡å‹è‡ªèº«åˆ†æ•°ï¼Œé€šè¿‡å‚æ•°Î»æ§åˆ¶æƒé‡ï¼‰ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè®©LLMçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ä¸ºASRé¢†åŸŸé€‚é…æä¾›æŒ‡å¯¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRLç®—æ³•é©±åŠ¨ASRå¾®è°ƒ  
æ¢ç´¢å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ç”¨äºASRæ¨¡å‹å¾®è°ƒï¼Œåˆ©ç”¨LLMç»™å‡ºçš„å¥–åŠ±ä¿¡å·ï¼Œè®©ASRæ¨¡å‹åœ¨æ— ç›‘ç£åœºæ™¯ä¸‹æœç€æ›´å¥‘åˆç›®æ ‡é¢†åŸŸè½¬å½•è´¨é‡çš„æ–¹å‘æ›´æ–°ï¼ŒåŒºåˆ«äºä¼ ç»Ÿä»…ä¾èµ–è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æˆ–äººå·¥åé¦ˆè®¾è®¡å¥–åŠ±çš„æ€è·¯ï¼ŒæŠŠLLMåé¦ˆæ·±åº¦èå…¥è®­ç»ƒè¿‡ç¨‹ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒèšç„¦é¢†åŸŸä¸åŒ¹é…åœºæ™¯ä¸‹å‘½åå®ä½“ç­‰è½¬å½•è´¨é‡æå‡ã€‚å¯¹æ¯”ä¼ ç»Ÿè‡ªè®­ç»ƒé¢†åŸŸé€‚é…æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å®ä½“è¯é”™è¯¯ç‡ï¼ˆEWERï¼‰ä¸Šå®ç°äº†é«˜è¾¾21%çš„ç›¸å¯¹æå‡ï¼Œæœ‰åŠ›è¯æ˜äº†LLMåé¦ˆåœ¨å¼•å¯¼ASRæ¨¡å‹é€‚é…è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†åŸºäºRLå’ŒLLMåé¦ˆçš„æ— ç›‘ç£é¢†åŸŸé€‚é…æ¡†æ¶çš„ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨æ¨¡æ€åä½œæ€è·¯ï¼šå±•ç¤ºäº†ASRä¸LLMç»“åˆçš„æ–°è·¯å¾„ï¼Œä¸å†å±€é™äºLLMåœ¨ASRåå¤„ç†ï¼ˆé‡æ’åºã€çº é”™ï¼‰ç¯èŠ‚ï¼Œè€Œæ˜¯è®©LLMæ·±åº¦å‚ä¸è®­ç»ƒé˜¶æ®µï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹åä½œæä¾›äº†æ–°èŒƒå¼å‚è€ƒã€‚  
2. æ— ç›‘ç£é€‚é…æ–¹æ¡ˆï¼šé’ˆå¯¹é¢†åŸŸé€‚é…ä¸­æ ‡ç­¾æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæä¾›äº†åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®+LLMåé¦ˆ+å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆæ— ç›‘ç£é€‚é…æ–¹æ¡ˆï¼Œåœ¨ä½èµ„æºã€é¢†åŸŸå·®å¼‚å¤§çš„åœºæ™¯ä¸‹æœ‰æ¨å¹¿æ½œåŠ›ã€‚  
3. å¥–åŠ±è®¾è®¡åˆ›æ–°ï¼šå€ŸåŠ©LLMå¤©ç„¶çš„è¯­è¨€ç†è§£å’Œè·¨åŸŸèƒ½åŠ›æ„å»ºå¥–åŠ±å‡½æ•°ï¼Œé¿å…äº†ä¼ ç»Ÿè‡ªè®­ç»ƒç½®ä¿¡åº¦åˆ†æ•°ä¸å¯é çš„ç¼ºé™·ï¼Œä¸ºåç»­æ¨¡å‹è¯„ä¼°ä¸å¥–åŠ±æœºåˆ¶è®¾è®¡æ‰“å¼€æ–°æ€è·¯ã€‚

## flattery--fluff--and-fog--diagnosing-and-mitigating-idiosyncratic-biases-in-preference-models
### Abstract
Language models serve as proxies for human preference judgements in alignment
and evaluation, yet they exhibit systematic miscalibration, prioritizing
superficial patterns over substantive qualities. This bias manifests as
overreliance on features like length, structure, and style, leading to issues
like reward hacking and unreliable evaluations. Evidence suggests these biases
originate in artifacts in human training data. In this work, we systematically
investigate the relationship between training data biases and preference model
miscalibration across five idiosyncratic features of language model
generations: length, structure, jargon, sycophancy and vagueness. Using
controlled counterfactual pairs, we first quantify the extent to which
preference models favor responses with magnified biases (skew), finding this
preference occurs in >60% of instances, and model preferences show high
miscalibration (~40%) compared to human preferences. Notably, bias features
only show mild negative correlations to human preference labels (mean r_human =
-0.12) but show moderately strong positive correlations with labels from a
strong reward model (mean r_model = +0.36), suggesting that models may overrely
on spurious cues. To mitigate these issues, we propose a simple post-training
method based on counterfactual data augmentation (CDA) using synthesized
contrastive examples. Finetuning models with CDA reduces average miscalibration
from 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%,
while maintaining overall RewardBench performance, showing that targeted
debiasing is effective for building reliable preference models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åå¥½æ¨¡å‹ä¸­â€œå¥‰æ‰¿ã€å†—é•¿ä¸æ¨¡ç³Šâ€ï¼šè¯Šæ–­å¹¶ç¼“è§£ç‰¹æ®Šåå·®

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯­è¨€æ¨¡å‹å¸¸è¢«ç”¨ä½œäººç±»åå¥½åˆ¤æ–­çš„ä»£ç†ï¼Œåœ¨å¯¹é½å’Œè¯„ä¼°ä»»åŠ¡ä¸­å‘æŒ¥ä½œç”¨ï¼Œä½†å­˜åœ¨ç³»ç»Ÿæ€§æ ¡å‡†åå·®ï¼Œä¼šä¼˜å…ˆå…³æ³¨é•¿åº¦ã€ç»“æ„ã€é£æ ¼ç­‰è¡¨é¢æ¨¡å¼è€Œéå®è´¨è´¨é‡ï¼Œå¼•å‘å¥–åŠ±é»‘å®¢ã€è¯„ä¼°ä¸å¯é ç­‰é—®é¢˜ã€‚å·²æœ‰è¯æ®è¡¨æ˜è¿™äº›åå·®æºäºäººç±»è®­ç»ƒæ•°æ®ä¸­çš„ä¼ªå½±ï¼Œè€Œç°æœ‰ç ”ç©¶å¤šå­¤ç«‹è®°å½•å•ä¸ªåå·®ï¼Œç¼ºä¹å¯¹è®­ç»ƒæ•°æ®ä¼ªå½±å¦‚ä½•åœ¨å¤šåå·®ç»´åº¦è½¬åŒ–ä¸ºæ¨¡å‹æ ¡å‡†åå·®çš„é‡åŒ–åˆ†æã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨ç³»ç»Ÿæ¢ç©¶è®­ç»ƒæ•°æ®åå·®ä¸åå¥½æ¨¡å‹æ ¡å‡†åå·®çš„å…³ç³»ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç³»ç»Ÿç ”ç©¶å¤šåå·®ç»´åº¦ä¸æ¨¡å‹æ ¡å‡†åå·®å…³ç³»  
èšç„¦è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ä¸­å¸¸è§çš„äº”ä¸ªç‰¹æ®Šåå·®ç‰¹å¾ï¼šé•¿åº¦ï¼ˆå†—é•¿æ€§ï¼‰ã€ç»“æ„ï¼ˆå¦‚åˆ—è¡¨æ ¼å¼ï¼‰ã€è¡Œè¯ï¼ˆè¿‡åº¦æŠ€æœ¯åŒ–è¯­è¨€ï¼‰ã€å¥‰æ‰¿ï¼ˆè¿‡åº¦è¿åˆç”¨æˆ·ï¼‰å’Œæ¨¡ç³Šæ€§ï¼ˆç¼ºä¹ç‰¹å¼‚æ€§ï¼‰ã€‚é€šè¿‡æ„å»ºåäº‹å®å“åº”å¯¹ï¼ˆåœ¨ä¿ç•™å…¶ä»–æœ‰æ„ä¹‰ç‰¹å¾çš„åŒæ—¶æ”¾å¤§ç›®æ ‡åå·®ç‰¹å¾ï¼‰ï¼Œé‡åŒ–åå¥½æ¨¡å‹å¯¹æœ‰åå·®å“åº”çš„åå¥½å€¾æ–œåº¦ï¼ˆskewï¼‰ä»¥åŠæ¨¡å‹ä¸äººç±»åå¥½çš„æ ¡å‡†åå·®ç‡ï¼ˆmiscalibration rateï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºåŸºäºåäº‹å®æ•°æ®å¢å¼ºï¼ˆCDAï¼‰çš„åè®­ç»ƒå»åæ–¹æ³•  
ä¸ºç¼“è§£åå·®é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨åˆæˆå¯¹æ¯”ç¤ºä¾‹çš„åäº‹å®æ•°æ®å¢å¼ºæ–¹æ³•ã€‚é’ˆå¯¹æ¯ä¸ªåå·®ç‰¹å¾ï¼Œåœ¨ç°æœ‰åå¥½æ•°æ®é›†ä¸Šæ‰©å……â€œç¿»è½¬å¯¹â€ï¼ˆå³æ˜ç¡®è®©è¢«æ‰°åŠ¨åæ”¾å¤§åå·®çš„å“åº”æˆä¸ºä¸è¢«åå¥½çš„æ ·æœ¬ï¼‰ï¼Œç”¨è¿™ç±»æ•°æ®å¾®è°ƒå¥–åŠ±æ¨¡å‹ï¼Œä»¥æƒ©ç½šæœ‰åå·®çš„åå¥½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
1. åå¥½å€¾æ–œä¸æ ¡å‡†åå·®å±‚é¢ï¼šæ¨¡å‹å¯¹æœ‰åå·®å“åº”å­˜åœ¨æ˜¾è‘—åå¥½ï¼ˆå¦‚å¯¹ç»“æ„åŒ–å“åº”åå¥½è¾¾89.5%ã€å¯¹å†—é•¿å“åº”åå¥½è¾¾60.1%ï¼‰ï¼Œæ¨¡ç³Šæ€§å’Œè¡Œè¯åå·®å¯¹åº”çš„æ ¡å‡†åå·®ç‡è¶…50%ï¼›æ•´ä½“ä¸Šï¼Œæ¨¡å‹åå¥½ä¸äººç±»å¤šæ•°åå¥½åœ¨39.4%çš„è¯„ä¼°ä¸­å­˜åœ¨å†²çªï¼Œæ ¡å‡†åå·®é«˜ã€‚  
2. è®­ç»ƒæ•°æ®å…³è”å±‚é¢ï¼šåˆ†æä¸»æµå¥–åŠ±æ¨¡å‹è®­ç»ƒæ•°æ®ï¼Œå‘ç°äººç±»é€‰æ‹©å’Œæ‹’ç»çš„å“åº”ä¸­åå·®å­˜åœ¨æ˜æ˜¾ä¸å¹³è¡¡ï¼Œä¸”åå·®ç‰¹å¾ä¸äººç±»åå¥½å¼±ç›¸å…³ï¼ˆå¦‚é•¿åº¦ä¸äººç±»åå¥½ç›¸å…³ç³»æ•°r=-0.09ï¼‰å´ä¸è®­ç»ƒåæ¨¡å‹åå¥½å¼ºç›¸å…³ï¼ˆr=0.67ï¼‰ï¼Œè¯´æ˜æ ‡å‡†RLHF pipelineä¼šæ— æ„æ”¾å¤§æ•°æ®ä¼ªå½±ä¸ºé”™ä½åå¥½ä¿¡å·ã€‚  
3. å»åæ•ˆæœå±‚é¢ï¼šç”¨CDAå¾®è°ƒæ¨¡å‹åï¼Œå¹³å‡æ ¡å‡†åå·®ä»39.4%é™è‡³32.5%ï¼Œå¹³å‡ç»å¯¹åå¥½å€¾æ–œå·®ä»20.5%é™è‡³10.0%ï¼ŒåŒæ—¶åœ¨RewardBenchä¸Šä¿æŒç«äº‰åŠ›ï¼Œè¯æ˜é’ˆå¯¹æ€§å»åå¯¹æ„å»ºå¯é åå¥½æ¨¡å‹æœ‰æ•ˆã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šç»´åº¦åå·®åˆ†ææ€è·¯ï¼šä»é•¿åº¦ã€ç»“æ„ç­‰å¤šä¸ªå®é™…æ˜“å‡ºç°çš„åå·®ç»´åº¦åˆ‡å…¥ï¼Œç³»ç»Ÿé‡åŒ–æ¨¡å‹è¡Œä¸ºä¸äººç±»åå¥½çš„å·®å¼‚ï¼Œä¸ºç†è§£åå¥½æ¨¡å‹åå·®æä¾›äº†å…¨é¢è§†è§’ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å¤šç»´åº¦æ‹†è§£åˆ†æçš„æ–¹å¼ã€‚  
2. åäº‹å®æ•°æ®å¢å¼ºå»åï¼šæå‡ºçš„CDAæ–¹æ³•ç®€å•ä¸”æœ‰æ•ˆï¼Œé€šè¿‡æ„é€ å¯¹æ¯”ç¤ºä¾‹æ‰©å……æ•°æ®æ¥å¼•å¯¼æ¨¡å‹ä¿®æ­£åå·®ï¼Œä¸ºåè®­ç»ƒé˜¶æ®µä¼˜åŒ–æ¨¡å‹åå¥½åˆ¤æ–­æä¾›äº†å¯è¡ŒèŒƒå¼ï¼Œåœ¨å¤„ç†ç±»ä¼¼â€œæ¨¡å‹è¿‡åº¦ä¾èµ–è¡¨é¢ç‰¹å¾â€é—®é¢˜æ—¶å¯å‚è€ƒè¯¥æ•°æ®å¢å¼º+å¾®è°ƒçš„æ€è·¯ã€‚  
3. è®­ç»ƒæ•°æ®ä¸æ¨¡å‹åå·®å…³è”åˆ†æï¼šæ­ç¤ºè®­ç»ƒæ•°æ®ä¸­åå·®ä¼ªå½±å¦‚ä½•å½±å“æ¨¡å‹ï¼Œå¼ºè°ƒäº†æ•°æ®å±‚é¢åˆ†æå¯¹ç†è§£å’Œæ”¹è¿›æ¨¡å‹è¡Œä¸ºçš„é‡è¦æ€§ï¼Œæé†’åç»­åœ¨æ„å»ºåå¥½æ¨¡å‹æ•°æ®æ—¶éœ€å…³æ³¨åå·®å¹³è¡¡ä¸ç‰¹å¾ç›¸å…³æ€§ã€‚

## a-smooth-sea-never-made-a-skilled-$\texttt{sailor}$--robust-imitation-via-learning-to-search
### Abstract
The fundamental limitation of the behavioral cloning (BC) approach to
imitation learning is that it only teaches an agent what the expert did at
states the expert visited. This means that when a BC agent makes a mistake
which takes them out of the support of the demonstrations, they often don't
know how to recover from it. In this sense, BC is akin to giving the agent the
fish -- giving them dense supervision across a narrow set of states -- rather
than teaching them to fish: to be able to reason independently about achieving
the expert's outcome even when faced with unseen situations at test-time. In
response, we explore learning to search (L2S) from expert demonstrations, i.e.
learning the components required to, at test time, plan to match expert
outcomes, even after making a mistake. These include (1) a world model and (2)
a reward model. We carefully ablate the set of algorithmic and design decisions
required to combine these and other components for stable and
sample/interaction-efficient learning of recovery behavior without additional
human corrections. Across a dozen visual manipulation tasks from three
benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs
state-of-the-art Diffusion Policies trained via BC on the same data.
Furthermore, scaling up the amount of demonstrations used for BC by
5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can
identify nuanced failures and is robust to reward hacking. Our code is
available at https://github.com/arnavkj1995/SAILOR .
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»â€œæˆé±¼â€åˆ°â€œæˆæ¸”â€ï¼šSAILORè®©æ™ºèƒ½ä½“å­¦ä¼šä»é”™è¯¯ä¸­è‡ªä¸»æ¢å¤

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ¨¡ä»¿å­¦ä¹ é¢†åŸŸï¼Œè¡Œä¸ºå…‹éš†ï¼ˆBehavioral Cloningï¼ŒBCï¼‰æ˜¯å¸¸ç”¨æ–¹æ³•ï¼Œå®ƒè®©æ™ºèƒ½ä½“å­¦ä¹ ä¸“å®¶åœ¨è®¿é—®è¿‡çš„çŠ¶æ€ä¸‹çš„è¡Œä¸ºã€‚ä½†BCå­˜åœ¨æ ¹æœ¬å±€é™ï¼šä¸€æ—¦æ™ºèƒ½ä½“çŠ¯é”™è„±ç¦»æ¼”ç¤ºæ•°æ®çš„â€œæ”¯æŒåŸŸâ€ï¼ˆå³é‡åˆ°è®­ç»ƒæ—¶æ²¡è§è¿‡çš„çŠ¶æ€ï¼‰ï¼Œå°±ä¸çŸ¥å¦‚ä½•æ¢å¤ã€‚è¿™å°±åƒåªç»™æ™ºèƒ½ä½“â€œé±¼â€ï¼ˆåœ¨æœ‰é™çŠ¶æ€ä¸‹å¯†é›†ç›‘ç£ï¼‰ï¼Œè€Œéæ•™å®ƒâ€œæ•é±¼â€ï¼ˆé¢å¯¹ unseen æƒ…å†µæ—¶è‡ªä¸»æ¨ç†è¾¾æˆç›®æ ‡ï¼‰ã€‚  

è¯­è¨€å»ºæ¨¡é¢†åŸŸçš„ç»éªŒä¹Ÿè¡¨æ˜ï¼Œä»…é â€œç¼©æ”¾ç‰ˆBCâ€ï¼ˆå¦‚å•çº¯å¢å¤§æ•°æ®é‡è®­ç»ƒæ›´å¼ºå¤§æ¨¡å‹ï¼‰ä¸å¤Ÿï¼Œè¿˜éœ€äº¤äº’å¼å­¦ä¹ ï¼ˆå¦‚RLHFï¼‰ã€‚æœºå™¨äººå­¦ä¹ åŒç†ï¼Œå³ä¾¿ç”¨å¤§é‡æ•°æ®å’Œå¼ºè¡¨è¾¾åŠ›æ¨¡å‹ï¼Œæ™ºèƒ½ä½“ä»å¯èƒ½å› è‡ªèº«åŠ¨ä½œé™·å…¥æœªçŸ¥çŠ¶æ€ï¼Œæ­¤æ—¶éœ€è‡ªä¸»æ¢å¤èƒ½åŠ›ã€‚è€Œä¼ ç»Ÿä¾èµ–äººç±»åœ¨çº¿çº é”™çš„æ–¹å¼éš¾ä»¥è§„æ¨¡åŒ–ï¼Œå› æ­¤æœ¬æ–‡å¸Œæœ›è®©æ™ºèƒ½ä½“åœ¨æ— é¢å¤–äººç±»åé¦ˆä¸‹ï¼Œå­¦ä¼šä»è‡ªèº«é”™è¯¯ä¸­æ¢å¤ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºâ€œå­¦ä¹ æœç´¢ï¼ˆLearning to Search, L2Sï¼‰â€èŒƒå¼  
ä¸å†åªä»ä¸“å®¶æ¼”ç¤ºå­¦ç­–ç•¥ï¼Œè€Œæ˜¯å­¦ä¹ **ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Model, WMï¼‰**å’Œ**å¥–åŠ±æ¨¡å‹ï¼ˆReward Model, RMï¼‰**ã€‚ä¸–ç•Œæ¨¡å‹ç”¨äºé¢„æµ‹åŠ¨ä½œåæœï¼Œå¥–åŠ±æ¨¡å‹ç”¨äºè¯„ä¼°ç»“æœä¼˜åŠ£ã€‚ç»“åˆè§„åˆ’ç®—æ³•åï¼Œæµ‹è¯•æ—¶æ™ºèƒ½ä½“èƒ½åŸºäºå­¦åˆ°çš„ WM å’Œ RMï¼Œæ¨ç†å¦‚ä½•ä»åŸºç¡€ç­–ç•¥ï¼ˆå¦‚Diffusion Policyï¼‰çš„é”™è¯¯ä¸­æ¢å¤ï¼Œå®ç°â€œä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ æœç´¢â€ï¼Œè€Œéå•çº¯æ¨¡ä»¿ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSAILORæ¶æ„å®ç°L2S  
é’ˆå¯¹é•¿ horizon è§†è§‰æ“ä½œä»»åŠ¡ï¼Œå®ä¾‹åŒ– L2S èŒƒå¼ï¼šç”¨åŸºç¡€Diffusion Policiesã€Dreamer World Models å’Œ MPPI Planner æ„å»ºåä¸º SAILORï¼ˆSearching Across Imagined Latents Online for Recoveryï¼‰çš„å¤åˆæ¶æ„ã€‚å®ƒå­¦ä¹ â€œæ®‹å·®è§„åˆ’å™¨â€ï¼Œæµ‹è¯•æ—¶æ‰§è¡Œå±€éƒ¨æœç´¢æ¥ä¿®æ­£åŸºç¡€ç­–ç•¥çš„é”™è¯¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå…³é”®è®¾è®¡å†³ç­–çš„æ¶ˆèä¸éªŒè¯  
ä¸ºè®©ç»„ä»¶ç¨³å®šä¸”æ ·æœ¬é«˜æ•ˆåœ°å­¦ä¹ ï¼ŒéªŒè¯äº†ä¸‰ç±»å…³é”®è®¾è®¡ï¼š  
- â€œçƒ­å¯åŠ¨â€æ··åˆä¸–ç•Œæ¨¡å‹è®­ç»ƒé˜¶æ®µï¼›  
- åœ¨çº¿å¾®è°ƒæ··åˆä¸–ç•Œæ¨¡å‹ï¼›  
- å‘¨æœŸæ€§å°†å­¦ä¹ åˆ°çš„æœç´¢ç®—æ³•è’¸é¦åˆ°åŸºç¡€ç­–ç•¥ï¼ˆç±»ä¼¼ expert iteration æˆ– Guided Policy Searchï¼‰ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ€§èƒ½è¶…è¶ŠSOTAï¼šåœ¨ä¸‰ä¸ªåŸºå‡†çš„åä½™é¡¹è§†è§‰æ“ä½œä»»åŠ¡ä¸­ï¼ŒSAILOR æŒç»­è¶…è¶ŠåŒæ•°æ®ä¸‹ç”¨ BC è®­ç»ƒçš„æœ€å…ˆè¿› Diffusion Policiesã€‚å³ä¾¿æŠŠ BC ç”¨çš„æ¼”ç¤ºæ•°æ®é‡æ‰©å¤§ 5 - 10 å€ï¼Œä»å­˜åœ¨æ€§èƒ½å·®è·ã€‚  
2. äº¤äº’æ•ˆç‡æ›´é«˜ï¼šæ¯”ä¼ ç»Ÿæ— æ¨¡å‹é€† RL æ–¹æ³•ï¼ˆå¦‚ç”¨ DPPO æ›´æ–°ç­–ç•¥å‚æ•°ï¼‰çš„äº¤äº’æ•ˆç‡æ˜¾è‘—æ›´é«˜ã€‚  
3. é²æ£’æ€§ä¸å¤±æ•ˆè¯†åˆ«ï¼šå­¦åˆ°çš„å¥–åŠ±æ¨¡å‹èƒ½è¯†åˆ«é•¿ horizon æ“ä½œä»»åŠ¡ä¸åŒé˜¶æ®µçš„ç»†å¾®å¤±æ•ˆï¼›æ•´ä¸ª SAILOR æ¶æ„å¯¹â€œå¥–åŠ±é»‘å®¢æ”»å‡»ï¼ˆreward hackingï¼‰â€é²æ£’ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼å±‚é¢ï¼šä»â€œæˆé±¼â€åˆ°â€œæˆæ¸”â€çš„æ€è·¯ã€‚åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œä¸åº”ä»…èšç„¦â€œå¤åˆ»ä¸“å®¶è¡Œä¸ºâ€ï¼Œæ›´è¦èµ‹äºˆæ™ºèƒ½ä½“è‡ªä¸»æ¨ç†ã€ä»é”™è¯¯æ¢å¤çš„èƒ½åŠ›ï¼Œè¿™ä¸ºè§£å†³ BC å›ºæœ‰ç¼ºé™·æä¾›äº†æ–°æ–¹å‘ã€‚  
2. æŠ€æœ¯å®ç°ï¼šå¯¹ä¸–ç•Œæ¨¡å‹ã€å¥–åŠ±æ¨¡å‹ä¸è§„åˆ’ç®—æ³•çš„ç»“åˆæ–¹å¼ï¼Œä»¥åŠçƒ­å¯åŠ¨ã€åœ¨çº¿å¾®è°ƒã€è’¸é¦ç­‰å·¥ç¨‹åŒ–æŠ€å·§çš„éªŒè¯ï¼Œä¸ºåç»­æ„å»ºé²æ£’æ¨¡ä»¿å­¦ä¹ ç³»ç»Ÿæä¾›äº†å¯å¤ç”¨çš„è®¾è®¡å‚è€ƒã€‚  
3. å®éªŒè§†è§’ï¼šåœ¨å¤šä»»åŠ¡ã€å¤šæ•°æ®è§„æ¨¡ä¸‹å¯¹æ¯” SOTA æ–¹æ³•ï¼Œå¹¶éªŒè¯é²æ£’æ€§ç­‰å±æ€§ï¼Œè¿™ç§å…¨é¢çš„å®éªŒè®¾è®¡æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œèƒ½æ›´å……åˆ†åœ°å±•ç°æ–¹æ³•ä»·å€¼ã€‚

## rewardanything--generalizable-principle-following-reward-models
### Abstract
Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RewardAnythingï¼šè®©å¥–åŠ±æ¨¡å‹çªç ´å›ºå®šåå¥½ï¼Œçµæ´»éµå¾ªè‡ªç„¶è¯­è¨€åŸåˆ™

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½äººç±»åå¥½æ˜¯å…³é”®æŒ‘æˆ˜ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ä½œä¸ºRLHFç­‰å¯¹é½æŠ€æœ¯çš„æ ¸å¿ƒï¼Œå½“å‰è®­ç»ƒæ–¹å¼å­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼šä¸€æ˜¯æ³›åŒ–ä¸é€‚åº”æ€§å—é™ï¼Œä¾èµ–å›ºå®šåå¥½æ•°æ®é›†è®­ç»ƒçš„RMséš¾ä»¥é€‚é…ä¸åŒçœŸå®åœºæ™¯ï¼ˆå¦‚å®¢æœéœ€ç®€æ´ã€ç§‘ç ”åŠ©æ‰‹éœ€è¯¦ç»†ï¼‰ï¼Œé‡æ–°æ”¶é›†æ•°æ®å†è®­ç»ƒæˆæœ¬é«˜ï¼›äºŒæ˜¯éšå¼åå¥½å­¦ä¹ å¸¦æ¥åå·®ä¸å¯è§£é‡Šæ€§éš¾é¢˜ï¼Œç°æœ‰RMsä»äººç±»æ ‡æ³¨åå¥½æ•°æ®å­¦ä¹ ï¼Œå¤šä»…ä¿ç•™ç»“æœç›‘ç£ï¼Œæ˜“è®©æ¨¡å‹é€šè¿‡è™šå‡å…³è”æ¨æ–­éšå¼ä»·å€¼ï¼Œå¯¼è‡´å¥–åŠ±åå·®ä¸”å¯è§£é‡Šæ€§å·®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºè½¬å‘**éµå¾ªåŸåˆ™çš„å¥–åŠ±æ¨¡å‹**ï¼Œè®©RMsèƒ½åŸºäºåŠ¨æ€è‡ªç„¶è¯­è¨€åŸåˆ™è°ƒæ•´å¥–åŠ±æ ‡å‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºâ€œéµå¾ªåŸåˆ™çš„å¥–åŠ±æ¨¡å‹â€æ¦‚å¿µèŒƒå¼  
æ˜ç¡®RMsåº”åƒLLMséµå¾ªæŒ‡ä»¤ä¸€æ ·ï¼Œç†è§£å¹¶éµå¾ªåŠ¨æ€æä¾›çš„è‡ªç„¶è¯­è¨€å¥–åŠ±åŸåˆ™æè¿°ï¼Œæ— éœ€ä¸ºæ¯ä¸ªåå¥½åœºæ™¯è®­ç»ƒæ–°æ¨¡å‹ï¼Œä½¿RMsæˆä¸ºè·¨å¤šæ ·åå¥½åœºæ™¯æ³›åŒ–çš„çµæ´»å·¥å…·ï¼Œä¸ºæ„å»ºé€‚é…å¤šå˜äººç±»åå¥½çš„AIç³»ç»Ÿç­‘ç‰¢èƒ½åŠ›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºRABenchåŸºå‡†æµ‹è¯•é›†  
æ‰“é€ å…¨é¢çš„RMsåŸºå‡†RABenchï¼Œèšç„¦è¯„ä¼°RMså¯¹æ–°é¢–è‡ªç„¶è¯­è¨€åŸåˆ™çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦†ç›–å¤šé¢†åŸŸã€æš´éœ²å½“å‰RMså±€é™ï¼Œä¸ºè¡¡é‡â€œéµå¾ªåŸåˆ™â€èƒ½åŠ›è¿›å±•æä¾›æ ‡å°ºï¼Œå¡«è¡¥é¢†åŸŸå†…ç³»ç»Ÿè¯„ä¼°ç©ºç™½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç ”å‘RewardAnythingæ¨¡å‹  
è®¾è®¡å¹¶è®­ç»ƒRewardAnythingè¿™ä¸€ç”Ÿæˆå¼RMï¼Œç»“åˆGRPOä¸Group Relative Preference LearningæŠ€æœ¯ï¼Œè®©æ¨¡å‹èƒ½åœ¨æ¨ç†æ—¶é«˜æ•ˆè§£è¯»ã€åº”ç”¨å¤šæ ·åå¥½åŸåˆ™ï¼Œå®ç°å¯¹å“åº”ç»„çš„é«˜æ•ˆæ’åºä¸æ‰“åˆ†ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šé‡è®­å°±èƒ½ä¿æŒé«˜è´¨é‡åå¥½åˆ¤æ–­ï¼Œè¿˜èƒ½é«˜æ•ˆæ”¯æ’‘PPOã€GRPOç­‰RLè®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¼ ç»ŸRMåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…é€šè¿‡æŒ‡å®šæ¸…æ™°åŸåˆ™ï¼ŒRewardAnythingå°±èƒ½å–å¾—SOTAæ€§èƒ½ï¼›åœ¨RABenchä¸Šï¼Œå±•ç°å‡ºæ— éœ€é‡è®­å°±èƒ½é€‚é…æ–°åŸåˆ™çš„å“è¶Šèƒ½åŠ›ï¼›æ­¤å¤–ï¼Œæ¡ˆä¾‹ç ”ç©¶éªŒè¯å…¶èƒ½ä»…ä»¥è‡ªç„¶è¯­è¨€åŸåˆ™ä¸ºæŒ‡å¯¼è‡ªåŠ¨é«˜æ•ˆå¯¹é½LLMï¼Œåœ¨ç»†å¾®å®‰å…¨ã€å¸®åŠ©æ€§ã€å“åº”è´¨é‡ç­‰ç»´åº¦å®ç°æ˜¾è‘—æå‡ï¼Œæœ‰åŠ›è¯æ˜â€œéµå¾ªåŸåˆ™â€èŒƒå¼åœ¨LLMé«˜æ•ˆçµæ´»å¯¹é½ä¸Šçš„ä»·å€¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°å±‚é¢ï¼šå°†â€œéµå¾ªæŒ‡ä»¤â€æ€è·¯è¿ç§»åˆ°å¥–åŠ±æ¨¡å‹ï¼Œæå‡ºâ€œéµå¾ªè‡ªç„¶è¯­è¨€åŸåˆ™â€çš„æ–°èŒƒå¼ï¼Œä¸ºæ‰“ç ´RMså›ºå®šåå¥½æŸç¼šã€é€‚é…å¤šæ ·çœŸå®åœºæ™¯æä¾›æ–¹å‘ï¼Œå¯å‘åç»­å¯¹é½æŠ€æœ¯ä»â€œé™æ€åå¥½æ‹Ÿåˆâ€è½¬å‘â€œåŠ¨æ€åŸåˆ™éµå¾ªâ€ã€‚  
2. åŸºå‡†æ„å»ºå±‚é¢ï¼šRABenchä¸ºè¯„ä¼°RMsæ³›åŒ–åˆ°æ–°åŸåˆ™çš„èƒ½åŠ›æä¾›äº†æ ‡å‡†åŒ–å·¥å…·ï¼Œæ¨åŠ¨é¢†åŸŸå†…å¯¹RMsâ€œçµæ´»æ€§â€â€œé€šç”¨æ€§â€çš„é‡åŒ–ç ”ç©¶ï¼Œåç»­å¯åŸºäºæ­¤æŒç»­è¿­ä»£ä¼˜åŒ–æ¨¡å‹ã€‚  
3. æ¨¡å‹è®¾è®¡å±‚é¢ï¼šRewardAnythingç»“åˆç‰¹å®šè®­ç»ƒæ–¹æ³•å®ç°æ¨ç†æ—¶åŸåˆ™éµå¾ªï¼Œå…¶æŠ€æœ¯è·¯çº¿ï¼ˆå¦‚GRPOç»“åˆã€ç”Ÿæˆå¼RMè®¾è®¡ï¼‰ä¸ºæ‰“é€ çµæ´»å¥–åŠ±æ¨¡å‹æä¾›äº†å¯å‚è€ƒçš„å·¥ç¨‹å®è·µï¼ŒåŠ©åŠ›åç»­é«˜æ•ˆRLHFæµç¨‹æ­å»ºã€‚

## densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models
### Abstract
Direct Preference Optimization (DPO) has recently been applied as a
post-training technique for text-to-video diffusion models. To obtain training
data, annotators are asked to provide preferences between two videos generated
from independent noise. However, this approach prohibits fine-grained
comparisons, and we point out that it biases the annotators towards low-motion
clips as they often contain fewer visual artifacts. In this work, we introduce
DenseDPO, a method that addresses these shortcomings by making three
contributions. First, we create each video pair for DPO by denoising corrupted
copies of a ground truth video. This results in aligned pairs with similar
motion structures while differing in local details, effectively neutralizing
the motion bias. Second, we leverage the resulting temporal alignment to label
preferences on short segments rather than entire clips, yielding a denser and
more precise learning signal. With only one-third of the labeled data, DenseDPO
greatly improves motion generation over vanilla DPO, while matching it in text
alignment, visual quality, and temporal consistency. Finally, we show that
DenseDPO unlocks automatic preference annotation using off-the-shelf Vision
Language Models (VLMs): GPT accurately predicts segment-level preferences
similar to task-specifically fine-tuned video reward models, and DenseDPO
trained on these labels achieves performance close to using human labels.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DenseDPOï¼šä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹å¸¦æ¥ç»†ç²’åº¦æ—¶åºåå¥½ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸï¼Œæ‰©æ•£æ¨¡å‹å–å¾—äº†ä¸å°‘è¿›å±•ï¼Œä½†ç°æœ‰è§†é¢‘ç”Ÿæˆå™¨åœ¨æ—¶é—´ä¸€è‡´æ€§ã€è§†è§‰ä¿çœŸåº¦å’Œæç¤ºå¯¹é½ç­‰æ–¹é¢ä»æœ‰ä¸è¶³ã€‚Direct Preference Optimizationï¼ˆDPOï¼‰æŠ€æœ¯è¢«ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„åè®­ç»ƒï¼Œä½†ä¼ ç»ŸDPOæ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šä¸€æ˜¯è®­ç»ƒæ•°æ®ç”±ç‹¬ç«‹å™ªå£°ç”Ÿæˆçš„è§†é¢‘å¯¹æ„å»ºï¼Œéš¾ä»¥è¿›è¡Œç»†ç²’åº¦æ¯”è¾ƒï¼›äºŒæ˜¯æ ‡æ³¨è€…æ˜“åå‘ä½è¿åŠ¨ç‰‡æ®µï¼ˆè¿™ç±»ç‰‡æ®µè§†è§‰ä¼ªå½±å°‘ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒä¸­å¼ºåŒ–å¯¹æ…¢åŠ¨ä½œå†…å®¹çš„åå‘ï¼ŒæŠ‘åˆ¶ç”ŸæˆåŠ¨æ€ä¸°å¯Œè§†é¢‘çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºDenseDPOæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ”¹è¿›DPOè§†é¢‘å¯¹æ„å»ºæ–¹å¼  
ä¼ ç»ŸDPOç”¨ç‹¬ç«‹å™ªå£°ç”Ÿæˆè§†é¢‘å¯¹ï¼Œè€ŒDenseDPOé€šè¿‡å¯¹çœŸå®è§†é¢‘çš„æŸåå‰¯æœ¬å»å™ªæ¥åˆ›å»ºè§†é¢‘å¯¹ã€‚è¿™æ ·å¾—åˆ°çš„è§†é¢‘å¯¹å…·æœ‰ç›¸ä¼¼è¿åŠ¨ç»“æ„ä½†å±€éƒ¨ç»†èŠ‚ä¸åŒï¼Œèƒ½æœ‰æ•ˆæ¶ˆé™¤è¿åŠ¨åå‘ï¼Œè®©è§†é¢‘å¯¹åœ¨è¯­ä¹‰å’Œè¿åŠ¨è½¨è¿¹å±‚é¢ä¿æŒé«˜ä¸€è‡´æ€§ï¼Œå‡å°‘æ— å…³å…³è”å¹²æ‰°ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»†ç²’åº¦æ—¶åºåå¥½æ ‡æ³¨  
å€ŸåŠ©ä¸Šè¿°æ„å»ºæ–¹å¼å¸¦æ¥çš„æ—¶é—´å¯¹é½ç‰¹æ€§ï¼ŒDenseDPOä¸å†å¯¹æ•´ä¸ªè§†é¢‘ç‰‡æ®µæ ‡æ³¨åå¥½ï¼Œè€Œæ˜¯å¯¹çŸ­ç‰‡æ®µï¼ˆå¦‚1ç§’åˆ‡ç‰‡ï¼‰æ ‡æ³¨ã€‚è¿™ç§æ–¹å¼èƒ½äº§ç”Ÿæ›´å¯†é›†ã€ç²¾ç¡®çš„å­¦ä¹ ä¿¡å·ï¼Œå³ä¾¿åªç”¨ä¸‰åˆ†ä¹‹ä¸€æ ‡æ³¨æ•°æ®ï¼Œä¹Ÿèƒ½åœ¨è¿åŠ¨ç”Ÿæˆä¸Šè¿œè¶… vanilla DPOï¼ŒåŒæ—¶åœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šä¸vanilla DPOæŒå¹³ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§£é”åŸºäºé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åå¥½æ ‡æ³¨  
DenseDPOå±•ç¤ºäº†åˆ©ç”¨ç°æˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®ç°è‡ªåŠ¨åå¥½æ ‡æ³¨çš„å¯èƒ½æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGPTèƒ½ç²¾å‡†é¢„æµ‹ç‰‡æ®µçº§åå¥½ï¼Œæ•ˆæœæ¥è¿‘ä¸ºç‰¹å®šä»»åŠ¡å¾®è°ƒçš„è§†é¢‘å¥–åŠ±æ¨¡å‹ï¼›ç”¨è¿™äº›è‡ªåŠ¨æ ‡æ³¨æ•°æ®è®­ç»ƒçš„DenseDPOï¼Œæ€§èƒ½æ¥è¿‘ä½¿ç”¨äººç±»æ ‡æ³¨æ•°æ®çš„ç‰ˆæœ¬ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è¿åŠ¨ç”Ÿæˆæ–¹é¢ï¼ŒDenseDPOä»…ç”¨ä¸‰åˆ†ä¹‹ä¸€æ ‡æ³¨æ•°æ®å°±å¤§å¹…è¶…è¶Švanilla DPOï¼›åœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ç­‰æŒ‡æ ‡ä¸Šï¼Œèƒ½ä¸vanilla DPOåŒ¹é…ã€‚æ­¤å¤–ï¼ŒåŸºäºé€šç”¨VLMsçš„è‡ªåŠ¨æ ‡æ³¨å®éªŒä¸­ï¼Œæ¨¡å‹åœ¨ç‰‡æ®µçº§åå¥½é¢„æµ‹è¡¨ç°è‰¯å¥½ï¼Œç”¨è‡ªåŠ¨æ ‡æ³¨è®­ç»ƒçš„DenseDPOæ€§èƒ½æ¥è¿‘äººç±»æ ‡æ³¨è®­ç»ƒçš„æ¨¡å‹ï¼ŒéªŒè¯äº†è‡ªåŠ¨æ ‡æ³¨æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºæ€è·¯ï¼šå½“éœ€å‡å°‘æ•°æ®åå·®æ—¶ï¼Œå¯å‚è€ƒâ€œåŸºäºçœŸå®æ ·æœ¬å˜ä½“æ„å»ºå¯¹æ¯”æ•°æ®â€çš„æ€è·¯ï¼Œè®©å¯¹æ¯”æ ·æœ¬åœ¨å…³é”®ç»“æ„ï¼ˆå¦‚è§†é¢‘è¿åŠ¨ç»“æ„ï¼‰ä¸Šä¿æŒä¸€è‡´ï¼Œåªåœ¨ç»†èŠ‚åŒºåˆ†ï¼Œä»¥å‰Šå¼±æ— å…³åå‘ã€‚  
2. ç»†ç²’åº¦ç›‘ç£ï¼šå¯¹äºæœ‰æ—¶é—´ã€ç©ºé—´ç­‰ç»´åº¦çš„ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚è§†é¢‘ã€é•¿æ–‡æœ¬ç­‰ï¼‰ï¼Œå¯å°è¯•å°†æ•´ä½“ç›‘ç£æ‹†è§£ä¸ºç»†ç²’åº¦å•å…ƒï¼ˆå¦‚è§†é¢‘çŸ­ç‰‡æ®µã€æ–‡æœ¬æ®µè½ï¼‰çš„ç›‘ç£ï¼Œæå‡å­¦ä¹ ä¿¡å·å¯†åº¦ä¸ç²¾åº¦ã€‚  
3. è‡ªåŠ¨æ ‡æ³¨æ¢ç´¢ï¼šåˆ©ç”¨ç°æœ‰é€šç”¨å¤§æ¨¡å‹ï¼ˆå¦‚VLMsï¼‰åšé¢†åŸŸå†…åå¥½ç­‰ä¿¡å·çš„è‡ªåŠ¨æ ‡æ³¨ï¼Œèƒ½é™ä½äººåŠ›æˆæœ¬ï¼Œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯æä¾›è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€äººå·¥æ ‡æ³¨åé¦ˆçš„ä»»åŠ¡ä¸­ã€‚

## autocircuit-rl--reinforcement-learning-driven-llm-for-automated-circuit-topology-generation
### Abstract
Analog circuit topology synthesis is integral to Electronic Design Automation
(EDA), enabling the automated creation of circuit structures tailored to
specific design requirements. However, the vast design search space and strict
constraint adherence make efficient synthesis challenging. Leveraging the
versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel
reinforcement learning (RL)-based framework for automated analog circuit
synthesis. The framework operates in two phases: instruction tuning, where an
LLM learns to generate circuit topologies from structured prompts encoding
design constraints, and RL refinement, which further improves the
instruction-tuned model using reward models that evaluate validity, efficiency,
and output voltage. The refined model is then used directly to generate
topologies that satisfy the design constraints. Empirical results show that
AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by
~14% compared to the best baselines, while reducing duplicate generation rates
by ~38%. It achieves over 60% success in synthesizing valid circuits with
limited training data, demonstrating strong generalization. These findings
highlight the framework's effectiveness in scaling to complex circuits while
maintaining efficiency and constraint adherence, marking a significant
advancement in AI-driven circuit design.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AUTOCIRCUIT - RLï¼šç”¨å¼ºåŒ–å­¦ä¹ é©±åŠ¨å¤§æ¨¡å‹ï¼Œé©æ–°æ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘ç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘åˆæˆæ˜¯ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰çš„å…³é”®éƒ¨åˆ†ï¼Œèƒ½ä¾æ®ç‰¹å®šè®¾è®¡éœ€æ±‚è‡ªåŠ¨ç”Ÿæˆç”µè·¯ç»“æ„ã€‚ç„¶è€Œï¼Œè®¾è®¡æœç´¢ç©ºé—´æä¸ºåºå¤§ï¼Œä¸”è¦ä¸¥æ ¼éµå¾ªçº¦æŸæ¡ä»¶ï¼Œè¿™è®©é«˜æ•ˆåˆæˆç”µè·¯å˜å¾—æå…·æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼ŒåŸºäºæœç´¢çš„AIç®—æ³•åœ¨æ‰©å±•æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼›ç”Ÿæˆå¼AIæ–¹æ³•é‡Œçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸å…³ç ”ç©¶ï¼Œåœ¨è§„æ¨¡ã€çµæ´»æ€§ä¸è¿­ä»£ä¼˜åŒ–ä¸Šä¹Ÿæœ‰å±€é™ï¼Œåƒå¤„ç†å¤æ‚æ‹“æ‰‘ã€å¤šç›®æ ‡çº¦æŸä¼˜åŒ–ç­‰åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè®ºæ–‡æå‡ºAUTOCIRCUIT - RLæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶
AUTOCIRCUIT - RLé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯æŒ‡ä»¤è°ƒä¼˜ï¼Œè®©LLMä»ç¼–ç è®¾è®¡çº¦æŸçš„ç»“æ„åŒ–æç¤ºä¸­å­¦ä¹ ç”Ÿæˆç”µè·¯æ‹“æ‰‘ï¼Œä½¿æ¨¡å‹åˆæ­¥å…·å¤‡æ ¹æ®çº¦æŸç”Ÿæˆæ‹“æ‰‘çš„èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–ï¼Œåˆ©ç”¨è¯„ä¼°æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè¾“å‡ºç”µå‹çš„å¥–åŠ±æ¨¡å‹ï¼Œå¯¹ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹è¿›ä¸€æ­¥æ”¹è¿›ï¼Œè®©æ¨¡å‹ç”Ÿæˆçš„ç”µè·¯åœ¨æ»¡è¶³çº¦æŸçš„åŒæ—¶æ€§èƒ½æ›´ä¼˜ï¼Œä¸”RLä¼˜åŒ–ä»…åœ¨è®­ç»ƒæ—¶è¿›è¡Œï¼Œæ¨ç†é˜¶æ®µä¸æ¶‰åŠã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèšç„¦çº¦æŸé©±åŠ¨ä¸å¤šç›®æ ‡ä¼˜åŒ–
è¯¥æ¡†æ¶ä»¥çº¦æŸé©±åŠ¨è®¾è®¡ä¸ºç›®æ ‡ï¼Œé’ˆå¯¹æ¨¡æ‹Ÿç”µè·¯åˆæˆæ‰“é€ RLæ¡†æ¶ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å…¼é¡¾æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€è¾“å‡ºç”µå‹ç­‰å¤šç›®æ ‡ï¼Œå€ŸåŠ©AIå¥–åŠ±æ¨¡å‹å¼•å¯¼æ¨¡å‹è¿­ä»£ï¼Œå®ç°äº†åœ¨æ‹“æ‰‘å’Œæ€§èƒ½æŒ‡æ ‡ä¸Šçš„åŒæ—¶ä¼˜åŒ–ï¼Œçªç ´äº†ä»¥å¾€æ–¹æ³•åœ¨å¤šç›®æ ‡çº¦æŸä¸‹çš„å±€é™ï¼Œèƒ½æ›´å¥½åœ°åº”å¯¹å®é™…è®¾è®¡ä¸­å¤æ‚çš„çº¦æŸè¦æ±‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸æœ€ä¼˜åŸºçº¿ç›¸æ¯”ï¼ŒAUTOCIRCUIT - RLç”Ÿæˆæœ‰æ•ˆç”µè·¯çš„æ¯”ä¾‹æå‡çº¦12%ï¼Œæ•ˆç‡æå‡çº¦14%ï¼Œé‡å¤ç”Ÿæˆç‡é™ä½çº¦38%ï¼›åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œåˆæˆæœ‰æ•ˆç”µè·¯çš„æˆåŠŸç‡è¶…è¿‡60%ï¼Œå±•ç°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›ï¼›è¿˜èƒ½æ”¯æŒå¤šè¾¾10ä¸ªç»„ä»¶çš„ç”µè·¯ç”Ÿæˆï¼Œåœ¨å°‘æ ·æœ¬å¾®è°ƒä¸‹å¯æ³›åŒ–åˆ°6 - 10ä¸ªç»„ä»¶çš„ç”µè·¯åœºæ™¯ï¼Œä½“ç°äº†è‰¯å¥½çš„æ‰©å±•æ€§ä¸é€‚åº”æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œä¸¤é˜¶æ®µè®­ç»ƒç»“åˆæŒ‡ä»¤è°ƒä¼˜ä¸RLä¼˜åŒ–çš„æ€è·¯ï¼Œä¸ºåˆ©ç”¨å¤§æ¨¡å‹å¤„ç†å¤æ‚ä»»åŠ¡ã€å®ç°å¤šç›®æ ‡ä¼˜åŒ–æä¾›äº†å‚è€ƒèŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€çº¦æŸé©±åŠ¨å’Œè¿­ä»£ä¼˜åŒ–çš„ç”Ÿæˆç±»ä»»åŠ¡ä¸­ï¼›ä»åº”ç”¨è§’åº¦ï¼Œè¯¥æ¡†æ¶åœ¨ç”µè·¯è®¾è®¡é¢†åŸŸæˆåŠŸçªç ´ä¼ ç»Ÿæ–¹æ³•å±€é™ï¼Œè¯æ˜äº†AIé©±åŠ¨ç”µè·¯è®¾è®¡åœ¨æ‰©å±•æ€§ã€æ•ˆç‡å’Œçº¦æŸéµå¾ªä¸Šçš„æ½œåŠ›ï¼Œä¸ºEDAé¢†åŸŸä¹ƒè‡³å…¶ä»–å·¥ç¨‹è®¾è®¡é¢†åŸŸå€ŸåŠ©å¤§æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å®ç°è‡ªåŠ¨åŒ–ã€æ™ºèƒ½åŒ–è®¾è®¡æä¾›äº†ä¼˜ç§€èŒƒä¾‹ï¼Œå¯å‘ç ”ç©¶è€…æ€è€ƒå¦‚ä½•å°†å¤§æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œè§£å†³ä¸“ä¸šé¢†åŸŸä¸­æœç´¢ç©ºé—´å¤§ã€çº¦æŸå¤æ‚çš„è®¾è®¡é—®é¢˜ã€‚

## badreward--clean-label-poisoning-of-reward-models-in-text-to-image-rlhf
### Abstract
Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.
### ğŸŒŸ è®ºæ–‡è§£è¯» | BadRewardï¼šå¤šæ¨¡æ€æ–‡æœ¬åˆ°å›¾åƒRLHFä¸­å¥–åŠ±æ¨¡å‹çš„â€œæ¸…æ´æ ‡ç­¾æŠ•æ¯’â€æ”»å‡»

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹è¿‘å¹´å‘å±•è¿…çŒ›ï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨è®©æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ä¸Šè‡³å…³é‡è¦ã€‚ä½†RLHFçš„åé¦ˆæœºåˆ¶ä¹Ÿç»™æ”»å‡»è€…ç•™ä¸‹å¯ä¹˜ä¹‹æœºã€‚æ­¤å‰é’ˆå¯¹å•æ¨¡æ€ï¼ˆæ–‡æœ¬ï¼‰çš„æŠ•æ¯’æŠ€æœ¯å­˜åœ¨ä¾èµ–è„æ ‡ç­¾æˆ–æ˜“è¢«æ£€æµ‹ç­‰é—®é¢˜ï¼Œä¸”å¤šæ¨¡æ€T2Ié¢†åŸŸçš„RLHFé˜¶æ®µæŠ•æ¯’ç ”ç©¶è¾ƒå°‘ã€‚åŒæ—¶ï¼Œç°æœ‰å¥–åŠ±æŠ•æ¯’æ–¹æ³•å¸¸éœ€æ§åˆ¶åå¥½æ ‡æ³¨æµç¨‹ï¼Œåœ¨ç°å®åœºæ™¯éš¾å®ç°ã€‚å› æ­¤ï¼Œæœ¬æ–‡èšç„¦å¤šæ¨¡æ€RLHFä¸­å¥–åŠ±æ¨¡å‹ï¼Œæå‡ºæ›´éšè”½ã€æ— éœ€æ§åˆ¶æ ‡æ³¨æµç¨‹çš„æŠ•æ¯’æ”»å‡»æ–¹æ³•BadRewardï¼Œæ¢ç´¢å¤šæ¨¡æ€ç³»ç»Ÿä¸‹RLHFé¢ä¸´çš„æ–°å¨èƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºBadRewardæ¸…æ´æ ‡ç­¾æŠ•æ¯’æ”»å‡»  
é’ˆå¯¹å¤šæ¨¡æ€RLHFä¸­çš„å¥–åŠ±æ¨¡å‹ï¼Œè®¾è®¡æ— éœ€æ§åˆ¶åå¥½æ ‡æ³¨æµç¨‹çš„æŠ•æ¯’æ”»å‡»ã€‚åŒºåˆ«äºä¾èµ–è„æ ‡ç­¾æˆ–æ˜æ˜¾å¯¹æŠ—å†…å®¹çš„ä¼ ç»Ÿæ–¹å¼ï¼ŒBadRewardç”¨â€œæ¸…æ´æ ‡ç­¾â€ï¼ˆä¸ç¯¡æ”¹åå¥½æ ‡ç­¾æœ¬èº«ï¼‰å®ç°æŠ•æ¯’ï¼Œæå‡æ”»å‡»éšè”½æ€§ä¸ç°å®å¨èƒæ€§ï¼Œèƒ½åœ¨æ³¨å…¥å°‘é‡è‡ªç„¶æ ·ä¾‹æ—¶è¯¯å¯¼å¥–åŠ±æ¨¡å‹ï¼Œè¿›è€Œè®©T2Iæ¨¡å‹ç”Ÿæˆæœ‰å®³æˆ–ä¸å½“è¾“å‡ºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§†è§‰ç‰¹å¾ç¢°æ’ç­–ç•¥  
é€šè¿‡åœ¨åµŒå…¥ç©ºé—´ä¸­è¯±å¯¼è§†è§‰ä¸ŠçŸ›ç›¾çš„åå¥½æ•°æ®å®ä¾‹é—´å‘ç”Ÿç‰¹å¾ç¢°æ’ï¼Œæ¥ç ´åå¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚ä¸ç›´æ¥ä¿®æ”¹åå¥½æ ‡ç­¾ï¼Œè€Œæ˜¯æ“çºµç‰¹å¾è¡¨ç¤ºï¼Œä»¥æ­¤è…èš€å¥–åŠ±ä¿¡å·ï¼Œè®©æ”»å‡»æ›´éšè”½ä¸”å…·å®ç”¨æ€§ï¼Œç»•å¼€å¯¹æ ‡æ³¨æµç¨‹çš„æ§åˆ¶éœ€æ±‚ï¼Œå¢å¼ºæ”»å‡»å¯è¡Œæ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Stable Diffusion v1.4å’ŒSD Turboç­‰ä¸»æµT2Iæ¨¡å‹ä¸Šå¼€å±•å¤§é‡å®éªŒï¼ŒéªŒè¯BadRewardèƒ½æŒç»­å¼•å¯¼æ¨¡å‹é’ˆå¯¹ç‰¹å®šæ¦‚å¿µç”Ÿæˆä¸å½“è¾“å‡ºï¼ˆå¦‚åè§ã€æš´åŠ›å›¾åƒç­‰ï¼‰ï¼Œå±•ç°äº†åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸è®¾ç½®ä¸‹çš„æœ‰æ•ˆæ€§ã€éšè”½æ€§ä¸å¯è¿ç§»æ€§ï¼Œæœ‰åŠ›è¯æ˜å¤šæ¨¡æ€RLHFåœºæ™¯ä¸‹è¯¥æ”»å‡»å¸¦æ¥çš„å¨èƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»å®‰å…¨ç ”ç©¶è§’åº¦ï¼Œæœ¬æ–‡é¦–æ¬¡æ·±å…¥å¤šæ¨¡æ€T2Içš„RLHFé˜¶æ®µå¥–åŠ±æ¨¡å‹æŠ•æ¯’ï¼Œæå‡ºçš„BadRewardä¸ºè¯¥é¢†åŸŸå®‰å…¨å¨èƒåˆ†ææä¾›æ–°è§†è§’ä¸æ–¹æ³•å‚è€ƒï¼Œæ¨åŠ¨å¯¹å¤šæ¨¡æ€ç³»ç»ŸRLHFå®‰å…¨çš„ç ”ç©¶ï¼›ä»é˜²å¾¡è§’åº¦ï¼Œå‡¸æ˜¾å¤šæ¨¡æ€RLHFé¢ä¸´çš„æ”¾å¤§å¨èƒï¼Œä¸ºåç»­ç ”å‘é²æ£’é˜²å¾¡æœºåˆ¶æ•²å“è­¦é’Ÿï¼ŒæŒ‡å¼•ç ”ç©¶è€…å…³æ³¨å¥–åŠ±æ¨¡å‹å±‚é¢çš„éšè”½æ”»å‡»å¹¶è®¾è®¡å¯¹åº”é˜²æŠ¤ç­–ç•¥ï¼›ä»æ–¹æ³•åˆ›æ–°çœ‹ï¼Œè§†è§‰ç‰¹å¾ç¢°æ’æ€è·¯ä¸ºæŠ•æ¯’æ”»å‡»åœ¨å¤šæ¨¡æ€åœºæ™¯çš„éšè”½å®æ–½æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å‘åç»­åœ¨ç‰¹å¾å±‚é¢åšæ›´ç²¾ç»†çš„æ”»å‡»æˆ–é˜²å¾¡æ¢ç´¢ã€‚ 

## smoothed-preference-optimization-via-renoise-inversion-for-aligning-diffusion-models-with-varied-human-preferences
### Abstract
Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation
models with human preferences using pairwise preference data. Although
substantial resources are expended in collecting and labeling datasets, a
critical aspect is often neglected: \textit{preferences vary across individuals
and should be represented with more granularity.} To address this, we propose
SmPO-Diffusion, a novel method for modeling preference distributions to improve
the DPO objective, along with a numerical upper bound estimation for the
diffusion optimization objective. First, we introduce a smoothed preference
distribution to replace the original binary distribution. We employ a reward
model to simulate human preferences and apply preference likelihood averaging
to improve the DPO loss, such that the loss function approaches zero when
preferences are similar. Furthermore, we utilize an inversion technique to
simulate the trajectory preference distribution of the diffusion model,
enabling more accurate alignment with the optimization objective. Our approach
effectively mitigates issues of excessive optimization and objective
misalignment present in existing methods through straightforward modifications.
Our SmPO-Diffusion achieves state-of-the-art performance in preference
evaluation, outperforming baselines across metrics with lower training costs.
The project page is https://jaydenlyh.github.io/SmPO-project-page/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¹³æ»‘åå¥½ä¼˜åŒ–ä¸ReNoiseé€†å˜æ¢ï¼Œè®©æ‰©æ•£æ¨¡å‹ç²¾å‡†å¯¹é½å¤šæ ·äººç±»åå¥½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹è™½å·²å¹¿æ³›æµè¡Œï¼Œä½†åœ¨æ–‡æœ¬æ¸²æŸ“ã€ç©ºé—´å¸ƒå±€ã€å…‰ç…§ç­‰æ–¹é¢ä»å­˜ä¸è¶³ã€‚å½“å‰åŸºäºäººç±»åé¦ˆå¯¹é½æ¨¡å‹çš„ç ”ç©¶ï¼Œä¸€æ–¹é¢æ”¶é›†å¸¦åå¥½æ ‡æ³¨æ•°æ®æˆæœ¬é«˜ä¸”é»˜è®¤äºŒå…ƒåå¥½åˆ†å¸ƒï¼Œå¿½ç•¥äº†äººç±»åå¥½çš„ä¸ªä½“å·®å¼‚ä¸ç»†ç²’åº¦ï¼›å¦ä¸€æ–¹é¢ç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡åå¥½å’Œä¼˜åŒ–ç›®æ ‡ä¼°è®¡ä¸Šå­˜åœ¨åå·®ï¼Œæ˜“å¯¼è‡´è¿‡åº¦ä¼˜åŒ–ã€ç›®æ ‡é”™ä½ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨æå‡ºæ›´é€‚é…äººç±»å¤šæ ·åå¥½çš„æ‰©æ•£æ¨¡å‹å¯¹é½æ–¹æ³•ï¼Œè§£å†³ä¸Šè¿°ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¹³æ»‘åå¥½å»ºæ¨¡ï¼ˆSmoothed Preference Modelingï¼‰  
è€ƒè™‘åˆ°â€œä¸€åƒä¸ªäººçœ¼ä¸­æœ‰ä¸€åƒä¸ªå“ˆå§†é›·ç‰¹â€å¼çš„äººç±»åå¥½å·®å¼‚ï¼Œè®ºæ–‡ç”¨**å¹³æ»‘åå¥½åˆ†å¸ƒ**æ›¿ä»£ä¼ ç»ŸäºŒå…ƒåå¥½åˆ†å¸ƒã€‚å€ŸåŠ©å¥–åŠ±æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå¹³æ»‘åå¥½ï¼ŒæŠŠåå¥½çš„å¹³å‡ä¼¼ç„¶èå…¥DPOæŸå¤±å‡½æ•°ï¼Œå½“åå¥½ç›¸ä¼¼æ—¶æŸå¤±è¶‹è¿‘äº0ï¼Œæ—¢å‡è½»æ ‡ç­¾åå·®ï¼Œåˆé™ä½äººå·¥æ ‡æ³¨æˆæœ¬ã€æå‡æ–¹æ³•é€‚åº”æ€§ï¼Œè®©æ¨¡å‹æ›´è´´åˆçœŸå®å¤šæ ·çš„äººç±»åå¥½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šReNoiseé€†å˜æ¢ä¼˜åŒ–ï¼ˆOptimization via ReNoise Inversionï¼‰  
ä¸ºç²¾å‡†ä¼°è®¡æ‰©æ•£æ¨¡å‹çš„è½¨è¿¹åå¥½åˆ†å¸ƒï¼Œè®ºæ–‡é‡‡ç”¨ReNoiseé€†å˜æ¢æ–¹æ³•æ¥ä¼°è®¡é‡‡æ ·è½¨è¿¹ï¼Œæ›¿ä»£è¿‡å¾€åŸºäºå‰å‘è¿‡ç¨‹çš„ä¼°è®¡æ–¹å¼ã€‚è¿™æœ‰æ•ˆè§£å†³äº†ä¼˜åŒ–ç›®æ ‡ä¸æœŸæœ›ç»“æœé”™ä½é—®é¢˜ï¼Œè®©æ¨¡å‹å¾®è°ƒæ›´é«˜æ•ˆï¼Œä½¿æ‰©æ•£æ¨¡å‹åœ¨ä¼˜åŒ–æ—¶èƒ½æ›´ç²¾å‡†å¯¹é½äººç±»åå¥½ç›®æ ‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡å°†æå‡ºçš„SmPO - Diffusionä¸ä¸»æµåŸºçº¿æ–¹æ³•åœ¨äººç±»åå¥½å¯¹é½ä»»åŠ¡ä¸Šå¯¹æ¯”ã€‚åœ¨å¤šä¸ªäººç±»åå¥½è¯„ä¼°æŒ‡æ ‡ä¸Šï¼ŒSmPO - Diffusionè¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ï¼›å›¾åƒç”Ÿæˆè´¨é‡ï¼ˆå¦‚ç©ºé—´å¸ƒå±€ã€æ–‡æœ¬æ¸²æŸ“ã€å…‰ç…§ç­‰ç»´åº¦ï¼‰æœ‰æ˜¾è‘—æå‡ï¼ˆæ–‡ä¸­å›¾1ã€3ã€4å±•ç¤ºç¤ºä¾‹ï¼‰ï¼›è®­ç»ƒæ•ˆç‡ä¹Ÿå¤§å¹…æé«˜ï¼Œæ¯”Diffusion - KTOå¿«26å€ï¼ˆè¡¨1æ•°æ®æ”¯æ’‘ï¼‰ï¼Œå®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„åŒé‡ä¼˜åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åå¥½å»ºæ¨¡æ€è·¯ï¼šé¢å¯¹äººç±»åå¥½çš„å¤šæ ·æ€§ä¸æ¨¡ç³Šæ€§ï¼Œç”¨â€œå¹³æ»‘åŒ–â€æ€è·¯æ›¿ä»£éé»‘å³ç™½çš„äºŒå…ƒå‡è®¾ï¼Œä¸ºå¤„ç†å¸¦ä¸»è§‚å€¾å‘çš„ä»»åŠ¡æä¾›äº†æ›´æŸ”æ€§çš„å»ºæ¨¡èŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è€ƒè™‘ç”¨æˆ·åå¥½å·®å¼‚çš„ç”Ÿæˆæˆ–å¯¹é½ä»»åŠ¡ã€‚  
2. ä¼˜åŒ–ç›®æ ‡ä¼°è®¡ï¼šé€šè¿‡é€†å˜æ¢æ–¹å¼æ”¹è¿›æ‰©æ•£æ¨¡å‹è½¨è¿¹ä¼°è®¡ï¼Œå¯å‘æˆ‘ä»¬åœ¨å¤„ç†ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–æ—¶ï¼Œè¦å…³æ³¨è¿‡ç¨‹å±‚é¢çš„ç²¾å‡†åº¦ï¼Œä»é‡‡æ ·è½¨è¿¹ç­‰æ›´ç»†ç²’åº¦ç¯èŠ‚ä¼˜åŒ–å¯¹é½æ•ˆæœã€‚  
3. é«˜æ•ˆè®­ç»ƒå®è·µï¼šåœ¨æå‡æ€§èƒ½åŒæ—¶å…¼é¡¾è®­ç»ƒæ•ˆç‡ï¼Œä¸ºå·¥ä¸šç•Œè½åœ°éœ€å¿«é€Ÿè¿­ä»£çš„åœºæ™¯æä¾›äº†å‚è€ƒï¼Œè¯æ˜äº†æ–¹æ³•åœ¨å®ç”¨æ€§ä¸Šçš„æ½œåŠ›ã€‚

