# Paper List from BIB File: tmp0v9vy7jo.bib
- [25/06] **SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis**  
[[Paper](http://arxiv.org/pdf/2506.02096v1)] [[Code/Page]()] [[TLDR/Notes](#synthrl--scaling-visual-reasoning-with-verifiable-data-synthesis)]

- [25/05] **SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond**  
[[Paper](http://arxiv.org/pdf/2505.19641v4)] [[Code/Page](https://github.com/MiniMax-AI/SynLogic.)] [[TLDR/Notes](#synlogic--synthesizing-verifiable-reasoning-data-at-scale-for-learning-logical-reasoning-and-beyond)]

- [25/05] **Synthetic Data RL: Task Definition Is All You Need**  
[[Paper](http://arxiv.org/pdf/2505.17063v1)] [[Code/Page](https://github.com/gydpku/Data_Synthesis_RL/.)] [[TLDR/Notes](#synthetic-data-rl--task-definition-is-all-you-need)]

- [25/04] **RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library**  
[[Paper](http://arxiv.org/pdf/2504.20426v1)] [[Code/Page]()] [[TLDR/Notes](#rv-syn--rational-and-verifiable-mathematical-reasoning-data-synthesis-based-on-structured-function-library)]

- [25/04] **Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use**  
[[Paper](http://arxiv.org/pdf/2504.04736v2)] [[Code/Page]()] [[TLDR/Notes](#synthetic-data-generation-&-multi-step-rl-for-reasoning-&-tool-use)]

- [25/05] **VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL**  
[[Paper](http://arxiv.org/pdf/2505.23977v1)] [[Code/Page]()] [[TLDR/Notes](#visualsphinx--large-scale-synthetic-vision-logic-puzzles-for-rl)]



# TLDR/Notes
## synthrl--scaling-visual-reasoning-with-verifiable-data-synthesis
### Abstract
Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SynthRLï¼šç”¨å¯éªŒè¯æ•°æ®åˆæˆæ‰©å±•è§†è§‰æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼ŒåŸºäºå¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œè™½å·²åœ¨é«˜æ•ˆæ‰©å±•æµ‹è¯•æ—¶è®¡ç®—æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¦‚ä½•è¿›ä¸€æ­¥åˆ©ç”¨åˆæˆçš„å¼ºåŒ–å­¦ä¹ æ•°æ®æ¥æå‡ RLVR æ•ˆæœä»æ˜¯å¾…æ¢ç´¢çš„æ–¹å‘ã€‚ç°æœ‰çš„è®­ç»ƒæ•°æ®è§„æ¨¡ä¸è´¨é‡ï¼Œå¯¹äºè®©æ¨¡å‹æŒæ¡æ›´å¤æ‚ã€æ›´å…·æŒ‘æˆ˜æ€§çš„æ¨ç†èƒ½åŠ›å­˜åœ¨ä¸è¶³ï¼Œå› æ­¤éœ€è¦ä¸€å¥—å¯æ‰©å±•ä¸”èƒ½ä¿éšœè´¨é‡çš„è‡ªåŠ¨æ•°æ®æ‰©å±• pipelineï¼Œæ¥ä¸ºé¢å‘æ¨ç†çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›æ”¯æŒï¼Œè¿™ä¾¿æ˜¯æœ¬æ–‡å·¥ä½œçš„å‡ºå‘ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º SynthRL  pipeline  
SynthRL æ˜¯ä¸€ä¸ªç”¨äºé¢å‘æ¨ç†çš„ RL è®­ç»ƒä¸­è‡ªåŠ¨æ•°æ®æ‰©å±•çš„å¯æ‰©å±•ä¸”æœ‰ä¿éšœçš„æµç¨‹ã€‚å®ƒåŒ…å«ä¸‰ä¸ªå…³é”®é˜¶æ®µï¼Œä»ç§å­é—®é¢˜é€‰æ‹©åˆ°é—®é¢˜å˜ä½“å¢å¼ºå†åˆ°éªŒè¯ï¼Œå½¢æˆå®Œæ•´çš„é«˜è´¨é‡æ•°æ®åˆæˆé“¾è·¯ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ›´ä¸°å¯Œä¸”å¯é çš„è®­ç»ƒç´ æã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰é˜¶æ®µååŒå·¥ä½œæœºåˆ¶  
- é˜¶æ®µä¸€ï¼šé€‰æ‹©å…·æœ‰åˆé€‚åˆ†å¸ƒçš„ç§å­é—®é¢˜ã€‚è¿™ä¸€æ­¥ä¸ºåç»­æ•°æ®åˆæˆå¥ å®šåŸºç¡€ï¼Œç¡®ä¿åˆå§‹ç§å­èƒ½è¦†ç›–åˆç†çš„åˆ†å¸ƒèŒƒå›´ï¼Œè®©åç»­åˆæˆæ•°æ®æœ‰è‰¯å¥½çš„â€œåŸºå› â€ã€‚  
- é˜¶æ®µäºŒï¼šåœ¨ä¿ç•™åŸå§‹ç­”æ¡ˆçš„åŒæ—¶å°†ç§å­é—®é¢˜å¢å¼ºä¸ºæ›´å…·æŒ‘æˆ˜æ€§çš„å˜ä½“ã€‚æ—¢ä¿è¯äº†ç­”æ¡ˆçš„ä¸€è‡´æ€§ï¼Œåˆèƒ½è®©æ¨¡å‹æ¥è§¦åˆ°æ›´å¤æ‚çš„é—®é¢˜å½¢å¼ï¼Œé”»ç‚¼æ¨ç†èƒ½åŠ›ã€‚  
- é˜¶æ®µä¸‰ï¼šä¿éšœå‹éªŒè¯é˜¶æ®µã€‚è¯¥é˜¶æ®µç¡®ä¿äº†åˆæˆåæ•°æ®è¿‘ä¹å®Œç¾çš„æ­£ç¡®æ€§ä»¥åŠéš¾åº¦çš„æœ‰æ•ˆæå‡ï¼Œè®©åˆæˆæ•°æ®åœ¨è´¨é‡ä¸Šæœ‰å¯é ä¿éšœï¼Œé¿å…å¼•å…¥é”™è¯¯æ•°æ®å¹²æ‰°æ¨¡å‹è®­ç»ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ MMK12 æ•°æ®é›†ä¸Šå¼€å±•çš„å®è¯å®éªŒæœ‰åŠ›åœ°å±•ç°äº† SynthRL çš„å¯æ‰©å±•æ€§ä¸æœ‰æ•ˆæ€§ã€‚ä»çº¦ 8K çš„ç§å­æ ·æœ¬ä¸­ï¼ŒSynthRL åˆæˆå‡ºè¶… 3.3K é¢å¤–çš„å¯éªŒè¯ä¸”æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä½¿ç”¨è¯¥åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨äº”ä¸ªåŸŸå¤–è§†è§‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æŒç»­æ€§èƒ½æå‡ï¼Œç›¸æ¯”ä»…ç”¨ç§å­æ•°æ®è®­ç»ƒçš„åŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹è¿›ã€‚æ›´å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œè¯¦ç»†åˆ†æè¡¨æ˜åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ ·æœ¬ä¸Šï¼Œæ€§èƒ½æå‡æ›´ä¸ºæ˜æ˜¾ï¼Œè¿™å‡¸æ˜¾äº† SynthRL åœ¨æ¿€å‘æ¨¡å‹æ›´æ·±å…¥ã€æ›´å¤æ‚æ¨ç†æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ•°æ®åˆæˆæ€è·¯ï¼šSynthRL å±•ç¤ºäº†é€šè¿‡åˆç†çš„å¤šé˜¶æ®µæµç¨‹æ¥åˆæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„èŒƒå¼ï¼Œä¸ºå…¶ä»–éœ€è¦æ•°æ®æ‰©å±•çš„ä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯æ¨ç†ç±»ä»»åŠ¡ï¼‰æä¾›äº†â€œé€‰æ‹© - å¢å¼º - éªŒè¯â€çš„å‚è€ƒæ¡†æ¶ï¼Œå¯å‘ç ”ç©¶è€…æ€è€ƒå¦‚ä½•ç³»ç»Ÿæ€§åœ°æ‰©å……è®­ç»ƒæ•°æ®ã€‚  
- éªŒè¯æœºåˆ¶åº”ç”¨ï¼šå…¶ä¿éšœå‹éªŒè¯é˜¶æ®µçš„è®¾è®¡ï¼Œè®©æˆ‘ä»¬æ„è¯†åˆ°åœ¨æ•°æ®åˆæˆè¿‡ç¨‹ä¸­åŠ å…¥ä¸¥æ ¼éªŒè¯ç¯èŠ‚å¯¹äºä¿è¯æ•°æ®è´¨é‡å’Œæ¨¡å‹è®­ç»ƒæ•ˆæœçš„é‡è¦æ€§ï¼Œå¯å€Ÿé‰´åˆ°å„ç±»éœ€è¦å¯¹åˆæˆæ•°æ®è´¨é‡æŠŠæ§çš„åœºæ™¯ä¸­ã€‚  
- å¤æ‚æ¨ç†èƒ½åŠ›æå‡ï¼šé’ˆå¯¹æå‡æ¨¡å‹åœ¨å¤æ‚ã€æŒ‘æˆ˜æ€§æ ·æœ¬ä¸Šçš„æ¨ç†è¡¨ç°ï¼ŒSynthRL çš„å®è·µè¯æ˜äº†åˆç†è®¾è®¡çš„æ•°æ®åˆæˆæ–¹æ³•èƒ½æœ‰æ•ˆå¼•å¯¼æ¨¡å‹æŒæ¡æ›´æ·±åº¦çš„æ¨ç†æ¨¡å¼ï¼Œè¿™ä¸ºæƒ³è¦æå‡æ¨¡å‹å¤æ‚ä»»åŠ¡å¤„ç†èƒ½åŠ›çš„ç ”ç©¶è€…æä¾›äº†æ•°æ®å±‚é¢çš„ä¼˜åŒ–æ€è·¯ã€‚  
```

## synlogic--synthesizing-verifiable-reasoning-data-at-scale-for-learning-logical-reasoning-and-beyond
### Abstract
Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SynLogicï¼šå¤§è§„æ¨¡åˆæˆå¯éªŒè¯æ¨ç†æ•°æ®ï¼ŒåŠ©åŠ›LLMé€»è¾‘æ¨ç†èƒ½åŠ›è·ƒå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•è¿›ç¨‹ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«è¯æ˜èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼ˆå¦‚OpenAI - o1ã€DeepSeek R1ç­‰æˆæœæ‰€ç¤ºï¼‰ã€‚ç„¶è€Œï¼Œå¼€æºç¤¾åŒºçš„å¤åˆ»å·¥ä½œå¤šèšç„¦äºæ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œé€šç”¨æ¨ç†èƒ½åŠ›çš„å¼€å‘æ–¹æ³•ä¸èµ„æºä»å¾…æ·±å…¥æ¢ç´¢ã€‚è¿™ä¸€ç¼ºå£éƒ¨åˆ†æºäºæ”¶é›†é€‚åˆRLçš„å¤šæ ·ä¸”å¯éªŒè¯æ¨ç†æ•°æ®çš„æŒ‘æˆ˜ã€‚é€»è¾‘æ¨ç†ä½œä¸ºæ¨ç†çš„åŸºç¡€æ„æˆè¦ç´ ï¼Œå¯¹é€šç”¨æ¨ç†èƒ½åŠ›å‘å±•è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œå¦‚ä½•å¤§è§„æ¨¡ç”Ÿæˆå¤šæ ·ã€å¯éªŒè¯çš„é€»è¾‘æ¨ç†æ•°æ®ä»¥æ”¯æ’‘RLè®­ç»ƒï¼Œæˆä¸ºæ¨åŠ¨LLMsé€šç”¨æ¨ç†èƒ½åŠ›è¿›æ­¥çš„å…³é”®é—®é¢˜ï¼Œè¿™ä¹Ÿæ­£æ˜¯æœ¬æ–‡å·¥ä½œçš„å‡ºå‘ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSynLogicæ•°æ®åˆæˆæ¡†æ¶ä¸æ•°æ®é›†
SynLogicèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆæ¶µç›–35ç§ä¸åŒé€»è¾‘æ¨ç†ä»»åŠ¡çš„é€»è¾‘æ¨ç†æ•°æ®ã€‚è¯¥æ¡†æ¶çªç ´äº†ä»¥å¾€æ•°æ®æ”¶é›†åœ¨å¤šæ ·æ€§ä¸è§„æ¨¡ä¸Šçš„é™åˆ¶ï¼Œä¸ºLLMsé€»è¾‘æ¨ç†èƒ½åŠ›è®­ç»ƒæä¾›äº†ä¸°å¯Œä¸”å¤šå…ƒçš„ç´ ææ¥æºï¼Œè¦†ç›–å¤šç§é€»è¾‘æ¨ç†åœºæ™¯ï¼Œè®©æ¨¡å‹æœ‰æœºä¼šå­¦ä¹ ä¸åŒç±»å‹çš„é€»è¾‘æ€è€ƒæ–¹å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¯æ§çš„æ•°æ®åˆæˆä¸å¯éªŒè¯æ€§è®¾è®¡
ä¸€æ–¹é¢ï¼ŒSynLogicæ”¯æŒå¯¹æ•°æ®éš¾åº¦å’Œæ•°é‡è¿›è¡Œçµæ´»è°ƒæ•´ï¼Œå¯æ ¹æ®è®­ç»ƒéœ€æ±‚å®šåˆ¶åŒ–ç”Ÿæˆæ•°æ®ï¼Œæ»¡è¶³ä¸åŒé˜¶æ®µã€ä¸åŒæ¨¡å‹è§„æ¨¡å¯¹è®­ç»ƒæ•°æ®çš„è¦æ±‚ï¼›å¦ä¸€æ–¹é¢ï¼Œæ‰€æœ‰ç”Ÿæˆçš„ç¤ºä¾‹éƒ½èƒ½é€šè¿‡ç®€å•è§„åˆ™éªŒè¯ï¼Œè¿™ä¸ºåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåˆ›é€ äº†ç†æƒ³æ¡ä»¶ï¼Œè§£å†³äº†æ¨ç†æ•°æ®éš¾ä»¥éªŒè¯ä»è€Œå½±å“RLè®­ç»ƒæœ‰æ•ˆæ€§çš„é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒä¸­ï¼ŒåŸºäº7Bå’Œ32Bè§„æ¨¡çš„æ¨¡å‹éªŒè¯äº†åœ¨SynLogicæ•°æ®é›†ä¸Šè¿›è¡ŒRLè®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚SynLogicåœ¨å¼€æºæ•°æ®é›†ä¸­å®ç°äº†é€»è¾‘æ¨ç†æ€§èƒ½çš„é¢†å…ˆï¼Œåœ¨BBEHåŸºå‡†ä¸Šè¶…è¶ŠDeepSeek - R1 - Distill - Qwen - 32Bè¾¾6ä¸ªç™¾åˆ†ç‚¹ï¼›æ­¤å¤–ï¼Œå°†SynLogicæ•°æ®ä¸æ•°å­¦ã€ç¼–ç ä»»åŠ¡æ•°æ®æ··åˆè®­ç»ƒæ—¶ï¼Œä¸ä»…æå‡äº†è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†æ¨ç†æ³›åŒ–èƒ½åŠ›ï¼Œæ··åˆè®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†DeepSeek - R1 - Zero - Qwen - 32Bã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢ï¼ŒSynLogicçš„æ•°æ®åˆæˆæ€è·¯ä¸ºè§£å†³é€šç”¨æ¨ç†æ•°æ®ç¨€ç¼ºã€éš¾éªŒè¯é—®é¢˜æä¾›äº†åˆ›æ–°èŒƒå¼ï¼Œå…¶å¯æ§åˆæˆä¸å¯éªŒè¯è®¾è®¡å¯è¢«å€Ÿé‰´åˆ°å…¶ä»–æ¨ç†ç±»æ•°æ®æ„å»ºåœºæ™¯ï¼ŒåŠ©åŠ›ä¸åŒé¢†åŸŸæ¨ç†æ•°æ®çš„ç”Ÿæˆï¼›ä»åº”ç”¨å±‚é¢ï¼Œè¯æ˜äº†é€»è¾‘æ¨ç†æ•°æ®å¯¹LLMsé€šç”¨æ¨ç†èƒ½åŠ›æå‡çš„å…³é”®ä½œç”¨ï¼Œä»¥åŠè·¨é¢†åŸŸæ•°æ®æ··åˆè®­ç»ƒå¯¹æ•ˆç‡å’Œæ³›åŒ–æ€§çš„å¢ç›Šï¼Œä¸ºåç»­LLMsåœ¨æ¨ç†èƒ½åŠ›è®­ç»ƒæ–¹å‘æä¾›äº†æ•°æ®æ„å»ºã€è®­ç»ƒç­–ç•¥ç­‰å¤šç»´åº¦çš„å‚è€ƒï¼ŒæŒ‡å¼•ç ”ç©¶è€…å…³æ³¨é€»è¾‘æ¨ç†åŸºç¡€åœ°ä½å¹¶æ¢ç´¢æ›´ä¼˜è®­ç»ƒæ•°æ®ç»„åˆæ–¹å¼ã€‚åŒæ—¶ï¼Œå¼€æºçš„æ•°æ®åˆæˆ pipeline å’Œæ•°æ®é›†ä¹Ÿä¸ºç¤¾åŒºæä¾›äº†ç›´æ¥å¯ç”¨çš„èµ„æºä¸ç ”ç©¶åŸºç¡€ï¼Œæ–¹ä¾¿æ›´å¤šäººåœ¨æ­¤ä¹‹ä¸Šå¼€å±•å·¥ä½œã€‚
```

## synthetic-data-rl--task-definition-is-all-you-need
### Abstract
Reinforcement learning (RL) is a powerful way to adapt foundation models to
specialized tasks, but its reliance on large-scale human-labeled data limits
broad adoption. We introduce Synthetic Data RL, a simple and general framework
that reinforcement fine-tunes models using only synthetic data generated from a
task definition. Our method first generates question and answer pairs from the
task definition and retrieved documents, then adapts the difficulty of the
question based on model solvability, and selects questions using the average
pass rate of the model across samples for RL training. On Qwen-2.5-7B, our
method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9
pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on
GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA
(finance). It surpasses supervised fine-tuning under the same data budget and
nearly matches RL with full human data across datasets (e.g., +17.2 pp on
GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only
by 0.4 pp, showing a limited added value. By reducing human data annotation,
Synthetic Data RL enables scalable and efficient RL-based model adaptation.
Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»…ç”¨ä»»åŠ¡å®šä¹‰ï¼ŒSynthetic Data RLé©æ–°å¼ºåŒ–å­¦ä¹ æ•°æ®èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯è®©åŸºç¡€æ¨¡å‹é€‚é…ç‰¹å®šä»»åŠ¡çš„æœ‰åŠ›æ‰‹æ®µï¼Œç„¶è€Œå…¶å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–é™åˆ¶äº†å¹¿æ³›åº”ç”¨ã€‚è·å–å¤§è§„æ¨¡é«˜è´¨é‡çš„äººå·¥æ ‡æ³¨æ•°æ®æˆæœ¬é«˜ã€è€—æ—¶é•¿ï¼Œè¿™æˆä¸ºRLåœ¨æ›´å¤šåœºæ™¯è½åœ°çš„é˜»ç¢ã€‚å› æ­¤ï¼Œå¦‚ä½•æ‘†è„±å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®çš„å¼ºä¾èµ–ï¼Œå®ç°é«˜æ•ˆçš„åŸºäºRLçš„æ¨¡å‹é€‚é…ï¼Œæ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºä»»åŠ¡å®šä¹‰ç”Ÿæˆåˆæˆæ•°æ®  
Synthetic Data RLæå‡ºä»…åˆ©ç”¨ä»»åŠ¡å®šä¹‰æ¥ç”Ÿæˆåˆæˆæ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„é€šç”¨æ¡†æ¶ã€‚é¦–å…ˆä»ä»»åŠ¡å®šä¹‰å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­ç”Ÿæˆé—®ç­”å¯¹ï¼Œæ‘†è„±äº†å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›åŸºç¡€æ•°æ®æ¥æºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€è°ƒæ•´é—®é¢˜éš¾åº¦ä¸é€‰æ‹©ç­–ç•¥  
æ ¹æ®æ¨¡å‹çš„å¯è§£æ€§æ¥è°ƒæ•´é—®é¢˜éš¾åº¦ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®èƒ½é€‚é…æ¨¡å‹å½“å‰èƒ½åŠ›ï¼›åŒæ—¶åˆ©ç”¨æ¨¡å‹åœ¨æ ·æœ¬ä¸Šçš„å¹³å‡é€šè¿‡ç‡æ¥é€‰æ‹©é—®é¢˜ç”¨äºRLè®­ç»ƒï¼Œè®©è®­ç»ƒæ•°æ®çš„é€‰æ‹©æ›´ç§‘å­¦åˆç†ï¼Œæå‡è®­ç»ƒæ•ˆç‡ä¸æ•ˆæœã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Qwen - 2.5 - 7Bæ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼šGSM8Kæ•°æ®é›†ä¸Šæ¯”åŸºç¡€æ¨¡å‹ç»å¯¹æå‡29.2%ï¼ˆæ¯”æŒ‡ä»¤å¾®è°ƒé«˜2.9ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¯”Self - Instructé«˜6.6ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼›MATHæ•°æ®é›†æå‡8.7%ï¼›GPQAæ•°æ®é›†æå‡13.1%ï¼ˆæ¯”SynthLLMé«˜7.0ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼›MedQAæå‡8.9%ï¼›CQAï¼ˆlawï¼‰æå‡17.7%ï¼›CFAï¼ˆfinanceï¼‰æå‡13.7%ã€‚åœ¨ç›¸åŒæ•°æ®é¢„ç®—ä¸‹è¶…è¶Šæœ‰ç›‘ç£å¾®è°ƒï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ ä¹èƒ½ä¸ä½¿ç”¨å®Œæ•´äººå·¥æ•°æ®çš„RLæ•ˆæœåŒ¹æ•Œï¼ˆå¦‚GSM8Kä¸Šé«˜17.2ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚å¦å¤–ï¼Œæ·»åŠ 100ä¸ªäººå·¥æ¼”ç¤ºä»…è®©GSM8Kæ€§èƒ½æå‡0.4ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯´æ˜äººå·¥æ•°æ®é™„åŠ å€¼æœ‰é™ï¼Œå‡¸æ˜¾äº†åˆæˆæ•°æ®æ–¹æ³•çš„ä¼˜åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è¯¥è®ºæ–‡æä¾›äº†ä¸€ç§æ‘†è„±å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®æŸç¼šçš„æ–°æ€è·¯ï¼Œå…¶åŸºäºä»»åŠ¡å®šä¹‰ç”Ÿæˆåˆæˆæ•°æ®ã€åŠ¨æ€è°ƒæ•´éš¾åº¦ä¸é€‰æ‹©è®­ç»ƒæ•°æ®çš„ç­–ç•¥ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ åœ¨æ¨¡å‹é€‚é…é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°èŒƒå¼ã€‚å¯¹äºæƒ³è¦åœ¨èµ„æºæœ‰é™æˆ–æ•°æ®æ ‡æ³¨å›°éš¾åœºæ™¯ä¸‹è¿›è¡Œæ¨¡å‹ä¼˜åŒ–çš„ç ”ç©¶è€…å’Œå¼€å‘è€…è€Œè¨€ï¼Œè¿™ç§åˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„å€Ÿé‰´æ„ä¹‰ï¼Œæœ‰æœ›æ¨åŠ¨RLæ›´é«˜æ•ˆã€å¯æ‰©å±•åœ°åº”ç”¨äºå„ç±»ç‰¹å®šä»»åŠ¡ä¸­ã€‚åŒæ—¶ï¼Œä»£ç å’Œæ¼”ç¤ºçš„å¼€æºä¹Ÿæ–¹ä¾¿ç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶ä¸æ‹“å±•è¯¥æ–¹æ³•ã€‚
```

## rv-syn--rational-and-verifiable-mathematical-reasoning-data-synthesis-based-on-structured-function-library
### Abstract
The advancement of reasoning capabilities in Large Language Models (LLMs)
requires substantial amounts of high-quality reasoning data, particularly in
mathematics. Existing data synthesis methods, such as data augmentation from
annotated training sets or direct question generation based on relevant
knowledge points and documents, have expanded datasets but face challenges in
mastering the inner logic of the problem during generation and ensuring the
verifiability of the solutions. To address these issues, we propose RV-Syn, a
novel Rational and Verifiable mathematical Synthesis approach. RV-Syn
constructs a structured mathematical operation function library based on
initial seed problems and generates computational graphs as solutions by
combining Python-formatted functions from this library. These graphs are then
back-translated into complex problems. Based on the constructed computation
graph, we achieve solution-guided logic-aware problem generation. Furthermore,
the executability of the computational graph ensures the verifiability of the
solving process. Experimental results show that RV-Syn surpasses existing
synthesis methods, including those involving human-generated problems,
achieving greater efficient data scaling. This approach provides a scalable
framework for generating high-quality reasoning datasets.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RV-Synï¼šåŸºäºç»“æ„åŒ–å‡½æ•°åº“çš„å¯è§£é‡Šã€å¯éªŒè¯æ•°å­¦æ¨ç†æ•°æ®åˆæˆæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æå‡ï¼Œå°¤å…¶æ˜¯æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œé«˜åº¦ä¾èµ–é«˜è´¨é‡çš„æ¨ç†æ•°æ®ã€‚ç°æœ‰æ•°æ®åˆæˆæ–¹æ³•ï¼ˆå¦‚åŸºäºæ ‡æ³¨è®­ç»ƒé›†çš„æ•°æ®å¢å¼ºã€åŸºäºçŸ¥è¯†ç‚¹/æ–‡æ¡£çš„ç›´æ¥é—®é¢˜ç”Ÿæˆï¼‰è™½æ‰©å……äº†æ•°æ®é›†ï¼Œä½†å­˜åœ¨ä¸¤å¤§æ ¸å¿ƒç—›ç‚¹ï¼š**ç”Ÿæˆæ—¶éš¾ä»¥æŠŠæ¡é—®é¢˜å†…åœ¨é€»è¾‘**ï¼ˆå¯¼è‡´åˆæˆé¢˜é€»è¾‘çŸ›ç›¾ã€æ¨ç†æ·±åº¦ä¸è¶³ï¼‰ï¼›**è§£ç­”è¿‡ç¨‹çš„å¯éªŒè¯æ€§å·®**ï¼ˆæ— æ³•åœ¨åˆæˆé˜¶æ®µéªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§ï¼Œå½±å“è®­ç»ƒæ•ˆç‡ï¼‰ã€‚ä¾‹å¦‚ç›´æ¥ç”Ÿæˆé¢˜å¯èƒ½è¡¨é¢åˆç†ï¼Œå®é™…å­˜åœ¨å†…éƒ¨çŸ›ç›¾æˆ–æ¨ç†æ¼æ´ï¼Œå¤æ‚å¤šæ­¥æ¨ç†é¢˜å°¤ç”šã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å…¼é¡¾é€»è¾‘åˆç†æ€§ä¸è§£ç­”å¯éªŒè¯æ€§çš„æ•°å­¦æ¨ç†æ•°æ®åˆæˆæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¨¡ä»¿äººç±»å‡ºé¢˜é€»è¾‘ï¼Œæ„å»ºâ€œè®¡ç®—å›¾-é€†ç”Ÿæˆâ€æµç¨‹  
RV-Syn å€Ÿé‰´äººç±»æ•™å¸ˆå‡ºé¢˜æ€è·¯â€”â€”å…ˆæ˜ç¡®è§£é¢˜æ­¥éª¤ï¼ˆè®¡ç®—ç›®æ ‡ï¼‰å†è¡ç”Ÿé¢˜ç›®ã€‚å…·ä½“åˆ†ä¸‰æ­¥ï¼šä»ç§å­é¢˜ä¸­æå–åŸºäºä»£ç çš„â€œè®¡ç®—å›¾â€ï¼ˆæ¯ä¸ªå‡½æ•°å¯¹åº”æ•°å­¦æ“ä½œï¼‰ï¼›æ„å»ºç»“æ„åŒ–å‡½æ•°åº“ï¼ˆä»¥å›¾æ ¼å¼æè¿°å‡½æ•°å…³ç³»ï¼‰ï¼›é€šè¿‡å›¾æ„ŸçŸ¥é‡‡æ ·ç»„åˆå‡½æ•°ç”Ÿæˆæ–°è®¡ç®—å›¾ï¼Œæ‰§è¡Œåé€†ç”Ÿæˆå¸¦æ­£ç¡®ç­”æ¡ˆçš„æ–°é¢˜ç›®ã€‚è¿™ç§â€œå…ˆæœ‰è§£ã€åæœ‰é¢˜â€çš„æ–¹å¼ï¼Œå¤©ç„¶ä¿è¯é¢˜ç›®é€»è¾‘è¿è´¯æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“æ„åŒ–å‡½æ•°åº“ + å¯æ‰§è¡Œè®¡ç®—å›¾ï¼Œä¿éšœå¯è§£é‡Šä¸å¯éªŒè¯  
- ç»“æ„åŒ–å‡½æ•°åº“ï¼šåŸºäºåˆå§‹ç§å­é—®é¢˜ï¼Œæ•´ç†å‡º Python æ ¼å¼çš„æ•°å­¦æ“ä½œå‡½æ•°é›†åˆï¼Œæ¸…æ™°å®šä¹‰å‡½æ•°é—´å…³ç³»ï¼Œä¸ºå¤æ‚æ¨ç†æä¾›â€œåŸå­æ“ä½œâ€çº§æ”¯æ’‘ã€‚  
- å¯æ‰§è¡Œè®¡ç®—å›¾ï¼šç”Ÿæˆçš„è®¡ç®—å›¾ä¸ä»…æ˜¯è§£é¢˜é€»è¾‘çš„æŠ½è±¡ï¼Œè¿˜èƒ½ç›´æ¥æ‰§è¡Œï¼Œæ—¢ä½œä¸ºé¢˜ç›®ç”Ÿæˆçš„â€œè“å›¾â€ï¼Œåˆèƒ½éªŒè¯è§£é¢˜è¿‡ç¨‹ï¼ˆæ‰§è¡Œå³éªŒè¯ï¼‰ã€‚è¿™è®©åˆæˆé¢˜ä»â€œé€»è¾‘åˆç†æ€§â€åˆ°â€œç­”æ¡ˆæ­£ç¡®æ€§â€éƒ½æœ‰è¿¹å¯å¾ªã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¯æŒå¤æ‚æ¨ç†ç»“æ„ç”Ÿæˆ  
ä¼ ç»Ÿç›´æ¥ç”Ÿæˆæ³•éš¾å¤„ç†å¤šè·³æ¨ç†ã€è¿­ä»£å¾ªç¯ã€æ¨¡å—åŒ–ç»“æ„ç­‰å¤æ‚é¢˜å‹ï¼ŒRV-Syn å€ŸåŠ©è®¡ç®—å›¾çš„ç»„åˆèƒ½åŠ›ï¼Œè‡ªç„¶æ”¯æŒè¿™äº›é«˜é˜¶æ¨ç†ç»“æ„ï¼ˆå¦‚å¤šæ­¥éª¤é¡ºåºæ‰§è¡Œã€for å¾ªç¯å¼è¿­ä»£ã€å‡½æ•°è°ƒç”¨å¼æ¨¡å—åŒ–ï¼‰ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨å¤æ‚æ¨ç†æ•°æ®åˆæˆä¸Šçš„ç©ºç™½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯ RV-Syn æ€§èƒ½ï¼š  
- å¯¹æ¯”ç°æœ‰åˆæˆæ–¹æ³•ï¼ˆå«äººå·¥è®¾è®¡é¢˜æ–¹æ³•ï¼‰ï¼ŒRV-Syn åœ¨ 5 ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå¹³å‡è¡¨ç°æ›´ä¼˜ï¼›  
- æ•ˆç‡å±‚é¢ï¼Œä»…ç”¨ä¸€åŠæ•°æ®é‡å°±è¶…è¶Šæ­¤å‰ SOTA æ–¹æ³•ï¼Œæ•°æ®è§„æ¨¡åŒ–æ›²çº¿æ›´é«˜æ•ˆï¼Œè¯æ˜å…¶â€œå°‘æ•°æ®ã€é«˜è´¨é‡â€çš„åˆæˆèƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ•°æ®åˆæˆèŒƒå¼è¿ç§»**ï¼šâ€œå…ˆè§£åé¢˜â€çš„é€†ç”Ÿæˆæ€è·¯å¯æ¨å¹¿åˆ°å…¶ä»–éœ€è¦é€»è¾‘è¿è´¯æ€§çš„é¢†åŸŸï¼ˆå¦‚ç§‘å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆï¼‰ï¼Œä¸ºå‚ç›´é¢†åŸŸæ•°æ®åˆæˆæä¾›æ–°è§†è§’ï¼›  
2. **ç»“æ„åŒ– + å¯æ‰§è¡Œçš„è®¾è®¡**ï¼šå°†ä»£ç çš„ç»“æ„åŒ–ã€å¯æ‰§è¡Œæ€§èå…¥æ•°æ®åˆæˆï¼Œæ—¢è§£å†³é€»è¾‘æ¨¡ç³Šæ€§ï¼Œåˆå®ç°è¿‡ç¨‹å¯éªŒè¯ï¼Œè¿™ç§â€œä»£ç å³è“å›¾â€çš„æ€è·¯å¯¹å·¥å…·å¢å¼ºå‹ LLM è®­ç»ƒæœ‰å‚è€ƒä»·å€¼ï¼›  
3. **å¤æ‚æ¨ç†æ•°æ®æ„å»º**ï¼šé’ˆå¯¹å¤šè·³ã€è¿­ä»£ã€æ¨¡å—åŒ–ç­‰å¤æ‚æ¨ç†åœºæ™¯ï¼ŒRV-Syn å±•ç¤ºäº†â€œä»åŸå­æ“ä½œåˆ°å¤æ‚é€»è¾‘â€çš„ç»„åˆå¼æ„å»ºè·¯å¾„ï¼Œä¸ºæå‡ LLM å¤æ‚ä»»åŠ¡å¤„ç†èƒ½åŠ›æä¾›æ•°æ®å±‚é¢çš„è§£æ³•ã€‚  
```

## synthetic-data-generation-&-multi-step-rl-for-reasoning-&-tool-use
### Abstract
Reinforcement learning has been shown to improve the performance of large
language models. However, traditional approaches like RLHF or RLAIF treat the
problem as single-step. As focus shifts toward more complex reasoning and
agentic tasks, language models must take multiple steps of text generation,
reasoning and environment interaction before generating a solution. We propose
a synthetic data generation and RL methodology targeting multi-step
optimization scenarios. This approach, called Step-Wise Reinforcement Learning
(SWiRL), iteratively generates multi-step reasoning and tool use data, and then
learns from that data. It employs a simple step-wise decomposition that breaks
each multi-step trajectory into multiple sub-trajectories corresponding to each
action by the original model. It then applies synthetic data filtering and RL
optimization on these sub-trajectories. We evaluated SWiRL on a number of
multi-step tool use, question answering, and mathematical reasoning tasks. Our
experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,
14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,
MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits
generalization across tasks: for example, training only on HotPotQA (text
question-answering) improves zero-shot performance on GSM8K (a math dataset) by
a relative 16.9%.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ­¥éª¤æ¨ç†ä¸å·¥å…·ä½¿ç”¨ï¼šåˆæˆæ•°æ®ç”Ÿæˆä¸åˆ†æ­¥å¼ºåŒ–å­¦ä¹ æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½æ–¹é¢å·²è¢«è¯å®æœ‰æ•ˆï¼Œç„¶è€Œä¼ ç»Ÿçš„RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰æˆ–RLAIFï¼ˆåŸºäºäººå·¥åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰ç­‰æ–¹æ³•å¤šèšç„¦äºâ€œå•æ­¥å†³ç­–â€åœºæ™¯ã€‚éšç€ç ”ç©¶é‡å¿ƒå‘æ›´å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€æ™ºèƒ½ä½“ä»»åŠ¡è½¬ç§»ï¼Œè¯­è¨€æ¨¡å‹éœ€è¦åœ¨ç”Ÿæˆæœ€ç»ˆè§£å†³æ–¹æ¡ˆå‰ï¼Œå®Œæˆå¤šè½®æ–‡æœ¬ç”Ÿæˆã€æ¨ç†ä»¥åŠä¸ç¯å¢ƒçš„äº¤äº’ã€‚è¿™ç§â€œå¤šæ­¥éª¤â€åœºæ™¯ä¸‹ï¼Œä¼ ç»Ÿå•æ­¥RLèŒƒå¼çš„å±€é™æ€§é€æ¸å‡¸æ˜¾â€”â€”æ— æ³•é«˜æ•ˆå¤„ç†å¤šè½®å†³ç­–é“¾æ¡ä¸­çš„ä¼˜åŒ–é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚ä½•ä¸ºå¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯è®¾è®¡æ›´é€‚é…çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæˆä¸ºæ¨åŠ¨å¤æ‚ä»»åŠ¡èƒ½åŠ›çªç ´çš„å…³é”®æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåˆ†æ­¥å¼ºåŒ–å­¦ä¹ ï¼ˆSWiRLï¼‰æ¡†æ¶  
SWiRLç„å‡†å¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯ï¼Œé‡‡ç”¨â€œè¿­ä»£ç”Ÿæˆå¤šæ­¥éª¤æ¨ç†ä¸å·¥å…·ä½¿ç”¨æ•°æ® + ä»æ•°æ®ä¸­å­¦ä¹ â€çš„åŒé˜¶æ®µæ€è·¯ã€‚å®ƒå°†å®Œæ•´çš„å¤šæ­¥éª¤è½¨è¿¹ï¼ˆtrajectoryï¼‰æ‹†è§£ä¸º**å¤šä¸ªå­è½¨è¿¹**ï¼Œæ¯ä¸ªå­è½¨è¿¹å¯¹åº”åŸå§‹æ¨¡å‹åœ¨æŸä¸€æ­¥çš„è¡ŒåŠ¨å†³ç­–ï¼›åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¯¹è¿™äº›å­è½¨è¿¹å¼€å±•åˆæˆæ•°æ®è¿‡æ»¤ä¸RLä¼˜åŒ–ï¼Œè®©æ¨¡å‹åœ¨å¤šè½®å†³ç­–ä¸­é€æ­¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆæˆæ•°æ®ç”Ÿæˆä¸å­è½¨è¿¹åˆ†è§£æœºåˆ¶  
ä¸ºäº†è®©æ¨¡å‹åœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­å……åˆ†å­¦ä¹ ï¼ŒSWiRLè®¾è®¡äº†â€œåˆ†æ­¥åˆ†è§£â€ç­–ç•¥ï¼šæŠŠä¸€æ¡å®Œæ•´çš„å¤šæ­¥éª¤äº¤äº’è¿‡ç¨‹æ‹†åˆ†æˆè‹¥å¹²å­è½¨è¿¹ï¼Œæ¯ä¸ªå­è½¨è¿¹éƒ½èƒ½ç‹¬ç«‹æ‰¿è½½â€œå†³ç­– - åé¦ˆâ€çš„å­¦ä¹ ä¿¡å·ã€‚åŒæ—¶ï¼Œé…åˆ**åˆæˆæ•°æ®è¿‡æ»¤**æœºåˆ¶ï¼Œç¡®ä¿ç”¨äºè®­ç»ƒçš„æ•°æ®è´¨é‡ï¼Œè®©æ¨¡å‹åœ¨å¤šæ­¥éª¤æ¨ç†ã€å·¥å…·è°ƒç”¨ç­‰ç¯èŠ‚ä¸­æ›´é«˜æ•ˆåœ°å¸æ”¶æœ‰æ•ˆä¿¡æ¯ï¼Œé¿å…å™ªå£°å¹²æ‰°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šç±»å¤šæ­¥éª¤ä»»åŠ¡ï¼ˆå·¥å…·ä½¿ç”¨ã€é—®ç­”ã€æ•°å­¦æ¨ç†ï¼‰ä¸­éªŒè¯äº†SWiRLçš„æ•ˆæœï¼š  
- åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡GSM8Kã€å¤šè·³é—®ç­”HotPotQAã€CofCAã€MuSiQueã€BeerQAä¸Šï¼ŒSWiRLç›¸å¯¹åŸºçº¿æ–¹æ³•çš„å‡†ç¡®ç‡åˆ†åˆ«æå‡21.5%ã€12.3%ã€14.8%ã€11.1%ã€15.3%ï¼›  
- è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›çªå‡ºï¼šä»…åœ¨æ–‡æœ¬é—®ç­”ä»»åŠ¡HotPotQAä¸Šè®­ç»ƒï¼Œå°±èƒ½è®©æ¨¡å‹åœ¨æ•°å­¦æ•°æ®é›†GSM8Kçš„é›¶æ ·æœ¬ï¼ˆzero - shotï¼‰è¡¨ç°æå‡16.9%ï¼Œå±•ç°äº†æ–¹æ³•åœ¨ä»»åŠ¡è¿ç§»ä¸Šçš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ­¥éª¤ä»»åŠ¡çš„åˆ†è§£æ€è·¯ï¼šé¢å¯¹å¤æ‚çš„å¤šè½®å†³ç­–æˆ–äº¤äº’å‹ä»»åŠ¡ï¼Œå°†é•¿è½¨è¿¹æ‹†åˆ†ä¸ºå­è½¨è¿¹çš„â€œåˆ†æ­¥ä¼˜åŒ–â€æ€è·¯ï¼Œä¸ºå¤„ç†é“¾æ¡å¼ã€é˜¶æ®µå¼çš„AIä»»åŠ¡æä¾›äº†æ¨¡å—åŒ–è§£å†³èŒƒå¼ï¼›  
2. åˆæˆæ•°æ®ä¸RLçš„ç»“åˆï¼šé€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆ + è¿‡æ»¤çš„æ–¹å¼æ‰©å……ä¼˜è´¨è®­ç»ƒç´ æï¼Œå†ç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œä¸ºæ•°æ®ç¨€ç¼ºæˆ–æ ‡æ³¨æˆæœ¬é«˜çš„å¤šæ­¥éª¤åœºæ™¯æä¾›äº†é«˜æ•ˆçš„â€œæ•°æ® - ç®—æ³•â€ååŒæ€è·¯ï¼›  
3. è·¨ä»»åŠ¡æ³›åŒ–çš„å¯å‘ï¼šSWiRLå±•ç°çš„è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›ï¼Œæç¤ºæˆ‘ä»¬åœ¨è®¾è®¡è®­ç»ƒæ–¹æ¡ˆæ—¶ï¼Œå¯æ›´å…³æ³¨ä»»åŠ¡é—´çš„åº•å±‚æ¨ç†é€»è¾‘å…±æ€§ï¼Œä¸ºé€šç”¨æ™ºèƒ½ä½“çš„è®­ç»ƒæä¾›äº†æ–¹å‘å‚è€ƒã€‚  
```

## visualsphinx--large-scale-synthetic-vision-logic-puzzles-for-rl
### Abstract
Vision language models (VLMs) are expected to perform effective multimodal
reasoning and make logically coherent decisions, which is critical to tasks
such as diagram understanding and spatial problem solving. However, current VLM
reasoning lacks large-scale and well-structured training datasets. To bridge
this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic
visual logical reasoning training data. To tackle the challenge of image
synthesis with grounding answers, we propose a rule-to-image synthesis
pipeline, which extracts and expands puzzle rules from seed questions and
generates the code of grounding synthesis image synthesis for puzzle sample
assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx
benefit from logical coherence and readability of our dataset and exhibit
improved performance on logical reasoning tasks. The enhanced reasoning
capabilities developed from VisualSphinx also benefit other reasoning tasks
such as algebraic reasoning, arithmetic reasoning and geometry reasoning.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | VisualSphinxï¼šä¸ºå¼ºåŒ–å­¦ä¹ æ‰“é€ å¤§è§„æ¨¡åˆæˆè§†è§‰é€»è¾‘è°œé¢˜æ•°æ®é›†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éœ€è¦å…·å¤‡é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ä¸é€»è¾‘è¿è´¯å†³ç­–èƒ½åŠ›ï¼Œè¿™å¯¹å›¾è¡¨ç†è§£ã€ç©ºé—´é—®é¢˜è§£å†³ç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰VLMæ¨ç†é¢†åŸŸç¼ºå°‘å¤§è§„æ¨¡ä¸”ç»“æ„è‰¯å¥½çš„è®­ç»ƒæ•°æ®é›†ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè®ºæ–‡æå‡ºæ„å»ºVisualSphinxè¿™ä¸€å¤§è§„æ¨¡åˆæˆè§†è§‰é€»è¾‘æ¨ç†è®­ç»ƒæ•°æ®ï¼ŒåŠ©åŠ›VLMåœ¨å¤šæ¨¡æ€é€»è¾‘æ¨ç†æ–¹å‘çš„å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºVisualSphinxå¤§è§„æ¨¡åˆæˆæ•°æ®é›†  
ä½œä¸ºé¦–ä¸ªè¯¥ç±»å¤§è§„æ¨¡åˆæˆè§†è§‰é€»è¾‘æ¨ç†è®­ç»ƒæ•°æ®ï¼ŒVisualSphinxè‡´åŠ›äºä¸ºVLMæä¾›å……è¶³ä¸”é«˜è´¨é‡çš„é€»è¾‘æ¨ç†è®­ç»ƒç´ æï¼Œè¦†ç›–è§†è§‰ä¸é€»è¾‘ç»“åˆçš„å¤šæ ·åœºæ™¯ï¼Œæ”¯æ’‘æ¨¡å‹å­¦ä¹ å¤šæ¨¡æ€ä¸‹çš„é€»è¾‘æ¨ç†æ¨¡å¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§„åˆ™åˆ°å›¾åƒçš„åˆæˆ pipeline  
é¢å¯¹å¸¦åŸºç¡€ç­”æ¡ˆçš„å›¾åƒåˆæˆæŒ‘æˆ˜ï¼Œè®¾è®¡äº†ä»ç§å­é—®é¢˜ä¸­æå–å¹¶æ‰©å±•è°œé¢˜è§„åˆ™ï¼Œå†ç”Ÿæˆç”¨äºè°œé¢˜æ ·æœ¬ç»„è£…çš„åŸºç¡€åˆæˆå›¾åƒä»£ç çš„ pipelineã€‚è¯¥æµç¨‹å®ç°äº†ä»æŠ½è±¡è§„åˆ™åˆ°å…·è±¡å›¾åƒåŠå¯¹åº”é€»è¾‘å…³ç³»çš„è½¬åŒ–ï¼Œä¿éšœæ•°æ®ç”Ÿæˆçš„é€»è¾‘æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸ºæ¨¡å‹æä¾›â€œå¯è§£é‡Šã€æœ‰ä¾æ®â€çš„è§†è§‰ - é€»è¾‘è®­ç»ƒæ ·æœ¬ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨GRPOåœ¨VisualSphinxä¸Šè®­ç»ƒçš„VLMï¼Œå—ç›Šäºæ•°æ®é›†çš„é€»è¾‘è¿è´¯æ€§ä¸å¯è¯»æ€§ï¼Œåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ€§èƒ½æå‡ã€‚å¹¶ä¸”ï¼Œåœ¨VisualSphinxä¸ŠåŸ¹å…»çš„å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œè¿˜èƒ½è¿ç§»åˆ°ä»£æ•°æ¨ç†ã€ç®—æœ¯æ¨ç†ã€å‡ ä½•æ¨ç†ç­‰å…¶ä»–æ¨ç†ä»»åŠ¡ä¸­ï¼Œå±•ç°äº†è¯¥æ•°æ®é›†åœ¨å¤šç»´åº¦æ¨ç†èƒ½åŠ›æå‡ä¸Šçš„ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ•°æ®æ„å»ºè§’åº¦ï¼Œé’ˆå¯¹å¤šæ¨¡æ€é€»è¾‘æ¨ç†åœºæ™¯ï¼Œé€šè¿‡â€œè§„åˆ™ - å›¾åƒâ€åˆæˆ pipeline ç”Ÿæˆå¤§è§„æ¨¡å¸¦é€»è¾‘å…³è”æ•°æ®çš„æ€è·¯ï¼Œä¸ºé¢†åŸŸå®šåˆ¶åŒ–æ•°æ®åˆ›å»ºæä¾›äº†èŒƒå¼ï¼Œå¯å¯å‘åç»­åœ¨åŒ»ç–—å½±åƒæ¨ç†ã€å·¥ç¨‹å›¾çº¸ç†è§£ç­‰å‚ç›´åœºæ™¯çš„æ•°æ®é›†æ„å»ºï¼›ä»æ¨¡å‹è®­ç»ƒè§’åº¦ï¼ŒéªŒè¯äº†é«˜è´¨é‡åˆæˆæ•°æ®å¯¹å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æå‡ä½œç”¨ï¼Œä»¥åŠèƒ½åŠ›è¿ç§»ç‰¹æ€§ï¼Œä¸ºå¤šä»»åŠ¡å­¦ä¹ ã€è·¨é¢†åŸŸæ¨ç†æ¨¡å‹è®­ç»ƒæä¾›äº†æ•°æ®å±‚é¢çš„å€Ÿé‰´æ–¹å‘ï¼Œè®©ç ”ç©¶è€…é‡è§†åˆæˆæ•°æ®åœ¨å¼¥è¡¥çœŸå®æ•°æ®ç¼ºå£ã€å®šå‘å¢å¼ºæ¨¡å‹èƒ½åŠ›ä¸Šçš„æ½œåŠ›ã€‚
```

