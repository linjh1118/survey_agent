# Paper List of Terms(Vision Language Model+Game)
- [25/09] **Measuring How (Not Just Whether) VLMs Build Common Ground**  
[[Paper](http://arxiv.org/pdf/2509.03805v1)] [[Code/Page]()] [[TLDR/Notes](#measuring-how-(not-just-whether)-vlms-build-common-ground)]

- [25/08] **HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing**  
[[Paper](http://arxiv.org/pdf/2508.05899v1)] [[Code/Page]()] [[TLDR/Notes](#holodeck-2-0--vision-language-guided-3d-world-generation-with-editing)]

- [25/08] **Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions**  
[[Paper](http://arxiv.org/pdf/2508.05430v1)] [[Code/Page]()] [[TLDR/Notes](#explaining-similarity-in-vision-language-encoders-with-weighted-banzhaf-interactions)]

- [25/08] **Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2508.04895v1)] [[Code/Page]()] [[TLDR/Notes](#automated-bug-frame-retrieval-from-gameplay-videos-using-vision-language-models)]

- [25/08] **Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success**  
[[Paper](http://arxiv.org/pdf/2508.04280v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-vision-language-model-training-with-reinforcement-learning-in-synthetic-worlds-for-real-world-success)]



# TLDR/Notes
## measuring-how-(not-just-whether)-vlms-build-common-ground
### Abstract
Large vision language models (VLMs) increasingly claim reasoning skills, yet
current benchmarks evaluate them in single-turn or question answering settings.
However, grounding is an interactive process in which people gradually develop
shared understanding through ongoing communication. We introduce a four-metric
suite (grounding efficiency, content alignment, lexical adaptation, and
human-likeness) to systematically evaluate VLM performance in interactive
grounding contexts. We deploy the suite on 150 self-play sessions of
interactive referential games between three proprietary VLMs and compare them
with human dyads. All three models diverge from human patterns on at least
three metrics, while GPT4o-mini is the closest overall. We find that (i) task
success scores do not indicate successful grounding and (ii) high
image-utterance alignment does not necessarily predict task success. Our metric
suite and findings offer a framework for future research on VLM grounding.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¡¡é‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚ä½•å»ºç«‹å…±åŒåŸºç¡€ï¼šä¸æ­¢æˆåŠŸä¸å¦ï¼Œæ›´çœ‹è¿‡ç¨‹æ–¹å¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¸¸å®£ç§°å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œä½†å½“å‰åŸºå‡†æµ‹è¯•å¤šåœ¨å•è½®æˆ–é—®ç­”åœºæ™¯ä¸‹è¯„ä¼°å®ƒä»¬ã€‚ç„¶è€Œï¼Œâ€œ groundingï¼ˆå»ºç«‹å…±åŒåŸºç¡€ï¼‰â€ æ˜¯ä¸€ä¸ªäº¤äº’å¼è¿‡ç¨‹ï¼Œäººç±»é€šè¿‡æŒç»­äº¤æµé€æ­¥å½¢æˆå…±äº«ç†è§£ï¼Œç°æœ‰è¯„ä¼°æ–¹å¼æœªå……åˆ†è€ƒé‡æ”¯æ’‘ grounding çš„äº¤äº’å¼æŠ€èƒ½ï¼ˆå¦‚å¤ç”¨å¯¹è¯ä¼™ä¼´è¯æ±‡ã€åœ¨è¾¾æˆå…±è¯†åç²¾ç®€å†—ä½™ç»†èŠ‚ç­‰ï¼‰ï¼Œä¸”å·²æœ‰è¯æ®è¡¨æ˜ VLMs åœ¨å¤šè½®äº¤äº’ä¸­æ²Ÿé€šæ•ˆç‡ä½ã€è¡¨ç°é€Šäºå•è½®åœºæ™¯ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨ä¸ºå¤šæ¨¡æ€å¯¹è¯ä¸­çš„ grounding æ“ä½œåŒ–å®šä¹‰ï¼Œå¹¶ç›´æ¥è¯„ä¼° VLMs åœ¨äº¤äº’å¼ grounding æƒ…å¢ƒä¸‹çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå››ç»´åº¦è¯„ä¼°æŒ‡æ ‡å¥—ä»¶  
å¼•å…¥åŒ…å« grounding efficiencyï¼ˆå»ºç«‹å…±åŒåŸºç¡€æ•ˆç‡ï¼‰ã€content alignmentï¼ˆå†…å®¹å¯¹é½åº¦ï¼‰ã€lexical adaptationï¼ˆè¯æ±‡é€‚åº”æ€§ï¼‰ã€human - likenessï¼ˆç±»äººæ€§ï¼‰çš„è¯„ä¼°æŒ‡æ ‡å¥—ä»¶ï¼Œç³»ç»Ÿè¯„ä¼° VLMs åœ¨äº¤äº’å¼ grounding æƒ…å¢ƒä¸‹çš„æ€§èƒ½ï¼Œè¯¥å¥—ä»¶ä¸ä¾èµ–ç‰¹å®šä»»åŠ¡ï¼Œå¯æ•æ‰å¤šæ–¹é¢ grounding è¡¨ç°ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäº PhotoBook ä»»åŠ¡å¼€å±•è¯„ä¼°ä¸å¯¹æ¯”  
é€‰æ‹© PhotoBook æŒ‡ä»£æ¸¸æˆï¼ˆå«äº”è½®å¯¹è¯ä»¥è¯†åˆ«å…±äº«å›¾åƒï¼‰å®ä¾‹åŒ–æŒ‡æ ‡å¥—ä»¶ï¼Œå¼€å±• 150 åœº VLMs è‡ªç©ä¼šè¯ï¼ˆæ¶‰åŠä¸‰ä¸ªä¸“æœ‰ VLMs ä¸¤ä¸¤äº¤äº’ï¼‰ï¼Œå¹¶ä¸äººç±»å¯¹è¯ç»„å¯¹æ¯”ï¼Œæ¢ç©¶ VLMs åœ¨å¤šç»´åº¦ä¸Šä¸äººç±»æ¨¡å¼çš„å·®å¼‚åŠæ¥è¿‘ç¨‹åº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. æ¨¡å‹è¡¨ç°ä¸äººç±»æ¨¡å¼å·®å¼‚ï¼šä¸‰ä¸ª VLMs è‡³å°‘åœ¨ä¸‰ä¸ªæŒ‡æ ‡ä¸Šä¸äººç±»æ¨¡å¼å­˜åœ¨åˆ†æ­§ï¼Œå…¶ä¸­ GPT4o - mini æ•´ä½“æœ€æ¥è¿‘äººç±»ï¼›  
2. ä»»åŠ¡æˆåŠŸä¸ grounding å…³ç³»ï¼šä»»åŠ¡æˆåŠŸåˆ†æ•°ä¸èƒ½è¡¨æ˜ grounding æˆåŠŸï¼Œå¦‚ GPT4.1 ä¼šåœ¨çœŸå€¼æ ‡ç­¾ä¸€è‡´æ—¶é€šè¿‡é•œåƒä¼™ä¼´åå¥½è™šå¢åˆ†æ•°ï¼›  
3. å›¾åƒ - è¯è¯­å¯¹é½ä¸ä»»åŠ¡æˆåŠŸå…³ç³»ï¼šé«˜å›¾åƒ - è¯è¯­å¯¹é½ï¼ˆå¦‚ CLIPScore è¡¡é‡ï¼‰ä¸ä¸€å®šèƒ½é¢„æµ‹ä»»åŠ¡æˆåŠŸï¼ŒäºŒè€…æ— ç›¸å…³æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¯„ä¼°ç»´åº¦æ‹“å±•ï¼šæå‡ºçš„å››ç»´åº¦æŒ‡æ ‡å¥—ä»¶ä¸ºæœªæ¥ VLM grounding ç ”ç©¶æä¾›äº†æ¡†æ¶ï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤å¥—ä»¶æ›´å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨äº¤äº’å¼åœºæ™¯ä¸‹å»ºç«‹å…±åŒåŸºç¡€çš„èƒ½åŠ›ï¼›  
2. è®¤çŸ¥ä¸äº¤äº’è§†è§’ï¼šä»äººç±»å¯¹è¯ä¸­ â€œå…±åŒåŸºç¡€å»ºç«‹â€ è¿™ä¸€è®¤çŸ¥å’Œäº¤äº’è§’åº¦åˆ‡å…¥è¯„ä¼° VLMsï¼Œä¸ºç†è§£æ¨¡å‹å¤šè½®äº¤äº’è¡Œä¸ºã€æ”¹è¿›æ¨¡å‹äº¤äº’å¼æŠ€èƒ½è®­ç»ƒæä¾›äº†æ–°è§†è§’ï¼›  
3. å®éªŒè®¾è®¡å‚è€ƒï¼šä»¥ PhotoBook ä»»åŠ¡ä¸ºä¾æ‰˜å¼€å±•æ¨¡å‹è‡ªç©ä¸äººç±»å¯¹æ¯”å®éªŒçš„æ–¹å¼ï¼Œä¸ºè¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šäº¤äº’å¼ä»»åŠ¡ä¸­è¡¨ç°æä¾›äº†å¯å‚è€ƒçš„å®éªŒè®¾è®¡èŒƒå¼ã€‚

## holodeck-2-0--vision-language-guided-3d-world-generation-with-editing
### Abstract
3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.
### ğŸŒŸ è®ºæ–‡è§£è¯» | HOLODECK 2.0ï¼šè§†è§‰-è¯­è¨€å¼•å¯¼çš„å¯ç¼–è¾‘3Dä¸–ç•Œç”Ÿæˆæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
3Dåœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€è‰ºæœ¯åˆ›ä½œã€è™šæ‹Ÿç°å®ç­‰è¯¸å¤šé¢†åŸŸè‡³å…³é‡è¦ã€‚ç„¶è€Œå½“å‰3Dåœºæ™¯è®¾è®¡ä¸¥é‡ä¾èµ–åˆ›ä½œè€…å¤§é‡æ‰‹åŠ¨å·¥ä½œï¼Œç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•éš¾ä»¥ç”Ÿæˆå¼€æ”¾åŸŸåœºæ™¯æˆ–æ”¯æŒçµæ´»ç¼–è¾‘ï¼Œå› æ­¤ä»æ–‡æœ¬ç›´æ¥ç”Ÿæˆ3Dä¸–ç•Œå—åˆ°è¶Šæ¥è¶Šå¤šå…³æ³¨ã€‚åŒæ—¶ï¼Œç°æœ‰æ–¹æ³•è¿˜å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™ï¼šåŸºäºèµ„äº§çš„æ–¹æ³•å—3Dæ¨¡å‹åº“è´¨é‡å’Œå¤šæ ·æ€§é™åˆ¶ï¼›éš¾ä»¥å®ç°ç»†ç²’åº¦é£æ ¼æ§åˆ¶ï¼›å¤§å¤šèšç„¦å®¤å†…ç¯å¢ƒï¼Œç¼ºä¹ç”Ÿæˆå¼€æ”¾ä¸–ç•Œåœºæ™¯çš„çµæ´»æ€§ã€‚è€Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œ3Dç”Ÿæˆæ¨¡å‹çš„çªç ´ä¸ºè§£å†³è¿™äº›å±€é™æä¾›äº†æœºé‡ï¼Œä½†å°šæœªæœ‰æ•ˆæ•´åˆåˆ°ç»Ÿä¸€åœºæ™¯ç”Ÿæˆæ¡†æ¶ä¸­ï¼ŒHOLODECK 2.0åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€æ¡†æ¶ç»“åˆVLMå¼•å¯¼ä¸3Dèµ„äº§ç”Ÿæˆ  
æå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œå°†VLMå¼•å¯¼çš„åœºæ™¯åˆ†è§£ä¸ç”Ÿæˆå¼3Dèµ„äº§åˆ›å»ºç‹¬ç‰¹ç»“åˆï¼Œåœ¨å¤šä¸ªé˜¶æ®µï¼ˆä»è§†è§‰å‚è€ƒç”Ÿæˆåˆ°å¯¹è±¡è§£æã€ç©ºé—´çº¦æŸç”Ÿæˆå’Œè¿­ä»£å¸ƒå±€ä¼˜åŒ–ï¼‰è¿ç”¨VLMsï¼Œå€ŸåŠ©3Dèµ„äº§ç”Ÿæˆæ¨¡å‹å’ŒVLMsçš„æ¨ç†èƒ½åŠ›ï¼Œç”Ÿæˆé«˜è´¨é‡ä¸”é£æ ¼ä¸€è‡´çš„3Dåœºæ™¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè·¨åœºæ™¯ä¸é£æ ¼çš„å“è¶Šè¡¨ç°  
åœ¨å®¤å†…å’Œå¼€æ”¾åŸŸåœºæ™¯ç”Ÿæˆä¸Šå±•ç°å‰æ²¿æ€§èƒ½ï¼Œèƒ½ä¿æŒä¸€è‡´è‰ºæœ¯é£æ ¼ï¼ˆå¦‚å†™å®ã€å¡é€šã€åŠ¨æ¼«ã€èµ›åšæœ‹å…‹ç­‰ï¼‰ã€‚é€šè¿‡å¤§é‡äººå·¥å’Œè‡ªåŠ¨è¯„ä¼°ï¼Œè¯æ˜å…¶åœ¨ä¸¤ç±»åœºæ™¯ç”Ÿæˆä¸­æ•ˆæœä¼˜å¼‚ï¼Œå¯ç”Ÿæˆä¸åŒé£æ ¼çš„åŒç±»å‹3Dåœºæ™¯ï¼Œåœºæ™¯ä¸­å¯¹è±¡è´¨é‡é«˜ä¸”ä¸æè¿°åŒ¹é…åº¦é«˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šäº¤äº’å¼ç¼–è¾‘ç³»ç»Ÿ  
å¼•å…¥äº¤äº’å¼ç¼–è¾‘ç³»ç»Ÿï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å®ç°é£æ ¼ä¸€è‡´çš„å¯¹è±¡æ›¿æ¢ä¸æ·»åŠ ï¼Œè¶…è¶Šä¼ ç»Ÿå¸ƒå±€è°ƒæ•´ï¼Œèƒ½çµæ´»é€‚åº”äººç±»åé¦ˆï¼Œæ”¯æŒå¸ƒå±€ä¼˜åŒ–å’Œé£æ ¼ä¸€è‡´çš„å¯¹è±¡ç¼–è¾‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå•†ä¸šæ¸¸æˆå¼•æ“é›†æˆéªŒè¯å®ç”¨æ€§  
é€šè¿‡ä¸å•†ä¸šæ¸¸æˆå¼•æ“ç›´æ¥é›†æˆéªŒè¯å®ç”¨æ€§ï¼Œå¦‚å°†æ ¹æ®è¯¦ç»†æ–‡æœ¬æè¿°ç”Ÿæˆçš„å¤æ‚åšç‰©é¦†åœºæ™¯æ— ç¼å¯¼å…¥Unreal Engineå¹¶å®ç°å®Œå…¨äº¤äº’ï¼Œç›¸æ¯”å…ˆå‰å·¥ä½œåœ¨å®¤å†…åœºæ™¯è´¨é‡æå‡10%ã€å¼€æ”¾åŸŸåœºæ™¯æå‡139% ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¤§è§„æ¨¡äººå·¥è¯„ä¼°å’ŒåŸºäºCLIPçš„è‡ªåŠ¨è¯„åˆ†éªŒè¯æ•ˆæœï¼Œåœ¨æ¶µç›–å®¤å†…å’Œå¼€æ”¾åŸŸåœºæ™¯çš„60ä¸ªæµ‹è¯•ç”¨ä¾‹ä¸­ï¼ŒHOLODECK 2.0åœ¨äººå·¥è¯„ä¼°å’ŒCLIPè‡ªåŠ¨è¯„åˆ†ä¸­å‡æŒç»­è¶…è¶ŠåŸºçº¿HOLODECKï¼Œåœ¨å¼€æ”¾åŸŸåœºæ™¯ä¸­ä¼˜åŠ¿å°¤ä¸ºæ˜æ˜¾ï¼Œæœ‰æ•ˆç”Ÿæˆäº†ä¸è¯¦ç»†æ–‡æœ¬æè¿°ç´§å¯†å¥‘åˆçš„é«˜è´¨é‡åœºæ™¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šé˜¶æ®µå¤šæ¨¡å‹ååŒæ€è·¯ï¼šå€Ÿé‰´å…¶åœ¨å¤šä¸ªé˜¶æ®µè¿ç”¨VLMså®ç°ä»åœºæ™¯åˆ†è§£åˆ°èµ„äº§ç”Ÿæˆã€å¸ƒå±€ä¼˜åŒ–ç­‰ç¯èŠ‚ååŒçš„æ€è·¯ï¼Œä¸ºè·¨æ¨¡å‹è·¨é˜¶æ®µåä½œå®Œæˆå¤æ‚ä»»åŠ¡æä¾›å‚è€ƒã€‚
2. é£æ ¼ä¸åœºæ™¯å¤šæ ·æ€§æŠŠæ§ï¼šåœ¨è¿½æ±‚åœºæ™¯å’Œé£æ ¼å¤šæ ·æ€§ã€ä¿æŒä¸€è‡´æ€§æ–¹é¢çš„æ¢ç´¢ï¼Œå¯¹éœ€è€ƒè™‘å¤šé£æ ¼å¤šåœºæ™¯ç”Ÿæˆçš„ç›¸å…³3Då†…å®¹åˆ›ä½œé¢†åŸŸæœ‰å¯å‘ã€‚
3. äº¤äº’å¼ç¼–è¾‘æ‹“å±•ï¼šå…¶äº¤äº’å¼ç¼–è¾‘ç³»ç»Ÿç»“åˆç”Ÿæˆæ¨¡å‹å®ç°å¯¹è±¡å±‚é¢é£æ ¼ä¸€è‡´ç¼–è¾‘çš„æ–¹å¼ï¼Œå¯ä¸º3Då†…å®¹ç¼–è¾‘å·¥å…·å¼€å‘ç­‰æä¾›æ–°æ€è·¯ã€‚ 
4. äº§ä¸šè½åœ°éªŒè¯ï¼šä¸å•†ä¸šæ¸¸æˆå¼•æ“é›†æˆéªŒè¯å®ç”¨æ€§çš„åšæ³•ï¼Œä¸ºæŠ€æœ¯å‘äº§ä¸šåº”ç”¨è½¬åŒ–æä¾›äº†å¯å‚è€ƒçš„è½åœ°è·¯å¾„ç¤ºèŒƒã€‚ 

## explaining-similarity-in-vision-language-encoders-with-weighted-banzhaf-interactions
### Abstract
Language-image pre-training (LIP) enables the development of vision-language
models capable of zero-shot classification, localization, multimodal retrieval,
and semantic understanding. Various explanation methods have been proposed to
visualize the importance of input image-text pairs on the model's similarity
outputs. However, popular saliency maps are limited by capturing only
first-order attributions, overlooking the complex cross-modal interactions
intrinsic to such encoders. We introduce faithful interaction explanations of
LIP models (FIxLIP) as a unified approach to decomposing the similarity in
vision-language encoders. FIxLIP is rooted in game theory, where we analyze how
using the weighted Banzhaf interaction index offers greater flexibility and
improves computational efficiency over the Shapley interaction quantification
framework. From a practical perspective, we propose how to naturally extend
explanation evaluation metrics, like the pointing game and area between the
insertion/deletion curves, to second-order interaction explanations.
Experiments on MS COCO and ImageNet-1k benchmarks validate that second-order
methods like FIxLIP outperform first-order attribution methods. Beyond
delivering high-quality explanations, we demonstrate the utility of FIxLIP in
comparing different models like CLIP vs. SigLIP-2 and ViT-B/32 vs. ViT-L/16.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨åŠ æƒBanzhafäº¤äº’è§£é‡Šè§†è§‰-è¯­è¨€ç¼–ç å™¨ä¸­çš„ç›¸ä¼¼æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆLIPï¼‰æ¨åŠ¨äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œè¿™ç±»æ¨¡å‹èƒ½å®Œæˆé›¶æ ·æœ¬åˆ†ç±»ã€å®šä½ã€å¤šæ¨¡æ€æ£€ç´¢å’Œè¯­ä¹‰ç†è§£ç­‰ä»»åŠ¡ã€‚ç°æœ‰è§£é‡Šæ–¹æ³•ï¼ˆå¦‚æ˜¾è‘—å›¾ï¼‰ä»…èƒ½æ•æ‰ä¸€é˜¶å½’å› ï¼Œå¿½ç•¥äº†è§†è§‰-è¯­è¨€ç¼–ç å™¨ä¸­å¤æ‚çš„è·¨æ¨¡æ€äº¤äº’ã€‚åŒæ—¶ï¼Œå½“ä½¿ç”¨ç§»é™¤å¼å¯è§£é‡Šæ€§æ–¹æ³•æ—¶ï¼Œç¨€ç–è¾“å…¥æ˜“å‡ºç°åˆ†å¸ƒå¤–é—®é¢˜ï¼ˆå¦‚å›¾åƒéš¾ä»¥è¯†åˆ«ã€æ–‡æœ¬æ¨¡ç³Šï¼‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œéœ€è¦ä¸€ç§èƒ½è€ƒè™‘è·¨æ¨¡æ€äº¤äº’ä¸”é«˜æ•ˆçš„è§£é‡Šæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºåšå¼ˆè®ºçš„è§†è§‰-è¯­è¨€ç¼–ç å™¨è§£é‡Šæ–¹æ³•  
æå‡ºå¿ å®äº¤äº’è§£é‡ŠLIPæ¨¡å‹ï¼ˆFIxLIPï¼‰ï¼Œå°†è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„è¾“å…¥tokenè§†ä¸ºåˆä½œåšå¼ˆä¸­çš„ç©å®¶è”ç›Ÿï¼ŒåŸºäºåŠ æƒBanzhafäº¤äº’æŒ‡æ•°åˆ†è§£ç›¸ä¼¼æ€§é¢„æµ‹ã€‚ç›¸æ¯”Shapleyäº¤äº’é‡åŒ–æ¡†æ¶ï¼ŒåŠ æƒBanzhafäº¤äº’æŒ‡æ•°æ›´çµæ´»ä¸”è®¡ç®—é«˜æ•ˆï¼Œè¿˜èƒ½è§£å†³åˆ†å¸ƒå¤–é—®é¢˜ï¼Œä¸ºæ¨¡å‹è§£é‡Šæä¾›äº†ç‹¬ç‰¹è§†è§’ï¼Œé¦–æ¬¡è®ºè¯åœ¨è¯¥é¢†åŸŸä½¿ç”¨åŠ æƒBanzhafäº¤äº’å…‹æœç§»é™¤å¼å¯è§£é‡Šæ€§åˆ†å¸ƒå¤–é—®é¢˜çš„å¿…è¦æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆè®¡ç®—ä¸æ‰©å±•ç­–ç•¥  
è®¾è®¡è·¨æ¨¡æ€é‡‡æ ·ç­–ç•¥è¿‘ä¼¼FIxLIPï¼Œä½¿è®¡ç®—æ•ˆç‡æ¯”ä¼ ç»ŸShapleyäº¤äº’é‡åŒ–æå‡5 - 20å€ã€‚é€šè¿‡ä¼˜å…ˆé€‰æ‹©tokenå­é›†ï¼Œå°†åŸºäºå›å½’çš„äº¤äº’è¿‘ä¼¼æ‰©å±•åˆ°æ•°ç™¾ä¸ªtokenï¼Œå®ç°å¯¹æ›´å¤§æ¨¡å‹çš„è§£é‡Šã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šäºŒé˜¶äº¤äº’çš„è¯„ä¼°æŒ‡æ ‡æ‰©å±•  
å°†æŒ‡å‘æ¸¸æˆï¼ˆpointing gameï¼‰å’Œæ’å…¥/åˆ é™¤æ›²çº¿ä¸‹é¢ç§¯ç­‰è§£é‡Šè¯„ä¼°æŒ‡æ ‡ï¼Œè‡ªç„¶æ‰©å±•åˆ°äºŒé˜¶äº¤äº’è§£é‡Šï¼Œä¸ºåŸºäºäº¤äº’çš„è§£é‡Šæ–¹æ³•æä¾›äº†å®è¯å¯¹æ¯”ä¾æ®ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MS COCOå’ŒImageNet - 1kåŸºå‡†æµ‹è¯•ä¸­ï¼ŒéªŒè¯äº†åƒFIxLIPè¿™æ ·çš„äºŒé˜¶æ–¹æ³•ä¼˜äºä¸€é˜¶å½’å› æ–¹æ³•ã€‚åŒæ—¶ï¼Œå±•ç¤ºäº†FIxLIPåœ¨æ¨¡å‹å¯¹æ¯”ï¼ˆå¦‚CLIP vs. SigLIP - 2ã€ViT - B/32 vs. ViT - L/16ï¼‰ä¸­çš„å®ç”¨æ€§ï¼Œèƒ½åŠ©åŠ›ç†è§£ä¸åŒè§†è§‰-è¯­è¨€ç¼–ç å™¨ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢ï¼Œåšå¼ˆè®ºè§†è§’ä¸ºå¤šæ¨¡æ€æ¨¡å‹è§£é‡Šæä¾›äº†æ–°èŒƒå¼ï¼ŒåŠ æƒBanzhafäº¤äº’æŒ‡æ•°åœ¨å¤„ç†åˆ†å¸ƒå¤–é—®é¢˜å’Œè®¡ç®—æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿å€¼å¾—å€Ÿé‰´ï¼›ä»è¯„ä¼°å±‚é¢ï¼ŒäºŒé˜¶äº¤äº’è¯„ä¼°æŒ‡æ ‡çš„æ‰©å±•æ€è·¯ï¼Œä¸ºå¤šæ¨¡æ€å¯è§£é‡Šæ€§è¯„ä¼°æä¾›äº†æ–°æ–¹å‘ï¼›ä»åº”ç”¨å±‚é¢ï¼ŒFIxLIPç”¨äºæ¨¡å‹å¯¹æ¯”çš„å®è·µï¼Œä¸ºç†è§£ä¸åŒè§†è§‰-è¯­è¨€ç¼–ç å™¨æä¾›äº†æœ‰æ•ˆå·¥å…·ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–å¤šæ¨¡æ€æ¨¡å‹åˆ†æåœºæ™¯ã€‚

## automated-bug-frame-retrieval-from-gameplay-videos-using-vision-language-models
### Abstract
Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä»æ¸¸æˆè§†é¢‘ä¸­è‡ªåŠ¨æå–Bugå…³é”®å¸§ï¼Œè§£æ”¾æ¸¸æˆQAäººåŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°ä»£æ¸¸æˆå·¥ä½œå®¤é¢‘ç¹å‘å¸ƒæ–°ç‰ˆæœ¬ä¸è¡¥ä¸ï¼Œäº§ç”Ÿå¤§é‡å«æ¸¸æˆè§†é¢‘çš„BugæŠ¥å‘Šã€‚å¼€å‘è€…éœ€æ‰‹åŠ¨è§‚çœ‹è§†é¢‘éªŒè¯å’Œåˆ†ç±»Bugï¼Œè¿‡ç¨‹è€—æ—¶è´¹åŠ›ä¸”éš¾è§„æ¨¡åŒ–ã€‚ä¼ ç»Ÿæ¸¸æˆQAä¾èµ–äººå·¥ï¼Œé¢å¯¹å«è§†é¢‘çš„BugæŠ¥å‘Šæ—¶ï¼Œäººå·¥å®¡é˜…æ•ˆç‡æä½ï¼›è€Œæ¸¸æˆBugçš„è§†è§‰ç‰¹æ€§ï¼ˆå¦‚çº¹ç†ç¼ºå¤±ã€å…‰ç…§é”™è¯¯ç­‰ï¼‰åˆè®©çº¯æ–‡å­—æè¿°ä¸å¤Ÿç›´è§‚ï¼Œå¿…é¡»ç»“åˆè§†é¢‘ï¼Œä½†çœ‹è§†é¢‘æˆæœ¬å¤ªé«˜ã€‚å› æ­¤ï¼Œè®ºæ–‡å¸Œæœ›æ‰“é€ è‡ªåŠ¨åŒ–æµæ°´çº¿ï¼ŒæŠŠBugè§†é¢‘æµ“ç¼©æˆæœ€èƒ½åŒ¹é…æè¿°çš„å•å¸§å›¾åƒï¼Œå¸®å¼€å‘è€…å¿«é€Ÿå®šä½Bugã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…³é”®å¸§æå–é™æœ¬å¢æ•ˆ  
ç”¨FFmpegæå–å…³é”®å¸§ï¼ŒæŠŠè§†é¢‘å‹ç¼©åˆ°ä»…åŸå¸§æ•°çš„1.90%ï¼ˆä¸­ä½æ•°ï¼‰ï¼Œå´èƒ½åœ¨98.79%çš„æ¡ˆä¾‹ä¸­æ•æ‰åˆ°Bugæ—¶åˆ»ã€‚å…³é”®å¸§å¤©ç„¶å¥‘åˆæ¸¸æˆåˆ†æåœºæ™¯â€”â€”å®ƒå¸¸å¯¹åº”åœºæ™¯çªå˜ï¼ˆå¦‚æ¸²æŸ“æ•…éšœã€é•œå¤´åˆ‡æ¢ï¼‰æˆ–å›ºå®šé—´éš”ï¼Œæ—¢èƒ½å¤§å¹…å‡å°‘éœ€å¤„ç†çš„å¸§æ•°ï¼Œåˆä¿ç•™è§†è§‰å…³é”®ä¿¡æ¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç²¾å‡†åŒ¹é…  
å¼•å…¥GPT - 4oè¿™ç±»è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¯¹æå–çš„å…³é”®å¸§å’Œæ–‡å­—Bugæè¿°åšåŒ¹é…æ’åºï¼Œé€‰å‡ºæœ€å…·ä»£è¡¨æ€§çš„å¸§ã€‚VLMèƒ½åŒæ—¶ç†è§£è§†è§‰è¾“å…¥ä¸æ–‡æœ¬æç¤ºï¼Œè®©â€œä»ä¸€å †å…³é”®å¸§é‡ŒæŒ‘å‡ºæœ€èƒ½ä½“ç°Bugçš„é‚£å¸§â€è¿™ä¸ªä»»åŠ¡è‡ªåŠ¨åŒ–ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æŸçƒ­é—¨FPSæ¸¸æˆçš„çœŸå®å¼€å‘è€…æäº¤è§†é¢‘ä¸JIRA BugæŠ¥å‘Šæ•°æ®é›†ä¸Šæµ‹è¯•ï¼š  
- æ•´ä½“Top - 1æ£€ç´¢å¸§çš„F1åˆ†æ•°è¾¾0.79ï¼Œå‡†ç¡®ç‡0.89ï¼›  
- ä¸åŒBugç±»åˆ«è¡¨ç°æœ‰å·®å¼‚ï¼šå…‰ç…§ä¸é˜´å½±ï¼ˆF1 = 0.94ï¼‰ã€ç‰©ç†ä¸ç¢°æ’ï¼ˆ0.86ï¼‰ã€UIä¸HUDï¼ˆ0.83ï¼‰è¡¨ç°æœ€å¥½ï¼ŒåŠ¨ç”»ä¸è§†è§‰ç‰¹æ•ˆï¼ˆ0.51ï¼‰æœ€ä½ï¼›  
- é‡å¤è¿è¡Œæ—¶æ•´ä½“F1ä¸­ä½æ•°ç¨³å®šåœ¨0.79ï¼Œä½†è¾“å‡ºå­˜åœ¨éç¡®å®šæ€§ï¼›350ä¸ªBugè§†é¢‘é‡Œï¼Œ183ä¸ªåœ¨å¤šæ¬¡è¿è¡Œä¸­ç»“æœå§‹ç»ˆæ­£ç¡®ï¼Œç®€å•å¤šæ•°æŠ•ç¥¨åæ­£ç¡®æ•°æå‡åˆ°215ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æŠ€æœ¯èåˆæ€è·¯ï¼šæŠŠè§†é¢‘å¤„ç†ï¼ˆå…³é”®å¸§æå–ï¼‰å’Œå‰æ²¿VLMèƒ½åŠ›ç»“åˆï¼Œè§£å†³ç‰¹å®šé¢†åŸŸï¼ˆæ¸¸æˆQAï¼‰çš„æ•ˆç‡ç—›ç‚¹ï¼Œä¸ºå…¶ä»–éœ€å¤„ç†â€œè§†é¢‘ + æ–‡æœ¬â€åœºæ™¯çš„é¢†åŸŸï¼ˆå¦‚å½±è§†å®¡æ ¸ã€æ•™è‚²å†…å®¹åˆ†æï¼‰æä¾›è·¨æŠ€æœ¯æ ˆæ•´åˆçš„å‚è€ƒï¼›  
2. æ•°æ®é©±åŠ¨éªŒè¯ï¼šåŸºäºçœŸå®æ¸¸æˆé¡¹ç›®çš„BugæŠ¥å‘Šä¸è§†é¢‘åšå®éªŒï¼Œé‡åŒ–ä¸åŒç»´åº¦è¡¨ç°ï¼ˆæ•´ä½“ç²¾åº¦ã€ç±»åˆ«å·®å¼‚ã€é‡å¤ä¸€è‡´æ€§ï¼‰ï¼Œè¿™ç§è´´è¿‘äº§ä¸šå®é™…çš„è¯„ä¼°æ–¹å¼ï¼Œèƒ½è®©æŠ€æœ¯ä»·å€¼æ›´æ˜“è¢«å·¥ä¸šç•Œç†è§£ï¼›  
3. é™æœ¬ææ•ˆèŒƒå¼ï¼šç”¨â€œå•å¼ ä¿¡æ¯å¸§æ›¿ä»£å®Œæ•´è§†é¢‘å®¡é˜…â€çš„æ€è·¯ï¼Œç›´å‡»äººå·¥æµç¨‹çš„è€—æ—¶ç‚¹ï¼Œè¯æ˜äº†AIè¾…åŠ©èƒ½åœ¨è½¯ä»¶ç»´æŠ¤ï¼ˆå°¤å…¶æ˜¯è§†è§‰å¯¼å‘çš„æ¸¸æˆé¢†åŸŸï¼‰ä¸­å¤§å¹…è§£æ”¾äººåŠ›ï¼Œä¸ºQAå›¢é˜Ÿå’Œå¼€å‘è€…ææ•ˆæä¾›æ–°è·¯å¾„ã€‚

## enhancing-vision-language-model-training-with-reinforcement-learning-in-synthetic-worlds-for-real-world-success
### Abstract
Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä½æˆæœ¬åˆæˆç¯å¢ƒ+ç®€æ´RLç®—æ³•ï¼Œè®©è§†è§‰è¯­è¨€æ¨¡å‹æ‹¥æœ‰çœŸå®ä¸–ç•Œèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šæ¨¡æ€äº¤äº’é¢†åŸŸï¼Œå½“å‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½èƒ½å‡ºè‰²æè¿°é™æ€å›¾åƒå’Œè§†é¢‘ï¼Œä½†åœ¨äº¤äº’å¼åœºæ™¯ä¸­å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨æ—¶ä»å­˜åœ¨ä¸è¶³ã€‚æ”¶é›†çœŸå®çš„è§†è§‰-è¯­è¨€äº¤äº’æ•°æ®æˆæœ¬é«˜ä¸”é€Ÿåº¦æ…¢ï¼Œå¤šæ•°è®­ç»ƒè¯­æ–™åªæœ‰é™æ€å›¾æ–‡å¯¹ï¼Œå¯¼è‡´VLMsâ€œæè¿°å¼ºã€è¡ŒåŠ¨å¼±â€ã€‚æ­¤å‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•æƒ³èµ‹äºˆVLMsè¡ŒåŠ¨èƒ½åŠ›ï¼Œå´å­˜åœ¨æ³›åŒ–æ€§å·®ã€ä¾èµ–è„†å¼±è¶…å‚æ•°è°ƒä¼˜æˆ–ä»…é€‚ç”¨äºçŠ¶æ€å˜å¼‚æ€§ä½çš„å¯†é›†å¥–åŠ±ç¯å¢ƒç­‰é—®é¢˜ï¼Œæ¯”å¦‚RL4VLMä¾èµ–éš¾è°ƒçš„æ··åˆç³»æ•°ã€LOOPåœ¨çŠ¶æ€å˜åŒ–å¤§æ—¶ä¿¡ç”¨åˆ†é…å¤±æ•ˆã€ArCHeréœ€è¦å¤§å›æ”¾ç¼“å†²åŒºå’Œå¯†é›†å¥–åŠ±ç­‰ï¼Œéš¾ä»¥åœ¨é•¿ä¸”ç¨€ç–çš„è§†è§‰æƒ…èŠ‚ä¸­ç¨³å®šè®­ç»ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºVL - DACç®—æ³•
Vision - Language Decoupled Actor - Criticï¼ˆVL - DACï¼‰æ˜¯ä¸€ç§è½»é‡ä¸”æ— è¶…å‚æ•°çš„RLç®—æ³•ã€‚å®ƒå¯¹åŠ¨ä½œtokenåº”ç”¨PPOæ›´æ–°ï¼Œè€Œä»…åœ¨ç¯å¢ƒæ­¥éª¤çº§åˆ«å­¦ä¹ ä»·å€¼ï¼šè¿™ç§tokenå’Œæ­¥éª¤çš„è§£è€¦æ–¹å¼åœ¨å¤§VLMsæˆ–LLMsä¸­æ­¤å‰æœªè¢«æ¢ç´¢è¿‡ã€‚è¯¥è§£è€¦å»é™¤äº†ä¸ç¨³å®šçš„æƒé‡é¡¹ï¼Œèƒ½å®ç°æ›´å¿«ã€æ›´å¯é çš„æ”¶æ•›ï¼Œè¿˜æ‘†è„±äº†æ—©æœŸæ–¹æ³•æ‰€éœ€çš„è„†å¼±æƒé‡é¡¹å’Œå›æ”¾ç¼“å†²åŒºï¼Œä»…ç”¨KLæ­£åˆ™åŒ–ã€ä»·å€¼é¢„çƒ­å’Œæ¢¯åº¦åœæ­¢ç­‰ minimal stabilization kit å°±èƒ½è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéªŒè¯ä½æˆæœ¬æ¨¡æ‹Ÿå™¨åˆ°çœŸå®ä»»åŠ¡çš„è¿ç§»
åœ¨MiniWorldã€Gym - Cardsã€ALFWorldæˆ–WebShopç­‰å»‰ä»·æ¨¡æ‹Ÿå™¨ä¸­ï¼Œæ¯æ¬¡ç”¨VL - DACè®­ç»ƒå•ä¸ªVLMï¼Œå°±èƒ½è®©æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒçš„æ™ºèƒ½ä½“æ§åˆ¶ã€ç©ºé—´æ¨ç†å’Œç½‘é¡µå¯¼èˆªç­‰çœŸå®ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­å–å¾—å¯è§‚æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¨¡æ‹Ÿå™¨çš„ç»æµæ€§å’Œç®—æ³•ç®€æ´æ€§æ˜¯å®ç°è¿ç§»çš„å…³é”®è¦ç´ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæŠ€èƒ½è¿ç§»ç ”ç©¶
é¦–æ¬¡ç³»ç»Ÿåˆ†æäº†æ¨¡æ‹Ÿå™¨ä¹ å¾—æŠ€èƒ½å¦‚ä½•æ˜ å°„åˆ°æ™ºèƒ½ä½“æ§åˆ¶ã€ç©ºé—´å’Œç½‘é¡µäº¤äº’ç­‰åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¿˜é€šè¿‡æ¶ˆèå®éªŒç¡®å®šäº†é©±åŠ¨VL - DACç¨³å®šæ€§å’Œæ³›åŒ–æ€§çš„è¦ç´ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªè½»é‡æ¨¡æ‹Ÿå™¨è®­ç»ƒåï¼Œæ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼šåœ¨ä»¥æ¸¸æˆä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“æ§åˆ¶åŸºå‡†BALROGä¸Šç›¸å¯¹æå‡50%ï¼›åœ¨VSI - Benchæœ€éš¾çš„ç©ºé—´è§„åˆ’éƒ¨åˆ†ç›¸å¯¹æå‡5%ï¼›åœ¨ç½‘é¡µå¯¼èˆªåŸºå‡†VisualWebBenchä¸Šæå‡2%ï¼›ä¸”è¿™äº›æå‡éƒ½æ²¡æœ‰é™ä½æ¨¡å‹å¯¹å›¾åƒçš„ä¸€èˆ¬ç†è§£ç²¾åº¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç®—æ³•è§’åº¦ï¼ŒVL - DACçš„è§£è€¦æ€è·¯ä¸ºå¤„ç†å¤§æ¨¡å‹RLè®­ç»ƒä¸­è¡ŒåŠ¨ä¸ä»·å€¼å­¦ä¹ çš„å¹²æ‰°æä¾›äº†æ–°æ–¹å‘ï¼Œå…¶è½»é‡æ— è¶…å‚æ•°çš„ç‰¹æ€§é€‚åˆæ¨å¹¿åˆ°ä¸åŒåœºæ™¯ï¼›ä»æ•°æ®å’Œè®­ç»ƒç¯å¢ƒè§’åº¦ï¼Œè¯æ˜äº†æ— éœ€è¿½æ±‚é«˜æˆæœ¬é«˜çœŸå®åº¦çš„è®­ç»ƒæ•°æ®/ç¯å¢ƒï¼Œåˆ©ç”¨å»‰ä»·æ¨¡æ‹Ÿå™¨ç»“åˆåˆé€‚ç®—æ³•å°±èƒ½è®©æ¨¡å‹ä¹ å¾—è¿ç§»æŠ€èƒ½ï¼Œä¸ºåç»­VLMsåœ¨å¤šæ­¥éª¤ã€äº¤äº’å¼ä»»åŠ¡ä¸­çš„è®­ç»ƒå¼€è¾Ÿäº†â€œä½æˆæœ¬åˆæˆç¯å¢ƒè®­ç»ƒ - çœŸå®ä¸–ç•Œèƒ½åŠ›è¿ç§»â€çš„å¯è¡Œè·¯å¾„ï¼Œä¹Ÿå¯å‘ç ”ç©¶è€…å…³æ³¨ç®—æ³•ç®€æ´æ€§å’Œç¯å¢ƒå®ç”¨æ€§åœ¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›åŸ¹å…»ä¸­çš„é‡è¦æ€§ã€‚

