# Paper List of Terms(Reward Function+RL)
- [25/06] **Scaffolding Dexterous Manipulation with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2506.19212v1)] [[Code/Page]()] [[TLDR/Notes](#scaffolding-dexterous-manipulation-with-vision-language-models)]

- [25/06] **AdapThink: Adaptive Thinking Preferences for Reasoning Language Model**  
[[Paper](http://arxiv.org/pdf/2506.18237v1)] [[Code/Page]()] [[TLDR/Notes](#adapthink--adaptive-thinking-preferences-for-reasoning-language-model)]

- [25/06] **Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach**  
[[Paper](http://arxiv.org/pdf/2506.17828v1)] [[Code/Page]()] [[TLDR/Notes](#aligning-frozen-llms-by-reinforcement-learning--an-iterative-reweight-then-optimize-approach)]

- [25/06] **Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators**  
[[Paper](http://arxiv.org/pdf/2506.17007v1)] [[Code/Page](https://github.com/marcojira/tgm.)] [[TLDR/Notes](#robust-reinforcement-learning-for-discrete-compositional-generation-via-general-soft-operators)]

- [25/06] **Learning Dexterous Object Handover**  
[[Paper](http://arxiv.org/pdf/2506.16822v1)] [[Code/Page]()] [[TLDR/Notes](#learning-dexterous-object-handover)]

- [25/06] **GoalLadder: Incremental Goal Discovery with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2506.16396v1)] [[Code/Page]()] [[TLDR/Notes](#goalladder--incremental-goal-discovery-with-vision-language-models)]

- [25/06] **Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations**  
[[Paper](http://arxiv.org/pdf/2506.16016v1)] [[Code/Page]()] [[TLDR/Notes](#dual-objective-reinforcement-learning-with-novel-hamilton-jacobi-bellman-formulations)]

- [25/06] **Design of an all-facet illuminator for high NA EUV lithography exposure tool based on deep reinforcement learning**  
[[Paper](http://arxiv.org/pdf/2506.15558v1)] [[Code/Page]()] [[TLDR/Notes](#design-of-an-all-facet-illuminator-for-high-na-euv-lithography-exposure-tool-based-on-deep-reinforcement-learning)]

- [25/06] **Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion**  
[[Paper](http://arxiv.org/pdf/2506.15132v1)] [[Code/Page](https://github.com/BoosterRobotics/booster_gym.)] [[TLDR/Notes](#booster-gym--an-end-to-end-reinforcement-learning-framework-for-humanoid-robot-locomotion)]

- [25/06] **IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards**  
[[Paper](http://arxiv.org/pdf/2506.14375v1)] [[Code/Page]()] [[TLDR/Notes](#intellilung--advancing-safe-mechanical-ventilation-using-offline-rl-with-hybrid-actions-and-clinically-aligned-rewards)]



# TLDR/Notes
## scaffolding-dexterous-manipulation-with-vision-language-models
### Abstract
Dexterous robotic hands are essential for performing complex manipulation
tasks, yet remain difficult to train due to the challenges of demonstration
collection and high-dimensional control. While reinforcement learning (RL) can
alleviate the data bottleneck by generating experience in simulation, it
typically relies on carefully designed, task-specific reward functions, which
hinder scalability and generalization. Thus, contemporary works in dexterous
manipulation have often bootstrapped from reference trajectories. These
trajectories specify target hand poses that guide the exploration of RL
policies and object poses that enable dense, task-agnostic rewards. However,
sourcing suitable trajectories - particularly for dexterous hands - remains a
significant challenge. Yet, the precise details in explicit reference
trajectories are often unnecessary, as RL ultimately refines the motion. Our
key insight is that modern vision-language models (VLMs) already encode the
commonsense spatial and semantic knowledge needed to specify tasks and guide
exploration effectively. Given a task description (e.g., "open the cabinet")
and a visual scene, our method uses an off-the-shelf VLM to first identify
task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D
trajectories for hand motion and object motion. Subsequently, we train a
low-level residual RL policy in simulation to track these coarse trajectories
or "scaffolds" with high fidelity. Across a number of simulated tasks involving
articulated objects and semantic understanding, we demonstrate that our method
is able to learn robust dexterous manipulation policies. Moreover, we showcase
that our method transfers to real-world robotic hands without any human
demonstrations or handcrafted rewards.
### 🌟 论文解读 | 用视觉语言模型为灵巧操作“搭脚手架”

### 📌 背景痛点/本文动机
灵巧机械手对执行复杂操作任务至关重要，但由于示范数据收集难、高维控制复杂，训练起来颇具挑战。强化学习（RL）虽能通过仿真生成经验缓解数据瓶颈，却依赖精心设计的特定任务奖励函数，限制了扩展性与泛化性。当下灵巧操作研究常借助参考轨迹引导RL策略探索，可获取合适轨迹（尤其针对灵巧手）仍是难题，且参考轨迹的精确细节对最终经RL优化的运动而言并非必需。于是，论文提出利用视觉语言模型（VLMs）编码的常识性空间与语义知识，来生成引导RL的“脚手架”轨迹。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：借助视觉语言模型生成“脚手架”轨迹  
给定任务描述（如“打开橱柜”）和视觉场景，利用现成的VLMs先识别任务相关关键点（如把手、按钮），再合成手和物体运动的3D轨迹。这些轨迹作为“粗粒度”引导，无需精确到人类示范级细节，却能为RL提供探索方向与奖励依据。  

💡 创新点2：残差强化学习策略跟踪轨迹  
在仿真环境中训练低层次残差RL策略，让其高精度跟踪VLMs生成的“脚手架”轨迹。RL通过优化每一步的偏移和手指动作来最大化跟踪奖励，无需人工设计复杂奖励函数，还能在过程中超越人类遥操作的性能与精度。  

💡 创新点3：利用VLMs特性提升泛化与鲁棒性  
通过重复查询VLMs，随机化初始关键点和高层轨迹，让策略在测试时能泛化到未见过的初始条件与新轨迹；当VLMs高层规划有误差时，提供上下文示例可大幅提升性能。  

### 📈 实验结果
在涉及铰接物体和语义理解的多个仿真任务中，论文方法在8个任务上，无需手动奖励设计就能达到接近“ oracle 级”手工轨迹的成功率与泛化能力。此外，还成功实现了向真实世界灵巧机械手的跨域迁移，在无人类示范和手工奖励情况下仍能达成鲁棒操作性能。  

### 💬 可借鉴之处
1. 跨模态协作思路：将VLMs的语义空间推理能力与RL的控制优化能力结合，为解决高维复杂控制任务提供了“高层语义引导 + 低层精确优化”的新范式，可推广到需语义理解与精细操作结合的机器人任务。  
2. 弱化对精确示范的依赖：证明粗粒度轨迹足以支撑RL训练，为减少机器人学习对大规模高质量示范数据集的依赖提供了可行路径，降低任务拓展时的数据采集成本。  
3. 泛化与迁移技巧：利用VLMs生成多样化轨迹来增强泛化、通过上下文示例提升鲁棒性等手段，为基于大模型的机器人学习在实际部署（如环境变化、模型误差场景）提供了实用方法论。

## adapthink--adaptive-thinking-preferences-for-reasoning-language-model
### Abstract
Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.
### 🌟 论文解读 | AdapThink：让推理语言模型拥有自适应“思考偏好”

### 📌 背景痛点/本文动机
基于强化学习（RL）的后训练极大提升了语言模型的复杂推理能力，催生了精细的自我反思过程（即“慢思考”范式）。但这一范式存在推理效率难题：模型面对简单问题时可能过度计算，面对复杂问题时又可能过早切换推理思路。以往机制依赖静态长度预算或预定义规则，缺乏对问题复杂度变化和模型能力演进的适应性。因此，需要一种自适应后训练框架，在维持推理性能的同时提升效率，AdapThink 应运而生。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：组相对奖励函数  
设计了一种新颖的组相对奖励函数来调整模型当前推理偏好。模型可基于生成响应的组内准确率，确定合适的反思偏好；同时通过统计训练样本组中关键过渡词数量，定量衡量推理效率，无需依赖固定长度偏好，而是利用模型置信度与响应特征动态调整反思类过渡词的偏好。  

💡 创新点2：多样性感知采样机制  
提出多样性感知采样机制平衡训练样本组的解题准确率与推理多样性。先对推理实例过采样，再用精心定义的多样性指标评估实例的最终答案与中间步骤，最后通过多样性感知下采样来筛选和提升用于 RL 后训练的实例整体质量。  


### 📈 实验结果
在多个数学推理数据集上，使用 DeepSeek 蒸馏模型进行实验。结果表明，AdapThink 在使模型具备自适应推理模式、缓解推理低效问题上展现优势。如对上下文长度限制仅 2K token 的 DeepSeek - 蒸馏 Qwen 模型进行后训练后，在 8K - token 限制下测试，相比多个长度控制基线方法表现更优，证明了其在多个推理基准上打造强大且高效的思维链（CoT）模型的有效性。

### 💬 可借鉴之处
1. 打破静态长度限制思维，通过分析样本组推理模式分布来调整长度偏好，为处理模型推理效率与复杂度适配问题提供了新视角，不再局限于固定规则约束。  
2. 组相对奖励函数与多样性感知采样机制的设计思路，可迁移到其他需要平衡性能与效率、兼顾多样性与准确性的模型训练任务中，尤其是涉及反思、推理类的语言模型优化场景。  
3. 对推理过程中关键过渡词（如“Pause - Validation”“Branch - Extension”类词汇）的分析与利用方式，为后续研究如何从语言表达特征层面引导模型推理行为提供了参考范式。

## aligning-frozen-llms-by-reinforcement-learning--an-iterative-reweight-then-optimize-approach
### Abstract
Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.
### 🌟 论文解读 | 无需修改模型权重，用强化学习对齐冻结大模型：迭代重加权优化法

### 📌 背景痛点/本文动机
大语言模型（LLMs）对齐人类偏好通常依赖RLHF、DPO等微调方法，这类方法需直接优化模型参数，在测试阶段无法提升性能，且模型权重不可访问时也无法使用。测试时对齐方法虽绕开权重更新，但推理成本高，且基于不完善奖励或价值函数的一次性指导易产生次优输出。在此背景下，本文聚焦于：给定冻结LLM和结果奖励模型（ORM），如何在测试时高效改进或定制模型，同时最小化推理成本。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出迭代重加权 - 优化（IRO）框架  
IRO是受强化学习启发的框架，无需修改冻结基础模型参数就能实现类RL的对齐。训练时通过三步迭代：从基础模型采样候选、用当前价值函数重采样、训练新轻量级价值函数指导下一次解码；测试时利用学习到的价值函数序列，通过基于搜索的优化过程引导基础模型生成。  

💡 创新点2：类策略迭代与高效推理  
从理论上证明，在温和条件下IRO属于一种策略迭代，测试时能以指数级更少的tokens达到Best - of - N（BoN）搜索的性能。且用户可类似OpenAI的强化微调（RFT），用IRO在自有数据集上以RL风格微调模型，还无需访问模型权重。  

### 📈 实验结果
在AlpacaEval 2.0等具有挑战性的指令跟随基准测试中，IRO显著提高了长度控制胜率。如Llama - 3 - 8B - Instruct从30.71%提升到43.80%，Meta - Llama - 3 - 70B - Instruct从43.11%提升到49.77%（与GPT - 4响应对比）。此外，即使使用小尺寸（1B或7B）价值函数引导大基础模型（6.9B或70B），IRO也持续优于BoN、weak - to - strong search等现有测试时对齐基线。  

### 💬 可借鉴之处
对于无法获取模型权重但需在测试阶段优化模型对齐人类偏好的场景，IRO提供了新范式，其迭代训练轻量价值函数 + 测试时搜索优化的思路，为低推理成本下的模型性能提升和领域适配开辟路径；理论上对策略迭代的证明，也为后续强化学习在冻结模型对齐方向的研究提供了理论支撑；同时在工业界，类似OpenAI RFT但无需权重访问的特性，让企业在自有数据定制模型时更具灵活性。

## robust-reinforcement-learning-for-discrete-compositional-generation-via-general-soft-operators
### Abstract
A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.
### 🌟 论文解读 | 离散组合生成任务中基于广义软算子的鲁棒强化学习

### 📌 背景痛点/本文动机
在科学发现领域，从蛋白质、分子等大规模组合对象集合中筛选出少量有前景的候选对象是关键瓶颈。传统方法依赖专家知识，而近期方法借助强化学习（RL）结合代理奖励函数与正则化来优化筛选，但代理奖励函数存在固有不确定性，现有按奖励比例采样的方法在大搜索空间下表现不佳，无法得到最优候选对象。因此，本文旨在通过鲁棒强化学习方法解决代理奖励函数不确定性带来的挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建组合式奖励不确定性模型  
提出离散组合生成（DCG）任务中奖励不确定性的组合模型，明确正则化RL与奖励鲁棒RL在价值函数上的联系，能分析不同软算子对应的奖励不确定性集合，为后续方法设计奠定理论基础。  

💡 创新点2：提出广义柔和最大化（GM）算子与TGM算法  
受RL中香农熵正则化的鲁棒性解释及DCG场景下其存在的问题启发，引入统一的广义柔和最大化（GM）算子，它能在生成流网络（GFN）算子和其他软RL算子间平滑插值，追求更尖锐的采样分布以应对奖励不确定性。基于此算子，进一步提出实用算法轨迹GM（TGM），用于在实际任务中生成候选对象。  

### 📈 实验结果
在合成任务与真实世界的序列设计任务（如蛋白质序列生成等）中，TGM在生成多样且高奖励的对象方面，表现优于GFNs和其他软RL基线方法，验证了其在应对奖励不确定性、生成优质候选对象上的有效性。  

### 💬 可借鉴之处
1. 理论层面：建立了正则化RL与鲁棒RL在DCG场景下的联系，为分析奖励不确定性提供了新视角，后续研究可借鉴这种跨领域联系的分析思路拓展方法边界。  
2. 方法层面：提出的GM算子和TGM算法为离散组合生成任务提供了更鲁棒灵活的解决方案，在药物分子设计、蛋白质工程等依赖从大规模组合中筛选的领域，可尝试引入该方法优化候选对象生成流程。  
3. 实验层面：在合成与真实任务中验证方法有效性的思路，为相关领域算法验证提供了参考范式，有助于后续新方法在实际场景下的性能评估。

## learning-dexterous-object-handover
### Abstract
Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.
### 🌟 论文解读 | 用强化学习实现灵巧的物体交接：双四元数奖励函数与鲁棒性验证

### 📌 背景痛点/本文动机
在人机协作、服务机器人等场景中，物体交接是一项关键技能。传统控制理论算法在应对此类复杂任务时存在局限（如传感器融合易累积误差、需为每个子任务设计策略）；而人类虽擅长这类需灵巧性、感知与协作的任务，但让机器人掌握物体交接仍面临挑战。此前工作要么用简单二指夹爪简化问题，要么依赖遥操作教学，且在旋转表示与策略鲁棒性上有提升空间。因此，本文聚焦于**双多指机器人手之间的灵巧物体交接**，探索用强化学习（RL）解决该问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：采用四指关节手，提升任务复杂度与灵巧性  
区别于过往用二指小夹爪的研究，本文使用四指铰接式机械手。这增加了自由度（DoF），让任务更接近真实复杂场景，但也提升了基于学习方法的应用难度，更具研究价值。  

💡 创新点2：基于双四元数的新型奖励函数  
为最小化旋转距离，本文提出基于双四元数的奖励函数。相比欧拉角、旋转矩阵等传统旋转表示，双四元数在处理SO(3)旋转约束时更具优势（避免欧拉角万向节锁、减少矩阵运算复杂度等）。通过双四元数的基本运算（如共轭、差、模长等，见表I）来设计奖励，让策略更高效学习手部姿态的精准控制。  

💡 创新点3：单智能体单阶段RL训练范式  
过往部分研究采用多智能体或多阶段训练，需调优大量超参数。本文设计**单智能体+单阶段**的训练流程，简化训练逻辑，降低超参数负担，同时仍能让机械臂与手学习“接近 - 抓取 - 转移”的完整交接流程。  


### 📈 实验结果
1. 对训练分布外物体的鲁棒性：在测试未参与训练的新物体时，策略表现出强鲁棒性。最优场景下，100次实验成功率达94%，证明对新物体的适应能力。  
2. 交接过程扰动的鲁棒性：当“递物”机器人在交接中移动（现实中常见扰动），最优策略性能仅下降13.8%，验证了策略在动态干扰下的稳定性。  


### 💬 可借鉴之处
- 旋转表示的创新应用：双四元数在机器人姿态控制的奖励设计中展现优势，为需精准旋转匹配的任务（如装配、操作）提供了更高效的数学工具参考。  
- 复杂机械臂+多指手的RL训练：证明在高自由度、高复杂度的机械系统上，RL能学习到有效策略，为后续更复杂的人机协作、多机器人协作任务铺路。  
- 鲁棒性验证范式：通过“训练外物体+过程扰动”的实验设计，为评估智能策略在真实场景的可靠性提供了可参考的测试框架。  

综上，本文从硬件复杂度、数学工具到训练范式都做出了创新尝试，且通过扎实实验验证了策略鲁棒性，是强化学习在机器人灵巧操作领域的一篇值得关注的实践型论文。

## goalladder--incremental-goal-discovery-with-vision-language-models
### Abstract
Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.
### 🌟 论文解读 | GoalLadder：用视觉语言模型实现增量式目标发现，革新强化学习任务指定

### 📌 背景痛点/本文动机
强化学习（RL）中，手动设计奖励函数需大量人力与领域专业知识。自然语言虽能简洁描述RL任务，但从语言指令中提取奖励函数仍具挑战，尤其在视觉环境下。现有利用大预训练语言模型的方法，或依赖非视觉环境表示、或需海量反馈、或生成噪声大且形状不佳的奖励函数。同时，视觉语言模型（VLMs）用于反馈时，需解决对噪声反馈的鲁棒性和查询效率两大关键问题，为此提出GoalLadder方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：增量式目标发现机制  
GoalLadder借助视觉语言模型，以增量方式发现能让智能体更接近自然语言指定任务完成的环境状态。通过查询VLM来识别代表任务进展提升的状态，并利用成对比较对这些状态排序，逐步明确更优状态，引导智能体向任务完成推进。  

💡 创新点2：基于ELO的评级系统抗噪声  
不同于完全信任VLM反馈的先前工作，GoalLadder采用基于ELO的评级系统对潜在目标状态排序。该方式降低了VLM噪声反馈的不利影响，在反复比较小部分视觉观测过程中，持续优化对状态效用的估计，让反馈利用更鲁棒。  

💡 创新点3：无监督视觉嵌入空间定义奖励  
训练过程中，智能体的任务是在一个基于无标签视觉数据训练的学习到的嵌入空间中，最小化与排名最高目标的距离。这一特性绕开了训练形状良好奖励函数通常所需的大量精确反馈，实现奖励对未见过状态的泛化，减少VLM查询次数。  

### 📈 实验结果
在经典控制和机器人操作环境中，GoalLadder显著超越现有相关方法。平均最终成功率约95%，而最佳竞品仅约45% 。甚至在与能获取真实奖励的“ oracle 智能体”对比时，GoalLadder在所有测试任务中几乎追平，且在一项任务上大幅超越。  

### 💬 可借鉴之处
1. 应对噪声反馈思路：采用ELO评级系统处理模型反馈噪声，为其他依赖模型反馈的任务（如多模态学习、人机交互反馈处理等）提供了抗噪声设计参考。  
2. 无监督嵌入空间用奖励：利用无标签数据训练嵌入空间来定义奖励实现泛化，启发在数据稀缺场景下，如何借助无监督或自监督学习手段优化强化学习奖励设计。  
3. 增量式目标发现：针对需逐步推进的复杂任务，这种增量发现更优状态的逻辑，可迁移到机器人长期任务规划、多阶段目标达成类任务中，提升任务分解与推进效率。

## dual-objective-reinforcement-learning-with-novel-hamilton-jacobi-bellman-formulations
### Abstract
Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.
### 🌟 论文解读 | 双目标强化学习新突破：基于哈密顿 - 雅可比 - 贝尔曼方程的创新方法

### 📌 背景痛点/本文动机
在强化学习（RL）中，硬约束无论是通过奖励函数还是模型架构施加，往往会降低策略性能。拉格朗日方法虽能融合目标与约束，但常需复杂的奖励工程和参数调优。同时，多目标RL、约束马尔可夫决策过程（CMDP）、时态逻辑RL等现有方法存在诸如需复杂奖励设计、难保证代理与原问题关系等不足。此外，在处理涉及平衡两个目标的 Reach - Always - Avoid（需达成不同奖励和惩罚阈值）和 Reach - Reach（需达成两个不同奖励阈值）问题时，现有方法缺乏有效手段。因此，本文旨在基于哈密顿 - 雅可比（HJ）方程与RL的联系，为双目标满足问题提出新价值函数，以更高效解决这类约束决策问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：定义双目标问题并分解求解
    - 明确 Reach - Always - Avoid（RAA）和 Reach - Reach（RR）问题。RAA要平衡奖励最大化和惩罚最小化，RR要平衡两个不同奖励的最大化，且目标结合方式为最坏情况确保双满足，与传统RL的“折扣奖励和”目标不同。
    - 对RAA，将其分解为避免问题和 Reach - Avoid 问题。先求解与负惩罚相关的避免问题得到最优价值函数 \( V^*_A(s) \)，再用修改后的奖励 \( r_{RAA}(s) = \min\{r(s), V^*_A(s)\} \) 求解 Reach - Avoid 问题，进而得到RAA的最优价值函数满足的贝尔曼方程 \( V^*_{RAA}(s)=\min\left\{\max\left\{\max_{a\in A}V^*_{RAA}(f(s,a)), r_{RAA}(s)\right\}, q(s)\right\} \) 。
    - 对RR，通过将系统状态增广（加入跟踪两个奖励历史最优值的辅助变量），把问题分解为多个可达性问题来求解（论文虽未完全展开，但提出了分解思路）。
💡 创新点2：状态增广应对历史依赖
    - 对于RAA和RR问题，仅依赖当前状态的策略存在缺陷（如RR中基于当前状态的确定性策略可能只能达成一个目标，RAA中当前状态最优决策依赖轨迹历史）。因此，对MDP进行状态增广，引入辅助变量跟踪轨迹历史中的关键信息（如RAA中跟踪最佳奖励和最差惩罚历史，RR中跟踪两个奖励的历史最佳值），使策略能利用轨迹历史信息，且理论证明这种增广下最优确定性策略已足够，无需额外信息或随机策略。
💡 创新点3：提出DO - HJ - PPO算法
基于对RAA和RR问题的分析，提出Proximal Policy Optimization的变体DO - HJ - PPO来解决这类双目标约束决策问题，利用新推导的价值函数和贝尔曼方程指导策略学习。

### 📈 实验结果
论文在安全到达和多目标达成等一系列任务中验证DO - HJ - PPO。结果表明，DO - HJ - PPO产生了与先前方法定性不同的行为，在各种指标上超越了多个基线方法，证明了其在解决双目标约束决策问题上的有效性。

### 💬 可借鉴之处
- 问题分解思路：将复杂的双目标约束问题分解为更易处理的子问题（如RAA分解为避免和Reach - Avoid问题），为解决多目标、带约束的强化学习问题提供了模块化的思考方式，可借鉴到其他复杂目标组合的RL任务中。
- 状态增广方法：当策略决策需轨迹历史信息时，通过合理增广状态引入辅助变量跟踪关键历史信息，为处理历史依赖型RL问题提供了有效技术手段。
- 算法创新：基于HJ方程与RL联系提出的DO - HJ - PPO算法，为解决双目标约束RL问题提供了新的算法范式，其在实验中展现的优势对后续相关算法设计有参考价值，可启发研究者针对不同约束和目标组合设计更高效的RL算法。

## design-of-an-all-facet-illuminator-for-high-na-euv-lithography-exposure-tool-based-on-deep-reinforcement-learning
### Abstract
Using the illuminator for high numerical aperture (NA) extreme ultraviolet
(EUV) exposure tool in EUV lithography can lead to support volume production of
sub-2 nm logic nodes and leading-edge DRAM nodes. However, the typical design
method of the illuminator has issues with the transmission owing to the
limitation of optical structure that cannot further reduce process parameter
k1, and uniformity due to the restriction of matching method that can only
consider one factor affecting uniformity. The all-facet illuminator can improve
transmission by removing relay system. Deep reinforcement learning (RL) can
improve the uniformity by considering multiple factors. In this paper, a design
method of the all-facet illuminator for high NA EUV lithography exposure tool
and a matching method based on deep RL for the double facets are proposed. The
all-facet illuminator is designed using matrix optics, and removing relay
system to achieve high transmission. The double facets is matched using the
deep RL framework, which includes the policy network with improved trainability
and low computational demands, and the reward function with great optimization
direction and fast convergence rate, enabling to rapidly generate multiple
matching results with high uniformity. An all-facet illuminator for a 0.55 NA
EUV lithography exposure tool is designed by the proposed method. Simulation
results indicate that the transmission is greater than 35%, and uniformity
exceed 99% under multiple illumination pupil shapes.
### 🌟 论文解读 | 基于深度强化学习的高NA极紫外光刻曝光机全反射面照明器设计

### 📌 背景痛点/本文动机
极紫外（EUV）光刻曝光机是半导体制造的关键模块，高数值孔径（NA）的EUV光刻曝光机可支撑亚2nm逻辑节点和前沿DRAM节点的量产。但传统照明器设计存在问题：光学结构限制导致透射率低，无法进一步降低工艺参数k₁；匹配方法仅考虑单一影响均匀性的因素，均匀性不佳。同时，现有照明器在提升透射率和均匀性方面还存在制造难度大、匹配结果单一等不足，因此设计高透射率且能优化均匀性的照明器十分关键。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：全反射面照明器设计方法  
利用矩阵光学先计算共轴初始结构（基于实现科勒照明的两个光学共轭关系），再通过倾斜和偏心得到离轴结构。该照明器去除中继系统，以此实现高透射率，为降低k₁、提升分辨率提供可能。  

💡 创新点2：基于深度强化学习（RL）的双反射面匹配方法  
构建深度RL框架用于双反射面（包含场反射面和光瞳反射面）匹配。框架中策略网络结合随机模块与深度学习模块，提升可训练性且计算量低；奖励函数纳入多个影响均匀性的因素，优化方向明确、收敛速度快。该方法能考虑多因素影响均匀性问题，快速生成多个高均匀性的匹配结果。  

### 📈 实验结果
采用所提方法设计了适用于0.55NA EUV光刻曝光机的全反射面照明器。仿真结果显示，该照明器透射率大于35%（如达到35.32%，比其他EUV光刻曝光机照明器至少高39%）；在多种照明光瞳形状下，均匀性超过99%。  

### 💬 可借鉴之处
在光学设计领域，将矩阵光学用于特殊结构（全反射面）照明器的初始设计，为复杂光学系统的结构搭建提供了从理论到实践的思路；深度强化学习与光学匹配问题的结合，展示了AI技术在传统光学工程中解决多因素优化、多结果生成等难题的潜力，为后续光学系统中涉及多变量、多目标优化的任务提供了AI赋能的范例，启发研究者在光学设计、制造等环节更多地探索AI技术的融合应用。

## booster-gym--an-end-to-end-reinforcement-learning-framework-for-humanoid-robot-locomotion
### Abstract
Recent advancements in reinforcement learning (RL) have led to significant
progress in humanoid robot locomotion, simplifying the design and training of
motion policies in simulation. However, the numerous implementation details
make transferring these policies to real-world robots a challenging task. To
address this, we have developed a comprehensive code framework that covers the
entire process from training to deployment, incorporating common RL training
methods, domain randomization, reward function design, and solutions for
handling parallel structures. This library is made available as a community
resource, with detailed descriptions of its design and experimental results. We
validate the framework on the Booster T1 robot, demonstrating that the trained
policies seamlessly transfer to the physical platform, enabling capabilities
such as omnidirectional walking, disturbance resistance, and terrain
adaptability. We hope this work provides a convenient tool for the robotics
community, accelerating the development of humanoid robots. The code can be
found in https://github.com/BoosterRobotics/booster_gym.
### 🌟 论文解读 | Booster Gym：助力人形机器人 locomotion 的端到端强化学习框架

### 📌 背景痛点/本文动机
近年来，强化学习（RL）在人形机器人运动控制领域取得显著进展，让仿真环境下运动策略的设计与训练更简便。但从仿真到真实世界的策略迁移面临诸多挑战，像机器人动力学复杂、感知噪声、硬件限制等，大量实现细节使策略落地困难。为解决该问题，团队开发了覆盖从训练到部署全流程的代码框架 Booster Gym，助力人形机器人研发。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：端到端解决方案  
提供从仿真训练到真实世界部署的完整端到端方案，涵盖整个流程，让基于 RL 的人形机器人运动策略训练与部署有了全面支撑，能更顺畅地实现从虚拟到物理平台的过渡。  

💡 创新点2：全面域随机化  
针对环境、机器人和执行器开展全面的域随机化，减少仿真到真实世界的差距，提升训练后策略在实体机器人上部署时的鲁棒性，让策略在不同真实场景下也能稳定发挥作用。  

💡 创新点3：易扩展的接口设计  
打造易于修改的环境和算法接口，方便研究人员根据不同任务高效调整奖励函数、网络架构和物理参数等，降低研究人员定制开发的门槛，提升研究灵活性。  

### 📈 实验结果
在 Booster T1 人形机器人上验证框架有效性，训练后的策略能无缝迁移到物理平台，实现全向行走、抗干扰、地形适应等能力；还在 Isaac Gym 训练、MuJoco 跨仿真测试、Webots 验证以及真实世界部署等多环节验证，展现框架在多环境下的实用性与泛化性。  

### 💬 可借鉴之处
对于机器人领域研究者，该框架是开源可用的工具，能加速人形机器人研发进程；其端到端思路、域随机化方法、易扩展接口设计等，为解决仿真到真实世界迁移难题、提升策略鲁棒性、灵活适配不同任务等提供了优秀范例，在机器人强化学习研究与工程实践中都有诸多可参考借鉴的地方，比如域随机化在缩小仿真现实差距的应用、接口设计对研究效率的提升等思路，都值得相关领域开发者学习。

## intellilung--advancing-safe-mechanical-ventilation-using-offline-rl-with-hybrid-actions-and-clinically-aligned-rewards
### Abstract
Invasive mechanical ventilation (MV) is a life-sustaining therapy for
critically ill patients in the intensive care unit (ICU). However, optimizing
its settings remains a complex and error-prone process due to patient-specific
variability. While Offline Reinforcement Learning (RL) shows promise for MV
control, current stateof-the-art (SOTA) methods struggle with the hybrid
(continuous and discrete) nature of MV actions. Discretizing the action space
limits available actions due to exponential growth in combinations and
introduces distribution shifts that can compromise safety. In this paper, we
propose optimizations that build upon prior work in action space reduction to
address the challenges of discrete action spaces. We also adapt SOTA offline RL
algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby
avoiding the pitfalls of discretization. Additionally, we introduce a
clinically grounded reward function based on ventilator-free days and
physiological targets, which provides a more meaningful optimization objective
compared to traditional sparse mortality-based rewards. Our findings
demonstrate that AI-assisted MV optimization may enhance patient safety and
enable individualized lung support, representing a significant advancement
toward intelligent, data-driven critical care solutions.
### 🌟 论文解读 | IntelliLung：用离线强化学习+混合动作+临床奖励推进安全机械通气

### 📌 背景痛点/本文动机
有创机械通气（MV）是重症监护病房（ICU）中危重症患者的生命维持疗法，在新冠疫情期间其关键作用愈发凸显。然而，因患者个体差异大，优化通气设置是复杂且易出错的过程，还存在引发呼吸机诱导肺损伤（VILI）风险；同时临床MV保护协议执行度差、高护士 - 患者比下易出现恢复不佳等问题。基于AI的决策支持系统（AI - DSS）可应对挑战，离线强化学习（RL）虽有潜力，但现有方法在处理MV混合（连续 + 离散）动作空间时存在缺陷：离散化动作空间会因组合爆炸限制可选动作、引入分布偏移影响安全性；且传统基于死亡率的稀疏奖励对评估MV干预不够可靠。因此，本文旨在用离线RL开发IntelliLung这一AI - DSS来解决上述MV优化难题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：临床对齐的奖励函数设计
以往MV优化多基于死亡率的稀疏奖励，而医学研究表明死亡率不是评估MV干预的可靠终点。本文引入基于无呼吸机天数（VFD）和生理参数范围的奖励函数，能更好契合减少VILI的医疗目标，同时平衡两方面因素作用，相比传统奖励更具意义。
💡 创新点2：动作空间优化策略
之前研究因动作空间随维度指数增长常限制离散动作数量，本文展示了一种简化动作空间的方法，并结合先前研究的优化手段，在增加动作数量同时提升安全性，解决离散动作空间挑战。
💡 创新点3：适配混合动作空间的离线RL算法
MV存在连续和离散设置（动作），离散化连续动作有诸多弊端。本文适配SOTA离线RL算法（IQL和EDAC），使其直接在混合动作空间运行，避免离散化陷阱；还指出以往离散化后再重建连续值会引入分布偏移和不安全策略，而该方法能规避此问题。

### 📈 实验结果
文中未详细展开实验结果数值等内容，但从整体论述可知，通过新奖励函数、动作空间优化、适配混合动作空间的算法等创新手段，证明了AI辅助的MV优化有望提升患者安全性、实现个性化肺部支持，向智能数据驱动的重症监护解决方案迈出重要一步。

### 💬 可借鉴之处
1. 奖励函数设计层面：在医疗等领域，当传统单一终点指标（如死亡率）不适宜时，可参考本文结合更贴合临床目标、多维度的指标（如VFD + 生理参数范围）设计奖励，让优化目标更合理。
2. 动作空间处理层面：面对混合动作空间任务，可借鉴本文对动作空间优化、适配算法到混合空间的思路，避免离散化弊端，尤其在类似医疗这种对安全性要求高的场景极具参考价值。
3. 跨领域协作层面：本文由医疗和技术多方合作，在确定队列、问题构建等环节紧密结合领域专家知识，这种产学研医结合模式为开发实用AI医疗系统提供了范例，后续相关领域项目可借鉴该协作模式确保方案实用性。

