# Paper List of Terms(Reward Function+RL)
- [25/06] **Scaffolding Dexterous Manipulation with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2506.19212v1)] [[Code/Page]()] [[TLDR/Notes](#scaffolding-dexterous-manipulation-with-vision-language-models)]

- [25/06] **AdapThink: Adaptive Thinking Preferences for Reasoning Language Model**  
[[Paper](http://arxiv.org/pdf/2506.18237v1)] [[Code/Page]()] [[TLDR/Notes](#adapthink--adaptive-thinking-preferences-for-reasoning-language-model)]

- [25/06] **Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach**  
[[Paper](http://arxiv.org/pdf/2506.17828v1)] [[Code/Page]()] [[TLDR/Notes](#aligning-frozen-llms-by-reinforcement-learning--an-iterative-reweight-then-optimize-approach)]

- [25/06] **Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators**  
[[Paper](http://arxiv.org/pdf/2506.17007v1)] [[Code/Page](https://github.com/marcojira/tgm.)] [[TLDR/Notes](#robust-reinforcement-learning-for-discrete-compositional-generation-via-general-soft-operators)]

- [25/06] **Learning Dexterous Object Handover**  
[[Paper](http://arxiv.org/pdf/2506.16822v1)] [[Code/Page]()] [[TLDR/Notes](#learning-dexterous-object-handover)]

- [25/06] **GoalLadder: Incremental Goal Discovery with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2506.16396v1)] [[Code/Page]()] [[TLDR/Notes](#goalladder--incremental-goal-discovery-with-vision-language-models)]

- [25/06] **Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations**  
[[Paper](http://arxiv.org/pdf/2506.16016v1)] [[Code/Page]()] [[TLDR/Notes](#dual-objective-reinforcement-learning-with-novel-hamilton-jacobi-bellman-formulations)]

- [25/06] **Design of an all-facet illuminator for high NA EUV lithography exposure tool based on deep reinforcement learning**  
[[Paper](http://arxiv.org/pdf/2506.15558v1)] [[Code/Page]()] [[TLDR/Notes](#design-of-an-all-facet-illuminator-for-high-na-euv-lithography-exposure-tool-based-on-deep-reinforcement-learning)]

- [25/06] **Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion**  
[[Paper](http://arxiv.org/pdf/2506.15132v1)] [[Code/Page](https://github.com/BoosterRobotics/booster_gym.)] [[TLDR/Notes](#booster-gym--an-end-to-end-reinforcement-learning-framework-for-humanoid-robot-locomotion)]

- [25/06] **IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards**  
[[Paper](http://arxiv.org/pdf/2506.14375v1)] [[Code/Page]()] [[TLDR/Notes](#intellilung--advancing-safe-mechanical-ventilation-using-offline-rl-with-hybrid-actions-and-clinically-aligned-rewards)]



# TLDR/Notes
## scaffolding-dexterous-manipulation-with-vision-language-models
### Abstract
Dexterous robotic hands are essential for performing complex manipulation
tasks, yet remain difficult to train due to the challenges of demonstration
collection and high-dimensional control. While reinforcement learning (RL) can
alleviate the data bottleneck by generating experience in simulation, it
typically relies on carefully designed, task-specific reward functions, which
hinder scalability and generalization. Thus, contemporary works in dexterous
manipulation have often bootstrapped from reference trajectories. These
trajectories specify target hand poses that guide the exploration of RL
policies and object poses that enable dense, task-agnostic rewards. However,
sourcing suitable trajectories - particularly for dexterous hands - remains a
significant challenge. Yet, the precise details in explicit reference
trajectories are often unnecessary, as RL ultimately refines the motion. Our
key insight is that modern vision-language models (VLMs) already encode the
commonsense spatial and semantic knowledge needed to specify tasks and guide
exploration effectively. Given a task description (e.g., "open the cabinet")
and a visual scene, our method uses an off-the-shelf VLM to first identify
task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D
trajectories for hand motion and object motion. Subsequently, we train a
low-level residual RL policy in simulation to track these coarse trajectories
or "scaffolds" with high fidelity. Across a number of simulated tasks involving
articulated objects and semantic understanding, we demonstrate that our method
is able to learn robust dexterous manipulation policies. Moreover, we showcase
that our method transfers to real-world robotic hands without any human
demonstrations or handcrafted rewards.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºçµå·§æ“ä½œâ€œæ­è„šæ‰‹æ¶â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
çµå·§æœºæ¢°æ‰‹å¯¹æ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†ç”±äºç¤ºèŒƒæ•°æ®æ”¶é›†éš¾ã€é«˜ç»´æ§åˆ¶å¤æ‚ï¼Œè®­ç»ƒèµ·æ¥é¢‡å…·æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½èƒ½é€šè¿‡ä»¿çœŸç”Ÿæˆç»éªŒç¼“è§£æ•°æ®ç“¶é¢ˆï¼Œå´ä¾èµ–ç²¾å¿ƒè®¾è®¡çš„ç‰¹å®šä»»åŠ¡å¥–åŠ±å‡½æ•°ï¼Œé™åˆ¶äº†æ‰©å±•æ€§ä¸æ³›åŒ–æ€§ã€‚å½“ä¸‹çµå·§æ“ä½œç ”ç©¶å¸¸å€ŸåŠ©å‚è€ƒè½¨è¿¹å¼•å¯¼RLç­–ç•¥æ¢ç´¢ï¼Œå¯è·å–åˆé€‚è½¨è¿¹ï¼ˆå°¤å…¶é’ˆå¯¹çµå·§æ‰‹ï¼‰ä»æ˜¯éš¾é¢˜ï¼Œä¸”å‚è€ƒè½¨è¿¹çš„ç²¾ç¡®ç»†èŠ‚å¯¹æœ€ç»ˆç»RLä¼˜åŒ–çš„è¿åŠ¨è€Œè¨€å¹¶éå¿…éœ€ã€‚äºæ˜¯ï¼Œè®ºæ–‡æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç¼–ç çš„å¸¸è¯†æ€§ç©ºé—´ä¸è¯­ä¹‰çŸ¥è¯†ï¼Œæ¥ç”Ÿæˆå¼•å¯¼RLçš„â€œè„šæ‰‹æ¶â€è½¨è¿¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå€ŸåŠ©è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆâ€œè„šæ‰‹æ¶â€è½¨è¿¹  
ç»™å®šä»»åŠ¡æè¿°ï¼ˆå¦‚â€œæ‰“å¼€æ©±æŸœâ€ï¼‰å’Œè§†è§‰åœºæ™¯ï¼Œåˆ©ç”¨ç°æˆçš„VLMså…ˆè¯†åˆ«ä»»åŠ¡ç›¸å…³å…³é”®ç‚¹ï¼ˆå¦‚æŠŠæ‰‹ã€æŒ‰é’®ï¼‰ï¼Œå†åˆæˆæ‰‹å’Œç‰©ä½“è¿åŠ¨çš„3Dè½¨è¿¹ã€‚è¿™äº›è½¨è¿¹ä½œä¸ºâ€œç²—ç²’åº¦â€å¼•å¯¼ï¼Œæ— éœ€ç²¾ç¡®åˆ°äººç±»ç¤ºèŒƒçº§ç»†èŠ‚ï¼Œå´èƒ½ä¸ºRLæä¾›æ¢ç´¢æ–¹å‘ä¸å¥–åŠ±ä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ®‹å·®å¼ºåŒ–å­¦ä¹ ç­–ç•¥è·Ÿè¸ªè½¨è¿¹  
åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒä½å±‚æ¬¡æ®‹å·®RLç­–ç•¥ï¼Œè®©å…¶é«˜ç²¾åº¦è·Ÿè¸ªVLMsç”Ÿæˆçš„â€œè„šæ‰‹æ¶â€è½¨è¿¹ã€‚RLé€šè¿‡ä¼˜åŒ–æ¯ä¸€æ­¥çš„åç§»å’Œæ‰‹æŒ‡åŠ¨ä½œæ¥æœ€å¤§åŒ–è·Ÿè¸ªå¥–åŠ±ï¼Œæ— éœ€äººå·¥è®¾è®¡å¤æ‚å¥–åŠ±å‡½æ•°ï¼Œè¿˜èƒ½åœ¨è¿‡ç¨‹ä¸­è¶…è¶Šäººç±»é¥æ“ä½œçš„æ€§èƒ½ä¸ç²¾åº¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆ©ç”¨VLMsç‰¹æ€§æå‡æ³›åŒ–ä¸é²æ£’æ€§  
é€šè¿‡é‡å¤æŸ¥è¯¢VLMsï¼ŒéšæœºåŒ–åˆå§‹å…³é”®ç‚¹å’Œé«˜å±‚è½¨è¿¹ï¼Œè®©ç­–ç•¥åœ¨æµ‹è¯•æ—¶èƒ½æ³›åŒ–åˆ°æœªè§è¿‡çš„åˆå§‹æ¡ä»¶ä¸æ–°è½¨è¿¹ï¼›å½“VLMsé«˜å±‚è§„åˆ’æœ‰è¯¯å·®æ—¶ï¼Œæä¾›ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯å¤§å¹…æå‡æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¶‰åŠé“°æ¥ç‰©ä½“å’Œè¯­ä¹‰ç†è§£çš„å¤šä¸ªä»¿çœŸä»»åŠ¡ä¸­ï¼Œè®ºæ–‡æ–¹æ³•åœ¨8ä¸ªä»»åŠ¡ä¸Šï¼Œæ— éœ€æ‰‹åŠ¨å¥–åŠ±è®¾è®¡å°±èƒ½è¾¾åˆ°æ¥è¿‘â€œ oracle çº§â€æ‰‹å·¥è½¨è¿¹çš„æˆåŠŸç‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æˆåŠŸå®ç°äº†å‘çœŸå®ä¸–ç•Œçµå·§æœºæ¢°æ‰‹çš„è·¨åŸŸè¿ç§»ï¼Œåœ¨æ— äººç±»ç¤ºèŒƒå’Œæ‰‹å·¥å¥–åŠ±æƒ…å†µä¸‹ä»èƒ½è¾¾æˆé²æ£’æ“ä½œæ€§èƒ½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨æ¨¡æ€åä½œæ€è·¯ï¼šå°†VLMsçš„è¯­ä¹‰ç©ºé—´æ¨ç†èƒ½åŠ›ä¸RLçš„æ§åˆ¶ä¼˜åŒ–èƒ½åŠ›ç»“åˆï¼Œä¸ºè§£å†³é«˜ç»´å¤æ‚æ§åˆ¶ä»»åŠ¡æä¾›äº†â€œé«˜å±‚è¯­ä¹‰å¼•å¯¼ + ä½å±‚ç²¾ç¡®ä¼˜åŒ–â€çš„æ–°èŒƒå¼ï¼Œå¯æ¨å¹¿åˆ°éœ€è¯­ä¹‰ç†è§£ä¸ç²¾ç»†æ“ä½œç»“åˆçš„æœºå™¨äººä»»åŠ¡ã€‚  
2. å¼±åŒ–å¯¹ç²¾ç¡®ç¤ºèŒƒçš„ä¾èµ–ï¼šè¯æ˜ç²—ç²’åº¦è½¨è¿¹è¶³ä»¥æ”¯æ’‘RLè®­ç»ƒï¼Œä¸ºå‡å°‘æœºå™¨äººå­¦ä¹ å¯¹å¤§è§„æ¨¡é«˜è´¨é‡ç¤ºèŒƒæ•°æ®é›†çš„ä¾èµ–æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œé™ä½ä»»åŠ¡æ‹“å±•æ—¶çš„æ•°æ®é‡‡é›†æˆæœ¬ã€‚  
3. æ³›åŒ–ä¸è¿ç§»æŠ€å·§ï¼šåˆ©ç”¨VLMsç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹æ¥å¢å¼ºæ³›åŒ–ã€é€šè¿‡ä¸Šä¸‹æ–‡ç¤ºä¾‹æå‡é²æ£’æ€§ç­‰æ‰‹æ®µï¼Œä¸ºåŸºäºå¤§æ¨¡å‹çš„æœºå™¨äººå­¦ä¹ åœ¨å®é™…éƒ¨ç½²ï¼ˆå¦‚ç¯å¢ƒå˜åŒ–ã€æ¨¡å‹è¯¯å·®åœºæ™¯ï¼‰æä¾›äº†å®ç”¨æ–¹æ³•è®ºã€‚

## adapthink--adaptive-thinking-preferences-for-reasoning-language-model
### Abstract
Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdapThinkï¼šè®©æ¨ç†è¯­è¨€æ¨¡å‹æ‹¥æœ‰è‡ªé€‚åº”â€œæ€è€ƒåå¥½â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæå¤§æå‡äº†è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå‚¬ç”Ÿäº†ç²¾ç»†çš„è‡ªæˆ‘åæ€è¿‡ç¨‹ï¼ˆå³â€œæ…¢æ€è€ƒâ€èŒƒå¼ï¼‰ã€‚ä½†è¿™ä¸€èŒƒå¼å­˜åœ¨æ¨ç†æ•ˆç‡éš¾é¢˜ï¼šæ¨¡å‹é¢å¯¹ç®€å•é—®é¢˜æ—¶å¯èƒ½è¿‡åº¦è®¡ç®—ï¼Œé¢å¯¹å¤æ‚é—®é¢˜æ—¶åˆå¯èƒ½è¿‡æ—©åˆ‡æ¢æ¨ç†æ€è·¯ã€‚ä»¥å¾€æœºåˆ¶ä¾èµ–é™æ€é•¿åº¦é¢„ç®—æˆ–é¢„å®šä¹‰è§„åˆ™ï¼Œç¼ºä¹å¯¹é—®é¢˜å¤æ‚åº¦å˜åŒ–å’Œæ¨¡å‹èƒ½åŠ›æ¼”è¿›çš„é€‚åº”æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§è‡ªé€‚åº”åè®­ç»ƒæ¡†æ¶ï¼Œåœ¨ç»´æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶æå‡æ•ˆç‡ï¼ŒAdapThink åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°  
è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°æ¥è°ƒæ•´æ¨¡å‹å½“å‰æ¨ç†åå¥½ã€‚æ¨¡å‹å¯åŸºäºç”Ÿæˆå“åº”çš„ç»„å†…å‡†ç¡®ç‡ï¼Œç¡®å®šåˆé€‚çš„åæ€åå¥½ï¼›åŒæ—¶é€šè¿‡ç»Ÿè®¡è®­ç»ƒæ ·æœ¬ç»„ä¸­å…³é”®è¿‡æ¸¡è¯æ•°é‡ï¼Œå®šé‡è¡¡é‡æ¨ç†æ•ˆç‡ï¼Œæ— éœ€ä¾èµ–å›ºå®šé•¿åº¦åå¥½ï¼Œè€Œæ˜¯åˆ©ç”¨æ¨¡å‹ç½®ä¿¡åº¦ä¸å“åº”ç‰¹å¾åŠ¨æ€è°ƒæ•´åæ€ç±»è¿‡æ¸¡è¯çš„åå¥½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶  
æå‡ºå¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶å¹³è¡¡è®­ç»ƒæ ·æœ¬ç»„çš„è§£é¢˜å‡†ç¡®ç‡ä¸æ¨ç†å¤šæ ·æ€§ã€‚å…ˆå¯¹æ¨ç†å®ä¾‹è¿‡é‡‡æ ·ï¼Œå†ç”¨ç²¾å¿ƒå®šä¹‰çš„å¤šæ ·æ€§æŒ‡æ ‡è¯„ä¼°å®ä¾‹çš„æœ€ç»ˆç­”æ¡ˆä¸ä¸­é—´æ­¥éª¤ï¼Œæœ€åé€šè¿‡å¤šæ ·æ€§æ„ŸçŸ¥ä¸‹é‡‡æ ·æ¥ç­›é€‰å’Œæå‡ç”¨äº RL åè®­ç»ƒçš„å®ä¾‹æ•´ä½“è´¨é‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ DeepSeek è’¸é¦æ¨¡å‹è¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒAdapThink åœ¨ä½¿æ¨¡å‹å…·å¤‡è‡ªé€‚åº”æ¨ç†æ¨¡å¼ã€ç¼“è§£æ¨ç†ä½æ•ˆé—®é¢˜ä¸Šå±•ç°ä¼˜åŠ¿ã€‚å¦‚å¯¹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ä»… 2K token çš„ DeepSeek - è’¸é¦ Qwen æ¨¡å‹è¿›è¡Œåè®­ç»ƒåï¼Œåœ¨ 8K - token é™åˆ¶ä¸‹æµ‹è¯•ï¼Œç›¸æ¯”å¤šä¸ªé•¿åº¦æ§åˆ¶åŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œè¯æ˜äº†å…¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ‰“é€ å¼ºå¤§ä¸”é«˜æ•ˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ‰“ç ´é™æ€é•¿åº¦é™åˆ¶æ€ç»´ï¼Œé€šè¿‡åˆ†ææ ·æœ¬ç»„æ¨ç†æ¨¡å¼åˆ†å¸ƒæ¥è°ƒæ•´é•¿åº¦åå¥½ï¼Œä¸ºå¤„ç†æ¨¡å‹æ¨ç†æ•ˆç‡ä¸å¤æ‚åº¦é€‚é…é—®é¢˜æä¾›äº†æ–°è§†è§’ï¼Œä¸å†å±€é™äºå›ºå®šè§„åˆ™çº¦æŸã€‚  
2. ç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°ä¸å¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è¦å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡ã€å…¼é¡¾å¤šæ ·æ€§ä¸å‡†ç¡®æ€§çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯æ¶‰åŠåæ€ã€æ¨ç†ç±»çš„è¯­è¨€æ¨¡å‹ä¼˜åŒ–åœºæ™¯ã€‚  
3. å¯¹æ¨ç†è¿‡ç¨‹ä¸­å…³é”®è¿‡æ¸¡è¯ï¼ˆå¦‚â€œPause - Validationâ€â€œBranch - Extensionâ€ç±»è¯æ±‡ï¼‰çš„åˆ†æä¸åˆ©ç”¨æ–¹å¼ï¼Œä¸ºåç»­ç ”ç©¶å¦‚ä½•ä»è¯­è¨€è¡¨è¾¾ç‰¹å¾å±‚é¢å¼•å¯¼æ¨¡å‹æ¨ç†è¡Œä¸ºæä¾›äº†å‚è€ƒèŒƒå¼ã€‚

## aligning-frozen-llms-by-reinforcement-learning--an-iterative-reweight-then-optimize-approach
### Abstract
Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œç”¨å¼ºåŒ–å­¦ä¹ å¯¹é½å†»ç»“å¤§æ¨¡å‹ï¼šè¿­ä»£é‡åŠ æƒä¼˜åŒ–æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½äººç±»åå¥½é€šå¸¸ä¾èµ–RLHFã€DPOç­‰å¾®è°ƒæ–¹æ³•ï¼Œè¿™ç±»æ–¹æ³•éœ€ç›´æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œåœ¨æµ‹è¯•é˜¶æ®µæ— æ³•æå‡æ€§èƒ½ï¼Œä¸”æ¨¡å‹æƒé‡ä¸å¯è®¿é—®æ—¶ä¹Ÿæ— æ³•ä½¿ç”¨ã€‚æµ‹è¯•æ—¶å¯¹é½æ–¹æ³•è™½ç»•å¼€æƒé‡æ›´æ–°ï¼Œä½†æ¨ç†æˆæœ¬é«˜ï¼Œä¸”åŸºäºä¸å®Œå–„å¥–åŠ±æˆ–ä»·å€¼å‡½æ•°çš„ä¸€æ¬¡æ€§æŒ‡å¯¼æ˜“äº§ç”Ÿæ¬¡ä¼˜è¾“å‡ºã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡èšç„¦äºï¼šç»™å®šå†»ç»“LLMå’Œç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ï¼Œå¦‚ä½•åœ¨æµ‹è¯•æ—¶é«˜æ•ˆæ”¹è¿›æˆ–å®šåˆ¶æ¨¡å‹ï¼ŒåŒæ—¶æœ€å°åŒ–æ¨ç†æˆæœ¬ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè¿­ä»£é‡åŠ æƒ - ä¼˜åŒ–ï¼ˆIROï¼‰æ¡†æ¶  
IROæ˜¯å—å¼ºåŒ–å­¦ä¹ å¯å‘çš„æ¡†æ¶ï¼Œæ— éœ€ä¿®æ”¹å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°å°±èƒ½å®ç°ç±»RLçš„å¯¹é½ã€‚è®­ç»ƒæ—¶é€šè¿‡ä¸‰æ­¥è¿­ä»£ï¼šä»åŸºç¡€æ¨¡å‹é‡‡æ ·å€™é€‰ã€ç”¨å½“å‰ä»·å€¼å‡½æ•°é‡é‡‡æ ·ã€è®­ç»ƒæ–°è½»é‡çº§ä»·å€¼å‡½æ•°æŒ‡å¯¼ä¸‹ä¸€æ¬¡è§£ç ï¼›æµ‹è¯•æ—¶åˆ©ç”¨å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°åºåˆ—ï¼Œé€šè¿‡åŸºäºæœç´¢çš„ä¼˜åŒ–è¿‡ç¨‹å¼•å¯¼åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç±»ç­–ç•¥è¿­ä»£ä¸é«˜æ•ˆæ¨ç†  
ä»ç†è®ºä¸Šè¯æ˜ï¼Œåœ¨æ¸©å’Œæ¡ä»¶ä¸‹IROå±äºä¸€ç§ç­–ç•¥è¿­ä»£ï¼Œæµ‹è¯•æ—¶èƒ½ä»¥æŒ‡æ•°çº§æ›´å°‘çš„tokensè¾¾åˆ°Best - of - Nï¼ˆBoNï¼‰æœç´¢çš„æ€§èƒ½ã€‚ä¸”ç”¨æˆ·å¯ç±»ä¼¼OpenAIçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ï¼Œç”¨IROåœ¨è‡ªæœ‰æ•°æ®é›†ä¸Šä»¥RLé£æ ¼å¾®è°ƒæ¨¡å‹ï¼Œè¿˜æ— éœ€è®¿é—®æ¨¡å‹æƒé‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AlpacaEval 2.0ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­ï¼ŒIROæ˜¾è‘—æé«˜äº†é•¿åº¦æ§åˆ¶èƒœç‡ã€‚å¦‚Llama - 3 - 8B - Instructä»30.71%æå‡åˆ°43.80%ï¼ŒMeta - Llama - 3 - 70B - Instructä»43.11%æå‡åˆ°49.77%ï¼ˆä¸GPT - 4å“åº”å¯¹æ¯”ï¼‰ã€‚æ­¤å¤–ï¼Œå³ä½¿ä½¿ç”¨å°å°ºå¯¸ï¼ˆ1Bæˆ–7Bï¼‰ä»·å€¼å‡½æ•°å¼•å¯¼å¤§åŸºç¡€æ¨¡å‹ï¼ˆ6.9Bæˆ–70Bï¼‰ï¼ŒIROä¹ŸæŒç»­ä¼˜äºBoNã€weak - to - strong searchç­‰ç°æœ‰æµ‹è¯•æ—¶å¯¹é½åŸºçº¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºæ— æ³•è·å–æ¨¡å‹æƒé‡ä½†éœ€åœ¨æµ‹è¯•é˜¶æ®µä¼˜åŒ–æ¨¡å‹å¯¹é½äººç±»åå¥½çš„åœºæ™¯ï¼ŒIROæä¾›äº†æ–°èŒƒå¼ï¼Œå…¶è¿­ä»£è®­ç»ƒè½»é‡ä»·å€¼å‡½æ•° + æµ‹è¯•æ—¶æœç´¢ä¼˜åŒ–çš„æ€è·¯ï¼Œä¸ºä½æ¨ç†æˆæœ¬ä¸‹çš„æ¨¡å‹æ€§èƒ½æå‡å’Œé¢†åŸŸé€‚é…å¼€è¾Ÿè·¯å¾„ï¼›ç†è®ºä¸Šå¯¹ç­–ç•¥è¿­ä»£çš„è¯æ˜ï¼Œä¹Ÿä¸ºåç»­å¼ºåŒ–å­¦ä¹ åœ¨å†»ç»“æ¨¡å‹å¯¹é½æ–¹å‘çš„ç ”ç©¶æä¾›äº†ç†è®ºæ”¯æ’‘ï¼›åŒæ—¶åœ¨å·¥ä¸šç•Œï¼Œç±»ä¼¼OpenAI RFTä½†æ— éœ€æƒé‡è®¿é—®çš„ç‰¹æ€§ï¼Œè®©ä¼ä¸šåœ¨è‡ªæœ‰æ•°æ®å®šåˆ¶æ¨¡å‹æ—¶æ›´å…·çµæ´»æ€§ã€‚

## robust-reinforcement-learning-for-discrete-compositional-generation-via-general-soft-operators
### Abstract
A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç¦»æ•£ç»„åˆç”Ÿæˆä»»åŠ¡ä¸­åŸºäºå¹¿ä¹‰è½¯ç®—å­çš„é²æ£’å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç§‘å­¦å‘ç°é¢†åŸŸï¼Œä»è›‹ç™½è´¨ã€åˆ†å­ç­‰å¤§è§„æ¨¡ç»„åˆå¯¹è±¡é›†åˆä¸­ç­›é€‰å‡ºå°‘é‡æœ‰å‰æ™¯çš„å€™é€‰å¯¹è±¡æ˜¯å…³é”®ç“¶é¢ˆã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ä¸“å®¶çŸ¥è¯†ï¼Œè€Œè¿‘æœŸæ–¹æ³•å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆä»£ç†å¥–åŠ±å‡½æ•°ä¸æ­£åˆ™åŒ–æ¥ä¼˜åŒ–ç­›é€‰ï¼Œä½†ä»£ç†å¥–åŠ±å‡½æ•°å­˜åœ¨å›ºæœ‰ä¸ç¡®å®šæ€§ï¼Œç°æœ‰æŒ‰å¥–åŠ±æ¯”ä¾‹é‡‡æ ·çš„æ–¹æ³•åœ¨å¤§æœç´¢ç©ºé—´ä¸‹è¡¨ç°ä¸ä½³ï¼Œæ— æ³•å¾—åˆ°æœ€ä¼˜å€™é€‰å¯¹è±¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡é²æ£’å¼ºåŒ–å­¦ä¹ æ–¹æ³•è§£å†³ä»£ç†å¥–åŠ±å‡½æ•°ä¸ç¡®å®šæ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºç»„åˆå¼å¥–åŠ±ä¸ç¡®å®šæ€§æ¨¡å‹  
æå‡ºç¦»æ•£ç»„åˆç”Ÿæˆï¼ˆDCGï¼‰ä»»åŠ¡ä¸­å¥–åŠ±ä¸ç¡®å®šæ€§çš„ç»„åˆæ¨¡å‹ï¼Œæ˜ç¡®æ­£åˆ™åŒ–RLä¸å¥–åŠ±é²æ£’RLåœ¨ä»·å€¼å‡½æ•°ä¸Šçš„è”ç³»ï¼Œèƒ½åˆ†æä¸åŒè½¯ç®—å­å¯¹åº”çš„å¥–åŠ±ä¸ç¡®å®šæ€§é›†åˆï¼Œä¸ºåç»­æ–¹æ³•è®¾è®¡å¥ å®šç†è®ºåŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºå¹¿ä¹‰æŸ”å’Œæœ€å¤§åŒ–ï¼ˆGMï¼‰ç®—å­ä¸TGMç®—æ³•  
å—RLä¸­é¦™å†œç†µæ­£åˆ™åŒ–çš„é²æ£’æ€§è§£é‡ŠåŠDCGåœºæ™¯ä¸‹å…¶å­˜åœ¨çš„é—®é¢˜å¯å‘ï¼Œå¼•å…¥ç»Ÿä¸€çš„å¹¿ä¹‰æŸ”å’Œæœ€å¤§åŒ–ï¼ˆGMï¼‰ç®—å­ï¼Œå®ƒèƒ½åœ¨ç”Ÿæˆæµç½‘ç»œï¼ˆGFNï¼‰ç®—å­å’Œå…¶ä»–è½¯RLç®—å­é—´å¹³æ»‘æ’å€¼ï¼Œè¿½æ±‚æ›´å°–é”çš„é‡‡æ ·åˆ†å¸ƒä»¥åº”å¯¹å¥–åŠ±ä¸ç¡®å®šæ€§ã€‚åŸºäºæ­¤ç®—å­ï¼Œè¿›ä¸€æ­¥æå‡ºå®ç”¨ç®—æ³•è½¨è¿¹GMï¼ˆTGMï¼‰ï¼Œç”¨äºåœ¨å®é™…ä»»åŠ¡ä¸­ç”Ÿæˆå€™é€‰å¯¹è±¡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åˆæˆä»»åŠ¡ä¸çœŸå®ä¸–ç•Œçš„åºåˆ—è®¾è®¡ä»»åŠ¡ï¼ˆå¦‚è›‹ç™½è´¨åºåˆ—ç”Ÿæˆç­‰ï¼‰ä¸­ï¼ŒTGMåœ¨ç”Ÿæˆå¤šæ ·ä¸”é«˜å¥–åŠ±çš„å¯¹è±¡æ–¹é¢ï¼Œè¡¨ç°ä¼˜äºGFNså’Œå…¶ä»–è½¯RLåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨åº”å¯¹å¥–åŠ±ä¸ç¡®å®šæ€§ã€ç”Ÿæˆä¼˜è´¨å€™é€‰å¯¹è±¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç†è®ºå±‚é¢ï¼šå»ºç«‹äº†æ­£åˆ™åŒ–RLä¸é²æ£’RLåœ¨DCGåœºæ™¯ä¸‹çš„è”ç³»ï¼Œä¸ºåˆ†æå¥–åŠ±ä¸ç¡®å®šæ€§æä¾›äº†æ–°è§†è§’ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§è·¨é¢†åŸŸè”ç³»çš„åˆ†ææ€è·¯æ‹“å±•æ–¹æ³•è¾¹ç•Œã€‚  
2. æ–¹æ³•å±‚é¢ï¼šæå‡ºçš„GMç®—å­å’ŒTGMç®—æ³•ä¸ºç¦»æ•£ç»„åˆç”Ÿæˆä»»åŠ¡æä¾›äº†æ›´é²æ£’çµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨è¯ç‰©åˆ†å­è®¾è®¡ã€è›‹ç™½è´¨å·¥ç¨‹ç­‰ä¾èµ–ä»å¤§è§„æ¨¡ç»„åˆä¸­ç­›é€‰çš„é¢†åŸŸï¼Œå¯å°è¯•å¼•å…¥è¯¥æ–¹æ³•ä¼˜åŒ–å€™é€‰å¯¹è±¡ç”Ÿæˆæµç¨‹ã€‚  
3. å®éªŒå±‚é¢ï¼šåœ¨åˆæˆä¸çœŸå®ä»»åŠ¡ä¸­éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§çš„æ€è·¯ï¼Œä¸ºç›¸å…³é¢†åŸŸç®—æ³•éªŒè¯æä¾›äº†å‚è€ƒèŒƒå¼ï¼Œæœ‰åŠ©äºåç»­æ–°æ–¹æ³•åœ¨å®é™…åœºæ™¯ä¸‹çš„æ€§èƒ½è¯„ä¼°ã€‚

## learning-dexterous-object-handover
### Abstract
Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¼ºåŒ–å­¦ä¹ å®ç°çµå·§çš„ç‰©ä½“äº¤æ¥ï¼šåŒå››å…ƒæ•°å¥–åŠ±å‡½æ•°ä¸é²æ£’æ€§éªŒè¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººæœºåä½œã€æœåŠ¡æœºå™¨äººç­‰åœºæ™¯ä¸­ï¼Œç‰©ä½“äº¤æ¥æ˜¯ä¸€é¡¹å…³é”®æŠ€èƒ½ã€‚ä¼ ç»Ÿæ§åˆ¶ç†è®ºç®—æ³•åœ¨åº”å¯¹æ­¤ç±»å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨å±€é™ï¼ˆå¦‚ä¼ æ„Ÿå™¨èåˆæ˜“ç´¯ç§¯è¯¯å·®ã€éœ€ä¸ºæ¯ä¸ªå­ä»»åŠ¡è®¾è®¡ç­–ç•¥ï¼‰ï¼›è€Œäººç±»è™½æ“…é•¿è¿™ç±»éœ€çµå·§æ€§ã€æ„ŸçŸ¥ä¸åä½œçš„ä»»åŠ¡ï¼Œä½†è®©æœºå™¨äººæŒæ¡ç‰©ä½“äº¤æ¥ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å‰å·¥ä½œè¦ä¹ˆç”¨ç®€å•äºŒæŒ‡å¤¹çˆªç®€åŒ–é—®é¢˜ï¼Œè¦ä¹ˆä¾èµ–é¥æ“ä½œæ•™å­¦ï¼Œä¸”åœ¨æ—‹è½¬è¡¨ç¤ºä¸ç­–ç•¥é²æ£’æ€§ä¸Šæœ‰æå‡ç©ºé—´ã€‚å› æ­¤ï¼Œæœ¬æ–‡èšç„¦äº**åŒå¤šæŒ‡æœºå™¨äººæ‰‹ä¹‹é—´çš„çµå·§ç‰©ä½“äº¤æ¥**ï¼Œæ¢ç´¢ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§£å†³è¯¥é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡‡ç”¨å››æŒ‡å…³èŠ‚æ‰‹ï¼Œæå‡ä»»åŠ¡å¤æ‚åº¦ä¸çµå·§æ€§  
åŒºåˆ«äºè¿‡å¾€ç”¨äºŒæŒ‡å°å¤¹çˆªçš„ç ”ç©¶ï¼Œæœ¬æ–‡ä½¿ç”¨å››æŒ‡é“°æ¥å¼æœºæ¢°æ‰‹ã€‚è¿™å¢åŠ äº†è‡ªç”±åº¦ï¼ˆDoFï¼‰ï¼Œè®©ä»»åŠ¡æ›´æ¥è¿‘çœŸå®å¤æ‚åœºæ™¯ï¼Œä½†ä¹Ÿæå‡äº†åŸºäºå­¦ä¹ æ–¹æ³•çš„åº”ç”¨éš¾åº¦ï¼Œæ›´å…·ç ”ç©¶ä»·å€¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºåŒå››å…ƒæ•°çš„æ–°å‹å¥–åŠ±å‡½æ•°  
ä¸ºæœ€å°åŒ–æ—‹è½¬è·ç¦»ï¼Œæœ¬æ–‡æå‡ºåŸºäºåŒå››å…ƒæ•°çš„å¥–åŠ±å‡½æ•°ã€‚ç›¸æ¯”æ¬§æ‹‰è§’ã€æ—‹è½¬çŸ©é˜µç­‰ä¼ ç»Ÿæ—‹è½¬è¡¨ç¤ºï¼ŒåŒå››å…ƒæ•°åœ¨å¤„ç†SO(3)æ—‹è½¬çº¦æŸæ—¶æ›´å…·ä¼˜åŠ¿ï¼ˆé¿å…æ¬§æ‹‰è§’ä¸‡å‘èŠ‚é”ã€å‡å°‘çŸ©é˜µè¿ç®—å¤æ‚åº¦ç­‰ï¼‰ã€‚é€šè¿‡åŒå››å…ƒæ•°çš„åŸºæœ¬è¿ç®—ï¼ˆå¦‚å…±è½­ã€å·®ã€æ¨¡é•¿ç­‰ï¼Œè§è¡¨Iï¼‰æ¥è®¾è®¡å¥–åŠ±ï¼Œè®©ç­–ç•¥æ›´é«˜æ•ˆå­¦ä¹ æ‰‹éƒ¨å§¿æ€çš„ç²¾å‡†æ§åˆ¶ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå•æ™ºèƒ½ä½“å•é˜¶æ®µRLè®­ç»ƒèŒƒå¼  
è¿‡å¾€éƒ¨åˆ†ç ”ç©¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“æˆ–å¤šé˜¶æ®µè®­ç»ƒï¼Œéœ€è°ƒä¼˜å¤§é‡è¶…å‚æ•°ã€‚æœ¬æ–‡è®¾è®¡**å•æ™ºèƒ½ä½“+å•é˜¶æ®µ**çš„è®­ç»ƒæµç¨‹ï¼Œç®€åŒ–è®­ç»ƒé€»è¾‘ï¼Œé™ä½è¶…å‚æ•°è´Ÿæ‹…ï¼ŒåŒæ—¶ä»èƒ½è®©æœºæ¢°è‡‚ä¸æ‰‹å­¦ä¹ â€œæ¥è¿‘ - æŠ“å– - è½¬ç§»â€çš„å®Œæ•´äº¤æ¥æµç¨‹ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. å¯¹è®­ç»ƒåˆ†å¸ƒå¤–ç‰©ä½“çš„é²æ£’æ€§ï¼šåœ¨æµ‹è¯•æœªå‚ä¸è®­ç»ƒçš„æ–°ç‰©ä½“æ—¶ï¼Œç­–ç•¥è¡¨ç°å‡ºå¼ºé²æ£’æ€§ã€‚æœ€ä¼˜åœºæ™¯ä¸‹ï¼Œ100æ¬¡å®éªŒæˆåŠŸç‡è¾¾94%ï¼Œè¯æ˜å¯¹æ–°ç‰©ä½“çš„é€‚åº”èƒ½åŠ›ã€‚  
2. äº¤æ¥è¿‡ç¨‹æ‰°åŠ¨çš„é²æ£’æ€§ï¼šå½“â€œé€’ç‰©â€æœºå™¨äººåœ¨äº¤æ¥ä¸­ç§»åŠ¨ï¼ˆç°å®ä¸­å¸¸è§æ‰°åŠ¨ï¼‰ï¼Œæœ€ä¼˜ç­–ç•¥æ€§èƒ½ä»…ä¸‹é™13.8%ï¼ŒéªŒè¯äº†ç­–ç•¥åœ¨åŠ¨æ€å¹²æ‰°ä¸‹çš„ç¨³å®šæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ—‹è½¬è¡¨ç¤ºçš„åˆ›æ–°åº”ç”¨ï¼šåŒå››å…ƒæ•°åœ¨æœºå™¨äººå§¿æ€æ§åˆ¶çš„å¥–åŠ±è®¾è®¡ä¸­å±•ç°ä¼˜åŠ¿ï¼Œä¸ºéœ€ç²¾å‡†æ—‹è½¬åŒ¹é…çš„ä»»åŠ¡ï¼ˆå¦‚è£…é…ã€æ“ä½œï¼‰æä¾›äº†æ›´é«˜æ•ˆçš„æ•°å­¦å·¥å…·å‚è€ƒã€‚  
- å¤æ‚æœºæ¢°è‡‚+å¤šæŒ‡æ‰‹çš„RLè®­ç»ƒï¼šè¯æ˜åœ¨é«˜è‡ªç”±åº¦ã€é«˜å¤æ‚åº¦çš„æœºæ¢°ç³»ç»Ÿä¸Šï¼ŒRLèƒ½å­¦ä¹ åˆ°æœ‰æ•ˆç­–ç•¥ï¼Œä¸ºåç»­æ›´å¤æ‚çš„äººæœºåä½œã€å¤šæœºå™¨äººåä½œä»»åŠ¡é“ºè·¯ã€‚  
- é²æ£’æ€§éªŒè¯èŒƒå¼ï¼šé€šè¿‡â€œè®­ç»ƒå¤–ç‰©ä½“+è¿‡ç¨‹æ‰°åŠ¨â€çš„å®éªŒè®¾è®¡ï¼Œä¸ºè¯„ä¼°æ™ºèƒ½ç­–ç•¥åœ¨çœŸå®åœºæ™¯çš„å¯é æ€§æä¾›äº†å¯å‚è€ƒçš„æµ‹è¯•æ¡†æ¶ã€‚  

ç»¼ä¸Šï¼Œæœ¬æ–‡ä»ç¡¬ä»¶å¤æ‚åº¦ã€æ•°å­¦å·¥å…·åˆ°è®­ç»ƒèŒƒå¼éƒ½åšå‡ºäº†åˆ›æ–°å°è¯•ï¼Œä¸”é€šè¿‡æ‰å®å®éªŒéªŒè¯äº†ç­–ç•¥é²æ£’æ€§ï¼Œæ˜¯å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººçµå·§æ“ä½œé¢†åŸŸçš„ä¸€ç¯‡å€¼å¾—å…³æ³¨çš„å®è·µå‹è®ºæ–‡ã€‚

## goalladder--incremental-goal-discovery-with-vision-language-models
### Abstract
Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GoalLadderï¼šç”¨è§†è§‰è¯­è¨€æ¨¡å‹å®ç°å¢é‡å¼ç›®æ ‡å‘ç°ï¼Œé©æ–°å¼ºåŒ–å­¦ä¹ ä»»åŠ¡æŒ‡å®š

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œæ‰‹åŠ¨è®¾è®¡å¥–åŠ±å‡½æ•°éœ€å¤§é‡äººåŠ›ä¸é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚è‡ªç„¶è¯­è¨€è™½èƒ½ç®€æ´æè¿°RLä»»åŠ¡ï¼Œä½†ä»è¯­è¨€æŒ‡ä»¤ä¸­æå–å¥–åŠ±å‡½æ•°ä»å…·æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨è§†è§‰ç¯å¢ƒä¸‹ã€‚ç°æœ‰åˆ©ç”¨å¤§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæˆ–ä¾èµ–éè§†è§‰ç¯å¢ƒè¡¨ç¤ºã€æˆ–éœ€æµ·é‡åé¦ˆã€æˆ–ç”Ÿæˆå™ªå£°å¤§ä¸”å½¢çŠ¶ä¸ä½³çš„å¥–åŠ±å‡½æ•°ã€‚åŒæ—¶ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”¨äºåé¦ˆæ—¶ï¼Œéœ€è§£å†³å¯¹å™ªå£°åé¦ˆçš„é²æ£’æ€§å’ŒæŸ¥è¯¢æ•ˆç‡ä¸¤å¤§å…³é”®é—®é¢˜ï¼Œä¸ºæ­¤æå‡ºGoalLadderæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¢é‡å¼ç›®æ ‡å‘ç°æœºåˆ¶  
GoalLadderå€ŸåŠ©è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥å¢é‡æ–¹å¼å‘ç°èƒ½è®©æ™ºèƒ½ä½“æ›´æ¥è¿‘è‡ªç„¶è¯­è¨€æŒ‡å®šä»»åŠ¡å®Œæˆçš„ç¯å¢ƒçŠ¶æ€ã€‚é€šè¿‡æŸ¥è¯¢VLMæ¥è¯†åˆ«ä»£è¡¨ä»»åŠ¡è¿›å±•æå‡çš„çŠ¶æ€ï¼Œå¹¶åˆ©ç”¨æˆå¯¹æ¯”è¾ƒå¯¹è¿™äº›çŠ¶æ€æ’åºï¼Œé€æ­¥æ˜ç¡®æ›´ä¼˜çŠ¶æ€ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å‘ä»»åŠ¡å®Œæˆæ¨è¿›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºELOçš„è¯„çº§ç³»ç»ŸæŠ—å™ªå£°  
ä¸åŒäºå®Œå…¨ä¿¡ä»»VLMåé¦ˆçš„å…ˆå‰å·¥ä½œï¼ŒGoalLadderé‡‡ç”¨åŸºäºELOçš„è¯„çº§ç³»ç»Ÿå¯¹æ½œåœ¨ç›®æ ‡çŠ¶æ€æ’åºã€‚è¯¥æ–¹å¼é™ä½äº†VLMå™ªå£°åé¦ˆçš„ä¸åˆ©å½±å“ï¼Œåœ¨åå¤æ¯”è¾ƒå°éƒ¨åˆ†è§†è§‰è§‚æµ‹è¿‡ç¨‹ä¸­ï¼ŒæŒç»­ä¼˜åŒ–å¯¹çŠ¶æ€æ•ˆç”¨çš„ä¼°è®¡ï¼Œè®©åé¦ˆåˆ©ç”¨æ›´é²æ£’ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ— ç›‘ç£è§†è§‰åµŒå…¥ç©ºé—´å®šä¹‰å¥–åŠ±  
è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“çš„ä»»åŠ¡æ˜¯åœ¨ä¸€ä¸ªåŸºäºæ— æ ‡ç­¾è§†è§‰æ•°æ®è®­ç»ƒçš„å­¦ä¹ åˆ°çš„åµŒå…¥ç©ºé—´ä¸­ï¼Œæœ€å°åŒ–ä¸æ’åæœ€é«˜ç›®æ ‡çš„è·ç¦»ã€‚è¿™ä¸€ç‰¹æ€§ç»•å¼€äº†è®­ç»ƒå½¢çŠ¶è‰¯å¥½å¥–åŠ±å‡½æ•°é€šå¸¸æ‰€éœ€çš„å¤§é‡ç²¾ç¡®åé¦ˆï¼Œå®ç°å¥–åŠ±å¯¹æœªè§è¿‡çŠ¶æ€çš„æ³›åŒ–ï¼Œå‡å°‘VLMæŸ¥è¯¢æ¬¡æ•°ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç»å…¸æ§åˆ¶å’Œæœºå™¨äººæ“ä½œç¯å¢ƒä¸­ï¼ŒGoalLadderæ˜¾è‘—è¶…è¶Šç°æœ‰ç›¸å…³æ–¹æ³•ã€‚å¹³å‡æœ€ç»ˆæˆåŠŸç‡çº¦95%ï¼Œè€Œæœ€ä½³ç«å“ä»…çº¦45% ã€‚ç”šè‡³åœ¨ä¸èƒ½è·å–çœŸå®å¥–åŠ±çš„â€œ oracle æ™ºèƒ½ä½“â€å¯¹æ¯”æ—¶ï¼ŒGoalLadderåœ¨æ‰€æœ‰æµ‹è¯•ä»»åŠ¡ä¸­å‡ ä¹è¿½å¹³ï¼Œä¸”åœ¨ä¸€é¡¹ä»»åŠ¡ä¸Šå¤§å¹…è¶…è¶Šã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åº”å¯¹å™ªå£°åé¦ˆæ€è·¯ï¼šé‡‡ç”¨ELOè¯„çº§ç³»ç»Ÿå¤„ç†æ¨¡å‹åé¦ˆå™ªå£°ï¼Œä¸ºå…¶ä»–ä¾èµ–æ¨¡å‹åé¦ˆçš„ä»»åŠ¡ï¼ˆå¦‚å¤šæ¨¡æ€å­¦ä¹ ã€äººæœºäº¤äº’åé¦ˆå¤„ç†ç­‰ï¼‰æä¾›äº†æŠ—å™ªå£°è®¾è®¡å‚è€ƒã€‚  
2. æ— ç›‘ç£åµŒå…¥ç©ºé—´ç”¨å¥–åŠ±ï¼šåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®è®­ç»ƒåµŒå…¥ç©ºé—´æ¥å®šä¹‰å¥–åŠ±å®ç°æ³›åŒ–ï¼Œå¯å‘åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹ï¼Œå¦‚ä½•å€ŸåŠ©æ— ç›‘ç£æˆ–è‡ªç›‘ç£å­¦ä¹ æ‰‹æ®µä¼˜åŒ–å¼ºåŒ–å­¦ä¹ å¥–åŠ±è®¾è®¡ã€‚  
3. å¢é‡å¼ç›®æ ‡å‘ç°ï¼šé’ˆå¯¹éœ€é€æ­¥æ¨è¿›çš„å¤æ‚ä»»åŠ¡ï¼Œè¿™ç§å¢é‡å‘ç°æ›´ä¼˜çŠ¶æ€çš„é€»è¾‘ï¼Œå¯è¿ç§»åˆ°æœºå™¨äººé•¿æœŸä»»åŠ¡è§„åˆ’ã€å¤šé˜¶æ®µç›®æ ‡è¾¾æˆç±»ä»»åŠ¡ä¸­ï¼Œæå‡ä»»åŠ¡åˆ†è§£ä¸æ¨è¿›æ•ˆç‡ã€‚

## dual-objective-reinforcement-learning-with-novel-hamilton-jacobi-bellman-formulations
### Abstract
Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒç›®æ ‡å¼ºåŒ–å­¦ä¹ æ–°çªç ´ï¼šåŸºäºå“ˆå¯†é¡¿ - é›…å¯æ¯” - è´å°”æ›¼æ–¹ç¨‹çš„åˆ›æ–°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œç¡¬çº¦æŸæ— è®ºæ˜¯é€šè¿‡å¥–åŠ±å‡½æ•°è¿˜æ˜¯æ¨¡å‹æ¶æ„æ–½åŠ ï¼Œå¾€å¾€ä¼šé™ä½ç­–ç•¥æ€§èƒ½ã€‚æ‹‰æ ¼æœ—æ—¥æ–¹æ³•è™½èƒ½èåˆç›®æ ‡ä¸çº¦æŸï¼Œä½†å¸¸éœ€å¤æ‚çš„å¥–åŠ±å·¥ç¨‹å’Œå‚æ•°è°ƒä¼˜ã€‚åŒæ—¶ï¼Œå¤šç›®æ ‡RLã€çº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆCMDPï¼‰ã€æ—¶æ€é€»è¾‘RLç­‰ç°æœ‰æ–¹æ³•å­˜åœ¨è¯¸å¦‚éœ€å¤æ‚å¥–åŠ±è®¾è®¡ã€éš¾ä¿è¯ä»£ç†ä¸åŸé—®é¢˜å…³ç³»ç­‰ä¸è¶³ã€‚æ­¤å¤–ï¼Œåœ¨å¤„ç†æ¶‰åŠå¹³è¡¡ä¸¤ä¸ªç›®æ ‡çš„ Reach - Always - Avoidï¼ˆéœ€è¾¾æˆä¸åŒå¥–åŠ±å’Œæƒ©ç½šé˜ˆå€¼ï¼‰å’Œ Reach - Reachï¼ˆéœ€è¾¾æˆä¸¤ä¸ªä¸åŒå¥–åŠ±é˜ˆå€¼ï¼‰é—®é¢˜æ—¶ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆæ‰‹æ®µã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨åŸºäºå“ˆå¯†é¡¿ - é›…å¯æ¯”ï¼ˆHJï¼‰æ–¹ç¨‹ä¸RLçš„è”ç³»ï¼Œä¸ºåŒç›®æ ‡æ»¡è¶³é—®é¢˜æå‡ºæ–°ä»·å€¼å‡½æ•°ï¼Œä»¥æ›´é«˜æ•ˆè§£å†³è¿™ç±»çº¦æŸå†³ç­–é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰åŒç›®æ ‡é—®é¢˜å¹¶åˆ†è§£æ±‚è§£
    - æ˜ç¡® Reach - Always - Avoidï¼ˆRAAï¼‰å’Œ Reach - Reachï¼ˆRRï¼‰é—®é¢˜ã€‚RAAè¦å¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–å’Œæƒ©ç½šæœ€å°åŒ–ï¼ŒRRè¦å¹³è¡¡ä¸¤ä¸ªä¸åŒå¥–åŠ±çš„æœ€å¤§åŒ–ï¼Œä¸”ç›®æ ‡ç»“åˆæ–¹å¼ä¸ºæœ€åæƒ…å†µç¡®ä¿åŒæ»¡è¶³ï¼Œä¸ä¼ ç»ŸRLçš„â€œæŠ˜æ‰£å¥–åŠ±å’Œâ€ç›®æ ‡ä¸åŒã€‚
    - å¯¹RAAï¼Œå°†å…¶åˆ†è§£ä¸ºé¿å…é—®é¢˜å’Œ Reach - Avoid é—®é¢˜ã€‚å…ˆæ±‚è§£ä¸è´Ÿæƒ©ç½šç›¸å…³çš„é¿å…é—®é¢˜å¾—åˆ°æœ€ä¼˜ä»·å€¼å‡½æ•° \( V^*_A(s) \)ï¼Œå†ç”¨ä¿®æ”¹åçš„å¥–åŠ± \( r_{RAA}(s) = \min\{r(s), V^*_A(s)\} \) æ±‚è§£ Reach - Avoid é—®é¢˜ï¼Œè¿›è€Œå¾—åˆ°RAAçš„æœ€ä¼˜ä»·å€¼å‡½æ•°æ»¡è¶³çš„è´å°”æ›¼æ–¹ç¨‹ \( V^*_{RAA}(s)=\min\left\{\max\left\{\max_{a\in A}V^*_{RAA}(f(s,a)), r_{RAA}(s)\right\}, q(s)\right\} \) ã€‚
    - å¯¹RRï¼Œé€šè¿‡å°†ç³»ç»ŸçŠ¶æ€å¢å¹¿ï¼ˆåŠ å…¥è·Ÿè¸ªä¸¤ä¸ªå¥–åŠ±å†å²æœ€ä¼˜å€¼çš„è¾…åŠ©å˜é‡ï¼‰ï¼ŒæŠŠé—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå¯è¾¾æ€§é—®é¢˜æ¥æ±‚è§£ï¼ˆè®ºæ–‡è™½æœªå®Œå…¨å±•å¼€ï¼Œä½†æå‡ºäº†åˆ†è§£æ€è·¯ï¼‰ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçŠ¶æ€å¢å¹¿åº”å¯¹å†å²ä¾èµ–
    - å¯¹äºRAAå’ŒRRé—®é¢˜ï¼Œä»…ä¾èµ–å½“å‰çŠ¶æ€çš„ç­–ç•¥å­˜åœ¨ç¼ºé™·ï¼ˆå¦‚RRä¸­åŸºäºå½“å‰çŠ¶æ€çš„ç¡®å®šæ€§ç­–ç•¥å¯èƒ½åªèƒ½è¾¾æˆä¸€ä¸ªç›®æ ‡ï¼ŒRAAä¸­å½“å‰çŠ¶æ€æœ€ä¼˜å†³ç­–ä¾èµ–è½¨è¿¹å†å²ï¼‰ã€‚å› æ­¤ï¼Œå¯¹MDPè¿›è¡ŒçŠ¶æ€å¢å¹¿ï¼Œå¼•å…¥è¾…åŠ©å˜é‡è·Ÿè¸ªè½¨è¿¹å†å²ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚RAAä¸­è·Ÿè¸ªæœ€ä½³å¥–åŠ±å’Œæœ€å·®æƒ©ç½šå†å²ï¼ŒRRä¸­è·Ÿè¸ªä¸¤ä¸ªå¥–åŠ±çš„å†å²æœ€ä½³å€¼ï¼‰ï¼Œä½¿ç­–ç•¥èƒ½åˆ©ç”¨è½¨è¿¹å†å²ä¿¡æ¯ï¼Œä¸”ç†è®ºè¯æ˜è¿™ç§å¢å¹¿ä¸‹æœ€ä¼˜ç¡®å®šæ€§ç­–ç•¥å·²è¶³å¤Ÿï¼Œæ— éœ€é¢å¤–ä¿¡æ¯æˆ–éšæœºç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºDO - HJ - PPOç®—æ³•
åŸºäºå¯¹RAAå’ŒRRé—®é¢˜çš„åˆ†æï¼Œæå‡ºProximal Policy Optimizationçš„å˜ä½“DO - HJ - PPOæ¥è§£å†³è¿™ç±»åŒç›®æ ‡çº¦æŸå†³ç­–é—®é¢˜ï¼Œåˆ©ç”¨æ–°æ¨å¯¼çš„ä»·å€¼å‡½æ•°å’Œè´å°”æ›¼æ–¹ç¨‹æŒ‡å¯¼ç­–ç•¥å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å®‰å…¨åˆ°è¾¾å’Œå¤šç›®æ ‡è¾¾æˆç­‰ä¸€ç³»åˆ—ä»»åŠ¡ä¸­éªŒè¯DO - HJ - PPOã€‚ç»“æœè¡¨æ˜ï¼ŒDO - HJ - PPOäº§ç”Ÿäº†ä¸å…ˆå‰æ–¹æ³•å®šæ€§ä¸åŒçš„è¡Œä¸ºï¼Œåœ¨å„ç§æŒ‡æ ‡ä¸Šè¶…è¶Šäº†å¤šä¸ªåŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³åŒç›®æ ‡çº¦æŸå†³ç­–é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- é—®é¢˜åˆ†è§£æ€è·¯ï¼šå°†å¤æ‚çš„åŒç›®æ ‡çº¦æŸé—®é¢˜åˆ†è§£ä¸ºæ›´æ˜“å¤„ç†çš„å­é—®é¢˜ï¼ˆå¦‚RAAåˆ†è§£ä¸ºé¿å…å’ŒReach - Avoidé—®é¢˜ï¼‰ï¼Œä¸ºè§£å†³å¤šç›®æ ‡ã€å¸¦çº¦æŸçš„å¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†æ¨¡å—åŒ–çš„æ€è€ƒæ–¹å¼ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–å¤æ‚ç›®æ ‡ç»„åˆçš„RLä»»åŠ¡ä¸­ã€‚
- çŠ¶æ€å¢å¹¿æ–¹æ³•ï¼šå½“ç­–ç•¥å†³ç­–éœ€è½¨è¿¹å†å²ä¿¡æ¯æ—¶ï¼Œé€šè¿‡åˆç†å¢å¹¿çŠ¶æ€å¼•å…¥è¾…åŠ©å˜é‡è·Ÿè¸ªå…³é”®å†å²ä¿¡æ¯ï¼Œä¸ºå¤„ç†å†å²ä¾èµ–å‹RLé—®é¢˜æä¾›äº†æœ‰æ•ˆæŠ€æœ¯æ‰‹æ®µã€‚
- ç®—æ³•åˆ›æ–°ï¼šåŸºäºHJæ–¹ç¨‹ä¸RLè”ç³»æå‡ºçš„DO - HJ - PPOç®—æ³•ï¼Œä¸ºè§£å†³åŒç›®æ ‡çº¦æŸRLé—®é¢˜æä¾›äº†æ–°çš„ç®—æ³•èŒƒå¼ï¼Œå…¶åœ¨å®éªŒä¸­å±•ç°çš„ä¼˜åŠ¿å¯¹åç»­ç›¸å…³ç®—æ³•è®¾è®¡æœ‰å‚è€ƒä»·å€¼ï¼Œå¯å¯å‘ç ”ç©¶è€…é’ˆå¯¹ä¸åŒçº¦æŸå’Œç›®æ ‡ç»„åˆè®¾è®¡æ›´é«˜æ•ˆçš„RLç®—æ³•ã€‚

## design-of-an-all-facet-illuminator-for-high-na-euv-lithography-exposure-tool-based-on-deep-reinforcement-learning
### Abstract
Using the illuminator for high numerical aperture (NA) extreme ultraviolet
(EUV) exposure tool in EUV lithography can lead to support volume production of
sub-2 nm logic nodes and leading-edge DRAM nodes. However, the typical design
method of the illuminator has issues with the transmission owing to the
limitation of optical structure that cannot further reduce process parameter
k1, and uniformity due to the restriction of matching method that can only
consider one factor affecting uniformity. The all-facet illuminator can improve
transmission by removing relay system. Deep reinforcement learning (RL) can
improve the uniformity by considering multiple factors. In this paper, a design
method of the all-facet illuminator for high NA EUV lithography exposure tool
and a matching method based on deep RL for the double facets are proposed. The
all-facet illuminator is designed using matrix optics, and removing relay
system to achieve high transmission. The double facets is matched using the
deep RL framework, which includes the policy network with improved trainability
and low computational demands, and the reward function with great optimization
direction and fast convergence rate, enabling to rapidly generate multiple
matching results with high uniformity. An all-facet illuminator for a 0.55 NA
EUV lithography exposure tool is designed by the proposed method. Simulation
results indicate that the transmission is greater than 35%, and uniformity
exceed 99% under multiple illumination pupil shapes.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é«˜NAæç´«å¤–å…‰åˆ»æ›å…‰æœºå…¨åå°„é¢ç…§æ˜å™¨è®¾è®¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æç´«å¤–ï¼ˆEUVï¼‰å…‰åˆ»æ›å…‰æœºæ˜¯åŠå¯¼ä½“åˆ¶é€ çš„å…³é”®æ¨¡å—ï¼Œé«˜æ•°å€¼å­”å¾„ï¼ˆNAï¼‰çš„EUVå…‰åˆ»æ›å…‰æœºå¯æ”¯æ’‘äºš2nmé€»è¾‘èŠ‚ç‚¹å’Œå‰æ²¿DRAMèŠ‚ç‚¹çš„é‡äº§ã€‚ä½†ä¼ ç»Ÿç…§æ˜å™¨è®¾è®¡å­˜åœ¨é—®é¢˜ï¼šå…‰å­¦ç»“æ„é™åˆ¶å¯¼è‡´é€å°„ç‡ä½ï¼Œæ— æ³•è¿›ä¸€æ­¥é™ä½å·¥è‰ºå‚æ•°kâ‚ï¼›åŒ¹é…æ–¹æ³•ä»…è€ƒè™‘å•ä¸€å½±å“å‡åŒ€æ€§çš„å› ç´ ï¼Œå‡åŒ€æ€§ä¸ä½³ã€‚åŒæ—¶ï¼Œç°æœ‰ç…§æ˜å™¨åœ¨æå‡é€å°„ç‡å’Œå‡åŒ€æ€§æ–¹é¢è¿˜å­˜åœ¨åˆ¶é€ éš¾åº¦å¤§ã€åŒ¹é…ç»“æœå•ä¸€ç­‰ä¸è¶³ï¼Œå› æ­¤è®¾è®¡é«˜é€å°„ç‡ä¸”èƒ½ä¼˜åŒ–å‡åŒ€æ€§çš„ç…§æ˜å™¨ååˆ†å…³é”®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨åå°„é¢ç…§æ˜å™¨è®¾è®¡æ–¹æ³•  
åˆ©ç”¨çŸ©é˜µå…‰å­¦å…ˆè®¡ç®—å…±è½´åˆå§‹ç»“æ„ï¼ˆåŸºäºå®ç°ç§‘å‹’ç…§æ˜çš„ä¸¤ä¸ªå…‰å­¦å…±è½­å…³ç³»ï¼‰ï¼Œå†é€šè¿‡å€¾æ–œå’Œåå¿ƒå¾—åˆ°ç¦»è½´ç»“æ„ã€‚è¯¥ç…§æ˜å™¨å»é™¤ä¸­ç»§ç³»ç»Ÿï¼Œä»¥æ­¤å®ç°é«˜é€å°„ç‡ï¼Œä¸ºé™ä½kâ‚ã€æå‡åˆ†è¾¨ç‡æä¾›å¯èƒ½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åŒåå°„é¢åŒ¹é…æ–¹æ³•  
æ„å»ºæ·±åº¦RLæ¡†æ¶ç”¨äºåŒåå°„é¢ï¼ˆåŒ…å«åœºåå°„é¢å’Œå…‰ç³åå°„é¢ï¼‰åŒ¹é…ã€‚æ¡†æ¶ä¸­ç­–ç•¥ç½‘ç»œç»“åˆéšæœºæ¨¡å—ä¸æ·±åº¦å­¦ä¹ æ¨¡å—ï¼Œæå‡å¯è®­ç»ƒæ€§ä¸”è®¡ç®—é‡ä½ï¼›å¥–åŠ±å‡½æ•°çº³å…¥å¤šä¸ªå½±å“å‡åŒ€æ€§çš„å› ç´ ï¼Œä¼˜åŒ–æ–¹å‘æ˜ç¡®ã€æ”¶æ•›é€Ÿåº¦å¿«ã€‚è¯¥æ–¹æ³•èƒ½è€ƒè™‘å¤šå› ç´ å½±å“å‡åŒ€æ€§é—®é¢˜ï¼Œå¿«é€Ÿç”Ÿæˆå¤šä¸ªé«˜å‡åŒ€æ€§çš„åŒ¹é…ç»“æœã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
é‡‡ç”¨æ‰€ææ–¹æ³•è®¾è®¡äº†é€‚ç”¨äº0.55NA EUVå…‰åˆ»æ›å…‰æœºçš„å…¨åå°„é¢ç…§æ˜å™¨ã€‚ä»¿çœŸç»“æœæ˜¾ç¤ºï¼Œè¯¥ç…§æ˜å™¨é€å°„ç‡å¤§äº35%ï¼ˆå¦‚è¾¾åˆ°35.32%ï¼Œæ¯”å…¶ä»–EUVå…‰åˆ»æ›å…‰æœºç…§æ˜å™¨è‡³å°‘é«˜39%ï¼‰ï¼›åœ¨å¤šç§ç…§æ˜å…‰ç³å½¢çŠ¶ä¸‹ï¼Œå‡åŒ€æ€§è¶…è¿‡99%ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨å…‰å­¦è®¾è®¡é¢†åŸŸï¼Œå°†çŸ©é˜µå…‰å­¦ç”¨äºç‰¹æ®Šç»“æ„ï¼ˆå…¨åå°„é¢ï¼‰ç…§æ˜å™¨çš„åˆå§‹è®¾è®¡ï¼Œä¸ºå¤æ‚å…‰å­¦ç³»ç»Ÿçš„ç»“æ„æ­å»ºæä¾›äº†ä»ç†è®ºåˆ°å®è·µçš„æ€è·¯ï¼›æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸å…‰å­¦åŒ¹é…é—®é¢˜çš„ç»“åˆï¼Œå±•ç¤ºäº†AIæŠ€æœ¯åœ¨ä¼ ç»Ÿå…‰å­¦å·¥ç¨‹ä¸­è§£å†³å¤šå› ç´ ä¼˜åŒ–ã€å¤šç»“æœç”Ÿæˆç­‰éš¾é¢˜çš„æ½œåŠ›ï¼Œä¸ºåç»­å…‰å­¦ç³»ç»Ÿä¸­æ¶‰åŠå¤šå˜é‡ã€å¤šç›®æ ‡ä¼˜åŒ–çš„ä»»åŠ¡æä¾›äº†AIèµ‹èƒ½çš„èŒƒä¾‹ï¼Œå¯å‘ç ”ç©¶è€…åœ¨å…‰å­¦è®¾è®¡ã€åˆ¶é€ ç­‰ç¯èŠ‚æ›´å¤šåœ°æ¢ç´¢AIæŠ€æœ¯çš„èåˆåº”ç”¨ã€‚

## booster-gym--an-end-to-end-reinforcement-learning-framework-for-humanoid-robot-locomotion
### Abstract
Recent advancements in reinforcement learning (RL) have led to significant
progress in humanoid robot locomotion, simplifying the design and training of
motion policies in simulation. However, the numerous implementation details
make transferring these policies to real-world robots a challenging task. To
address this, we have developed a comprehensive code framework that covers the
entire process from training to deployment, incorporating common RL training
methods, domain randomization, reward function design, and solutions for
handling parallel structures. This library is made available as a community
resource, with detailed descriptions of its design and experimental results. We
validate the framework on the Booster T1 robot, demonstrating that the trained
policies seamlessly transfer to the physical platform, enabling capabilities
such as omnidirectional walking, disturbance resistance, and terrain
adaptability. We hope this work provides a convenient tool for the robotics
community, accelerating the development of humanoid robots. The code can be
found in https://github.com/BoosterRobotics/booster_gym.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Booster Gymï¼šåŠ©åŠ›äººå½¢æœºå™¨äºº locomotion çš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œè®©ä»¿çœŸç¯å¢ƒä¸‹è¿åŠ¨ç­–ç•¥çš„è®¾è®¡ä¸è®­ç»ƒæ›´ç®€ä¾¿ã€‚ä½†ä»ä»¿çœŸåˆ°çœŸå®ä¸–ç•Œçš„ç­–ç•¥è¿ç§»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œåƒæœºå™¨äººåŠ¨åŠ›å­¦å¤æ‚ã€æ„ŸçŸ¥å™ªå£°ã€ç¡¬ä»¶é™åˆ¶ç­‰ï¼Œå¤§é‡å®ç°ç»†èŠ‚ä½¿ç­–ç•¥è½åœ°å›°éš¾ã€‚ä¸ºè§£å†³è¯¥é—®é¢˜ï¼Œå›¢é˜Ÿå¼€å‘äº†è¦†ç›–ä»è®­ç»ƒåˆ°éƒ¨ç½²å…¨æµç¨‹çš„ä»£ç æ¡†æ¶ Booster Gymï¼ŒåŠ©åŠ›äººå½¢æœºå™¨äººç ”å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆ  
æä¾›ä»ä»¿çœŸè®­ç»ƒåˆ°çœŸå®ä¸–ç•Œéƒ¨ç½²çš„å®Œæ•´ç«¯åˆ°ç«¯æ–¹æ¡ˆï¼Œæ¶µç›–æ•´ä¸ªæµç¨‹ï¼Œè®©åŸºäº RL çš„äººå½¢æœºå™¨äººè¿åŠ¨ç­–ç•¥è®­ç»ƒä¸éƒ¨ç½²æœ‰äº†å…¨é¢æ”¯æ’‘ï¼Œèƒ½æ›´é¡ºç•…åœ°å®ç°ä»è™šæ‹Ÿåˆ°ç‰©ç†å¹³å°çš„è¿‡æ¸¡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢åŸŸéšæœºåŒ–  
é’ˆå¯¹ç¯å¢ƒã€æœºå™¨äººå’Œæ‰§è¡Œå™¨å¼€å±•å…¨é¢çš„åŸŸéšæœºåŒ–ï¼Œå‡å°‘ä»¿çœŸåˆ°çœŸå®ä¸–ç•Œçš„å·®è·ï¼Œæå‡è®­ç»ƒåç­–ç•¥åœ¨å®ä½“æœºå™¨äººä¸Šéƒ¨ç½²æ—¶çš„é²æ£’æ€§ï¼Œè®©ç­–ç•¥åœ¨ä¸åŒçœŸå®åœºæ™¯ä¸‹ä¹Ÿèƒ½ç¨³å®šå‘æŒ¥ä½œç”¨ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ˜“æ‰©å±•çš„æ¥å£è®¾è®¡  
æ‰“é€ æ˜“äºä¿®æ”¹çš„ç¯å¢ƒå’Œç®—æ³•æ¥å£ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜æ ¹æ®ä¸åŒä»»åŠ¡é«˜æ•ˆè°ƒæ•´å¥–åŠ±å‡½æ•°ã€ç½‘ç»œæ¶æ„å’Œç‰©ç†å‚æ•°ç­‰ï¼Œé™ä½ç ”ç©¶äººå‘˜å®šåˆ¶å¼€å‘çš„é—¨æ§›ï¼Œæå‡ç ”ç©¶çµæ´»æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ Booster T1 äººå½¢æœºå™¨äººä¸ŠéªŒè¯æ¡†æ¶æœ‰æ•ˆæ€§ï¼Œè®­ç»ƒåçš„ç­–ç•¥èƒ½æ— ç¼è¿ç§»åˆ°ç‰©ç†å¹³å°ï¼Œå®ç°å…¨å‘è¡Œèµ°ã€æŠ—å¹²æ‰°ã€åœ°å½¢é€‚åº”ç­‰èƒ½åŠ›ï¼›è¿˜åœ¨ Isaac Gym è®­ç»ƒã€MuJoco è·¨ä»¿çœŸæµ‹è¯•ã€Webots éªŒè¯ä»¥åŠçœŸå®ä¸–ç•Œéƒ¨ç½²ç­‰å¤šç¯èŠ‚éªŒè¯ï¼Œå±•ç°æ¡†æ¶åœ¨å¤šç¯å¢ƒä¸‹çš„å®ç”¨æ€§ä¸æ³›åŒ–æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºæœºå™¨äººé¢†åŸŸç ”ç©¶è€…ï¼Œè¯¥æ¡†æ¶æ˜¯å¼€æºå¯ç”¨çš„å·¥å…·ï¼Œèƒ½åŠ é€Ÿäººå½¢æœºå™¨äººç ”å‘è¿›ç¨‹ï¼›å…¶ç«¯åˆ°ç«¯æ€è·¯ã€åŸŸéšæœºåŒ–æ–¹æ³•ã€æ˜“æ‰©å±•æ¥å£è®¾è®¡ç­‰ï¼Œä¸ºè§£å†³ä»¿çœŸåˆ°çœŸå®ä¸–ç•Œè¿ç§»éš¾é¢˜ã€æå‡ç­–ç•¥é²æ£’æ€§ã€çµæ´»é€‚é…ä¸åŒä»»åŠ¡ç­‰æä¾›äº†ä¼˜ç§€èŒƒä¾‹ï¼Œåœ¨æœºå™¨äººå¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸å·¥ç¨‹å®è·µä¸­éƒ½æœ‰è¯¸å¤šå¯å‚è€ƒå€Ÿé‰´çš„åœ°æ–¹ï¼Œæ¯”å¦‚åŸŸéšæœºåŒ–åœ¨ç¼©å°ä»¿çœŸç°å®å·®è·çš„åº”ç”¨ã€æ¥å£è®¾è®¡å¯¹ç ”ç©¶æ•ˆç‡çš„æå‡ç­‰æ€è·¯ï¼Œéƒ½å€¼å¾—ç›¸å…³é¢†åŸŸå¼€å‘è€…å­¦ä¹ ã€‚

## intellilung--advancing-safe-mechanical-ventilation-using-offline-rl-with-hybrid-actions-and-clinically-aligned-rewards
### Abstract
Invasive mechanical ventilation (MV) is a life-sustaining therapy for
critically ill patients in the intensive care unit (ICU). However, optimizing
its settings remains a complex and error-prone process due to patient-specific
variability. While Offline Reinforcement Learning (RL) shows promise for MV
control, current stateof-the-art (SOTA) methods struggle with the hybrid
(continuous and discrete) nature of MV actions. Discretizing the action space
limits available actions due to exponential growth in combinations and
introduces distribution shifts that can compromise safety. In this paper, we
propose optimizations that build upon prior work in action space reduction to
address the challenges of discrete action spaces. We also adapt SOTA offline RL
algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby
avoiding the pitfalls of discretization. Additionally, we introduce a
clinically grounded reward function based on ventilator-free days and
physiological targets, which provides a more meaningful optimization objective
compared to traditional sparse mortality-based rewards. Our findings
demonstrate that AI-assisted MV optimization may enhance patient safety and
enable individualized lung support, representing a significant advancement
toward intelligent, data-driven critical care solutions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | IntelliLungï¼šç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ +æ··åˆåŠ¨ä½œ+ä¸´åºŠå¥–åŠ±æ¨è¿›å®‰å…¨æœºæ¢°é€šæ°”

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æœ‰åˆ›æœºæ¢°é€šæ°”ï¼ˆMVï¼‰æ˜¯é‡ç—‡ç›‘æŠ¤ç—…æˆ¿ï¼ˆICUï¼‰ä¸­å±é‡ç—‡æ‚£è€…çš„ç”Ÿå‘½ç»´æŒç–—æ³•ï¼Œåœ¨æ–°å† ç–«æƒ…æœŸé—´å…¶å…³é”®ä½œç”¨æ„ˆå‘å‡¸æ˜¾ã€‚ç„¶è€Œï¼Œå› æ‚£è€…ä¸ªä½“å·®å¼‚å¤§ï¼Œä¼˜åŒ–é€šæ°”è®¾ç½®æ˜¯å¤æ‚ä¸”æ˜“å‡ºé”™çš„è¿‡ç¨‹ï¼Œè¿˜å­˜åœ¨å¼•å‘å‘¼å¸æœºè¯±å¯¼è‚ºæŸä¼¤ï¼ˆVILIï¼‰é£é™©ï¼›åŒæ—¶ä¸´åºŠMVä¿æŠ¤åè®®æ‰§è¡Œåº¦å·®ã€é«˜æŠ¤å£« - æ‚£è€…æ¯”ä¸‹æ˜“å‡ºç°æ¢å¤ä¸ä½³ç­‰é—®é¢˜ã€‚åŸºäºAIçš„å†³ç­–æ”¯æŒç³»ç»Ÿï¼ˆAI - DSSï¼‰å¯åº”å¯¹æŒ‘æˆ˜ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†MVæ··åˆï¼ˆè¿ç»­ + ç¦»æ•£ï¼‰åŠ¨ä½œç©ºé—´æ—¶å­˜åœ¨ç¼ºé™·ï¼šç¦»æ•£åŒ–åŠ¨ä½œç©ºé—´ä¼šå› ç»„åˆçˆ†ç‚¸é™åˆ¶å¯é€‰åŠ¨ä½œã€å¼•å…¥åˆ†å¸ƒåç§»å½±å“å®‰å…¨æ€§ï¼›ä¸”ä¼ ç»ŸåŸºäºæ­»äº¡ç‡çš„ç¨€ç–å¥–åŠ±å¯¹è¯„ä¼°MVå¹²é¢„ä¸å¤Ÿå¯é ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨ç”¨ç¦»çº¿RLå¼€å‘IntelliLungè¿™ä¸€AI - DSSæ¥è§£å†³ä¸Šè¿°MVä¼˜åŒ–éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸´åºŠå¯¹é½çš„å¥–åŠ±å‡½æ•°è®¾è®¡
ä»¥å¾€MVä¼˜åŒ–å¤šåŸºäºæ­»äº¡ç‡çš„ç¨€ç–å¥–åŠ±ï¼Œè€ŒåŒ»å­¦ç ”ç©¶è¡¨æ˜æ­»äº¡ç‡ä¸æ˜¯è¯„ä¼°MVå¹²é¢„çš„å¯é ç»ˆç‚¹ã€‚æœ¬æ–‡å¼•å…¥åŸºäºæ— å‘¼å¸æœºå¤©æ•°ï¼ˆVFDï¼‰å’Œç”Ÿç†å‚æ•°èŒƒå›´çš„å¥–åŠ±å‡½æ•°ï¼Œèƒ½æ›´å¥½å¥‘åˆå‡å°‘VILIçš„åŒ»ç–—ç›®æ ‡ï¼ŒåŒæ—¶å¹³è¡¡ä¸¤æ–¹é¢å› ç´ ä½œç”¨ï¼Œç›¸æ¯”ä¼ ç»Ÿå¥–åŠ±æ›´å…·æ„ä¹‰ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨ä½œç©ºé—´ä¼˜åŒ–ç­–ç•¥
ä¹‹å‰ç ”ç©¶å› åŠ¨ä½œç©ºé—´éšç»´åº¦æŒ‡æ•°å¢é•¿å¸¸é™åˆ¶ç¦»æ•£åŠ¨ä½œæ•°é‡ï¼Œæœ¬æ–‡å±•ç¤ºäº†ä¸€ç§ç®€åŒ–åŠ¨ä½œç©ºé—´çš„æ–¹æ³•ï¼Œå¹¶ç»“åˆå…ˆå‰ç ”ç©¶çš„ä¼˜åŒ–æ‰‹æ®µï¼Œåœ¨å¢åŠ åŠ¨ä½œæ•°é‡åŒæ—¶æå‡å®‰å…¨æ€§ï¼Œè§£å†³ç¦»æ•£åŠ¨ä½œç©ºé—´æŒ‘æˆ˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé€‚é…æ··åˆåŠ¨ä½œç©ºé—´çš„ç¦»çº¿RLç®—æ³•
MVå­˜åœ¨è¿ç»­å’Œç¦»æ•£è®¾ç½®ï¼ˆåŠ¨ä½œï¼‰ï¼Œç¦»æ•£åŒ–è¿ç»­åŠ¨ä½œæœ‰è¯¸å¤šå¼Šç«¯ã€‚æœ¬æ–‡é€‚é…SOTAç¦»çº¿RLç®—æ³•ï¼ˆIQLå’ŒEDACï¼‰ï¼Œä½¿å…¶ç›´æ¥åœ¨æ··åˆåŠ¨ä½œç©ºé—´è¿è¡Œï¼Œé¿å…ç¦»æ•£åŒ–é™·é˜±ï¼›è¿˜æŒ‡å‡ºä»¥å¾€ç¦»æ•£åŒ–åå†é‡å»ºè¿ç»­å€¼ä¼šå¼•å…¥åˆ†å¸ƒåç§»å’Œä¸å®‰å…¨ç­–ç•¥ï¼Œè€Œè¯¥æ–¹æ³•èƒ½è§„é¿æ­¤é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªè¯¦ç»†å±•å¼€å®éªŒç»“æœæ•°å€¼ç­‰å†…å®¹ï¼Œä½†ä»æ•´ä½“è®ºè¿°å¯çŸ¥ï¼Œé€šè¿‡æ–°å¥–åŠ±å‡½æ•°ã€åŠ¨ä½œç©ºé—´ä¼˜åŒ–ã€é€‚é…æ··åˆåŠ¨ä½œç©ºé—´çš„ç®—æ³•ç­‰åˆ›æ–°æ‰‹æ®µï¼Œè¯æ˜äº†AIè¾…åŠ©çš„MVä¼˜åŒ–æœ‰æœ›æå‡æ‚£è€…å®‰å…¨æ€§ã€å®ç°ä¸ªæ€§åŒ–è‚ºéƒ¨æ”¯æŒï¼Œå‘æ™ºèƒ½æ•°æ®é©±åŠ¨çš„é‡ç—‡ç›‘æŠ¤è§£å†³æ–¹æ¡ˆè¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¥–åŠ±å‡½æ•°è®¾è®¡å±‚é¢ï¼šåœ¨åŒ»ç–—ç­‰é¢†åŸŸï¼Œå½“ä¼ ç»Ÿå•ä¸€ç»ˆç‚¹æŒ‡æ ‡ï¼ˆå¦‚æ­»äº¡ç‡ï¼‰ä¸é€‚å®œæ—¶ï¼Œå¯å‚è€ƒæœ¬æ–‡ç»“åˆæ›´è´´åˆä¸´åºŠç›®æ ‡ã€å¤šç»´åº¦çš„æŒ‡æ ‡ï¼ˆå¦‚VFD + ç”Ÿç†å‚æ•°èŒƒå›´ï¼‰è®¾è®¡å¥–åŠ±ï¼Œè®©ä¼˜åŒ–ç›®æ ‡æ›´åˆç†ã€‚
2. åŠ¨ä½œç©ºé—´å¤„ç†å±‚é¢ï¼šé¢å¯¹æ··åˆåŠ¨ä½œç©ºé—´ä»»åŠ¡ï¼Œå¯å€Ÿé‰´æœ¬æ–‡å¯¹åŠ¨ä½œç©ºé—´ä¼˜åŒ–ã€é€‚é…ç®—æ³•åˆ°æ··åˆç©ºé—´çš„æ€è·¯ï¼Œé¿å…ç¦»æ•£åŒ–å¼Šç«¯ï¼Œå°¤å…¶åœ¨ç±»ä¼¼åŒ»ç–—è¿™ç§å¯¹å®‰å…¨æ€§è¦æ±‚é«˜çš„åœºæ™¯æå…·å‚è€ƒä»·å€¼ã€‚
3. è·¨é¢†åŸŸåä½œå±‚é¢ï¼šæœ¬æ–‡ç”±åŒ»ç–—å’ŒæŠ€æœ¯å¤šæ–¹åˆä½œï¼Œåœ¨ç¡®å®šé˜Ÿåˆ—ã€é—®é¢˜æ„å»ºç­‰ç¯èŠ‚ç´§å¯†ç»“åˆé¢†åŸŸä¸“å®¶çŸ¥è¯†ï¼Œè¿™ç§äº§å­¦ç ”åŒ»ç»“åˆæ¨¡å¼ä¸ºå¼€å‘å®ç”¨AIåŒ»ç–—ç³»ç»Ÿæä¾›äº†èŒƒä¾‹ï¼Œåç»­ç›¸å…³é¢†åŸŸé¡¹ç›®å¯å€Ÿé‰´è¯¥åä½œæ¨¡å¼ç¡®ä¿æ–¹æ¡ˆå®ç”¨æ€§ã€‚

