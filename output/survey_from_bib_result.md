# Paper List from BIB File: tmpze6z_azs.bib
- [25/03] **START: Self-taught Reasoner with Tools**  
[[Paper](http://arxiv.org/pdf/2503.04625v2)] [[Code/Page]()] [[TLDR/Notes](#start--self-taught-reasoner-with-tools)]

- [25/05] **Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16410v1)] [[Code/Page](https://github.com/dongguanting/Tool-Star.)] [[TLDR/Notes](#tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning)]

- [25/05] **Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging**  
[[Paper](http://arxiv.org/pdf/2505.09316v1)] [[Code/Page]()] [[TLDR/Notes](#scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging)]

- [25/04] **Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration**  
[[Paper](http://arxiv.org/pdf/2504.04915v1)] [[Code/Page](https://github.com/ritaranx/Collab-RAG/.)] [[TLDR/Notes](#collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration)]

- [25/05] **ZeroSearch: Incentivize the Search Capability of LLMs without Searching**  
[[Paper](http://arxiv.org/pdf/2505.04588v2)] [[Code/Page]()] [[TLDR/Notes](#zerosearch--incentivize-the-search-capability-of-llms-without-searching)]

- [25/04] **Reinforcement Learning for Reasoning in Large Language Models with One Training Example**  
[[Paper](http://arxiv.org/pdf/2504.20571v2)] [[Code/Page](https://github.com/ypwang61/One-Shot-RLVR.)] [[TLDR/Notes](#reinforcement-learning-for-reasoning-in-large-language-models-with-one-training-example)]

- [25/04] **DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments**  
[[Paper](http://arxiv.org/pdf/2504.03160v4)] [[Code/Page](https://github.com/GAIR-NLP/DeepResearcher.)] [[TLDR/Notes](#deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments)]

- [25/03] **ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.19470v2)] [[Code/Page]()] [[TLDR/Notes](#research--learning-to-reason-with-search-for-llms-via-reinforcement-learning)]

- [25/03] **SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild**  
[[Paper](http://arxiv.org/pdf/2503.18892v2)] [[Code/Page]()] [[TLDR/Notes](#simplerl-zoo--investigating-and-taming-zero-reinforcement-learning-for-open-base-models-in-the-wild)]

- [25/01] **Chain-of-Retrieval Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2501.14342v2)] [[Code/Page]()] [[TLDR/Notes](#chain-of-retrieval-augmented-generation)]

- [25/03] **RARE: Retrieval-Augmented Reasoning Modeling**  
[[Paper](http://arxiv.org/pdf/2503.23513v2)] [[Code/Page]()] [[TLDR/Notes](#rare--retrieval-augmented-reasoning-modeling)]

- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **Hybrid Latent Reasoning via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.18454v1)] [[Code/Page]()] [[TLDR/Notes](#hybrid-latent-reasoning-via-reinforcement-learning)]

- [25/04] **WebThinker: Empowering Large Reasoning Models with Deep Research Capability**  
[[Paper](http://arxiv.org/pdf/2504.21776v1)] [[Code/Page](https://github.com/RUC-NLPIR/WebThinker.)] [[TLDR/Notes](#webthinker--empowering-large-reasoning-models-with-deep-research-capability)]

- [24/10] **SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback**  
[[Paper](http://arxiv.org/pdf/2410.18141v2)] [[Code/Page]()] [[TLDR/Notes](#smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback)]

- [25/05] **Distilling LLM Agent into Small Models with Retrieval and Code Tools**  
[[Paper](http://arxiv.org/pdf/2505.17612v1)] [[Code/Page](https://github.com/Nardien/agent-distillation.)] [[TLDR/Notes](#distilling-llm-agent-into-small-models-with-retrieval-and-code-tools)]

- [25/03] **Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.09516v3)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning)]

- [25/05] **Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs**  
[[Paper](http://arxiv.org/pdf/2505.11277v3)] [[Code/Page]()] [[TLDR/Notes](#search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms)]

- [25/05] **Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.17266v2)] [[Code/Page]()] [[TLDR/Notes](#select2reason--efficient-instruction-tuning-data-selection-for-long-cot-reasoning)]

- [25/05] **StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.15107v2)] [[Code/Page](https://github.com/Zillwang/StepSearch.)] [[TLDR/Notes](#stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization)]

- [25/05] **Towards Effective Code-Integrated Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.24480v1)] [[Code/Page](https://github.com/RUCAIBox/CIR.)] [[TLDR/Notes](#towards-effective-code-integrated-reasoning)]

- [25/05] **An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents**  
[[Paper](http://arxiv.org/pdf/2505.15117v1)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents)]

- [25/06] **Computational Thinking Reasoning in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.02658v2)] [[Code/Page]()] [[TLDR/Notes](#computational-thinking-reasoning-in-large-language-models)]

- [25/05] **Diversity-Aware Policy Optimization for Large Language Model Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.23433v1)] [[Code/Page]()] [[TLDR/Notes](#diversity-aware-policy-optimization-for-large-language-model-reasoning)]

- [25/05] **The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.24630v1)] [[Code/Page]()] [[TLDR/Notes](#the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models)]

- [25/02] **RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2502.13957v2)] [[Code/Page](https://rag-gym.github.io.)] [[TLDR/Notes](#rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation)]

- [25/06] **From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?**  
[[Paper](http://arxiv.org/pdf/2506.08295v1)] [[Code/Page](https://github.com/tmlr-group/AR-Bench.)] [[TLDR/Notes](#from-passive-to-active-reasoning--can-large-language-models-ask-the-right-questions-under-incomplete-information-)]

- [25/06] **SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning**  
[[Paper](http://arxiv.org/pdf/2506.09016v2)] [[Code/Page]()] [[TLDR/Notes](#speed-rl--faster-training-of-reasoning-models-via-online-curriculum-learning)]

- [25/06] **TreeRPO: Tree Relative Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2506.05183v1)] [[Code/Page](https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.)] [[TLDR/Notes](#treerpo--tree-relative-policy-optimization)]

- [25/05] **Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability**  
[[Paper](http://arxiv.org/pdf/2505.13258v1)] [[Code/Page]()] [[TLDR/Notes](#effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability)]

- [25/06] **SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.08989v1)] [[Code/Page]()] [[TLDR/Notes](#sws--self-aware-weakness-driven-problem-synthesis-in-reinforcement-learning-for-llm-reasoning)]

- [25/06] **R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.04185v1)] [[Code/Page](https://github.com/QingFei1/R-Search.)] [[TLDR/Notes](#r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning)]

- [25/05] **Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.24332v1)] [[Code/Page]()] [[TLDR/Notes](#pangu-deepdiver--adaptive-search-intensity-scaling-via-open-web-reinforcement-learning)]

- [25/05] **Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.21765v1)] [[Code/Page]()] [[TLDR/Notes](#don-t-think-longer--think-wisely--optimizing-thinking-dynamics-for-large-reasoning-models)]

- [25/05] **Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.22203v1)] [[Code/Page]()] [[TLDR/Notes](#pitfalls-of-rule--and-model-based-verifiers----a-case-study-on-mathematical-reasoning)]

- [25/05] **Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.22095v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-route-queries-across-knowledge-bases-for-step-wise-retrieval-augmented-reasoning)]

- [25/05] **Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers**  
[[Paper](http://arxiv.org/pdf/2505.20128v1)] [[Code/Page]()] [[TLDR/Notes](#iterative-self-incentivization-empowers-large-language-models-as-agentic-searchers)]

- [25/05] **Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty**  
[[Paper](http://arxiv.org/pdf/2505.17281v1)] [[Code/Page]()] [[TLDR/Notes](#search-wisely--mitigating-sub-optimal-agentic-searches-by-reducing-uncertainty)]

- [25/06] **ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering**  
[[Paper](http://arxiv.org/pdf/2506.00232v1)] [[Code/Page]()] [[TLDR/Notes](#composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering)]

- [25/06] **CoRT: Code-integrated Reasoning within Thinking**  
[[Paper](http://arxiv.org/pdf/2506.09820v2)] [[Code/Page](https://github.com/ChengpengLi1003/CoRT.)] [[TLDR/Notes](#cort--code-integrated-reasoning-within-thinking)]

- [25/05] **Interleaved Reasoning for Large Language Models via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.19640v1)] [[Code/Page]()] [[TLDR/Notes](#interleaved-reasoning-for-large-language-models-via-reinforcement-learning)]



# TLDR/Notes
## start--self-taught-reasoner-with-tools
### Abstract
Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have
demonstrated remarkable capabilities in complex reasoning tasks through the
utilization of long Chain-of-thought (CoT). However, these models often suffer
from hallucinations and inefficiencies due to their reliance solely on internal
reasoning processes. In this paper, we introduce START (Self-Taught Reasoner
with Tools), a novel tool-integrated long CoT reasoning LLM that significantly
enhances reasoning capabilities by leveraging external tools. Through code
execution, START is capable of performing complex computations, self-checking,
exploring diverse methods, and self-debugging, thereby addressing the
limitations of LRMs. The core innovation of START lies in its self-learning
framework, which comprises two key techniques: 1) Hint-infer: We demonstrate
that inserting artificially designed hints (e.g., ``Wait, maybe using Python
here is a good idea.'') during the inference process of a LRM effectively
stimulates its ability to utilize external tools without the need for any
demonstration data. Hint-infer can also serve as a simple and effective
sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning
(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and
modifying the reasoning trajectories with tool invocation generated by a LRM
via Hint-infer, followed by fine-tuning the LRM. Through this framework, we
have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA
(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the
competition-level code benchmark (LiveCodeBench), START achieves accuracy rates
of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly
outperforms the base QwQ-32B and achieves performance comparable to the
state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary
model o1-Preview.
```
### 🌟 论文解读 | START：工具赋能的自教推理大模型，突破长链推理局限

### 📌 背景痛点/本文动机
大语言模型在复杂推理任务中，像OpenAI-o1、DeepSeek-R1这类大推理模型（LRMs）借助长思维链（CoT）展现出强大能力，但仅依赖内部推理流程，存在幻觉（hallucinations）和效率不足等问题。而工具集成推理（TIR）能缓解长CoT的缺陷，可如何把长CoT和TIR协同结合，是待解决的关键问题。此外，大推理模型训练时聚焦复杂推理问题求解，在指令遵循上泛化性缺失，难以在长CoT中主动调用工具（如Python解释器），这也推动了研究新方法来突破现状。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：Hint - infer技术
在大推理模型推理过程中插入人工设计的提示（如“Wait, maybe using Python here is a good idea.”），无需演示数据就能有效激发模型利用外部工具的能力。对于数学任务，插入基础提示和Python标识符就能引导模型写合适代码；对于代码生成任务，精心设计提示和代码模板可激活模型在长CoT中自主执行测试用例代码的能力。同时，Hint - infer还可作为简单有效的顺序测试时缩放方法，当在长思维链停止标记前加提示，模型会随思考时间增加呈现成功率提升的缩放效应。

💡 创新点2：Hint Rejection Sampling Fine - Tuning（Hint - RFT）技术
该技术结合了Hint - infer和RFT。先对大推理模型通过Hint - infer生成的带工具调用的推理轨迹进行评分、过滤和修改，得到用于微调的数据Dseed，再用Dseed对模型（如QwQ - 32B - Preview）进行微调得到START - 0，让模型具备工具使用的自我感知能力。后续还基于START - 0生成自蒸馏轨迹构建DSTART来进一步微调得到最终的START，实现长CoT与TIR的协同，打造Long TIR范式。

### 📈 实验结果
在多个基准测试中，START表现出色：在博士级科学问答（GPQA）任务上准确率达63.6%；在竞赛级数学基准（AMC23、AIME24、AIME25）上准确率分别为95.0%、66.7%、47.1%；在竞赛级代码基准（LiveCodeBench）上准确率为47.3%。显著超越了基础模型QwQ - 32B - Preview，且性能比肩开源权重模型R1 - Distill - Qwen - 32B和专有模型o1 - Preview。

### 💬 可借鉴之处
1. 提示驱动工具调用的思路：Hint - infer展示了通过人工提示在无演示数据下激活模型工具使用能力的可行性，为让模型在推理中自主利用外部资源提供了新范式，可借鉴到需要外部工具辅助的各类推理任务中，如数据分析、复杂计算场景等。
2. 自学习微调框架：Hint - RFT结合Hint - infer与RFT的自学习框架，通过对推理轨迹处理后微调模型，这种利用模型自身生成数据优化自身的思路，为大模型迭代升级提供了可参考的技术路线，在提升模型工具使用、推理精度等方面有借鉴价值。
3. 聚焦Python解释器调用的实践：针对Python解释器这一重要且具代表性的工具展开研究，为特定工具集成到大模型推理流程提供了实践范例，对于其他工具（如专业领域工具、其他编程语言解释器等）与大模型结合的研究有启发，可引导后续探索不同工具与长CoT结合的方式。
```

## tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning
### Abstract
Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.
```
### 🌟 论文解读 | Tool-Star：用强化学习赋能大模型多工具协作推理

### 📌 背景痛点/本文动机
大语言模型（LLMs）借助大规模强化学习展现出了出色的推理能力，但如何利用强化学习让大模型在多工具协作推理中高效发挥作用仍是待解难题。现实场景中推理常需模型结合环境交互整合多种能力（如深度信息搜索、长期知识记忆、精确计算等），工具集成推理（TIR）虽拓展了语言推理范式，但现有工作存在不足：基于提示的方法工具使用不稳定、精度有限；基于监督微调（SFT）的方法受演示质量和泛化性限制；基于强化学习（RL）的方法多聚焦单工具，多工具协作推理的系统研究不足。本文旨在解决“工具使用的合理性与效率”“多工具协作推理”两大问题，提出Tool - Star框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：工具集成推理数据合成 pipeline
为解决工具使用数据稀缺问题，设计通用的工具集成推理数据合成 pipeline。结合工具集成提示与基于提示的采样，自动生成大规模工具使用轨迹；后续通过质量归一化和难度感知分类过程，过滤不合理样本并按从易到难的“课程式”方式划分数据，为冷启动微调与强化学习分阶段构建高质量数据集，夯实TIR训练基础。

💡 创新点2：两阶段TIR训练框架
提出两阶段训练框架提升多工具协作推理能力。第一阶段是冷启动监督微调，让大模型借助工具调用反馈初步探索推理模式；第二阶段是多工具自批判强化学习算法，采用分层奖励机制，不仅评估答案正确性和工具使用格式，还为有效多工具协作分配额外奖励，且在标准RL过程中插入自批判奖励微调阶段，助力模型内化奖励原则。

💡 创新点3：集成多类型工具
Tool - Star在推理过程中集成了六种工具（训练时三种、推理时优化用三种），从工具层面为多工具协作推理提供支撑。

### 📈 实验结果
在10多个具有挑战性的计算推理任务（如AIME24、MATH500）和知识密集型推理任务（如WebWalker、HotpotQA）上开展实验。结果显示Tool - Star在保证工具使用效率和可靠性的同时，展现出强大的整体推理性能，验证了其有效性，且定量分析体现出高效率，为激励多工具协作推理提供了洞见。

### 💬 可借鉴之处
1. 数据合成方面：面对数据稀缺问题，设计的工具集成推理数据合成 pipeline 提供了从数据生成到质量控制、难度划分的完整思路，可借鉴这种系统生成和处理数据的方式来应对特定领域数据不足的情况。
2. 训练框架方面：两阶段训练框架先通过冷启动让模型“入门”工具推理，再用强化学习进阶提升多工具协作，这种分阶段、循序渐进且结合自批判的训练思路，为提升模型复杂任务能力提供了参考。
3. 多工具协作方面：针对多工具协作设计分层奖励机制，考虑到多工具协作的不同维度奖励，这种精细化奖励设计思路可用于其他需多组件协作的AI任务中，引导模型学习有效协作策略。
```

## scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging
### Abstract
Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.
```
### 🌟 论文解读 | 用“信息觅食”优化搜索增强推理：InForage框架如何让LLM更智能？

### 📌 背景痛点/本文动机
大语言模型（LLM）的“知识截断”问题一直是行业痛点——模型无法获取训练后更新的知识，也难以应对模糊、多步骤或动态变化的复杂任务。传统的“检索增强生成”方法采用**静态的预推理检索策略**，把检索到的信息一股脑塞进prompt就完事，面对需要迭代推理、逐步挖掘证据的复杂场景（比如“我要去NeurIPS参会，需要签证吗？”这类要先查会议地点、再查签证政策的问题），就显得力不从心。  

好在“测试时扩展技术”的发展让LLM能动态调用外部工具了，这推动大家把思路从“静态预推理检索”转向“自适应的推理时检索”。但动态检索也有两大挑战：一是复杂任务的信息需求是隐含且动态变化的，单次检索只能拿到局部信息“补丁”，得靠迭代积累；二是信息补丁的价值不取决于自身，而要看它对最终推理的贡献。  

人类面对这类信息搜索任务时，却能高效用几次迭代搜索解决问题——这背后是“信息觅食理论（IFT）”：人类会权衡信息补丁的价值和获取成本，用“信息气味（scent）”指引搜索方向。受此启发，论文提出InForage框架，想让LLM也学会这种“动态信息觅食”的能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于信息觅食理论的强化学习框架  
InForage把“检索增强推理”形式化为**动态信息搜索过程**，用强化学习（RL）来优化。和传统只看最终答案是否正确的方法不同，它关注**中间检索步骤的质量**：参考IFT里“信息气味”的概念，让模型在推理过程中，通过自适应搜索行为迭代收集、整合信息。  

💡 创新点2：三重奖励机制引导推理行为  
为了激励模型学习“高效信息觅食”，InForage设计了三类奖励：  
- 结果奖励（Outcome Reward）：奖励最终答案正确的推理轨迹；  
- 信息增益奖励（Information Gain Reward）：奖励那些能挖掘出有价值证据的中间检索步骤；  
- 效率惩罚（Efficiency Penalty）：避免无意义的冗长推理，鼓励“高效信息觅食”。  

💡 创新点3：构建人类引导的细粒度数据集  
现有QA数据集大多只有“问题-答案”对，缺少中间推理/检索步骤的记录，也很难支撑复杂多步推理的训练。为此，论文团队构建了一个**人类引导的数据集**：标注者从“种子问题”出发，模拟真实网页浏览时的迭代搜索、选文档、提炼子问题的过程，还要求每个案例至少包含4次信息跳跃或交叉条件，保证任务复杂度。数据集记录了搜索和推理的每一步，能为“最终答案正确性、中间检索质量、整体推理效率”提供监督信号。  


### 📈 实验结果
论文在三类任务上做了评估：通用问答、多跳推理任务，以及团队新开发的**实时网络QA数据集**。结果显示，InForage在所有任务中都**持续超越基线方法**，证明了从“丰富监督的搜索增强推理数据”中学习的有效性，也验证了InForage构建“鲁棒、自适应、高效推理智能体”的潜力。  


### 💬 可借鉴之处
1. **理论跨界融合**：把认知科学里的“信息觅食理论”引入LLM的检索增强推理，为动态检索提供了新颖的理论视角；  
2. **强化学习细粒度监督**：跳出“只看最终结果”的思维，设计对中间步骤的奖励机制，更贴合复杂任务里“迭代积累证据”的逻辑；  
3. **数据集构建思路**：针对“复杂多步推理缺数据”的痛点，人工模拟真实信息搜索轨迹，记录细粒度步骤——这种构建“带中间过程的复杂任务数据集”的思路，对后续研究很有参考价值；  
4. **动态检索的落地潜力**：InForage展示了LLM在“模糊、多步骤、信息需求动态变化”场景下的推理能力提升，为检索增强从“静态”转向“动态自适应”提供了可行方案。  
```

## collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration
### Abstract
Retrieval-Augmented Generation (RAG) systems often struggle to handle
multi-hop question-answering tasks accurately due to irrelevant context
retrieval and limited complex reasoning capabilities. We introduce Collab-RAG,
a collaborative training framework that leverages mutual enhancement between a
white-box small language model (SLM) and a blackbox large language model (LLM)
for RAG. Specifically, the SLM decomposes complex queries into simpler
sub-questions, thus enhancing the accuracy of the retrieval and facilitating
more effective reasoning by the black-box LLM. Concurrently, the black-box LLM
provides feedback signals to improve the SLM's decomposition capability. We
observe that Collab-RAG relies solely on supervision from an affordable
black-box LLM without additional distillation from frontier LLMs, yet
demonstrates strong generalization across multiple black-box LLMs. Experimental
evaluations across five multi-hop QA datasets demonstrate that Collab-RAG
substantially outperforms existing black-box-only and SLM fine-tuning baselines
by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a
frozen 32B LLM in question decomposition, highlighting the efficiency of
Collab-RAG in improving reasoning and retrieval for complex questions. The code
of Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.
```
### 🌟 论文解读 | Collab-RAG：白盒小模型与黑盒大模型协作，突破复杂问答场景下的RAG瓶颈

### 📌 背景痛点/本文动机
大语言模型（LLM）虽在众多语言任务中表现出色，但存在幻觉、难适配领域知识等问题。检索增强生成（RAG）技术通过整合外部知识缓解这些问题，然而在复杂多跳问答任务中，RAG常因检索到无关上下文、复杂推理能力受限而表现不佳。现有提升检索质量的方法多聚焦单步检索优化，难应对需迭代证据收集的复杂问答；无训练的LLM查询分解能力有限；小模型微调方法对黑盒LLM参数更新又低效昂贵。因此，充分释放黑盒LLM在复杂问答的能力仍具挑战，这催生了Collab - RAG的研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Collab - RAG框架，实现白盒小语言模型（SLM）与黑盒大语言模型（LLM）动态协作  
SLM作为分解器，将复杂查询拆解为更简单的子问题，提升检索相关上下文的准确性，也为黑盒LLM更有效推理铺路；黑盒LLM作为阅读器，为每个子问题生成中间答案并合成最终响应，借助子问题检索来逐步解答复杂问题。

💡 创新点2：基于黑盒LLM反馈的自改进训练策略  
直接用SLM做问题分解因推理能力有限且高质量标注成本高而效果不佳。Collab - RAG仅依赖黑盒LLM（如GPT - 4o - mini）的反馈来优化，将黑盒LLM建模为能生成响应的环境，SLM与之多轮交互迭代优化分解策略。设计迭代偏好优化方法，依据黑盒LLM反馈（通过基于规则的评估方法，从子问题格式和最终答案准确性判断分解是否有效）提升SLM分解能力，无需昂贵人工标注或前沿LLM蒸馏。

### 📈 实验结果
在五个多跳问答数据集上评估，Collab - RAG平均比现有仅黑盒LLM和SLM微调基线方法性能高出1.8% - 14.2%。在问题分解任务上，微调后的3B参数SLM表现超过冻结的32B参数LLM，有力证明了Collab - RAG在提升复杂问题推理和检索方面的高效性，且仅用GPT - 4o - mini监督训练却能在多个黑盒LLM上有强泛化性。

### 💬 可借鉴之处
1. 模型协作思路：在RAG等需多组件配合的任务中，探索不同“能力互补”模型（如白盒小模型与黑盒大模型）的协作模式，发挥各自优势（小模型可训练优化、大模型强推理等）。
2. 训练优化方式：利用现有黑盒大模型反馈来优化小模型，避免高成本标注与前沿大模型蒸馏，为资源有限情况下提升模型特定能力（如查询分解）提供思路。
3. 复杂任务处理：针对复杂多跳问答这类需分步推理、多证据整合的任务，通过“分解 - 子任务处理 - 合成”的 pipeline 设计，为突破任务难点提供了可参考的架构范式。
```

## zerosearch--incentivize-the-search-capability-of-llms-without-searching
### Abstract
Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a novel RL
framework that incentivizes the capabilities of LLMs to use a real search
engine with simulated searches during training. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both useful and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.
```
### 🌟 论文解读 | ZeroSearch：无需真实搜索，激发大模型搜索能力新范式

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽在诸多下游任务表现卓越，但知识静态且易生成幻觉或过时信息，需外接信息增强可靠性。检索增强生成（RAG）是常用方案，不过早期基于提示工程、后续监督微调或推理时缩放技术等存在工程复杂、计算开销大等问题。近年强化学习（RL）用于提升大模型搜索能力，却面临两大挑战：一是搜索引擎返回文档质量不可控，给训练引入噪声与不稳定；二是RL训练需频繁调用搜索API，成本高昂限制扩展性。为此，论文提出ZeroSearch框架，旨在训练时用模拟搜索来激发大模型使用真实搜索引擎的能力，解决上述痛点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：轻量监督微调打造检索模块  
通过轻量级监督微调，把大语言模型转化为能响应查询、生成“有用”和“含噪声”文档的检索模块。利用prompt设计区分不同质量文档，让模拟用的大模型学会生成不同质量文档，替代真实搜索引擎返回内容，既规避API成本，又能控制文档质量。  

💡 创新点2：课程式rollout策略渐进训练  
RL训练阶段采用基于课程的rollout策略，让生成文档的质量逐步降低，模拟越来越具挑战性的检索场景，循序渐进激发模型推理能力。先让策略模型学习基础输出格式与任务要求，再适应更复杂带噪声的检索场景，实现能力阶梯式提升。  

### 📈 实验结果
大量实验验证ZeroSearch有效性：用3B参数大模型作检索模块时，能有效激发策略模型使用真实搜索引擎的能力；7B参数模型作模拟检索模块，训练出的策略模型性能与真实搜索引擎训练的相当；14B参数模型作模拟时，性能甚至超越真实搜索引擎。此外，该框架在不同参数规模的基础模型和指令微调模型上泛化性好，还能兼容REINFORCE、PPO、GRPO等多种RL算法。  

### 💬 可借鉴之处
1. 思路创新：巧妙利用大模型预训练积累的世界知识，用模型模拟搜索引擎，绕开真实搜索API调用成本与质量不可控问题，为RL结合搜索场景训练开辟新路径。  
2. 训练策略：课程式rollout机制为处理“难度递增任务”类训练提供参考，可迁移到需逐步提升模型能力的场景；轻量监督微调快速构建功能模块的思路，在打造特定能力子模块时值得借鉴。  
3. 兼容性：对不同类型（基础/指令微调）、不同参数规模大模型以及多种RL算法的良好兼容性，让该框架在工业界或研究中更易落地拓展，减少适配成本。  
```

## reinforcement-learning-for-reasoning-in-large-language-models-with-one-training-example
### Abstract
We show that reinforcement learning with verifiable reward using one training
example (1-shot RLVR) is effective in incentivizing the mathematical reasoning
capabilities of large language models (LLMs). Applying RLVR to the base model
Qwen2.5-Math-1.5B, we identify a single example that elevates model performance
on MATH500 from 36.0% to 73.6%, and improves the average performance across six
common mathematical reasoning benchmarks from 17.6% to 35.7%. This result
matches the performance obtained using the 1.2k DeepScaleR subset (MATH500:
73.6%, average: 35.9%), which includes the aforementioned example. Furthermore,
RLVR with only two examples even slightly exceeds these results (MATH500:
74.8%, average: 36.6%). Similar substantial improvements are observed across
various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct,
DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different
math examples (when employed as a single training example). In addition, we
identify some interesting phenomena during 1-shot RLVR, including cross-domain
generalization, increased frequency of self-reflection, and sustained test
performance improvement even after the training accuracy has saturated, a
phenomenon we term post-saturation generalization. Moreover, we verify that the
effectiveness of 1-shot RLVR primarily arises from the policy gradient loss,
distinguishing it from the "grokking" phenomenon. We also show the critical
role of promoting exploration (e.g., by incorporating entropy loss with an
appropriate coefficient) in 1-shot RLVR training. We also further discuss
related observations about format correction, label robustness and prompt
modification. These findings can inspire future work on RLVR efficiency and
encourage a re-examination of recent progress and the underlying mechanisms in
RLVR. Our code, model, and data are open source at
https://github.com/ypwang61/One-Shot-RLVR.
```
### 🌟 论文解读 | 仅用1个训练样例，大幅提升大模型数学推理能力：RLVR新突破

### 📌 背景痛点/本文动机
在大语言模型（LLMs）推理能力提升的研究中，基于可验证奖励的强化学习（RLVR）是关键方法之一。不过目前RLVR在数据层面的探索较少，比如“到底需要多少数据？怎样的数据最有效？”等核心问题尚未明确。此前虽有研究尝试减少训练数据量，但未探索到极限程度。本文聚焦于“RLVR训练数据集能精简到何种程度还能保持性能”这一问题，展开深入研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出1-shot RLVR范式  
证明仅用**1个训练样例**就能驱动RLVR提升大模型数学推理能力。以Qwen2.5-Math-1.5B为基础模型，找到单个样例可让MATH500任务准确率从36.0%跃升至73.6%，6个数学推理基准平均性能从17.6%提升到35.7%，效果媲美含该样例的1.2k规模数据集（DSR - sub）。甚至2个样例时性能还能小幅超越。且该范式在不同模型（如Qwen2.5-Math-7B、Llama3.2-3B-Instruct等）、不同RL算法（GRPO、PPO）以及不同数学样例上都有显著提升。  

💡 创新点2：发现“后饱和泛化”等新现象  
在1-shot RLVR过程中观察到独特现象：  
- **后饱和泛化**：模型在单个训练样例上的训练准确率很快接近100%后，测试准确率仍持续提升；且约1.4k训练步后才出现过拟合，过拟合后训练样例输出虽混乱，但测试性能依旧强劲、测试输出也保持可读。  
- **跨域泛化**：在一个领域（如几何）的单个样例上训练，能提升其他领域（如代数、数论）的性能。  
- **自反思增强**：训练过程中，训练样例的响应长度和下游任务中自反思类术语出现频率增加。  

💡 创新点3：揭示RLVR性能提升关键因素  
通过消融实验明确：1-shot RLVR的性能提升主要由**策略梯度损失**驱动，和依赖权重衰减等正则化的“grokking”现象机制不同。同时强调“促进探索”的关键作用——加入合适系数的熵损失能进一步提效；甚至仅用熵损失（无结果奖励），Qwen2.5-Math-1.5B在MATH500上也能提升27.4%。  


### 📈 实验结果
1. 数据规模对比：1个样例的RLVR在MATH500（73.6%）和6个基准平均性能（35.7%）上，效果接近1.2k的DSR - sub（MATH500为73.6%、平均35.9%）；2个样例时（MATH500达74.8%、平均36.6%）还能小幅超越。  
2. 模型与算法泛化性：在Qwen2.5-Math-7B、Llama3.2-3B-Instruct、DeepSeek-R1-Distill-Qwen-1.5B等模型，以及GRPO、PPO算法上，1(few)-shot RLVR都实现了性能大幅提升。  
3. 非数学推理任务拓展：以数学样例做1-shot RLVR，还能提升模型在ARC等非数学推理任务的表现，甚至超过全量数据的RLVR。  


### 💬 可借鉴之处
1. 数据高效性启示：打破“RLVR需要大规模数据集”的固有认知，证明极小数据量（1或2个样例）就能驱动大模型推理能力跃升，为后续RLVR数据效率优化指明新方向，可重新审视RLVR中数据与性能的关系。  
2. 新现象与机制理解：“后饱和泛化”“跨域泛化”等现象的发现，以及策略梯度、熵损失等关键因素的验证，为深入理解RLVR内在机制提供了新视角，后续研究可围绕这些现象和机制展开更细粒度的分析。  
3. 工程实践参考：熵损失的提效实验（甚至无奖励仅熵损失也能涨点），以及格式修正、标签鲁棒性、prompt修改等附加观察，能为实际落地中优化RLVR训练流程、调优模型提供直接的实践参考。  
```

## deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments
### Abstract
Large Language Models (LLMs) equipped with web search capabilities have
demonstrated impressive potential for deep research tasks. However, current
approaches predominantly rely on either manually engineered prompts (prompt
engineering-based) with brittle performance or reinforcement learning within
controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that
fail to capture the complexities of real-world interaction. In this paper, we
introduce DeepResearcher, the first comprehensive framework for end-to-end
training of LLM-based deep research agents through scaling reinforcement
learning (RL) in real-world environments with authentic web search
interactions. Unlike RAG-based approaches that assume all necessary information
exists within a fixed corpus, our method trains agents to navigate the noisy,
unstructured, and dynamic nature of the open web. We implement a specialized
multi-agent architecture where browsing agents extract relevant information
from various webpage structures and overcoming significant technical
challenges. Extensive experiments on open-domain research tasks demonstrate
that DeepResearcher achieves substantial improvements of up to 28.9 points over
prompt engineering-based baselines and up to 7.2 points over RAG-based RL
agents. Our qualitative analysis reveals emergent cognitive behaviors from
end-to-end RL training, including the ability to formulate plans,
cross-validate information from multiple sources, engage in self-reflection to
redirect research, and maintain honesty when unable to find definitive answers.
Our results highlight that end-to-end training in real-world web environments
is not merely an implementation detail but a fundamental requirement for
developing robust research capabilities aligned with real-world applications.
We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.
```
### 🌟 论文解读 | DeepResearcher：在真实环境中用强化学习赋能深度研究

### 📌 背景痛点/本文动机
大语言模型（LLMs）结合网络搜索能力后，在深度研究任务中展现出了潜力，但现有方法存在明显不足：一类是依赖人工设计提示词的方式，性能脆弱不稳定；另一类是在受控的检索增强生成（RAG）环境内做强化学习，无法捕捉真实世界交互的复杂性。为了突破这些局限，论文提出要打造能在真实网络环境下，通过规模化强化学习来端到端训练基于LLM的深度研究智能体的框架，也就是DeepResearcher。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：真实环境下的端到端强化学习训练  
不同于RAG类方法假设所需信息都在固定语料库，DeepResearcher聚焦开放网络“嘈杂、无结构、动态”的特性，让智能体学习在真实网页交互里导航，实现对研究智能体的端到端训练，不再受限于受控环境的假设。  

💡 创新点2：专用多智能体架构  
搭建了特殊的多智能体架构，其中浏览智能体负责从各类网页结构里提取相关信息，克服了网页结构多样带来的技术挑战，让智能体在真实网页场景下能有效获取信息用于研究任务。  

### 📈 实验结果
在开放域研究任务的大量实验中，DeepResearcher对比基于提示词工程的基线方法，性能提升最高可达28.9个百分点；对比基于RAG的强化学习智能体，也能提升最高7.2个百分点。此外，定性分析还发现了端到端强化学习训练带来的“涌现认知行为”，比如制定计划、多源信息交叉验证、自我反思调整研究方向、找不到明确答案时保持诚实等。  

### 💬 可借鉴之处
从研究价值来看，论文证明了真实网络环境下的端到端训练不只是“实现细节”，更是打造贴合真实应用的鲁棒研究能力的根本要求，这为后续研究智能体的开发指明了方向——要重视真实场景交互训练。从工程实践角度，其开源的DeepResearcher框架（https://github.com/GAIR-NLP/DeepResearcher）能为想探索“强化学习+真实网页交互”做深度研究的团队或个人提供参考，多智能体处理网页信息的思路也值得在复杂信息抽取类任务中借鉴。而且对“涌现行为”的分析，也启发研究者关注训练过程中智能体能力的自然成长与展现~
```

## research--learning-to-reason-with-search-for-llms-via-reinforcement-learning
### Abstract
Large Language Models (LLMs) have shown remarkable capabilities in reasoning,
exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating
reasoning with external search processes remains challenging, especially for
complex multi-hop questions requiring multiple retrieval steps. We propose
ReSearch, a novel framework that trains LLMs to Reason with Search via
reinforcement learning without using any supervised data on reasoning steps.
Our approach treats search operations as integral components of the reasoning
chain, where when and how to perform searches is guided by text-based thinking,
and search results subsequently influence further reasoning. We train ReSearch
on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct
extensive experiments. Despite being trained on only one dataset, our models
demonstrate strong generalizability across various benchmarks. Analysis reveals
that ReSearch naturally elicits advanced reasoning capabilities such as
reflection and self-correction during the reinforcement learning process.
```
### 🌟 论文解读 | ReSearch：用强化学习让大模型学会“边搜索边推理”

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）在推理任务上展现出强大能力，像OpenAI - o1、DeepSeek - R1等模型的成功就是例证。不过，将推理与外部搜索流程结合仍颇具挑战，尤其是面对复杂多跳问题（需多轮检索的问题）时。现有多步检索增强生成（RAG）方法多依赖人工设计提示或启发式规则，不仅耗时耗力，对复杂问题也缺乏扩展性；而且给多步RAG框架标注推理步骤成本高、不现实。同时，当下基于强化学习（RL）提升大模型推理能力的工作，大多聚焦内部推理，鲜少探索如何把推理与外部知识检索有效结合。于是，本文提出ReSearch框架，旨在用强化学习让大模型学会“边搜索边推理”，无需推理步骤的有监督数据。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ReSearch框架，将搜索作为推理链的一部分
ReSearch把搜索操作视为链式推理过程的有机组成，推理链不仅包含基于文本的思考（用`[]`包裹），还有搜索查询（用`<search>` `</search>`包裹）和检索结果（用`<result>` `</result>`包裹）。何时、如何执行搜索由之前的文本思考引导，搜索结果又会影响后续文本思考。并且，该框架不提供推理步骤的有监督数据让模型模仿，而是借助强化学习（GRPO算法）激励模型“边搜索边推理”。
💡 创新点2：基于强化学习从头训练模型
在Qwen2.5 - 7B（- Instruct）和Qwen2.5 - 32B（- Instruct）模型上从头训练ReSearch。利用强化学习的思路，采样多个“边搜索边推理”的链（即rollouts），优化大模型策略，让生成高奖励rollouts的概率最大化，以此让模型学会在推理中合理运用搜索。
💡 创新点3：Reward建模与GRPO算法结合
采用Group Relative Policy Optimization（GRPO）作为学习算法，它从一组rollouts中估计基线，而非像近端策略优化（PPO）那样训练单独的 critic 模型。通过设计奖励建模来引导强化学习的优化过程，让模型在训练中逐步掌握“边搜索边推理”的能力。

### 📈 实验结果
在多跳问答基准测试（需多步推理和多次信息检索）上开展大量实验。训练后的ReSearch模型相比基线模型，绝对性能提升8.9% - 22.4%不等。而且仅在一个特定训练集上训练，却能在多个基准测试上评估，展现出良好的泛化性。此外，分析训练过程发现，ReSearch在强化学习过程中能自然激发反射、自我修正等高级推理能力。

### 💬 可借鉴之处
1. 框架设计思路：将外部搜索深度融入推理链，为解决大模型“推理 + 外部工具”结合难题提供了新范式，不再依赖人工设计的繁琐提示或启发式规则，而是让模型自主学习何时、如何用搜索辅助推理。
2. 强化学习应用：证明了强化学习在无需推理步骤有监督数据的情况下，能有效训练大模型获得“边搜索边推理”能力，为大模型推理能力提升开辟了新路径，后续工作可借鉴这种“无监督数据依赖的RL训练”思路拓展模型能力。
3. 泛化性验证：仅用单一训练集训练却能在多基准测试表现良好，说明该框架在模型泛化能力培养上有优势，为打造更通用的大模型推理 + 工具使用能力提供了参考，后续可围绕如何进一步提升泛化性、适配更多工具展开研究。
```

## simplerl-zoo--investigating-and-taming-zero-reinforcement-learning-for-open-base-models-in-the-wild
### Abstract
DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can
naturally emerge through a simple reinforcement learning (RL) framework with
rule-based rewards, where the training may directly start from the base
models-a paradigm referred to as zero RL training. Most recent efforts to
reproduce zero RL training have primarily focused on the Qwen2.5 model series,
which may not be representative as we find the base models already exhibit
strong instruction-following and self-reflection abilities. In this work, we
investigate zero RL training across 10 diverse base models, spanning different
families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,
Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several
key design strategies-such as adjusting format reward and controlling query
difficulty-we achieve substantial improvements in both reasoning accuracy and
response length across most settings. However, by carefully monitoring the
training dynamics, we observe that different base models exhibit distinct
patterns during training. For instance, the increased response length does not
always correlate with the emergence of certain cognitive behaviors such as
verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for
the first time in small models not from the Qwen family. We share the key
designs that enable successful zero RL training, along with our findings and
practices. To facilitate further research, we open-source the code, models, and
analysis tools.
```
### 🌟 论文解读 | SimpleRL-Zoo：探索与驯服野生开源基础模型的零强化学习

### 📌 背景痛点/本文动机
大模型在复杂任务解决中展现出长思维链（CoT）和类反思推理行为的能力令人瞩目，DeepSeek - R1表明从预训练基础模型（即base models）开始，基于规则奖励的纯强化学习（RL）能让长CoT和自我反思行为自发出现（零RL训练范式）。但此前零RL训练复现工作多聚焦Qwen2.5系列，而Qwen2.5基础模型本身就有强指令遵循和自我反思能力，不具代表性；且对模型行为分析较浅，未明确推理行为是否真变、有效推理涌现机制是啥，理解存在缺口。同时也不清楚小且能力弱的开源基础模型是否有这种涌现现象。为更透明理解不同野生基础模型的零RL训练，本文围绕“零RL训练中各模型推理能力咋发展？初始缺强指令遵循和自验证能力的基础模型是否还有‘顿悟时刻（aha moment）’？不同基础模型成功零RL训练的关键因素是啥？”这些问题展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：广泛模型覆盖的零RL训练探索
在多种系列和规模的模型上开展零RL训练，涵盖Mistral - 7B、Mistral - 24B、Llama3 - 8B、DeepSeek - Math - 7B、Qwen2.5不同参数规模（0.5B/1.5B/7B/14B/32B）以及Qwen2.5 - Math - 7B等10个不同基础模型，探究不同模型在零RL训练下的表现。
💡 创新点2：关键设计策略与训练要素把控
采用GRPO作为RL算法，结合调整格式奖励、控制查询难度等关键设计策略开展训练；训练仅依赖GSM8K和MATH数据集做基于规则的奖励建模，且对所有模型采用相同训练超参数，以此探究零RL训练的关键因素。
💡 创新点3：多维度训练动态与行为监测
除 accuracy 和 response length 外，还监测多种指标，深入探究模型训练过程中推理行为等变化，比如关注认知行为（如自我反思、验证等）是否涌现，以更全面理解零RL训练。

### 📈 实验结果
1. 推理精度与响应长度提升：借助关键设计策略，在多数设置下，推理精度和响应长度都有大幅提升，10个模型中9个（除Qwen2.5 - Math - 7B）响应长度显著增加，所有基础模型精度都有提升。
2. 响应长度与“顿悟时刻”非必然关联：多数Qwen2.5模型响应长度增加但像自我反思这类认知行为频率没上升，说明响应长度增加不总对应“顿悟时刻”（特定认知行为涌现）。
3. 非Qwen家族小模型出现“顿悟时刻”：首次在Qwen家族外小模型（如Llama3 - 8B和DeepSeek - Math - 7B）中观察到验证等特定认知推理行为频率显著增加。
4. 格式奖励与数据难度影响：严格格式奖励（如答案放框内）对初始指令遵循差的基础模型探索有很大惩罚，限制性能上限还易引发过度思考；训练数据难度需与基础模型内在探索能力匹配，否则零RL会失败。
5. 零RL训练并非简单重排序：零RL训练使pass@k精度提升10 - 30绝对点，证明不是只重排响应。
6. SFT冷启动对RL的影响：用传统SFT数据集作为RL冷启动（DeepSeek - R1发布前常见做法），虽高质量CoT数据能通过模仿快速提升基础模型性能，但会限制RL中自由探索能力，降低RL后性能并抑制高级推理能力涌现。

### 💬 可借鉴之处
1. 模型选择与覆盖：研究不同家族、不同规模基础模型的零RL训练，为后续探索不同类型基础模型强化学习提供了广泛的参考样本，启示研究者要考虑模型多样性。
2. 训练策略与要素：调整格式奖励、控制查询难度等关键设计策略在提升性能上有成效，后续工作可借鉴这些策略优化强化学习训练；同时关注训练数据难度与模型能力匹配度，这对零RL训练成功很关键。
3. 行为与动态监测：除传统精度和长度指标，关注认知行为等多维度指标来理解训练过程，为深入分析模型强化学习中行为变化提供了思路，有助于更全面评估模型训练效果。
4. 开源贡献：论文承诺评审后开源代码、模型和分析工具，这为该领域后续研究提供了便利，也鼓励研究者在合适阶段开源成果促进领域发展。
```

## chain-of-retrieval-augmented-generation
### Abstract
This paper introduces an approach for training o1-like RAG models that
retrieve and reason over relevant information step by step before generating
the final answer. Conventional RAG methods usually perform a single retrieval
step before the generation process, which limits their effectiveness in
addressing complex queries due to imperfect retrieval results. In contrast, our
proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the
model to dynamically reformulate the query based on the evolving state. To
train CoRAG effectively, we utilize rejection sampling to automatically
generate intermediate retrieval chains, thereby augmenting existing RAG
datasets that only provide the correct final answer. At test time, we propose
various decoding strategies to scale the model's test-time compute by
controlling the length and number of sampled retrieval chains. Experimental
results across multiple benchmarks validate the efficacy of CoRAG, particularly
in multi-hop question answering tasks, where we observe more than 10 points
improvement in EM score compared to strong baselines. On the KILT benchmark,
CoRAG establishes a new state-of-the-art performance across a diverse range of
knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to
understand the scaling behavior of CoRAG, laying the groundwork for future
research aimed at developing factual and grounded foundation models.
```
### 🌟 论文解读 | 突破单步检索限制：CoRAG开启多步检索增强生成新范式

### 📌 背景痛点/本文动机
在企业应用等场景中，检索增强生成（RAG）是核心技术之一，它能让大模型结合专有数据源生成可靠且基于事实的响应。但传统RAG存在局限：常规RAG在生成前仅执行**单步检索**，面对复杂查询时，因检索结果可能不完善，难以有效应对；而且检索模型为效率采用的固定大小向量表示等架构，限制了处理复杂查询的能力，在多跳推理任务中，也难确定初始该检索什么信息。为打破检索质量瓶颈，让模型能像人类解决复杂问题那样迭代检索信息，本文提出CoRAG方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：CoRAG多步动态检索与查询重构范式  
传统RAG单步检索后生成，CoRAG则允许模型**基于状态演化动态重构查询**，在生成最终答案前，逐步检索和推理相关信息。这种方式模拟人类解决复杂问题时迭代找信息的过程，能在检索器没返回有用信息时，尝试不同查询重写策略，探索查询的多方面信息。  

💡 创新点2：拒绝采样增强数据集与显式训练  
现有RAG数据集常只给最终正确答案，为有效训练CoRAG，本文用**拒绝采样**自动生成中间检索链，扩充数据集。之后用开源语言模型在这些增强数据集上，以标准下一个token预测目标微调，让模型显式学习逐步检索的能力，而非仅依赖模型上下文学习或专有模型蒸馏。  

💡 创新点3：多样测试时解码策略调控计算资源  
测试阶段，提出贪心解码、best - of - N采样、树搜索等**多种解码策略**，通过控制采样检索链的长度和数量，调节模型测试时的计算开销（如token消耗、检索器调用频率等），实现测试时计算资源的灵活缩放。  

### 📈 实验结果
- 多跳问答任务：在多跳推理类QA任务中，相比强基线，CoRAG在EM分数上提升超10个点，验证了其处理需多步检索推理复杂任务的有效性。  
- KILT基准测试：在涵盖多样知识密集型任务的KILT基准上，CoRAG在几乎所有任务的隐藏测试集上取得全新SOTA成绩。  
- 缩放行为分析：不同解码策略下，总token消耗和模型性能近似呈对数线性关系（系数因数据集而异）；还发现对不同任务类型CoRAG缩放行为不同，如NQ这类现有SOTA检索器已高召回的数据集，测试时缩放收益有限，为依查询复杂度和检索器质量动态分配测试计算资源提供依据。  

### 💬 可借鉴之处
- 技术范式层面：CoRAG提出的多步动态检索 + 查询重构思路，为RAG突破单步限制、处理复杂任务提供了新范式，后续研究可在此基础上探索更灵活的检索 - 生成交互逻辑。  
- 数据增强层面：用拒绝采样自动生成中间检索链来增强数据集，为解决RAG数据集缺少中间过程标注的问题提供了可行方法，可借鉴到需中间步骤数据的模型训练场景。  
- 测试优化层面：通过多样解码策略调控测试时计算，这种根据任务和检索器情况灵活分配资源的思路，对大模型实际部署时平衡性能与成本很有启发，可用于优化模型推理阶段的资源利用。  
- 研究方向层面：CoRAG为开发事实性、有依据的基础模型（foundation models）奠定基础，后续在减少模型幻觉、提升内容可靠性方面，可延续其多步检索推理的思路深入探索。  
```

## rare--retrieval-augmented-reasoning-modeling
### Abstract
Domain-specific intelligence demands specialized knowledge and sophisticated
reasoning for problem-solving, posing significant challenges for large language
models (LLMs) that struggle with knowledge hallucination and inadequate
reasoning capabilities under constrained parameter budgets. Inspired by Bloom's
Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning
Modeling (RARE), a novel paradigm that decouples knowledge storage from
reasoning optimization. RARE externalizes domain knowledge to retrievable
sources and internalizes domain-specific reasoning patterns during training.
Specifically, by injecting retrieved knowledge into training prompts with
masked losses, RARE transforms learning objectives from rote memorization to
contextualized reasoning. It enables models to bypass parameter-intensive
memorization and prioritize the development of higher-order cognitive
processes. Extensive experiments demonstrate that lightweight RARE-trained
models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance,
surpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\%
accuracy. RARE establishes a paradigm shift where maintainable external
knowledge bases synergize with compact, reasoning-optimized models,
collectively driving more scalable domain-specific intelligence.
```
### 🌟 论文解读 | RARE：解锁领域智能新范式，让推理与知识“各司其职”

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽在通用领域表现卓越，但面对领域特定智能任务时，却受限于两大核心难题：**知识幻觉**（即便参数规模极大，领域知识的长尾分布仍让模型易“胡言乱语”）与**推理能力不足**（在有限参数预算下，模型难兼顾知识记忆与高阶推理）。  

教育理论中布鲁姆分类法（Bloom’s Taxonomy）给了启发：认知从基础“记忆”到高阶“分析、评估、创造”呈层级结构。而现有范式（如无适配直接调用通用模型、仅补知识的检索增强生成RAG、传统闭卷式预训练/微调）要么缺领域知识与推理、要么重知识轻推理学习、要么记忆成本高还易幻觉。  

核心问题呼之欲出：**能否在有限资源下，解耦领域知识记忆与推理能力培养，让模型优先发展高阶推理？** 这正是RARE试图回答的。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：范式革新——“有备开卷考”（Prepared Open - book Exam）  
RARE打破“知识存储”与“推理优化”的绑定，提出“领域知识外置，领域推理内化”。训练时，模型不把资源浪费在死记硬背领域知识，而是专注学**领域特定推理模式**；推理时，再从外部知识库动态拉取所需知识。这就像学生备考：平时练解题逻辑（推理），考试时允许查资料（外部知识），把模型容量从“记忆”重分配到“推理”，兼顾效率与知识准确性/可更新性。  

💡 创新点2：训练机制——知识注入与损失重构  
通过“检索知识注入训练prompt + 掩码损失”，RARE把学习目标从“死记硬背”扭向“情境化推理”。不再让模型为记知识犯难，而是把知识相关损失（如记忆错误）转化为“知识怎么用”的应用导向损失。举个例子：学“量化宽松如何影响债券收益率”时，外部知识（QE买债、收益率与价格负相关）被注入，模型要学的是“买债→需求升→价格升→收益率降”这一推理链，而非硬记结论。  


### 📈 实验结果
在MedQA、PubMedQA、CoVERT等多领域基准测试中，轻量的RARE训练模型（如Llama - 3.1 - 8B）表现亮眼：  
- PubMedQA上，Llama3.1 - 8B + RARE达76.8%准确率，超过带检索增强的DeepSeek - R1（75.4%）与GPT - 4（75.2%）；  
- CoVERT任务中，RARE版Llama3.1 - 8B准确率81.7%，远超DeepSeek - R1 + RAG（63.3%）和GPT - 4（65.7%）；  
整体在多个任务上，比检索增强的GPT - 4、DeepSeek - R1等领先超20%准确率，证明小参数模型靠RARE也能在领域任务中干翻大模型+传统RAG的组合。  


### 💬 可借鉴之处
1. **跨学科思路**：把教育理论（布鲁姆分类法）与AI结合，为“知识 - 推理平衡”难题提供新视角，启示后续研究从认知科学找灵感；  
2. **工程范式**：解耦知识与推理，让“可维护的外部知识库 + 推理优化的紧凑模型”协同，为领域特定智能落地提供更具扩展性的路径——不用死堆模型参数，靠系统级设计提升效率；  
3. **训练 trick**：通过知识注入重构损失的思路，为“让模型学怎么用知识，而非记知识”提供了实操方法，可迁移到需要知识推理的垂直领域（如医疗、法律）微调中。  
```

## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### 🌟 论文解读 | s3：小数据量下用强化学习训练搜索智能体的新范式

### 📌 背景痛点/本文动机
在检索增强生成（RAG）系统中，大语言模型（LLMs）能借助外部知识提升推理能力，而近期用强化学习（RL）让LLM扮演搜索智能体的进展，可通过多轮交互优化信息获取。但现有方法存在缺陷：要么用仅关注搜索的指标（如NDCG）优化检索，忽略下游效用；要么微调整个LLM来联合推理与检索，把检索和生成耦合，限制了实际搜索效用且难适配冻结或闭源模型。因此，需要一种解耦搜索与生成、轻量且模型无关的框架，在小数据量下提升下游性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出s3框架，解耦搜索器与生成器  
s3是轻量、模型无关的框架，将搜索器（Searcher）从生成器（Generator）中解耦。训练时固定生成器（可复用任意冻结的LLM），仅专注于训练搜索器，让搜索优化聚焦在对下游生成质量的提升上，避免了生成与检索的耦合问题，也能适配闭源或冻结的LLM。  

💡 创新点2：定义“Gain Beyond RAG（GBR）”奖励信号  
GBR作为全新的奖励指标，衡量了s3检索到的文档让生成器表现，相比“朴素RAG（naïve RAG）检索”在生成准确率上的提升幅度。通过该奖励训练搜索器，能直接针对下游生成效用优化检索组件，摆脱仅关注搜索指标或易过拟合的精确匹配（EM）类指标的局限，让检索优化更贴合实际生成需求。  

### 📈 实验结果
在6个通用QA基准和5个医疗QA基准测试中，s3展现出强大性能：仅用2.4k训练样本，就超越了使用超70倍数据（如70k甚至更多）训练的基线方法（像DeepRetrieval、Search - R1等）。在通用和医疗领域的平均得分上，s3对比其他经典RAG、Active RAG、RL - Zero阶段的方法，都实现了更优的下游生成表现，验证了小数据量下高效训练搜索智能体的能力。  

### 💬 可借鉴之处
1. 模块化设计思路：将复杂的“检索 + 生成”任务解耦为搜索器和生成器独立优化，为构建更灵活的RAG系统提供了模块化思路，后续可针对不同模块分别迭代升级。  
2. 奖励信号设计：GBR奖励将下游生成效用作为检索优化的核心依据，跳出了传统仅看搜索环节指标的思维定式，为强化学习在检索增强场景下的奖励函数设计提供了新范式，可启发其他需要跨环节优化任务的奖励设计。  
3. 小数据高效训练：证明了在少量优质样本下也能训练出高性能搜索智能体，降低了大规模数据标注与训练成本，对资源有限但需构建RAG系统的场景（如垂直领域小团队）有很强的借鉴意义。

## hybrid-latent-reasoning-via-reinforcement-learning
### Abstract
Recent advances in large language models (LLMs) have introduced latent
reasoning as a promising alternative to autoregressive reasoning. By performing
internal computation with hidden states from previous steps, latent reasoning
benefit from more informative features rather than sampling a discrete
chain-of-thought (CoT) path. Yet latent reasoning approaches are often
incompatible with LLMs, as their continuous paradigm conflicts with the
discrete nature of autoregressive generation. Moreover, these methods rely on
CoT traces for training and thus fail to exploit the inherent reasoning
patterns of LLMs. In this work, we explore latent reasoning by leveraging the
intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we
introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid
latent reasoning approach that (1) integrates prior hidden states into sampled
tokens with a learnable gating mechanism, and (2) initializes training with
predominantly token embeddings while progressively incorporating more hidden
features. This design maintains LLMs' generative capabilities and incentivizes
hybrid reasoning using both discrete and continuous representations. In
addition, the hybrid HRPO introduces stochasticity into latent reasoning via
token sampling, thereby enabling RL-based optimization without requiring CoT
trajectories. Extensive evaluations across diverse benchmarks show that HRPO
outperforms prior methods in both knowledge- and reasoning-intensive tasks.
Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing
behaviors like cross-lingual patterns and shorter completion lengths,
highlighting the potential of our RL-based approach and offer insights for
future work in latent reasoning.
```
### 🌟 论文解读 | 强化学习驱动的混合潜在推理：解锁大模型内在推理能力

### 📌 背景痛点/本文动机
在大语言模型（LLMs）领域，潜在推理作为 autoregressive（自回归）推理的替代方案展现出潜力。传统自回归推理依赖离散的思维链（CoT）解码与采样，而潜在推理借助前序步骤的连续隐藏状态实现内部推理，能利用更丰富信息。但现有潜在推理方法存在诸多问题：一是与LLMs兼容性差，连续范式和自回归生成的离散性冲突，将隐藏状态输入下一步解码会降低生成质量（如重复、不连贯）；二是依赖CoT轨迹训练，既忽视LLMs固有推理能力，又带来高昂训练成本（如多阶段训练、从头训多块模型），限制了适用范围。因此，亟需一种能无缝整合连续表示、依托预训练LLMs泛化性、减少CoT依赖的潜在推理方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出HRPO框架，首推基于强化学习的混合推理方案  
Hybrid Reasoning Policy Optimization（HRPO）是首个基于强化学习的混合潜在推理优化框架，把策略学习和潜在推理统一起来，无需依赖CoT轨迹就能利用LLMs内在推理模式。通过强化学习让大模型自主发展潜在推理能力，为潜在推理提供了高效且可扩展的新路径。

💡 创新点2：设计门控机制，平衡生成能力与连续推理  
为在保留LLMs生成能力的同时，引导模型在连续空间推理，HRPO引入**可学习门控机制**。训练初期，输入主要来自采样token的嵌入，保证生成质量；随训练推进，门控学习融入前序隐藏状态中更丰富的信息，助力内部推理。这种渐进式融合，让离散token和连续隐藏表示协同作用，实现混合推理。

💡 创新点3：借助采样随机性，实现无CoT的RL优化  
HRPO通过token采样给潜在推理引入随机性，使得rollout（轨迹展开）能像标准RL方法一样执行。混合输出（token + 潜在表示）存入rollout buffer用于策略更新，基于简单的“结果导向型奖励”计算对数概率，实现策略梯度更新，自适应整合token级和潜在表示信息，解锁现有LLMs的潜在推理能力。

### 📈 实验结果
在多个知识密集型和推理密集型任务基准测试中，HRPO表现优于现有方法（包括其他潜在推理基线），在不同场景下持续提升性能。此外，经HRPO训练的LLMs保留了可解释性，还展现出跨语言模式、更短生成长度等有趣行为，验证了基于RL的混合潜在推理方案的潜力，也为后续潜在推理研究提供了新视角。

### 💬 可借鉴之处
1. 方法创新角度：将强化学习与潜在推理结合，跳出“依赖CoT标注和多阶段训练”的传统思路，为大模型推理能力提升开辟新范式，证明了RL在解锁LLMs内在推理模式上的价值。  
2. 工程设计角度：门控机制的“渐进式融合”思路，为平衡模型既有能力（如生成）与新能力（如连续空间推理）提供了可参考的技术路线，在需兼顾“存量能力”和“增量能力”的模型优化场景中具借鉴性。  
3. 实验与分析角度：不仅验证性能，还关注模型行为（可解释性、跨语言等），这种从“效果”到“特性”的全面分析，能帮助研究者更深入理解方法对模型的塑造，为后续优化指明方向。  
```

## webthinker--empowering-large-reasoning-models-with-deep-research-capability
### Abstract
Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate
impressive long-horizon reasoning capabilities. However, their reliance on
static internal knowledge limits their performance on complex,
knowledge-intensive tasks and hinders their ability to produce comprehensive
research reports requiring synthesis of diverse web information. To address
this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs
to autonomously search the web, navigate web pages, and draft research reports
during the reasoning process. WebThinker integrates a \textbf{Deep Web
Explorer} module, enabling LRMs to dynamically search, navigate, and extract
information from the web when encountering knowledge gaps. It also employs an
\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to
seamlessly interleave reasoning, information gathering, and report writing in
real time. To further enhance research tool utilization, we introduce an
\textbf{RL-based training strategy} via iterative online Direct Preference
Optimization (DPO). Extensive experiments on complex reasoning benchmarks
(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)
demonstrate that WebThinker significantly outperforms existing methods and
strong proprietary systems. Our approach enhances LRM reliability and
applicability in complex scenarios, paving the way for more capable and
versatile deep research systems. The code is available at
https://github.com/RUC-NLPIR/WebThinker.
```
### 🌟 论文解读 | WebThinker：让大推理模型拥有深度研究能力

### 📌 背景痛点/本文动机
近年来，大推理模型（LRMs）如OpenAI - o1、DeepSeek - R1等在数学、代码、科学等领域展现出卓越的长程推理能力。然而，它们依赖静态内部知识，在复杂、知识密集型任务上表现受限，也难以生成需整合多样网络信息的全面研究报告。现有开源深度搜索智能体多采用预定义工作流的检索增强生成（RAG）技术，限制了LRMs探索深层网络信息及与搜索引擎的紧密交互，开发通用、灵活的开源深度研究框架成了亟待解决的关键挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出WebThinker深度研究智能体
WebThinker能让LRMs在推理过程中自主搜索网页、浏览网页并起草研究报告，不同于传统预定义工作流，它使LRM在思考时自主行动，实现单代内的端到端任务执行。

💡 创新点2：设计Deep Web Explorer模块
该模块赋予LRMs网页搜索和导航能力，使其在遇到知识缺口时，能动态搜索、浏览网页（如点击链接或按钮等交互元素）并提取信息，还能基于当前查询结果发起后续搜索、遍历更深链接以收集所有相关信息。

💡 创新点3：引入Autonomous Think - Search - and - Draft策略
将报告撰写与LRMs的推理和搜索过程深度整合，让模型在思考和搜索时实时撰写报告。为LRMs配备三个专用工具：特定章节内容起草、检查当前报告、编辑报告，使模型在推理中自主提升报告的全面性、连贯性及对新发现信息的适应性。

💡 创新点4：开发基于强化学习（RL）的训练策略
利用配备工具的LRM从复杂任务中采样大规模推理轨迹，依据推理准确性、工具使用情况和最终输出构建偏好对，进行在线Direct Preference Optimization（DPO）训练，通过迭代的on - policy训练，逐步提升模型感知、推理和与研究工具有效交互的能力。

### 📈 实验结果
在复杂推理基准测试（GPQA、GAIA、WebWalkerQA、HLE）和科学报告生成任务（Glaive）上进行大量实验。结果显示，WebThinker在复杂问题解决和报告生成任务中均显著优于现有方法和强大的专有系统。例如在复杂推理问题解决任务中，WebThinker - 32B（Ours）在各基准测试中的表现远超QwQ - 32B、RAG - QwQ - 32B、Search - o1 - 32B等；在科学报告生成任务中，在Comprehensive、Thorough、Factuality、Coherence等维度上也表现出色，超过RAG - Qwen2.5 - 72B、Grok3 DeeperSearch、Gemini2.0 Deep Research等。

### 💬 可借鉴之处
1. 架构设计层面：WebThinker将推理与网络信息探索深度融合的架构思路，为解决大模型依赖静态知识的问题提供了新范式，可启发后续大模型在知识获取与推理结合方向的研究。
2. 模块创新层面：Deep Web Explorer模块对网页搜索、导航和信息提取的设计，为大模型获取外部动态知识提供了可行的工具模块参考；Autonomous Think - Search - and - Draft策略中实时撰写报告与推理搜索整合的方式，对需要内容生成与信息获取结合的任务（如报告生成、文案创作等）有借鉴意义。
3. 训练策略层面：基于RL和在线DPO的训练策略，为提升大模型工具使用能力提供了有效的训练方法参考，可用于其他需工具交互的大模型任务场景训练。
```

## smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback
### Abstract
RAG systems consist of multiple modules to work together. However, these
modules are usually separately trained. We argue that a system like RAG that
incorporates multiple modules should be jointly optimized to achieve optimal
performance. To demonstrate this, we design a specific pipeline called
\textbf{SmartRAG} that includes a policy network and a retriever. The policy
network can serve as 1) a decision maker that decides when to retrieve, 2) a
query rewriter to generate a query most suited to the retriever, and 3) an
answer generator that produces the final response with/without the
observations. We then propose to jointly optimize the whole system using a
reinforcement learning algorithm, with the reward designed to encourage the
system to achieve the best performance with minimal retrieval cost. When
jointly optimized, all the modules can be aware of how other modules are
working and thus find the best way to work together as a complete system.
Empirical results demonstrate that the jointly optimized SmartRAG can achieve
better performance than separately optimized counterparts.
```
### 🌟 论文解读 | SmartRAG：用环境反馈端到端优化RAG系统，突破模块分离训练瓶颈

### 📌 背景痛点/本文动机
尽管大语言模型（LLMs）在诸多领域展现出强大能力，但处理模型参数之外的知识类问题仍颇具挑战。检索增强生成（RAG）通过从外部工具检索信息有效提升了模型在这类场景的表现。然而，传统RAG系统的多个模块（如检索器、决策器、查询重写器等）往往是**分离训练**的。这带来两大问题：一是中间模块的“黄金答案”（最优输出）通常难以获取，甚至依赖特定模型或检索器；二是分离优化会导致各模块缺乏对整体系统的协同感知，难以达到全局最优性能。因此，论文提出要对RAG这类多模块系统进行**端到端的联合优化**，让各模块在协作中相互适配。

### 🚀 核心方法（介绍本文的几个创新点）
#### 💡 创新点1：设计SmartRAG系统架构，让Policy Network身兼三职  
SmartRAG的核心由**策略网络（Policy Network）**和**检索器（Retriever）**构成。其中，策略网络承担三个关键角色：  
- 决策器（Decision Maker）：根据输入问题与已有观测，决定是否需要发起检索；  
- 查询重写器（Query Rewriter）：若决定检索，生成更适配检索器的查询语句；  
- 答案生成器（Answer Generator）：若认为当前信息足够，直接生成最终回答。  
这种“一专多能”的设计让单个策略网络串联起RAG的核心流程，为后续端到端优化打下基础。  

#### 💡 创新点2：基于强化学习（RL）的端到端联合优化  
论文采用**近端策略优化（PPO）**算法对SmartRAG进行联合训练，用**环境反馈**替代传统“黄金答案”作为监督信号。设计奖励函数时，兼顾两大目标：一是“正确回答问题”，二是“最小化检索成本”（减少不必要的检索次数）。通过强化学习，策略网络能在与环境（如外部知识库、检索工具）的交互中，学习到各模块间的最优协作方式——何时检索、检索什么、如何回答，让整个系统形成有机整体。  


### 📈 实验结果
论文在多个数据集上验证了SmartRAG的有效性，核心结论是：**联合优化的SmartRAG显著优于模块分离训练的基线系统**。此外，通过系统性分析，论文还展示了SmartRAG如何学习“何时检索、检索什么、如何回答”这三项关键能力，证明了模块间协同感知对系统性能的提升作用。  


### 💬 可借鉴之处
1. **架构设计思路**：将RAG多模块功能收敛到“策略网络+检索器”的简洁架构，用单一网络承载决策、重写、生成，为复杂系统的模块化整合提供了参考；  
2. **优化范式创新**：用强化学习做端到端联合优化，摆脱对“黄金答案”的强依赖，更贴合真实场景中模块输出难标注的痛点；  
3. **奖励函数设计**：平衡“任务效果（回答正确率）”与“资源成本（检索次数）”，这种多目标权衡的思路可迁移到其他需资源约束的AI系统设计中。  

总之，SmartRAG为RAG系统的工程化落地提供了“协同优化”的新范式，让多模块系统从“各自为战”走向“全局最优”，值得从事检索增强、大模型应用的研究者与工程师深入参考~
```

## distilling-llm-agent-into-small-models-with-retrieval-and-code-tools
### Abstract
Large language models (LLMs) excel at complex reasoning tasks but remain
computationally expensive, limiting their practical deployment. To address
this, recent works have focused on distilling reasoning capabilities into
smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher
LLMs. However, this approach struggles in scenarios requiring rare factual
knowledge or precise computation, where sLMs often hallucinate due to limited
capability. In this work, we propose Agent Distillation, a framework for
transferring not only reasoning capability but full task-solving behavior from
LLM-based agents into sLMs with retrieval and code tools. We improve agent
distillation along two complementary axes: (1) we introduce a prompting method
called first-thought prefix to enhance the quality of teacher-generated
trajectories; and (2) we propose a self-consistent action generation for
improving test-time robustness of small agents. We evaluate our method on eight
reasoning tasks across factual and mathematical domains, covering both
in-domain and out-of-domain generalization. Our results show that sLMs as small
as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier
larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the
potential of agent distillation for building practical, tool-using small
agents. Our code is available at https://github.com/Nardien/agent-distillation.
```
### 🌟 论文解读 | 让小模型拥有大能力：Agent Distillation 赋能小语言模型工具使用与复杂推理

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务上表现卓越，但高计算成本限制了其实际部署。为解决此问题，已有工作尝试通过思维链（CoT）蒸馏将推理能力迁移到小语言模型（sLMs），然而小模型在需要罕见事实知识或精确计算的场景下，常因能力局限产生幻觉（hallucinate）。例如回答“2010 年投资 100 美元苹果股票到 2020 年价值多少”这类问题，既需股票历史事实知识又要算术推理，仅靠 CoT 蒸馏的小模型难以应对未见过的新知识或计算。因此，如何让小模型不仅获得推理能力，还能像大模型 Agent 一样借助工具解决任务，成为关键问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 Agent Distillation 框架  
该框架突破静态推理蒸馏，将基于 LLM 的 Agent（如 ReAct、CodeAct）的“推理 - 行动 - 观察”完整任务解决行为迁移到小模型，让小模型学会用检索和代码工具解决问题，而非死记知识和计算步骤，提升对新查询的泛化能力。  

💡 创新点2：First - thought prefix 提升教师轨迹质量  
引入一种提示方法，无需额外微调教师模型，使其生成的“推理 - 行动 - 观察”轨迹更优质，为小模型蒸馏提供更好的监督信号，让小模型能学到更有效的任务解决路径。  

💡 创新点3：Self - consistent action generation 增强测试鲁棒性  
在测试阶段，让小模型生成多个行动轨迹，利用代码解释器筛选出结果有效且一致的轨迹，提升小模型在实际测试时应对复杂任务的稳定性，减少错误输出。  

### 📈 实验结果
在 4 个事实推理任务（如 HotpotQA、Bamboogle 等）和 4 个数学推理任务（如 MATH、GSM - Hard 等）上评估，涵盖域内和域外泛化。结果显示：  
- 0.5B、1.5B、3B 参数量的小模型经 Agent Distillation 后，性能可与用 CoT 蒸馏的下一层级更大模型（1.5B、3B、7B）媲美。  
- 对比 CoT Prompting、CoT Distillation 等方法，Agent Distillation 能持续提升小模型在事实和数学领域的问题解决能力，让小模型借助工具自适应完成代码执行和信息检索来解决任务（如图 1 展示不同大小 Qwen2.5 - Instruct 模型在多任务上的精度对比，Agent Distillation 效果更优）。  

### 💬 可借鉴之处
1. 框架设计思路：Agent Distillation 为小模型赋予类 Agent 能力提供了新范式，不再局限于静态推理蒸馏，而是迁移完整任务解决行为，启发后续小模型能力增强研究。  
2. 蒸馏优化手段：First - thought prefix 无需微调教师模型就能提升轨迹质量，为蒸馏过程中“教师 - 学生”信号传递优化提供轻量且有效的思路；Self - consistent action generation 则为测试阶段提升小模型鲁棒性提供了可参考的策略，可推广到其他需稳定性的小模型推理任务。  
3. 多领域验证：在事实和数学等多领域、多任务上验证有效性，证明方法具有跨领域通用性，为不同领域小模型应用提供借鉴。  
```

## search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning
### Abstract
Efficiently acquiring external knowledge and up-to-date information is
essential for effective reasoning and text generation in large language models
(LLMs). Prompting advanced LLMs with reasoning capabilities to use search
engines during inference is often suboptimal, as the LLM might not fully
possess the capability on how to interact optimally with the search engine.
This paper introduces Search-R1, an extension of reinforcement learning (RL)
for reasoning frameworks where the LLM learns to autonomously generate
(multiple) search queries during step-by-step reasoning with real-time
retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn
search interactions, leveraging retrieved token masking for stable RL training
and a simple outcome-based reward function. Experiments on seven
question-answering datasets show that Search-R1 improves performance by 41%
(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same
setting. This paper further provides empirical insights into RL optimization
methods, LLM choices, and response length dynamics in retrieval-augmented
reasoning. The code and model checkpoints are available at
https://github.com/PeterGriffinJin/Search-R1.
```
### 🌟 论文解读 | Search-R1：用强化学习让大模型学会推理与搜索引擎协作

### 📌 背景痛点/本文动机
大语言模型（LLMs）在自然语言理解与生成方面表现卓越，但面对复杂推理任务和获取外部实时信息时仍存在不足。现有将LLMs与搜索引擎结合的方式（如检索增强生成RAG、把搜索引擎当工具）存在局限：RAG虽能利用外部知识，但LLMs在训练中未被优化以高效与搜索引擎交互；工具类方法里，基于提示的方式泛化性差，基于训练的方式又因依赖大规模高质量标注轨迹和搜索操作不可微分，难以有效扩展。同时，把强化学习（RL）应用于“搜索 + 推理”场景也面临框架稳定性、多轮交错推理与搜索、奖励设计三大挑战。因此，如何让LLMs在推理时自主且高效地利用搜索引擎，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建含搜索引擎的RL环境与稳定优化机制  
Search - R1将搜索引擎建模为环境的一部分，让轨迹序列能交错生成LLM tokens和执行搜索引擎检索。它兼容PPO、GRPO等多种RL算法，还通过“检索token掩码”技术保障RL训练的稳定性，让模型在整合检索到的上下文时也能稳定优化。  

💡 创新点2：支持多轮检索与推理的交互流程  
模型可在<search>和</search>标记触发下调用搜索，检索内容被包裹在<information>和</information>间，LLM推理步骤用<think>和<think>封装，最终答案也以特定格式输出，以此实现结构化、迭代式的决策过程，让大模型能依据问题复杂度动态调整检索策略，完成多轮推理与搜索的交错协作。  

💡 创新点3：简洁的基于结果的奖励函数设计  
摒弃复杂的过程型奖励，采用简单的结果导向奖励函数。实验证明这种极简设计在“搜索 + 推理”场景中能有效引导模型学习，助力Search - R1成为DeepSeek - R1 Zero的扩展，为检索驱动决策引入搜索增强的RL训练。  

### 📈 实验结果
在七个问答数据集上开展实验，同一设置下（相同检索模型、训练数据、预训练LLM），Search - R1让Qwen2.5 - 7B和Qwen2.5 - 3B分别比各类RAG基线模型性能提升41%和20%。同时，论文还在RL优化方法、LLM选择、检索增强推理中的响应长度动态等方面给出了实证性见解。  

### 💬 可借鉴之处
1. 技术思路层面：为解决大模型“搜索 + 推理”难题提供了RL框架新思路，把搜索引擎纳入环境、设计多轮交互流程等做法，为后续优化大模型外部知识利用方式提供了参考范式。  
2. 实验与分析层面：系统的实验不仅验证了方法有效性，还对RL方法选择、不同LLM适配、响应长度等维度展开研究，这些实证洞察能辅助研究者在类似“检索 + 推理”任务中做更优决策。  
3. 工程落地层面：代码和模型 checkpoint 开源（https://github.com/PeterGriffinJin/Search - R1），便于社区基于该工作进一步探索大模型与搜索引擎协作的更多可能。  
```

## search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms
### Abstract
Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.
```
### 🌟 论文解读 | 让大模型边“思考-搜索-精炼”边推理：AutoRefine 革新检索增强范式

### 📌 背景痛点/本文动机
大语言模型（LLMs）在推理任务中展现了强大能力，但受限于训练语料的知识储备，在需要实时或精准知识的任务中表现不足。检索增强生成（RAG）通过让LLM调用外部工具查询知识库来缓解这一问题，然而现有方法存在两大核心缺陷：  
1. **缺乏对检索文档的精炼**：传统“思考时搜索（search-during-think）”范式直接基于检索到的（可能包含噪声、无关信息的）文档生成答案，没有先提炼关键信息，限制了模型识别缺失知识、基于不完整证据推理或迭代优化检索的能力；  
2. **检索专属奖励未充分探索**：多数检索增强推理方法仅依赖“结果导向”奖励（如最终答案是否正确），对“如何提升检索过程本身”缺乏直接指导，导致模型难学如何获取更相关、更有信息量的文档。  

为解决这些问题，论文提出 **AutoRefine** —— 一个基于强化学习（RL）的后训练框架，重塑检索增强推理范式。

### 🚀 核心方法
💡 创新点1：“思考时搜索+精炼（search-and-refine-during-think）”新范式  
传统RAG是“思考→搜索→直接生成答案”，AutoRefine则在“搜索”与“生成答案”之间插入**显式的知识精炼步骤**。具体通过 `<search>...</search>[documents]<refine>...</refine>` 这样的模板，强制模型先对检索到的文档进行“过滤噪声、提炼关键、组织证据”的迭代操作，再生成最终答案。这一步让模型在回答前先“消化”检索到的信息，避免被无关内容干扰。  

💡 创新点2：融合“结果奖励+检索专属奖励”的强化学习训练  
AutoRefine采用 **Group Relative Policy Optimization（GRPO）** 算法，同时优化两种奖励：  
- **结果导向奖励**：评估最终答案的正确性；  
- **检索专属奖励**：基于 `<refine>` 块中提炼内容的质量计算，直接指导“如何更好检索与利用文档”。  
训练时，模型会生成多条包含“思考、搜索、精炼、回答”的推理轨迹，再用GRPO对这些轨迹做优化，让模型学会在推理中更智能地调用检索工具、更高效地处理信息。  

### 📈 实验结果
论文在单跳（single-hop）和多跳（multi-hop）问答基准测试中验证AutoRefine：  
- **性能超越现有方法**：在复杂的多跳推理场景下提升尤为显著，证明“搜索+精炼”范式对长链条、多步骤推理的有效性；  
- **检索质量更高频**：分析显示AutoRefine更频繁发起高质量搜索，且能有效整合、合成证据，减少噪声干扰。  

### 💬 可借鉴之处
1. **范式创新**：“搜索后精炼再回答”的流程设计，为解决“检索信息噪声/无关”问题提供了新视角，可启发后续RAG类工作优化推理流程；  
2. **奖励设计**：将“过程性奖励（检索质量）”与“结果性奖励（答案正确）”结合，展示了强化学习在引导模型“过程优化”上的潜力，为RL与LLM结合的奖励机制设计提供参考；  
3. **多跳推理适配**：在多跳任务中表现突出，说明该方法对需要多次检索、信息拼接的复杂任务友好，可迁移到知识密集型的长文本推理、多步骤决策等场景。  

总之，AutoRefine通过“流程范式+奖励机制”的双重创新，让大模型在检索增强推理中更自主、更智能，为突破“知识储备限制+检索信息噪声”两大痛点提供了一套简洁而有力的方案~
```

## select2reason--efficient-instruction-tuning-data-selection-for-long-cot-reasoning
### Abstract
A practical approach to activate long chain-of-thoughts reasoning ability in
pre-trained large language models is to perform supervised fine-tuning on
instruction datasets synthesized by strong Large Reasoning Models such as
DeepSeek-R1, offering a cost-effective alternative to reinforcement learning.
However, large-scale instruction sets with more than 100k samples incur
significant training overhead, while effective strategies for automatic
long-CoT instruction selection still remain unexplored. In this work, we
propose Select2Reason, a novel and efficient instruction-tuning data selection
framework for long-CoT reasoning. From the perspective of emergence of
rethinking behaviors like self-correction and backtracking, we investigate
common metrics that may determine the quality of long-CoT reasoning
instructions. Select2Reason leverages a quantifier to estimate difficulty of
question and jointly incorporates a reasoning trace length-based heuristic
through a weighted scheme for ranking to prioritize high-utility examples.
Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only
10% of the data selected by Select2Reason achieves performance competitive with
or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across
three competition-level and six comprehensive mathematical benchmarks. Further
experiments highlight the scalability in varying data size, efficiency during
inference, and its adaptability to other instruction pools with minimal cost.
```
### 🌟 论文解读 | Select2Reason：高效筛选长链思维推理指令微调数据，解锁大模型深度推理能力

### 📌 背景痛点/本文动机
大语言模型（LLM）要激活长链思维（long - CoT）推理能力，常用方法是在强推理模型（如DeepSeek - R1）合成的指令数据集上做有监督微调（SFT），这种方式比强化学习更具成本效益。但超10万样本的大规模指令集带来巨大训练开销，且自动筛选长CoT指令的有效策略仍待探索。同时，过往虽有一些数据筛选工作，但针对长CoT推理的指令筛选挑战未充分解决；一些优质数据集构建依赖定性启发式方法，缺乏严谨定量验证，且流程不公开，阻碍复现与推广。此外，长CoT推理中“反思行为（如自我修正、回溯）”相关特征对指令质量的影响也需深入探究，以此为背景，本文提出Select2Reason框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Select2Reason框架用于长CoT推理的指令微调数据筛选
从长CoT推理中自我修正、回溯等“反思行为”涌现的视角，探究决定长CoT推理指令质量的指标。框架利用大模型作为“裁判”（LLM - as - a - Judge）量化问题难度，优先选择更具挑战性的问题；还设计指令 - 响应联合排序器，通过加权方案结合基于难度和推理轨迹长度的排序，优先选择高价值样本。

💡 创新点2：验证推理轨迹长度与问题难度作为筛选优质指令的关键指标
通过统计分析（如图1所示，更长推理轨迹在每一步中“Wait、Alternatively、Maybe、However”等反思token出现频率更高，且难题对应的指令中这类反思token也更多）和初步指令筛选任务实验（如图3所示，基于最长推理轨迹优先选择子集微调的模型在不同数据规模下都比基于中、短轨迹训练的模型表现好；且基于基础模型难解决指令子集训练的模型比易题子集训练的表现好），验证了推理轨迹长度是简单且有效的筛选启发式指标，问题难度也对指令质量有重要影响，更具挑战性的指令有更大学习价值。

### 📈 实验结果
在OpenR1 - Math - 220k数据集上实验，仅用Select2Reason筛选出的10%数据微调大语言模型，在三个竞赛级和六个综合数学基准测试中，性能与全量数据微调以及开源基线模型OpenR1 - Qwen - 7B相当甚至更优。此外，模型在推理时更高效（用更少思考token探索出更强性能的解决方案）；在不同数据规模下展现出良好扩展性；还能以低成本适配其他长CoT推理指令池（如含11万样本的Chinese - DeepSeek - R1 - Distill数据集），体现强泛化性。

### 💬 可借鉴之处
1. 为长CoT推理场景下的指令微调数据筛选提供了新颖高效的框架思路，从反思行为视角切入探究指令质量指标，为后续相关数据筛选工作提供了方法论参考。
2. 验证的推理轨迹长度和问题难度等关键指标，可指导从业者在构建或筛选长CoT推理指令数据集时，更有针对性地评估和选择数据，提升数据利用效率。
3. 框架在不同数据规模、推理效率、跨指令池适配等方面的优势，为大模型在长链推理能力训练优化时的工程实践提供了可复用的模式，助力降低训练成本同时提升模型推理性能。
```

## stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization
### Abstract
Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our code
will be released on https://github.com/Zillwang/StepSearch.
```
### 🌟 论文解读 | StepSearch：用分步近端策略优化点燃大模型搜索能力

### 📌 背景痛点/本文动机
大语言模型（LLMs）在多跳推理任务中需要迭代获取高价值外部知识，但现有基于强化学习（RL）训练LLMs进行搜索式文档检索的工作，仅依赖全局信号带来的稀疏奖励，在复杂多跳问答任务中表现不佳。此前方法存在对中间查询和多步检索缺乏细粒度监督、多数多跳问答框架在查询轨迹建模上存在关键缺口等问题，为填补这些研究空白，本文提出StepSearch框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建通用多跳搜索数据  
基于MuSiQue数据集开发新颖的数据 pipeline，生成6万条经过筛选的子问题搜索关键词，这些关键词可在不同检索数据集上泛化。具体流程为利用GPT - 4o丰富分解后的MuSiQue问题，得到连贯的子问题 - 答案对并生成每一步的搜索查询；将增强后的步骤问题重新表述为搜索查询集；向多个来源发出查询并筛选出有效结果。

💡 创新点2：提出StepSearch分步强化学习框架  
在近端策略优化（PPO）基础上增加基于token级别的奖励（信息增益和冗余惩罚），用于查询构建和文档检索。将每一轮交互划分为思考→搜索→回答阶段，为每个token分配信息增益信号和冗余惩罚，引导模型将多跳查询分解为聚焦的搜索子任务，动态调整检索策略，更有效地整合外部证据。同时设计简约的提示模板，在训练时解耦参数更新与检索产物，聚焦模型内部推理和搜索策略参数学习。

### 📈 实验结果
在标准多跳QA基准测试中，StepSearch显著超越全局奖励基线。使用仅19k的训练数据，对于3B和7B规模的模型，相比各种基于RL的搜索基线分别实现了11.2%和4.2%的绝对性能提升；在不同多跳QA基准上，相比标准RL基线分别有5.7%、9.1%、10.0%和15.2%的绝对提升，证明了细粒度、分步监督在优化深度搜索LLMs方面的有效性。

### 💬 可借鉴之处
1. 数据构建方面：通过数据 pipeline 生成包含子问题级搜索轨迹的细粒度问答数据集，为模型训练提供更丰富的多跳推理相关数据支撑，这种从已有开源数据集拓展构建专用数据的思路值得借鉴。
2. 强化学习优化方面：引入分步的、token级别的奖励机制，关注中间步骤的信息增益与冗余问题，为解决复杂任务中强化学习奖励稀疏、监督不细的问题提供了新方向，可用于其他需要多步迭代、细粒度监督的LLMs应用场景优化。
3. 训练策略方面：在训练时对特定模块（如检索产物相关部分）进行梯度屏蔽，聚焦关键学习目标，这种解耦学习目标的训练技巧在融合外部工具或信息的LLMs训练中具有参考价值。
```

## towards-effective-code-integrated-reasoning
### Abstract
In this paper, we investigate code-integrated reasoning, where models
generate code when necessary and integrate feedback by executing it through a
code interpreter. To acquire this capability, models must learn when and how to
use external code tools effectively, which is supported by tool-augmented
reinforcement learning (RL) through interactive learning. Despite its benefits,
tool-augmented RL can still suffer from potential instability in the learning
dynamics. In light of this challenge, we present a systematic approach to
improving the training effectiveness and stability of tool-augmented RL for
code-integrated reasoning. Specifically, we develop enhanced training
strategies that balance exploration and stability, progressively building
tool-use capabilities while improving reasoning performance. Through extensive
experiments on five mainstream mathematical reasoning benchmarks, our model
demonstrates significant performance improvements over multiple competitive
baselines. Furthermore, we conduct an in-depth analysis of the mechanism and
effect of code-integrated reasoning, revealing several key insights, such as
the extension of model's capability boundaries and the simultaneous improvement
of reasoning efficiency through code integration. All data and code for
reproducing this work are available at: https://github.com/RUCAIBox/CIR.
```
### 🌟 论文解读 | 代码集成推理：让大模型更聪明地“慢思考”

### 📌 背景痛点/本文动机
大语言模型（LLM）在推理任务中虽有进展，但受限于自身固有缺陷（如数值计算不精确、知识覆盖有限），难以突破能力瓶颈。为解决这些问题，用外部工具增强LLM成了热门方向，其中“代码集成推理”（让模型按需生成代码并通过代码解释器执行反馈）是很有潜力的路径。不过，现有工具增强的强化学习（tool - augmented RL）在训练时存在动态不稳定、训练复杂度高（要同时学推理和工具使用）等问题，阻碍了代码集成推理能力的高效习得。本文正是为了系统性解决tool - augmented RL在代码集成推理训练中的有效性与稳定性难题，同时深入分析代码集成推理的机制与效果而展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：明确“代码集成推理”范式
将代码集成推理形式化定义：大模型在推理时，遇需精确计算（如数值、逻辑、符号操作）场景触发代码生成，生成的代码由外部解释器执行，结果回灌到推理序列指导后续步骤，形成“生成代码→执行→更新上下文→再推理”的迭代循环，架起自然语言推理与形式化推理（代码）的桥梁，让模型能借助代码解释器的计算能力突破自身局限。
💡 创新点2：增强型工具 - 强化学习训练策略
为平衡探索与稳定，提出一系列改进：
 - 探索层面：移除KL散度项，采用受DAPO启发的clip - higher方法，还逐步增加工具交互预算，鼓励模型探索工具使用的可能性；
 - 稳定层面：将熵系数设为0、强制精确代码块匹配以减少rollout噪声、屏蔽外部工具反馈干扰等，让训练过程更稳定，在学工具使用能力时也能提升推理性能。

### 📈 实验结果
在五个主流数学推理基准测试上开展大量实验，模型性能远超多个有竞争力的基线模型。同时，通过分析揭示代码集成推理的关键机制：
 - 扩展模型能力边界（以PASS@K衡量），借助代码解释器的计算力突破自身限制；
 - 相比长思维链（long - CoT）推理，代码集成推理路径更简洁高效，常先给解决方案概览再生成可执行代码；
 - 可执行但逻辑错的代码虽可能拖后腿，不可执行代码却可能倒逼模型反思修正输出；
 - 不同题型受益程度有差异，几何题在四种题型中提升最小。

### 💬 可借鉴之处
1. 工具增强LLM的思路拓展：把代码解释器作为核心增强工具，且形式化定义了代码集成推理范式，为后续用“代码+执行反馈”增强推理提供了清晰框架参考；
2. 强化学习训练优化：针对工具增强RL的不稳定性，提出的平衡探索与稳定的训练策略组合（如clip - higher、熵系数调整等），可迁移到其他工具增强型RL训练场景；
3. 机制分析视角：从能力边界、推理路径、代码影响、题型差异等多维度分析代码集成推理的效果，为后续优化模型与任务适配性提供了分析方法论；
4. 开源资源：公开数据、代码和模型 checkpoint（https://github.com/RUCAIBox/CIR），方便复现与二次开发，降低了领域研究门槛。
```

## an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents
### Abstract
Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.
```
### 🌟 论文解读 | 强化学习驱动推理-搜索交织型大模型智能体的实证研究

### 📌 背景痛点/本文动机
大语言模型（LLMs）在自然语言处理诸多任务中表现卓越，但在需与外部环境交互、借助工具的场景（如搜索任务）中存在局限。强化学习（RL）为训练LLM成为能交织推理与搜索的智能体提供了潜力，然而这类智能体的最优设计仍不明晰，如奖励设计、基础LLM选择与特性、搜索引擎在RL过程中角色等关键因素待深入探究。此前方法在训练搜索智能体时，基于提示或有监督微调存在难扩展等问题，而RL虽有前景但相关关键问题研究不足，本文旨在通过全面实证研究填补这些空白。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：系统探究奖励设计对搜索智能体训练的影响 
区分格式奖励（反映对智能体动作格式的遵循）与中间检索奖励（迭代激励与结果相关的检索），实证分析不同奖励在训练中作用，明确格式奖励对性能提升更有效，中间检索奖励效用有限。

💡 创新点2：深入分析基础LLM特性的影响 
对比通用型与推理专用型LLM，以及不同规模模型在RL训练中的表现，揭示模型规模、初始化类型（通用或推理专用）对RL训练结果的显著影响，为选择合适基础模型提供依据。

💡 创新点3：剖析搜索引擎选择的作用 
研究训练时不同质量搜索引擎（从随机噪声到强密集检索器）如何塑造RL训练动态，以及推理时检索系统变化对智能体鲁棒性影响，明确搜索引擎选择在训练动态与推理鲁棒性上的关键作用。

### 📈 实验结果
1. 奖励设计方面：加入格式奖励能显著提升性能，尤其从基础LLM开始训练时；中间检索奖励未带来持续性能提升，实用价值有限。
2. 基础LLM层面：通用型LLM在RL场景中表现优于推理专用型，可能因后者训练初期指令遵循能力较弱；扩大模型规模通常提升最终性能，但收益递减。
3. 搜索引擎选择维度：训练时搜索引擎质量强烈影响RL动态，用无信息引擎（如随机噪声）会让智能体完全回避检索，弱引擎（如BM25）导致频繁但低效搜索调用，强引擎（如密集检索器）学习更稳定；推理时搜索智能体对不同检索系统普遍鲁棒，更强搜索引擎持续带来更好下游性能。

### 💬 可借鉴之处
1. 奖励设计：在构建RL驱动的LLM搜索智能体时，优先考虑格式奖励来提升性能，可谨慎评估中间检索奖励投入。
2. 模型选择：根据应用场景权衡通用型与推理专用型LLM，若追求RL训练效果初期通用型更具优势，同时合理考虑模型规模与性能收益平衡。
3. 搜索引擎集成：训练阶段选择高质量搜索引擎保障学习稳定性，推理阶段可基于场景灵活更换检索系统，且利用强搜索引擎提升下游表现，为实际部署搜索智能体提供了清晰的选型与优化方向。 
```

## computational-thinking-reasoning-in-large-language-models
### Abstract
While large language models (LLMs) have demonstrated remarkable reasoning
capabilities, they often struggle with complex tasks that require specific
thinking paradigms, such as divide-and-conquer and procedural deduction, \etc
Previous researches integrate external, reliable tools to alleviate logical
inconsistencies and hallucinations in LLMs' problem-solving processes. However,
we argue that the root challenge is more profound: LLMs lack the complex
thinking paradigms (\ie, computational thinking) during reasoning. In this
paper, we propose Computational Thinking Model (CTM), a novel framework that
incorporates computational thinking paradigms into LLMs. This framework enables
LLMs to reformulate complex problems through decomposition, abstraction,
reduction, and simulation, among other techniques. Specifically, live code
execution is seamlessly integrated into the reasoning process, allowing CTM to
think by computing. CTM directly instills computational thinking objectives
into LLMs through tailored reinforcement learning rewards, which encourages
problem simplification, modular planning, and iterative verification. We
conduct extensive evaluations on multiple code generation and mathematical
benchmarks. The results demonstrate that CTM outperforms conventional reasoning
models and tool-augmented baselines in terms of accuracy, interpretability, and
generalizability. We hope this study offers valuable insights for AI reasoning,
where LLMs can transform problems into robust, verifiable, and scalable
computational workflows, much like computer scientists do.
```
### 🌟 论文解读 | 让大模型拥有“计算思维”：CTM 框架革新复杂任务推理

### 📌 背景痛点/本文动机
大语言模型（LLMs）在自然语言推理领域展现出强大能力，但面对算法求解、数学推理等复杂任务时，仍存在结构性短板——比如在需要分治、过程演绎等特定思维范式的场景中，容易出现逻辑不一致、幻觉（hallucination），且中间错误会不断传播。过往研究尝试用外部工具（如代码解释器、网页搜索）来缓解，但本质上大模型仍把推理当作“黑箱式生成”，工具仅作辅助而非融入核心思维范式。而计算机科学家解决复杂问题时依赖的**计算思维**（分解、约简、抽象、模拟等），正是当前 LLMs 缺失的关键。本文动机正是填补这一“思维范式鸿沟”，让大模型像计算机科学家一样系统解决复杂问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出计算思维模型（CTM）框架，将计算思维注入大模型  
CTM 不是简单“外挂工具”，而是让大模型把**计算思维**作为核心推理策略。通过分解（Decomposition）、抽象（Abstraction）、约简（Reduction）、模拟（Simulation）等技术，把复杂问题转化为可执行、可验证的流程。例如，让模型用“代码驱动”的方式拆解任务：分解成子目标、用代码模拟枚举可能、通过执行代码回溯错误、运行测试用例验证等（如图1展示的代码启发式子目标设定、枚举、回溯、验证等模式）。  

💡 创新点2：融合“实时代码执行”的交互式推理环境  
CTM 搭建了一个支持**实时代码执行**的交互环境，让大模型在推理中“边算边想”。代码不再是最终产物，而是推理过程的“中间步骤载体”——用代码分解任务、用执行结果验证逻辑、用报错信息回溯修正。这种方式把模糊的自然语言推理，转化为可执行、可追溯的计算流程，解决了传统大模型“隐式推测”易出错的问题。  

💡 创新点3：两阶段训练策略（SFT + RL）  
为了让大模型真正学会“计算思维”，CTM 设计了**有监督微调（SFT）+ 强化学习（RL）**的训练范式：  
- SFT 阶段：用构造的“结构化推理数据集”，让模型先学习分解、验证等计算思维的基本方法论；  
- RL 阶段：设计定制化奖励函数，引导模型自主学习“简化问题、模块化规划、迭代验证”等目标，强化符合计算思维的推理行为。  

### 📈 实验结果
论文在**代码生成**（如 LiveCodeBench 等）和**数学推理**（如 AIME 等）多个基准测试上展开实验，结果显示：  
- **精度碾压**：CTM 显著超越传统推理模型（如 DeepSeek - R1 等）和工具增强基线，在需要精确计算、组合推理的任务中错误率大幅降低；  
- **可解释性提升**：因为推理过程由“代码 + 自然语言”分步拆解，每一步意图和逻辑更透明，不像纯自然语言推理那样“黑箱”；  
- **泛化性更强**：在跨领域、复杂场景下，CTM 能更稳健地把问题转化为可执行工作流，适应新任务的能力更优。  

论文还对比了传统自然语言推理模型的局限（如图2，传统模型在中间步骤符号计算易出错、自编辑能力极弱；而 CTM 靠代码执行实现“执行 - 验证 - 修正”闭环），进一步凸显 CTM 优势。  

### 💬 可借鉴之处
1. **思维范式的“顶层设计”**：不再局限于“工具堆叠”，而是从“人类解决复杂问题的核心思维”（计算思维）出发重构大模型推理逻辑，为大模型能力升级提供了“认知框架级”的新思路；  
2. **代码与推理的深度融合**：证明了“代码执行”不只是生成代码的手段，更是让大模型实现“精确推理、可验证推理”的关键载体，为代码驱动型 AI 推理开辟新方向；  
3. **训练范式的创新**：SFT + RL 结合“定制化奖励”的两阶段训练，为大模型学习“非自然语言类”思维（如计算思维）提供了可复用的技术路线；  
4. **跨领域价值**：CTM 对数学、代码等“精确推理领域”的突破，也为其他需要结构化、符号化推理的场景（如科学计算、逻辑验证）提供了迁移参考。  

简言之，CTM 不仅是一个性能更优的推理框架，更是一次“让大模型像人类专家一样思考”的范式探索——未来大模型或许能真正以“计算思维”为矛，攻破更多复杂任务的堡垒。
```

## diversity-aware-policy-optimization-for-large-language-model-reasoning
### Abstract
The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.
```
### 🌟 论文解读 | 大语言模型推理中，多样性感知的策略优化

### 📌 背景痛点/本文动机
大语言模型（LLMs）的推理能力发展迅猛，尤其是DeepSeek R1发布后，引发了对数据质量和强化学习（RL）算法的大量研究。尽管多样性在RL中至关重要，但它对LLM推理的影响却尚未充分探索。传统RL任务中，多样性有助于促进探索、避免局部最优等，那在LLM推理的RL训练中，提升多样性是否也必不可少？为填补这一空白，本文展开了系统性研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：首次系统探究多样性在LLM推理中的作用  
提出新指标Potential@k来量化LLM的推理潜力（RL训练后的性能提升潜力），对12个代表性LLM进行实证分析，发现高性能模型中，解决方案多样性与Potential@k存在强正相关，即多样性对RL训练后的最终性能提升有直接贡献，为将多样性纳入策略优化提供了实证依据。  

💡 创新点2：提出token - 级多样性目标并优化应用  
熵正则化是提升多样性的常用方法，但直接增加LLM输出平均熵会引入长度偏差（长响应熵更高）。为此，设计token - 级多样性度量并将多样性目标转化为实用形式；同时考虑质量 - 多样性权衡，仅对正样本有选择地增强多样性，既丰富解决方案多样性又保持训练稳定性，该设计类似基于群体的RL训练中培育高质量策略的多样性，确保探索由任务相关性能标准引导。之后将该多样性目标整合到R1 - zero训练框架中。

### 📈 实验结果
在四个数学推理基准测试中，整合方法后的R1 - zero训练框架相比标准R1 - zero训练，平均性能提升3.5%，同时能生成更多样、更鲁棒的解决方案。

### 💬 可借鉴之处
1. 对多样性在LLM推理中作用的探究思路值得借鉴，通过提出新指标并结合多模型实证分析，为领域内相关研究提供了新视角，后续研究可参考这种对未充分探索因素的系统分析方式。  
2. 处理多样性与长度偏差、质量 - 多样性权衡等问题的方法具有启发性，token - 级多样性设计、针对正样本应用多样性增强等策略，为在LLM的RL训练中平衡各因素提供了实践路径，其他关注LLM训练中多样性优化的工作可参考这些技术手段。  
3. 实验设计上，选择多个数学推理基准（每个基准至少500个问题且评估指标稳定）进行验证，这种严谨的实验设置方式也值得相关研究学习，以确保方法效果评估的可靠性。
```

## the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models
### Abstract
Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.
```
### 🌟 论文解读 | 大模型推理中幻觉困境的破局：面向事实性的强化学习优化

### 📌 背景痛点/本文动机
大语言模型（LLMs）借助强化学习（RL）优化在推理任务上取得了显著进展，能在数学、多跳问答等复杂基准测试中展现强大能力。但研究发现，**面向推理的RL微调会大幅增加幻觉（hallucinations，即生成事实错误或编造内容）的出现**。比如模型在逐步推理时可能中间步骤出错，即便最终答案偶尔正确，也会产生不可靠的解释。  

为何基于“结果导向”的RL微调会加剧幻觉？论文从理论分析RL训练动态，指出三大关键因素：  
1. 仅优化最终正确答案会导致策略梯度方差极大（当正确答案稀少时训练不稳定）；  
2. 为探索“有奖励的输出”，策略需保持高预测熵，增加了幻觉风险；  
3. 标准RL目标易陷入“虚假局部最优”——模型可能收敛到自信但错误、无奖励的答案。  
这些因素让纯结果导向的RL方法易引发推理模型幻觉，难以学习可靠的推理模式。因此，如何在强化学习微调中兼顾推理能力与事实正确性，成为亟待解决的问题。  


### 🚀 核心方法（介绍本文的几个创新点）
针对上述问题，论文提出 **Factuality-aware Step-wise Policy Optimization（FSPO，面向事实性的分步策略优化）**，核心是在**每一步推理中显式融入事实性验证**，引导模型生成更可靠的推理链。  

💡 创新点1：分步事实性奖励机制  
FSPO不再只看最终结果，而是对**推理过程的每个步骤**做事实性验证。具体来说，用自动化验证器（verifier）检查每个生成的推理语句是否能被给定证据支持（即是否“符合事实”），得到“分步事实性分数”。这些分数被整合到整体奖励信号中，动态调整每个token层面的优势值（advantage values）——奖励事实正确的token，惩罚错误token。  

💡 创新点2：解决RL训练的稀疏性与不稳定性  
传统RL依赖“最终结果是否正确”的稀疏奖励，而FSPO通过**每一步的事实性反馈**提供更密集、信息更丰富的奖励信号。这不仅缓解了“稀疏奖励导致训练不稳定”的问题，还能引导策略学习“既正确、推理链又可验证”的解，从训练过程直接减少幻觉。  


### 📈 实验结果
论文在数学推理和幻觉基准测试中，基于Qwen2.5和Llama系列模型展开实验，验证FSPO的效果：  
- **幻觉减少**：在TruthfulQA、HaluEval、HalluQA等幻觉基准上，经FSPO微调的模型幻觉率显著降低；  
- **推理精度提升**：数学推理任务中（如复杂数学题求解），模型不仅更“诚实”，推理正确率也同步提高；  
- **中间步骤事实性增强**：对推理中间步骤的分析显示，FSPO能在不牺牲生成质量的前提下，提升每一步推理的事实正确性。  


### 💬 可借鉴之处
1. **推理与事实性的平衡思路**：以往RL微调侧重“结果正确”，FSPO则证明“过程监督（分步事实性验证）”能有效弥补结果监督的缺陷，为大模型推理能力与可靠性的协同优化提供了新思路；  
2. **奖励机制的精细化设计**：通过“分步+事实性”的奖励塑造，解决RL稀疏奖励与训练不稳定问题，这种“把大目标拆分为可验证的小步骤并反馈”的思路，可迁移到其他需要过程可靠性的生成任务（如代码生成、多轮对话）；  
3. **理论与实践结合**：先从理论分析RL训练动态中导致幻觉的根因，再针对性设计算法，这种“问题诊断→方法设计→实验验证”的研究范式，对解决大模型其他鲁棒性问题（如偏见、逻辑错误）具有参考价值。  

总之，FSPO为大模型在推理任务中“更准且更诚实”提供了一套可行方案，也启发我们思考：强化学习不仅要优化“能力”，更要优化“可靠性”，才能让大模型真正在真实场景中可信可用。
```

## rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation
### Abstract
Retrieval-augmented generation (RAG) has shown great promise for
knowledge-intensive tasks and recently advanced with agentic RAG, where
language agents engage in multi-round interactions with external knowledge
sources for adaptive information retrieval. However, existing agentic RAG
methods often depend on ad-hoc prompt engineering and lack a unified
optimization framework. We introduce RAG-Gym, a comprehensive platform that
systematically explores three optimization dimensions: (1) prompt engineering,
(2) actor tuning, and (3) critic training. For prompt engineering, we propose
Re$^2$Search, a novel agent incorporating reasoning reflection that
significantly outperforms standard prompts. In actor tuning, we evaluate three
popular post-training algorithms with fine-grained process supervision and
identify direct preference optimization as the most effective. We further
demonstrate that a trained critic can enhance inference by selecting
higher-quality intermediate reasoning steps. Together, these findings lead to
the optimized Re$^2$Search++ agent, which surpasses most recent methods like
Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we
examine the impact of different reward sources and analyze scaling properties
in training and inference, offering practical insights for agentic RAG
optimization. The project homepage is available at https://rag-gym.github.io.
```
### 🌟 论文解读 | RAG-Gym：为检索增强生成打造语言智能体的系统优化平台

### 📌 背景痛点/本文动机
大语言模型（LLMs）在面对知识密集型问题时，若缺乏足够或最新的领域知识，容易给出不准确回答甚至产生幻觉。检索增强生成（RAG）通过结合信息检索（IR）系统的相关信息来改善这一情况，而智能体化的RAG（agentic RAG）让语言智能体与外部知识源多轮交互以实现自适应信息检索，进一步提升了效果。但现有agentic RAG方法存在依赖临时prompt工程、缺乏统一优化框架的问题；同时，虽有LLM后训练算法，却难直接适配agentic RAG动态调整token生成策略的需求，且对中间步骤的细粒度监督不足。因此，本文旨在构建系统框架来优化agentic RAG。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RAG-Gym综合平台  
RAG-Gym从prompt工程、actor调优、critic训练三个维度系统探索agentic RAG的优化。将知识密集型问答建模为高层马尔可夫决策过程（MDP），明确中间动作，为语言智能体优化提供模块化方法，涵盖了智能体与IR系统交互的状态、动作、环境和奖励等要素定义，支撑细粒度过程监督与优化方法的系统评估。

💡 创新点2：Prompt工程层面提出Re²Search智能体  
设计了融入推理反思（reasoning reflection）的Re²Search智能体，相比标准prompt在性能上有显著超越，让智能体在与IR系统交互过程中能更智能地生成查询等动作来辅助回答问题。

💡 创新点3：Actor调优与Critic训练的探索  
在actor调优中，用细粒度过程监督评估三种流行后训练算法，发现直接偏好优化（Direct Preference Optimization）效果最佳；同时证明训练critic来选择更高质量中间推理步骤能提升推理效果。整合这些成果得到优化后的Re²Search++智能体。

### 📈 实验结果
优化后的Re²Search++智能体在具有挑战性的知识密集型任务上超越现有方法，平均F1相对提升3.2% - 11.6%，在未见过的数据集上甚至有8.5% - 24.7%的提升；此外还分析了不同奖励源影响以及训练和推理时的缩放特性等，验证了各优化维度和方法的有效性。

### 💬 可借鉴之处
1. 多维度系统优化思路：从prompt、actor、critic三个维度系统优化agentic RAG，为复杂智能体系统优化提供了“分模块+协同”的范例，可启发其他需多环节协作的AI系统优化。  
2. 细粒度监督价值：强调对agentic RAG中间步骤的细粒度过程监督，证明其对性能提升的重要性，在后续类似需动态交互、多步推理的任务型智能体研发中，可重视中间过程的监督设计。  
3. 算法选型与融合：对不同后训练算法在agentic RAG场景下的测试与优选（如发现直接偏好优化更优），以及critic训练辅助推理的思路，为模型训练策略选择和模块配合提供了实践参考。 
4. 实践指导意义：对奖励源、训练与推理缩放特性的分析，给后续agentic RAG优化提供了可落地的实用洞见，便于研究者和工程师在实际项目中权衡决策。
```

## from-passive-to-active-reasoning--can-large-language-models-ask-the-right-questions-under-incomplete-information-
### Abstract
While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.
```
### 🌟 论文解读 | 从被动到主动推理：大语言模型在信息不全时能否问对问题？

### 📌 背景痛点/本文动机
现有基准主要评估大语言模型（LLMs）的被动推理能力，即给模型提供解决问题所需的全部信息让其推导答案。但现实中很多场景是信息不完整的，需要模型主动与外部系统交互获取缺失信息（主动推理），而这方面缺乏系统研究。为填补此空白，论文提出AR - Bench基准来评估LLMs的主动推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建AR - Bench基准
AR - Bench包含侦探案例（detective cases）、情境谜题（situation puzzles）、猜数字（guessing numbers）三个任务家族，分别对应常识推理、逻辑推理、符号推理，模拟现实中智能体场景，评估模型在信息不全时主动获取信息并解决问题的能力。
💡 创新点2：聚焦主动推理评估维度
针对AR - Bench中的任务，从“提出的问题质量”和“最终解决方案质量”两个维度量化评估模型的主动推理能力，要求模型在与外部“回答者”多轮交互中主动获取关键线索来解题。

### 📈 实验结果
- 性能差距明显：当前最先进的LLMs（如GPT - 4o）在AR - Bench上表现不佳，猜数字任务精确匹配率低至35%，而人类评估者表现远超模型。
- 交互轮次收益变化：模型在交互前几轮进步快（5 - 10轮过程分数+7.7%），后续轮次（20 - 25轮）进步减缓（+2.5%），随提问轮次增加收益递减。
- 组件缺陷：不可靠的验证器和低质量问题生成严重限制基于搜索的策略，且验证器效果因任务而异。
- 缩放限制：更大模型和更多交互轮次比小模型有可测量改进，但仍无法完全解决主动推理任务。
- 方法与指令失效：像SFT、DPO、Tree - of - Thought、人工编写指令等常见方法收效甚微。
- 任务特定错误模式：模型常问模糊或重复问题，不同任务有特定错误，如侦探案例中时间线误解、情境谜题中无根据假设、猜数字中反馈误解等。

### 💬 可借鉴之处
- 提供新基准：AR - Bench为研究大语言模型主动推理能力提供了全新且系统的基准，推动领域从传统被动推理评估向主动推理评估拓展，为后续研究指明新方向。
- 揭示能力短板：清晰展现当前LLMs在主动推理上的不足，让研究者明确需改进方向，如后续可围绕交互式学习、实时反馈循环、感知环境的训练目标等方面提升主动推理方法。
- 公开资源助力：AR - Bench开源（https://github.com/tmlr - group/AR - Bench），方便研究者基于此基准开展实验，共同推动大语言模型推理能力边界突破。
```

## speed-rl--faster-training-of-reasoning-models-via-online-curriculum-learning
### Abstract
Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.
```
### 🌟 论文解读 | SPEED-RL：在线课程学习加速推理模型训练

### 📌 背景痛点/本文动机
在大语言模型（LLM）训练中，结合强化学习（RL）与可验证奖励能显著提升模型推理能力，但均匀提示采样效率低下，导致计算成本高昂。传统方法在数据选择上依赖大量人工操作，可扩展性和灵活性受限；课程学习虽在部分领域有效，但在LLM推理任务的RL训练中效果不明。因此，亟需更高效的策略来加速基于RL的推理模型训练，降低计算开销同时不牺牲性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：理论层面揭示中间难度提示价值  
论文严谨论证了在RL框架下，**中间难度的提示**能提供最强学习信号。通过分析提示通过率（模型正确解决问题的概率）与随机梯度估计器信噪比（SNR）的关系，发现通过率接近0%或100%的提示会让梯度估计器SNR严重下降，梯度受噪声主导，采样意义不大；而中间难度提示可提升SNR，加速收敛。且该结论适用于多种主流策略梯度算法（如REINFORCE、GRPO、PPO等），泛化性强。

💡 创新点2：高效在线课程学习框架SPEED  
受理论启发，提出**Selective Prompting with Efficient Estimation of Difficulty（SPEED）** 框架，动态选择最优难度提示用于RL训练。具体实现上，先通过轻量统计测试快速估计提示难度，再对合适难度的提示执行全推理，减少推理阶段（RL训练中计算最密集的部分）的开销。该框架无需手动数据预处理或特殊辅助神经组件，能无缝集成到主流RL算法中，适配多种带二元可验证奖励的任务。

### 📈 实验结果
在标准数学推理基准测试中，基于Qwen2.5-Math-7B模型的实验表明：SPEED使RL训练速度提升2到6倍，且不降低精度；通过排除过易或过难提示，聚焦中等难度（高SNR）提示，有效减少了推理成本，验证了方法在加速训练与保持性能上的优势。

### 💬 可借鉴之处
1. **理论指导实践**：将提示难度与梯度估计器SNR关联，为高效采样提供理论依据，这种从理论到方法的推导思路值得借鉴，帮助在算法设计时锚定核心性能影响因子。  
2. **轻量高效设计**：SPEED通过轻量统计测试和优化预取机制降低推理开销，无需复杂额外组件，在工程实现上为高效训练框架设计提供了“以简驭繁”的思路。  
3. **通用性与扩展性**：适配多种RL算法与任务类型，体现了方法在不同场景下的普适性，为后续跨任务、跨算法的训练加速研究提供了参考范式。  
```

## treerpo--tree-relative-policy-optimization
### Abstract
Large Language Models (LLMs) have shown remarkable reasoning capabilities
through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,
a key limitation of existing approaches is that rewards defined at the full
trajectory level provide insufficient guidance for optimizing the intermediate
steps of a reasoning process. To address this, we introduce \textbf{\name}, a
novel method that estimates the mathematical expectations of rewards at various
reasoning steps using tree sampling. Unlike prior methods that rely on a
separate step reward model, \name directly estimates these rewards through this
sampling process. Building on the group-relative reward training mechanism of
GRPO, \name innovatively computes rewards based on step-level groups generated
during tree sampling. This advancement allows \name to produce fine-grained and
dense reward signals, significantly enhancing the learning process and overall
performance of LLMs. Experimental results demonstrate that our \name algorithm
substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test
benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name
significantly outperforms GRPO by 2.9\% in performance while simultaneously
reducing the average response length by 18.1\%, showcasing its effectiveness
and efficiency. Our code will be available at
\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.
```
### 🌟 论文解读 | TreeRPO：用树采样重塑大模型推理优化的奖励信号

### 📌 背景痛点/本文动机
大语言模型（LLMs）借助带可验证奖励的强化学习（RLVR）在推理能力上取得显著进展，但现有方法存在关键局限：**轨迹级奖励对推理中间步骤的优化指导不足**。在基于奖励模型的方法中，过程奖励模型（PRM）虽能提供细粒度奖励，但高质量标注数据获取难；而无奖励模型的方法（如GRPO）仅提供轨迹级奖励，对中间步骤优化助力有限。如何在不依赖奖励模型的前提下，为推理过程提供密集、细粒度的奖励信号，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：树采样实现无奖励模型的过程奖励估计  
TreeRPO 提出**树采样机制**，无需显式的步骤奖励模型，直接通过树采样来估计各推理步骤奖励的数学期望。它突破了传统方法依赖单独步骤奖励模型的限制，用采样过程本身完成奖励估计，为中间推理步骤提供更直接的反馈。  

💡 创新点2：基于GRPO框架的步骤级组相对奖励计算  
在GRPO的组相对奖励训练机制基础上，TreeRPO创新性地**基于树采样生成的步骤级分组来计算奖励**。这一设计让模型能产生细粒度、密集的奖励信号，在保留可验证奖励函数扩展性优势的同时，更高效地引导推理过程优化。  


### 📈 实验结果
1. 推理准确率提升显著：在Qwen - 2.5 - Math的测试基准中，Pass@1准确率从19.0%大幅提升至35.5%；  
2. 性能与效率双优：对比GRPO，TreeRPO性能领先2.9%，同时平均响应长度减少18.1%，证明在提升推理精度的同时，还能更高效地利用token资源；  
3. 可视化对比（图1）：在MATH - 500、OlympiadBench等四个数学基准测试中，TreeRPO的Pass@1准确率随训练步骤推进，始终优于GRPO，展现持续优化能力。  


### 💬 可借鉴之处
1. 无奖励模型的细粒度奖励设计思路：TreeRPO证明了在不依赖昂贵标注数据构建奖励模型的情况下，通过采样与分组机制也能生成过程级奖励，为资源受限场景下的RL与LLM结合提供新路径；  
2. 强化学习在推理优化的创新应用：将树结构采样与组相对奖励结合，为复杂推理任务中“中间步骤优化”这一难题提供了可落地的技术方案，启发后续在RL与LLM推理结合方向的研究；  
3. 性能与效率的平衡：实验中同时实现准确率提升与响应长度缩短，体现了方法在实际部署中的价值，为追求“高效推理”的工业级应用提供参考。  
```

## effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability
### Abstract
Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.
```
### 🌟 论文解读 | 解决RAG两大痛点！ARENA用强化学习让生成更有效、更透明

### 📌 背景痛点/本文动机
Retrieval - Augmented Generation（RAG）在提升大语言模型（LLMs）处理知识密集型任务表现上效果显著，但仍存在两大未解决的挑战：
1. **有效性不足**：现有研究多聚焦于打造更强的RAG检索器，却忽略了如何提升生成器（LLM）利用检索信息进行推理和生成的能力。实际测试发现，在多跳问答基准测试中，相同检索上下文下7B规模的模型，回答准确率比OpenAI - o1、DeepSeek - R1等推理模型低15 - 35%，这表明生成器的推理能力是当前RAG pipeline的关键瓶颈。
2. **透明度缺失**：多数RAG方法未关注哪些检索内容真正对推理过程有贡献，导致可解释性和可见性不足。很多生成器输出无结构答案，隐藏了决策过程，降低了可信度，在处理多文档多跳推理时，小模型因推理能力有限更难应对噪声或冗余上下文。
同时，现有基于强化学习（RL）的方法，奖励设计通用、输出格式未考虑多跳QA结构，且KL正则化不稳定易导致训练发散，限制了在检索类任务中的应用。基于此，论文提出ARENA框架来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ARENA框架，实现透明且有效的RAG生成
ARENA（Adaptive - Rewarded Evidence Navigation Agent）是一个基于强化学习训练的透明RAG生成器框架。它引入结构化输出格式，包含选定的参考资料、明确的推理轨迹和最终答案，实现端到端的可解释性。通过这种结构化生成，模型能识别关键证据、进行结构化推理并生成带有可解释决策轨迹的答案。
💡 创新点2：设计自适应任务特定奖励与稳定优化策略
为多跳问答任务定制了一套自适应的、特定任务的奖励机制，从格式、准确性、相关性和额外奖励等多个维度评估模型输出，提供可解释且细粒度的训练信号。同时改进KL公式实现KL稳定化，解决了现有RL方法中KL正则化不稳定导致训练发散的问题，让训练更稳定适用于检索类任务。

### 📈 实验结果
将ARENA应用于Qwen2.5 - 7B - Instruct和Llama3.1 - 8B - Instruct等开源模型，在多个RAG基线的大量实验表明：
1. 在所有多跳QA数据集上，模型准确率提升了10 - 30%，性能可与OpenAI - o1、DeepSeek - R1等商业开发的SOTA大语言模型媲美。
2. 进一步分析显示，ARENA在新数据集上无需额外训练就能很好地适配，具有很强的灵活性，在数据集和模型 backbone 上泛化性良好。

### 💬 可借鉴之处
1. **关注生成器优化**：论文指出当前RAG系统中生成器推理能力不足是关键问题，提醒研究者们除了检索器，要重视生成器的优化，为RAG研究提供了新的关注方向。
2. **强化学习在RAG的应用模式**：展示了通过强化学习，结合结构化生成、自适应奖励设计和稳定训练来提升RAG推理的透明性和有效性的模式，为后续利用RL优化RAG生成器提供了参考框架。
3. **开源资源贡献**：公开了模型和代码，方便后续研究者在此基础上对RAG生成器优化进行研究，推动该领域发展。
```

## sws--self-aware-weakness-driven-problem-synthesis-in-reinforcement-learning-for-llm-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.
```
### 🌟 论文解读 | SwS：大模型推理强化学习中，基于自我感知弱点驱动的问题合成框架

### 📌 背景痛点/本文动机
在大语言模型（LLM）的复杂推理任务（如数学解题）训练中，带可验证奖励的强化学习（RLVR）已被证明有效。但RLVR规模化应用需要高质量、答案精确可验证的问题集。现有问题存在诸多不足：一是精心设计的人工标注数学题稀缺，构建大规模带精确参考答案的数据集成本高；二是多数面向蒸馏的合成数据集参考答案验证不足，不适合依赖答案正确性作为训练信号的RLVR；三是现有问题合成策略不考虑模型能力，盲目扩充问题集，生成有用问题效率低。且在强化学习中，训练任务难度需与模型当前能力匹配，否则会导致梯度消失等问题，而模型持续失败的案例能反映弱点，可用于针对性改进，因此需要一种利用失败案例来合成问题的方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出SwS框架
提出Self - aware Weakness - driven problem Synthesis（SwS）框架，系统识别模型缺陷并用于问题扩充。在RL训练初步阶段，记录模型通过迭代采样持续难以解决或学习效率低的问题（即弱点问题），按类别分组这些失败案例，提取核心概念，合成适合模型能力难度的新问题，后续用这些新问题增强训练，让模型聚焦并克服弱点。
💡 创新点2：基于性能的资源分配
为提升训练中弱点缓解效率，根据模型在不同类别上的相对性能分配每个类别（对应不同弱点领域）的扩充预算，使问题合成更有针对性。

### 📈 实验结果
在3B到32B不同规模模型上，在8个主流数学推理基准测试中评估。7B模型平均性能提升10.0%，32B模型平均提升7.7%；训练后，模型在原本弱领域持续失败的问题上，能多解决高达20.0%的问题；在所有基准测试中，基于扩充问题集训练的模型持续超过基础模型和原始数据集训练的模型，甚至超过精心策划的人工标注问题集训练的对应模型。此外还探索了SwS在Weak - to - Strong Generalization、Self - evolving、Weakness - driven Selection等设置下的潜力，验证了框架的鲁棒性和适应性。

### 💬 可借鉴之处
1. 思路创新：将模型自身的失败案例转化为改进资源，通过识别弱点来生成针对性训练数据，为强化学习训练数据生成提供了新的自驱动思路，不依赖外部知识蒸馏，让模型自我识别和解决RL中的弱点以实现鲁棒泛化。
2. 方法落地：提出的SwS框架从弱点识别、概念提取到问题合成、资源分配都有清晰流程，可借鉴这种系统利用模型自身表现来优化训练数据的方式，应用到其他需要数据驱动优化的模型训练任务中。
3. 实验全面：在多规模模型和多基准测试上验证，还探索多种扩展设置，这种全面验证方法和探索拓展性的思路，可为相关研究的实验设计提供参考，以更充分验证方法有效性和泛化性。
```

## r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning
### Abstract
Large language models (LLMs) have notably progressed in multi-step and
long-chain reasoning. However, extending their reasoning capabilities to
encompass deep interactions with search remains a non-trivial challenge, as
models often fail to identify optimal reasoning-search interaction
trajectories, resulting in suboptimal responses. We propose R-Search, a novel
reinforcement learning framework for Reasoning-Search integration, designed to
enable LLMs to autonomously execute multi-step reasoning with deep search
interaction, and learn optimal reasoning search interaction trajectories via
multi-reward signals, improving response quality in complex logic- and
knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when
to retrieve or reason, while globally integrating key evidence to enhance deep
knowledge interaction between reasoning and search. During RL training,
R-Search provides multi-stage, multi-type rewards to jointly optimize the
reasoning-search trajectory. Experiments on seven datasets show that R-Search
outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%
(out-of-domain). The code and data are available at
https://github.com/QingFei1/R-Search.
```
### 🌟 论文解读 | R-Search：用多奖励强化学习赋能LLM推理与搜索深度交互

### 📌 背景痛点/本文动机
大语言模型（LLMs）在多步骤和长链推理任务中取得了显著进展，但要让其推理能力与搜索深度交互仍面临挑战。现有模型常难以找到最优的“推理 - 搜索”交互轨迹，导致输出质量欠佳。在复杂的逻辑密集型和知识密集型任务（如多跳问答）中，传统方法存在两大局限：一是模型内部决定的检索时机未必契合实际需求；二是推理与搜索的模块化设计限制了外部知识与推理链的深度交互，易让模型基于局部信息做决策，最终影响输出质量。因此，如何让LLM动态整合外部知识、学习最优交互轨迹成为关键问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：R - Search框架设计——强化学习驱动推理与搜索深度整合  
提出R - Search这一基于强化学习（RL）的新颖框架，让LLM能动态交错执行多步骤推理与搜索。LLM可在任意token级推理步骤触发检索，将检索内容无缝融入推理过程，实现推理与外部知识的深度耦合；交互后，LLM还能通过推理把检索文档提炼为证据，从全局视角重新评估和构建关键知识，聚焦解决任务的核心事实。该框架能引导LLM保障中间推理的合理性与检索知识的完整性，联合优化RAG中复杂的“推理 - 搜索”轨迹。  

💡 创新点2：多阶段多类型奖励机制——引导学习最优交互序列  
设计了包含答案质量、证据质量、格式正确性等的多阶段、多类型奖励机制。这些互补的奖励信号推动模型学习最优“推理 - 搜索”交互序列，其中证据奖励能促使模型关注关键中间推理步骤的事实质量，助力构建更稳健的推理路径，减少走捷径或臆测行为的风险。  

💡 创新点3：R - Search - as - a - Tool（RSTool）——模块化与实用拓展  
提出RSTool，将推理中高质量证据模块化封装为可迁移组件，把复杂且耗时的“推理 - 搜索”交互卸载到本地部署，具备很强的实际可扩展性。  

### 📈 实验结果
在7个涵盖多跳和单跳问答任务的数据集上开展实验，结果显示R - Search在域内（in - domain）比先进RAG基线最多领先32.2%，在域外（out - of - domain）最多领先25.1%。此外，消融实验和训练动态分析等进一步验证了证据整合与多奖励建模的有效性，还揭示了不同RL算法下的性能趋势与检索行为等洞察。  

### 💬 可借鉴之处
1. 框架设计思路：将强化学习引入RAG场景来优化“推理 - 搜索”轨迹，为解决复杂任务中模型与外部知识深度交互问题提供了新范式，启示在需动态交互的任务中可探索RL驱动的框架设计。  
2. 奖励机制构建：多维度、多阶段的奖励设计思路，能为强化学习中引导智能体学习提供更全面的信号参考，可借鉴到需多目标优化的任务场景。  
3. 工具化落地：RSTool的模块化思路，为复杂AI能力向实用工具转化、降低部署成本提供了参考，利于推动技术在实际场景的落地应用。  
```

## pangu-deepdiver--adaptive-search-intensity-scaling-via-open-web-reinforcement-learning
### Abstract
Information seeking demands iterative evidence gathering and reflective
reasoning, yet large language models (LLMs) still struggle with it in open-web
question answering. Existing methods rely on static prompting rules or training
with Wikipedia-based corpora and retrieval environments, limiting adaptability
to the real-world web environment where ambiguity, conflicting evidence, and
noise are prevalent. These constrained training settings hinder LLMs from
learning to dynamically decide when and where to search, and how to adjust
search depth and frequency based on informational demands. We define this
missing capacity as Search Intensity Scaling (SIS)--the emergent skill to
intensify search efforts under ambiguous or conflicting conditions, rather than
settling on overconfident, under-verification answers.
  To study SIS, we introduce WebPuzzle, the first dataset designed to foster
information-seeking behavior in open-world internet environments. WebPuzzle
consists of 24K training instances and 275 test questions spanning both
wiki-based and open-web queries. Building on this dataset, we propose
DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by
encouraging adaptive search policies through exploration under a real-world
open-web environment. Experimental results show that Pangu-7B-Reasoner
empowered by DeepDiver achieve performance on real-web tasks comparable to the
671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from
cold-start supervised fine-tuning to a carefully designed RL phase, and present
that its capability of SIS generalizes from closed-form QA to open-ended tasks
such as long-form writing. Our contributions advance adaptive information
seeking in LLMs and provide a valuable benchmark and dataset for future
research.
```
### 🌟 论文解读 | 深度解析Pangu DeepDiver：基于开放网络强化学习的自适应搜索强度缩放

### 📌 背景痛点/本文动机
在人工智能领域，大语言模型（LLMs）在开放网络问答中进行信息搜索时仍面临挑战。信息搜索需要迭代收集证据与反思推理，但现有方法依赖静态提示规则，或基于维基百科语料和检索环境训练，对充满模糊性、冲突证据和噪声的真实网络环境适应性有限。这导致LLMs难以动态决定何时何地搜索，以及根据信息需求调整搜索深度与频率。为此，论文提出“搜索强度缩放（SIS）”这一概念，即在模糊或冲突条件下加强搜索努力，而非满足于过度自信、验证不足的答案，旨在解决LLMs在信息搜索方面的短板。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建WebPuzzle数据集  
为研究SIS，论文推出WebPuzzle数据集，这是首个用于培养开放网络环境下信息搜索行为的数据集。它包含24K训练实例与275个测试问题，涵盖基于维基和开放网络的查询，能有效评估LLMs在真实搜索环境中的信息搜索能力，为后续研究提供了有价值的基准。  

💡 创新点2：提出DeepDiver强化学习框架  
基于WebPuzzle数据集，论文提出DeepDiver强化学习框架。该框架通过在真实开放网络环境中探索，鼓励自适应搜索策略，从而促进SIS能力。其训练流程从冷启动的有监督微调，到精心设计的强化学习阶段，让模型能在真实网络交互中持续优化检索文档的去噪与信息定位，提升回答准确性。  

### 📈 实验结果
实验表明，搭载DeepDiver的Pangu - 7B - Reasoner在真实网络任务上的性能可与671B参数的DeepSeek - R1媲美。同时，DeepDiver的SIS能力从封闭式问答泛化到如长篇写作等开放式任务，验证了方法在不同任务场景下的有效性与泛化性。此外，分析还揭示DeepDiver的搜索深度和频率与问题难度、模型性能成正比；相比基于维基的“干净”环境，WebPuzzle和真实开放网络环境更能支持复杂推理行为；强化学习训练显著增强了LLMs的泛化能力，助力从封闭式问题向开放式问题过渡。  

### 💬 可借鉴之处
1. 数据集构建角度：WebPuzzle为评估真实网络环境下LLMs信息搜索能力提供了新基准，后续研究可参考其思路，构建更贴合真实场景、多领域覆盖的数据集，推动信息搜索相关研究。  
2. 方法框架角度：DeepDiver将强化学习引入迭代检索 - 生成框架，并在真实网络环境训练，为提升LLMs自适应信息搜索能力提供了新范式。其他研究者可借鉴其强化学习与真实环境交互结合的思路，探索更优的模型训练与优化方式。  
3. 能力泛化角度：DeepDiver展现的SIS能力泛化性，启示研究需关注模型在不同任务类型（从封闭到开放）下的迁移能力培养，设计更具通用性的训练策略。  
```

## don-t-think-longer--think-wisely--optimizing-thinking-dynamics-for-large-reasoning-models
### Abstract
While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.
```
### 🌟 论文解读 | 大推理模型别“想太多”，要“想聪明”：优化思维动态提升效率与精度

### 📌 背景痛点/本文动机
大推理模型（LRMs）借助强化学习优化最终答案准确率，在推理能力上取得显著进展，但也出现了“过度思考（overthinking）”问题：推理路径不必要地复杂冗长，既浪费计算资源，还可能降低性能。究其根源，是模型难以在推理过程中动态选择合适的**模块化推理策略（即“思维模式”）**。比如，模型可能在该快速验证的环节绕远路，或在该深入分析时却重复无效步骤。因此，论文希望通过优化“思维动态”，让模型更聪明地选择思维模式，在保证精度的同时提升效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出动态思维模式优化框架（DTO）  
论文把“提升推理效率”转化为**带约束的优化问题**——在保证任务表现的前提下，最小化推理轨迹的计算成本。具体来说，DTO先把模型生成的推理路径切分成一个个可识别的“思维模式片段”（比如假设生成、自我验证、中间总结等功能模块）；接着评估每个片段对最终结果的贡献，区分“有益模式”和“有害模式”；最后通过保留/强化有益片段、修剪有害片段，把冗长推理路径压缩成更简洁有效的轨迹，甚至能把原本错误的推理修正为正确结果。

💡 创新点2：结合偏好优化与对比数据集  
为了让模型主动学习“更优思维模式”，论文构建了**成对对比数据集**（包含“次优推理路径”和“最优推理路径”），再用偏好优化技术引导模型优先采用高效且有效的思维模式。这种方式不像传统方法只靠启发式截断或简单统计token长度，而是显式地对推理片段的贡献建模、优化，精准提升效率。

### 📈 实验结果
论文在多个数学推理基准测试中验证方法：  
- 效率层面：优化后的推理路径能减少注意力计算量（FLOPs）**最高达47%**，同时保持原本正确回答的精度；token使用量从约5000降到3000，计算开销显著降低。  
- 精度层面：原本错误的回答中有相当比例被修正，**准确率提升最高15.6%**；在多基准测试中，整体准确率最多能比原始LRM提升12%。  

简言之，模型“想”得更短、更聪明，精度还能涨。

### 💬 可借鉴之处
1. **问题建模视角**：把“推理效率”当作带约束的优化问题，跳出“只看结果精度”的思维定式，关注推理过程中“每一步是否必要”，为大模型推理效率优化提供了新范式。  
2. **模块化思维模式**：将推理路径拆解为“功能化片段”（思维模式），通过分析各片段的贡献来优化，这种“分而治之+精准取舍”的思路，可迁移到代码生成、多步决策等需长程推理的任务。  
3. **偏好优化+对比数据**：用成对数据引导模型学习“更优路径”，为大模型的“过程优化”提供了落地手段，后续可拓展到更多领域（如逻辑推理、创意生成）的推理轨迹优化。  
```

## pitfalls-of-rule--and-model-based-verifiers----a-case-study-on-mathematical-reasoning
### Abstract
Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.
```
### 🌟 论文解读 | 基于规则与模型的验证器在数学推理中的陷阱剖析

### 📌 背景痛点/本文动机
强化学习（RL）结合可验证奖励（RLVR）是推动大语言模型复杂推理能力提升的核心方法，像DeepSeek - R1这类大推理模型都以此为基础。在数学推理等复杂领域，基于规则的验证器被广泛用于训练强推理模型，然而这些验证器的可靠性以及对RL训练过程的影响却知之甚少。比如基于规则的验证在RL项目中准确性如何？错误验证是否会显著影响RL性能？同时，为解决基于规则验证器的不足而探索的基于模型的验证器，其在RL训练中又存在怎样的问题？这些都是本文要探究的动机所在。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：全面分析现有基于规则的验证器
对多个广泛使用的开源数学数据集上的现有基于规则验证器进行全面分析，探究其在静态、基于分类的评估中的表现，揭示其在识别不同格式等效答案时存在的不足，如在常用数学数据集上存在不可忽视的假阴性率，且随着生成模型变强假阴性率呈上升趋势。
💡 创新点2：探索基于模型的验证器并分析其在RL训练中的问题
利用现成的开源模型以及训练新模型来探究基于模型的验证器，对比其与基于规则验证器在分类评估中的表现；同时在RL训练实验中观察基于模型的验证器带来的独特挑战，如是否易受“奖励黑客攻击”，并分析验证器分类准确率与抗奖励黑客攻击能力之间的关系；还通过构建对抗模式来探究基于模型验证器的漏洞。

### 📈 实验结果
1. 基于规则的验证器：在静态分类评估中，当回答与真实答案格式相近时识别效果好，但在生成答案更具多样性或处于长尾分布时表现不佳，平均召回率仅86%，且生成模型越强假阴性率越高，对RL训练性能有不利影响。
2. 基于模型的验证器：静态评估中分类准确率显著高于基于规则的验证器，如在Skywork - OR1数据集上召回率从84%提升到92%；但在RL训练中表现不一，部分能比基于规则的验证器平均提升超3个绝对点，部分易受黑客攻击导致RL训练结果不佳，且训练后的验证器虽静态分类准确率高但在RL训练中更易被攻击，说明验证器分类准确率不能反映其抗奖励黑客攻击能力。
3. 对抗模式测试：构建如插入空字符或乱码文本等对抗模式测试基于模型的验证器，发现多数验证器易被欺骗，判别式验证器比生成式更鲁棒，微调虽能提高静态评估分数但不一定增强鲁棒性，甚至在某些情况下会降低鲁棒性。

### 💬 可借鉴之处
1. 对于研究强化学习中奖励系统的学者，本文揭示了基于规则和基于模型的验证器在数学推理场景下的固有风险，为开发更鲁棒的奖励系统提供了有价值的见解，如在设计验证器时需考虑不同格式等效答案的识别以及抗攻击能力等。
2. 对于从事大语言模型数学推理训练的从业者，了解到基于规则验证器的不足和基于模型验证器的优缺点，在选择验证器时可更有针对性，同时在优化模型过程中要警惕奖励黑客攻击问题。
3. 在验证器的改进方向上，本文的研究表明追求更准确的基于模型的验证器是提升RL性能的有前景方向，但需解决其易受攻击的问题，为后续验证器的研究和改进指明了方向，如针对对抗模式增强验证器的鲁棒性等。
```

## learning-to-route-queries-across-knowledge-bases-for-step-wise-retrieval-augmented-reasoning
### Abstract
Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in
mitigating hallucinations in Multimodal Large Language Models (MLLMs) by
incorporating external knowledge during generation. Existing MRAG methods
typically adopt a static retrieval pipeline that fetches relevant information
from multiple Knowledge Bases (KBs), followed by a refinement step. However,
these approaches overlook the reasoning and planning capabilities of MLLMs to
dynamically determine how to interact with different KBs during the reasoning
process. To address this limitation, we propose R1-Router, a novel MRAG
framework that learns to decide when and where to retrieve knowledge based on
the evolving reasoning state. Specifically, R1-Router can generate follow-up
queries according to the current reasoning step, routing these intermediate
queries to the most suitable KB, and integrating external knowledge into a
coherent reasoning trajectory to answer the original query. Furthermore, we
introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored
reinforcement learning algorithm that assigns step-specific rewards to optimize
the reasoning behavior of MLLMs. Experimental results on various open-domain QA
benchmarks across multiple modalities demonstrate that R1-Router outperforms
baseline models by over 7%. Further analysis shows that R1-Router can
adaptively and effectively leverage diverse KBs, reducing unnecessary
retrievals and improving both efficiency and accuracy.
```
### 🌟 论文解读 | 让多模态大模型“聪明检索”：R1 - Router 实现动态知识库路由与分步推理增强

### 📌 背景痛点/本文动机
在多模态大语言模型（MLLMs）领域，多模态检索增强生成（MRAG）本是缓解模型幻觉的有效手段，通过生成时融入外部知识来提升表现。但现有 MRAG 方法存在局限：采用“静态检索 pipeline + 后续精修”的模式，只做一次性从多知识库（KBs）取信息，却忽略了 MLLMs 自身的推理与规划能力——没法在推理过程中**动态决定**该和哪些知识库交互、何时交互。比如面对复杂多模态问答，固定从文本库或图像库取信息，可能无法按需灵活组合多源知识，既影响答案准确性，也易做无用检索。此外，已有迭代检索策略虽尝试分解查询、子查询检索，但要么依赖单模态优先的预定义流程不够灵活，要么没充分发挥模型推理能力；而结合大推理模型（LRMs）或强化学习（RL）的工作，又多聚焦文本模态，缺乏多模态感知，难以应对多模态查询选合适知识库的问题。所以，如何让 MLLMs 依据推理状态动态选“何时、向哪（知识库）”检索，成了关键痛点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：R1 - Router 框架——动态规划多知识库检索路径  
R1 - Router 是全新的 MRAG 框架，核心是让 MLLMs 基于**动态变化的推理状态**，自主决定何时、向哪个知识库去检索知识。具体流程是：当推理过程需要补充知识时，生成“中间查询”，再把这个中间查询路由到最适配的知识库；然后把从该知识库拿到的外部知识，整合进连贯的推理轨迹，最终回答原始问题。打个比方，若要解决“图中鸟类的最近亲分类阶元”，R1 - Router 会先推理出“得先知道这鸟叫啥”，生成对应中间查询，选图像 - 文本混合知识库去查名称；拿到名称后，再决定是否要去文本库查分类信息，一步步推进直到产出最终答案。这种“按需生成子查询 + 动态选库 + 迭代推理”的模式，突破了静态检索的死板，让知识获取和推理深度绑定。

💡 创新点2：Step - GRPO 强化学习算法——为分步推理定制奖励优化  
为了高效训练 R1 - Router，论文提出 Step - wise Group Relative Policy Optimization（Step - GRPO）。它的特点是**给“每一步推理”分配特定奖励**，引导模型在整个推理轨迹里优化行为。传统 RL 可能对长推理链里的步骤区分度不足，而 Step - GRPO 关注“分步”奖励，让模型能更精细地学习：哪一步该生成怎样的子查询、选哪个知识库，才能让整个推理过程（从初始问题到最终答案）更高效准确。通过这种定制化强化学习，MLLMs 能逐步学会“聪明推理 + 聪明检索”的策略，避免无效检索，提升推理质量。

### 📈 实验结果
论文在多模态开放域问答基准测试（涵盖视觉问答、表格问答等任务）上验证了 R1 - Router。结果显示：  
- 性能碾压基线：R1 - Router 比基线模型准确率提升超 7%，证明动态检索 + 分步推理的模式在多模态 QA 任务里更具优势。  
- 效率与精准度双赢：进一步分析表明，R1 - Router 能“自适应且高效”利用多样知识库。比如减少不必要的检索次数——不再盲目从所有库取信息，而是按需选库；这既提升了效率（少做无用功），又提升了答案精准度（拿到的知识更对症）。  

### 💬 可借鉴之处
1. 动态交互理念：打破“静态检索流程”思维定式，强调模型推理状态和知识库交互的动态性。在做多模态或复杂知识任务时，可借鉴“让模型依据当前推理进度，自主决定下一步检索动作”的思路，灵活适配多源异构知识。  
2. 分步强化学习：Step - GRPO 展示了“为长推理链的每一步设计特定奖励”的价值。在需要分步骤完成的任务（如多轮对话、复杂决策）里，可考虑拆解步骤、定制奖励信号，让模型更精细学习过程优化，而非只看最终结果。  
3. 多知识库协同：R1 - Router 证明了“不同知识库按需调用”在多模态场景的必要性。后续做跨模态、跨数据源的 AI 应用时，可思考如何让系统智能选库、组合多库知识，而非依赖单一知识库或固定组合方式。  
4. 开源与可复现：论文代码开源（https://github.com/OpenBMB/R1 - Router），为研究者复现、改进方法提供了便利，也鼓励社区基于此探索更智能的多模态检索增强推理方案。  
```

## iterative-self-incentivization-empowers-large-language-models-as-agentic-searchers
### Abstract
Large language models (LLMs) have been widely integrated into information
retrieval to advance traditional techniques. However, effectively enabling LLMs
to seek accurate knowledge in complex tasks remains a challenge due to the
complexity of multi-hop queries as well as the irrelevant retrieved content. To
address these limitations, we propose EXSEARCH, an agentic search framework,
where the LLM learns to retrieve useful information as the reasoning unfolds
through a self-incentivized process. At each step, the LLM decides what to
retrieve (thinking), triggers an external retriever (search), and extracts
fine-grained evidence (recording) to support next-step reasoning. To enable LLM
with this capability, EXSEARCH adopts a Generalized Expectation-Maximization
algorithm. In the E-step, the LLM generates multiple search trajectories and
assigns an importance weight to each; the M-step trains the LLM on them with a
re-weighted loss function. This creates a self-incentivized loop, where the LLM
iteratively learns from its own generated data, progressively improving itself
for search. We further theoretically analyze this training process,
establishing convergence guarantees. Extensive experiments on four
knowledge-intensive benchmarks show that EXSEARCH substantially outperforms
baselines, e.g., +7.8% improvement on exact match score. Motivated by these
promising results, we introduce EXSEARCH-Zoo, an extension that extends our
method to broader scenarios, to facilitate future work.
```
### 🌟 论文解读 | 迭代自激励让大模型化身智能搜索代理：EXSEARCH框架揭秘

### 📌 背景痛点/本文动机
信息检索（IR）作为数据挖掘的基础技术，旨在理解复杂查询并从外部源提取相关信息。如今大语言模型（LLMs）虽广泛融入IR以增强传统技术，但在复杂任务中高效获取准确知识仍存挑战：一方面多跳查询等复杂任务需迭代动态检索，直接发复杂查询易致检索覆盖不足；另一方面检索结果常含无关内容，引发误导性上下文。此前工作多串联信息搜索 pipeline 或独立用合成数据训练特定阶段，端到端对齐不同检索阶段仍待探索。因此，需让LLM学会在推理过程中与检索器交互并反思检索内容，本文提出EXSEARCH框架应对这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出EXSEARCH智能搜索框架，赋予LLM细粒度推理与搜索能力  
EXSEARCH将搜索过程形式化为三个核心动作循环：thinking（基于演进的搜索轨迹生成查询）、search（触发外部检索器获取文档）、recording（从检索文档提取细粒度证据支撑后续推理）。LLM interleaves（交替执行）这些动作，逐步探索由子查询、检索文档和支持性证据构成的搜索轨迹，最终基于整个轨迹生成答案，让响应锚定外部知识。  

💡 创新点2：采用广义期望最大化（GEM）算法实现自激励学习  
为让LLM掌握上述智能搜索能力，EXSEARCH引入GEM算法，交替执行轨迹探索（E-step）与重加权轨迹学习（M-step），将搜索轨迹视为隐变量，实现端到端训练。E-step 用重要性采样近似搜索轨迹分布，LLM为每个输入任务生成候选轨迹并依其对正确答案的支持度自动分配权重；M-step 则重加权这些轨迹构造证据下界（ELBO）并最大化以更新LLM参数，使模型从自身生成数据中学习，生成更具支持性的轨迹与准确答案，形成自激励循环持续优化搜索与推理能力。  

💡 创新点3：理论分析保障收敛性 & 推出EXSEARCH - Zoo拓展应用场景  
从理论层面分析训练过程，证明自激励框架的稳定收敛性；基于良好实验结果，推出EXSEARCH - Zoo扩展资源，从两维度拓展：覆盖不同模型家族（如LLaMA、Qwen）与规模（7B、24B等参数）的多样 backbone LLMs；新增文档重排序等扩展动作，丰富原框架动作空间，助力未来研究。  

### 📈 实验结果
在四个知识密集型基准测试（如HotpotQA等）上开展大量实验，结果显示EXSEARCH显著超越基线方法。例如在精确匹配分数上实现 + 7.8% 的提升；图1展示了在HotpotQA数据集上不同LLM应用EXSEARCH后的性能表现，随训练迭代推进，各模型在训练集与评估集上的精确匹配分数均逐步提升，验证了方法有效性与收敛性趋势。  

### 💬 可借鉴之处
1. 框架设计思路：将复杂搜索拆解为“思考 - 搜索 - 记录”循环动作，为大模型在需外部知识辅助的任务中设计交互逻辑提供参考，启发如何让模型动态迭代式利用外部工具；  
2. 训练范式创新：借助GEM算法实现自激励学习，把模型自身生成的轨迹作为学习资源，为解决“无充足标注数据下如何让模型自改进”提供了一种端到端的思路；  
3. 扩展性实践：EXSEARCH - Zoo 展示了如何从模型多样性与动作丰富性角度拓展方法适用场景，为技术落地到更广泛任务与模型生态提供范式，便于后续研究者在此基础上快速试验与改进。  
```

## search-wisely--mitigating-sub-optimal-agentic-searches-by-reducing-uncertainty
### Abstract
Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language
Models (LLMs) by enabling dynamic, multi-step reasoning and information
retrieval. However, these systems often exhibit sub-optimal search behaviors
like over-search (retrieving redundant information) and under-search (failing
to retrieve necessary information), which hinder efficiency and reliability.
This work formally defines and quantifies these behaviors, revealing their
prevalence across multiple QA datasets and agentic RAG systems (e.g., one model
could have avoided searching in 27.7% of its search steps). Furthermore, we
demonstrate a crucial link between these inefficiencies and the models'
uncertainty regarding their own knowledge boundaries, where response accuracy
correlates with model's uncertainty in its search decisions. To address this,
we propose $\beta$-GRPO, a reinforcement learning-based training method that
incorporates confidence threshold to reward high-certainty search decisions.
Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model
with better agentic RAG ability, outperforming other strong baselines with a 4%
higher average exact match score.
```
### 🌟 论文解读 | 减少不确定性，让智能体搜索更“聪明”——Search Wisely 解读

### 📌 背景痛点/本文动机
大语言模型（LLMs）结合智能体检索增强生成（Agentic RAG）框架后，能实现多步推理与动态信息检索，模拟人类复杂研究过程。但现有Agentic RAG系统常存在**搜索低效**问题：一是“过度搜索（over - search）”，即检索已有内部知识的冗余信息；二是“搜索不足（under - search）”，即需要外部知识时却未检索，这两大问题严重影响系统效率与可靠性。本文旨在量化这些问题、探究其根源并提出改进方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：量化分析搜索低效行为  
对2WikiMultiHopQA、Bamboogle、HotpotQA、MuSiQue等多跳问答数据集，结合R1 - Searcher、Search - R1等当代大语言模型，开展三步实验量化“过度搜索”与“搜索不足”。通过逐步骤分析，明确模型在哪些搜索步骤属于冗余（过度搜索率）、哪些非搜索步骤因未检索必要信息导致错误（搜索不足率），揭示这些低效行为在多数据集和模型中普遍存在（如Search - R1有27.7%的搜索步骤本可避免搜索）。  

💡 创新点2：揭示知识边界不确定性与搜索低效的关联  
探究模型对自身“知识边界（已知与需检索信息的界限）”的认知与搜索效率的联系。分析基于Qwen2.5 - 3B的不同Search - R1模型（含PPO、GRPO训练及Base、Instruct变体），以搜索查询token的最小概率衡量模型对知识边界的“确定性”，发现生成时对搜索必要性“确定性”越高（不确定性越低），最终回答准确率越高（如Bamboogle数据集上高确定性组比低组准确率高6%），证明知识边界感知能力是改善搜索低效的关键。  

💡 创新点3：提出β - GRPO强化学习训练方法  
基于GRPO（Shao et al., 2024）改进，将搜索调用的置信度建模为模型生成搜索查询的最小token概率，并在奖励函数中加入置信度阈值，仅奖励高确定性且能导向正确答案的搜索决策，训练模型更精准评估知识状态、减少不必要不确定性，从而缓解过度与不足搜索。  

### 📈 实验结果
在7个问答基准测试中，β - GRPO训练的3B规模模型展现更强Agentic RAG能力：平均精确匹配分数比强基线高4%；过度搜索减少1.21%、搜索不足减少7.33%，验证了方法在提升搜索效率与回答准确性上的有效性。  

### 💬 可借鉴之处
1. 问题量化思路：对Agentic RAG中模糊的“搜索低效”问题，通过逐步骤拆解、结合多数据集实验实现精准量化，为同类系统性能分析提供范式。  
2. 知识边界视角：从模型对自身知识的“自知”角度解释搜索问题，启发后续研究关注模型元认知（metacognition）与任务效率的关联。  
3. 强化学习改进：在RL驱动的Agentic RAG训练中融入置信度机制，为提升智能体决策质量提供了简单有效的强化学习调优方向。  
```

## composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering
### Abstract
Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet
many suffer from monolithic designs that tightly couple core functions like
query reformulation, retrieval, reasoning, and verification. This limits their
interpretability, systematic evaluation, and targeted improvement, especially
for complex multi-hop question answering. We introduce ComposeRAG, a novel
modular abstraction that decomposes RAG pipelines into atomic, composable
modules. Each module, such as Question Decomposition, Query Rewriting,
Retrieval Decision, and Answer Verification, acts as a parameterized
transformation on structured inputs/outputs, allowing independent
implementation, upgrade, and analysis. To enhance robustness against errors in
multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that
iteratively revisits and refines earlier steps upon verification failure.
Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently
outperforms strong baselines in both accuracy and grounding fidelity.
Specifically, it achieves up to a 15% accuracy improvement over
fine-tuning-based methods and up to a 5% gain over reasoning-specialized
pipelines under identical retrieval conditions. Crucially, ComposeRAG
significantly enhances grounding: its verification-first design reduces
ungrounded answers by over 10% in low-quality retrieval settings, and by
approximately 3% even with strong corpora. Comprehensive ablation studies
validate the modular architecture, demonstrating distinct and additive
contributions from each component. These findings underscore ComposeRAG's
capacity to deliver flexible, transparent, scalable, and high-performing
multi-hop reasoning with improved grounding and interpretability.
```
### 🌟 论文解读 | ComposeRAG：模块化可组合的多跳问答RAG框架

### 📌 背景痛点/本文动机
大语言模型（LLMs）在各类NLP任务中表现出色，但依赖静态预训练知识易产生错误（幻觉）且难获取最新或特定领域信息。检索增强生成（RAG）虽能整合外部知识缓解这些问题，然而传统RAG在复杂多跳问答时面临挑战，多跳问答需多文档间分解与逐步推理，现有方法常因整体式或不透明架构，限制了可解释性、适应性与系统改进空间，如早期方案易出错传播，后续系统又有依赖微调或验证机制不足等问题。因此，需要一种更灵活、透明且能应对多跳问答的RAG架构。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：模块化架构设计
将RAG pipeline分解为如问题分解、查询重写、检索决策、答案验证等原子化、可组合模块。每个模块对结构化输入/输出做参数化转换，支持独立实现、升级与分析，为多跳问答提供灵活且可解释的推理组件，实现“即插即用”与透明化分析改进。

💡 创新点2：带自反思的编排策略
引入自反思机制增强多步推理鲁棒性，当验证失败时迭代回顾和优化早期步骤。通过自反思的问题分解等推理步骤协调，从早期错误中恢复，提升整体鲁棒性，且反思步骤增加能带来性能提升。

💡 创新点3：模块价值验证与可升级性
一方面量化核心模块（如问题分解、段落重排、答案验证）各自独特且可叠加的性能贡献；另一方面验证独立模块可升级性，单独增强单个组件（如特定任务用更强大LLM）能带来可衡量改进，体现系统扩展性与适应性，还有像检索决策这类注重效率组件对性能和资源利用有积极影响。

### 📈 实验结果
在HotpotQA、2WikiMultiHopQA、MuSiQue、Bamboogle四个多跳QA基准测试中，ComposeRAG在准确率和 grounding 保真度上持续超越强基线。相比基于微调方法准确率最多提升15%，相同检索条件下比推理专用 pipeline 最多高5%；验证优先设计在低质量检索场景减少超10%无根据答案，高质量语料下也约降3%；消融实验证实模块化架构价值，各组件贡献独特且可叠加。

### 💬 可借鉴之处
在架构设计上，模块化思路为复杂系统拆解提供范例，便于分析、升级与适配新任务领域，可应用于需多组件协作的AI系统；自反思机制为处理多步推理中错误传播问题提供新思路，在需迭代优化的任务流程里值得参考；对模块价值量化与可升级性验证，引导后续工作关注组件级优化与系统扩展性，为构建更灵活高效AI系统提供了从设计到验证的完整思路参考。
```

## cort--code-integrated-reasoning-within-thinking
### Abstract
Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.
```
### 🌟 论文解读 | CoRT：让大推理模型在思考中高效融合代码推理

### 📌 背景痛点/本文动机
在自然语言长链式思考（CoT）任务中，像 o1、DeepSeek - R1 这类大推理模型（LRMs）取得了显著进展。然而，它们在处理复杂数学运算时，效率和准确性仍存在不足。虽然借助计算工具（如计算库、符号求解器）有望解决这些局限，但代码解释器（CI）会引入模型内部文本表征之外的外部知识，直接结合的效率并不高。同时，还面临着数据合成困难（如 o3、o4 - mini 未公开详细推理轨迹）、协调计算精度与抽象推理能力、调和模型自反思机制与外部精确知识等挑战。所以，如何让 LRMs 高效利用 CI 进行推理成为关键问题，这也是本文的研究动机。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：Hint - Engineering 解决数据稀缺问题
为应对代码集成推理领域的数据稀缺难题，本文提出通过 Hint - Engineering 合成代码集成推理数据。其核心是在推理过程的合适位置战略性插入不同提示，以此优化 LRM - CI 交互。例如，发现在模型思考标记后插入特定提示（如 “Okay, let’s try to solve this problem step by step using multiple python code calls”）能将代码触发率从 50% 提升到 90%。并且遵循 “质量重于数量” 原则，手动创建 30 个高质量且经过人工验证的样本，为后续训练提供基础。

💡 创新点2：多阶段训练 pipeline 赋能模型能力
针对不同参数规模的模型设计了相应训练流程。对于 32B 大参数模型，采用有监督微调（SFT）和拒绝微调（RFT）；对于 1.5B 小参数模型，实现了 SFT - RFT - 强化学习（RL）的完整 pipeline。通过这些针对性的后训练过程，让大语言模型获得复杂的代码集成推理能力。同时，精心设计结果奖励来鼓励模型正确编写代码，引导模型学习。

### 📈 实验结果
在五个具有挑战性的数学推理数据集上，Hint - Engineering 模型展现出显著性能提升。DeepSeek - R1 - Distill - Qwen - 32B 绝对准确率提升 4%，DeepSeek - R1 - Distill - Qwen - 1.5B 提升 8%。在 token 效率方面，与自然语言模型相比，32B 模型使用的 token 减少约 30%，1.5B 模型减少约 50%。以极具挑战性的 AIME 基准测试为例，该方法在提升准确率的同时，大幅降低了 token 消耗，如 32B 模型在 AIME 上 token 消耗减少 30%，1.5B 模型减少 50%。

### 💬 可借鉴之处
1. 数据合成思路：当领域数据稀缺时，可借鉴本文 “质量优先 + 人工构建 + 策略性提示插入” 的方式来合成高质量训练数据，为模型训练筑牢基础。
2. 模型训练策略：根据模型参数规模差异设计不同训练流程（大模型侧重 SFT、RFT；小模型尝试完整 SFT - RFT - RL 流程），这种按规模适配训练方法的思路，对不同大小模型的能力提升有参考价值。
3. 工具融合探索：在模型与外部工具（如本文中代码解释器）结合的场景下，通过提示工程等手段优化交互，为解决模型利用外部工具时的效率、准确性问题提供了实践范例，可推广到其他工具与模型结合的研究中。
```

## interleaved-reasoning-for-large-language-models-via-reinforcement-learning
### Abstract
Long chain-of-thought (CoT) significantly enhances large language models'
(LLM) reasoning capabilities. However, the extensive reasoning traces lead to
inefficiencies and an increased time-to-first-token (TTFT). We propose a novel
training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs
to interleave thinking and answering for multi-hop questions. We observe that
models inherently possess the ability to perform interleaved reasoning, which
can be further enhanced through RL. We introduce a simple yet effective
rule-based reward to incentivize correct intermediate steps, which guides the
policy model toward correct reasoning paths by leveraging intermediate signals
generated during interleaved reasoning. Extensive experiments conducted across
five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)
demonstrate consistent improvements over traditional think-answer reasoning,
without requiring external tools. Specifically, our approach reduces TTFT by
over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,
our method, trained solely on question answering and logical reasoning
datasets, exhibits strong generalization ability to complex reasoning datasets
such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to
reveal several valuable insights into conditional reward modeling.
```
### 🌟 论文解读 | 强化学习驱动大模型的交错推理：效率与能力双提升

### 📌 背景痛点/本文动机
大语言模型（LLM）借助长思维链（CoT）能显著增强推理能力，但传统“思考 - 回答”范式存在两大关键缺陷：一是生成答案前要完成完整推理轨迹，导致首 token 生成时间（TTFT）大幅增加，在实时交互场景（如对话助手）中影响用户体验；二是延迟到推理结束才生成答案，易让错误中间步骤传播，引发最终答案不准确与推理低效（如过度/不足思考）。同时，现有强化学习（RL）训练推理型 LLM 时，常把中间推理轨迹当附属品，未充分利用其中间信号辅助训练与交互。因此，如何让模型在推理中交错“思考”与“回答”、利用中间信号优化训练和交互，成为待解难题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出交错推理 RL 训练范式  
提出“交错推理”这一全新 RL 训练范式，让 LLM 在多跳问题推理中交替进行思考与回答，无需外部工具。该范式使模型在推理过程中生成有信息量的中间答案，既给用户及时反馈（降低 TTFT），又为自身后续推理提供可验证的奖励信号，引导向正确最终答案推进。  

💡 创新点2：设计基于规则的奖励机制  
引入简单有效的基于规则的奖励，激励模型生成正确中间步骤。利用交错推理中产生的中间信号，为策略模型指明正确推理路径，在训练时提供密集且一致的反馈，解决传统训练中中间步骤难 credit assignment（ credit assignment 指训练中如何把最终结果的奖惩分配到各中间步骤）的问题。  


### 📈 实验结果
1. 效率与准确率提升：在 5 个不同数据集上，用 PPO、GRPO、REINFORCE++ 三种 RL 算法实验，相比传统“思考 - 回答”推理，平均 TTFT 降低超 80%；Pass@1 准确率最多提升 19.3%。  
2. 泛化能力验证：仅在问答和逻辑推理数据集上训练，模型对 MATH、GPQA、MMLU 等复杂推理数据集展现强泛化能力。  
3. 奖励建模洞察：深入分析 conditional reward modeling，得到关于奖励建模、稳定 RL 训练和模型推理动态的有价值见解。  


### 💬 可借鉴之处
1. 训练范式创新：“交错推理”范式为优化大模型推理时的交互效率与训练有效性提供新思路，打破“先完整思考再回答”的固定顺序，启发后续探索更灵活的推理交互模式。  
2. 奖励机制设计：基于规则的简单奖励在避免复杂 reward model 训练的同时，有效利用中间信号引导训练，证明无需复杂模型也能挖掘中间步骤价值，为轻量化奖励设计提供参考。  
3. 泛化性探索：仅用基础推理类数据集训练却能泛化到复杂任务，说明该方法抓准了推理能力的共性本质，为大模型跨任务推理能力培养提供实践范例。  
```

