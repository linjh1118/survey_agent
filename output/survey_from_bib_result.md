# Paper List from BIB File: tmpze6z_azs.bib
- [25/03] **START: Self-taught Reasoner with Tools**  
[[Paper](http://arxiv.org/pdf/2503.04625v2)] [[Code/Page]()] [[TLDR/Notes](#start--self-taught-reasoner-with-tools)]

- [25/05] **Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16410v1)] [[Code/Page](https://github.com/dongguanting/Tool-Star.)] [[TLDR/Notes](#tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning)]

- [25/05] **Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging**  
[[Paper](http://arxiv.org/pdf/2505.09316v1)] [[Code/Page]()] [[TLDR/Notes](#scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging)]

- [25/04] **Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration**  
[[Paper](http://arxiv.org/pdf/2504.04915v1)] [[Code/Page](https://github.com/ritaranx/Collab-RAG/.)] [[TLDR/Notes](#collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration)]

- [25/05] **ZeroSearch: Incentivize the Search Capability of LLMs without Searching**  
[[Paper](http://arxiv.org/pdf/2505.04588v2)] [[Code/Page]()] [[TLDR/Notes](#zerosearch--incentivize-the-search-capability-of-llms-without-searching)]

- [25/04] **Reinforcement Learning for Reasoning in Large Language Models with One Training Example**  
[[Paper](http://arxiv.org/pdf/2504.20571v2)] [[Code/Page](https://github.com/ypwang61/One-Shot-RLVR.)] [[TLDR/Notes](#reinforcement-learning-for-reasoning-in-large-language-models-with-one-training-example)]

- [25/04] **DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments**  
[[Paper](http://arxiv.org/pdf/2504.03160v4)] [[Code/Page](https://github.com/GAIR-NLP/DeepResearcher.)] [[TLDR/Notes](#deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments)]

- [25/03] **ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.19470v2)] [[Code/Page]()] [[TLDR/Notes](#research--learning-to-reason-with-search-for-llms-via-reinforcement-learning)]

- [25/03] **SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild**  
[[Paper](http://arxiv.org/pdf/2503.18892v2)] [[Code/Page]()] [[TLDR/Notes](#simplerl-zoo--investigating-and-taming-zero-reinforcement-learning-for-open-base-models-in-the-wild)]

- [25/01] **Chain-of-Retrieval Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2501.14342v2)] [[Code/Page]()] [[TLDR/Notes](#chain-of-retrieval-augmented-generation)]

- [25/03] **RARE: Retrieval-Augmented Reasoning Modeling**  
[[Paper](http://arxiv.org/pdf/2503.23513v2)] [[Code/Page]()] [[TLDR/Notes](#rare--retrieval-augmented-reasoning-modeling)]

- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **Hybrid Latent Reasoning via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.18454v1)] [[Code/Page]()] [[TLDR/Notes](#hybrid-latent-reasoning-via-reinforcement-learning)]

- [25/04] **WebThinker: Empowering Large Reasoning Models with Deep Research Capability**  
[[Paper](http://arxiv.org/pdf/2504.21776v1)] [[Code/Page](https://github.com/RUC-NLPIR/WebThinker.)] [[TLDR/Notes](#webthinker--empowering-large-reasoning-models-with-deep-research-capability)]

- [24/10] **SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback**  
[[Paper](http://arxiv.org/pdf/2410.18141v2)] [[Code/Page]()] [[TLDR/Notes](#smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback)]

- [25/05] **Distilling LLM Agent into Small Models with Retrieval and Code Tools**  
[[Paper](http://arxiv.org/pdf/2505.17612v1)] [[Code/Page](https://github.com/Nardien/agent-distillation.)] [[TLDR/Notes](#distilling-llm-agent-into-small-models-with-retrieval-and-code-tools)]

- [25/03] **Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.09516v3)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning)]

- [25/05] **Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs**  
[[Paper](http://arxiv.org/pdf/2505.11277v3)] [[Code/Page]()] [[TLDR/Notes](#search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms)]

- [25/05] **Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.17266v2)] [[Code/Page]()] [[TLDR/Notes](#select2reason--efficient-instruction-tuning-data-selection-for-long-cot-reasoning)]

- [25/05] **StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.15107v2)] [[Code/Page](https://github.com/Zillwang/StepSearch.)] [[TLDR/Notes](#stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization)]

- [25/05] **Towards Effective Code-Integrated Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.24480v1)] [[Code/Page](https://github.com/RUCAIBox/CIR.)] [[TLDR/Notes](#towards-effective-code-integrated-reasoning)]

- [25/05] **An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents**  
[[Paper](http://arxiv.org/pdf/2505.15117v1)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents)]

- [25/06] **Computational Thinking Reasoning in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.02658v2)] [[Code/Page]()] [[TLDR/Notes](#computational-thinking-reasoning-in-large-language-models)]

- [25/05] **Diversity-Aware Policy Optimization for Large Language Model Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.23433v1)] [[Code/Page]()] [[TLDR/Notes](#diversity-aware-policy-optimization-for-large-language-model-reasoning)]

- [25/05] **The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.24630v1)] [[Code/Page]()] [[TLDR/Notes](#the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models)]

- [25/02] **RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2502.13957v2)] [[Code/Page](https://rag-gym.github.io.)] [[TLDR/Notes](#rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation)]

- [25/06] **From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?**  
[[Paper](http://arxiv.org/pdf/2506.08295v1)] [[Code/Page](https://github.com/tmlr-group/AR-Bench.)] [[TLDR/Notes](#from-passive-to-active-reasoning--can-large-language-models-ask-the-right-questions-under-incomplete-information-)]

- [25/06] **SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning**  
[[Paper](http://arxiv.org/pdf/2506.09016v2)] [[Code/Page]()] [[TLDR/Notes](#speed-rl--faster-training-of-reasoning-models-via-online-curriculum-learning)]

- [25/06] **TreeRPO: Tree Relative Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2506.05183v1)] [[Code/Page](https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.)] [[TLDR/Notes](#treerpo--tree-relative-policy-optimization)]

- [25/05] **Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability**  
[[Paper](http://arxiv.org/pdf/2505.13258v1)] [[Code/Page]()] [[TLDR/Notes](#effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability)]

- [25/06] **SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.08989v1)] [[Code/Page]()] [[TLDR/Notes](#sws--self-aware-weakness-driven-problem-synthesis-in-reinforcement-learning-for-llm-reasoning)]

- [25/06] **R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.04185v1)] [[Code/Page](https://github.com/QingFei1/R-Search.)] [[TLDR/Notes](#r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning)]

- [25/05] **Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.24332v1)] [[Code/Page]()] [[TLDR/Notes](#pangu-deepdiver--adaptive-search-intensity-scaling-via-open-web-reinforcement-learning)]

- [25/05] **Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.21765v1)] [[Code/Page]()] [[TLDR/Notes](#don-t-think-longer--think-wisely--optimizing-thinking-dynamics-for-large-reasoning-models)]

- [25/05] **Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.22203v1)] [[Code/Page]()] [[TLDR/Notes](#pitfalls-of-rule--and-model-based-verifiers----a-case-study-on-mathematical-reasoning)]

- [25/05] **Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.22095v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-route-queries-across-knowledge-bases-for-step-wise-retrieval-augmented-reasoning)]

- [25/05] **Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers**  
[[Paper](http://arxiv.org/pdf/2505.20128v1)] [[Code/Page]()] [[TLDR/Notes](#iterative-self-incentivization-empowers-large-language-models-as-agentic-searchers)]

- [25/05] **Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty**  
[[Paper](http://arxiv.org/pdf/2505.17281v1)] [[Code/Page]()] [[TLDR/Notes](#search-wisely--mitigating-sub-optimal-agentic-searches-by-reducing-uncertainty)]

- [25/06] **ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering**  
[[Paper](http://arxiv.org/pdf/2506.00232v1)] [[Code/Page]()] [[TLDR/Notes](#composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering)]

- [25/06] **CoRT: Code-integrated Reasoning within Thinking**  
[[Paper](http://arxiv.org/pdf/2506.09820v2)] [[Code/Page](https://github.com/ChengpengLi1003/CoRT.)] [[TLDR/Notes](#cort--code-integrated-reasoning-within-thinking)]

- [25/05] **Interleaved Reasoning for Large Language Models via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.19640v1)] [[Code/Page]()] [[TLDR/Notes](#interleaved-reasoning-for-large-language-models-via-reinforcement-learning)]



# TLDR/Notes
## start--self-taught-reasoner-with-tools
### Abstract
Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have
demonstrated remarkable capabilities in complex reasoning tasks through the
utilization of long Chain-of-thought (CoT). However, these models often suffer
from hallucinations and inefficiencies due to their reliance solely on internal
reasoning processes. In this paper, we introduce START (Self-Taught Reasoner
with Tools), a novel tool-integrated long CoT reasoning LLM that significantly
enhances reasoning capabilities by leveraging external tools. Through code
execution, START is capable of performing complex computations, self-checking,
exploring diverse methods, and self-debugging, thereby addressing the
limitations of LRMs. The core innovation of START lies in its self-learning
framework, which comprises two key techniques: 1) Hint-infer: We demonstrate
that inserting artificially designed hints (e.g., ``Wait, maybe using Python
here is a good idea.'') during the inference process of a LRM effectively
stimulates its ability to utilize external tools without the need for any
demonstration data. Hint-infer can also serve as a simple and effective
sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning
(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and
modifying the reasoning trajectories with tool invocation generated by a LRM
via Hint-infer, followed by fine-tuning the LRM. Through this framework, we
have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA
(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the
competition-level code benchmark (LiveCodeBench), START achieves accuracy rates
of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly
outperforms the base QwQ-32B and achieves performance comparable to the
state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary
model o1-Preview.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | STARTï¼šå·¥å…·èµ‹èƒ½çš„è‡ªæ•™æ¨ç†å¤§æ¨¡å‹ï¼Œçªç ´é•¿é“¾æ¨ç†å±€é™

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼ŒåƒOpenAI-o1ã€DeepSeek-R1è¿™ç±»å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å€ŸåŠ©é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†ä»…ä¾èµ–å†…éƒ¨æ¨ç†æµç¨‹ï¼Œå­˜åœ¨å¹»è§‰ï¼ˆhallucinationsï¼‰å’Œæ•ˆç‡ä¸è¶³ç­‰é—®é¢˜ã€‚è€Œå·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰èƒ½ç¼“è§£é•¿CoTçš„ç¼ºé™·ï¼Œå¯å¦‚ä½•æŠŠé•¿CoTå’ŒTIRååŒç»“åˆï¼Œæ˜¯å¾…è§£å†³çš„å…³é”®é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¤§æ¨ç†æ¨¡å‹è®­ç»ƒæ—¶èšç„¦å¤æ‚æ¨ç†é—®é¢˜æ±‚è§£ï¼Œåœ¨æŒ‡ä»¤éµå¾ªä¸Šæ³›åŒ–æ€§ç¼ºå¤±ï¼Œéš¾ä»¥åœ¨é•¿CoTä¸­ä¸»åŠ¨è°ƒç”¨å·¥å…·ï¼ˆå¦‚Pythonè§£é‡Šå™¨ï¼‰ï¼Œè¿™ä¹Ÿæ¨åŠ¨äº†ç ”ç©¶æ–°æ–¹æ³•æ¥çªç ´ç°çŠ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šHint - inferæŠ€æœ¯
åœ¨å¤§æ¨ç†æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­æ’å…¥äººå·¥è®¾è®¡çš„æç¤ºï¼ˆå¦‚â€œWait, maybe using Python here is a good idea.â€ï¼‰ï¼Œæ— éœ€æ¼”ç¤ºæ•°æ®å°±èƒ½æœ‰æ•ˆæ¿€å‘æ¨¡å‹åˆ©ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ã€‚å¯¹äºæ•°å­¦ä»»åŠ¡ï¼Œæ’å…¥åŸºç¡€æç¤ºå’ŒPythonæ ‡è¯†ç¬¦å°±èƒ½å¼•å¯¼æ¨¡å‹å†™åˆé€‚ä»£ç ï¼›å¯¹äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œç²¾å¿ƒè®¾è®¡æç¤ºå’Œä»£ç æ¨¡æ¿å¯æ¿€æ´»æ¨¡å‹åœ¨é•¿CoTä¸­è‡ªä¸»æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹ä»£ç çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒHint - inferè¿˜å¯ä½œä¸ºç®€å•æœ‰æ•ˆçš„é¡ºåºæµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œå½“åœ¨é•¿æ€ç»´é“¾åœæ­¢æ ‡è®°å‰åŠ æç¤ºï¼Œæ¨¡å‹ä¼šéšæ€è€ƒæ—¶é—´å¢åŠ å‘ˆç°æˆåŠŸç‡æå‡çš„ç¼©æ”¾æ•ˆåº”ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šHint Rejection Sampling Fine - Tuningï¼ˆHint - RFTï¼‰æŠ€æœ¯
è¯¥æŠ€æœ¯ç»“åˆäº†Hint - inferå’ŒRFTã€‚å…ˆå¯¹å¤§æ¨ç†æ¨¡å‹é€šè¿‡Hint - inferç”Ÿæˆçš„å¸¦å·¥å…·è°ƒç”¨çš„æ¨ç†è½¨è¿¹è¿›è¡Œè¯„åˆ†ã€è¿‡æ»¤å’Œä¿®æ”¹ï¼Œå¾—åˆ°ç”¨äºå¾®è°ƒçš„æ•°æ®Dseedï¼Œå†ç”¨Dseedå¯¹æ¨¡å‹ï¼ˆå¦‚QwQ - 32B - Previewï¼‰è¿›è¡Œå¾®è°ƒå¾—åˆ°START - 0ï¼Œè®©æ¨¡å‹å…·å¤‡å·¥å…·ä½¿ç”¨çš„è‡ªæˆ‘æ„ŸçŸ¥èƒ½åŠ›ã€‚åç»­è¿˜åŸºäºSTART - 0ç”Ÿæˆè‡ªè’¸é¦è½¨è¿¹æ„å»ºDSTARTæ¥è¿›ä¸€æ­¥å¾®è°ƒå¾—åˆ°æœ€ç»ˆçš„STARTï¼Œå®ç°é•¿CoTä¸TIRçš„ååŒï¼Œæ‰“é€ Long TIRèŒƒå¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSTARTè¡¨ç°å‡ºè‰²ï¼šåœ¨åšå£«çº§ç§‘å­¦é—®ç­”ï¼ˆGPQAï¼‰ä»»åŠ¡ä¸Šå‡†ç¡®ç‡è¾¾63.6%ï¼›åœ¨ç«èµ›çº§æ•°å­¦åŸºå‡†ï¼ˆAMC23ã€AIME24ã€AIME25ï¼‰ä¸Šå‡†ç¡®ç‡åˆ†åˆ«ä¸º95.0%ã€66.7%ã€47.1%ï¼›åœ¨ç«èµ›çº§ä»£ç åŸºå‡†ï¼ˆLiveCodeBenchï¼‰ä¸Šå‡†ç¡®ç‡ä¸º47.3%ã€‚æ˜¾è‘—è¶…è¶Šäº†åŸºç¡€æ¨¡å‹QwQ - 32B - Previewï¼Œä¸”æ€§èƒ½æ¯”è‚©å¼€æºæƒé‡æ¨¡å‹R1 - Distill - Qwen - 32Bå’Œä¸“æœ‰æ¨¡å‹o1 - Previewã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æç¤ºé©±åŠ¨å·¥å…·è°ƒç”¨çš„æ€è·¯ï¼šHint - inferå±•ç¤ºäº†é€šè¿‡äººå·¥æç¤ºåœ¨æ— æ¼”ç¤ºæ•°æ®ä¸‹æ¿€æ´»æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„å¯è¡Œæ€§ï¼Œä¸ºè®©æ¨¡å‹åœ¨æ¨ç†ä¸­è‡ªä¸»åˆ©ç”¨å¤–éƒ¨èµ„æºæä¾›äº†æ–°èŒƒå¼ï¼Œå¯å€Ÿé‰´åˆ°éœ€è¦å¤–éƒ¨å·¥å…·è¾…åŠ©çš„å„ç±»æ¨ç†ä»»åŠ¡ä¸­ï¼Œå¦‚æ•°æ®åˆ†æã€å¤æ‚è®¡ç®—åœºæ™¯ç­‰ã€‚
2. è‡ªå­¦ä¹ å¾®è°ƒæ¡†æ¶ï¼šHint - RFTç»“åˆHint - inferä¸RFTçš„è‡ªå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¨ç†è½¨è¿¹å¤„ç†åå¾®è°ƒæ¨¡å‹ï¼Œè¿™ç§åˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆæ•°æ®ä¼˜åŒ–è‡ªèº«çš„æ€è·¯ï¼Œä¸ºå¤§æ¨¡å‹è¿­ä»£å‡çº§æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼Œåœ¨æå‡æ¨¡å‹å·¥å…·ä½¿ç”¨ã€æ¨ç†ç²¾åº¦ç­‰æ–¹é¢æœ‰å€Ÿé‰´ä»·å€¼ã€‚
3. èšç„¦Pythonè§£é‡Šå™¨è°ƒç”¨çš„å®è·µï¼šé’ˆå¯¹Pythonè§£é‡Šå™¨è¿™ä¸€é‡è¦ä¸”å…·ä»£è¡¨æ€§çš„å·¥å…·å±•å¼€ç ”ç©¶ï¼Œä¸ºç‰¹å®šå·¥å…·é›†æˆåˆ°å¤§æ¨¡å‹æ¨ç†æµç¨‹æä¾›äº†å®è·µèŒƒä¾‹ï¼Œå¯¹äºå…¶ä»–å·¥å…·ï¼ˆå¦‚ä¸“ä¸šé¢†åŸŸå·¥å…·ã€å…¶ä»–ç¼–ç¨‹è¯­è¨€è§£é‡Šå™¨ç­‰ï¼‰ä¸å¤§æ¨¡å‹ç»“åˆçš„ç ”ç©¶æœ‰å¯å‘ï¼Œå¯å¼•å¯¼åç»­æ¢ç´¢ä¸åŒå·¥å…·ä¸é•¿CoTç»“åˆçš„æ–¹å¼ã€‚
```

## tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning
### Abstract
Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Tool-Starï¼šç”¨å¼ºåŒ–å­¦ä¹ èµ‹èƒ½å¤§æ¨¡å‹å¤šå·¥å…·åä½œæ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºäº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹åœ¨å¤šå·¥å…·åä½œæ¨ç†ä¸­é«˜æ•ˆå‘æŒ¥ä½œç”¨ä»æ˜¯å¾…è§£éš¾é¢˜ã€‚ç°å®åœºæ™¯ä¸­æ¨ç†å¸¸éœ€æ¨¡å‹ç»“åˆç¯å¢ƒäº¤äº’æ•´åˆå¤šç§èƒ½åŠ›ï¼ˆå¦‚æ·±åº¦ä¿¡æ¯æœç´¢ã€é•¿æœŸçŸ¥è¯†è®°å¿†ã€ç²¾ç¡®è®¡ç®—ç­‰ï¼‰ï¼Œå·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰è™½æ‹“å±•äº†è¯­è¨€æ¨ç†èŒƒå¼ï¼Œä½†ç°æœ‰å·¥ä½œå­˜åœ¨ä¸è¶³ï¼šåŸºäºæç¤ºçš„æ–¹æ³•å·¥å…·ä½¿ç”¨ä¸ç¨³å®šã€ç²¾åº¦æœ‰é™ï¼›åŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ–¹æ³•å—æ¼”ç¤ºè´¨é‡å’Œæ³›åŒ–æ€§é™åˆ¶ï¼›åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•å¤šèšç„¦å•å·¥å…·ï¼Œå¤šå·¥å…·åä½œæ¨ç†çš„ç³»ç»Ÿç ”ç©¶ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³â€œå·¥å…·ä½¿ç”¨çš„åˆç†æ€§ä¸æ•ˆç‡â€â€œå¤šå·¥å…·åä½œæ¨ç†â€ä¸¤å¤§é—®é¢˜ï¼Œæå‡ºTool - Staræ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå·¥å…·é›†æˆæ¨ç†æ•°æ®åˆæˆ pipeline
ä¸ºè§£å†³å·¥å…·ä½¿ç”¨æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œè®¾è®¡é€šç”¨çš„å·¥å…·é›†æˆæ¨ç†æ•°æ®åˆæˆ pipelineã€‚ç»“åˆå·¥å…·é›†æˆæç¤ºä¸åŸºäºæç¤ºçš„é‡‡æ ·ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡å·¥å…·ä½¿ç”¨è½¨è¿¹ï¼›åç»­é€šè¿‡è´¨é‡å½’ä¸€åŒ–å’Œéš¾åº¦æ„ŸçŸ¥åˆ†ç±»è¿‡ç¨‹ï¼Œè¿‡æ»¤ä¸åˆç†æ ·æœ¬å¹¶æŒ‰ä»æ˜“åˆ°éš¾çš„â€œè¯¾ç¨‹å¼â€æ–¹å¼åˆ’åˆ†æ•°æ®ï¼Œä¸ºå†·å¯åŠ¨å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ åˆ†é˜¶æ®µæ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œå¤¯å®TIRè®­ç»ƒåŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸¤é˜¶æ®µTIRè®­ç»ƒæ¡†æ¶
æå‡ºä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶æå‡å¤šå·¥å…·åä½œæ¨ç†èƒ½åŠ›ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯å†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼Œè®©å¤§æ¨¡å‹å€ŸåŠ©å·¥å…·è°ƒç”¨åé¦ˆåˆæ­¥æ¢ç´¢æ¨ç†æ¨¡å¼ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯å¤šå·¥å…·è‡ªæ‰¹åˆ¤å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé‡‡ç”¨åˆ†å±‚å¥–åŠ±æœºåˆ¶ï¼Œä¸ä»…è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§å’Œå·¥å…·ä½¿ç”¨æ ¼å¼ï¼Œè¿˜ä¸ºæœ‰æ•ˆå¤šå·¥å…·åä½œåˆ†é…é¢å¤–å¥–åŠ±ï¼Œä¸”åœ¨æ ‡å‡†RLè¿‡ç¨‹ä¸­æ’å…¥è‡ªæ‰¹åˆ¤å¥–åŠ±å¾®è°ƒé˜¶æ®µï¼ŒåŠ©åŠ›æ¨¡å‹å†…åŒ–å¥–åŠ±åŸåˆ™ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé›†æˆå¤šç±»å‹å·¥å…·
Tool - Staråœ¨æ¨ç†è¿‡ç¨‹ä¸­é›†æˆäº†å…­ç§å·¥å…·ï¼ˆè®­ç»ƒæ—¶ä¸‰ç§ã€æ¨ç†æ—¶ä¼˜åŒ–ç”¨ä¸‰ç§ï¼‰ï¼Œä»å·¥å…·å±‚é¢ä¸ºå¤šå·¥å…·åä½œæ¨ç†æä¾›æ”¯æ’‘ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨10å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è®¡ç®—æ¨ç†ä»»åŠ¡ï¼ˆå¦‚AIME24ã€MATH500ï¼‰å’ŒçŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ï¼ˆå¦‚WebWalkerã€HotpotQAï¼‰ä¸Šå¼€å±•å®éªŒã€‚ç»“æœæ˜¾ç¤ºTool - Staråœ¨ä¿è¯å·¥å…·ä½¿ç”¨æ•ˆç‡å’Œå¯é æ€§çš„åŒæ—¶ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ•´ä½“æ¨ç†æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¸”å®šé‡åˆ†æä½“ç°å‡ºé«˜æ•ˆç‡ï¼Œä¸ºæ¿€åŠ±å¤šå·¥å…·åä½œæ¨ç†æä¾›äº†æ´è§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆæˆæ–¹é¢ï¼šé¢å¯¹æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œè®¾è®¡çš„å·¥å…·é›†æˆæ¨ç†æ•°æ®åˆæˆ pipeline æä¾›äº†ä»æ•°æ®ç”Ÿæˆåˆ°è´¨é‡æ§åˆ¶ã€éš¾åº¦åˆ’åˆ†çš„å®Œæ•´æ€è·¯ï¼Œå¯å€Ÿé‰´è¿™ç§ç³»ç»Ÿç”Ÿæˆå’Œå¤„ç†æ•°æ®çš„æ–¹å¼æ¥åº”å¯¹ç‰¹å®šé¢†åŸŸæ•°æ®ä¸è¶³çš„æƒ…å†µã€‚
2. è®­ç»ƒæ¡†æ¶æ–¹é¢ï¼šä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶å…ˆé€šè¿‡å†·å¯åŠ¨è®©æ¨¡å‹â€œå…¥é—¨â€å·¥å…·æ¨ç†ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ è¿›é˜¶æå‡å¤šå·¥å…·åä½œï¼Œè¿™ç§åˆ†é˜¶æ®µã€å¾ªåºæ¸è¿›ä¸”ç»“åˆè‡ªæ‰¹åˆ¤çš„è®­ç»ƒæ€è·¯ï¼Œä¸ºæå‡æ¨¡å‹å¤æ‚ä»»åŠ¡èƒ½åŠ›æä¾›äº†å‚è€ƒã€‚
3. å¤šå·¥å…·åä½œæ–¹é¢ï¼šé’ˆå¯¹å¤šå·¥å…·åä½œè®¾è®¡åˆ†å±‚å¥–åŠ±æœºåˆ¶ï¼Œè€ƒè™‘åˆ°å¤šå·¥å…·åä½œçš„ä¸åŒç»´åº¦å¥–åŠ±ï¼Œè¿™ç§ç²¾ç»†åŒ–å¥–åŠ±è®¾è®¡æ€è·¯å¯ç”¨äºå…¶ä»–éœ€å¤šç»„ä»¶åä½œçš„AIä»»åŠ¡ä¸­ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æœ‰æ•ˆåä½œç­–ç•¥ã€‚
```

## scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging
### Abstract
Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨â€œä¿¡æ¯è§…é£Ÿâ€ä¼˜åŒ–æœç´¢å¢å¼ºæ¨ç†ï¼šInForageæ¡†æ¶å¦‚ä½•è®©LLMæ›´æ™ºèƒ½ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œçŸ¥è¯†æˆªæ–­â€é—®é¢˜ä¸€ç›´æ˜¯è¡Œä¸šç—›ç‚¹â€”â€”æ¨¡å‹æ— æ³•è·å–è®­ç»ƒåæ›´æ–°çš„çŸ¥è¯†ï¼Œä¹Ÿéš¾ä»¥åº”å¯¹æ¨¡ç³Šã€å¤šæ­¥éª¤æˆ–åŠ¨æ€å˜åŒ–çš„å¤æ‚ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„â€œæ£€ç´¢å¢å¼ºç”Ÿæˆâ€æ–¹æ³•é‡‡ç”¨**é™æ€çš„é¢„æ¨ç†æ£€ç´¢ç­–ç•¥**ï¼ŒæŠŠæ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸€è‚¡è„‘å¡è¿›promptå°±å®Œäº‹ï¼Œé¢å¯¹éœ€è¦è¿­ä»£æ¨ç†ã€é€æ­¥æŒ–æ˜è¯æ®çš„å¤æ‚åœºæ™¯ï¼ˆæ¯”å¦‚â€œæˆ‘è¦å»NeurIPSå‚ä¼šï¼Œéœ€è¦ç­¾è¯å—ï¼Ÿâ€è¿™ç±»è¦å…ˆæŸ¥ä¼šè®®åœ°ç‚¹ã€å†æŸ¥ç­¾è¯æ”¿ç­–çš„é—®é¢˜ï¼‰ï¼Œå°±æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚  

å¥½åœ¨â€œæµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯â€çš„å‘å±•è®©LLMèƒ½åŠ¨æ€è°ƒç”¨å¤–éƒ¨å·¥å…·äº†ï¼Œè¿™æ¨åŠ¨å¤§å®¶æŠŠæ€è·¯ä»â€œé™æ€é¢„æ¨ç†æ£€ç´¢â€è½¬å‘â€œè‡ªé€‚åº”çš„æ¨ç†æ—¶æ£€ç´¢â€ã€‚ä½†åŠ¨æ€æ£€ç´¢ä¹Ÿæœ‰ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯å¤æ‚ä»»åŠ¡çš„ä¿¡æ¯éœ€æ±‚æ˜¯éšå«ä¸”åŠ¨æ€å˜åŒ–çš„ï¼Œå•æ¬¡æ£€ç´¢åªèƒ½æ‹¿åˆ°å±€éƒ¨ä¿¡æ¯â€œè¡¥ä¸â€ï¼Œå¾—é è¿­ä»£ç§¯ç´¯ï¼›äºŒæ˜¯ä¿¡æ¯è¡¥ä¸çš„ä»·å€¼ä¸å–å†³äºè‡ªèº«ï¼Œè€Œè¦çœ‹å®ƒå¯¹æœ€ç»ˆæ¨ç†çš„è´¡çŒ®ã€‚  

äººç±»é¢å¯¹è¿™ç±»ä¿¡æ¯æœç´¢ä»»åŠ¡æ—¶ï¼Œå´èƒ½é«˜æ•ˆç”¨å‡ æ¬¡è¿­ä»£æœç´¢è§£å†³é—®é¢˜â€”â€”è¿™èƒŒåæ˜¯â€œä¿¡æ¯è§…é£Ÿç†è®ºï¼ˆIFTï¼‰â€ï¼šäººç±»ä¼šæƒè¡¡ä¿¡æ¯è¡¥ä¸çš„ä»·å€¼å’Œè·å–æˆæœ¬ï¼Œç”¨â€œä¿¡æ¯æ°”å‘³ï¼ˆscentï¼‰â€æŒ‡å¼•æœç´¢æ–¹å‘ã€‚å—æ­¤å¯å‘ï¼Œè®ºæ–‡æå‡ºInForageæ¡†æ¶ï¼Œæƒ³è®©LLMä¹Ÿå­¦ä¼šè¿™ç§â€œåŠ¨æ€ä¿¡æ¯è§…é£Ÿâ€çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºä¿¡æ¯è§…é£Ÿç†è®ºçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶  
InForageæŠŠâ€œæ£€ç´¢å¢å¼ºæ¨ç†â€å½¢å¼åŒ–ä¸º**åŠ¨æ€ä¿¡æ¯æœç´¢è¿‡ç¨‹**ï¼Œç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–ã€‚å’Œä¼ ç»Ÿåªçœ‹æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®çš„æ–¹æ³•ä¸åŒï¼Œå®ƒå…³æ³¨**ä¸­é—´æ£€ç´¢æ­¥éª¤çš„è´¨é‡**ï¼šå‚è€ƒIFTé‡Œâ€œä¿¡æ¯æ°”å‘³â€çš„æ¦‚å¿µï¼Œè®©æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è‡ªé€‚åº”æœç´¢è¡Œä¸ºè¿­ä»£æ”¶é›†ã€æ•´åˆä¿¡æ¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰é‡å¥–åŠ±æœºåˆ¶å¼•å¯¼æ¨ç†è¡Œä¸º  
ä¸ºäº†æ¿€åŠ±æ¨¡å‹å­¦ä¹ â€œé«˜æ•ˆä¿¡æ¯è§…é£Ÿâ€ï¼ŒInForageè®¾è®¡äº†ä¸‰ç±»å¥–åŠ±ï¼š  
- ç»“æœå¥–åŠ±ï¼ˆOutcome Rewardï¼‰ï¼šå¥–åŠ±æœ€ç»ˆç­”æ¡ˆæ­£ç¡®çš„æ¨ç†è½¨è¿¹ï¼›  
- ä¿¡æ¯å¢ç›Šå¥–åŠ±ï¼ˆInformation Gain Rewardï¼‰ï¼šå¥–åŠ±é‚£äº›èƒ½æŒ–æ˜å‡ºæœ‰ä»·å€¼è¯æ®çš„ä¸­é—´æ£€ç´¢æ­¥éª¤ï¼›  
- æ•ˆç‡æƒ©ç½šï¼ˆEfficiency Penaltyï¼‰ï¼šé¿å…æ— æ„ä¹‰çš„å†—é•¿æ¨ç†ï¼Œé¼“åŠ±â€œé«˜æ•ˆä¿¡æ¯è§…é£Ÿâ€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºäººç±»å¼•å¯¼çš„ç»†ç²’åº¦æ•°æ®é›†  
ç°æœ‰QAæ•°æ®é›†å¤§å¤šåªæœ‰â€œé—®é¢˜-ç­”æ¡ˆâ€å¯¹ï¼Œç¼ºå°‘ä¸­é—´æ¨ç†/æ£€ç´¢æ­¥éª¤çš„è®°å½•ï¼Œä¹Ÿå¾ˆéš¾æ”¯æ’‘å¤æ‚å¤šæ­¥æ¨ç†çš„è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ª**äººç±»å¼•å¯¼çš„æ•°æ®é›†**ï¼šæ ‡æ³¨è€…ä»â€œç§å­é—®é¢˜â€å‡ºå‘ï¼Œæ¨¡æ‹ŸçœŸå®ç½‘é¡µæµè§ˆæ—¶çš„è¿­ä»£æœç´¢ã€é€‰æ–‡æ¡£ã€æç‚¼å­é—®é¢˜çš„è¿‡ç¨‹ï¼Œè¿˜è¦æ±‚æ¯ä¸ªæ¡ˆä¾‹è‡³å°‘åŒ…å«4æ¬¡ä¿¡æ¯è·³è·ƒæˆ–äº¤å‰æ¡ä»¶ï¼Œä¿è¯ä»»åŠ¡å¤æ‚åº¦ã€‚æ•°æ®é›†è®°å½•äº†æœç´¢å’Œæ¨ç†çš„æ¯ä¸€æ­¥ï¼Œèƒ½ä¸ºâ€œæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§ã€ä¸­é—´æ£€ç´¢è´¨é‡ã€æ•´ä½“æ¨ç†æ•ˆç‡â€æä¾›ç›‘ç£ä¿¡å·ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ä¸‰ç±»ä»»åŠ¡ä¸Šåšäº†è¯„ä¼°ï¼šé€šç”¨é—®ç­”ã€å¤šè·³æ¨ç†ä»»åŠ¡ï¼Œä»¥åŠå›¢é˜Ÿæ–°å¼€å‘çš„**å®æ—¶ç½‘ç»œQAæ•°æ®é›†**ã€‚ç»“æœæ˜¾ç¤ºï¼ŒInForageåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½**æŒç»­è¶…è¶ŠåŸºçº¿æ–¹æ³•**ï¼Œè¯æ˜äº†ä»â€œä¸°å¯Œç›‘ç£çš„æœç´¢å¢å¼ºæ¨ç†æ•°æ®â€ä¸­å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œä¹ŸéªŒè¯äº†InForageæ„å»ºâ€œé²æ£’ã€è‡ªé€‚åº”ã€é«˜æ•ˆæ¨ç†æ™ºèƒ½ä½“â€çš„æ½œåŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºè·¨ç•Œèåˆ**ï¼šæŠŠè®¤çŸ¥ç§‘å­¦é‡Œçš„â€œä¿¡æ¯è§…é£Ÿç†è®ºâ€å¼•å…¥LLMçš„æ£€ç´¢å¢å¼ºæ¨ç†ï¼Œä¸ºåŠ¨æ€æ£€ç´¢æä¾›äº†æ–°é¢–çš„ç†è®ºè§†è§’ï¼›  
2. **å¼ºåŒ–å­¦ä¹ ç»†ç²’åº¦ç›‘ç£**ï¼šè·³å‡ºâ€œåªçœ‹æœ€ç»ˆç»“æœâ€çš„æ€ç»´ï¼Œè®¾è®¡å¯¹ä¸­é—´æ­¥éª¤çš„å¥–åŠ±æœºåˆ¶ï¼Œæ›´è´´åˆå¤æ‚ä»»åŠ¡é‡Œâ€œè¿­ä»£ç§¯ç´¯è¯æ®â€çš„é€»è¾‘ï¼›  
3. **æ•°æ®é›†æ„å»ºæ€è·¯**ï¼šé’ˆå¯¹â€œå¤æ‚å¤šæ­¥æ¨ç†ç¼ºæ•°æ®â€çš„ç—›ç‚¹ï¼Œäººå·¥æ¨¡æ‹ŸçœŸå®ä¿¡æ¯æœç´¢è½¨è¿¹ï¼Œè®°å½•ç»†ç²’åº¦æ­¥éª¤â€”â€”è¿™ç§æ„å»ºâ€œå¸¦ä¸­é—´è¿‡ç¨‹çš„å¤æ‚ä»»åŠ¡æ•°æ®é›†â€çš„æ€è·¯ï¼Œå¯¹åç»­ç ”ç©¶å¾ˆæœ‰å‚è€ƒä»·å€¼ï¼›  
4. **åŠ¨æ€æ£€ç´¢çš„è½åœ°æ½œåŠ›**ï¼šInForageå±•ç¤ºäº†LLMåœ¨â€œæ¨¡ç³Šã€å¤šæ­¥éª¤ã€ä¿¡æ¯éœ€æ±‚åŠ¨æ€å˜åŒ–â€åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›æå‡ï¼Œä¸ºæ£€ç´¢å¢å¼ºä»â€œé™æ€â€è½¬å‘â€œåŠ¨æ€è‡ªé€‚åº”â€æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚  
```

## collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration
### Abstract
Retrieval-Augmented Generation (RAG) systems often struggle to handle
multi-hop question-answering tasks accurately due to irrelevant context
retrieval and limited complex reasoning capabilities. We introduce Collab-RAG,
a collaborative training framework that leverages mutual enhancement between a
white-box small language model (SLM) and a blackbox large language model (LLM)
for RAG. Specifically, the SLM decomposes complex queries into simpler
sub-questions, thus enhancing the accuracy of the retrieval and facilitating
more effective reasoning by the black-box LLM. Concurrently, the black-box LLM
provides feedback signals to improve the SLM's decomposition capability. We
observe that Collab-RAG relies solely on supervision from an affordable
black-box LLM without additional distillation from frontier LLMs, yet
demonstrates strong generalization across multiple black-box LLMs. Experimental
evaluations across five multi-hop QA datasets demonstrate that Collab-RAG
substantially outperforms existing black-box-only and SLM fine-tuning baselines
by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a
frozen 32B LLM in question decomposition, highlighting the efficiency of
Collab-RAG in improving reasoning and retrieval for complex questions. The code
of Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Collab-RAGï¼šç™½ç›’å°æ¨¡å‹ä¸é»‘ç›’å¤§æ¨¡å‹åä½œï¼Œçªç ´å¤æ‚é—®ç­”åœºæ™¯ä¸‹çš„RAGç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½åœ¨ä¼—å¤šè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å¹»è§‰ã€éš¾é€‚é…é¢†åŸŸçŸ¥è¯†ç­‰é—®é¢˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯é€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œç„¶è€Œåœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­ï¼ŒRAGå¸¸å› æ£€ç´¢åˆ°æ— å…³ä¸Šä¸‹æ–‡ã€å¤æ‚æ¨ç†èƒ½åŠ›å—é™è€Œè¡¨ç°ä¸ä½³ã€‚ç°æœ‰æå‡æ£€ç´¢è´¨é‡çš„æ–¹æ³•å¤šèšç„¦å•æ­¥æ£€ç´¢ä¼˜åŒ–ï¼Œéš¾åº”å¯¹éœ€è¿­ä»£è¯æ®æ”¶é›†çš„å¤æ‚é—®ç­”ï¼›æ— è®­ç»ƒçš„LLMæŸ¥è¯¢åˆ†è§£èƒ½åŠ›æœ‰é™ï¼›å°æ¨¡å‹å¾®è°ƒæ–¹æ³•å¯¹é»‘ç›’LLMå‚æ•°æ›´æ–°åˆä½æ•ˆæ˜‚è´µã€‚å› æ­¤ï¼Œå……åˆ†é‡Šæ”¾é»‘ç›’LLMåœ¨å¤æ‚é—®ç­”çš„èƒ½åŠ›ä»å…·æŒ‘æˆ˜ï¼Œè¿™å‚¬ç”Ÿäº†Collab - RAGçš„ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCollab - RAGæ¡†æ¶ï¼Œå®ç°ç™½ç›’å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸é»‘ç›’å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠ¨æ€åä½œ  
SLMä½œä¸ºåˆ†è§£å™¨ï¼Œå°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºæ›´ç®€å•çš„å­é—®é¢˜ï¼Œæå‡æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡çš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºé»‘ç›’LLMæ›´æœ‰æ•ˆæ¨ç†é“ºè·¯ï¼›é»‘ç›’LLMä½œä¸ºé˜…è¯»å™¨ï¼Œä¸ºæ¯ä¸ªå­é—®é¢˜ç”Ÿæˆä¸­é—´ç­”æ¡ˆå¹¶åˆæˆæœ€ç»ˆå“åº”ï¼Œå€ŸåŠ©å­é—®é¢˜æ£€ç´¢æ¥é€æ­¥è§£ç­”å¤æ‚é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºé»‘ç›’LLMåé¦ˆçš„è‡ªæ”¹è¿›è®­ç»ƒç­–ç•¥  
ç›´æ¥ç”¨SLMåšé—®é¢˜åˆ†è§£å› æ¨ç†èƒ½åŠ›æœ‰é™ä¸”é«˜è´¨é‡æ ‡æ³¨æˆæœ¬é«˜è€Œæ•ˆæœä¸ä½³ã€‚Collab - RAGä»…ä¾èµ–é»‘ç›’LLMï¼ˆå¦‚GPT - 4o - miniï¼‰çš„åé¦ˆæ¥ä¼˜åŒ–ï¼Œå°†é»‘ç›’LLMå»ºæ¨¡ä¸ºèƒ½ç”Ÿæˆå“åº”çš„ç¯å¢ƒï¼ŒSLMä¸ä¹‹å¤šè½®äº¤äº’è¿­ä»£ä¼˜åŒ–åˆ†è§£ç­–ç•¥ã€‚è®¾è®¡è¿­ä»£åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œä¾æ®é»‘ç›’LLMåé¦ˆï¼ˆé€šè¿‡åŸºäºè§„åˆ™çš„è¯„ä¼°æ–¹æ³•ï¼Œä»å­é—®é¢˜æ ¼å¼å’Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§åˆ¤æ–­åˆ†è§£æ˜¯å¦æœ‰æ•ˆï¼‰æå‡SLMåˆ†è§£èƒ½åŠ›ï¼Œæ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨æˆ–å‰æ²¿LLMè’¸é¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒCollab - RAGå¹³å‡æ¯”ç°æœ‰ä»…é»‘ç›’LLMå’ŒSLMå¾®è°ƒåŸºçº¿æ–¹æ³•æ€§èƒ½é«˜å‡º1.8% - 14.2%ã€‚åœ¨é—®é¢˜åˆ†è§£ä»»åŠ¡ä¸Šï¼Œå¾®è°ƒåçš„3Bå‚æ•°SLMè¡¨ç°è¶…è¿‡å†»ç»“çš„32Bå‚æ•°LLMï¼Œæœ‰åŠ›è¯æ˜äº†Collab - RAGåœ¨æå‡å¤æ‚é—®é¢˜æ¨ç†å’Œæ£€ç´¢æ–¹é¢çš„é«˜æ•ˆæ€§ï¼Œä¸”ä»…ç”¨GPT - 4o - miniç›‘ç£è®­ç»ƒå´èƒ½åœ¨å¤šä¸ªé»‘ç›’LLMä¸Šæœ‰å¼ºæ³›åŒ–æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹åä½œæ€è·¯ï¼šåœ¨RAGç­‰éœ€å¤šç»„ä»¶é…åˆçš„ä»»åŠ¡ä¸­ï¼Œæ¢ç´¢ä¸åŒâ€œèƒ½åŠ›äº’è¡¥â€æ¨¡å‹ï¼ˆå¦‚ç™½ç›’å°æ¨¡å‹ä¸é»‘ç›’å¤§æ¨¡å‹ï¼‰çš„åä½œæ¨¡å¼ï¼Œå‘æŒ¥å„è‡ªä¼˜åŠ¿ï¼ˆå°æ¨¡å‹å¯è®­ç»ƒä¼˜åŒ–ã€å¤§æ¨¡å‹å¼ºæ¨ç†ç­‰ï¼‰ã€‚
2. è®­ç»ƒä¼˜åŒ–æ–¹å¼ï¼šåˆ©ç”¨ç°æœ‰é»‘ç›’å¤§æ¨¡å‹åé¦ˆæ¥ä¼˜åŒ–å°æ¨¡å‹ï¼Œé¿å…é«˜æˆæœ¬æ ‡æ³¨ä¸å‰æ²¿å¤§æ¨¡å‹è’¸é¦ï¼Œä¸ºèµ„æºæœ‰é™æƒ…å†µä¸‹æå‡æ¨¡å‹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚æŸ¥è¯¢åˆ†è§£ï¼‰æä¾›æ€è·¯ã€‚
3. å¤æ‚ä»»åŠ¡å¤„ç†ï¼šé’ˆå¯¹å¤æ‚å¤šè·³é—®ç­”è¿™ç±»éœ€åˆ†æ­¥æ¨ç†ã€å¤šè¯æ®æ•´åˆçš„ä»»åŠ¡ï¼Œé€šè¿‡â€œåˆ†è§£ - å­ä»»åŠ¡å¤„ç† - åˆæˆâ€çš„ pipeline è®¾è®¡ï¼Œä¸ºçªç ´ä»»åŠ¡éš¾ç‚¹æä¾›äº†å¯å‚è€ƒçš„æ¶æ„èŒƒå¼ã€‚
```

## zerosearch--incentivize-the-search-capability-of-llms-without-searching
### Abstract
Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a novel RL
framework that incentivizes the capabilities of LLMs to use a real search
engine with simulated searches during training. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both useful and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ZeroSearchï¼šæ— éœ€çœŸå®æœç´¢ï¼Œæ¿€å‘å¤§æ¨¡å‹æœç´¢èƒ½åŠ›æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨è¯¸å¤šä¸‹æ¸¸ä»»åŠ¡è¡¨ç°å“è¶Šï¼Œä½†çŸ¥è¯†é™æ€ä¸”æ˜“ç”Ÿæˆå¹»è§‰æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Œéœ€å¤–æ¥ä¿¡æ¯å¢å¼ºå¯é æ€§ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯å¸¸ç”¨æ–¹æ¡ˆï¼Œä¸è¿‡æ—©æœŸåŸºäºæç¤ºå·¥ç¨‹ã€åç»­ç›‘ç£å¾®è°ƒæˆ–æ¨ç†æ—¶ç¼©æ”¾æŠ€æœ¯ç­‰å­˜åœ¨å·¥ç¨‹å¤æ‚ã€è®¡ç®—å¼€é”€å¤§ç­‰é—®é¢˜ã€‚è¿‘å¹´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæå‡å¤§æ¨¡å‹æœç´¢èƒ½åŠ›ï¼Œå´é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯æœç´¢å¼•æ“è¿”å›æ–‡æ¡£è´¨é‡ä¸å¯æ§ï¼Œç»™è®­ç»ƒå¼•å…¥å™ªå£°ä¸ä¸ç¨³å®šï¼›äºŒæ˜¯RLè®­ç»ƒéœ€é¢‘ç¹è°ƒç”¨æœç´¢APIï¼Œæˆæœ¬é«˜æ˜‚é™åˆ¶æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºZeroSearchæ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒæ—¶ç”¨æ¨¡æ‹Ÿæœç´¢æ¥æ¿€å‘å¤§æ¨¡å‹ä½¿ç”¨çœŸå®æœç´¢å¼•æ“çš„èƒ½åŠ›ï¼Œè§£å†³ä¸Šè¿°ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè½»é‡ç›‘ç£å¾®è°ƒæ‰“é€ æ£€ç´¢æ¨¡å—  
é€šè¿‡è½»é‡çº§ç›‘ç£å¾®è°ƒï¼ŒæŠŠå¤§è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºèƒ½å“åº”æŸ¥è¯¢ã€ç”Ÿæˆâ€œæœ‰ç”¨â€å’Œâ€œå«å™ªå£°â€æ–‡æ¡£çš„æ£€ç´¢æ¨¡å—ã€‚åˆ©ç”¨promptè®¾è®¡åŒºåˆ†ä¸åŒè´¨é‡æ–‡æ¡£ï¼Œè®©æ¨¡æ‹Ÿç”¨çš„å¤§æ¨¡å‹å­¦ä¼šç”Ÿæˆä¸åŒè´¨é‡æ–‡æ¡£ï¼Œæ›¿ä»£çœŸå®æœç´¢å¼•æ“è¿”å›å†…å®¹ï¼Œæ—¢è§„é¿APIæˆæœ¬ï¼Œåˆèƒ½æ§åˆ¶æ–‡æ¡£è´¨é‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯¾ç¨‹å¼rolloutç­–ç•¥æ¸è¿›è®­ç»ƒ  
RLè®­ç»ƒé˜¶æ®µé‡‡ç”¨åŸºäºè¯¾ç¨‹çš„rolloutç­–ç•¥ï¼Œè®©ç”Ÿæˆæ–‡æ¡£çš„è´¨é‡é€æ­¥é™ä½ï¼Œæ¨¡æ‹Ÿè¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„æ£€ç´¢åœºæ™¯ï¼Œå¾ªåºæ¸è¿›æ¿€å‘æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚å…ˆè®©ç­–ç•¥æ¨¡å‹å­¦ä¹ åŸºç¡€è¾“å‡ºæ ¼å¼ä¸ä»»åŠ¡è¦æ±‚ï¼Œå†é€‚åº”æ›´å¤æ‚å¸¦å™ªå£°çš„æ£€ç´¢åœºæ™¯ï¼Œå®ç°èƒ½åŠ›é˜¶æ¢¯å¼æå‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒéªŒè¯ZeroSearchæœ‰æ•ˆæ€§ï¼šç”¨3Bå‚æ•°å¤§æ¨¡å‹ä½œæ£€ç´¢æ¨¡å—æ—¶ï¼Œèƒ½æœ‰æ•ˆæ¿€å‘ç­–ç•¥æ¨¡å‹ä½¿ç”¨çœŸå®æœç´¢å¼•æ“çš„èƒ½åŠ›ï¼›7Bå‚æ•°æ¨¡å‹ä½œæ¨¡æ‹Ÿæ£€ç´¢æ¨¡å—ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥æ¨¡å‹æ€§èƒ½ä¸çœŸå®æœç´¢å¼•æ“è®­ç»ƒçš„ç›¸å½“ï¼›14Bå‚æ•°æ¨¡å‹ä½œæ¨¡æ‹Ÿæ—¶ï¼Œæ€§èƒ½ç”šè‡³è¶…è¶ŠçœŸå®æœç´¢å¼•æ“ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä¸Šæ³›åŒ–æ€§å¥½ï¼Œè¿˜èƒ½å…¼å®¹REINFORCEã€PPOã€GRPOç­‰å¤šç§RLç®—æ³•ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šå·§å¦™åˆ©ç”¨å¤§æ¨¡å‹é¢„è®­ç»ƒç§¯ç´¯çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œç”¨æ¨¡å‹æ¨¡æ‹Ÿæœç´¢å¼•æ“ï¼Œç»•å¼€çœŸå®æœç´¢APIè°ƒç”¨æˆæœ¬ä¸è´¨é‡ä¸å¯æ§é—®é¢˜ï¼Œä¸ºRLç»“åˆæœç´¢åœºæ™¯è®­ç»ƒå¼€è¾Ÿæ–°è·¯å¾„ã€‚  
2. è®­ç»ƒç­–ç•¥ï¼šè¯¾ç¨‹å¼rolloutæœºåˆ¶ä¸ºå¤„ç†â€œéš¾åº¦é€’å¢ä»»åŠ¡â€ç±»è®­ç»ƒæä¾›å‚è€ƒï¼Œå¯è¿ç§»åˆ°éœ€é€æ­¥æå‡æ¨¡å‹èƒ½åŠ›çš„åœºæ™¯ï¼›è½»é‡ç›‘ç£å¾®è°ƒå¿«é€Ÿæ„å»ºåŠŸèƒ½æ¨¡å—çš„æ€è·¯ï¼Œåœ¨æ‰“é€ ç‰¹å®šèƒ½åŠ›å­æ¨¡å—æ—¶å€¼å¾—å€Ÿé‰´ã€‚  
3. å…¼å®¹æ€§ï¼šå¯¹ä¸åŒç±»å‹ï¼ˆåŸºç¡€/æŒ‡ä»¤å¾®è°ƒï¼‰ã€ä¸åŒå‚æ•°è§„æ¨¡å¤§æ¨¡å‹ä»¥åŠå¤šç§RLç®—æ³•çš„è‰¯å¥½å…¼å®¹æ€§ï¼Œè®©è¯¥æ¡†æ¶åœ¨å·¥ä¸šç•Œæˆ–ç ”ç©¶ä¸­æ›´æ˜“è½åœ°æ‹“å±•ï¼Œå‡å°‘é€‚é…æˆæœ¬ã€‚  
```

## reinforcement-learning-for-reasoning-in-large-language-models-with-one-training-example
### Abstract
We show that reinforcement learning with verifiable reward using one training
example (1-shot RLVR) is effective in incentivizing the mathematical reasoning
capabilities of large language models (LLMs). Applying RLVR to the base model
Qwen2.5-Math-1.5B, we identify a single example that elevates model performance
on MATH500 from 36.0% to 73.6%, and improves the average performance across six
common mathematical reasoning benchmarks from 17.6% to 35.7%. This result
matches the performance obtained using the 1.2k DeepScaleR subset (MATH500:
73.6%, average: 35.9%), which includes the aforementioned example. Furthermore,
RLVR with only two examples even slightly exceeds these results (MATH500:
74.8%, average: 36.6%). Similar substantial improvements are observed across
various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct,
DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different
math examples (when employed as a single training example). In addition, we
identify some interesting phenomena during 1-shot RLVR, including cross-domain
generalization, increased frequency of self-reflection, and sustained test
performance improvement even after the training accuracy has saturated, a
phenomenon we term post-saturation generalization. Moreover, we verify that the
effectiveness of 1-shot RLVR primarily arises from the policy gradient loss,
distinguishing it from the "grokking" phenomenon. We also show the critical
role of promoting exploration (e.g., by incorporating entropy loss with an
appropriate coefficient) in 1-shot RLVR training. We also further discuss
related observations about format correction, label robustness and prompt
modification. These findings can inspire future work on RLVR efficiency and
encourage a re-examination of recent progress and the underlying mechanisms in
RLVR. Our code, model, and data are open source at
https://github.com/ypwang61/One-Shot-RLVR.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»…ç”¨1ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œå¤§å¹…æå‡å¤§æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ï¼šRLVRæ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æå‡çš„ç ”ç©¶ä¸­ï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ˜¯å…³é”®æ–¹æ³•ä¹‹ä¸€ã€‚ä¸è¿‡ç›®å‰RLVRåœ¨æ•°æ®å±‚é¢çš„æ¢ç´¢è¾ƒå°‘ï¼Œæ¯”å¦‚â€œåˆ°åº•éœ€è¦å¤šå°‘æ•°æ®ï¼Ÿæ€æ ·çš„æ•°æ®æœ€æœ‰æ•ˆï¼Ÿâ€ç­‰æ ¸å¿ƒé—®é¢˜å°šæœªæ˜ç¡®ã€‚æ­¤å‰è™½æœ‰ç ”ç©¶å°è¯•å‡å°‘è®­ç»ƒæ•°æ®é‡ï¼Œä½†æœªæ¢ç´¢åˆ°æé™ç¨‹åº¦ã€‚æœ¬æ–‡èšç„¦äºâ€œRLVRè®­ç»ƒæ•°æ®é›†èƒ½ç²¾ç®€åˆ°ä½•ç§ç¨‹åº¦è¿˜èƒ½ä¿æŒæ€§èƒ½â€è¿™ä¸€é—®é¢˜ï¼Œå±•å¼€æ·±å…¥ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º1-shot RLVRèŒƒå¼  
è¯æ˜ä»…ç”¨**1ä¸ªè®­ç»ƒæ ·ä¾‹**å°±èƒ½é©±åŠ¨RLVRæå‡å¤§æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä»¥Qwen2.5-Math-1.5Bä¸ºåŸºç¡€æ¨¡å‹ï¼Œæ‰¾åˆ°å•ä¸ªæ ·ä¾‹å¯è®©MATH500ä»»åŠ¡å‡†ç¡®ç‡ä»36.0%è·ƒå‡è‡³73.6%ï¼Œ6ä¸ªæ•°å­¦æ¨ç†åŸºå‡†å¹³å‡æ€§èƒ½ä»17.6%æå‡åˆ°35.7%ï¼Œæ•ˆæœåª²ç¾å«è¯¥æ ·ä¾‹çš„1.2kè§„æ¨¡æ•°æ®é›†ï¼ˆDSR - subï¼‰ã€‚ç”šè‡³2ä¸ªæ ·ä¾‹æ—¶æ€§èƒ½è¿˜èƒ½å°å¹…è¶…è¶Šã€‚ä¸”è¯¥èŒƒå¼åœ¨ä¸åŒæ¨¡å‹ï¼ˆå¦‚Qwen2.5-Math-7Bã€Llama3.2-3B-Instructç­‰ï¼‰ã€ä¸åŒRLç®—æ³•ï¼ˆGRPOã€PPOï¼‰ä»¥åŠä¸åŒæ•°å­¦æ ·ä¾‹ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå‘ç°â€œåé¥±å’Œæ³›åŒ–â€ç­‰æ–°ç°è±¡  
åœ¨1-shot RLVRè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°ç‹¬ç‰¹ç°è±¡ï¼š  
- **åé¥±å’Œæ³›åŒ–**ï¼šæ¨¡å‹åœ¨å•ä¸ªè®­ç»ƒæ ·ä¾‹ä¸Šçš„è®­ç»ƒå‡†ç¡®ç‡å¾ˆå¿«æ¥è¿‘100%åï¼Œæµ‹è¯•å‡†ç¡®ç‡ä»æŒç»­æå‡ï¼›ä¸”çº¦1.4kè®­ç»ƒæ­¥åæ‰å‡ºç°è¿‡æ‹Ÿåˆï¼Œè¿‡æ‹Ÿåˆåè®­ç»ƒæ ·ä¾‹è¾“å‡ºè™½æ··ä¹±ï¼Œä½†æµ‹è¯•æ€§èƒ½ä¾æ—§å¼ºåŠ²ã€æµ‹è¯•è¾“å‡ºä¹Ÿä¿æŒå¯è¯»ã€‚  
- **è·¨åŸŸæ³›åŒ–**ï¼šåœ¨ä¸€ä¸ªé¢†åŸŸï¼ˆå¦‚å‡ ä½•ï¼‰çš„å•ä¸ªæ ·ä¾‹ä¸Šè®­ç»ƒï¼Œèƒ½æå‡å…¶ä»–é¢†åŸŸï¼ˆå¦‚ä»£æ•°ã€æ•°è®ºï¼‰çš„æ€§èƒ½ã€‚  
- **è‡ªåæ€å¢å¼º**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®­ç»ƒæ ·ä¾‹çš„å“åº”é•¿åº¦å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­è‡ªåæ€ç±»æœ¯è¯­å‡ºç°é¢‘ç‡å¢åŠ ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ­ç¤ºRLVRæ€§èƒ½æå‡å…³é”®å› ç´   
é€šè¿‡æ¶ˆèå®éªŒæ˜ç¡®ï¼š1-shot RLVRçš„æ€§èƒ½æå‡ä¸»è¦ç”±**ç­–ç•¥æ¢¯åº¦æŸå¤±**é©±åŠ¨ï¼Œå’Œä¾èµ–æƒé‡è¡°å‡ç­‰æ­£åˆ™åŒ–çš„â€œgrokkingâ€ç°è±¡æœºåˆ¶ä¸åŒã€‚åŒæ—¶å¼ºè°ƒâ€œä¿ƒè¿›æ¢ç´¢â€çš„å…³é”®ä½œç”¨â€”â€”åŠ å…¥åˆé€‚ç³»æ•°çš„ç†µæŸå¤±èƒ½è¿›ä¸€æ­¥ææ•ˆï¼›ç”šè‡³ä»…ç”¨ç†µæŸå¤±ï¼ˆæ— ç»“æœå¥–åŠ±ï¼‰ï¼ŒQwen2.5-Math-1.5Båœ¨MATH500ä¸Šä¹Ÿèƒ½æå‡27.4%ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ•°æ®è§„æ¨¡å¯¹æ¯”ï¼š1ä¸ªæ ·ä¾‹çš„RLVRåœ¨MATH500ï¼ˆ73.6%ï¼‰å’Œ6ä¸ªåŸºå‡†å¹³å‡æ€§èƒ½ï¼ˆ35.7%ï¼‰ä¸Šï¼Œæ•ˆæœæ¥è¿‘1.2kçš„DSR - subï¼ˆMATH500ä¸º73.6%ã€å¹³å‡35.9%ï¼‰ï¼›2ä¸ªæ ·ä¾‹æ—¶ï¼ˆMATH500è¾¾74.8%ã€å¹³å‡36.6%ï¼‰è¿˜èƒ½å°å¹…è¶…è¶Šã€‚  
2. æ¨¡å‹ä¸ç®—æ³•æ³›åŒ–æ€§ï¼šåœ¨Qwen2.5-Math-7Bã€Llama3.2-3B-Instructã€DeepSeek-R1-Distill-Qwen-1.5Bç­‰æ¨¡å‹ï¼Œä»¥åŠGRPOã€PPOç®—æ³•ä¸Šï¼Œ1(few)-shot RLVRéƒ½å®ç°äº†æ€§èƒ½å¤§å¹…æå‡ã€‚  
3. éæ•°å­¦æ¨ç†ä»»åŠ¡æ‹“å±•ï¼šä»¥æ•°å­¦æ ·ä¾‹åš1-shot RLVRï¼Œè¿˜èƒ½æå‡æ¨¡å‹åœ¨ARCç­‰éæ•°å­¦æ¨ç†ä»»åŠ¡çš„è¡¨ç°ï¼Œç”šè‡³è¶…è¿‡å…¨é‡æ•°æ®çš„RLVRã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é«˜æ•ˆæ€§å¯ç¤ºï¼šæ‰“ç ´â€œRLVRéœ€è¦å¤§è§„æ¨¡æ•°æ®é›†â€çš„å›ºæœ‰è®¤çŸ¥ï¼Œè¯æ˜æå°æ•°æ®é‡ï¼ˆ1æˆ–2ä¸ªæ ·ä¾‹ï¼‰å°±èƒ½é©±åŠ¨å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›è·ƒå‡ï¼Œä¸ºåç»­RLVRæ•°æ®æ•ˆç‡ä¼˜åŒ–æŒ‡æ˜æ–°æ–¹å‘ï¼Œå¯é‡æ–°å®¡è§†RLVRä¸­æ•°æ®ä¸æ€§èƒ½çš„å…³ç³»ã€‚  
2. æ–°ç°è±¡ä¸æœºåˆ¶ç†è§£ï¼šâ€œåé¥±å’Œæ³›åŒ–â€â€œè·¨åŸŸæ³›åŒ–â€ç­‰ç°è±¡çš„å‘ç°ï¼Œä»¥åŠç­–ç•¥æ¢¯åº¦ã€ç†µæŸå¤±ç­‰å…³é”®å› ç´ çš„éªŒè¯ï¼Œä¸ºæ·±å…¥ç†è§£RLVRå†…åœ¨æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œåç»­ç ”ç©¶å¯å›´ç»•è¿™äº›ç°è±¡å’Œæœºåˆ¶å±•å¼€æ›´ç»†ç²’åº¦çš„åˆ†æã€‚  
3. å·¥ç¨‹å®è·µå‚è€ƒï¼šç†µæŸå¤±çš„ææ•ˆå®éªŒï¼ˆç”šè‡³æ— å¥–åŠ±ä»…ç†µæŸå¤±ä¹Ÿèƒ½æ¶¨ç‚¹ï¼‰ï¼Œä»¥åŠæ ¼å¼ä¿®æ­£ã€æ ‡ç­¾é²æ£’æ€§ã€promptä¿®æ”¹ç­‰é™„åŠ è§‚å¯Ÿï¼Œèƒ½ä¸ºå®é™…è½åœ°ä¸­ä¼˜åŒ–RLVRè®­ç»ƒæµç¨‹ã€è°ƒä¼˜æ¨¡å‹æä¾›ç›´æ¥çš„å®è·µå‚è€ƒã€‚  
```

## deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments
### Abstract
Large Language Models (LLMs) equipped with web search capabilities have
demonstrated impressive potential for deep research tasks. However, current
approaches predominantly rely on either manually engineered prompts (prompt
engineering-based) with brittle performance or reinforcement learning within
controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that
fail to capture the complexities of real-world interaction. In this paper, we
introduce DeepResearcher, the first comprehensive framework for end-to-end
training of LLM-based deep research agents through scaling reinforcement
learning (RL) in real-world environments with authentic web search
interactions. Unlike RAG-based approaches that assume all necessary information
exists within a fixed corpus, our method trains agents to navigate the noisy,
unstructured, and dynamic nature of the open web. We implement a specialized
multi-agent architecture where browsing agents extract relevant information
from various webpage structures and overcoming significant technical
challenges. Extensive experiments on open-domain research tasks demonstrate
that DeepResearcher achieves substantial improvements of up to 28.9 points over
prompt engineering-based baselines and up to 7.2 points over RAG-based RL
agents. Our qualitative analysis reveals emergent cognitive behaviors from
end-to-end RL training, including the ability to formulate plans,
cross-validate information from multiple sources, engage in self-reflection to
redirect research, and maintain honesty when unable to find definitive answers.
Our results highlight that end-to-end training in real-world web environments
is not merely an implementation detail but a fundamental requirement for
developing robust research capabilities aligned with real-world applications.
We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | DeepResearcherï¼šåœ¨çœŸå®ç¯å¢ƒä¸­ç”¨å¼ºåŒ–å­¦ä¹ èµ‹èƒ½æ·±åº¦ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆç½‘ç»œæœç´¢èƒ½åŠ›åï¼Œåœ¨æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼šä¸€ç±»æ˜¯ä¾èµ–äººå·¥è®¾è®¡æç¤ºè¯çš„æ–¹å¼ï¼Œæ€§èƒ½è„†å¼±ä¸ç¨³å®šï¼›å¦ä¸€ç±»æ˜¯åœ¨å—æ§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç¯å¢ƒå†…åšå¼ºåŒ–å­¦ä¹ ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œäº¤äº’çš„å¤æ‚æ€§ã€‚ä¸ºäº†çªç ´è¿™äº›å±€é™ï¼Œè®ºæ–‡æå‡ºè¦æ‰“é€ èƒ½åœ¨çœŸå®ç½‘ç»œç¯å¢ƒä¸‹ï¼Œé€šè¿‡è§„æ¨¡åŒ–å¼ºåŒ–å­¦ä¹ æ¥ç«¯åˆ°ç«¯è®­ç»ƒåŸºäºLLMçš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œä¹Ÿå°±æ˜¯DeepResearcherã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçœŸå®ç¯å¢ƒä¸‹çš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒ  
ä¸åŒäºRAGç±»æ–¹æ³•å‡è®¾æ‰€éœ€ä¿¡æ¯éƒ½åœ¨å›ºå®šè¯­æ–™åº“ï¼ŒDeepResearcherèšç„¦å¼€æ”¾ç½‘ç»œâ€œå˜ˆæ‚ã€æ— ç»“æ„ã€åŠ¨æ€â€çš„ç‰¹æ€§ï¼Œè®©æ™ºèƒ½ä½“å­¦ä¹ åœ¨çœŸå®ç½‘é¡µäº¤äº’é‡Œå¯¼èˆªï¼Œå®ç°å¯¹ç ”ç©¶æ™ºèƒ½ä½“çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œä¸å†å—é™äºå—æ§ç¯å¢ƒçš„å‡è®¾ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸“ç”¨å¤šæ™ºèƒ½ä½“æ¶æ„  
æ­å»ºäº†ç‰¹æ®Šçš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œå…¶ä¸­æµè§ˆæ™ºèƒ½ä½“è´Ÿè´£ä»å„ç±»ç½‘é¡µç»“æ„é‡Œæå–ç›¸å…³ä¿¡æ¯ï¼Œå…‹æœäº†ç½‘é¡µç»“æ„å¤šæ ·å¸¦æ¥çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œè®©æ™ºèƒ½ä½“åœ¨çœŸå®ç½‘é¡µåœºæ™¯ä¸‹èƒ½æœ‰æ•ˆè·å–ä¿¡æ¯ç”¨äºç ”ç©¶ä»»åŠ¡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡çš„å¤§é‡å®éªŒä¸­ï¼ŒDeepResearcherå¯¹æ¯”åŸºäºæç¤ºè¯å·¥ç¨‹çš„åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡æœ€é«˜å¯è¾¾28.9ä¸ªç™¾åˆ†ç‚¹ï¼›å¯¹æ¯”åŸºäºRAGçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œä¹Ÿèƒ½æå‡æœ€é«˜7.2ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†æè¿˜å‘ç°äº†ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¸¦æ¥çš„â€œæ¶Œç°è®¤çŸ¥è¡Œä¸ºâ€ï¼Œæ¯”å¦‚åˆ¶å®šè®¡åˆ’ã€å¤šæºä¿¡æ¯äº¤å‰éªŒè¯ã€è‡ªæˆ‘åæ€è°ƒæ•´ç ”ç©¶æ–¹å‘ã€æ‰¾ä¸åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ç­‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç ”ç©¶ä»·å€¼æ¥çœ‹ï¼Œè®ºæ–‡è¯æ˜äº†çœŸå®ç½‘ç»œç¯å¢ƒä¸‹çš„ç«¯åˆ°ç«¯è®­ç»ƒä¸åªæ˜¯â€œå®ç°ç»†èŠ‚â€ï¼Œæ›´æ˜¯æ‰“é€ è´´åˆçœŸå®åº”ç”¨çš„é²æ£’ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ï¼Œè¿™ä¸ºåç»­ç ”ç©¶æ™ºèƒ½ä½“çš„å¼€å‘æŒ‡æ˜äº†æ–¹å‘â€”â€”è¦é‡è§†çœŸå®åœºæ™¯äº¤äº’è®­ç»ƒã€‚ä»å·¥ç¨‹å®è·µè§’åº¦ï¼Œå…¶å¼€æºçš„DeepResearcheræ¡†æ¶ï¼ˆhttps://github.com/GAIR-NLP/DeepResearcherï¼‰èƒ½ä¸ºæƒ³æ¢ç´¢â€œå¼ºåŒ–å­¦ä¹ +çœŸå®ç½‘é¡µäº¤äº’â€åšæ·±åº¦ç ”ç©¶çš„å›¢é˜Ÿæˆ–ä¸ªäººæä¾›å‚è€ƒï¼Œå¤šæ™ºèƒ½ä½“å¤„ç†ç½‘é¡µä¿¡æ¯çš„æ€è·¯ä¹Ÿå€¼å¾—åœ¨å¤æ‚ä¿¡æ¯æŠ½å–ç±»ä»»åŠ¡ä¸­å€Ÿé‰´ã€‚è€Œä¸”å¯¹â€œæ¶Œç°è¡Œä¸ºâ€çš„åˆ†æï¼Œä¹Ÿå¯å‘ç ”ç©¶è€…å…³æ³¨è®­ç»ƒè¿‡ç¨‹ä¸­æ™ºèƒ½ä½“èƒ½åŠ›çš„è‡ªç„¶æˆé•¿ä¸å±•ç°~
```

## research--learning-to-reason-with-search-for-llms-via-reinforcement-learning
### Abstract
Large Language Models (LLMs) have shown remarkable capabilities in reasoning,
exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating
reasoning with external search processes remains challenging, especially for
complex multi-hop questions requiring multiple retrieval steps. We propose
ReSearch, a novel framework that trains LLMs to Reason with Search via
reinforcement learning without using any supervised data on reasoning steps.
Our approach treats search operations as integral components of the reasoning
chain, where when and how to perform searches is guided by text-based thinking,
and search results subsequently influence further reasoning. We train ReSearch
on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct
extensive experiments. Despite being trained on only one dataset, our models
demonstrate strong generalizability across various benchmarks. Analysis reveals
that ReSearch naturally elicits advanced reasoning capabilities such as
reflection and self-correction during the reinforcement learning process.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReSearchï¼šç”¨å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹å­¦ä¼šâ€œè¾¹æœç´¢è¾¹æ¨ç†â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼ŒåƒOpenAI - o1ã€DeepSeek - R1ç­‰æ¨¡å‹çš„æˆåŠŸå°±æ˜¯ä¾‹è¯ã€‚ä¸è¿‡ï¼Œå°†æ¨ç†ä¸å¤–éƒ¨æœç´¢æµç¨‹ç»“åˆä»é¢‡å…·æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é¢å¯¹å¤æ‚å¤šè·³é—®é¢˜ï¼ˆéœ€å¤šè½®æ£€ç´¢çš„é—®é¢˜ï¼‰æ—¶ã€‚ç°æœ‰å¤šæ­¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•å¤šä¾èµ–äººå·¥è®¾è®¡æç¤ºæˆ–å¯å‘å¼è§„åˆ™ï¼Œä¸ä»…è€—æ—¶è€—åŠ›ï¼Œå¯¹å¤æ‚é—®é¢˜ä¹Ÿç¼ºä¹æ‰©å±•æ€§ï¼›è€Œä¸”ç»™å¤šæ­¥RAGæ¡†æ¶æ ‡æ³¨æ¨ç†æ­¥éª¤æˆæœ¬é«˜ã€ä¸ç°å®ã€‚åŒæ—¶ï¼Œå½“ä¸‹åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å·¥ä½œï¼Œå¤§å¤šèšç„¦å†…éƒ¨æ¨ç†ï¼Œé²œå°‘æ¢ç´¢å¦‚ä½•æŠŠæ¨ç†ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æœ‰æ•ˆç»“åˆã€‚äºæ˜¯ï¼Œæœ¬æ–‡æå‡ºReSearchæ¡†æ¶ï¼Œæ—¨åœ¨ç”¨å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹å­¦ä¼šâ€œè¾¹æœç´¢è¾¹æ¨ç†â€ï¼Œæ— éœ€æ¨ç†æ­¥éª¤çš„æœ‰ç›‘ç£æ•°æ®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReSearchæ¡†æ¶ï¼Œå°†æœç´¢ä½œä¸ºæ¨ç†é“¾çš„ä¸€éƒ¨åˆ†
ReSearchæŠŠæœç´¢æ“ä½œè§†ä¸ºé“¾å¼æ¨ç†è¿‡ç¨‹çš„æœ‰æœºç»„æˆï¼Œæ¨ç†é“¾ä¸ä»…åŒ…å«åŸºäºæ–‡æœ¬çš„æ€è€ƒï¼ˆç”¨`[]`åŒ…è£¹ï¼‰ï¼Œè¿˜æœ‰æœç´¢æŸ¥è¯¢ï¼ˆç”¨`<search>` `</search>`åŒ…è£¹ï¼‰å’Œæ£€ç´¢ç»“æœï¼ˆç”¨`<result>` `</result>`åŒ…è£¹ï¼‰ã€‚ä½•æ—¶ã€å¦‚ä½•æ‰§è¡Œæœç´¢ç”±ä¹‹å‰çš„æ–‡æœ¬æ€è€ƒå¼•å¯¼ï¼Œæœç´¢ç»“æœåˆä¼šå½±å“åç»­æ–‡æœ¬æ€è€ƒã€‚å¹¶ä¸”ï¼Œè¯¥æ¡†æ¶ä¸æä¾›æ¨ç†æ­¥éª¤çš„æœ‰ç›‘ç£æ•°æ®è®©æ¨¡å‹æ¨¡ä»¿ï¼Œè€Œæ˜¯å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOç®—æ³•ï¼‰æ¿€åŠ±æ¨¡å‹â€œè¾¹æœç´¢è¾¹æ¨ç†â€ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ ä»å¤´è®­ç»ƒæ¨¡å‹
åœ¨Qwen2.5 - 7Bï¼ˆ- Instructï¼‰å’ŒQwen2.5 - 32Bï¼ˆ- Instructï¼‰æ¨¡å‹ä¸Šä»å¤´è®­ç»ƒReSearchã€‚åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ€è·¯ï¼Œé‡‡æ ·å¤šä¸ªâ€œè¾¹æœç´¢è¾¹æ¨ç†â€çš„é“¾ï¼ˆå³rolloutsï¼‰ï¼Œä¼˜åŒ–å¤§æ¨¡å‹ç­–ç•¥ï¼Œè®©ç”Ÿæˆé«˜å¥–åŠ±rolloutsçš„æ¦‚ç‡æœ€å¤§åŒ–ï¼Œä»¥æ­¤è®©æ¨¡å‹å­¦ä¼šåœ¨æ¨ç†ä¸­åˆç†è¿ç”¨æœç´¢ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRewardå»ºæ¨¡ä¸GRPOç®—æ³•ç»“åˆ
é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä½œä¸ºå­¦ä¹ ç®—æ³•ï¼Œå®ƒä»ä¸€ç»„rolloutsä¸­ä¼°è®¡åŸºçº¿ï¼Œè€Œéåƒè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰é‚£æ ·è®­ç»ƒå•ç‹¬çš„ critic æ¨¡å‹ã€‚é€šè¿‡è®¾è®¡å¥–åŠ±å»ºæ¨¡æ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒä¸­é€æ­¥æŒæ¡â€œè¾¹æœç´¢è¾¹æ¨ç†â€çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆéœ€å¤šæ­¥æ¨ç†å’Œå¤šæ¬¡ä¿¡æ¯æ£€ç´¢ï¼‰ä¸Šå¼€å±•å¤§é‡å®éªŒã€‚è®­ç»ƒåçš„ReSearchæ¨¡å‹ç›¸æ¯”åŸºçº¿æ¨¡å‹ï¼Œç»å¯¹æ€§èƒ½æå‡8.9% - 22.4%ä¸ç­‰ã€‚è€Œä¸”ä»…åœ¨ä¸€ä¸ªç‰¹å®šè®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œå´èƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œåˆ†æè®­ç»ƒè¿‡ç¨‹å‘ç°ï¼ŒReSearchåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½è‡ªç„¶æ¿€å‘åå°„ã€è‡ªæˆ‘ä¿®æ­£ç­‰é«˜çº§æ¨ç†èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šå°†å¤–éƒ¨æœç´¢æ·±åº¦èå…¥æ¨ç†é“¾ï¼Œä¸ºè§£å†³å¤§æ¨¡å‹â€œæ¨ç† + å¤–éƒ¨å·¥å…·â€ç»“åˆéš¾é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œä¸å†ä¾èµ–äººå·¥è®¾è®¡çš„ç¹çæç¤ºæˆ–å¯å‘å¼è§„åˆ™ï¼Œè€Œæ˜¯è®©æ¨¡å‹è‡ªä¸»å­¦ä¹ ä½•æ—¶ã€å¦‚ä½•ç”¨æœç´¢è¾…åŠ©æ¨ç†ã€‚
2. å¼ºåŒ–å­¦ä¹ åº”ç”¨ï¼šè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨æ— éœ€æ¨ç†æ­¥éª¤æœ‰ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½æœ‰æ•ˆè®­ç»ƒå¤§æ¨¡å‹è·å¾—â€œè¾¹æœç´¢è¾¹æ¨ç†â€èƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡å¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œåç»­å·¥ä½œå¯å€Ÿé‰´è¿™ç§â€œæ— ç›‘ç£æ•°æ®ä¾èµ–çš„RLè®­ç»ƒâ€æ€è·¯æ‹“å±•æ¨¡å‹èƒ½åŠ›ã€‚
3. æ³›åŒ–æ€§éªŒè¯ï¼šä»…ç”¨å•ä¸€è®­ç»ƒé›†è®­ç»ƒå´èƒ½åœ¨å¤šåŸºå‡†æµ‹è¯•è¡¨ç°è‰¯å¥½ï¼Œè¯´æ˜è¯¥æ¡†æ¶åœ¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›åŸ¹å…»ä¸Šæœ‰ä¼˜åŠ¿ï¼Œä¸ºæ‰“é€ æ›´é€šç”¨çš„å¤§æ¨¡å‹æ¨ç† + å·¥å…·ä½¿ç”¨èƒ½åŠ›æä¾›äº†å‚è€ƒï¼Œåç»­å¯å›´ç»•å¦‚ä½•è¿›ä¸€æ­¥æå‡æ³›åŒ–æ€§ã€é€‚é…æ›´å¤šå·¥å…·å±•å¼€ç ”ç©¶ã€‚
```

## simplerl-zoo--investigating-and-taming-zero-reinforcement-learning-for-open-base-models-in-the-wild
### Abstract
DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can
naturally emerge through a simple reinforcement learning (RL) framework with
rule-based rewards, where the training may directly start from the base
models-a paradigm referred to as zero RL training. Most recent efforts to
reproduce zero RL training have primarily focused on the Qwen2.5 model series,
which may not be representative as we find the base models already exhibit
strong instruction-following and self-reflection abilities. In this work, we
investigate zero RL training across 10 diverse base models, spanning different
families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,
Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several
key design strategies-such as adjusting format reward and controlling query
difficulty-we achieve substantial improvements in both reasoning accuracy and
response length across most settings. However, by carefully monitoring the
training dynamics, we observe that different base models exhibit distinct
patterns during training. For instance, the increased response length does not
always correlate with the emergence of certain cognitive behaviors such as
verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for
the first time in small models not from the Qwen family. We share the key
designs that enable successful zero RL training, along with our findings and
practices. To facilitate further research, we open-source the code, models, and
analysis tools.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SimpleRL-Zooï¼šæ¢ç´¢ä¸é©¯æœé‡ç”Ÿå¼€æºåŸºç¡€æ¨¡å‹çš„é›¶å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡è§£å†³ä¸­å±•ç°å‡ºé•¿æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œç±»åæ€æ¨ç†è¡Œä¸ºçš„èƒ½åŠ›ä»¤äººç©ç›®ï¼ŒDeepSeek - R1è¡¨æ˜ä»é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆå³base modelsï¼‰å¼€å§‹ï¼ŒåŸºäºè§„åˆ™å¥–åŠ±çš„çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½è®©é•¿CoTå’Œè‡ªæˆ‘åæ€è¡Œä¸ºè‡ªå‘å‡ºç°ï¼ˆé›¶RLè®­ç»ƒèŒƒå¼ï¼‰ã€‚ä½†æ­¤å‰é›¶RLè®­ç»ƒå¤ç°å·¥ä½œå¤šèšç„¦Qwen2.5ç³»åˆ—ï¼Œè€ŒQwen2.5åŸºç¡€æ¨¡å‹æœ¬èº«å°±æœ‰å¼ºæŒ‡ä»¤éµå¾ªå’Œè‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œä¸å…·ä»£è¡¨æ€§ï¼›ä¸”å¯¹æ¨¡å‹è¡Œä¸ºåˆ†æè¾ƒæµ…ï¼Œæœªæ˜ç¡®æ¨ç†è¡Œä¸ºæ˜¯å¦çœŸå˜ã€æœ‰æ•ˆæ¨ç†æ¶Œç°æœºåˆ¶æ˜¯å•¥ï¼Œç†è§£å­˜åœ¨ç¼ºå£ã€‚åŒæ—¶ä¹Ÿä¸æ¸…æ¥šå°ä¸”èƒ½åŠ›å¼±çš„å¼€æºåŸºç¡€æ¨¡å‹æ˜¯å¦æœ‰è¿™ç§æ¶Œç°ç°è±¡ã€‚ä¸ºæ›´é€æ˜ç†è§£ä¸åŒé‡ç”ŸåŸºç¡€æ¨¡å‹çš„é›¶RLè®­ç»ƒï¼Œæœ¬æ–‡å›´ç»•â€œé›¶RLè®­ç»ƒä¸­å„æ¨¡å‹æ¨ç†èƒ½åŠ›å’‹å‘å±•ï¼Ÿåˆå§‹ç¼ºå¼ºæŒ‡ä»¤éµå¾ªå’Œè‡ªéªŒè¯èƒ½åŠ›çš„åŸºç¡€æ¨¡å‹æ˜¯å¦è¿˜æœ‰â€˜é¡¿æ‚Ÿæ—¶åˆ»ï¼ˆaha momentï¼‰â€™ï¼Ÿä¸åŒåŸºç¡€æ¨¡å‹æˆåŠŸé›¶RLè®­ç»ƒçš„å…³é”®å› ç´ æ˜¯å•¥ï¼Ÿâ€è¿™äº›é—®é¢˜å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¹¿æ³›æ¨¡å‹è¦†ç›–çš„é›¶RLè®­ç»ƒæ¢ç´¢
åœ¨å¤šç§ç³»åˆ—å’Œè§„æ¨¡çš„æ¨¡å‹ä¸Šå¼€å±•é›¶RLè®­ç»ƒï¼Œæ¶µç›–Mistral - 7Bã€Mistral - 24Bã€Llama3 - 8Bã€DeepSeek - Math - 7Bã€Qwen2.5ä¸åŒå‚æ•°è§„æ¨¡ï¼ˆ0.5B/1.5B/7B/14B/32Bï¼‰ä»¥åŠQwen2.5 - Math - 7Bç­‰10ä¸ªä¸åŒåŸºç¡€æ¨¡å‹ï¼Œæ¢ç©¶ä¸åŒæ¨¡å‹åœ¨é›¶RLè®­ç»ƒä¸‹çš„è¡¨ç°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…³é”®è®¾è®¡ç­–ç•¥ä¸è®­ç»ƒè¦ç´ æŠŠæ§
é‡‡ç”¨GRPOä½œä¸ºRLç®—æ³•ï¼Œç»“åˆè°ƒæ•´æ ¼å¼å¥–åŠ±ã€æ§åˆ¶æŸ¥è¯¢éš¾åº¦ç­‰å…³é”®è®¾è®¡ç­–ç•¥å¼€å±•è®­ç»ƒï¼›è®­ç»ƒä»…ä¾èµ–GSM8Kå’ŒMATHæ•°æ®é›†åšåŸºäºè§„åˆ™çš„å¥–åŠ±å»ºæ¨¡ï¼Œä¸”å¯¹æ‰€æœ‰æ¨¡å‹é‡‡ç”¨ç›¸åŒè®­ç»ƒè¶…å‚æ•°ï¼Œä»¥æ­¤æ¢ç©¶é›¶RLè®­ç»ƒçš„å…³é”®å› ç´ ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šç»´åº¦è®­ç»ƒåŠ¨æ€ä¸è¡Œä¸ºç›‘æµ‹
é™¤ accuracy å’Œ response length å¤–ï¼Œè¿˜ç›‘æµ‹å¤šç§æŒ‡æ ‡ï¼Œæ·±å…¥æ¢ç©¶æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æ¨ç†è¡Œä¸ºç­‰å˜åŒ–ï¼Œæ¯”å¦‚å…³æ³¨è®¤çŸ¥è¡Œä¸ºï¼ˆå¦‚è‡ªæˆ‘åæ€ã€éªŒè¯ç­‰ï¼‰æ˜¯å¦æ¶Œç°ï¼Œä»¥æ›´å…¨é¢ç†è§£é›¶RLè®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. æ¨ç†ç²¾åº¦ä¸å“åº”é•¿åº¦æå‡ï¼šå€ŸåŠ©å…³é”®è®¾è®¡ç­–ç•¥ï¼Œåœ¨å¤šæ•°è®¾ç½®ä¸‹ï¼Œæ¨ç†ç²¾åº¦å’Œå“åº”é•¿åº¦éƒ½æœ‰å¤§å¹…æå‡ï¼Œ10ä¸ªæ¨¡å‹ä¸­9ä¸ªï¼ˆé™¤Qwen2.5 - Math - 7Bï¼‰å“åº”é•¿åº¦æ˜¾è‘—å¢åŠ ï¼Œæ‰€æœ‰åŸºç¡€æ¨¡å‹ç²¾åº¦éƒ½æœ‰æå‡ã€‚
2. å“åº”é•¿åº¦ä¸â€œé¡¿æ‚Ÿæ—¶åˆ»â€éå¿…ç„¶å…³è”ï¼šå¤šæ•°Qwen2.5æ¨¡å‹å“åº”é•¿åº¦å¢åŠ ä½†åƒè‡ªæˆ‘åæ€è¿™ç±»è®¤çŸ¥è¡Œä¸ºé¢‘ç‡æ²¡ä¸Šå‡ï¼Œè¯´æ˜å“åº”é•¿åº¦å¢åŠ ä¸æ€»å¯¹åº”â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼ˆç‰¹å®šè®¤çŸ¥è¡Œä¸ºæ¶Œç°ï¼‰ã€‚
3. éQwenå®¶æ—å°æ¨¡å‹å‡ºç°â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼šé¦–æ¬¡åœ¨Qwenå®¶æ—å¤–å°æ¨¡å‹ï¼ˆå¦‚Llama3 - 8Bå’ŒDeepSeek - Math - 7Bï¼‰ä¸­è§‚å¯Ÿåˆ°éªŒè¯ç­‰ç‰¹å®šè®¤çŸ¥æ¨ç†è¡Œä¸ºé¢‘ç‡æ˜¾è‘—å¢åŠ ã€‚
4. æ ¼å¼å¥–åŠ±ä¸æ•°æ®éš¾åº¦å½±å“ï¼šä¸¥æ ¼æ ¼å¼å¥–åŠ±ï¼ˆå¦‚ç­”æ¡ˆæ”¾æ¡†å†…ï¼‰å¯¹åˆå§‹æŒ‡ä»¤éµå¾ªå·®çš„åŸºç¡€æ¨¡å‹æ¢ç´¢æœ‰å¾ˆå¤§æƒ©ç½šï¼Œé™åˆ¶æ€§èƒ½ä¸Šé™è¿˜æ˜“å¼•å‘è¿‡åº¦æ€è€ƒï¼›è®­ç»ƒæ•°æ®éš¾åº¦éœ€ä¸åŸºç¡€æ¨¡å‹å†…åœ¨æ¢ç´¢èƒ½åŠ›åŒ¹é…ï¼Œå¦åˆ™é›¶RLä¼šå¤±è´¥ã€‚
5. é›¶RLè®­ç»ƒå¹¶éç®€å•é‡æ’åºï¼šé›¶RLè®­ç»ƒä½¿pass@kç²¾åº¦æå‡10 - 30ç»å¯¹ç‚¹ï¼Œè¯æ˜ä¸æ˜¯åªé‡æ’å“åº”ã€‚
6. SFTå†·å¯åŠ¨å¯¹RLçš„å½±å“ï¼šç”¨ä¼ ç»ŸSFTæ•°æ®é›†ä½œä¸ºRLå†·å¯åŠ¨ï¼ˆDeepSeek - R1å‘å¸ƒå‰å¸¸è§åšæ³•ï¼‰ï¼Œè™½é«˜è´¨é‡CoTæ•°æ®èƒ½é€šè¿‡æ¨¡ä»¿å¿«é€Ÿæå‡åŸºç¡€æ¨¡å‹æ€§èƒ½ï¼Œä½†ä¼šé™åˆ¶RLä¸­è‡ªç”±æ¢ç´¢èƒ½åŠ›ï¼Œé™ä½RLåæ€§èƒ½å¹¶æŠ‘åˆ¶é«˜çº§æ¨ç†èƒ½åŠ›æ¶Œç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹é€‰æ‹©ä¸è¦†ç›–ï¼šç ”ç©¶ä¸åŒå®¶æ—ã€ä¸åŒè§„æ¨¡åŸºç¡€æ¨¡å‹çš„é›¶RLè®­ç»ƒï¼Œä¸ºåç»­æ¢ç´¢ä¸åŒç±»å‹åŸºç¡€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æä¾›äº†å¹¿æ³›çš„å‚è€ƒæ ·æœ¬ï¼Œå¯ç¤ºç ”ç©¶è€…è¦è€ƒè™‘æ¨¡å‹å¤šæ ·æ€§ã€‚
2. è®­ç»ƒç­–ç•¥ä¸è¦ç´ ï¼šè°ƒæ•´æ ¼å¼å¥–åŠ±ã€æ§åˆ¶æŸ¥è¯¢éš¾åº¦ç­‰å…³é”®è®¾è®¡ç­–ç•¥åœ¨æå‡æ€§èƒ½ä¸Šæœ‰æˆæ•ˆï¼Œåç»­å·¥ä½œå¯å€Ÿé‰´è¿™äº›ç­–ç•¥ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼›åŒæ—¶å…³æ³¨è®­ç»ƒæ•°æ®éš¾åº¦ä¸æ¨¡å‹èƒ½åŠ›åŒ¹é…åº¦ï¼Œè¿™å¯¹é›¶RLè®­ç»ƒæˆåŠŸå¾ˆå…³é”®ã€‚
3. è¡Œä¸ºä¸åŠ¨æ€ç›‘æµ‹ï¼šé™¤ä¼ ç»Ÿç²¾åº¦å’Œé•¿åº¦æŒ‡æ ‡ï¼Œå…³æ³¨è®¤çŸ¥è¡Œä¸ºç­‰å¤šç»´åº¦æŒ‡æ ‡æ¥ç†è§£è®­ç»ƒè¿‡ç¨‹ï¼Œä¸ºæ·±å…¥åˆ†ææ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­è¡Œä¸ºå˜åŒ–æä¾›äº†æ€è·¯ï¼Œæœ‰åŠ©äºæ›´å…¨é¢è¯„ä¼°æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚
4. å¼€æºè´¡çŒ®ï¼šè®ºæ–‡æ‰¿è¯ºè¯„å®¡åå¼€æºä»£ç ã€æ¨¡å‹å’Œåˆ†æå·¥å…·ï¼Œè¿™ä¸ºè¯¥é¢†åŸŸåç»­ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œä¹Ÿé¼“åŠ±ç ”ç©¶è€…åœ¨åˆé€‚é˜¶æ®µå¼€æºæˆæœä¿ƒè¿›é¢†åŸŸå‘å±•ã€‚
```

## chain-of-retrieval-augmented-generation
### Abstract
This paper introduces an approach for training o1-like RAG models that
retrieve and reason over relevant information step by step before generating
the final answer. Conventional RAG methods usually perform a single retrieval
step before the generation process, which limits their effectiveness in
addressing complex queries due to imperfect retrieval results. In contrast, our
proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the
model to dynamically reformulate the query based on the evolving state. To
train CoRAG effectively, we utilize rejection sampling to automatically
generate intermediate retrieval chains, thereby augmenting existing RAG
datasets that only provide the correct final answer. At test time, we propose
various decoding strategies to scale the model's test-time compute by
controlling the length and number of sampled retrieval chains. Experimental
results across multiple benchmarks validate the efficacy of CoRAG, particularly
in multi-hop question answering tasks, where we observe more than 10 points
improvement in EM score compared to strong baselines. On the KILT benchmark,
CoRAG establishes a new state-of-the-art performance across a diverse range of
knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to
understand the scaling behavior of CoRAG, laying the groundwork for future
research aimed at developing factual and grounded foundation models.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´å•æ­¥æ£€ç´¢é™åˆ¶ï¼šCoRAGå¼€å¯å¤šæ­¥æ£€ç´¢å¢å¼ºç”Ÿæˆæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ä¼ä¸šåº”ç”¨ç­‰åœºæ™¯ä¸­ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒèƒ½è®©å¤§æ¨¡å‹ç»“åˆä¸“æœ‰æ•°æ®æºç”Ÿæˆå¯é ä¸”åŸºäºäº‹å®çš„å“åº”ã€‚ä½†ä¼ ç»ŸRAGå­˜åœ¨å±€é™ï¼šå¸¸è§„RAGåœ¨ç”Ÿæˆå‰ä»…æ‰§è¡Œ**å•æ­¥æ£€ç´¢**ï¼Œé¢å¯¹å¤æ‚æŸ¥è¯¢æ—¶ï¼Œå› æ£€ç´¢ç»“æœå¯èƒ½ä¸å®Œå–„ï¼Œéš¾ä»¥æœ‰æ•ˆåº”å¯¹ï¼›è€Œä¸”æ£€ç´¢æ¨¡å‹ä¸ºæ•ˆç‡é‡‡ç”¨çš„å›ºå®šå¤§å°å‘é‡è¡¨ç¤ºç­‰æ¶æ„ï¼Œé™åˆ¶äº†å¤„ç†å¤æ‚æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œåœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¹Ÿéš¾ç¡®å®šåˆå§‹è¯¥æ£€ç´¢ä»€ä¹ˆä¿¡æ¯ã€‚ä¸ºæ‰“ç ´æ£€ç´¢è´¨é‡ç“¶é¢ˆï¼Œè®©æ¨¡å‹èƒ½åƒäººç±»è§£å†³å¤æ‚é—®é¢˜é‚£æ ·è¿­ä»£æ£€ç´¢ä¿¡æ¯ï¼Œæœ¬æ–‡æå‡ºCoRAGæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCoRAGå¤šæ­¥åŠ¨æ€æ£€ç´¢ä¸æŸ¥è¯¢é‡æ„èŒƒå¼  
ä¼ ç»ŸRAGå•æ­¥æ£€ç´¢åç”Ÿæˆï¼ŒCoRAGåˆ™å…è®¸æ¨¡å‹**åŸºäºçŠ¶æ€æ¼”åŒ–åŠ¨æ€é‡æ„æŸ¥è¯¢**ï¼Œåœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå‰ï¼Œé€æ­¥æ£€ç´¢å’Œæ¨ç†ç›¸å…³ä¿¡æ¯ã€‚è¿™ç§æ–¹å¼æ¨¡æ‹Ÿäººç±»è§£å†³å¤æ‚é—®é¢˜æ—¶è¿­ä»£æ‰¾ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œèƒ½åœ¨æ£€ç´¢å™¨æ²¡è¿”å›æœ‰ç”¨ä¿¡æ¯æ—¶ï¼Œå°è¯•ä¸åŒæŸ¥è¯¢é‡å†™ç­–ç•¥ï¼Œæ¢ç´¢æŸ¥è¯¢çš„å¤šæ–¹é¢ä¿¡æ¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‹’ç»é‡‡æ ·å¢å¼ºæ•°æ®é›†ä¸æ˜¾å¼è®­ç»ƒ  
ç°æœ‰RAGæ•°æ®é›†å¸¸åªç»™æœ€ç»ˆæ­£ç¡®ç­”æ¡ˆï¼Œä¸ºæœ‰æ•ˆè®­ç»ƒCoRAGï¼Œæœ¬æ–‡ç”¨**æ‹’ç»é‡‡æ ·**è‡ªåŠ¨ç”Ÿæˆä¸­é—´æ£€ç´¢é“¾ï¼Œæ‰©å……æ•°æ®é›†ã€‚ä¹‹åç”¨å¼€æºè¯­è¨€æ¨¡å‹åœ¨è¿™äº›å¢å¼ºæ•°æ®é›†ä¸Šï¼Œä»¥æ ‡å‡†ä¸‹ä¸€ä¸ªtokené¢„æµ‹ç›®æ ‡å¾®è°ƒï¼Œè®©æ¨¡å‹æ˜¾å¼å­¦ä¹ é€æ­¥æ£€ç´¢çš„èƒ½åŠ›ï¼Œè€Œéä»…ä¾èµ–æ¨¡å‹ä¸Šä¸‹æ–‡å­¦ä¹ æˆ–ä¸“æœ‰æ¨¡å‹è’¸é¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ ·æµ‹è¯•æ—¶è§£ç ç­–ç•¥è°ƒæ§è®¡ç®—èµ„æº  
æµ‹è¯•é˜¶æ®µï¼Œæå‡ºè´ªå¿ƒè§£ç ã€best - of - Né‡‡æ ·ã€æ ‘æœç´¢ç­‰**å¤šç§è§£ç ç­–ç•¥**ï¼Œé€šè¿‡æ§åˆ¶é‡‡æ ·æ£€ç´¢é“¾çš„é•¿åº¦å’Œæ•°é‡ï¼Œè°ƒèŠ‚æ¨¡å‹æµ‹è¯•æ—¶çš„è®¡ç®—å¼€é”€ï¼ˆå¦‚tokenæ¶ˆè€—ã€æ£€ç´¢å™¨è°ƒç”¨é¢‘ç‡ç­‰ï¼‰ï¼Œå®ç°æµ‹è¯•æ—¶è®¡ç®—èµ„æºçš„çµæ´»ç¼©æ”¾ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
- å¤šè·³é—®ç­”ä»»åŠ¡ï¼šåœ¨å¤šè·³æ¨ç†ç±»QAä»»åŠ¡ä¸­ï¼Œç›¸æ¯”å¼ºåŸºçº¿ï¼ŒCoRAGåœ¨EMåˆ†æ•°ä¸Šæå‡è¶…10ä¸ªç‚¹ï¼ŒéªŒè¯äº†å…¶å¤„ç†éœ€å¤šæ­¥æ£€ç´¢æ¨ç†å¤æ‚ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚  
- KILTåŸºå‡†æµ‹è¯•ï¼šåœ¨æ¶µç›–å¤šæ ·çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„KILTåŸºå‡†ä¸Šï¼ŒCoRAGåœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡çš„éšè—æµ‹è¯•é›†ä¸Šå–å¾—å…¨æ–°SOTAæˆç»©ã€‚  
- ç¼©æ”¾è¡Œä¸ºåˆ†æï¼šä¸åŒè§£ç ç­–ç•¥ä¸‹ï¼Œæ€»tokenæ¶ˆè€—å’Œæ¨¡å‹æ€§èƒ½è¿‘ä¼¼å‘ˆå¯¹æ•°çº¿æ€§å…³ç³»ï¼ˆç³»æ•°å› æ•°æ®é›†è€Œå¼‚ï¼‰ï¼›è¿˜å‘ç°å¯¹ä¸åŒä»»åŠ¡ç±»å‹CoRAGç¼©æ”¾è¡Œä¸ºä¸åŒï¼Œå¦‚NQè¿™ç±»ç°æœ‰SOTAæ£€ç´¢å™¨å·²é«˜å¬å›çš„æ•°æ®é›†ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾æ”¶ç›Šæœ‰é™ï¼Œä¸ºä¾æŸ¥è¯¢å¤æ‚åº¦å’Œæ£€ç´¢å™¨è´¨é‡åŠ¨æ€åˆ†é…æµ‹è¯•è®¡ç®—èµ„æºæä¾›ä¾æ®ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æŠ€æœ¯èŒƒå¼å±‚é¢ï¼šCoRAGæå‡ºçš„å¤šæ­¥åŠ¨æ€æ£€ç´¢ + æŸ¥è¯¢é‡æ„æ€è·¯ï¼Œä¸ºRAGçªç ´å•æ­¥é™åˆ¶ã€å¤„ç†å¤æ‚ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ï¼Œåç»­ç ”ç©¶å¯åœ¨æ­¤åŸºç¡€ä¸Šæ¢ç´¢æ›´çµæ´»çš„æ£€ç´¢ - ç”Ÿæˆäº¤äº’é€»è¾‘ã€‚  
- æ•°æ®å¢å¼ºå±‚é¢ï¼šç”¨æ‹’ç»é‡‡æ ·è‡ªåŠ¨ç”Ÿæˆä¸­é—´æ£€ç´¢é“¾æ¥å¢å¼ºæ•°æ®é›†ï¼Œä¸ºè§£å†³RAGæ•°æ®é›†ç¼ºå°‘ä¸­é—´è¿‡ç¨‹æ ‡æ³¨çš„é—®é¢˜æä¾›äº†å¯è¡Œæ–¹æ³•ï¼Œå¯å€Ÿé‰´åˆ°éœ€ä¸­é—´æ­¥éª¤æ•°æ®çš„æ¨¡å‹è®­ç»ƒåœºæ™¯ã€‚  
- æµ‹è¯•ä¼˜åŒ–å±‚é¢ï¼šé€šè¿‡å¤šæ ·è§£ç ç­–ç•¥è°ƒæ§æµ‹è¯•æ—¶è®¡ç®—ï¼Œè¿™ç§æ ¹æ®ä»»åŠ¡å’Œæ£€ç´¢å™¨æƒ…å†µçµæ´»åˆ†é…èµ„æºçš„æ€è·¯ï¼Œå¯¹å¤§æ¨¡å‹å®é™…éƒ¨ç½²æ—¶å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬å¾ˆæœ‰å¯å‘ï¼Œå¯ç”¨äºä¼˜åŒ–æ¨¡å‹æ¨ç†é˜¶æ®µçš„èµ„æºåˆ©ç”¨ã€‚  
- ç ”ç©¶æ–¹å‘å±‚é¢ï¼šCoRAGä¸ºå¼€å‘äº‹å®æ€§ã€æœ‰ä¾æ®çš„åŸºç¡€æ¨¡å‹ï¼ˆfoundation modelsï¼‰å¥ å®šåŸºç¡€ï¼Œåç»­åœ¨å‡å°‘æ¨¡å‹å¹»è§‰ã€æå‡å†…å®¹å¯é æ€§æ–¹é¢ï¼Œå¯å»¶ç»­å…¶å¤šæ­¥æ£€ç´¢æ¨ç†çš„æ€è·¯æ·±å…¥æ¢ç´¢ã€‚  
```

## rare--retrieval-augmented-reasoning-modeling
### Abstract
Domain-specific intelligence demands specialized knowledge and sophisticated
reasoning for problem-solving, posing significant challenges for large language
models (LLMs) that struggle with knowledge hallucination and inadequate
reasoning capabilities under constrained parameter budgets. Inspired by Bloom's
Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning
Modeling (RARE), a novel paradigm that decouples knowledge storage from
reasoning optimization. RARE externalizes domain knowledge to retrievable
sources and internalizes domain-specific reasoning patterns during training.
Specifically, by injecting retrieved knowledge into training prompts with
masked losses, RARE transforms learning objectives from rote memorization to
contextualized reasoning. It enables models to bypass parameter-intensive
memorization and prioritize the development of higher-order cognitive
processes. Extensive experiments demonstrate that lightweight RARE-trained
models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance,
surpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\%
accuracy. RARE establishes a paradigm shift where maintainable external
knowledge bases synergize with compact, reasoning-optimized models,
collectively driving more scalable domain-specific intelligence.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RAREï¼šè§£é”é¢†åŸŸæ™ºèƒ½æ–°èŒƒå¼ï¼Œè®©æ¨ç†ä¸çŸ¥è¯†â€œå„å¸å…¶èŒâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½†é¢å¯¹é¢†åŸŸç‰¹å®šæ™ºèƒ½ä»»åŠ¡æ—¶ï¼Œå´å—é™äºä¸¤å¤§æ ¸å¿ƒéš¾é¢˜ï¼š**çŸ¥è¯†å¹»è§‰**ï¼ˆå³ä¾¿å‚æ•°è§„æ¨¡æå¤§ï¼Œé¢†åŸŸçŸ¥è¯†çš„é•¿å°¾åˆ†å¸ƒä»è®©æ¨¡å‹æ˜“â€œèƒ¡è¨€ä¹±è¯­â€ï¼‰ä¸**æ¨ç†èƒ½åŠ›ä¸è¶³**ï¼ˆåœ¨æœ‰é™å‚æ•°é¢„ç®—ä¸‹ï¼Œæ¨¡å‹éš¾å…¼é¡¾çŸ¥è¯†è®°å¿†ä¸é«˜é˜¶æ¨ç†ï¼‰ã€‚  

æ•™è‚²ç†è®ºä¸­å¸ƒé²å§†åˆ†ç±»æ³•ï¼ˆBloomâ€™s Taxonomyï¼‰ç»™äº†å¯å‘ï¼šè®¤çŸ¥ä»åŸºç¡€â€œè®°å¿†â€åˆ°é«˜é˜¶â€œåˆ†æã€è¯„ä¼°ã€åˆ›é€ â€å‘ˆå±‚çº§ç»“æ„ã€‚è€Œç°æœ‰èŒƒå¼ï¼ˆå¦‚æ— é€‚é…ç›´æ¥è°ƒç”¨é€šç”¨æ¨¡å‹ã€ä»…è¡¥çŸ¥è¯†çš„æ£€ç´¢å¢å¼ºç”ŸæˆRAGã€ä¼ ç»Ÿé—­å·å¼é¢„è®­ç»ƒ/å¾®è°ƒï¼‰è¦ä¹ˆç¼ºé¢†åŸŸçŸ¥è¯†ä¸æ¨ç†ã€è¦ä¹ˆé‡çŸ¥è¯†è½»æ¨ç†å­¦ä¹ ã€è¦ä¹ˆè®°å¿†æˆæœ¬é«˜è¿˜æ˜“å¹»è§‰ã€‚  

æ ¸å¿ƒé—®é¢˜å‘¼ä¹‹æ¬²å‡ºï¼š**èƒ½å¦åœ¨æœ‰é™èµ„æºä¸‹ï¼Œè§£è€¦é¢†åŸŸçŸ¥è¯†è®°å¿†ä¸æ¨ç†èƒ½åŠ›åŸ¹å…»ï¼Œè®©æ¨¡å‹ä¼˜å…ˆå‘å±•é«˜é˜¶æ¨ç†ï¼Ÿ** è¿™æ­£æ˜¯RAREè¯•å›¾å›ç­”çš„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šèŒƒå¼é©æ–°â€”â€”â€œæœ‰å¤‡å¼€å·è€ƒâ€ï¼ˆPrepared Open - book Examï¼‰  
RAREæ‰“ç ´â€œçŸ¥è¯†å­˜å‚¨â€ä¸â€œæ¨ç†ä¼˜åŒ–â€çš„ç»‘å®šï¼Œæå‡ºâ€œé¢†åŸŸçŸ¥è¯†å¤–ç½®ï¼Œé¢†åŸŸæ¨ç†å†…åŒ–â€ã€‚è®­ç»ƒæ—¶ï¼Œæ¨¡å‹ä¸æŠŠèµ„æºæµªè´¹åœ¨æ­»è®°ç¡¬èƒŒé¢†åŸŸçŸ¥è¯†ï¼Œè€Œæ˜¯ä¸“æ³¨å­¦**é¢†åŸŸç‰¹å®šæ¨ç†æ¨¡å¼**ï¼›æ¨ç†æ—¶ï¼Œå†ä»å¤–éƒ¨çŸ¥è¯†åº“åŠ¨æ€æ‹‰å–æ‰€éœ€çŸ¥è¯†ã€‚è¿™å°±åƒå­¦ç”Ÿå¤‡è€ƒï¼šå¹³æ—¶ç»ƒè§£é¢˜é€»è¾‘ï¼ˆæ¨ç†ï¼‰ï¼Œè€ƒè¯•æ—¶å…è®¸æŸ¥èµ„æ–™ï¼ˆå¤–éƒ¨çŸ¥è¯†ï¼‰ï¼ŒæŠŠæ¨¡å‹å®¹é‡ä»â€œè®°å¿†â€é‡åˆ†é…åˆ°â€œæ¨ç†â€ï¼Œå…¼é¡¾æ•ˆç‡ä¸çŸ¥è¯†å‡†ç¡®æ€§/å¯æ›´æ–°æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒæœºåˆ¶â€”â€”çŸ¥è¯†æ³¨å…¥ä¸æŸå¤±é‡æ„  
é€šè¿‡â€œæ£€ç´¢çŸ¥è¯†æ³¨å…¥è®­ç»ƒprompt + æ©ç æŸå¤±â€ï¼ŒRAREæŠŠå­¦ä¹ ç›®æ ‡ä»â€œæ­»è®°ç¡¬èƒŒâ€æ‰­å‘â€œæƒ…å¢ƒåŒ–æ¨ç†â€ã€‚ä¸å†è®©æ¨¡å‹ä¸ºè®°çŸ¥è¯†çŠ¯éš¾ï¼Œè€Œæ˜¯æŠŠçŸ¥è¯†ç›¸å…³æŸå¤±ï¼ˆå¦‚è®°å¿†é”™è¯¯ï¼‰è½¬åŒ–ä¸ºâ€œçŸ¥è¯†æ€ä¹ˆç”¨â€çš„åº”ç”¨å¯¼å‘æŸå¤±ã€‚ä¸¾ä¸ªä¾‹å­ï¼šå­¦â€œé‡åŒ–å®½æ¾å¦‚ä½•å½±å“å€ºåˆ¸æ”¶ç›Šç‡â€æ—¶ï¼Œå¤–éƒ¨çŸ¥è¯†ï¼ˆQEä¹°å€ºã€æ”¶ç›Šç‡ä¸ä»·æ ¼è´Ÿç›¸å…³ï¼‰è¢«æ³¨å…¥ï¼Œæ¨¡å‹è¦å­¦çš„æ˜¯â€œä¹°å€ºâ†’éœ€æ±‚å‡â†’ä»·æ ¼å‡â†’æ”¶ç›Šç‡é™â€è¿™ä¸€æ¨ç†é“¾ï¼Œè€Œéç¡¬è®°ç»“è®ºã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MedQAã€PubMedQAã€CoVERTç­‰å¤šé¢†åŸŸåŸºå‡†æµ‹è¯•ä¸­ï¼Œè½»é‡çš„RAREè®­ç»ƒæ¨¡å‹ï¼ˆå¦‚Llama - 3.1 - 8Bï¼‰è¡¨ç°äº®çœ¼ï¼š  
- PubMedQAä¸Šï¼ŒLlama3.1 - 8B + RAREè¾¾76.8%å‡†ç¡®ç‡ï¼Œè¶…è¿‡å¸¦æ£€ç´¢å¢å¼ºçš„DeepSeek - R1ï¼ˆ75.4%ï¼‰ä¸GPT - 4ï¼ˆ75.2%ï¼‰ï¼›  
- CoVERTä»»åŠ¡ä¸­ï¼ŒRAREç‰ˆLlama3.1 - 8Bå‡†ç¡®ç‡81.7%ï¼Œè¿œè¶…DeepSeek - R1 + RAGï¼ˆ63.3%ï¼‰å’ŒGPT - 4ï¼ˆ65.7%ï¼‰ï¼›  
æ•´ä½“åœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼Œæ¯”æ£€ç´¢å¢å¼ºçš„GPT - 4ã€DeepSeek - R1ç­‰é¢†å…ˆè¶…20%å‡†ç¡®ç‡ï¼Œè¯æ˜å°å‚æ•°æ¨¡å‹é RAREä¹Ÿèƒ½åœ¨é¢†åŸŸä»»åŠ¡ä¸­å¹²ç¿»å¤§æ¨¡å‹+ä¼ ç»ŸRAGçš„ç»„åˆã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **è·¨å­¦ç§‘æ€è·¯**ï¼šæŠŠæ•™è‚²ç†è®ºï¼ˆå¸ƒé²å§†åˆ†ç±»æ³•ï¼‰ä¸AIç»“åˆï¼Œä¸ºâ€œçŸ¥è¯† - æ¨ç†å¹³è¡¡â€éš¾é¢˜æä¾›æ–°è§†è§’ï¼Œå¯ç¤ºåç»­ç ”ç©¶ä»è®¤çŸ¥ç§‘å­¦æ‰¾çµæ„Ÿï¼›  
2. **å·¥ç¨‹èŒƒå¼**ï¼šè§£è€¦çŸ¥è¯†ä¸æ¨ç†ï¼Œè®©â€œå¯ç»´æŠ¤çš„å¤–éƒ¨çŸ¥è¯†åº“ + æ¨ç†ä¼˜åŒ–çš„ç´§å‡‘æ¨¡å‹â€ååŒï¼Œä¸ºé¢†åŸŸç‰¹å®šæ™ºèƒ½è½åœ°æä¾›æ›´å…·æ‰©å±•æ€§çš„è·¯å¾„â€”â€”ä¸ç”¨æ­»å †æ¨¡å‹å‚æ•°ï¼Œé ç³»ç»Ÿçº§è®¾è®¡æå‡æ•ˆç‡ï¼›  
3. **è®­ç»ƒ trick**ï¼šé€šè¿‡çŸ¥è¯†æ³¨å…¥é‡æ„æŸå¤±çš„æ€è·¯ï¼Œä¸ºâ€œè®©æ¨¡å‹å­¦æ€ä¹ˆç”¨çŸ¥è¯†ï¼Œè€Œéè®°çŸ¥è¯†â€æä¾›äº†å®æ“æ–¹æ³•ï¼Œå¯è¿ç§»åˆ°éœ€è¦çŸ¥è¯†æ¨ç†çš„å‚ç›´é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ï¼‰å¾®è°ƒä¸­ã€‚  
```

## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | s3ï¼šå°æ•°æ®é‡ä¸‹ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœç´¢æ™ºèƒ½ä½“çš„æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å€ŸåŠ©å¤–éƒ¨çŸ¥è¯†æå‡æ¨ç†èƒ½åŠ›ï¼Œè€Œè¿‘æœŸç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®©LLMæ‰®æ¼”æœç´¢æ™ºèƒ½ä½“çš„è¿›å±•ï¼Œå¯é€šè¿‡å¤šè½®äº¤äº’ä¼˜åŒ–ä¿¡æ¯è·å–ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šè¦ä¹ˆç”¨ä»…å…³æ³¨æœç´¢çš„æŒ‡æ ‡ï¼ˆå¦‚NDCGï¼‰ä¼˜åŒ–æ£€ç´¢ï¼Œå¿½ç•¥ä¸‹æ¸¸æ•ˆç”¨ï¼›è¦ä¹ˆå¾®è°ƒæ•´ä¸ªLLMæ¥è”åˆæ¨ç†ä¸æ£€ç´¢ï¼ŒæŠŠæ£€ç´¢å’Œç”Ÿæˆè€¦åˆï¼Œé™åˆ¶äº†å®é™…æœç´¢æ•ˆç”¨ä¸”éš¾é€‚é…å†»ç»“æˆ–é—­æºæ¨¡å‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§è§£è€¦æœç´¢ä¸ç”Ÿæˆã€è½»é‡ä¸”æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œåœ¨å°æ•°æ®é‡ä¸‹æå‡ä¸‹æ¸¸æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºs3æ¡†æ¶ï¼Œè§£è€¦æœç´¢å™¨ä¸ç”Ÿæˆå™¨  
s3æ˜¯è½»é‡ã€æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œå°†æœç´¢å™¨ï¼ˆSearcherï¼‰ä»ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ä¸­è§£è€¦ã€‚è®­ç»ƒæ—¶å›ºå®šç”Ÿæˆå™¨ï¼ˆå¯å¤ç”¨ä»»æ„å†»ç»“çš„LLMï¼‰ï¼Œä»…ä¸“æ³¨äºè®­ç»ƒæœç´¢å™¨ï¼Œè®©æœç´¢ä¼˜åŒ–èšç„¦åœ¨å¯¹ä¸‹æ¸¸ç”Ÿæˆè´¨é‡çš„æå‡ä¸Šï¼Œé¿å…äº†ç”Ÿæˆä¸æ£€ç´¢çš„è€¦åˆé—®é¢˜ï¼Œä¹Ÿèƒ½é€‚é…é—­æºæˆ–å†»ç»“çš„LLMã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰â€œGain Beyond RAGï¼ˆGBRï¼‰â€å¥–åŠ±ä¿¡å·  
GBRä½œä¸ºå…¨æ–°çš„å¥–åŠ±æŒ‡æ ‡ï¼Œè¡¡é‡äº†s3æ£€ç´¢åˆ°çš„æ–‡æ¡£è®©ç”Ÿæˆå™¨è¡¨ç°ï¼Œç›¸æ¯”â€œæœ´ç´ RAGï¼ˆnaÃ¯ve RAGï¼‰æ£€ç´¢â€åœ¨ç”Ÿæˆå‡†ç¡®ç‡ä¸Šçš„æå‡å¹…åº¦ã€‚é€šè¿‡è¯¥å¥–åŠ±è®­ç»ƒæœç´¢å™¨ï¼Œèƒ½ç›´æ¥é’ˆå¯¹ä¸‹æ¸¸ç”Ÿæˆæ•ˆç”¨ä¼˜åŒ–æ£€ç´¢ç»„ä»¶ï¼Œæ‘†è„±ä»…å…³æ³¨æœç´¢æŒ‡æ ‡æˆ–æ˜“è¿‡æ‹Ÿåˆçš„ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ç±»æŒ‡æ ‡çš„å±€é™ï¼Œè®©æ£€ç´¢ä¼˜åŒ–æ›´è´´åˆå®é™…ç”Ÿæˆéœ€æ±‚ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨6ä¸ªé€šç”¨QAåŸºå‡†å’Œ5ä¸ªåŒ»ç–—QAåŸºå‡†æµ‹è¯•ä¸­ï¼Œs3å±•ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼šä»…ç”¨2.4kè®­ç»ƒæ ·æœ¬ï¼Œå°±è¶…è¶Šäº†ä½¿ç”¨è¶…70å€æ•°æ®ï¼ˆå¦‚70kç”šè‡³æ›´å¤šï¼‰è®­ç»ƒçš„åŸºçº¿æ–¹æ³•ï¼ˆåƒDeepRetrievalã€Search - R1ç­‰ï¼‰ã€‚åœ¨é€šç”¨å’ŒåŒ»ç–—é¢†åŸŸçš„å¹³å‡å¾—åˆ†ä¸Šï¼Œs3å¯¹æ¯”å…¶ä»–ç»å…¸RAGã€Active RAGã€RL - Zeroé˜¶æ®µçš„æ–¹æ³•ï¼Œéƒ½å®ç°äº†æ›´ä¼˜çš„ä¸‹æ¸¸ç”Ÿæˆè¡¨ç°ï¼ŒéªŒè¯äº†å°æ•°æ®é‡ä¸‹é«˜æ•ˆè®­ç»ƒæœç´¢æ™ºèƒ½ä½“çš„èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å—åŒ–è®¾è®¡æ€è·¯ï¼šå°†å¤æ‚çš„â€œæ£€ç´¢ + ç”Ÿæˆâ€ä»»åŠ¡è§£è€¦ä¸ºæœç´¢å™¨å’Œç”Ÿæˆå™¨ç‹¬ç«‹ä¼˜åŒ–ï¼Œä¸ºæ„å»ºæ›´çµæ´»çš„RAGç³»ç»Ÿæä¾›äº†æ¨¡å—åŒ–æ€è·¯ï¼Œåç»­å¯é’ˆå¯¹ä¸åŒæ¨¡å—åˆ†åˆ«è¿­ä»£å‡çº§ã€‚  
2. å¥–åŠ±ä¿¡å·è®¾è®¡ï¼šGBRå¥–åŠ±å°†ä¸‹æ¸¸ç”Ÿæˆæ•ˆç”¨ä½œä¸ºæ£€ç´¢ä¼˜åŒ–çš„æ ¸å¿ƒä¾æ®ï¼Œè·³å‡ºäº†ä¼ ç»Ÿä»…çœ‹æœç´¢ç¯èŠ‚æŒ‡æ ‡çš„æ€ç»´å®šå¼ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨æ£€ç´¢å¢å¼ºåœºæ™¯ä¸‹çš„å¥–åŠ±å‡½æ•°è®¾è®¡æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘å…¶ä»–éœ€è¦è·¨ç¯èŠ‚ä¼˜åŒ–ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡ã€‚  
3. å°æ•°æ®é«˜æ•ˆè®­ç»ƒï¼šè¯æ˜äº†åœ¨å°‘é‡ä¼˜è´¨æ ·æœ¬ä¸‹ä¹Ÿèƒ½è®­ç»ƒå‡ºé«˜æ€§èƒ½æœç´¢æ™ºèƒ½ä½“ï¼Œé™ä½äº†å¤§è§„æ¨¡æ•°æ®æ ‡æ³¨ä¸è®­ç»ƒæˆæœ¬ï¼Œå¯¹èµ„æºæœ‰é™ä½†éœ€æ„å»ºRAGç³»ç»Ÿçš„åœºæ™¯ï¼ˆå¦‚å‚ç›´é¢†åŸŸå°å›¢é˜Ÿï¼‰æœ‰å¾ˆå¼ºçš„å€Ÿé‰´æ„ä¹‰ã€‚

## hybrid-latent-reasoning-via-reinforcement-learning
### Abstract
Recent advances in large language models (LLMs) have introduced latent
reasoning as a promising alternative to autoregressive reasoning. By performing
internal computation with hidden states from previous steps, latent reasoning
benefit from more informative features rather than sampling a discrete
chain-of-thought (CoT) path. Yet latent reasoning approaches are often
incompatible with LLMs, as their continuous paradigm conflicts with the
discrete nature of autoregressive generation. Moreover, these methods rely on
CoT traces for training and thus fail to exploit the inherent reasoning
patterns of LLMs. In this work, we explore latent reasoning by leveraging the
intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we
introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid
latent reasoning approach that (1) integrates prior hidden states into sampled
tokens with a learnable gating mechanism, and (2) initializes training with
predominantly token embeddings while progressively incorporating more hidden
features. This design maintains LLMs' generative capabilities and incentivizes
hybrid reasoning using both discrete and continuous representations. In
addition, the hybrid HRPO introduces stochasticity into latent reasoning via
token sampling, thereby enabling RL-based optimization without requiring CoT
trajectories. Extensive evaluations across diverse benchmarks show that HRPO
outperforms prior methods in both knowledge- and reasoning-intensive tasks.
Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing
behaviors like cross-lingual patterns and shorter completion lengths,
highlighting the potential of our RL-based approach and offer insights for
future work in latent reasoning.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„æ··åˆæ½œåœ¨æ¨ç†ï¼šè§£é”å¤§æ¨¡å‹å†…åœ¨æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼Œæ½œåœ¨æ¨ç†ä½œä¸º autoregressiveï¼ˆè‡ªå›å½’ï¼‰æ¨ç†çš„æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºæ½œåŠ›ã€‚ä¼ ç»Ÿè‡ªå›å½’æ¨ç†ä¾èµ–ç¦»æ•£çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è§£ç ä¸é‡‡æ ·ï¼Œè€Œæ½œåœ¨æ¨ç†å€ŸåŠ©å‰åºæ­¥éª¤çš„è¿ç»­éšè—çŠ¶æ€å®ç°å†…éƒ¨æ¨ç†ï¼Œèƒ½åˆ©ç”¨æ›´ä¸°å¯Œä¿¡æ¯ã€‚ä½†ç°æœ‰æ½œåœ¨æ¨ç†æ–¹æ³•å­˜åœ¨è¯¸å¤šé—®é¢˜ï¼šä¸€æ˜¯ä¸LLMså…¼å®¹æ€§å·®ï¼Œè¿ç»­èŒƒå¼å’Œè‡ªå›å½’ç”Ÿæˆçš„ç¦»æ•£æ€§å†²çªï¼Œå°†éšè—çŠ¶æ€è¾“å…¥ä¸‹ä¸€æ­¥è§£ç ä¼šé™ä½ç”Ÿæˆè´¨é‡ï¼ˆå¦‚é‡å¤ã€ä¸è¿è´¯ï¼‰ï¼›äºŒæ˜¯ä¾èµ–CoTè½¨è¿¹è®­ç»ƒï¼Œæ—¢å¿½è§†LLMså›ºæœ‰æ¨ç†èƒ½åŠ›ï¼Œåˆå¸¦æ¥é«˜æ˜‚è®­ç»ƒæˆæœ¬ï¼ˆå¦‚å¤šé˜¶æ®µè®­ç»ƒã€ä»å¤´è®­å¤šå—æ¨¡å‹ï¼‰ï¼Œé™åˆ¶äº†é€‚ç”¨èŒƒå›´ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½æ— ç¼æ•´åˆè¿ç»­è¡¨ç¤ºã€ä¾æ‰˜é¢„è®­ç»ƒLLMsæ³›åŒ–æ€§ã€å‡å°‘CoTä¾èµ–çš„æ½œåœ¨æ¨ç†æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºHRPOæ¡†æ¶ï¼Œé¦–æ¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ··åˆæ¨ç†æ–¹æ¡ˆ  
Hybrid Reasoning Policy Optimizationï¼ˆHRPOï¼‰æ˜¯é¦–ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ··åˆæ½œåœ¨æ¨ç†ä¼˜åŒ–æ¡†æ¶ï¼ŒæŠŠç­–ç•¥å­¦ä¹ å’Œæ½œåœ¨æ¨ç†ç»Ÿä¸€èµ·æ¥ï¼Œæ— éœ€ä¾èµ–CoTè½¨è¿¹å°±èƒ½åˆ©ç”¨LLMså†…åœ¨æ¨ç†æ¨¡å¼ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹è‡ªä¸»å‘å±•æ½œåœ¨æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ½œåœ¨æ¨ç†æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–°è·¯å¾„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡é—¨æ§æœºåˆ¶ï¼Œå¹³è¡¡ç”Ÿæˆèƒ½åŠ›ä¸è¿ç»­æ¨ç†  
ä¸ºåœ¨ä¿ç•™LLMsç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå¼•å¯¼æ¨¡å‹åœ¨è¿ç»­ç©ºé—´æ¨ç†ï¼ŒHRPOå¼•å…¥**å¯å­¦ä¹ é—¨æ§æœºåˆ¶**ã€‚è®­ç»ƒåˆæœŸï¼Œè¾“å…¥ä¸»è¦æ¥è‡ªé‡‡æ ·tokençš„åµŒå…¥ï¼Œä¿è¯ç”Ÿæˆè´¨é‡ï¼›éšè®­ç»ƒæ¨è¿›ï¼Œé—¨æ§å­¦ä¹ èå…¥å‰åºéšè—çŠ¶æ€ä¸­æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼ŒåŠ©åŠ›å†…éƒ¨æ¨ç†ã€‚è¿™ç§æ¸è¿›å¼èåˆï¼Œè®©ç¦»æ•£tokenå’Œè¿ç»­éšè—è¡¨ç¤ºååŒä½œç”¨ï¼Œå®ç°æ··åˆæ¨ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå€ŸåŠ©é‡‡æ ·éšæœºæ€§ï¼Œå®ç°æ— CoTçš„RLä¼˜åŒ–  
HRPOé€šè¿‡tokené‡‡æ ·ç»™æ½œåœ¨æ¨ç†å¼•å…¥éšæœºæ€§ï¼Œä½¿å¾—rolloutï¼ˆè½¨è¿¹å±•å¼€ï¼‰èƒ½åƒæ ‡å‡†RLæ–¹æ³•ä¸€æ ·æ‰§è¡Œã€‚æ··åˆè¾“å‡ºï¼ˆtoken + æ½œåœ¨è¡¨ç¤ºï¼‰å­˜å…¥rollout bufferç”¨äºç­–ç•¥æ›´æ–°ï¼ŒåŸºäºç®€å•çš„â€œç»“æœå¯¼å‘å‹å¥–åŠ±â€è®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼Œå®ç°ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œè‡ªé€‚åº”æ•´åˆtokençº§å’Œæ½œåœ¨è¡¨ç¤ºä¿¡æ¯ï¼Œè§£é”ç°æœ‰LLMsçš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHRPOè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬å…¶ä»–æ½œåœ¨æ¨ç†åŸºçº¿ï¼‰ï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹æŒç»­æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç»HRPOè®­ç»ƒçš„LLMsä¿ç•™äº†å¯è§£é‡Šæ€§ï¼Œè¿˜å±•ç°å‡ºè·¨è¯­è¨€æ¨¡å¼ã€æ›´çŸ­ç”Ÿæˆé•¿åº¦ç­‰æœ‰è¶£è¡Œä¸ºï¼ŒéªŒè¯äº†åŸºäºRLçš„æ··åˆæ½œåœ¨æ¨ç†æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œä¹Ÿä¸ºåç»­æ½œåœ¨æ¨ç†ç ”ç©¶æä¾›äº†æ–°è§†è§’ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•åˆ›æ–°è§’åº¦ï¼šå°†å¼ºåŒ–å­¦ä¹ ä¸æ½œåœ¨æ¨ç†ç»“åˆï¼Œè·³å‡ºâ€œä¾èµ–CoTæ ‡æ³¨å’Œå¤šé˜¶æ®µè®­ç»ƒâ€çš„ä¼ ç»Ÿæ€è·¯ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡å¼€è¾Ÿæ–°èŒƒå¼ï¼Œè¯æ˜äº†RLåœ¨è§£é”LLMså†…åœ¨æ¨ç†æ¨¡å¼ä¸Šçš„ä»·å€¼ã€‚  
2. å·¥ç¨‹è®¾è®¡è§’åº¦ï¼šé—¨æ§æœºåˆ¶çš„â€œæ¸è¿›å¼èåˆâ€æ€è·¯ï¼Œä¸ºå¹³è¡¡æ¨¡å‹æ—¢æœ‰èƒ½åŠ›ï¼ˆå¦‚ç”Ÿæˆï¼‰ä¸æ–°èƒ½åŠ›ï¼ˆå¦‚è¿ç»­ç©ºé—´æ¨ç†ï¼‰æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼Œåœ¨éœ€å…¼é¡¾â€œå­˜é‡èƒ½åŠ›â€å’Œâ€œå¢é‡èƒ½åŠ›â€çš„æ¨¡å‹ä¼˜åŒ–åœºæ™¯ä¸­å…·å€Ÿé‰´æ€§ã€‚  
3. å®éªŒä¸åˆ†æè§’åº¦ï¼šä¸ä»…éªŒè¯æ€§èƒ½ï¼Œè¿˜å…³æ³¨æ¨¡å‹è¡Œä¸ºï¼ˆå¯è§£é‡Šæ€§ã€è·¨è¯­è¨€ç­‰ï¼‰ï¼Œè¿™ç§ä»â€œæ•ˆæœâ€åˆ°â€œç‰¹æ€§â€çš„å…¨é¢åˆ†æï¼Œèƒ½å¸®åŠ©ç ”ç©¶è€…æ›´æ·±å…¥ç†è§£æ–¹æ³•å¯¹æ¨¡å‹çš„å¡‘é€ ï¼Œä¸ºåç»­ä¼˜åŒ–æŒ‡æ˜æ–¹å‘ã€‚  
```

## webthinker--empowering-large-reasoning-models-with-deep-research-capability
### Abstract
Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate
impressive long-horizon reasoning capabilities. However, their reliance on
static internal knowledge limits their performance on complex,
knowledge-intensive tasks and hinders their ability to produce comprehensive
research reports requiring synthesis of diverse web information. To address
this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs
to autonomously search the web, navigate web pages, and draft research reports
during the reasoning process. WebThinker integrates a \textbf{Deep Web
Explorer} module, enabling LRMs to dynamically search, navigate, and extract
information from the web when encountering knowledge gaps. It also employs an
\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to
seamlessly interleave reasoning, information gathering, and report writing in
real time. To further enhance research tool utilization, we introduce an
\textbf{RL-based training strategy} via iterative online Direct Preference
Optimization (DPO). Extensive experiments on complex reasoning benchmarks
(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)
demonstrate that WebThinker significantly outperforms existing methods and
strong proprietary systems. Our approach enhances LRM reliability and
applicability in complex scenarios, paving the way for more capable and
versatile deep research systems. The code is available at
https://github.com/RUC-NLPIR/WebThinker.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebThinkerï¼šè®©å¤§æ¨ç†æ¨¡å‹æ‹¥æœ‰æ·±åº¦ç ”ç©¶èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI - o1ã€DeepSeek - R1ç­‰åœ¨æ•°å­¦ã€ä»£ç ã€ç§‘å­¦ç­‰é¢†åŸŸå±•ç°å‡ºå“è¶Šçš„é•¿ç¨‹æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–é™æ€å†…éƒ¨çŸ¥è¯†ï¼Œåœ¨å¤æ‚ã€çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°å—é™ï¼Œä¹Ÿéš¾ä»¥ç”Ÿæˆéœ€æ•´åˆå¤šæ ·ç½‘ç»œä¿¡æ¯çš„å…¨é¢ç ”ç©¶æŠ¥å‘Šã€‚ç°æœ‰å¼€æºæ·±åº¦æœç´¢æ™ºèƒ½ä½“å¤šé‡‡ç”¨é¢„å®šä¹‰å·¥ä½œæµçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé™åˆ¶äº†LRMsæ¢ç´¢æ·±å±‚ç½‘ç»œä¿¡æ¯åŠä¸æœç´¢å¼•æ“çš„ç´§å¯†äº¤äº’ï¼Œå¼€å‘é€šç”¨ã€çµæ´»çš„å¼€æºæ·±åº¦ç ”ç©¶æ¡†æ¶æˆäº†äºŸå¾…è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºWebThinkeræ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“
WebThinkerèƒ½è®©LRMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»æœç´¢ç½‘é¡µã€æµè§ˆç½‘é¡µå¹¶èµ·è‰ç ”ç©¶æŠ¥å‘Šï¼Œä¸åŒäºä¼ ç»Ÿé¢„å®šä¹‰å·¥ä½œæµï¼Œå®ƒä½¿LRMåœ¨æ€è€ƒæ—¶è‡ªä¸»è¡ŒåŠ¨ï¼Œå®ç°å•ä»£å†…çš„ç«¯åˆ°ç«¯ä»»åŠ¡æ‰§è¡Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡Deep Web Exploreræ¨¡å—
è¯¥æ¨¡å—èµ‹äºˆLRMsç½‘é¡µæœç´¢å’Œå¯¼èˆªèƒ½åŠ›ï¼Œä½¿å…¶åœ¨é‡åˆ°çŸ¥è¯†ç¼ºå£æ—¶ï¼Œèƒ½åŠ¨æ€æœç´¢ã€æµè§ˆç½‘é¡µï¼ˆå¦‚ç‚¹å‡»é“¾æ¥æˆ–æŒ‰é’®ç­‰äº¤äº’å…ƒç´ ï¼‰å¹¶æå–ä¿¡æ¯ï¼Œè¿˜èƒ½åŸºäºå½“å‰æŸ¥è¯¢ç»“æœå‘èµ·åç»­æœç´¢ã€éå†æ›´æ·±é“¾æ¥ä»¥æ”¶é›†æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥Autonomous Think - Search - and - Draftç­–ç•¥
å°†æŠ¥å‘Šæ’°å†™ä¸LRMsçš„æ¨ç†å’Œæœç´¢è¿‡ç¨‹æ·±åº¦æ•´åˆï¼Œè®©æ¨¡å‹åœ¨æ€è€ƒå’Œæœç´¢æ—¶å®æ—¶æ’°å†™æŠ¥å‘Šã€‚ä¸ºLRMsé…å¤‡ä¸‰ä¸ªä¸“ç”¨å·¥å…·ï¼šç‰¹å®šç« èŠ‚å†…å®¹èµ·è‰ã€æ£€æŸ¥å½“å‰æŠ¥å‘Šã€ç¼–è¾‘æŠ¥å‘Šï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†ä¸­è‡ªä¸»æå‡æŠ¥å‘Šçš„å…¨é¢æ€§ã€è¿è´¯æ€§åŠå¯¹æ–°å‘ç°ä¿¡æ¯çš„é€‚åº”æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¼€å‘åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒç­–ç•¥
åˆ©ç”¨é…å¤‡å·¥å…·çš„LRMä»å¤æ‚ä»»åŠ¡ä¸­é‡‡æ ·å¤§è§„æ¨¡æ¨ç†è½¨è¿¹ï¼Œä¾æ®æ¨ç†å‡†ç¡®æ€§ã€å·¥å…·ä½¿ç”¨æƒ…å†µå’Œæœ€ç»ˆè¾“å‡ºæ„å»ºåå¥½å¯¹ï¼Œè¿›è¡Œåœ¨çº¿Direct Preference Optimizationï¼ˆDPOï¼‰è®­ç»ƒï¼Œé€šè¿‡è¿­ä»£çš„on - policyè®­ç»ƒï¼Œé€æ­¥æå‡æ¨¡å‹æ„ŸçŸ¥ã€æ¨ç†å’Œä¸ç ”ç©¶å·¥å…·æœ‰æ•ˆäº¤äº’çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤æ‚æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆGPQAã€GAIAã€WebWalkerQAã€HLEï¼‰å’Œç§‘å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ï¼ˆGlaiveï¼‰ä¸Šè¿›è¡Œå¤§é‡å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒWebThinkeråœ¨å¤æ‚é—®é¢˜è§£å†³å’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•å’Œå¼ºå¤§çš„ä¸“æœ‰ç³»ç»Ÿã€‚ä¾‹å¦‚åœ¨å¤æ‚æ¨ç†é—®é¢˜è§£å†³ä»»åŠ¡ä¸­ï¼ŒWebThinker - 32Bï¼ˆOursï¼‰åœ¨å„åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¿œè¶…QwQ - 32Bã€RAG - QwQ - 32Bã€Search - o1 - 32Bç­‰ï¼›åœ¨ç§‘å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­ï¼Œåœ¨Comprehensiveã€Thoroughã€Factualityã€Coherenceç­‰ç»´åº¦ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡RAG - Qwen2.5 - 72Bã€Grok3 DeeperSearchã€Gemini2.0 Deep Researchç­‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¶æ„è®¾è®¡å±‚é¢ï¼šWebThinkerå°†æ¨ç†ä¸ç½‘ç»œä¿¡æ¯æ¢ç´¢æ·±åº¦èåˆçš„æ¶æ„æ€è·¯ï¼Œä¸ºè§£å†³å¤§æ¨¡å‹ä¾èµ–é™æ€çŸ¥è¯†çš„é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘åç»­å¤§æ¨¡å‹åœ¨çŸ¥è¯†è·å–ä¸æ¨ç†ç»“åˆæ–¹å‘çš„ç ”ç©¶ã€‚
2. æ¨¡å—åˆ›æ–°å±‚é¢ï¼šDeep Web Exploreræ¨¡å—å¯¹ç½‘é¡µæœç´¢ã€å¯¼èˆªå’Œä¿¡æ¯æå–çš„è®¾è®¡ï¼Œä¸ºå¤§æ¨¡å‹è·å–å¤–éƒ¨åŠ¨æ€çŸ¥è¯†æä¾›äº†å¯è¡Œçš„å·¥å…·æ¨¡å—å‚è€ƒï¼›Autonomous Think - Search - and - Draftç­–ç•¥ä¸­å®æ—¶æ’°å†™æŠ¥å‘Šä¸æ¨ç†æœç´¢æ•´åˆçš„æ–¹å¼ï¼Œå¯¹éœ€è¦å†…å®¹ç”Ÿæˆä¸ä¿¡æ¯è·å–ç»“åˆçš„ä»»åŠ¡ï¼ˆå¦‚æŠ¥å‘Šç”Ÿæˆã€æ–‡æ¡ˆåˆ›ä½œç­‰ï¼‰æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
3. è®­ç»ƒç­–ç•¥å±‚é¢ï¼šåŸºäºRLå’Œåœ¨çº¿DPOçš„è®­ç»ƒç­–ç•¥ï¼Œä¸ºæå‡å¤§æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•å‚è€ƒï¼Œå¯ç”¨äºå…¶ä»–éœ€å·¥å…·äº¤äº’çš„å¤§æ¨¡å‹ä»»åŠ¡åœºæ™¯è®­ç»ƒã€‚
```

## smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback
### Abstract
RAG systems consist of multiple modules to work together. However, these
modules are usually separately trained. We argue that a system like RAG that
incorporates multiple modules should be jointly optimized to achieve optimal
performance. To demonstrate this, we design a specific pipeline called
\textbf{SmartRAG} that includes a policy network and a retriever. The policy
network can serve as 1) a decision maker that decides when to retrieve, 2) a
query rewriter to generate a query most suited to the retriever, and 3) an
answer generator that produces the final response with/without the
observations. We then propose to jointly optimize the whole system using a
reinforcement learning algorithm, with the reward designed to encourage the
system to achieve the best performance with minimal retrieval cost. When
jointly optimized, all the modules can be aware of how other modules are
working and thus find the best way to work together as a complete system.
Empirical results demonstrate that the jointly optimized SmartRAG can achieve
better performance than separately optimized counterparts.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SmartRAGï¼šç”¨ç¯å¢ƒåé¦ˆç«¯åˆ°ç«¯ä¼˜åŒ–RAGç³»ç»Ÿï¼Œçªç ´æ¨¡å—åˆ†ç¦»è®­ç»ƒç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¸å¤šé¢†åŸŸå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å¤„ç†æ¨¡å‹å‚æ•°ä¹‹å¤–çš„çŸ¥è¯†ç±»é—®é¢˜ä»é¢‡å…·æŒ‘æˆ˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä»å¤–éƒ¨å·¥å…·æ£€ç´¢ä¿¡æ¯æœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨è¿™ç±»åœºæ™¯çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRAGç³»ç»Ÿçš„å¤šä¸ªæ¨¡å—ï¼ˆå¦‚æ£€ç´¢å™¨ã€å†³ç­–å™¨ã€æŸ¥è¯¢é‡å†™å™¨ç­‰ï¼‰å¾€å¾€æ˜¯**åˆ†ç¦»è®­ç»ƒ**çš„ã€‚è¿™å¸¦æ¥ä¸¤å¤§é—®é¢˜ï¼šä¸€æ˜¯ä¸­é—´æ¨¡å—çš„â€œé»„é‡‘ç­”æ¡ˆâ€ï¼ˆæœ€ä¼˜è¾“å‡ºï¼‰é€šå¸¸éš¾ä»¥è·å–ï¼Œç”šè‡³ä¾èµ–ç‰¹å®šæ¨¡å‹æˆ–æ£€ç´¢å™¨ï¼›äºŒæ˜¯åˆ†ç¦»ä¼˜åŒ–ä¼šå¯¼è‡´å„æ¨¡å—ç¼ºä¹å¯¹æ•´ä½“ç³»ç»Ÿçš„ååŒæ„ŸçŸ¥ï¼Œéš¾ä»¥è¾¾åˆ°å…¨å±€æœ€ä¼˜æ€§èƒ½ã€‚å› æ­¤ï¼Œè®ºæ–‡æå‡ºè¦å¯¹RAGè¿™ç±»å¤šæ¨¡å—ç³»ç»Ÿè¿›è¡Œ**ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–**ï¼Œè®©å„æ¨¡å—åœ¨åä½œä¸­ç›¸äº’é€‚é…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡SmartRAGç³»ç»Ÿæ¶æ„ï¼Œè®©Policy Networkèº«å…¼ä¸‰èŒ  
SmartRAGçš„æ ¸å¿ƒç”±**ç­–ç•¥ç½‘ç»œï¼ˆPolicy Networkï¼‰**å’Œ**æ£€ç´¢å™¨ï¼ˆRetrieverï¼‰**æ„æˆã€‚å…¶ä¸­ï¼Œç­–ç•¥ç½‘ç»œæ‰¿æ‹…ä¸‰ä¸ªå…³é”®è§’è‰²ï¼š  
- å†³ç­–å™¨ï¼ˆDecision Makerï¼‰ï¼šæ ¹æ®è¾“å…¥é—®é¢˜ä¸å·²æœ‰è§‚æµ‹ï¼Œå†³å®šæ˜¯å¦éœ€è¦å‘èµ·æ£€ç´¢ï¼›  
- æŸ¥è¯¢é‡å†™å™¨ï¼ˆQuery Rewriterï¼‰ï¼šè‹¥å†³å®šæ£€ç´¢ï¼Œç”Ÿæˆæ›´é€‚é…æ£€ç´¢å™¨çš„æŸ¥è¯¢è¯­å¥ï¼›  
- ç­”æ¡ˆç”Ÿæˆå™¨ï¼ˆAnswer Generatorï¼‰ï¼šè‹¥è®¤ä¸ºå½“å‰ä¿¡æ¯è¶³å¤Ÿï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚  
è¿™ç§â€œä¸€ä¸“å¤šèƒ½â€çš„è®¾è®¡è®©å•ä¸ªç­–ç•¥ç½‘ç»œä¸²è”èµ·RAGçš„æ ¸å¿ƒæµç¨‹ï¼Œä¸ºåç»­ç«¯åˆ°ç«¯ä¼˜åŒ–æ‰“ä¸‹åŸºç¡€ã€‚  

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–  
è®ºæ–‡é‡‡ç”¨**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰**ç®—æ³•å¯¹SmartRAGè¿›è¡Œè”åˆè®­ç»ƒï¼Œç”¨**ç¯å¢ƒåé¦ˆ**æ›¿ä»£ä¼ ç»Ÿâ€œé»„é‡‘ç­”æ¡ˆâ€ä½œä¸ºç›‘ç£ä¿¡å·ã€‚è®¾è®¡å¥–åŠ±å‡½æ•°æ—¶ï¼Œå…¼é¡¾ä¸¤å¤§ç›®æ ‡ï¼šä¸€æ˜¯â€œæ­£ç¡®å›ç­”é—®é¢˜â€ï¼ŒäºŒæ˜¯â€œæœ€å°åŒ–æ£€ç´¢æˆæœ¬â€ï¼ˆå‡å°‘ä¸å¿…è¦çš„æ£€ç´¢æ¬¡æ•°ï¼‰ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œç­–ç•¥ç½‘ç»œèƒ½åœ¨ä¸ç¯å¢ƒï¼ˆå¦‚å¤–éƒ¨çŸ¥è¯†åº“ã€æ£€ç´¢å·¥å…·ï¼‰çš„äº¤äº’ä¸­ï¼Œå­¦ä¹ åˆ°å„æ¨¡å—é—´çš„æœ€ä¼˜åä½œæ–¹å¼â€”â€”ä½•æ—¶æ£€ç´¢ã€æ£€ç´¢ä»€ä¹ˆã€å¦‚ä½•å›ç­”ï¼Œè®©æ•´ä¸ªç³»ç»Ÿå½¢æˆæœ‰æœºæ•´ä½“ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†SmartRAGçš„æœ‰æ•ˆæ€§ï¼Œæ ¸å¿ƒç»“è®ºæ˜¯ï¼š**è”åˆä¼˜åŒ–çš„SmartRAGæ˜¾è‘—ä¼˜äºæ¨¡å—åˆ†ç¦»è®­ç»ƒçš„åŸºçº¿ç³»ç»Ÿ**ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åˆ†æï¼Œè®ºæ–‡è¿˜å±•ç¤ºäº†SmartRAGå¦‚ä½•å­¦ä¹ â€œä½•æ—¶æ£€ç´¢ã€æ£€ç´¢ä»€ä¹ˆã€å¦‚ä½•å›ç­”â€è¿™ä¸‰é¡¹å…³é”®èƒ½åŠ›ï¼Œè¯æ˜äº†æ¨¡å—é—´ååŒæ„ŸçŸ¥å¯¹ç³»ç»Ÿæ€§èƒ½çš„æå‡ä½œç”¨ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ¶æ„è®¾è®¡æ€è·¯**ï¼šå°†RAGå¤šæ¨¡å—åŠŸèƒ½æ”¶æ•›åˆ°â€œç­–ç•¥ç½‘ç»œ+æ£€ç´¢å™¨â€çš„ç®€æ´æ¶æ„ï¼Œç”¨å•ä¸€ç½‘ç»œæ‰¿è½½å†³ç­–ã€é‡å†™ã€ç”Ÿæˆï¼Œä¸ºå¤æ‚ç³»ç»Ÿçš„æ¨¡å—åŒ–æ•´åˆæä¾›äº†å‚è€ƒï¼›  
2. **ä¼˜åŒ–èŒƒå¼åˆ›æ–°**ï¼šç”¨å¼ºåŒ–å­¦ä¹ åšç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼Œæ‘†è„±å¯¹â€œé»„é‡‘ç­”æ¡ˆâ€çš„å¼ºä¾èµ–ï¼Œæ›´è´´åˆçœŸå®åœºæ™¯ä¸­æ¨¡å—è¾“å‡ºéš¾æ ‡æ³¨çš„ç—›ç‚¹ï¼›  
3. **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šå¹³è¡¡â€œä»»åŠ¡æ•ˆæœï¼ˆå›ç­”æ­£ç¡®ç‡ï¼‰â€ä¸â€œèµ„æºæˆæœ¬ï¼ˆæ£€ç´¢æ¬¡æ•°ï¼‰â€ï¼Œè¿™ç§å¤šç›®æ ‡æƒè¡¡çš„æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€èµ„æºçº¦æŸçš„AIç³»ç»Ÿè®¾è®¡ä¸­ã€‚  

æ€»ä¹‹ï¼ŒSmartRAGä¸ºRAGç³»ç»Ÿçš„å·¥ç¨‹åŒ–è½åœ°æä¾›äº†â€œååŒä¼˜åŒ–â€çš„æ–°èŒƒå¼ï¼Œè®©å¤šæ¨¡å—ç³»ç»Ÿä»â€œå„è‡ªä¸ºæˆ˜â€èµ°å‘â€œå…¨å±€æœ€ä¼˜â€ï¼Œå€¼å¾—ä»äº‹æ£€ç´¢å¢å¼ºã€å¤§æ¨¡å‹åº”ç”¨çš„ç ”ç©¶è€…ä¸å·¥ç¨‹å¸ˆæ·±å…¥å‚è€ƒ~
```

## distilling-llm-agent-into-small-models-with-retrieval-and-code-tools
### Abstract
Large language models (LLMs) excel at complex reasoning tasks but remain
computationally expensive, limiting their practical deployment. To address
this, recent works have focused on distilling reasoning capabilities into
smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher
LLMs. However, this approach struggles in scenarios requiring rare factual
knowledge or precise computation, where sLMs often hallucinate due to limited
capability. In this work, we propose Agent Distillation, a framework for
transferring not only reasoning capability but full task-solving behavior from
LLM-based agents into sLMs with retrieval and code tools. We improve agent
distillation along two complementary axes: (1) we introduce a prompting method
called first-thought prefix to enhance the quality of teacher-generated
trajectories; and (2) we propose a self-consistent action generation for
improving test-time robustness of small agents. We evaluate our method on eight
reasoning tasks across factual and mathematical domains, covering both
in-domain and out-of-domain generalization. Our results show that sLMs as small
as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier
larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the
potential of agent distillation for building practical, tool-using small
agents. Our code is available at https://github.com/Nardien/agent-distillation.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å°æ¨¡å‹æ‹¥æœ‰å¤§èƒ½åŠ›ï¼šAgent Distillation èµ‹èƒ½å°è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨ä¸å¤æ‚æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶å®é™…éƒ¨ç½²ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå·²æœ‰å·¥ä½œå°è¯•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰è’¸é¦å°†æ¨ç†èƒ½åŠ›è¿ç§»åˆ°å°è¯­è¨€æ¨¡å‹ï¼ˆsLMsï¼‰ï¼Œç„¶è€Œå°æ¨¡å‹åœ¨éœ€è¦ç½•è§äº‹å®çŸ¥è¯†æˆ–ç²¾ç¡®è®¡ç®—çš„åœºæ™¯ä¸‹ï¼Œå¸¸å› èƒ½åŠ›å±€é™äº§ç”Ÿå¹»è§‰ï¼ˆhallucinateï¼‰ã€‚ä¾‹å¦‚å›ç­”â€œ2010 å¹´æŠ•èµ„ 100 ç¾å…ƒè‹¹æœè‚¡ç¥¨åˆ° 2020 å¹´ä»·å€¼å¤šå°‘â€è¿™ç±»é—®é¢˜ï¼Œæ—¢éœ€è‚¡ç¥¨å†å²äº‹å®çŸ¥è¯†åˆè¦ç®—æœ¯æ¨ç†ï¼Œä»…é  CoT è’¸é¦çš„å°æ¨¡å‹éš¾ä»¥åº”å¯¹æœªè§è¿‡çš„æ–°çŸ¥è¯†æˆ–è®¡ç®—ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©å°æ¨¡å‹ä¸ä»…è·å¾—æ¨ç†èƒ½åŠ›ï¼Œè¿˜èƒ½åƒå¤§æ¨¡å‹ Agent ä¸€æ ·å€ŸåŠ©å·¥å…·è§£å†³ä»»åŠ¡ï¼Œæˆä¸ºå…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º Agent Distillation æ¡†æ¶  
è¯¥æ¡†æ¶çªç ´é™æ€æ¨ç†è’¸é¦ï¼Œå°†åŸºäº LLM çš„ Agentï¼ˆå¦‚ ReActã€CodeActï¼‰çš„â€œæ¨ç† - è¡ŒåŠ¨ - è§‚å¯Ÿâ€å®Œæ•´ä»»åŠ¡è§£å†³è¡Œä¸ºè¿ç§»åˆ°å°æ¨¡å‹ï¼Œè®©å°æ¨¡å‹å­¦ä¼šç”¨æ£€ç´¢å’Œä»£ç å·¥å…·è§£å†³é—®é¢˜ï¼Œè€Œéæ­»è®°çŸ¥è¯†å’Œè®¡ç®—æ­¥éª¤ï¼Œæå‡å¯¹æ–°æŸ¥è¯¢çš„æ³›åŒ–èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šFirst - thought prefix æå‡æ•™å¸ˆè½¨è¿¹è´¨é‡  
å¼•å…¥ä¸€ç§æç¤ºæ–¹æ³•ï¼Œæ— éœ€é¢å¤–å¾®è°ƒæ•™å¸ˆæ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆçš„â€œæ¨ç† - è¡ŒåŠ¨ - è§‚å¯Ÿâ€è½¨è¿¹æ›´ä¼˜è´¨ï¼Œä¸ºå°æ¨¡å‹è’¸é¦æä¾›æ›´å¥½çš„ç›‘ç£ä¿¡å·ï¼Œè®©å°æ¨¡å‹èƒ½å­¦åˆ°æ›´æœ‰æ•ˆçš„ä»»åŠ¡è§£å†³è·¯å¾„ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šSelf - consistent action generation å¢å¼ºæµ‹è¯•é²æ£’æ€§  
åœ¨æµ‹è¯•é˜¶æ®µï¼Œè®©å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªè¡ŒåŠ¨è½¨è¿¹ï¼Œåˆ©ç”¨ä»£ç è§£é‡Šå™¨ç­›é€‰å‡ºç»“æœæœ‰æ•ˆä¸”ä¸€è‡´çš„è½¨è¿¹ï¼Œæå‡å°æ¨¡å‹åœ¨å®é™…æµ‹è¯•æ—¶åº”å¯¹å¤æ‚ä»»åŠ¡çš„ç¨³å®šæ€§ï¼Œå‡å°‘é”™è¯¯è¾“å‡ºã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ 4 ä¸ªäº‹å®æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ HotpotQAã€Bamboogle ç­‰ï¼‰å’Œ 4 ä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ MATHã€GSM - Hard ç­‰ï¼‰ä¸Šè¯„ä¼°ï¼Œæ¶µç›–åŸŸå†…å’ŒåŸŸå¤–æ³›åŒ–ã€‚ç»“æœæ˜¾ç¤ºï¼š  
- 0.5Bã€1.5Bã€3B å‚æ•°é‡çš„å°æ¨¡å‹ç» Agent Distillation åï¼Œæ€§èƒ½å¯ä¸ç”¨ CoT è’¸é¦çš„ä¸‹ä¸€å±‚çº§æ›´å¤§æ¨¡å‹ï¼ˆ1.5Bã€3Bã€7Bï¼‰åª²ç¾ã€‚  
- å¯¹æ¯” CoT Promptingã€CoT Distillation ç­‰æ–¹æ³•ï¼ŒAgent Distillation èƒ½æŒç»­æå‡å°æ¨¡å‹åœ¨äº‹å®å’Œæ•°å­¦é¢†åŸŸçš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œè®©å°æ¨¡å‹å€ŸåŠ©å·¥å…·è‡ªé€‚åº”å®Œæˆä»£ç æ‰§è¡Œå’Œä¿¡æ¯æ£€ç´¢æ¥è§£å†³ä»»åŠ¡ï¼ˆå¦‚å›¾ 1 å±•ç¤ºä¸åŒå¤§å° Qwen2.5 - Instruct æ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸Šçš„ç²¾åº¦å¯¹æ¯”ï¼ŒAgent Distillation æ•ˆæœæ›´ä¼˜ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šAgent Distillation ä¸ºå°æ¨¡å‹èµ‹äºˆç±» Agent èƒ½åŠ›æä¾›äº†æ–°èŒƒå¼ï¼Œä¸å†å±€é™äºé™æ€æ¨ç†è’¸é¦ï¼Œè€Œæ˜¯è¿ç§»å®Œæ•´ä»»åŠ¡è§£å†³è¡Œä¸ºï¼Œå¯å‘åç»­å°æ¨¡å‹èƒ½åŠ›å¢å¼ºç ”ç©¶ã€‚  
2. è’¸é¦ä¼˜åŒ–æ‰‹æ®µï¼šFirst - thought prefix æ— éœ€å¾®è°ƒæ•™å¸ˆæ¨¡å‹å°±èƒ½æå‡è½¨è¿¹è´¨é‡ï¼Œä¸ºè’¸é¦è¿‡ç¨‹ä¸­â€œæ•™å¸ˆ - å­¦ç”Ÿâ€ä¿¡å·ä¼ é€’ä¼˜åŒ–æä¾›è½»é‡ä¸”æœ‰æ•ˆçš„æ€è·¯ï¼›Self - consistent action generation åˆ™ä¸ºæµ‹è¯•é˜¶æ®µæå‡å°æ¨¡å‹é²æ£’æ€§æä¾›äº†å¯å‚è€ƒçš„ç­–ç•¥ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€ç¨³å®šæ€§çš„å°æ¨¡å‹æ¨ç†ä»»åŠ¡ã€‚  
3. å¤šé¢†åŸŸéªŒè¯ï¼šåœ¨äº‹å®å’Œæ•°å­¦ç­‰å¤šé¢†åŸŸã€å¤šä»»åŠ¡ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ï¼Œè¯æ˜æ–¹æ³•å…·æœ‰è·¨é¢†åŸŸé€šç”¨æ€§ï¼Œä¸ºä¸åŒé¢†åŸŸå°æ¨¡å‹åº”ç”¨æä¾›å€Ÿé‰´ã€‚  
```

## search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning
### Abstract
Efficiently acquiring external knowledge and up-to-date information is
essential for effective reasoning and text generation in large language models
(LLMs). Prompting advanced LLMs with reasoning capabilities to use search
engines during inference is often suboptimal, as the LLM might not fully
possess the capability on how to interact optimally with the search engine.
This paper introduces Search-R1, an extension of reinforcement learning (RL)
for reasoning frameworks where the LLM learns to autonomously generate
(multiple) search queries during step-by-step reasoning with real-time
retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn
search interactions, leveraging retrieved token masking for stable RL training
and a simple outcome-based reward function. Experiments on seven
question-answering datasets show that Search-R1 improves performance by 41%
(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same
setting. This paper further provides empirical insights into RL optimization
methods, LLM choices, and response length dynamics in retrieval-augmented
reasoning. The code and model checkpoints are available at
https://github.com/PeterGriffinJin/Search-R1.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Search-R1ï¼šç”¨å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹å­¦ä¼šæ¨ç†ä¸æœç´¢å¼•æ“åä½œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆæ–¹é¢è¡¨ç°å“è¶Šï¼Œä½†é¢å¯¹å¤æ‚æ¨ç†ä»»åŠ¡å’Œè·å–å¤–éƒ¨å®æ—¶ä¿¡æ¯æ—¶ä»å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰å°†LLMsä¸æœç´¢å¼•æ“ç»“åˆçš„æ–¹å¼ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”ŸæˆRAGã€æŠŠæœç´¢å¼•æ“å½“å·¥å…·ï¼‰å­˜åœ¨å±€é™ï¼šRAGè™½èƒ½åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œä½†LLMsåœ¨è®­ç»ƒä¸­æœªè¢«ä¼˜åŒ–ä»¥é«˜æ•ˆä¸æœç´¢å¼•æ“äº¤äº’ï¼›å·¥å…·ç±»æ–¹æ³•é‡Œï¼ŒåŸºäºæç¤ºçš„æ–¹å¼æ³›åŒ–æ€§å·®ï¼ŒåŸºäºè®­ç»ƒçš„æ–¹å¼åˆå› ä¾èµ–å¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨è½¨è¿¹å’Œæœç´¢æ“ä½œä¸å¯å¾®åˆ†ï¼Œéš¾ä»¥æœ‰æ•ˆæ‰©å±•ã€‚åŒæ—¶ï¼ŒæŠŠå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºâ€œæœç´¢ + æ¨ç†â€åœºæ™¯ä¹Ÿé¢ä¸´æ¡†æ¶ç¨³å®šæ€§ã€å¤šè½®äº¤é”™æ¨ç†ä¸æœç´¢ã€å¥–åŠ±è®¾è®¡ä¸‰å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLMsåœ¨æ¨ç†æ—¶è‡ªä¸»ä¸”é«˜æ•ˆåœ°åˆ©ç”¨æœç´¢å¼•æ“ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå«æœç´¢å¼•æ“çš„RLç¯å¢ƒä¸ç¨³å®šä¼˜åŒ–æœºåˆ¶  
Search - R1å°†æœç´¢å¼•æ“å»ºæ¨¡ä¸ºç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼Œè®©è½¨è¿¹åºåˆ—èƒ½äº¤é”™ç”ŸæˆLLM tokenså’Œæ‰§è¡Œæœç´¢å¼•æ“æ£€ç´¢ã€‚å®ƒå…¼å®¹PPOã€GRPOç­‰å¤šç§RLç®—æ³•ï¼Œè¿˜é€šè¿‡â€œæ£€ç´¢tokenæ©ç â€æŠ€æœ¯ä¿éšœRLè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè®©æ¨¡å‹åœ¨æ•´åˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ—¶ä¹Ÿèƒ½ç¨³å®šä¼˜åŒ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šè½®æ£€ç´¢ä¸æ¨ç†çš„äº¤äº’æµç¨‹  
æ¨¡å‹å¯åœ¨<search>å’Œ</search>æ ‡è®°è§¦å‘ä¸‹è°ƒç”¨æœç´¢ï¼Œæ£€ç´¢å†…å®¹è¢«åŒ…è£¹åœ¨<information>å’Œ</information>é—´ï¼ŒLLMæ¨ç†æ­¥éª¤ç”¨<think>å’Œ<think>å°è£…ï¼Œæœ€ç»ˆç­”æ¡ˆä¹Ÿä»¥ç‰¹å®šæ ¼å¼è¾“å‡ºï¼Œä»¥æ­¤å®ç°ç»“æ„åŒ–ã€è¿­ä»£å¼çš„å†³ç­–è¿‡ç¨‹ï¼Œè®©å¤§æ¨¡å‹èƒ½ä¾æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼Œå®Œæˆå¤šè½®æ¨ç†ä¸æœç´¢çš„äº¤é”™åä½œã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç®€æ´çš„åŸºäºç»“æœçš„å¥–åŠ±å‡½æ•°è®¾è®¡  
æ‘’å¼ƒå¤æ‚çš„è¿‡ç¨‹å‹å¥–åŠ±ï¼Œé‡‡ç”¨ç®€å•çš„ç»“æœå¯¼å‘å¥–åŠ±å‡½æ•°ã€‚å®éªŒè¯æ˜è¿™ç§æç®€è®¾è®¡åœ¨â€œæœç´¢ + æ¨ç†â€åœºæ™¯ä¸­èƒ½æœ‰æ•ˆå¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼ŒåŠ©åŠ›Search - R1æˆä¸ºDeepSeek - R1 Zeroçš„æ‰©å±•ï¼Œä¸ºæ£€ç´¢é©±åŠ¨å†³ç­–å¼•å…¥æœç´¢å¢å¼ºçš„RLè®­ç»ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šå¼€å±•å®éªŒï¼ŒåŒä¸€è®¾ç½®ä¸‹ï¼ˆç›¸åŒæ£€ç´¢æ¨¡å‹ã€è®­ç»ƒæ•°æ®ã€é¢„è®­ç»ƒLLMï¼‰ï¼ŒSearch - R1è®©Qwen2.5 - 7Bå’ŒQwen2.5 - 3Båˆ†åˆ«æ¯”å„ç±»RAGåŸºçº¿æ¨¡å‹æ€§èƒ½æå‡41%å’Œ20%ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜åœ¨RLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©ã€æ£€ç´¢å¢å¼ºæ¨ç†ä¸­çš„å“åº”é•¿åº¦åŠ¨æ€ç­‰æ–¹é¢ç»™å‡ºäº†å®è¯æ€§è§è§£ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æŠ€æœ¯æ€è·¯å±‚é¢ï¼šä¸ºè§£å†³å¤§æ¨¡å‹â€œæœç´¢ + æ¨ç†â€éš¾é¢˜æä¾›äº†RLæ¡†æ¶æ–°æ€è·¯ï¼ŒæŠŠæœç´¢å¼•æ“çº³å…¥ç¯å¢ƒã€è®¾è®¡å¤šè½®äº¤äº’æµç¨‹ç­‰åšæ³•ï¼Œä¸ºåç»­ä¼˜åŒ–å¤§æ¨¡å‹å¤–éƒ¨çŸ¥è¯†åˆ©ç”¨æ–¹å¼æä¾›äº†å‚è€ƒèŒƒå¼ã€‚  
2. å®éªŒä¸åˆ†æå±‚é¢ï¼šç³»ç»Ÿçš„å®éªŒä¸ä»…éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿˜å¯¹RLæ–¹æ³•é€‰æ‹©ã€ä¸åŒLLMé€‚é…ã€å“åº”é•¿åº¦ç­‰ç»´åº¦å±•å¼€ç ”ç©¶ï¼Œè¿™äº›å®è¯æ´å¯Ÿèƒ½è¾…åŠ©ç ”ç©¶è€…åœ¨ç±»ä¼¼â€œæ£€ç´¢ + æ¨ç†â€ä»»åŠ¡ä¸­åšæ›´ä¼˜å†³ç­–ã€‚  
3. å·¥ç¨‹è½åœ°å±‚é¢ï¼šä»£ç å’Œæ¨¡å‹ checkpoint å¼€æºï¼ˆhttps://github.com/PeterGriffinJin/Search - R1ï¼‰ï¼Œä¾¿äºç¤¾åŒºåŸºäºè¯¥å·¥ä½œè¿›ä¸€æ­¥æ¢ç´¢å¤§æ¨¡å‹ä¸æœç´¢å¼•æ“åä½œçš„æ›´å¤šå¯èƒ½ã€‚  
```

## search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms
### Abstract
Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤§æ¨¡å‹è¾¹â€œæ€è€ƒ-æœç´¢-ç²¾ç‚¼â€è¾¹æ¨ç†ï¼šAutoRefine é©æ–°æ£€ç´¢å¢å¼ºèŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§èƒ½åŠ›ï¼Œä½†å—é™äºè®­ç»ƒè¯­æ–™çš„çŸ¥è¯†å‚¨å¤‡ï¼Œåœ¨éœ€è¦å®æ—¶æˆ–ç²¾å‡†çŸ¥è¯†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡è®©LLMè°ƒç”¨å¤–éƒ¨å·¥å…·æŸ¥è¯¢çŸ¥è¯†åº“æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤å¤§æ ¸å¿ƒç¼ºé™·ï¼š  
1. **ç¼ºä¹å¯¹æ£€ç´¢æ–‡æ¡£çš„ç²¾ç‚¼**ï¼šä¼ ç»Ÿâ€œæ€è€ƒæ—¶æœç´¢ï¼ˆsearch-during-thinkï¼‰â€èŒƒå¼ç›´æ¥åŸºäºæ£€ç´¢åˆ°çš„ï¼ˆå¯èƒ½åŒ…å«å™ªå£°ã€æ— å…³ä¿¡æ¯çš„ï¼‰æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼Œæ²¡æœ‰å…ˆæç‚¼å…³é”®ä¿¡æ¯ï¼Œé™åˆ¶äº†æ¨¡å‹è¯†åˆ«ç¼ºå¤±çŸ¥è¯†ã€åŸºäºä¸å®Œæ•´è¯æ®æ¨ç†æˆ–è¿­ä»£ä¼˜åŒ–æ£€ç´¢çš„èƒ½åŠ›ï¼›  
2. **æ£€ç´¢ä¸“å±å¥–åŠ±æœªå……åˆ†æ¢ç´¢**ï¼šå¤šæ•°æ£€ç´¢å¢å¼ºæ¨ç†æ–¹æ³•ä»…ä¾èµ–â€œç»“æœå¯¼å‘â€å¥–åŠ±ï¼ˆå¦‚æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼‰ï¼Œå¯¹â€œå¦‚ä½•æå‡æ£€ç´¢è¿‡ç¨‹æœ¬èº«â€ç¼ºä¹ç›´æ¥æŒ‡å¯¼ï¼Œå¯¼è‡´æ¨¡å‹éš¾å­¦å¦‚ä½•è·å–æ›´ç›¸å…³ã€æ›´æœ‰ä¿¡æ¯é‡çš„æ–‡æ¡£ã€‚  

ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡º **AutoRefine** â€”â€” ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæ¡†æ¶ï¼Œé‡å¡‘æ£€ç´¢å¢å¼ºæ¨ç†èŒƒå¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šâ€œæ€è€ƒæ—¶æœç´¢+ç²¾ç‚¼ï¼ˆsearch-and-refine-during-thinkï¼‰â€æ–°èŒƒå¼  
ä¼ ç»ŸRAGæ˜¯â€œæ€è€ƒâ†’æœç´¢â†’ç›´æ¥ç”Ÿæˆç­”æ¡ˆâ€ï¼ŒAutoRefineåˆ™åœ¨â€œæœç´¢â€ä¸â€œç”Ÿæˆç­”æ¡ˆâ€ä¹‹é—´æ’å…¥**æ˜¾å¼çš„çŸ¥è¯†ç²¾ç‚¼æ­¥éª¤**ã€‚å…·ä½“é€šè¿‡ `<search>...</search>[documents]<refine>...</refine>` è¿™æ ·çš„æ¨¡æ¿ï¼Œå¼ºåˆ¶æ¨¡å‹å…ˆå¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œâ€œè¿‡æ»¤å™ªå£°ã€æç‚¼å…³é”®ã€ç»„ç»‡è¯æ®â€çš„è¿­ä»£æ“ä½œï¼Œå†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚è¿™ä¸€æ­¥è®©æ¨¡å‹åœ¨å›ç­”å‰å…ˆâ€œæ¶ˆåŒ–â€æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œé¿å…è¢«æ— å…³å†…å®¹å¹²æ‰°ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèåˆâ€œç»“æœå¥–åŠ±+æ£€ç´¢ä¸“å±å¥–åŠ±â€çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ  
AutoRefineé‡‡ç”¨ **Group Relative Policy Optimizationï¼ˆGRPOï¼‰** ç®—æ³•ï¼ŒåŒæ—¶ä¼˜åŒ–ä¸¤ç§å¥–åŠ±ï¼š  
- **ç»“æœå¯¼å‘å¥–åŠ±**ï¼šè¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼›  
- **æ£€ç´¢ä¸“å±å¥–åŠ±**ï¼šåŸºäº `<refine>` å—ä¸­æç‚¼å†…å®¹çš„è´¨é‡è®¡ç®—ï¼Œç›´æ¥æŒ‡å¯¼â€œå¦‚ä½•æ›´å¥½æ£€ç´¢ä¸åˆ©ç”¨æ–‡æ¡£â€ã€‚  
è®­ç»ƒæ—¶ï¼Œæ¨¡å‹ä¼šç”Ÿæˆå¤šæ¡åŒ…å«â€œæ€è€ƒã€æœç´¢ã€ç²¾ç‚¼ã€å›ç­”â€çš„æ¨ç†è½¨è¿¹ï¼Œå†ç”¨GRPOå¯¹è¿™äº›è½¨è¿¹åšä¼˜åŒ–ï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨æ¨ç†ä¸­æ›´æ™ºèƒ½åœ°è°ƒç”¨æ£€ç´¢å·¥å…·ã€æ›´é«˜æ•ˆåœ°å¤„ç†ä¿¡æ¯ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å•è·³ï¼ˆsingle-hopï¼‰å’Œå¤šè·³ï¼ˆmulti-hopï¼‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­éªŒè¯AutoRefineï¼š  
- **æ€§èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•**ï¼šåœ¨å¤æ‚çš„å¤šè·³æ¨ç†åœºæ™¯ä¸‹æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œè¯æ˜â€œæœç´¢+ç²¾ç‚¼â€èŒƒå¼å¯¹é•¿é“¾æ¡ã€å¤šæ­¥éª¤æ¨ç†çš„æœ‰æ•ˆæ€§ï¼›  
- **æ£€ç´¢è´¨é‡æ›´é«˜é¢‘**ï¼šåˆ†ææ˜¾ç¤ºAutoRefineæ›´é¢‘ç¹å‘èµ·é«˜è´¨é‡æœç´¢ï¼Œä¸”èƒ½æœ‰æ•ˆæ•´åˆã€åˆæˆè¯æ®ï¼Œå‡å°‘å™ªå£°å¹²æ‰°ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **èŒƒå¼åˆ›æ–°**ï¼šâ€œæœç´¢åç²¾ç‚¼å†å›ç­”â€çš„æµç¨‹è®¾è®¡ï¼Œä¸ºè§£å†³â€œæ£€ç´¢ä¿¡æ¯å™ªå£°/æ— å…³â€é—®é¢˜æä¾›äº†æ–°è§†è§’ï¼Œå¯å¯å‘åç»­RAGç±»å·¥ä½œä¼˜åŒ–æ¨ç†æµç¨‹ï¼›  
2. **å¥–åŠ±è®¾è®¡**ï¼šå°†â€œè¿‡ç¨‹æ€§å¥–åŠ±ï¼ˆæ£€ç´¢è´¨é‡ï¼‰â€ä¸â€œç»“æœæ€§å¥–åŠ±ï¼ˆç­”æ¡ˆæ­£ç¡®ï¼‰â€ç»“åˆï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¼•å¯¼æ¨¡å‹â€œè¿‡ç¨‹ä¼˜åŒ–â€ä¸Šçš„æ½œåŠ›ï¼Œä¸ºRLä¸LLMç»“åˆçš„å¥–åŠ±æœºåˆ¶è®¾è®¡æä¾›å‚è€ƒï¼›  
3. **å¤šè·³æ¨ç†é€‚é…**ï¼šåœ¨å¤šè·³ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œè¯´æ˜è¯¥æ–¹æ³•å¯¹éœ€è¦å¤šæ¬¡æ£€ç´¢ã€ä¿¡æ¯æ‹¼æ¥çš„å¤æ‚ä»»åŠ¡å‹å¥½ï¼Œå¯è¿ç§»åˆ°çŸ¥è¯†å¯†é›†å‹çš„é•¿æ–‡æœ¬æ¨ç†ã€å¤šæ­¥éª¤å†³ç­–ç­‰åœºæ™¯ã€‚  

æ€»ä¹‹ï¼ŒAutoRefineé€šè¿‡â€œæµç¨‹èŒƒå¼+å¥–åŠ±æœºåˆ¶â€çš„åŒé‡åˆ›æ–°ï¼Œè®©å¤§æ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºæ¨ç†ä¸­æ›´è‡ªä¸»ã€æ›´æ™ºèƒ½ï¼Œä¸ºçªç ´â€œçŸ¥è¯†å‚¨å¤‡é™åˆ¶+æ£€ç´¢ä¿¡æ¯å™ªå£°â€ä¸¤å¤§ç—›ç‚¹æä¾›äº†ä¸€å¥—ç®€æ´è€Œæœ‰åŠ›çš„æ–¹æ¡ˆ~
```

## select2reason--efficient-instruction-tuning-data-selection-for-long-cot-reasoning
### Abstract
A practical approach to activate long chain-of-thoughts reasoning ability in
pre-trained large language models is to perform supervised fine-tuning on
instruction datasets synthesized by strong Large Reasoning Models such as
DeepSeek-R1, offering a cost-effective alternative to reinforcement learning.
However, large-scale instruction sets with more than 100k samples incur
significant training overhead, while effective strategies for automatic
long-CoT instruction selection still remain unexplored. In this work, we
propose Select2Reason, a novel and efficient instruction-tuning data selection
framework for long-CoT reasoning. From the perspective of emergence of
rethinking behaviors like self-correction and backtracking, we investigate
common metrics that may determine the quality of long-CoT reasoning
instructions. Select2Reason leverages a quantifier to estimate difficulty of
question and jointly incorporates a reasoning trace length-based heuristic
through a weighted scheme for ranking to prioritize high-utility examples.
Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only
10% of the data selected by Select2Reason achieves performance competitive with
or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across
three competition-level and six comprehensive mathematical benchmarks. Further
experiments highlight the scalability in varying data size, efficiency during
inference, and its adaptability to other instruction pools with minimal cost.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Select2Reasonï¼šé«˜æ•ˆç­›é€‰é•¿é“¾æ€ç»´æ¨ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œè§£é”å¤§æ¨¡å‹æ·±åº¦æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¦æ¿€æ´»é•¿é“¾æ€ç»´ï¼ˆlong - CoTï¼‰æ¨ç†èƒ½åŠ›ï¼Œå¸¸ç”¨æ–¹æ³•æ˜¯åœ¨å¼ºæ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek - R1ï¼‰åˆæˆçš„æŒ‡ä»¤æ•°æ®é›†ä¸Šåšæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™ç§æ–¹å¼æ¯”å¼ºåŒ–å­¦ä¹ æ›´å…·æˆæœ¬æ•ˆç›Šã€‚ä½†è¶…10ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æŒ‡ä»¤é›†å¸¦æ¥å·¨å¤§è®­ç»ƒå¼€é”€ï¼Œä¸”è‡ªåŠ¨ç­›é€‰é•¿CoTæŒ‡ä»¤çš„æœ‰æ•ˆç­–ç•¥ä»å¾…æ¢ç´¢ã€‚åŒæ—¶ï¼Œè¿‡å¾€è™½æœ‰ä¸€äº›æ•°æ®ç­›é€‰å·¥ä½œï¼Œä½†é’ˆå¯¹é•¿CoTæ¨ç†çš„æŒ‡ä»¤ç­›é€‰æŒ‘æˆ˜æœªå……åˆ†è§£å†³ï¼›ä¸€äº›ä¼˜è´¨æ•°æ®é›†æ„å»ºä¾èµ–å®šæ€§å¯å‘å¼æ–¹æ³•ï¼Œç¼ºä¹ä¸¥è°¨å®šé‡éªŒè¯ï¼Œä¸”æµç¨‹ä¸å…¬å¼€ï¼Œé˜»ç¢å¤ç°ä¸æ¨å¹¿ã€‚æ­¤å¤–ï¼Œé•¿CoTæ¨ç†ä¸­â€œåæ€è¡Œä¸ºï¼ˆå¦‚è‡ªæˆ‘ä¿®æ­£ã€å›æº¯ï¼‰â€ç›¸å…³ç‰¹å¾å¯¹æŒ‡ä»¤è´¨é‡çš„å½±å“ä¹Ÿéœ€æ·±å…¥æ¢ç©¶ï¼Œä»¥æ­¤ä¸ºèƒŒæ™¯ï¼Œæœ¬æ–‡æå‡ºSelect2Reasonæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSelect2Reasonæ¡†æ¶ç”¨äºé•¿CoTæ¨ç†çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ç­›é€‰
ä»é•¿CoTæ¨ç†ä¸­è‡ªæˆ‘ä¿®æ­£ã€å›æº¯ç­‰â€œåæ€è¡Œä¸ºâ€æ¶Œç°çš„è§†è§’ï¼Œæ¢ç©¶å†³å®šé•¿CoTæ¨ç†æŒ‡ä»¤è´¨é‡çš„æŒ‡æ ‡ã€‚æ¡†æ¶åˆ©ç”¨å¤§æ¨¡å‹ä½œä¸ºâ€œè£åˆ¤â€ï¼ˆLLM - as - a - Judgeï¼‰é‡åŒ–é—®é¢˜éš¾åº¦ï¼Œä¼˜å…ˆé€‰æ‹©æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼›è¿˜è®¾è®¡æŒ‡ä»¤ - å“åº”è”åˆæ’åºå™¨ï¼Œé€šè¿‡åŠ æƒæ–¹æ¡ˆç»“åˆåŸºäºéš¾åº¦å’Œæ¨ç†è½¨è¿¹é•¿åº¦çš„æ’åºï¼Œä¼˜å…ˆé€‰æ‹©é«˜ä»·å€¼æ ·æœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéªŒè¯æ¨ç†è½¨è¿¹é•¿åº¦ä¸é—®é¢˜éš¾åº¦ä½œä¸ºç­›é€‰ä¼˜è´¨æŒ‡ä»¤çš„å…³é”®æŒ‡æ ‡
é€šè¿‡ç»Ÿè®¡åˆ†æï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼Œæ›´é•¿æ¨ç†è½¨è¿¹åœ¨æ¯ä¸€æ­¥ä¸­â€œWaitã€Alternativelyã€Maybeã€Howeverâ€ç­‰åæ€tokenå‡ºç°é¢‘ç‡æ›´é«˜ï¼Œä¸”éš¾é¢˜å¯¹åº”çš„æŒ‡ä»¤ä¸­è¿™ç±»åæ€tokenä¹Ÿæ›´å¤šï¼‰å’Œåˆæ­¥æŒ‡ä»¤ç­›é€‰ä»»åŠ¡å®éªŒï¼ˆå¦‚å›¾3æ‰€ç¤ºï¼ŒåŸºäºæœ€é•¿æ¨ç†è½¨è¿¹ä¼˜å…ˆé€‰æ‹©å­é›†å¾®è°ƒçš„æ¨¡å‹åœ¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹éƒ½æ¯”åŸºäºä¸­ã€çŸ­è½¨è¿¹è®­ç»ƒçš„æ¨¡å‹è¡¨ç°å¥½ï¼›ä¸”åŸºäºåŸºç¡€æ¨¡å‹éš¾è§£å†³æŒ‡ä»¤å­é›†è®­ç»ƒçš„æ¨¡å‹æ¯”æ˜“é¢˜å­é›†è®­ç»ƒçš„è¡¨ç°å¥½ï¼‰ï¼ŒéªŒè¯äº†æ¨ç†è½¨è¿¹é•¿åº¦æ˜¯ç®€å•ä¸”æœ‰æ•ˆçš„ç­›é€‰å¯å‘å¼æŒ‡æ ‡ï¼Œé—®é¢˜éš¾åº¦ä¹Ÿå¯¹æŒ‡ä»¤è´¨é‡æœ‰é‡è¦å½±å“ï¼Œæ›´å…·æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤æœ‰æ›´å¤§å­¦ä¹ ä»·å€¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨OpenR1 - Math - 220kæ•°æ®é›†ä¸Šå®éªŒï¼Œä»…ç”¨Select2Reasonç­›é€‰å‡ºçš„10%æ•°æ®å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸‰ä¸ªç«èµ›çº§å’Œå…­ä¸ªç»¼åˆæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸å…¨é‡æ•°æ®å¾®è°ƒä»¥åŠå¼€æºåŸºçº¿æ¨¡å‹OpenR1 - Qwen - 7Bç›¸å½“ç”šè‡³æ›´ä¼˜ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ¨ç†æ—¶æ›´é«˜æ•ˆï¼ˆç”¨æ›´å°‘æ€è€ƒtokenæ¢ç´¢å‡ºæ›´å¼ºæ€§èƒ½çš„è§£å†³æ–¹æ¡ˆï¼‰ï¼›åœ¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹å±•ç°å‡ºè‰¯å¥½æ‰©å±•æ€§ï¼›è¿˜èƒ½ä»¥ä½æˆæœ¬é€‚é…å…¶ä»–é•¿CoTæ¨ç†æŒ‡ä»¤æ± ï¼ˆå¦‚å«11ä¸‡æ ·æœ¬çš„Chinese - DeepSeek - R1 - Distillæ•°æ®é›†ï¼‰ï¼Œä½“ç°å¼ºæ³›åŒ–æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä¸ºé•¿CoTæ¨ç†åœºæ™¯ä¸‹çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ç­›é€‰æä¾›äº†æ–°é¢–é«˜æ•ˆçš„æ¡†æ¶æ€è·¯ï¼Œä»åæ€è¡Œä¸ºè§†è§’åˆ‡å…¥æ¢ç©¶æŒ‡ä»¤è´¨é‡æŒ‡æ ‡ï¼Œä¸ºåç»­ç›¸å…³æ•°æ®ç­›é€‰å·¥ä½œæä¾›äº†æ–¹æ³•è®ºå‚è€ƒã€‚
2. éªŒè¯çš„æ¨ç†è½¨è¿¹é•¿åº¦å’Œé—®é¢˜éš¾åº¦ç­‰å…³é”®æŒ‡æ ‡ï¼Œå¯æŒ‡å¯¼ä»ä¸šè€…åœ¨æ„å»ºæˆ–ç­›é€‰é•¿CoTæ¨ç†æŒ‡ä»¤æ•°æ®é›†æ—¶ï¼Œæ›´æœ‰é’ˆå¯¹æ€§åœ°è¯„ä¼°å’Œé€‰æ‹©æ•°æ®ï¼Œæå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚
3. æ¡†æ¶åœ¨ä¸åŒæ•°æ®è§„æ¨¡ã€æ¨ç†æ•ˆç‡ã€è·¨æŒ‡ä»¤æ± é€‚é…ç­‰æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸ºå¤§æ¨¡å‹åœ¨é•¿é“¾æ¨ç†èƒ½åŠ›è®­ç»ƒä¼˜åŒ–æ—¶çš„å·¥ç¨‹å®è·µæä¾›äº†å¯å¤ç”¨çš„æ¨¡å¼ï¼ŒåŠ©åŠ›é™ä½è®­ç»ƒæˆæœ¬åŒæ—¶æå‡æ¨¡å‹æ¨ç†æ€§èƒ½ã€‚
```

## stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization
### Abstract
Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our code
will be released on https://github.com/Zillwang/StepSearch.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | StepSearchï¼šç”¨åˆ†æ­¥è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç‚¹ç‡ƒå¤§æ¨¡å‹æœç´¢èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­éœ€è¦è¿­ä»£è·å–é«˜ä»·å€¼å¤–éƒ¨çŸ¥è¯†ï¼Œä½†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒLLMsè¿›è¡Œæœç´¢å¼æ–‡æ¡£æ£€ç´¢çš„å·¥ä½œï¼Œä»…ä¾èµ–å…¨å±€ä¿¡å·å¸¦æ¥çš„ç¨€ç–å¥–åŠ±ï¼Œåœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æ­¤å‰æ–¹æ³•å­˜åœ¨å¯¹ä¸­é—´æŸ¥è¯¢å’Œå¤šæ­¥æ£€ç´¢ç¼ºä¹ç»†ç²’åº¦ç›‘ç£ã€å¤šæ•°å¤šè·³é—®ç­”æ¡†æ¶åœ¨æŸ¥è¯¢è½¨è¿¹å»ºæ¨¡ä¸Šå­˜åœ¨å…³é”®ç¼ºå£ç­‰é—®é¢˜ï¼Œä¸ºå¡«è¡¥è¿™äº›ç ”ç©¶ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºStepSearchæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºé€šç”¨å¤šè·³æœç´¢æ•°æ®  
åŸºäºMuSiQueæ•°æ®é›†å¼€å‘æ–°é¢–çš„æ•°æ® pipelineï¼Œç”Ÿæˆ6ä¸‡æ¡ç»è¿‡ç­›é€‰çš„å­é—®é¢˜æœç´¢å…³é”®è¯ï¼Œè¿™äº›å…³é”®è¯å¯åœ¨ä¸åŒæ£€ç´¢æ•°æ®é›†ä¸Šæ³›åŒ–ã€‚å…·ä½“æµç¨‹ä¸ºåˆ©ç”¨GPT - 4oä¸°å¯Œåˆ†è§£åçš„MuSiQueé—®é¢˜ï¼Œå¾—åˆ°è¿è´¯çš„å­é—®é¢˜ - ç­”æ¡ˆå¯¹å¹¶ç”Ÿæˆæ¯ä¸€æ­¥çš„æœç´¢æŸ¥è¯¢ï¼›å°†å¢å¼ºåçš„æ­¥éª¤é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæœç´¢æŸ¥è¯¢é›†ï¼›å‘å¤šä¸ªæ¥æºå‘å‡ºæŸ¥è¯¢å¹¶ç­›é€‰å‡ºæœ‰æ•ˆç»“æœã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºStepSearchåˆ†æ­¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶  
åœ¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åŸºç¡€ä¸Šå¢åŠ åŸºäºtokençº§åˆ«çš„å¥–åŠ±ï¼ˆä¿¡æ¯å¢ç›Šå’Œå†—ä½™æƒ©ç½šï¼‰ï¼Œç”¨äºæŸ¥è¯¢æ„å»ºå’Œæ–‡æ¡£æ£€ç´¢ã€‚å°†æ¯ä¸€è½®äº¤äº’åˆ’åˆ†ä¸ºæ€è€ƒâ†’æœç´¢â†’å›ç­”é˜¶æ®µï¼Œä¸ºæ¯ä¸ªtokenåˆ†é…ä¿¡æ¯å¢ç›Šä¿¡å·å’Œå†—ä½™æƒ©ç½šï¼Œå¼•å¯¼æ¨¡å‹å°†å¤šè·³æŸ¥è¯¢åˆ†è§£ä¸ºèšç„¦çš„æœç´¢å­ä»»åŠ¡ï¼ŒåŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼Œæ›´æœ‰æ•ˆåœ°æ•´åˆå¤–éƒ¨è¯æ®ã€‚åŒæ—¶è®¾è®¡ç®€çº¦çš„æç¤ºæ¨¡æ¿ï¼Œåœ¨è®­ç»ƒæ—¶è§£è€¦å‚æ•°æ›´æ–°ä¸æ£€ç´¢äº§ç‰©ï¼Œèšç„¦æ¨¡å‹å†…éƒ¨æ¨ç†å’Œæœç´¢ç­–ç•¥å‚æ•°å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ ‡å‡†å¤šè·³QAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒStepSearchæ˜¾è‘—è¶…è¶Šå…¨å±€å¥–åŠ±åŸºçº¿ã€‚ä½¿ç”¨ä»…19kçš„è®­ç»ƒæ•°æ®ï¼Œå¯¹äº3Bå’Œ7Bè§„æ¨¡çš„æ¨¡å‹ï¼Œç›¸æ¯”å„ç§åŸºäºRLçš„æœç´¢åŸºçº¿åˆ†åˆ«å®ç°äº†11.2%å’Œ4.2%çš„ç»å¯¹æ€§èƒ½æå‡ï¼›åœ¨ä¸åŒå¤šè·³QAåŸºå‡†ä¸Šï¼Œç›¸æ¯”æ ‡å‡†RLåŸºçº¿åˆ†åˆ«æœ‰5.7%ã€9.1%ã€10.0%å’Œ15.2%çš„ç»å¯¹æå‡ï¼Œè¯æ˜äº†ç»†ç²’åº¦ã€åˆ†æ­¥ç›‘ç£åœ¨ä¼˜åŒ–æ·±åº¦æœç´¢LLMsæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºæ–¹é¢ï¼šé€šè¿‡æ•°æ® pipeline ç”ŸæˆåŒ…å«å­é—®é¢˜çº§æœç´¢è½¨è¿¹çš„ç»†ç²’åº¦é—®ç­”æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ›´ä¸°å¯Œçš„å¤šè·³æ¨ç†ç›¸å…³æ•°æ®æ”¯æ’‘ï¼Œè¿™ç§ä»å·²æœ‰å¼€æºæ•°æ®é›†æ‹“å±•æ„å»ºä¸“ç”¨æ•°æ®çš„æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚
2. å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹é¢ï¼šå¼•å…¥åˆ†æ­¥çš„ã€tokençº§åˆ«çš„å¥–åŠ±æœºåˆ¶ï¼Œå…³æ³¨ä¸­é—´æ­¥éª¤çš„ä¿¡æ¯å¢ç›Šä¸å†—ä½™é—®é¢˜ï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡ä¸­å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç¨€ç–ã€ç›‘ç£ä¸ç»†çš„é—®é¢˜æä¾›äº†æ–°æ–¹å‘ï¼Œå¯ç”¨äºå…¶ä»–éœ€è¦å¤šæ­¥è¿­ä»£ã€ç»†ç²’åº¦ç›‘ç£çš„LLMsåº”ç”¨åœºæ™¯ä¼˜åŒ–ã€‚
3. è®­ç»ƒç­–ç•¥æ–¹é¢ï¼šåœ¨è®­ç»ƒæ—¶å¯¹ç‰¹å®šæ¨¡å—ï¼ˆå¦‚æ£€ç´¢äº§ç‰©ç›¸å…³éƒ¨åˆ†ï¼‰è¿›è¡Œæ¢¯åº¦å±è”½ï¼Œèšç„¦å…³é”®å­¦ä¹ ç›®æ ‡ï¼Œè¿™ç§è§£è€¦å­¦ä¹ ç›®æ ‡çš„è®­ç»ƒæŠ€å·§åœ¨èåˆå¤–éƒ¨å·¥å…·æˆ–ä¿¡æ¯çš„LLMsè®­ç»ƒä¸­å…·æœ‰å‚è€ƒä»·å€¼ã€‚
```

## towards-effective-code-integrated-reasoning
### Abstract
In this paper, we investigate code-integrated reasoning, where models
generate code when necessary and integrate feedback by executing it through a
code interpreter. To acquire this capability, models must learn when and how to
use external code tools effectively, which is supported by tool-augmented
reinforcement learning (RL) through interactive learning. Despite its benefits,
tool-augmented RL can still suffer from potential instability in the learning
dynamics. In light of this challenge, we present a systematic approach to
improving the training effectiveness and stability of tool-augmented RL for
code-integrated reasoning. Specifically, we develop enhanced training
strategies that balance exploration and stability, progressively building
tool-use capabilities while improving reasoning performance. Through extensive
experiments on five mainstream mathematical reasoning benchmarks, our model
demonstrates significant performance improvements over multiple competitive
baselines. Furthermore, we conduct an in-depth analysis of the mechanism and
effect of code-integrated reasoning, revealing several key insights, such as
the extension of model's capability boundaries and the simultaneous improvement
of reasoning efficiency through code integration. All data and code for
reproducing this work are available at: https://github.com/RUCAIBox/CIR.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»£ç é›†æˆæ¨ç†ï¼šè®©å¤§æ¨¡å‹æ›´èªæ˜åœ°â€œæ…¢æ€è€ƒâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è™½æœ‰è¿›å±•ï¼Œä½†å—é™äºè‡ªèº«å›ºæœ‰ç¼ºé™·ï¼ˆå¦‚æ•°å€¼è®¡ç®—ä¸ç²¾ç¡®ã€çŸ¥è¯†è¦†ç›–æœ‰é™ï¼‰ï¼Œéš¾ä»¥çªç ´èƒ½åŠ›ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç”¨å¤–éƒ¨å·¥å…·å¢å¼ºLLMæˆäº†çƒ­é—¨æ–¹å‘ï¼Œå…¶ä¸­â€œä»£ç é›†æˆæ¨ç†â€ï¼ˆè®©æ¨¡å‹æŒ‰éœ€ç”Ÿæˆä»£ç å¹¶é€šè¿‡ä»£ç è§£é‡Šå™¨æ‰§è¡Œåé¦ˆï¼‰æ˜¯å¾ˆæœ‰æ½œåŠ›çš„è·¯å¾„ã€‚ä¸è¿‡ï¼Œç°æœ‰å·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆtool - augmented RLï¼‰åœ¨è®­ç»ƒæ—¶å­˜åœ¨åŠ¨æ€ä¸ç¨³å®šã€è®­ç»ƒå¤æ‚åº¦é«˜ï¼ˆè¦åŒæ—¶å­¦æ¨ç†å’Œå·¥å…·ä½¿ç”¨ï¼‰ç­‰é—®é¢˜ï¼Œé˜»ç¢äº†ä»£ç é›†æˆæ¨ç†èƒ½åŠ›çš„é«˜æ•ˆä¹ å¾—ã€‚æœ¬æ–‡æ­£æ˜¯ä¸ºäº†ç³»ç»Ÿæ€§è§£å†³tool - augmented RLåœ¨ä»£ç é›†æˆæ¨ç†è®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ä¸ç¨³å®šæ€§éš¾é¢˜ï¼ŒåŒæ—¶æ·±å…¥åˆ†æä»£ç é›†æˆæ¨ç†çš„æœºåˆ¶ä¸æ•ˆæœè€Œå±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ˜ç¡®â€œä»£ç é›†æˆæ¨ç†â€èŒƒå¼
å°†ä»£ç é›†æˆæ¨ç†å½¢å¼åŒ–å®šä¹‰ï¼šå¤§æ¨¡å‹åœ¨æ¨ç†æ—¶ï¼Œé‡éœ€ç²¾ç¡®è®¡ç®—ï¼ˆå¦‚æ•°å€¼ã€é€»è¾‘ã€ç¬¦å·æ“ä½œï¼‰åœºæ™¯è§¦å‘ä»£ç ç”Ÿæˆï¼Œç”Ÿæˆçš„ä»£ç ç”±å¤–éƒ¨è§£é‡Šå™¨æ‰§è¡Œï¼Œç»“æœå›çŒåˆ°æ¨ç†åºåˆ—æŒ‡å¯¼åç»­æ­¥éª¤ï¼Œå½¢æˆâ€œç”Ÿæˆä»£ç â†’æ‰§è¡Œâ†’æ›´æ–°ä¸Šä¸‹æ–‡â†’å†æ¨ç†â€çš„è¿­ä»£å¾ªç¯ï¼Œæ¶èµ·è‡ªç„¶è¯­è¨€æ¨ç†ä¸å½¢å¼åŒ–æ¨ç†ï¼ˆä»£ç ï¼‰çš„æ¡¥æ¢ï¼Œè®©æ¨¡å‹èƒ½å€ŸåŠ©ä»£ç è§£é‡Šå™¨çš„è®¡ç®—èƒ½åŠ›çªç ´è‡ªèº«å±€é™ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¢å¼ºå‹å·¥å…· - å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥
ä¸ºå¹³è¡¡æ¢ç´¢ä¸ç¨³å®šï¼Œæå‡ºä¸€ç³»åˆ—æ”¹è¿›ï¼š
 - æ¢ç´¢å±‚é¢ï¼šç§»é™¤KLæ•£åº¦é¡¹ï¼Œé‡‡ç”¨å—DAPOå¯å‘çš„clip - higheræ–¹æ³•ï¼Œè¿˜é€æ­¥å¢åŠ å·¥å…·äº¤äº’é¢„ç®—ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢å·¥å…·ä½¿ç”¨çš„å¯èƒ½æ€§ï¼›
 - ç¨³å®šå±‚é¢ï¼šå°†ç†µç³»æ•°è®¾ä¸º0ã€å¼ºåˆ¶ç²¾ç¡®ä»£ç å—åŒ¹é…ä»¥å‡å°‘rolloutå™ªå£°ã€å±è”½å¤–éƒ¨å·¥å…·åé¦ˆå¹²æ‰°ç­‰ï¼Œè®©è®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šï¼Œåœ¨å­¦å·¥å…·ä½¿ç”¨èƒ½åŠ›æ—¶ä¹Ÿèƒ½æå‡æ¨ç†æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¼€å±•å¤§é‡å®éªŒï¼Œæ¨¡å‹æ€§èƒ½è¿œè¶…å¤šä¸ªæœ‰ç«äº‰åŠ›çš„åŸºçº¿æ¨¡å‹ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ†ææ­ç¤ºä»£ç é›†æˆæ¨ç†çš„å…³é”®æœºåˆ¶ï¼š
 - æ‰©å±•æ¨¡å‹èƒ½åŠ›è¾¹ç•Œï¼ˆä»¥PASS@Kè¡¡é‡ï¼‰ï¼Œå€ŸåŠ©ä»£ç è§£é‡Šå™¨çš„è®¡ç®—åŠ›çªç ´è‡ªèº«é™åˆ¶ï¼›
 - ç›¸æ¯”é•¿æ€ç»´é“¾ï¼ˆlong - CoTï¼‰æ¨ç†ï¼Œä»£ç é›†æˆæ¨ç†è·¯å¾„æ›´ç®€æ´é«˜æ•ˆï¼Œå¸¸å…ˆç»™è§£å†³æ–¹æ¡ˆæ¦‚è§ˆå†ç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼›
 - å¯æ‰§è¡Œä½†é€»è¾‘é”™çš„ä»£ç è™½å¯èƒ½æ‹–åè…¿ï¼Œä¸å¯æ‰§è¡Œä»£ç å´å¯èƒ½å€’é€¼æ¨¡å‹åæ€ä¿®æ­£è¾“å‡ºï¼›
 - ä¸åŒé¢˜å‹å—ç›Šç¨‹åº¦æœ‰å·®å¼‚ï¼Œå‡ ä½•é¢˜åœ¨å››ç§é¢˜å‹ä¸­æå‡æœ€å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å·¥å…·å¢å¼ºLLMçš„æ€è·¯æ‹“å±•ï¼šæŠŠä»£ç è§£é‡Šå™¨ä½œä¸ºæ ¸å¿ƒå¢å¼ºå·¥å…·ï¼Œä¸”å½¢å¼åŒ–å®šä¹‰äº†ä»£ç é›†æˆæ¨ç†èŒƒå¼ï¼Œä¸ºåç»­ç”¨â€œä»£ç +æ‰§è¡Œåé¦ˆâ€å¢å¼ºæ¨ç†æä¾›äº†æ¸…æ™°æ¡†æ¶å‚è€ƒï¼›
2. å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¼˜åŒ–ï¼šé’ˆå¯¹å·¥å…·å¢å¼ºRLçš„ä¸ç¨³å®šæ€§ï¼Œæå‡ºçš„å¹³è¡¡æ¢ç´¢ä¸ç¨³å®šçš„è®­ç»ƒç­–ç•¥ç»„åˆï¼ˆå¦‚clip - higherã€ç†µç³»æ•°è°ƒæ•´ç­‰ï¼‰ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å·¥å…·å¢å¼ºå‹RLè®­ç»ƒåœºæ™¯ï¼›
3. æœºåˆ¶åˆ†æè§†è§’ï¼šä»èƒ½åŠ›è¾¹ç•Œã€æ¨ç†è·¯å¾„ã€ä»£ç å½±å“ã€é¢˜å‹å·®å¼‚ç­‰å¤šç»´åº¦åˆ†æä»£ç é›†æˆæ¨ç†çš„æ•ˆæœï¼Œä¸ºåç»­ä¼˜åŒ–æ¨¡å‹ä¸ä»»åŠ¡é€‚é…æ€§æä¾›äº†åˆ†ææ–¹æ³•è®ºï¼›
4. å¼€æºèµ„æºï¼šå…¬å¼€æ•°æ®ã€ä»£ç å’Œæ¨¡å‹ checkpointï¼ˆhttps://github.com/RUCAIBox/CIRï¼‰ï¼Œæ–¹ä¾¿å¤ç°ä¸äºŒæ¬¡å¼€å‘ï¼Œé™ä½äº†é¢†åŸŸç ”ç©¶é—¨æ§›ã€‚
```

## an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents
### Abstract
Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ é©±åŠ¨æ¨ç†-æœç´¢äº¤ç»‡å‹å¤§æ¨¡å‹æ™ºèƒ½ä½“çš„å®è¯ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†è¯¸å¤šä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨éœ€ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ã€å€ŸåŠ©å·¥å…·çš„åœºæ™¯ï¼ˆå¦‚æœç´¢ä»»åŠ¡ï¼‰ä¸­å­˜åœ¨å±€é™ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºè®­ç»ƒLLMæˆä¸ºèƒ½äº¤ç»‡æ¨ç†ä¸æœç´¢çš„æ™ºèƒ½ä½“æä¾›äº†æ½œåŠ›ï¼Œç„¶è€Œè¿™ç±»æ™ºèƒ½ä½“çš„æœ€ä¼˜è®¾è®¡ä»ä¸æ˜æ™°ï¼Œå¦‚å¥–åŠ±è®¾è®¡ã€åŸºç¡€LLMé€‰æ‹©ä¸ç‰¹æ€§ã€æœç´¢å¼•æ“åœ¨RLè¿‡ç¨‹ä¸­è§’è‰²ç­‰å…³é”®å› ç´ å¾…æ·±å…¥æ¢ç©¶ã€‚æ­¤å‰æ–¹æ³•åœ¨è®­ç»ƒæœç´¢æ™ºèƒ½ä½“æ—¶ï¼ŒåŸºäºæç¤ºæˆ–æœ‰ç›‘ç£å¾®è°ƒå­˜åœ¨éš¾æ‰©å±•ç­‰é—®é¢˜ï¼Œè€ŒRLè™½æœ‰å‰æ™¯ä½†ç›¸å…³å…³é”®é—®é¢˜ç ”ç©¶ä¸è¶³ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡å…¨é¢å®è¯ç ”ç©¶å¡«è¡¥è¿™äº›ç©ºç™½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç³»ç»Ÿæ¢ç©¶å¥–åŠ±è®¾è®¡å¯¹æœç´¢æ™ºèƒ½ä½“è®­ç»ƒçš„å½±å“ 
åŒºåˆ†æ ¼å¼å¥–åŠ±ï¼ˆåæ˜ å¯¹æ™ºèƒ½ä½“åŠ¨ä½œæ ¼å¼çš„éµå¾ªï¼‰ä¸ä¸­é—´æ£€ç´¢å¥–åŠ±ï¼ˆè¿­ä»£æ¿€åŠ±ä¸ç»“æœç›¸å…³çš„æ£€ç´¢ï¼‰ï¼Œå®è¯åˆ†æä¸åŒå¥–åŠ±åœ¨è®­ç»ƒä¸­ä½œç”¨ï¼Œæ˜ç¡®æ ¼å¼å¥–åŠ±å¯¹æ€§èƒ½æå‡æ›´æœ‰æ•ˆï¼Œä¸­é—´æ£€ç´¢å¥–åŠ±æ•ˆç”¨æœ‰é™ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ·±å…¥åˆ†æåŸºç¡€LLMç‰¹æ€§çš„å½±å“ 
å¯¹æ¯”é€šç”¨å‹ä¸æ¨ç†ä¸“ç”¨å‹LLMï¼Œä»¥åŠä¸åŒè§„æ¨¡æ¨¡å‹åœ¨RLè®­ç»ƒä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºæ¨¡å‹è§„æ¨¡ã€åˆå§‹åŒ–ç±»å‹ï¼ˆé€šç”¨æˆ–æ¨ç†ä¸“ç”¨ï¼‰å¯¹RLè®­ç»ƒç»“æœçš„æ˜¾è‘—å½±å“ï¼Œä¸ºé€‰æ‹©åˆé€‚åŸºç¡€æ¨¡å‹æä¾›ä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå‰–ææœç´¢å¼•æ“é€‰æ‹©çš„ä½œç”¨ 
ç ”ç©¶è®­ç»ƒæ—¶ä¸åŒè´¨é‡æœç´¢å¼•æ“ï¼ˆä»éšæœºå™ªå£°åˆ°å¼ºå¯†é›†æ£€ç´¢å™¨ï¼‰å¦‚ä½•å¡‘é€ RLè®­ç»ƒåŠ¨æ€ï¼Œä»¥åŠæ¨ç†æ—¶æ£€ç´¢ç³»ç»Ÿå˜åŒ–å¯¹æ™ºèƒ½ä½“é²æ£’æ€§å½±å“ï¼Œæ˜ç¡®æœç´¢å¼•æ“é€‰æ‹©åœ¨è®­ç»ƒåŠ¨æ€ä¸æ¨ç†é²æ£’æ€§ä¸Šçš„å…³é”®ä½œç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. å¥–åŠ±è®¾è®¡æ–¹é¢ï¼šåŠ å…¥æ ¼å¼å¥–åŠ±èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå°¤å…¶ä»åŸºç¡€LLMå¼€å§‹è®­ç»ƒæ—¶ï¼›ä¸­é—´æ£€ç´¢å¥–åŠ±æœªå¸¦æ¥æŒç»­æ€§èƒ½æå‡ï¼Œå®ç”¨ä»·å€¼æœ‰é™ã€‚
2. åŸºç¡€LLMå±‚é¢ï¼šé€šç”¨å‹LLMåœ¨RLåœºæ™¯ä¸­è¡¨ç°ä¼˜äºæ¨ç†ä¸“ç”¨å‹ï¼Œå¯èƒ½å› åè€…è®­ç»ƒåˆæœŸæŒ‡ä»¤éµå¾ªèƒ½åŠ›è¾ƒå¼±ï¼›æ‰©å¤§æ¨¡å‹è§„æ¨¡é€šå¸¸æå‡æœ€ç»ˆæ€§èƒ½ï¼Œä½†æ”¶ç›Šé€’å‡ã€‚
3. æœç´¢å¼•æ“é€‰æ‹©ç»´åº¦ï¼šè®­ç»ƒæ—¶æœç´¢å¼•æ“è´¨é‡å¼ºçƒˆå½±å“RLåŠ¨æ€ï¼Œç”¨æ— ä¿¡æ¯å¼•æ“ï¼ˆå¦‚éšæœºå™ªå£°ï¼‰ä¼šè®©æ™ºèƒ½ä½“å®Œå…¨å›é¿æ£€ç´¢ï¼Œå¼±å¼•æ“ï¼ˆå¦‚BM25ï¼‰å¯¼è‡´é¢‘ç¹ä½†ä½æ•ˆæœç´¢è°ƒç”¨ï¼Œå¼ºå¼•æ“ï¼ˆå¦‚å¯†é›†æ£€ç´¢å™¨ï¼‰å­¦ä¹ æ›´ç¨³å®šï¼›æ¨ç†æ—¶æœç´¢æ™ºèƒ½ä½“å¯¹ä¸åŒæ£€ç´¢ç³»ç»Ÿæ™®éé²æ£’ï¼Œæ›´å¼ºæœç´¢å¼•æ“æŒç»­å¸¦æ¥æ›´å¥½ä¸‹æ¸¸æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¥–åŠ±è®¾è®¡ï¼šåœ¨æ„å»ºRLé©±åŠ¨çš„LLMæœç´¢æ™ºèƒ½ä½“æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘æ ¼å¼å¥–åŠ±æ¥æå‡æ€§èƒ½ï¼Œå¯è°¨æ…è¯„ä¼°ä¸­é—´æ£€ç´¢å¥–åŠ±æŠ•å…¥ã€‚
2. æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®åº”ç”¨åœºæ™¯æƒè¡¡é€šç”¨å‹ä¸æ¨ç†ä¸“ç”¨å‹LLMï¼Œè‹¥è¿½æ±‚RLè®­ç»ƒæ•ˆæœåˆæœŸé€šç”¨å‹æ›´å…·ä¼˜åŠ¿ï¼ŒåŒæ—¶åˆç†è€ƒè™‘æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½æ”¶ç›Šå¹³è¡¡ã€‚
3. æœç´¢å¼•æ“é›†æˆï¼šè®­ç»ƒé˜¶æ®µé€‰æ‹©é«˜è´¨é‡æœç´¢å¼•æ“ä¿éšœå­¦ä¹ ç¨³å®šæ€§ï¼Œæ¨ç†é˜¶æ®µå¯åŸºäºåœºæ™¯çµæ´»æ›´æ¢æ£€ç´¢ç³»ç»Ÿï¼Œä¸”åˆ©ç”¨å¼ºæœç´¢å¼•æ“æå‡ä¸‹æ¸¸è¡¨ç°ï¼Œä¸ºå®é™…éƒ¨ç½²æœç´¢æ™ºèƒ½ä½“æä¾›äº†æ¸…æ™°çš„é€‰å‹ä¸ä¼˜åŒ–æ–¹å‘ã€‚ 
```

## computational-thinking-reasoning-in-large-language-models
### Abstract
While large language models (LLMs) have demonstrated remarkable reasoning
capabilities, they often struggle with complex tasks that require specific
thinking paradigms, such as divide-and-conquer and procedural deduction, \etc
Previous researches integrate external, reliable tools to alleviate logical
inconsistencies and hallucinations in LLMs' problem-solving processes. However,
we argue that the root challenge is more profound: LLMs lack the complex
thinking paradigms (\ie, computational thinking) during reasoning. In this
paper, we propose Computational Thinking Model (CTM), a novel framework that
incorporates computational thinking paradigms into LLMs. This framework enables
LLMs to reformulate complex problems through decomposition, abstraction,
reduction, and simulation, among other techniques. Specifically, live code
execution is seamlessly integrated into the reasoning process, allowing CTM to
think by computing. CTM directly instills computational thinking objectives
into LLMs through tailored reinforcement learning rewards, which encourages
problem simplification, modular planning, and iterative verification. We
conduct extensive evaluations on multiple code generation and mathematical
benchmarks. The results demonstrate that CTM outperforms conventional reasoning
models and tool-augmented baselines in terms of accuracy, interpretability, and
generalizability. We hope this study offers valuable insights for AI reasoning,
where LLMs can transform problems into robust, verifiable, and scalable
computational workflows, much like computer scientists do.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤§æ¨¡å‹æ‹¥æœ‰â€œè®¡ç®—æ€ç»´â€ï¼šCTM æ¡†æ¶é©æ–°å¤æ‚ä»»åŠ¡æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†é¢†åŸŸå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†é¢å¯¹ç®—æ³•æ±‚è§£ã€æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡æ—¶ï¼Œä»å­˜åœ¨ç»“æ„æ€§çŸ­æ¿â€”â€”æ¯”å¦‚åœ¨éœ€è¦åˆ†æ²»ã€è¿‡ç¨‹æ¼”ç»ç­‰ç‰¹å®šæ€ç»´èŒƒå¼çš„åœºæ™¯ä¸­ï¼Œå®¹æ˜“å‡ºç°é€»è¾‘ä¸ä¸€è‡´ã€å¹»è§‰ï¼ˆhallucinationï¼‰ï¼Œä¸”ä¸­é—´é”™è¯¯ä¼šä¸æ–­ä¼ æ’­ã€‚è¿‡å¾€ç ”ç©¶å°è¯•ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ã€ç½‘é¡µæœç´¢ï¼‰æ¥ç¼“è§£ï¼Œä½†æœ¬è´¨ä¸Šå¤§æ¨¡å‹ä»æŠŠæ¨ç†å½“ä½œâ€œé»‘ç®±å¼ç”Ÿæˆâ€ï¼Œå·¥å…·ä»…ä½œè¾…åŠ©è€Œéèå…¥æ ¸å¿ƒæ€ç»´èŒƒå¼ã€‚è€Œè®¡ç®—æœºç§‘å­¦å®¶è§£å†³å¤æ‚é—®é¢˜æ—¶ä¾èµ–çš„**è®¡ç®—æ€ç»´**ï¼ˆåˆ†è§£ã€çº¦ç®€ã€æŠ½è±¡ã€æ¨¡æ‹Ÿç­‰ï¼‰ï¼Œæ­£æ˜¯å½“å‰ LLMs ç¼ºå¤±çš„å…³é”®ã€‚æœ¬æ–‡åŠ¨æœºæ­£æ˜¯å¡«è¡¥è¿™ä¸€â€œæ€ç»´èŒƒå¼é¸¿æ²Ÿâ€ï¼Œè®©å¤§æ¨¡å‹åƒè®¡ç®—æœºç§‘å­¦å®¶ä¸€æ ·ç³»ç»Ÿè§£å†³å¤æ‚é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè®¡ç®—æ€ç»´æ¨¡å‹ï¼ˆCTMï¼‰æ¡†æ¶ï¼Œå°†è®¡ç®—æ€ç»´æ³¨å…¥å¤§æ¨¡å‹  
CTM ä¸æ˜¯ç®€å•â€œå¤–æŒ‚å·¥å…·â€ï¼Œè€Œæ˜¯è®©å¤§æ¨¡å‹æŠŠ**è®¡ç®—æ€ç»´**ä½œä¸ºæ ¸å¿ƒæ¨ç†ç­–ç•¥ã€‚é€šè¿‡åˆ†è§£ï¼ˆDecompositionï¼‰ã€æŠ½è±¡ï¼ˆAbstractionï¼‰ã€çº¦ç®€ï¼ˆReductionï¼‰ã€æ¨¡æ‹Ÿï¼ˆSimulationï¼‰ç­‰æŠ€æœ¯ï¼ŒæŠŠå¤æ‚é—®é¢˜è½¬åŒ–ä¸ºå¯æ‰§è¡Œã€å¯éªŒè¯çš„æµç¨‹ã€‚ä¾‹å¦‚ï¼Œè®©æ¨¡å‹ç”¨â€œä»£ç é©±åŠ¨â€çš„æ–¹å¼æ‹†è§£ä»»åŠ¡ï¼šåˆ†è§£æˆå­ç›®æ ‡ã€ç”¨ä»£ç æ¨¡æ‹Ÿæšä¸¾å¯èƒ½ã€é€šè¿‡æ‰§è¡Œä»£ç å›æº¯é”™è¯¯ã€è¿è¡Œæµ‹è¯•ç”¨ä¾‹éªŒè¯ç­‰ï¼ˆå¦‚å›¾1å±•ç¤ºçš„ä»£ç å¯å‘å¼å­ç›®æ ‡è®¾å®šã€æšä¸¾ã€å›æº¯ã€éªŒè¯ç­‰æ¨¡å¼ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèåˆâ€œå®æ—¶ä»£ç æ‰§è¡Œâ€çš„äº¤äº’å¼æ¨ç†ç¯å¢ƒ  
CTM æ­å»ºäº†ä¸€ä¸ªæ”¯æŒ**å®æ—¶ä»£ç æ‰§è¡Œ**çš„äº¤äº’ç¯å¢ƒï¼Œè®©å¤§æ¨¡å‹åœ¨æ¨ç†ä¸­â€œè¾¹ç®—è¾¹æƒ³â€ã€‚ä»£ç ä¸å†æ˜¯æœ€ç»ˆäº§ç‰©ï¼Œè€Œæ˜¯æ¨ç†è¿‡ç¨‹çš„â€œä¸­é—´æ­¥éª¤è½½ä½“â€â€”â€”ç”¨ä»£ç åˆ†è§£ä»»åŠ¡ã€ç”¨æ‰§è¡Œç»“æœéªŒè¯é€»è¾‘ã€ç”¨æŠ¥é”™ä¿¡æ¯å›æº¯ä¿®æ­£ã€‚è¿™ç§æ–¹å¼æŠŠæ¨¡ç³Šçš„è‡ªç„¶è¯­è¨€æ¨ç†ï¼Œè½¬åŒ–ä¸ºå¯æ‰§è¡Œã€å¯è¿½æº¯çš„è®¡ç®—æµç¨‹ï¼Œè§£å†³äº†ä¼ ç»Ÿå¤§æ¨¡å‹â€œéšå¼æ¨æµ‹â€æ˜“å‡ºé”™çš„é—®é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆSFT + RLï¼‰  
ä¸ºäº†è®©å¤§æ¨¡å‹çœŸæ­£å­¦ä¼šâ€œè®¡ç®—æ€ç»´â€ï¼ŒCTM è®¾è®¡äº†**æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰+ å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**çš„è®­ç»ƒèŒƒå¼ï¼š  
- SFT é˜¶æ®µï¼šç”¨æ„é€ çš„â€œç»“æ„åŒ–æ¨ç†æ•°æ®é›†â€ï¼Œè®©æ¨¡å‹å…ˆå­¦ä¹ åˆ†è§£ã€éªŒè¯ç­‰è®¡ç®—æ€ç»´çš„åŸºæœ¬æ–¹æ³•è®ºï¼›  
- RL é˜¶æ®µï¼šè®¾è®¡å®šåˆ¶åŒ–å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹è‡ªä¸»å­¦ä¹ â€œç®€åŒ–é—®é¢˜ã€æ¨¡å—åŒ–è§„åˆ’ã€è¿­ä»£éªŒè¯â€ç­‰ç›®æ ‡ï¼Œå¼ºåŒ–ç¬¦åˆè®¡ç®—æ€ç»´çš„æ¨ç†è¡Œä¸ºã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨**ä»£ç ç”Ÿæˆ**ï¼ˆå¦‚ LiveCodeBench ç­‰ï¼‰å’Œ**æ•°å­¦æ¨ç†**ï¼ˆå¦‚ AIME ç­‰ï¼‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå±•å¼€å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼š  
- **ç²¾åº¦ç¢¾å‹**ï¼šCTM æ˜¾è‘—è¶…è¶Šä¼ ç»Ÿæ¨ç†æ¨¡å‹ï¼ˆå¦‚ DeepSeek - R1 ç­‰ï¼‰å’Œå·¥å…·å¢å¼ºåŸºçº¿ï¼Œåœ¨éœ€è¦ç²¾ç¡®è®¡ç®—ã€ç»„åˆæ¨ç†çš„ä»»åŠ¡ä¸­é”™è¯¯ç‡å¤§å¹…é™ä½ï¼›  
- **å¯è§£é‡Šæ€§æå‡**ï¼šå› ä¸ºæ¨ç†è¿‡ç¨‹ç”±â€œä»£ç  + è‡ªç„¶è¯­è¨€â€åˆ†æ­¥æ‹†è§£ï¼Œæ¯ä¸€æ­¥æ„å›¾å’Œé€»è¾‘æ›´é€æ˜ï¼Œä¸åƒçº¯è‡ªç„¶è¯­è¨€æ¨ç†é‚£æ ·â€œé»‘ç®±â€ï¼›  
- **æ³›åŒ–æ€§æ›´å¼º**ï¼šåœ¨è·¨é¢†åŸŸã€å¤æ‚åœºæ™¯ä¸‹ï¼ŒCTM èƒ½æ›´ç¨³å¥åœ°æŠŠé—®é¢˜è½¬åŒ–ä¸ºå¯æ‰§è¡Œå·¥ä½œæµï¼Œé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›æ›´ä¼˜ã€‚  

è®ºæ–‡è¿˜å¯¹æ¯”äº†ä¼ ç»Ÿè‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹çš„å±€é™ï¼ˆå¦‚å›¾2ï¼Œä¼ ç»Ÿæ¨¡å‹åœ¨ä¸­é—´æ­¥éª¤ç¬¦å·è®¡ç®—æ˜“å‡ºé”™ã€è‡ªç¼–è¾‘èƒ½åŠ›æå¼±ï¼›è€Œ CTM é ä»£ç æ‰§è¡Œå®ç°â€œæ‰§è¡Œ - éªŒè¯ - ä¿®æ­£â€é—­ç¯ï¼‰ï¼Œè¿›ä¸€æ­¥å‡¸æ˜¾ CTM ä¼˜åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ€ç»´èŒƒå¼çš„â€œé¡¶å±‚è®¾è®¡â€**ï¼šä¸å†å±€é™äºâ€œå·¥å…·å †å â€ï¼Œè€Œæ˜¯ä»â€œäººç±»è§£å†³å¤æ‚é—®é¢˜çš„æ ¸å¿ƒæ€ç»´â€ï¼ˆè®¡ç®—æ€ç»´ï¼‰å‡ºå‘é‡æ„å¤§æ¨¡å‹æ¨ç†é€»è¾‘ï¼Œä¸ºå¤§æ¨¡å‹èƒ½åŠ›å‡çº§æä¾›äº†â€œè®¤çŸ¥æ¡†æ¶çº§â€çš„æ–°æ€è·¯ï¼›  
2. **ä»£ç ä¸æ¨ç†çš„æ·±åº¦èåˆ**ï¼šè¯æ˜äº†â€œä»£ç æ‰§è¡Œâ€ä¸åªæ˜¯ç”Ÿæˆä»£ç çš„æ‰‹æ®µï¼Œæ›´æ˜¯è®©å¤§æ¨¡å‹å®ç°â€œç²¾ç¡®æ¨ç†ã€å¯éªŒè¯æ¨ç†â€çš„å…³é”®è½½ä½“ï¼Œä¸ºä»£ç é©±åŠ¨å‹ AI æ¨ç†å¼€è¾Ÿæ–°æ–¹å‘ï¼›  
3. **è®­ç»ƒèŒƒå¼çš„åˆ›æ–°**ï¼šSFT + RL ç»“åˆâ€œå®šåˆ¶åŒ–å¥–åŠ±â€çš„ä¸¤é˜¶æ®µè®­ç»ƒï¼Œä¸ºå¤§æ¨¡å‹å­¦ä¹ â€œéè‡ªç„¶è¯­è¨€ç±»â€æ€ç»´ï¼ˆå¦‚è®¡ç®—æ€ç»´ï¼‰æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯è·¯çº¿ï¼›  
4. **è·¨é¢†åŸŸä»·å€¼**ï¼šCTM å¯¹æ•°å­¦ã€ä»£ç ç­‰â€œç²¾ç¡®æ¨ç†é¢†åŸŸâ€çš„çªç ´ï¼Œä¹Ÿä¸ºå…¶ä»–éœ€è¦ç»“æ„åŒ–ã€ç¬¦å·åŒ–æ¨ç†çš„åœºæ™¯ï¼ˆå¦‚ç§‘å­¦è®¡ç®—ã€é€»è¾‘éªŒè¯ï¼‰æä¾›äº†è¿ç§»å‚è€ƒã€‚  

ç®€è¨€ä¹‹ï¼ŒCTM ä¸ä»…æ˜¯ä¸€ä¸ªæ€§èƒ½æ›´ä¼˜çš„æ¨ç†æ¡†æ¶ï¼Œæ›´æ˜¯ä¸€æ¬¡â€œè®©å¤§æ¨¡å‹åƒäººç±»ä¸“å®¶ä¸€æ ·æ€è€ƒâ€çš„èŒƒå¼æ¢ç´¢â€”â€”æœªæ¥å¤§æ¨¡å‹æˆ–è®¸èƒ½çœŸæ­£ä»¥â€œè®¡ç®—æ€ç»´â€ä¸ºçŸ›ï¼Œæ”»ç ´æ›´å¤šå¤æ‚ä»»åŠ¡çš„å ¡å’ã€‚
```

## diversity-aware-policy-optimization-for-large-language-model-reasoning
### Abstract
The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­ï¼Œå¤šæ ·æ€§æ„ŸçŸ¥çš„ç­–ç•¥ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å‘å±•è¿…çŒ›ï¼Œå°¤å…¶æ˜¯DeepSeek R1å‘å¸ƒåï¼Œå¼•å‘äº†å¯¹æ•°æ®è´¨é‡å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•çš„å¤§é‡ç ”ç©¶ã€‚å°½ç®¡å¤šæ ·æ€§åœ¨RLä¸­è‡³å…³é‡è¦ï¼Œä½†å®ƒå¯¹LLMæ¨ç†çš„å½±å“å´å°šæœªå……åˆ†æ¢ç´¢ã€‚ä¼ ç»ŸRLä»»åŠ¡ä¸­ï¼Œå¤šæ ·æ€§æœ‰åŠ©äºä¿ƒè¿›æ¢ç´¢ã€é¿å…å±€éƒ¨æœ€ä¼˜ç­‰ï¼Œé‚£åœ¨LLMæ¨ç†çš„RLè®­ç»ƒä¸­ï¼Œæå‡å¤šæ ·æ€§æ˜¯å¦ä¹Ÿå¿…ä¸å¯å°‘ï¼Ÿä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡å±•å¼€äº†ç³»ç»Ÿæ€§ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡ç³»ç»Ÿæ¢ç©¶å¤šæ ·æ€§åœ¨LLMæ¨ç†ä¸­çš„ä½œç”¨  
æå‡ºæ–°æŒ‡æ ‡Potential@kæ¥é‡åŒ–LLMçš„æ¨ç†æ½œåŠ›ï¼ˆRLè®­ç»ƒåçš„æ€§èƒ½æå‡æ½œåŠ›ï¼‰ï¼Œå¯¹12ä¸ªä»£è¡¨æ€§LLMè¿›è¡Œå®è¯åˆ†æï¼Œå‘ç°é«˜æ€§èƒ½æ¨¡å‹ä¸­ï¼Œè§£å†³æ–¹æ¡ˆå¤šæ ·æ€§ä¸Potential@kå­˜åœ¨å¼ºæ­£ç›¸å…³ï¼Œå³å¤šæ ·æ€§å¯¹RLè®­ç»ƒåçš„æœ€ç»ˆæ€§èƒ½æå‡æœ‰ç›´æ¥è´¡çŒ®ï¼Œä¸ºå°†å¤šæ ·æ€§çº³å…¥ç­–ç•¥ä¼˜åŒ–æä¾›äº†å®è¯ä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºtoken - çº§å¤šæ ·æ€§ç›®æ ‡å¹¶ä¼˜åŒ–åº”ç”¨  
ç†µæ­£åˆ™åŒ–æ˜¯æå‡å¤šæ ·æ€§çš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†ç›´æ¥å¢åŠ LLMè¾“å‡ºå¹³å‡ç†µä¼šå¼•å…¥é•¿åº¦åå·®ï¼ˆé•¿å“åº”ç†µæ›´é«˜ï¼‰ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡token - çº§å¤šæ ·æ€§åº¦é‡å¹¶å°†å¤šæ ·æ€§ç›®æ ‡è½¬åŒ–ä¸ºå®ç”¨å½¢å¼ï¼›åŒæ—¶è€ƒè™‘è´¨é‡ - å¤šæ ·æ€§æƒè¡¡ï¼Œä»…å¯¹æ­£æ ·æœ¬æœ‰é€‰æ‹©åœ°å¢å¼ºå¤šæ ·æ€§ï¼Œæ—¢ä¸°å¯Œè§£å†³æ–¹æ¡ˆå¤šæ ·æ€§åˆä¿æŒè®­ç»ƒç¨³å®šæ€§ï¼Œè¯¥è®¾è®¡ç±»ä¼¼åŸºäºç¾¤ä½“çš„RLè®­ç»ƒä¸­åŸ¹è‚²é«˜è´¨é‡ç­–ç•¥çš„å¤šæ ·æ€§ï¼Œç¡®ä¿æ¢ç´¢ç”±ä»»åŠ¡ç›¸å…³æ€§èƒ½æ ‡å‡†å¼•å¯¼ã€‚ä¹‹åå°†è¯¥å¤šæ ·æ€§ç›®æ ‡æ•´åˆåˆ°R1 - zeroè®­ç»ƒæ¡†æ¶ä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ•´åˆæ–¹æ³•åçš„R1 - zeroè®­ç»ƒæ¡†æ¶ç›¸æ¯”æ ‡å‡†R1 - zeroè®­ç»ƒï¼Œå¹³å‡æ€§èƒ½æå‡3.5%ï¼ŒåŒæ—¶èƒ½ç”Ÿæˆæ›´å¤šæ ·ã€æ›´é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹å¤šæ ·æ€§åœ¨LLMæ¨ç†ä¸­ä½œç”¨çš„æ¢ç©¶æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œé€šè¿‡æå‡ºæ–°æŒ‡æ ‡å¹¶ç»“åˆå¤šæ¨¡å‹å®è¯åˆ†æï¼Œä¸ºé¢†åŸŸå†…ç›¸å…³ç ”ç©¶æä¾›äº†æ–°è§†è§’ï¼Œåç»­ç ”ç©¶å¯å‚è€ƒè¿™ç§å¯¹æœªå……åˆ†æ¢ç´¢å› ç´ çš„ç³»ç»Ÿåˆ†ææ–¹å¼ã€‚  
2. å¤„ç†å¤šæ ·æ€§ä¸é•¿åº¦åå·®ã€è´¨é‡ - å¤šæ ·æ€§æƒè¡¡ç­‰é—®é¢˜çš„æ–¹æ³•å…·æœ‰å¯å‘æ€§ï¼Œtoken - çº§å¤šæ ·æ€§è®¾è®¡ã€é’ˆå¯¹æ­£æ ·æœ¬åº”ç”¨å¤šæ ·æ€§å¢å¼ºç­‰ç­–ç•¥ï¼Œä¸ºåœ¨LLMçš„RLè®­ç»ƒä¸­å¹³è¡¡å„å› ç´ æä¾›äº†å®è·µè·¯å¾„ï¼Œå…¶ä»–å…³æ³¨LLMè®­ç»ƒä¸­å¤šæ ·æ€§ä¼˜åŒ–çš„å·¥ä½œå¯å‚è€ƒè¿™äº›æŠ€æœ¯æ‰‹æ®µã€‚  
3. å®éªŒè®¾è®¡ä¸Šï¼Œé€‰æ‹©å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ï¼ˆæ¯ä¸ªåŸºå‡†è‡³å°‘500ä¸ªé—®é¢˜ä¸”è¯„ä¼°æŒ‡æ ‡ç¨³å®šï¼‰è¿›è¡ŒéªŒè¯ï¼Œè¿™ç§ä¸¥è°¨çš„å®éªŒè®¾ç½®æ–¹å¼ä¹Ÿå€¼å¾—ç›¸å…³ç ”ç©¶å­¦ä¹ ï¼Œä»¥ç¡®ä¿æ–¹æ³•æ•ˆæœè¯„ä¼°çš„å¯é æ€§ã€‚
```

## the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models
### Abstract
Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§æ¨¡å‹æ¨ç†ä¸­å¹»è§‰å›°å¢ƒçš„ç ´å±€ï¼šé¢å‘äº‹å®æ€§çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–åœ¨æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½åœ¨æ•°å­¦ã€å¤šè·³é—®ç­”ç­‰å¤æ‚åŸºå‡†æµ‹è¯•ä¸­å±•ç°å¼ºå¤§èƒ½åŠ›ã€‚ä½†ç ”ç©¶å‘ç°ï¼Œ**é¢å‘æ¨ç†çš„RLå¾®è°ƒä¼šå¤§å¹…å¢åŠ å¹»è§‰ï¼ˆhallucinationsï¼Œå³ç”Ÿæˆäº‹å®é”™è¯¯æˆ–ç¼–é€ å†…å®¹ï¼‰çš„å‡ºç°**ã€‚æ¯”å¦‚æ¨¡å‹åœ¨é€æ­¥æ¨ç†æ—¶å¯èƒ½ä¸­é—´æ­¥éª¤å‡ºé”™ï¼Œå³ä¾¿æœ€ç»ˆç­”æ¡ˆå¶å°”æ­£ç¡®ï¼Œä¹Ÿä¼šäº§ç”Ÿä¸å¯é çš„è§£é‡Šã€‚  

ä¸ºä½•åŸºäºâ€œç»“æœå¯¼å‘â€çš„RLå¾®è°ƒä¼šåŠ å‰§å¹»è§‰ï¼Ÿè®ºæ–‡ä»ç†è®ºåˆ†æRLè®­ç»ƒåŠ¨æ€ï¼ŒæŒ‡å‡ºä¸‰å¤§å…³é”®å› ç´ ï¼š  
1. ä»…ä¼˜åŒ–æœ€ç»ˆæ­£ç¡®ç­”æ¡ˆä¼šå¯¼è‡´ç­–ç•¥æ¢¯åº¦æ–¹å·®æå¤§ï¼ˆå½“æ­£ç¡®ç­”æ¡ˆç¨€å°‘æ—¶è®­ç»ƒä¸ç¨³å®šï¼‰ï¼›  
2. ä¸ºæ¢ç´¢â€œæœ‰å¥–åŠ±çš„è¾“å‡ºâ€ï¼Œç­–ç•¥éœ€ä¿æŒé«˜é¢„æµ‹ç†µï¼Œå¢åŠ äº†å¹»è§‰é£é™©ï¼›  
3. æ ‡å‡†RLç›®æ ‡æ˜“é™·å…¥â€œè™šå‡å±€éƒ¨æœ€ä¼˜â€â€”â€”æ¨¡å‹å¯èƒ½æ”¶æ•›åˆ°è‡ªä¿¡ä½†é”™è¯¯ã€æ— å¥–åŠ±çš„ç­”æ¡ˆã€‚  
è¿™äº›å› ç´ è®©çº¯ç»“æœå¯¼å‘çš„RLæ–¹æ³•æ˜“å¼•å‘æ¨ç†æ¨¡å‹å¹»è§‰ï¼Œéš¾ä»¥å­¦ä¹ å¯é çš„æ¨ç†æ¨¡å¼ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­å…¼é¡¾æ¨ç†èƒ½åŠ›ä¸äº‹å®æ­£ç¡®æ€§ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œè®ºæ–‡æå‡º **Factuality-aware Step-wise Policy Optimizationï¼ˆFSPOï¼Œé¢å‘äº‹å®æ€§çš„åˆ†æ­¥ç­–ç•¥ä¼˜åŒ–ï¼‰**ï¼Œæ ¸å¿ƒæ˜¯åœ¨**æ¯ä¸€æ­¥æ¨ç†ä¸­æ˜¾å¼èå…¥äº‹å®æ€§éªŒè¯**ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å¯é çš„æ¨ç†é“¾ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ†æ­¥äº‹å®æ€§å¥–åŠ±æœºåˆ¶  
FSPOä¸å†åªçœ‹æœ€ç»ˆç»“æœï¼Œè€Œæ˜¯å¯¹**æ¨ç†è¿‡ç¨‹çš„æ¯ä¸ªæ­¥éª¤**åšäº‹å®æ€§éªŒè¯ã€‚å…·ä½“æ¥è¯´ï¼Œç”¨è‡ªåŠ¨åŒ–éªŒè¯å™¨ï¼ˆverifierï¼‰æ£€æŸ¥æ¯ä¸ªç”Ÿæˆçš„æ¨ç†è¯­å¥æ˜¯å¦èƒ½è¢«ç»™å®šè¯æ®æ”¯æŒï¼ˆå³æ˜¯å¦â€œç¬¦åˆäº‹å®â€ï¼‰ï¼Œå¾—åˆ°â€œåˆ†æ­¥äº‹å®æ€§åˆ†æ•°â€ã€‚è¿™äº›åˆ†æ•°è¢«æ•´åˆåˆ°æ•´ä½“å¥–åŠ±ä¿¡å·ä¸­ï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªtokenå±‚é¢çš„ä¼˜åŠ¿å€¼ï¼ˆadvantage valuesï¼‰â€”â€”å¥–åŠ±äº‹å®æ­£ç¡®çš„tokenï¼Œæƒ©ç½šé”™è¯¯tokenã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£å†³RLè®­ç»ƒçš„ç¨€ç–æ€§ä¸ä¸ç¨³å®šæ€§  
ä¼ ç»ŸRLä¾èµ–â€œæœ€ç»ˆç»“æœæ˜¯å¦æ­£ç¡®â€çš„ç¨€ç–å¥–åŠ±ï¼Œè€ŒFSPOé€šè¿‡**æ¯ä¸€æ­¥çš„äº‹å®æ€§åé¦ˆ**æä¾›æ›´å¯†é›†ã€ä¿¡æ¯æ›´ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·ã€‚è¿™ä¸ä»…ç¼“è§£äº†â€œç¨€ç–å¥–åŠ±å¯¼è‡´è®­ç»ƒä¸ç¨³å®šâ€çš„é—®é¢˜ï¼Œè¿˜èƒ½å¼•å¯¼ç­–ç•¥å­¦ä¹ â€œæ—¢æ­£ç¡®ã€æ¨ç†é“¾åˆå¯éªŒè¯â€çš„è§£ï¼Œä»è®­ç»ƒè¿‡ç¨‹ç›´æ¥å‡å°‘å¹»è§‰ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨æ•°å­¦æ¨ç†å’Œå¹»è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºQwen2.5å’ŒLlamaç³»åˆ—æ¨¡å‹å±•å¼€å®éªŒï¼ŒéªŒè¯FSPOçš„æ•ˆæœï¼š  
- **å¹»è§‰å‡å°‘**ï¼šåœ¨TruthfulQAã€HaluEvalã€HalluQAç­‰å¹»è§‰åŸºå‡†ä¸Šï¼Œç»FSPOå¾®è°ƒçš„æ¨¡å‹å¹»è§‰ç‡æ˜¾è‘—é™ä½ï¼›  
- **æ¨ç†ç²¾åº¦æå‡**ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ˆå¦‚å¤æ‚æ•°å­¦é¢˜æ±‚è§£ï¼‰ï¼Œæ¨¡å‹ä¸ä»…æ›´â€œè¯šå®â€ï¼Œæ¨ç†æ­£ç¡®ç‡ä¹ŸåŒæ­¥æé«˜ï¼›  
- **ä¸­é—´æ­¥éª¤äº‹å®æ€§å¢å¼º**ï¼šå¯¹æ¨ç†ä¸­é—´æ­¥éª¤çš„åˆ†ææ˜¾ç¤ºï¼ŒFSPOèƒ½åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæå‡æ¯ä¸€æ­¥æ¨ç†çš„äº‹å®æ­£ç¡®æ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ¨ç†ä¸äº‹å®æ€§çš„å¹³è¡¡æ€è·¯**ï¼šä»¥å¾€RLå¾®è°ƒä¾§é‡â€œç»“æœæ­£ç¡®â€ï¼ŒFSPOåˆ™è¯æ˜â€œè¿‡ç¨‹ç›‘ç£ï¼ˆåˆ†æ­¥äº‹å®æ€§éªŒè¯ï¼‰â€èƒ½æœ‰æ•ˆå¼¥è¡¥ç»“æœç›‘ç£çš„ç¼ºé™·ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸å¯é æ€§çš„ååŒä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ï¼›  
2. **å¥–åŠ±æœºåˆ¶çš„ç²¾ç»†åŒ–è®¾è®¡**ï¼šé€šè¿‡â€œåˆ†æ­¥+äº‹å®æ€§â€çš„å¥–åŠ±å¡‘é€ ï¼Œè§£å†³RLç¨€ç–å¥–åŠ±ä¸è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œè¿™ç§â€œæŠŠå¤§ç›®æ ‡æ‹†åˆ†ä¸ºå¯éªŒè¯çš„å°æ­¥éª¤å¹¶åé¦ˆâ€çš„æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è¦è¿‡ç¨‹å¯é æ€§çš„ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€å¤šè½®å¯¹è¯ï¼‰ï¼›  
3. **ç†è®ºä¸å®è·µç»“åˆ**ï¼šå…ˆä»ç†è®ºåˆ†æRLè®­ç»ƒåŠ¨æ€ä¸­å¯¼è‡´å¹»è§‰çš„æ ¹å› ï¼Œå†é’ˆå¯¹æ€§è®¾è®¡ç®—æ³•ï¼Œè¿™ç§â€œé—®é¢˜è¯Šæ–­â†’æ–¹æ³•è®¾è®¡â†’å®éªŒéªŒè¯â€çš„ç ”ç©¶èŒƒå¼ï¼Œå¯¹è§£å†³å¤§æ¨¡å‹å…¶ä»–é²æ£’æ€§é—®é¢˜ï¼ˆå¦‚åè§ã€é€»è¾‘é”™è¯¯ï¼‰å…·æœ‰å‚è€ƒä»·å€¼ã€‚  

æ€»ä¹‹ï¼ŒFSPOä¸ºå¤§æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­â€œæ›´å‡†ä¸”æ›´è¯šå®â€æä¾›äº†ä¸€å¥—å¯è¡Œæ–¹æ¡ˆï¼Œä¹Ÿå¯å‘æˆ‘ä»¬æ€è€ƒï¼šå¼ºåŒ–å­¦ä¹ ä¸ä»…è¦ä¼˜åŒ–â€œèƒ½åŠ›â€ï¼Œæ›´è¦ä¼˜åŒ–â€œå¯é æ€§â€ï¼Œæ‰èƒ½è®©å¤§æ¨¡å‹çœŸæ­£åœ¨çœŸå®åœºæ™¯ä¸­å¯ä¿¡å¯ç”¨ã€‚
```

## rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation
### Abstract
Retrieval-augmented generation (RAG) has shown great promise for
knowledge-intensive tasks and recently advanced with agentic RAG, where
language agents engage in multi-round interactions with external knowledge
sources for adaptive information retrieval. However, existing agentic RAG
methods often depend on ad-hoc prompt engineering and lack a unified
optimization framework. We introduce RAG-Gym, a comprehensive platform that
systematically explores three optimization dimensions: (1) prompt engineering,
(2) actor tuning, and (3) critic training. For prompt engineering, we propose
Re$^2$Search, a novel agent incorporating reasoning reflection that
significantly outperforms standard prompts. In actor tuning, we evaluate three
popular post-training algorithms with fine-grained process supervision and
identify direct preference optimization as the most effective. We further
demonstrate that a trained critic can enhance inference by selecting
higher-quality intermediate reasoning steps. Together, these findings lead to
the optimized Re$^2$Search++ agent, which surpasses most recent methods like
Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we
examine the impact of different reward sources and analyze scaling properties
in training and inference, offering practical insights for agentic RAG
optimization. The project homepage is available at https://rag-gym.github.io.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RAG-Gymï¼šä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆæ‰“é€ è¯­è¨€æ™ºèƒ½ä½“çš„ç³»ç»Ÿä¼˜åŒ–å¹³å°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹çŸ¥è¯†å¯†é›†å‹é—®é¢˜æ—¶ï¼Œè‹¥ç¼ºä¹è¶³å¤Ÿæˆ–æœ€æ–°çš„é¢†åŸŸçŸ¥è¯†ï¼Œå®¹æ˜“ç»™å‡ºä¸å‡†ç¡®å›ç­”ç”šè‡³äº§ç”Ÿå¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ç»“åˆä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ç³»ç»Ÿçš„ç›¸å…³ä¿¡æ¯æ¥æ”¹å–„è¿™ä¸€æƒ…å†µï¼Œè€Œæ™ºèƒ½ä½“åŒ–çš„RAGï¼ˆagentic RAGï¼‰è®©è¯­è¨€æ™ºèƒ½ä½“ä¸å¤–éƒ¨çŸ¥è¯†æºå¤šè½®äº¤äº’ä»¥å®ç°è‡ªé€‚åº”ä¿¡æ¯æ£€ç´¢ï¼Œè¿›ä¸€æ­¥æå‡äº†æ•ˆæœã€‚ä½†ç°æœ‰agentic RAGæ–¹æ³•å­˜åœ¨ä¾èµ–ä¸´æ—¶promptå·¥ç¨‹ã€ç¼ºä¹ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶çš„é—®é¢˜ï¼›åŒæ—¶ï¼Œè™½æœ‰LLMåè®­ç»ƒç®—æ³•ï¼Œå´éš¾ç›´æ¥é€‚é…agentic RAGåŠ¨æ€è°ƒæ•´tokenç”Ÿæˆç­–ç•¥çš„éœ€æ±‚ï¼Œä¸”å¯¹ä¸­é—´æ­¥éª¤çš„ç»†ç²’åº¦ç›‘ç£ä¸è¶³ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ„å»ºç³»ç»Ÿæ¡†æ¶æ¥ä¼˜åŒ–agentic RAGã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRAG-Gymç»¼åˆå¹³å°  
RAG-Gymä»promptå·¥ç¨‹ã€actorè°ƒä¼˜ã€criticè®­ç»ƒä¸‰ä¸ªç»´åº¦ç³»ç»Ÿæ¢ç´¢agentic RAGçš„ä¼˜åŒ–ã€‚å°†çŸ¥è¯†å¯†é›†å‹é—®ç­”å»ºæ¨¡ä¸ºé«˜å±‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œæ˜ç¡®ä¸­é—´åŠ¨ä½œï¼Œä¸ºè¯­è¨€æ™ºèƒ½ä½“ä¼˜åŒ–æä¾›æ¨¡å—åŒ–æ–¹æ³•ï¼Œæ¶µç›–äº†æ™ºèƒ½ä½“ä¸IRç³»ç»Ÿäº¤äº’çš„çŠ¶æ€ã€åŠ¨ä½œã€ç¯å¢ƒå’Œå¥–åŠ±ç­‰è¦ç´ å®šä¹‰ï¼Œæ”¯æ’‘ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£ä¸ä¼˜åŒ–æ–¹æ³•çš„ç³»ç»Ÿè¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šPromptå·¥ç¨‹å±‚é¢æå‡ºReÂ²Searchæ™ºèƒ½ä½“  
è®¾è®¡äº†èå…¥æ¨ç†åæ€ï¼ˆreasoning reflectionï¼‰çš„ReÂ²Searchæ™ºèƒ½ä½“ï¼Œç›¸æ¯”æ ‡å‡†promptåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—è¶…è¶Šï¼Œè®©æ™ºèƒ½ä½“åœ¨ä¸IRç³»ç»Ÿäº¤äº’è¿‡ç¨‹ä¸­èƒ½æ›´æ™ºèƒ½åœ°ç”ŸæˆæŸ¥è¯¢ç­‰åŠ¨ä½œæ¥è¾…åŠ©å›ç­”é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šActorè°ƒä¼˜ä¸Criticè®­ç»ƒçš„æ¢ç´¢  
åœ¨actorè°ƒä¼˜ä¸­ï¼Œç”¨ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£è¯„ä¼°ä¸‰ç§æµè¡Œåè®­ç»ƒç®—æ³•ï¼Œå‘ç°ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰æ•ˆæœæœ€ä½³ï¼›åŒæ—¶è¯æ˜è®­ç»ƒcriticæ¥é€‰æ‹©æ›´é«˜è´¨é‡ä¸­é—´æ¨ç†æ­¥éª¤èƒ½æå‡æ¨ç†æ•ˆæœã€‚æ•´åˆè¿™äº›æˆæœå¾—åˆ°ä¼˜åŒ–åçš„ReÂ²Search++æ™ºèƒ½ä½“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä¼˜åŒ–åçš„ReÂ²Search++æ™ºèƒ½ä½“åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹³å‡F1ç›¸å¯¹æå‡3.2% - 11.6%ï¼Œåœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šç”šè‡³æœ‰8.5% - 24.7%çš„æå‡ï¼›æ­¤å¤–è¿˜åˆ†æäº†ä¸åŒå¥–åŠ±æºå½±å“ä»¥åŠè®­ç»ƒå’Œæ¨ç†æ—¶çš„ç¼©æ”¾ç‰¹æ€§ç­‰ï¼ŒéªŒè¯äº†å„ä¼˜åŒ–ç»´åº¦å’Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šç»´åº¦ç³»ç»Ÿä¼˜åŒ–æ€è·¯ï¼šä»promptã€actorã€criticä¸‰ä¸ªç»´åº¦ç³»ç»Ÿä¼˜åŒ–agentic RAGï¼Œä¸ºå¤æ‚æ™ºèƒ½ä½“ç³»ç»Ÿä¼˜åŒ–æä¾›äº†â€œåˆ†æ¨¡å—+ååŒâ€çš„èŒƒä¾‹ï¼Œå¯å¯å‘å…¶ä»–éœ€å¤šç¯èŠ‚åä½œçš„AIç³»ç»Ÿä¼˜åŒ–ã€‚  
2. ç»†ç²’åº¦ç›‘ç£ä»·å€¼ï¼šå¼ºè°ƒå¯¹agentic RAGä¸­é—´æ­¥éª¤çš„ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£ï¼Œè¯æ˜å…¶å¯¹æ€§èƒ½æå‡çš„é‡è¦æ€§ï¼Œåœ¨åç»­ç±»ä¼¼éœ€åŠ¨æ€äº¤äº’ã€å¤šæ­¥æ¨ç†çš„ä»»åŠ¡å‹æ™ºèƒ½ä½“ç ”å‘ä¸­ï¼Œå¯é‡è§†ä¸­é—´è¿‡ç¨‹çš„ç›‘ç£è®¾è®¡ã€‚  
3. ç®—æ³•é€‰å‹ä¸èåˆï¼šå¯¹ä¸åŒåè®­ç»ƒç®—æ³•åœ¨agentic RAGåœºæ™¯ä¸‹çš„æµ‹è¯•ä¸ä¼˜é€‰ï¼ˆå¦‚å‘ç°ç›´æ¥åå¥½ä¼˜åŒ–æ›´ä¼˜ï¼‰ï¼Œä»¥åŠcriticè®­ç»ƒè¾…åŠ©æ¨ç†çš„æ€è·¯ï¼Œä¸ºæ¨¡å‹è®­ç»ƒç­–ç•¥é€‰æ‹©å’Œæ¨¡å—é…åˆæä¾›äº†å®è·µå‚è€ƒã€‚ 
4. å®è·µæŒ‡å¯¼æ„ä¹‰ï¼šå¯¹å¥–åŠ±æºã€è®­ç»ƒä¸æ¨ç†ç¼©æ”¾ç‰¹æ€§çš„åˆ†æï¼Œç»™åç»­agentic RAGä¼˜åŒ–æä¾›äº†å¯è½åœ°çš„å®ç”¨æ´è§ï¼Œä¾¿äºç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆåœ¨å®é™…é¡¹ç›®ä¸­æƒè¡¡å†³ç­–ã€‚
```

## from-passive-to-active-reasoning--can-large-language-models-ask-the-right-questions-under-incomplete-information-
### Abstract
While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»è¢«åŠ¨åˆ°ä¸»åŠ¨æ¨ç†ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯ä¸å…¨æ—¶èƒ½å¦é—®å¯¹é—®é¢˜ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰åŸºå‡†ä¸»è¦è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¢«åŠ¨æ¨ç†èƒ½åŠ›ï¼Œå³ç»™æ¨¡å‹æä¾›è§£å†³é—®é¢˜æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯è®©å…¶æ¨å¯¼ç­”æ¡ˆã€‚ä½†ç°å®ä¸­å¾ˆå¤šåœºæ™¯æ˜¯ä¿¡æ¯ä¸å®Œæ•´çš„ï¼Œéœ€è¦æ¨¡å‹ä¸»åŠ¨ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’è·å–ç¼ºå¤±ä¿¡æ¯ï¼ˆä¸»åŠ¨æ¨ç†ï¼‰ï¼Œè€Œè¿™æ–¹é¢ç¼ºä¹ç³»ç»Ÿç ”ç©¶ã€‚ä¸ºå¡«è¡¥æ­¤ç©ºç™½ï¼Œè®ºæ–‡æå‡ºAR - BenchåŸºå‡†æ¥è¯„ä¼°LLMsçš„ä¸»åŠ¨æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºAR - BenchåŸºå‡†
AR - BenchåŒ…å«ä¾¦æ¢æ¡ˆä¾‹ï¼ˆdetective casesï¼‰ã€æƒ…å¢ƒè°œé¢˜ï¼ˆsituation puzzlesï¼‰ã€çŒœæ•°å­—ï¼ˆguessing numbersï¼‰ä¸‰ä¸ªä»»åŠ¡å®¶æ—ï¼Œåˆ†åˆ«å¯¹åº”å¸¸è¯†æ¨ç†ã€é€»è¾‘æ¨ç†ã€ç¬¦å·æ¨ç†ï¼Œæ¨¡æ‹Ÿç°å®ä¸­æ™ºèƒ½ä½“åœºæ™¯ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¿¡æ¯ä¸å…¨æ—¶ä¸»åŠ¨è·å–ä¿¡æ¯å¹¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèšç„¦ä¸»åŠ¨æ¨ç†è¯„ä¼°ç»´åº¦
é’ˆå¯¹AR - Benchä¸­çš„ä»»åŠ¡ï¼Œä»â€œæå‡ºçš„é—®é¢˜è´¨é‡â€å’Œâ€œæœ€ç»ˆè§£å†³æ–¹æ¡ˆè´¨é‡â€ä¸¤ä¸ªç»´åº¦é‡åŒ–è¯„ä¼°æ¨¡å‹çš„ä¸»åŠ¨æ¨ç†èƒ½åŠ›ï¼Œè¦æ±‚æ¨¡å‹åœ¨ä¸å¤–éƒ¨â€œå›ç­”è€…â€å¤šè½®äº¤äº’ä¸­ä¸»åŠ¨è·å–å…³é”®çº¿ç´¢æ¥è§£é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- æ€§èƒ½å·®è·æ˜æ˜¾ï¼šå½“å‰æœ€å…ˆè¿›çš„LLMsï¼ˆå¦‚GPT - 4oï¼‰åœ¨AR - Benchä¸Šè¡¨ç°ä¸ä½³ï¼ŒçŒœæ•°å­—ä»»åŠ¡ç²¾ç¡®åŒ¹é…ç‡ä½è‡³35%ï¼Œè€Œäººç±»è¯„ä¼°è€…è¡¨ç°è¿œè¶…æ¨¡å‹ã€‚
- äº¤äº’è½®æ¬¡æ”¶ç›Šå˜åŒ–ï¼šæ¨¡å‹åœ¨äº¤äº’å‰å‡ è½®è¿›æ­¥å¿«ï¼ˆ5 - 10è½®è¿‡ç¨‹åˆ†æ•°+7.7%ï¼‰ï¼Œåç»­è½®æ¬¡ï¼ˆ20 - 25è½®ï¼‰è¿›æ­¥å‡ç¼“ï¼ˆ+2.5%ï¼‰ï¼Œéšæé—®è½®æ¬¡å¢åŠ æ”¶ç›Šé€’å‡ã€‚
- ç»„ä»¶ç¼ºé™·ï¼šä¸å¯é çš„éªŒè¯å™¨å’Œä½è´¨é‡é—®é¢˜ç”Ÿæˆä¸¥é‡é™åˆ¶åŸºäºæœç´¢çš„ç­–ç•¥ï¼Œä¸”éªŒè¯å™¨æ•ˆæœå› ä»»åŠ¡è€Œå¼‚ã€‚
- ç¼©æ”¾é™åˆ¶ï¼šæ›´å¤§æ¨¡å‹å’Œæ›´å¤šäº¤äº’è½®æ¬¡æ¯”å°æ¨¡å‹æœ‰å¯æµ‹é‡æ”¹è¿›ï¼Œä½†ä»æ— æ³•å®Œå…¨è§£å†³ä¸»åŠ¨æ¨ç†ä»»åŠ¡ã€‚
- æ–¹æ³•ä¸æŒ‡ä»¤å¤±æ•ˆï¼šåƒSFTã€DPOã€Tree - of - Thoughtã€äººå·¥ç¼–å†™æŒ‡ä»¤ç­‰å¸¸è§æ–¹æ³•æ”¶æ•ˆç”šå¾®ã€‚
- ä»»åŠ¡ç‰¹å®šé”™è¯¯æ¨¡å¼ï¼šæ¨¡å‹å¸¸é—®æ¨¡ç³Šæˆ–é‡å¤é—®é¢˜ï¼Œä¸åŒä»»åŠ¡æœ‰ç‰¹å®šé”™è¯¯ï¼Œå¦‚ä¾¦æ¢æ¡ˆä¾‹ä¸­æ—¶é—´çº¿è¯¯è§£ã€æƒ…å¢ƒè°œé¢˜ä¸­æ— æ ¹æ®å‡è®¾ã€çŒœæ•°å­—ä¸­åé¦ˆè¯¯è§£ç­‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æä¾›æ–°åŸºå‡†ï¼šAR - Benchä¸ºç ”ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸»åŠ¨æ¨ç†èƒ½åŠ›æä¾›äº†å…¨æ–°ä¸”ç³»ç»Ÿçš„åŸºå‡†ï¼Œæ¨åŠ¨é¢†åŸŸä»ä¼ ç»Ÿè¢«åŠ¨æ¨ç†è¯„ä¼°å‘ä¸»åŠ¨æ¨ç†è¯„ä¼°æ‹“å±•ï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜æ–°æ–¹å‘ã€‚
- æ­ç¤ºèƒ½åŠ›çŸ­æ¿ï¼šæ¸…æ™°å±•ç°å½“å‰LLMsåœ¨ä¸»åŠ¨æ¨ç†ä¸Šçš„ä¸è¶³ï¼Œè®©ç ”ç©¶è€…æ˜ç¡®éœ€æ”¹è¿›æ–¹å‘ï¼Œå¦‚åç»­å¯å›´ç»•äº¤äº’å¼å­¦ä¹ ã€å®æ—¶åé¦ˆå¾ªç¯ã€æ„ŸçŸ¥ç¯å¢ƒçš„è®­ç»ƒç›®æ ‡ç­‰æ–¹é¢æå‡ä¸»åŠ¨æ¨ç†æ–¹æ³•ã€‚
- å…¬å¼€èµ„æºåŠ©åŠ›ï¼šAR - Benchå¼€æºï¼ˆhttps://github.com/tmlr - group/AR - Benchï¼‰ï¼Œæ–¹ä¾¿ç ”ç©¶è€…åŸºäºæ­¤åŸºå‡†å¼€å±•å®éªŒï¼Œå…±åŒæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›è¾¹ç•Œçªç ´ã€‚
```

## speed-rl--faster-training-of-reasoning-models-via-online-curriculum-learning
### Abstract
Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SPEED-RLï¼šåœ¨çº¿è¯¾ç¨‹å­¦ä¹ åŠ é€Ÿæ¨ç†æ¨¡å‹è®­ç»ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¯éªŒè¯å¥–åŠ±èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œä½†å‡åŒ€æç¤ºé‡‡æ ·æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨æ•°æ®é€‰æ‹©ä¸Šä¾èµ–å¤§é‡äººå·¥æ“ä½œï¼Œå¯æ‰©å±•æ€§å’Œçµæ´»æ€§å—é™ï¼›è¯¾ç¨‹å­¦ä¹ è™½åœ¨éƒ¨åˆ†é¢†åŸŸæœ‰æ•ˆï¼Œä½†åœ¨LLMæ¨ç†ä»»åŠ¡çš„RLè®­ç»ƒä¸­æ•ˆæœä¸æ˜ã€‚å› æ­¤ï¼ŒäºŸéœ€æ›´é«˜æ•ˆçš„ç­–ç•¥æ¥åŠ é€ŸåŸºäºRLçš„æ¨ç†æ¨¡å‹è®­ç»ƒï¼Œé™ä½è®¡ç®—å¼€é”€åŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç†è®ºå±‚é¢æ­ç¤ºä¸­é—´éš¾åº¦æç¤ºä»·å€¼  
è®ºæ–‡ä¸¥è°¨è®ºè¯äº†åœ¨RLæ¡†æ¶ä¸‹ï¼Œ**ä¸­é—´éš¾åº¦çš„æç¤º**èƒ½æä¾›æœ€å¼ºå­¦ä¹ ä¿¡å·ã€‚é€šè¿‡åˆ†ææç¤ºé€šè¿‡ç‡ï¼ˆæ¨¡å‹æ­£ç¡®è§£å†³é—®é¢˜çš„æ¦‚ç‡ï¼‰ä¸éšæœºæ¢¯åº¦ä¼°è®¡å™¨ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰çš„å…³ç³»ï¼Œå‘ç°é€šè¿‡ç‡æ¥è¿‘0%æˆ–100%çš„æç¤ºä¼šè®©æ¢¯åº¦ä¼°è®¡å™¨SNRä¸¥é‡ä¸‹é™ï¼Œæ¢¯åº¦å—å™ªå£°ä¸»å¯¼ï¼Œé‡‡æ ·æ„ä¹‰ä¸å¤§ï¼›è€Œä¸­é—´éš¾åº¦æç¤ºå¯æå‡SNRï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚ä¸”è¯¥ç»“è®ºé€‚ç”¨äºå¤šç§ä¸»æµç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼ˆå¦‚REINFORCEã€GRPOã€PPOç­‰ï¼‰ï¼Œæ³›åŒ–æ€§å¼ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆåœ¨çº¿è¯¾ç¨‹å­¦ä¹ æ¡†æ¶SPEED  
å—ç†è®ºå¯å‘ï¼Œæå‡º**Selective Prompting with Efficient Estimation of Difficultyï¼ˆSPEEDï¼‰** æ¡†æ¶ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä¼˜éš¾åº¦æç¤ºç”¨äºRLè®­ç»ƒã€‚å…·ä½“å®ç°ä¸Šï¼Œå…ˆé€šè¿‡è½»é‡ç»Ÿè®¡æµ‹è¯•å¿«é€Ÿä¼°è®¡æç¤ºéš¾åº¦ï¼Œå†å¯¹åˆé€‚éš¾åº¦çš„æç¤ºæ‰§è¡Œå…¨æ¨ç†ï¼Œå‡å°‘æ¨ç†é˜¶æ®µï¼ˆRLè®­ç»ƒä¸­è®¡ç®—æœ€å¯†é›†çš„éƒ¨åˆ†ï¼‰çš„å¼€é”€ã€‚è¯¥æ¡†æ¶æ— éœ€æ‰‹åŠ¨æ•°æ®é¢„å¤„ç†æˆ–ç‰¹æ®Šè¾…åŠ©ç¥ç»ç»„ä»¶ï¼Œèƒ½æ— ç¼é›†æˆåˆ°ä¸»æµRLç®—æ³•ä¸­ï¼Œé€‚é…å¤šç§å¸¦äºŒå…ƒå¯éªŒè¯å¥–åŠ±çš„ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ ‡å‡†æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºQwen2.5-Math-7Bæ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼šSPEEDä½¿RLè®­ç»ƒé€Ÿåº¦æå‡2åˆ°6å€ï¼Œä¸”ä¸é™ä½ç²¾åº¦ï¼›é€šè¿‡æ’é™¤è¿‡æ˜“æˆ–è¿‡éš¾æç¤ºï¼Œèšç„¦ä¸­ç­‰éš¾åº¦ï¼ˆé«˜SNRï¼‰æç¤ºï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨ç†æˆæœ¬ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨åŠ é€Ÿè®­ç»ƒä¸ä¿æŒæ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºæŒ‡å¯¼å®è·µ**ï¼šå°†æç¤ºéš¾åº¦ä¸æ¢¯åº¦ä¼°è®¡å™¨SNRå…³è”ï¼Œä¸ºé«˜æ•ˆé‡‡æ ·æä¾›ç†è®ºä¾æ®ï¼Œè¿™ç§ä»ç†è®ºåˆ°æ–¹æ³•çš„æ¨å¯¼æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œå¸®åŠ©åœ¨ç®—æ³•è®¾è®¡æ—¶é”šå®šæ ¸å¿ƒæ€§èƒ½å½±å“å› å­ã€‚  
2. **è½»é‡é«˜æ•ˆè®¾è®¡**ï¼šSPEEDé€šè¿‡è½»é‡ç»Ÿè®¡æµ‹è¯•å’Œä¼˜åŒ–é¢„å–æœºåˆ¶é™ä½æ¨ç†å¼€é”€ï¼Œæ— éœ€å¤æ‚é¢å¤–ç»„ä»¶ï¼Œåœ¨å·¥ç¨‹å®ç°ä¸Šä¸ºé«˜æ•ˆè®­ç»ƒæ¡†æ¶è®¾è®¡æä¾›äº†â€œä»¥ç®€é©­ç¹â€çš„æ€è·¯ã€‚  
3. **é€šç”¨æ€§ä¸æ‰©å±•æ€§**ï¼šé€‚é…å¤šç§RLç®—æ³•ä¸ä»»åŠ¡ç±»å‹ï¼Œä½“ç°äº†æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ™®é€‚æ€§ï¼Œä¸ºåç»­è·¨ä»»åŠ¡ã€è·¨ç®—æ³•çš„è®­ç»ƒåŠ é€Ÿç ”ç©¶æä¾›äº†å‚è€ƒèŒƒå¼ã€‚  
```

## treerpo--tree-relative-policy-optimization
### Abstract
Large Language Models (LLMs) have shown remarkable reasoning capabilities
through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,
a key limitation of existing approaches is that rewards defined at the full
trajectory level provide insufficient guidance for optimizing the intermediate
steps of a reasoning process. To address this, we introduce \textbf{\name}, a
novel method that estimates the mathematical expectations of rewards at various
reasoning steps using tree sampling. Unlike prior methods that rely on a
separate step reward model, \name directly estimates these rewards through this
sampling process. Building on the group-relative reward training mechanism of
GRPO, \name innovatively computes rewards based on step-level groups generated
during tree sampling. This advancement allows \name to produce fine-grained and
dense reward signals, significantly enhancing the learning process and overall
performance of LLMs. Experimental results demonstrate that our \name algorithm
substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test
benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name
significantly outperforms GRPO by 2.9\% in performance while simultaneously
reducing the average response length by 18.1\%, showcasing its effectiveness
and efficiency. Our code will be available at
\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | TreeRPOï¼šç”¨æ ‘é‡‡æ ·é‡å¡‘å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–çš„å¥–åŠ±ä¿¡å·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å…³é”®å±€é™ï¼š**è½¨è¿¹çº§å¥–åŠ±å¯¹æ¨ç†ä¸­é—´æ­¥éª¤çš„ä¼˜åŒ–æŒ‡å¯¼ä¸è¶³**ã€‚åœ¨åŸºäºå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ä¸­ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è™½èƒ½æä¾›ç»†ç²’åº¦å¥–åŠ±ï¼Œä½†é«˜è´¨é‡æ ‡æ³¨æ•°æ®è·å–éš¾ï¼›è€Œæ— å¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ä»…æä¾›è½¨è¿¹çº§å¥–åŠ±ï¼Œå¯¹ä¸­é—´æ­¥éª¤ä¼˜åŒ–åŠ©åŠ›æœ‰é™ã€‚å¦‚ä½•åœ¨ä¸ä¾èµ–å¥–åŠ±æ¨¡å‹çš„å‰æä¸‹ï¼Œä¸ºæ¨ç†è¿‡ç¨‹æä¾›å¯†é›†ã€ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ‘é‡‡æ ·å®ç°æ— å¥–åŠ±æ¨¡å‹çš„è¿‡ç¨‹å¥–åŠ±ä¼°è®¡  
TreeRPO æå‡º**æ ‘é‡‡æ ·æœºåˆ¶**ï¼Œæ— éœ€æ˜¾å¼çš„æ­¥éª¤å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥é€šè¿‡æ ‘é‡‡æ ·æ¥ä¼°è®¡å„æ¨ç†æ­¥éª¤å¥–åŠ±çš„æ•°å­¦æœŸæœ›ã€‚å®ƒçªç ´äº†ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å•ç‹¬æ­¥éª¤å¥–åŠ±æ¨¡å‹çš„é™åˆ¶ï¼Œç”¨é‡‡æ ·è¿‡ç¨‹æœ¬èº«å®Œæˆå¥–åŠ±ä¼°è®¡ï¼Œä¸ºä¸­é—´æ¨ç†æ­¥éª¤æä¾›æ›´ç›´æ¥çš„åé¦ˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºGRPOæ¡†æ¶çš„æ­¥éª¤çº§ç»„ç›¸å¯¹å¥–åŠ±è®¡ç®—  
åœ¨GRPOçš„ç»„ç›¸å¯¹å¥–åŠ±è®­ç»ƒæœºåˆ¶åŸºç¡€ä¸Šï¼ŒTreeRPOåˆ›æ–°æ€§åœ°**åŸºäºæ ‘é‡‡æ ·ç”Ÿæˆçš„æ­¥éª¤çº§åˆ†ç»„æ¥è®¡ç®—å¥–åŠ±**ã€‚è¿™ä¸€è®¾è®¡è®©æ¨¡å‹èƒ½äº§ç”Ÿç»†ç²’åº¦ã€å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œåœ¨ä¿ç•™å¯éªŒè¯å¥–åŠ±å‡½æ•°æ‰©å±•æ€§ä¼˜åŠ¿çš„åŒæ—¶ï¼Œæ›´é«˜æ•ˆåœ°å¼•å¯¼æ¨ç†è¿‡ç¨‹ä¼˜åŒ–ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ¨ç†å‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼šåœ¨Qwen - 2.5 - Mathçš„æµ‹è¯•åŸºå‡†ä¸­ï¼ŒPass@1å‡†ç¡®ç‡ä»19.0%å¤§å¹…æå‡è‡³35.5%ï¼›  
2. æ€§èƒ½ä¸æ•ˆç‡åŒä¼˜ï¼šå¯¹æ¯”GRPOï¼ŒTreeRPOæ€§èƒ½é¢†å…ˆ2.9%ï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘18.1%ï¼Œè¯æ˜åœ¨æå‡æ¨ç†ç²¾åº¦çš„åŒæ—¶ï¼Œè¿˜èƒ½æ›´é«˜æ•ˆåœ°åˆ©ç”¨tokenèµ„æºï¼›  
3. å¯è§†åŒ–å¯¹æ¯”ï¼ˆå›¾1ï¼‰ï¼šåœ¨MATH - 500ã€OlympiadBenchç­‰å››ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTreeRPOçš„Pass@1å‡†ç¡®ç‡éšè®­ç»ƒæ­¥éª¤æ¨è¿›ï¼Œå§‹ç»ˆä¼˜äºGRPOï¼Œå±•ç°æŒç»­ä¼˜åŒ–èƒ½åŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ— å¥–åŠ±æ¨¡å‹çš„ç»†ç²’åº¦å¥–åŠ±è®¾è®¡æ€è·¯ï¼šTreeRPOè¯æ˜äº†åœ¨ä¸ä¾èµ–æ˜‚è´µæ ‡æ³¨æ•°æ®æ„å»ºå¥–åŠ±æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡é‡‡æ ·ä¸åˆ†ç»„æœºåˆ¶ä¹Ÿèƒ½ç”Ÿæˆè¿‡ç¨‹çº§å¥–åŠ±ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹çš„RLä¸LLMç»“åˆæä¾›æ–°è·¯å¾„ï¼›  
2. å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä¼˜åŒ–çš„åˆ›æ–°åº”ç”¨ï¼šå°†æ ‘ç»“æ„é‡‡æ ·ä¸ç»„ç›¸å¯¹å¥–åŠ±ç»“åˆï¼Œä¸ºå¤æ‚æ¨ç†ä»»åŠ¡ä¸­â€œä¸­é—´æ­¥éª¤ä¼˜åŒ–â€è¿™ä¸€éš¾é¢˜æä¾›äº†å¯è½åœ°çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œå¯å‘åç»­åœ¨RLä¸LLMæ¨ç†ç»“åˆæ–¹å‘çš„ç ”ç©¶ï¼›  
3. æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ï¼šå®éªŒä¸­åŒæ—¶å®ç°å‡†ç¡®ç‡æå‡ä¸å“åº”é•¿åº¦ç¼©çŸ­ï¼Œä½“ç°äº†æ–¹æ³•åœ¨å®é™…éƒ¨ç½²ä¸­çš„ä»·å€¼ï¼Œä¸ºè¿½æ±‚â€œé«˜æ•ˆæ¨ç†â€çš„å·¥ä¸šçº§åº”ç”¨æä¾›å‚è€ƒã€‚  
```

## effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability
### Abstract
Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£å†³RAGä¸¤å¤§ç—›ç‚¹ï¼ARENAç”¨å¼ºåŒ–å­¦ä¹ è®©ç”Ÿæˆæ›´æœ‰æ•ˆã€æ›´é€æ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Retrieval - Augmented Generationï¼ˆRAGï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡è¡¨ç°ä¸Šæ•ˆæœæ˜¾è‘—ï¼Œä½†ä»å­˜åœ¨ä¸¤å¤§æœªè§£å†³çš„æŒ‘æˆ˜ï¼š
1. **æœ‰æ•ˆæ€§ä¸è¶³**ï¼šç°æœ‰ç ”ç©¶å¤šèšç„¦äºæ‰“é€ æ›´å¼ºçš„RAGæ£€ç´¢å™¨ï¼Œå´å¿½ç•¥äº†å¦‚ä½•æå‡ç”Ÿæˆå™¨ï¼ˆLLMï¼‰åˆ©ç”¨æ£€ç´¢ä¿¡æ¯è¿›è¡Œæ¨ç†å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚å®é™…æµ‹è¯•å‘ç°ï¼Œåœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸åŒæ£€ç´¢ä¸Šä¸‹æ–‡ä¸‹7Bè§„æ¨¡çš„æ¨¡å‹ï¼Œå›ç­”å‡†ç¡®ç‡æ¯”OpenAI - o1ã€DeepSeek - R1ç­‰æ¨ç†æ¨¡å‹ä½15 - 35%ï¼Œè¿™è¡¨æ˜ç”Ÿæˆå™¨çš„æ¨ç†èƒ½åŠ›æ˜¯å½“å‰RAG pipelineçš„å…³é”®ç“¶é¢ˆã€‚
2. **é€æ˜åº¦ç¼ºå¤±**ï¼šå¤šæ•°RAGæ–¹æ³•æœªå…³æ³¨å“ªäº›æ£€ç´¢å†…å®¹çœŸæ­£å¯¹æ¨ç†è¿‡ç¨‹æœ‰è´¡çŒ®ï¼Œå¯¼è‡´å¯è§£é‡Šæ€§å’Œå¯è§æ€§ä¸è¶³ã€‚å¾ˆå¤šç”Ÿæˆå™¨è¾“å‡ºæ— ç»“æ„ç­”æ¡ˆï¼Œéšè—äº†å†³ç­–è¿‡ç¨‹ï¼Œé™ä½äº†å¯ä¿¡åº¦ï¼Œåœ¨å¤„ç†å¤šæ–‡æ¡£å¤šè·³æ¨ç†æ—¶ï¼Œå°æ¨¡å‹å› æ¨ç†èƒ½åŠ›æœ‰é™æ›´éš¾åº”å¯¹å™ªå£°æˆ–å†—ä½™ä¸Šä¸‹æ–‡ã€‚
åŒæ—¶ï¼Œç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œå¥–åŠ±è®¾è®¡é€šç”¨ã€è¾“å‡ºæ ¼å¼æœªè€ƒè™‘å¤šè·³QAç»“æ„ï¼Œä¸”KLæ­£åˆ™åŒ–ä¸ç¨³å®šæ˜“å¯¼è‡´è®­ç»ƒå‘æ•£ï¼Œé™åˆ¶äº†åœ¨æ£€ç´¢ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºARENAæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºARENAæ¡†æ¶ï¼Œå®ç°é€æ˜ä¸”æœ‰æ•ˆçš„RAGç”Ÿæˆ
ARENAï¼ˆAdaptive - Rewarded Evidence Navigation Agentï¼‰æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„é€æ˜RAGç”Ÿæˆå™¨æ¡†æ¶ã€‚å®ƒå¼•å…¥ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒ…å«é€‰å®šçš„å‚è€ƒèµ„æ–™ã€æ˜ç¡®çš„æ¨ç†è½¨è¿¹å’Œæœ€ç»ˆç­”æ¡ˆï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–ç”Ÿæˆï¼Œæ¨¡å‹èƒ½è¯†åˆ«å…³é”®è¯æ®ã€è¿›è¡Œç»“æ„åŒ–æ¨ç†å¹¶ç”Ÿæˆå¸¦æœ‰å¯è§£é‡Šå†³ç­–è½¨è¿¹çš„ç­”æ¡ˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è‡ªé€‚åº”ä»»åŠ¡ç‰¹å®šå¥–åŠ±ä¸ç¨³å®šä¼˜åŒ–ç­–ç•¥
ä¸ºå¤šè·³é—®ç­”ä»»åŠ¡å®šåˆ¶äº†ä¸€å¥—è‡ªé€‚åº”çš„ã€ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±æœºåˆ¶ï¼Œä»æ ¼å¼ã€å‡†ç¡®æ€§ã€ç›¸å…³æ€§å’Œé¢å¤–å¥–åŠ±ç­‰å¤šä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹è¾“å‡ºï¼Œæä¾›å¯è§£é‡Šä¸”ç»†ç²’åº¦çš„è®­ç»ƒä¿¡å·ã€‚åŒæ—¶æ”¹è¿›KLå…¬å¼å®ç°KLç¨³å®šåŒ–ï¼Œè§£å†³äº†ç°æœ‰RLæ–¹æ³•ä¸­KLæ­£åˆ™åŒ–ä¸ç¨³å®šå¯¼è‡´è®­ç»ƒå‘æ•£çš„é—®é¢˜ï¼Œè®©è®­ç»ƒæ›´ç¨³å®šé€‚ç”¨äºæ£€ç´¢ç±»ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å°†ARENAåº”ç”¨äºQwen2.5 - 7B - Instructå’ŒLlama3.1 - 8B - Instructç­‰å¼€æºæ¨¡å‹ï¼Œåœ¨å¤šä¸ªRAGåŸºçº¿çš„å¤§é‡å®éªŒè¡¨æ˜ï¼š
1. åœ¨æ‰€æœ‰å¤šè·³QAæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹å‡†ç¡®ç‡æå‡äº†10 - 30%ï¼Œæ€§èƒ½å¯ä¸OpenAI - o1ã€DeepSeek - R1ç­‰å•†ä¸šå¼€å‘çš„SOTAå¤§è¯­è¨€æ¨¡å‹åª²ç¾ã€‚
2. è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼ŒARENAåœ¨æ–°æ•°æ®é›†ä¸Šæ— éœ€é¢å¤–è®­ç»ƒå°±èƒ½å¾ˆå¥½åœ°é€‚é…ï¼Œå…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§ï¼Œåœ¨æ•°æ®é›†å’Œæ¨¡å‹ backbone ä¸Šæ³›åŒ–æ€§è‰¯å¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å…³æ³¨ç”Ÿæˆå™¨ä¼˜åŒ–**ï¼šè®ºæ–‡æŒ‡å‡ºå½“å‰RAGç³»ç»Ÿä¸­ç”Ÿæˆå™¨æ¨ç†èƒ½åŠ›ä¸è¶³æ˜¯å…³é”®é—®é¢˜ï¼Œæé†’ç ”ç©¶è€…ä»¬é™¤äº†æ£€ç´¢å™¨ï¼Œè¦é‡è§†ç”Ÿæˆå™¨çš„ä¼˜åŒ–ï¼Œä¸ºRAGç ”ç©¶æä¾›äº†æ–°çš„å…³æ³¨æ–¹å‘ã€‚
2. **å¼ºåŒ–å­¦ä¹ åœ¨RAGçš„åº”ç”¨æ¨¡å¼**ï¼šå±•ç¤ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆç»“æ„åŒ–ç”Ÿæˆã€è‡ªé€‚åº”å¥–åŠ±è®¾è®¡å’Œç¨³å®šè®­ç»ƒæ¥æå‡RAGæ¨ç†çš„é€æ˜æ€§å’Œæœ‰æ•ˆæ€§çš„æ¨¡å¼ï¼Œä¸ºåç»­åˆ©ç”¨RLä¼˜åŒ–RAGç”Ÿæˆå™¨æä¾›äº†å‚è€ƒæ¡†æ¶ã€‚
3. **å¼€æºèµ„æºè´¡çŒ®**ï¼šå…¬å¼€äº†æ¨¡å‹å’Œä»£ç ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šå¯¹RAGç”Ÿæˆå™¨ä¼˜åŒ–è¿›è¡Œç ”ç©¶ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸå‘å±•ã€‚
```

## sws--self-aware-weakness-driven-problem-synthesis-in-reinforcement-learning-for-llm-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SwSï¼šå¤§æ¨¡å‹æ¨ç†å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒåŸºäºè‡ªæˆ‘æ„ŸçŸ¥å¼±ç‚¹é©±åŠ¨çš„é—®é¢˜åˆæˆæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è§£é¢˜ï¼‰è®­ç»ƒä¸­ï¼Œå¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å·²è¢«è¯æ˜æœ‰æ•ˆã€‚ä½†RLVRè§„æ¨¡åŒ–åº”ç”¨éœ€è¦é«˜è´¨é‡ã€ç­”æ¡ˆç²¾ç¡®å¯éªŒè¯çš„é—®é¢˜é›†ã€‚ç°æœ‰é—®é¢˜å­˜åœ¨è¯¸å¤šä¸è¶³ï¼šä¸€æ˜¯ç²¾å¿ƒè®¾è®¡çš„äººå·¥æ ‡æ³¨æ•°å­¦é¢˜ç¨€ç¼ºï¼Œæ„å»ºå¤§è§„æ¨¡å¸¦ç²¾ç¡®å‚è€ƒç­”æ¡ˆçš„æ•°æ®é›†æˆæœ¬é«˜ï¼›äºŒæ˜¯å¤šæ•°é¢å‘è’¸é¦çš„åˆæˆæ•°æ®é›†å‚è€ƒç­”æ¡ˆéªŒè¯ä¸è¶³ï¼Œä¸é€‚åˆä¾èµ–ç­”æ¡ˆæ­£ç¡®æ€§ä½œä¸ºè®­ç»ƒä¿¡å·çš„RLVRï¼›ä¸‰æ˜¯ç°æœ‰é—®é¢˜åˆæˆç­–ç•¥ä¸è€ƒè™‘æ¨¡å‹èƒ½åŠ›ï¼Œç›²ç›®æ‰©å……é—®é¢˜é›†ï¼Œç”Ÿæˆæœ‰ç”¨é—®é¢˜æ•ˆç‡ä½ã€‚ä¸”åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œè®­ç»ƒä»»åŠ¡éš¾åº¦éœ€ä¸æ¨¡å‹å½“å‰èƒ½åŠ›åŒ¹é…ï¼Œå¦åˆ™ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ï¼Œè€Œæ¨¡å‹æŒç»­å¤±è´¥çš„æ¡ˆä¾‹èƒ½åæ˜ å¼±ç‚¹ï¼Œå¯ç”¨äºé’ˆå¯¹æ€§æ”¹è¿›ï¼Œå› æ­¤éœ€è¦ä¸€ç§åˆ©ç”¨å¤±è´¥æ¡ˆä¾‹æ¥åˆæˆé—®é¢˜çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSwSæ¡†æ¶
æå‡ºSelf - aware Weakness - driven problem Synthesisï¼ˆSwSï¼‰æ¡†æ¶ï¼Œç³»ç»Ÿè¯†åˆ«æ¨¡å‹ç¼ºé™·å¹¶ç”¨äºé—®é¢˜æ‰©å……ã€‚åœ¨RLè®­ç»ƒåˆæ­¥é˜¶æ®µï¼Œè®°å½•æ¨¡å‹é€šè¿‡è¿­ä»£é‡‡æ ·æŒç»­éš¾ä»¥è§£å†³æˆ–å­¦ä¹ æ•ˆç‡ä½çš„é—®é¢˜ï¼ˆå³å¼±ç‚¹é—®é¢˜ï¼‰ï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„è¿™äº›å¤±è´¥æ¡ˆä¾‹ï¼Œæå–æ ¸å¿ƒæ¦‚å¿µï¼Œåˆæˆé€‚åˆæ¨¡å‹èƒ½åŠ›éš¾åº¦çš„æ–°é—®é¢˜ï¼Œåç»­ç”¨è¿™äº›æ–°é—®é¢˜å¢å¼ºè®­ç»ƒï¼Œè®©æ¨¡å‹èšç„¦å¹¶å…‹æœå¼±ç‚¹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºæ€§èƒ½çš„èµ„æºåˆ†é…
ä¸ºæå‡è®­ç»ƒä¸­å¼±ç‚¹ç¼“è§£æ•ˆç‡ï¼Œæ ¹æ®æ¨¡å‹åœ¨ä¸åŒç±»åˆ«ä¸Šçš„ç›¸å¯¹æ€§èƒ½åˆ†é…æ¯ä¸ªç±»åˆ«ï¼ˆå¯¹åº”ä¸åŒå¼±ç‚¹é¢†åŸŸï¼‰çš„æ‰©å……é¢„ç®—ï¼Œä½¿é—®é¢˜åˆæˆæ›´æœ‰é’ˆå¯¹æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨3Båˆ°32Bä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šï¼Œåœ¨8ä¸ªä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°ã€‚7Bæ¨¡å‹å¹³å‡æ€§èƒ½æå‡10.0%ï¼Œ32Bæ¨¡å‹å¹³å‡æå‡7.7%ï¼›è®­ç»ƒåï¼Œæ¨¡å‹åœ¨åŸæœ¬å¼±é¢†åŸŸæŒç»­å¤±è´¥çš„é—®é¢˜ä¸Šï¼Œèƒ½å¤šè§£å†³é«˜è¾¾20.0%çš„é—®é¢˜ï¼›åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºæ‰©å……é—®é¢˜é›†è®­ç»ƒçš„æ¨¡å‹æŒç»­è¶…è¿‡åŸºç¡€æ¨¡å‹å’ŒåŸå§‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡ç²¾å¿ƒç­–åˆ’çš„äººå·¥æ ‡æ³¨é—®é¢˜é›†è®­ç»ƒçš„å¯¹åº”æ¨¡å‹ã€‚æ­¤å¤–è¿˜æ¢ç´¢äº†SwSåœ¨Weak - to - Strong Generalizationã€Self - evolvingã€Weakness - driven Selectionç­‰è®¾ç½®ä¸‹çš„æ½œåŠ›ï¼ŒéªŒè¯äº†æ¡†æ¶çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šå°†æ¨¡å‹è‡ªèº«çš„å¤±è´¥æ¡ˆä¾‹è½¬åŒ–ä¸ºæ”¹è¿›èµ„æºï¼Œé€šè¿‡è¯†åˆ«å¼±ç‚¹æ¥ç”Ÿæˆé’ˆå¯¹æ€§è®­ç»ƒæ•°æ®ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®ç”Ÿæˆæä¾›äº†æ–°çš„è‡ªé©±åŠ¨æ€è·¯ï¼Œä¸ä¾èµ–å¤–éƒ¨çŸ¥è¯†è’¸é¦ï¼Œè®©æ¨¡å‹è‡ªæˆ‘è¯†åˆ«å’Œè§£å†³RLä¸­çš„å¼±ç‚¹ä»¥å®ç°é²æ£’æ³›åŒ–ã€‚
2. æ–¹æ³•è½åœ°ï¼šæå‡ºçš„SwSæ¡†æ¶ä»å¼±ç‚¹è¯†åˆ«ã€æ¦‚å¿µæå–åˆ°é—®é¢˜åˆæˆã€èµ„æºåˆ†é…éƒ½æœ‰æ¸…æ™°æµç¨‹ï¼Œå¯å€Ÿé‰´è¿™ç§ç³»ç»Ÿåˆ©ç”¨æ¨¡å‹è‡ªèº«è¡¨ç°æ¥ä¼˜åŒ–è®­ç»ƒæ•°æ®çš„æ–¹å¼ï¼Œåº”ç”¨åˆ°å…¶ä»–éœ€è¦æ•°æ®é©±åŠ¨ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡ä¸­ã€‚
3. å®éªŒå…¨é¢ï¼šåœ¨å¤šè§„æ¨¡æ¨¡å‹å’Œå¤šåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯ï¼Œè¿˜æ¢ç´¢å¤šç§æ‰©å±•è®¾ç½®ï¼Œè¿™ç§å…¨é¢éªŒè¯æ–¹æ³•å’Œæ¢ç´¢æ‹“å±•æ€§çš„æ€è·¯ï¼Œå¯ä¸ºç›¸å…³ç ”ç©¶çš„å®éªŒè®¾è®¡æä¾›å‚è€ƒï¼Œä»¥æ›´å……åˆ†éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚
```

## r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning
### Abstract
Large language models (LLMs) have notably progressed in multi-step and
long-chain reasoning. However, extending their reasoning capabilities to
encompass deep interactions with search remains a non-trivial challenge, as
models often fail to identify optimal reasoning-search interaction
trajectories, resulting in suboptimal responses. We propose R-Search, a novel
reinforcement learning framework for Reasoning-Search integration, designed to
enable LLMs to autonomously execute multi-step reasoning with deep search
interaction, and learn optimal reasoning search interaction trajectories via
multi-reward signals, improving response quality in complex logic- and
knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when
to retrieve or reason, while globally integrating key evidence to enhance deep
knowledge interaction between reasoning and search. During RL training,
R-Search provides multi-stage, multi-type rewards to jointly optimize the
reasoning-search trajectory. Experiments on seven datasets show that R-Search
outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%
(out-of-domain). The code and data are available at
https://github.com/QingFei1/R-Search.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | R-Searchï¼šç”¨å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ èµ‹èƒ½LLMæ¨ç†ä¸æœç´¢æ·±åº¦äº¤äº’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥éª¤å’Œé•¿é“¾æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¦è®©å…¶æ¨ç†èƒ½åŠ›ä¸æœç´¢æ·±åº¦äº¤äº’ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹å¸¸éš¾ä»¥æ‰¾åˆ°æœ€ä¼˜çš„â€œæ¨ç† - æœç´¢â€äº¤äº’è½¨è¿¹ï¼Œå¯¼è‡´è¾“å‡ºè´¨é‡æ¬ ä½³ã€‚åœ¨å¤æ‚çš„é€»è¾‘å¯†é›†å‹å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚å¤šè·³é—®ç­”ï¼‰ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯æ¨¡å‹å†…éƒ¨å†³å®šçš„æ£€ç´¢æ—¶æœºæœªå¿…å¥‘åˆå®é™…éœ€æ±‚ï¼›äºŒæ˜¯æ¨ç†ä¸æœç´¢çš„æ¨¡å—åŒ–è®¾è®¡é™åˆ¶äº†å¤–éƒ¨çŸ¥è¯†ä¸æ¨ç†é“¾çš„æ·±åº¦äº¤äº’ï¼Œæ˜“è®©æ¨¡å‹åŸºäºå±€éƒ¨ä¿¡æ¯åšå†³ç­–ï¼Œæœ€ç»ˆå½±å“è¾“å‡ºè´¨é‡ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLMåŠ¨æ€æ•´åˆå¤–éƒ¨çŸ¥è¯†ã€å­¦ä¹ æœ€ä¼˜äº¤äº’è½¨è¿¹æˆä¸ºå…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šR - Searchæ¡†æ¶è®¾è®¡â€”â€”å¼ºåŒ–å­¦ä¹ é©±åŠ¨æ¨ç†ä¸æœç´¢æ·±åº¦æ•´åˆ  
æå‡ºR - Searchè¿™ä¸€åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°é¢–æ¡†æ¶ï¼Œè®©LLMèƒ½åŠ¨æ€äº¤é”™æ‰§è¡Œå¤šæ­¥éª¤æ¨ç†ä¸æœç´¢ã€‚LLMå¯åœ¨ä»»æ„tokençº§æ¨ç†æ­¥éª¤è§¦å‘æ£€ç´¢ï¼Œå°†æ£€ç´¢å†…å®¹æ— ç¼èå…¥æ¨ç†è¿‡ç¨‹ï¼Œå®ç°æ¨ç†ä¸å¤–éƒ¨çŸ¥è¯†çš„æ·±åº¦è€¦åˆï¼›äº¤äº’åï¼ŒLLMè¿˜èƒ½é€šè¿‡æ¨ç†æŠŠæ£€ç´¢æ–‡æ¡£æç‚¼ä¸ºè¯æ®ï¼Œä»å…¨å±€è§†è§’é‡æ–°è¯„ä¼°å’Œæ„å»ºå…³é”®çŸ¥è¯†ï¼Œèšç„¦è§£å†³ä»»åŠ¡çš„æ ¸å¿ƒäº‹å®ã€‚è¯¥æ¡†æ¶èƒ½å¼•å¯¼LLMä¿éšœä¸­é—´æ¨ç†çš„åˆç†æ€§ä¸æ£€ç´¢çŸ¥è¯†çš„å®Œæ•´æ€§ï¼Œè”åˆä¼˜åŒ–RAGä¸­å¤æ‚çš„â€œæ¨ç† - æœç´¢â€è½¨è¿¹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé˜¶æ®µå¤šç±»å‹å¥–åŠ±æœºåˆ¶â€”â€”å¼•å¯¼å­¦ä¹ æœ€ä¼˜äº¤äº’åºåˆ—  
è®¾è®¡äº†åŒ…å«ç­”æ¡ˆè´¨é‡ã€è¯æ®è´¨é‡ã€æ ¼å¼æ­£ç¡®æ€§ç­‰çš„å¤šé˜¶æ®µã€å¤šç±»å‹å¥–åŠ±æœºåˆ¶ã€‚è¿™äº›äº’è¡¥çš„å¥–åŠ±ä¿¡å·æ¨åŠ¨æ¨¡å‹å­¦ä¹ æœ€ä¼˜â€œæ¨ç† - æœç´¢â€äº¤äº’åºåˆ—ï¼Œå…¶ä¸­è¯æ®å¥–åŠ±èƒ½ä¿ƒä½¿æ¨¡å‹å…³æ³¨å…³é”®ä¸­é—´æ¨ç†æ­¥éª¤çš„äº‹å®è´¨é‡ï¼ŒåŠ©åŠ›æ„å»ºæ›´ç¨³å¥çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘èµ°æ·å¾„æˆ–è‡†æµ‹è¡Œä¸ºçš„é£é™©ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šR - Search - as - a - Toolï¼ˆRSToolï¼‰â€”â€”æ¨¡å—åŒ–ä¸å®ç”¨æ‹“å±•  
æå‡ºRSToolï¼Œå°†æ¨ç†ä¸­é«˜è´¨é‡è¯æ®æ¨¡å—åŒ–å°è£…ä¸ºå¯è¿ç§»ç»„ä»¶ï¼ŒæŠŠå¤æ‚ä¸”è€—æ—¶çš„â€œæ¨ç† - æœç´¢â€äº¤äº’å¸è½½åˆ°æœ¬åœ°éƒ¨ç½²ï¼Œå…·å¤‡å¾ˆå¼ºçš„å®é™…å¯æ‰©å±•æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨7ä¸ªæ¶µç›–å¤šè·³å’Œå•è·³é—®ç­”ä»»åŠ¡çš„æ•°æ®é›†ä¸Šå¼€å±•å®éªŒï¼Œç»“æœæ˜¾ç¤ºR - Searchåœ¨åŸŸå†…ï¼ˆin - domainï¼‰æ¯”å…ˆè¿›RAGåŸºçº¿æœ€å¤šé¢†å…ˆ32.2%ï¼Œåœ¨åŸŸå¤–ï¼ˆout - of - domainï¼‰æœ€å¤šé¢†å…ˆ25.1%ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒå’Œè®­ç»ƒåŠ¨æ€åˆ†æç­‰è¿›ä¸€æ­¥éªŒè¯äº†è¯æ®æ•´åˆä¸å¤šå¥–åŠ±å»ºæ¨¡çš„æœ‰æ•ˆæ€§ï¼Œè¿˜æ­ç¤ºäº†ä¸åŒRLç®—æ³•ä¸‹çš„æ€§èƒ½è¶‹åŠ¿ä¸æ£€ç´¢è¡Œä¸ºç­‰æ´å¯Ÿã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥RAGåœºæ™¯æ¥ä¼˜åŒ–â€œæ¨ç† - æœç´¢â€è½¨è¿¹ï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡ä¸­æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†æ·±åº¦äº¤äº’é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯ç¤ºåœ¨éœ€åŠ¨æ€äº¤äº’çš„ä»»åŠ¡ä¸­å¯æ¢ç´¢RLé©±åŠ¨çš„æ¡†æ¶è®¾è®¡ã€‚  
2. å¥–åŠ±æœºåˆ¶æ„å»ºï¼šå¤šç»´åº¦ã€å¤šé˜¶æ®µçš„å¥–åŠ±è®¾è®¡æ€è·¯ï¼Œèƒ½ä¸ºå¼ºåŒ–å­¦ä¹ ä¸­å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ æä¾›æ›´å…¨é¢çš„ä¿¡å·å‚è€ƒï¼Œå¯å€Ÿé‰´åˆ°éœ€å¤šç›®æ ‡ä¼˜åŒ–çš„ä»»åŠ¡åœºæ™¯ã€‚  
3. å·¥å…·åŒ–è½åœ°ï¼šRSToolçš„æ¨¡å—åŒ–æ€è·¯ï¼Œä¸ºå¤æ‚AIèƒ½åŠ›å‘å®ç”¨å·¥å…·è½¬åŒ–ã€é™ä½éƒ¨ç½²æˆæœ¬æä¾›äº†å‚è€ƒï¼Œåˆ©äºæ¨åŠ¨æŠ€æœ¯åœ¨å®é™…åœºæ™¯çš„è½åœ°åº”ç”¨ã€‚  
```

## pangu-deepdiver--adaptive-search-intensity-scaling-via-open-web-reinforcement-learning
### Abstract
Information seeking demands iterative evidence gathering and reflective
reasoning, yet large language models (LLMs) still struggle with it in open-web
question answering. Existing methods rely on static prompting rules or training
with Wikipedia-based corpora and retrieval environments, limiting adaptability
to the real-world web environment where ambiguity, conflicting evidence, and
noise are prevalent. These constrained training settings hinder LLMs from
learning to dynamically decide when and where to search, and how to adjust
search depth and frequency based on informational demands. We define this
missing capacity as Search Intensity Scaling (SIS)--the emergent skill to
intensify search efforts under ambiguous or conflicting conditions, rather than
settling on overconfident, under-verification answers.
  To study SIS, we introduce WebPuzzle, the first dataset designed to foster
information-seeking behavior in open-world internet environments. WebPuzzle
consists of 24K training instances and 275 test questions spanning both
wiki-based and open-web queries. Building on this dataset, we propose
DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by
encouraging adaptive search policies through exploration under a real-world
open-web environment. Experimental results show that Pangu-7B-Reasoner
empowered by DeepDiver achieve performance on real-web tasks comparable to the
671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from
cold-start supervised fine-tuning to a carefully designed RL phase, and present
that its capability of SIS generalizes from closed-form QA to open-ended tasks
such as long-form writing. Our contributions advance adaptive information
seeking in LLMs and provide a valuable benchmark and dataset for future
research.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦è§£æPangu DeepDiverï¼šåŸºäºå¼€æ”¾ç½‘ç»œå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æœç´¢å¼ºåº¦ç¼©æ”¾

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼€æ”¾ç½‘ç»œé—®ç­”ä¸­è¿›è¡Œä¿¡æ¯æœç´¢æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¿¡æ¯æœç´¢éœ€è¦è¿­ä»£æ”¶é›†è¯æ®ä¸åæ€æ¨ç†ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–é™æ€æç¤ºè§„åˆ™ï¼Œæˆ–åŸºäºç»´åŸºç™¾ç§‘è¯­æ–™å’Œæ£€ç´¢ç¯å¢ƒè®­ç»ƒï¼Œå¯¹å……æ»¡æ¨¡ç³Šæ€§ã€å†²çªè¯æ®å’Œå™ªå£°çš„çœŸå®ç½‘ç»œç¯å¢ƒé€‚åº”æ€§æœ‰é™ã€‚è¿™å¯¼è‡´LLMséš¾ä»¥åŠ¨æ€å†³å®šä½•æ—¶ä½•åœ°æœç´¢ï¼Œä»¥åŠæ ¹æ®ä¿¡æ¯éœ€æ±‚è°ƒæ•´æœç´¢æ·±åº¦ä¸é¢‘ç‡ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºâ€œæœç´¢å¼ºåº¦ç¼©æ”¾ï¼ˆSISï¼‰â€è¿™ä¸€æ¦‚å¿µï¼Œå³åœ¨æ¨¡ç³Šæˆ–å†²çªæ¡ä»¶ä¸‹åŠ å¼ºæœç´¢åŠªåŠ›ï¼Œè€Œéæ»¡è¶³äºè¿‡åº¦è‡ªä¿¡ã€éªŒè¯ä¸è¶³çš„ç­”æ¡ˆï¼Œæ—¨åœ¨è§£å†³LLMsåœ¨ä¿¡æ¯æœç´¢æ–¹é¢çš„çŸ­æ¿ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºWebPuzzleæ•°æ®é›†  
ä¸ºç ”ç©¶SISï¼Œè®ºæ–‡æ¨å‡ºWebPuzzleæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåŸ¹å…»å¼€æ”¾ç½‘ç»œç¯å¢ƒä¸‹ä¿¡æ¯æœç´¢è¡Œä¸ºçš„æ•°æ®é›†ã€‚å®ƒåŒ…å«24Kè®­ç»ƒå®ä¾‹ä¸275ä¸ªæµ‹è¯•é—®é¢˜ï¼Œæ¶µç›–åŸºäºç»´åŸºå’Œå¼€æ”¾ç½‘ç»œçš„æŸ¥è¯¢ï¼Œèƒ½æœ‰æ•ˆè¯„ä¼°LLMsåœ¨çœŸå®æœç´¢ç¯å¢ƒä¸­çš„ä¿¡æ¯æœç´¢èƒ½åŠ›ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºDeepDiverå¼ºåŒ–å­¦ä¹ æ¡†æ¶  
åŸºäºWebPuzzleæ•°æ®é›†ï¼Œè®ºæ–‡æå‡ºDeepDiverå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨çœŸå®å¼€æ”¾ç½‘ç»œç¯å¢ƒä¸­æ¢ç´¢ï¼Œé¼“åŠ±è‡ªé€‚åº”æœç´¢ç­–ç•¥ï¼Œä»è€Œä¿ƒè¿›SISèƒ½åŠ›ã€‚å…¶è®­ç»ƒæµç¨‹ä»å†·å¯åŠ¨çš„æœ‰ç›‘ç£å¾®è°ƒï¼Œåˆ°ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œè®©æ¨¡å‹èƒ½åœ¨çœŸå®ç½‘ç»œäº¤äº’ä¸­æŒç»­ä¼˜åŒ–æ£€ç´¢æ–‡æ¡£çš„å»å™ªä¸ä¿¡æ¯å®šä½ï¼Œæå‡å›ç­”å‡†ç¡®æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œæ­è½½DeepDiverçš„Pangu - 7B - Reasoneråœ¨çœŸå®ç½‘ç»œä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸671Bå‚æ•°çš„DeepSeek - R1åª²ç¾ã€‚åŒæ—¶ï¼ŒDeepDiverçš„SISèƒ½åŠ›ä»å°é—­å¼é—®ç­”æ³›åŒ–åˆ°å¦‚é•¿ç¯‡å†™ä½œç­‰å¼€æ”¾å¼ä»»åŠ¡ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ä¸æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œåˆ†æè¿˜æ­ç¤ºDeepDiverçš„æœç´¢æ·±åº¦å’Œé¢‘ç‡ä¸é—®é¢˜éš¾åº¦ã€æ¨¡å‹æ€§èƒ½æˆæ­£æ¯”ï¼›ç›¸æ¯”åŸºäºç»´åŸºçš„â€œå¹²å‡€â€ç¯å¢ƒï¼ŒWebPuzzleå’ŒçœŸå®å¼€æ”¾ç½‘ç»œç¯å¢ƒæ›´èƒ½æ”¯æŒå¤æ‚æ¨ç†è¡Œä¸ºï¼›å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ˜¾è‘—å¢å¼ºäº†LLMsçš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŠ©åŠ›ä»å°é—­å¼é—®é¢˜å‘å¼€æ”¾å¼é—®é¢˜è¿‡æ¸¡ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºè§’åº¦ï¼šWebPuzzleä¸ºè¯„ä¼°çœŸå®ç½‘ç»œç¯å¢ƒä¸‹LLMsä¿¡æ¯æœç´¢èƒ½åŠ›æä¾›äº†æ–°åŸºå‡†ï¼Œåç»­ç ”ç©¶å¯å‚è€ƒå…¶æ€è·¯ï¼Œæ„å»ºæ›´è´´åˆçœŸå®åœºæ™¯ã€å¤šé¢†åŸŸè¦†ç›–çš„æ•°æ®é›†ï¼Œæ¨åŠ¨ä¿¡æ¯æœç´¢ç›¸å…³ç ”ç©¶ã€‚  
2. æ–¹æ³•æ¡†æ¶è§’åº¦ï¼šDeepDiverå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥è¿­ä»£æ£€ç´¢ - ç”Ÿæˆæ¡†æ¶ï¼Œå¹¶åœ¨çœŸå®ç½‘ç»œç¯å¢ƒè®­ç»ƒï¼Œä¸ºæå‡LLMsè‡ªé€‚åº”ä¿¡æ¯æœç´¢èƒ½åŠ›æä¾›äº†æ–°èŒƒå¼ã€‚å…¶ä»–ç ”ç©¶è€…å¯å€Ÿé‰´å…¶å¼ºåŒ–å­¦ä¹ ä¸çœŸå®ç¯å¢ƒäº¤äº’ç»“åˆçš„æ€è·¯ï¼Œæ¢ç´¢æ›´ä¼˜çš„æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–æ–¹å¼ã€‚  
3. èƒ½åŠ›æ³›åŒ–è§’åº¦ï¼šDeepDiverå±•ç°çš„SISèƒ½åŠ›æ³›åŒ–æ€§ï¼Œå¯ç¤ºç ”ç©¶éœ€å…³æ³¨æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ç±»å‹ï¼ˆä»å°é—­åˆ°å¼€æ”¾ï¼‰ä¸‹çš„è¿ç§»èƒ½åŠ›åŸ¹å…»ï¼Œè®¾è®¡æ›´å…·é€šç”¨æ€§çš„è®­ç»ƒç­–ç•¥ã€‚  
```

## don-t-think-longer--think-wisely--optimizing-thinking-dynamics-for-large-reasoning-models
### Abstract
While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§æ¨ç†æ¨¡å‹åˆ«â€œæƒ³å¤ªå¤šâ€ï¼Œè¦â€œæƒ³èªæ˜â€ï¼šä¼˜åŒ–æ€ç»´åŠ¨æ€æå‡æ•ˆç‡ä¸ç²¾åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡ï¼Œåœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿå‡ºç°äº†â€œè¿‡åº¦æ€è€ƒï¼ˆoverthinkingï¼‰â€é—®é¢˜ï¼šæ¨ç†è·¯å¾„ä¸å¿…è¦åœ°å¤æ‚å†—é•¿ï¼Œæ—¢æµªè´¹è®¡ç®—èµ„æºï¼Œè¿˜å¯èƒ½é™ä½æ€§èƒ½ã€‚ç©¶å…¶æ ¹æºï¼Œæ˜¯æ¨¡å‹éš¾ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©åˆé€‚çš„**æ¨¡å—åŒ–æ¨ç†ç­–ç•¥ï¼ˆå³â€œæ€ç»´æ¨¡å¼â€ï¼‰**ã€‚æ¯”å¦‚ï¼Œæ¨¡å‹å¯èƒ½åœ¨è¯¥å¿«é€ŸéªŒè¯çš„ç¯èŠ‚ç»•è¿œè·¯ï¼Œæˆ–åœ¨è¯¥æ·±å…¥åˆ†ææ—¶å´é‡å¤æ— æ•ˆæ­¥éª¤ã€‚å› æ­¤ï¼Œè®ºæ–‡å¸Œæœ›é€šè¿‡ä¼˜åŒ–â€œæ€ç»´åŠ¨æ€â€ï¼Œè®©æ¨¡å‹æ›´èªæ˜åœ°é€‰æ‹©æ€ç»´æ¨¡å¼ï¼Œåœ¨ä¿è¯ç²¾åº¦çš„åŒæ—¶æå‡æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåŠ¨æ€æ€ç»´æ¨¡å¼ä¼˜åŒ–æ¡†æ¶ï¼ˆDTOï¼‰  
è®ºæ–‡æŠŠâ€œæå‡æ¨ç†æ•ˆç‡â€è½¬åŒ–ä¸º**å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜**â€”â€”åœ¨ä¿è¯ä»»åŠ¡è¡¨ç°çš„å‰æä¸‹ï¼Œæœ€å°åŒ–æ¨ç†è½¨è¿¹çš„è®¡ç®—æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼ŒDTOå…ˆæŠŠæ¨¡å‹ç”Ÿæˆçš„æ¨ç†è·¯å¾„åˆ‡åˆ†æˆä¸€ä¸ªä¸ªå¯è¯†åˆ«çš„â€œæ€ç»´æ¨¡å¼ç‰‡æ®µâ€ï¼ˆæ¯”å¦‚å‡è®¾ç”Ÿæˆã€è‡ªæˆ‘éªŒè¯ã€ä¸­é—´æ€»ç»“ç­‰åŠŸèƒ½æ¨¡å—ï¼‰ï¼›æ¥ç€è¯„ä¼°æ¯ä¸ªç‰‡æ®µå¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼ŒåŒºåˆ†â€œæœ‰ç›Šæ¨¡å¼â€å’Œâ€œæœ‰å®³æ¨¡å¼â€ï¼›æœ€åé€šè¿‡ä¿ç•™/å¼ºåŒ–æœ‰ç›Šç‰‡æ®µã€ä¿®å‰ªæœ‰å®³ç‰‡æ®µï¼ŒæŠŠå†—é•¿æ¨ç†è·¯å¾„å‹ç¼©æˆæ›´ç®€æ´æœ‰æ•ˆçš„è½¨è¿¹ï¼Œç”šè‡³èƒ½æŠŠåŸæœ¬é”™è¯¯çš„æ¨ç†ä¿®æ­£ä¸ºæ­£ç¡®ç»“æœã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆåå¥½ä¼˜åŒ–ä¸å¯¹æ¯”æ•°æ®é›†  
ä¸ºäº†è®©æ¨¡å‹ä¸»åŠ¨å­¦ä¹ â€œæ›´ä¼˜æ€ç»´æ¨¡å¼â€ï¼Œè®ºæ–‡æ„å»ºäº†**æˆå¯¹å¯¹æ¯”æ•°æ®é›†**ï¼ˆåŒ…å«â€œæ¬¡ä¼˜æ¨ç†è·¯å¾„â€å’Œâ€œæœ€ä¼˜æ¨ç†è·¯å¾„â€ï¼‰ï¼Œå†ç”¨åå¥½ä¼˜åŒ–æŠ€æœ¯å¼•å¯¼æ¨¡å‹ä¼˜å…ˆé‡‡ç”¨é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ€ç»´æ¨¡å¼ã€‚è¿™ç§æ–¹å¼ä¸åƒä¼ ç»Ÿæ–¹æ³•åªé å¯å‘å¼æˆªæ–­æˆ–ç®€å•ç»Ÿè®¡tokené•¿åº¦ï¼Œè€Œæ˜¯æ˜¾å¼åœ°å¯¹æ¨ç†ç‰‡æ®µçš„è´¡çŒ®å»ºæ¨¡ã€ä¼˜åŒ–ï¼Œç²¾å‡†æå‡æ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­éªŒè¯æ–¹æ³•ï¼š  
- æ•ˆç‡å±‚é¢ï¼šä¼˜åŒ–åçš„æ¨ç†è·¯å¾„èƒ½å‡å°‘æ³¨æ„åŠ›è®¡ç®—é‡ï¼ˆFLOPsï¼‰**æœ€é«˜è¾¾47%**ï¼ŒåŒæ—¶ä¿æŒåŸæœ¬æ­£ç¡®å›ç­”çš„ç²¾åº¦ï¼›tokenä½¿ç”¨é‡ä»çº¦5000é™åˆ°3000ï¼Œè®¡ç®—å¼€é”€æ˜¾è‘—é™ä½ã€‚  
- ç²¾åº¦å±‚é¢ï¼šåŸæœ¬é”™è¯¯çš„å›ç­”ä¸­æœ‰ç›¸å½“æ¯”ä¾‹è¢«ä¿®æ­£ï¼Œ**å‡†ç¡®ç‡æå‡æœ€é«˜15.6%**ï¼›åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ•´ä½“å‡†ç¡®ç‡æœ€å¤šèƒ½æ¯”åŸå§‹LRMæå‡12%ã€‚  

ç®€è¨€ä¹‹ï¼Œæ¨¡å‹â€œæƒ³â€å¾—æ›´çŸ­ã€æ›´èªæ˜ï¼Œç²¾åº¦è¿˜èƒ½æ¶¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **é—®é¢˜å»ºæ¨¡è§†è§’**ï¼šæŠŠâ€œæ¨ç†æ•ˆç‡â€å½“ä½œå¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼Œè·³å‡ºâ€œåªçœ‹ç»“æœç²¾åº¦â€çš„æ€ç»´å®šå¼ï¼Œå…³æ³¨æ¨ç†è¿‡ç¨‹ä¸­â€œæ¯ä¸€æ­¥æ˜¯å¦å¿…è¦â€ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†æ•ˆç‡ä¼˜åŒ–æä¾›äº†æ–°èŒƒå¼ã€‚  
2. **æ¨¡å—åŒ–æ€ç»´æ¨¡å¼**ï¼šå°†æ¨ç†è·¯å¾„æ‹†è§£ä¸ºâ€œåŠŸèƒ½åŒ–ç‰‡æ®µâ€ï¼ˆæ€ç»´æ¨¡å¼ï¼‰ï¼Œé€šè¿‡åˆ†æå„ç‰‡æ®µçš„è´¡çŒ®æ¥ä¼˜åŒ–ï¼Œè¿™ç§â€œåˆ†è€Œæ²»ä¹‹+ç²¾å‡†å–èˆâ€çš„æ€è·¯ï¼Œå¯è¿ç§»åˆ°ä»£ç ç”Ÿæˆã€å¤šæ­¥å†³ç­–ç­‰éœ€é•¿ç¨‹æ¨ç†çš„ä»»åŠ¡ã€‚  
3. **åå¥½ä¼˜åŒ–+å¯¹æ¯”æ•°æ®**ï¼šç”¨æˆå¯¹æ•°æ®å¼•å¯¼æ¨¡å‹å­¦ä¹ â€œæ›´ä¼˜è·¯å¾„â€ï¼Œä¸ºå¤§æ¨¡å‹çš„â€œè¿‡ç¨‹ä¼˜åŒ–â€æä¾›äº†è½åœ°æ‰‹æ®µï¼Œåç»­å¯æ‹“å±•åˆ°æ›´å¤šé¢†åŸŸï¼ˆå¦‚é€»è¾‘æ¨ç†ã€åˆ›æ„ç”Ÿæˆï¼‰çš„æ¨ç†è½¨è¿¹ä¼˜åŒ–ã€‚  
```

## pitfalls-of-rule--and-model-based-verifiers----a-case-study-on-mathematical-reasoning
### Abstract
Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºè§„åˆ™ä¸æ¨¡å‹çš„éªŒè¯å™¨åœ¨æ•°å­¦æ¨ç†ä¸­çš„é™·é˜±å‰–æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†èƒ½åŠ›æå‡çš„æ ¸å¿ƒæ–¹æ³•ï¼ŒåƒDeepSeek - R1è¿™ç±»å¤§æ¨ç†æ¨¡å‹éƒ½ä»¥æ­¤ä¸ºåŸºç¡€ã€‚åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚é¢†åŸŸï¼ŒåŸºäºè§„åˆ™çš„éªŒè¯å™¨è¢«å¹¿æ³›ç”¨äºè®­ç»ƒå¼ºæ¨ç†æ¨¡å‹ï¼Œç„¶è€Œè¿™äº›éªŒè¯å™¨çš„å¯é æ€§ä»¥åŠå¯¹RLè®­ç»ƒè¿‡ç¨‹çš„å½±å“å´çŸ¥ä¹‹ç”šå°‘ã€‚æ¯”å¦‚åŸºäºè§„åˆ™çš„éªŒè¯åœ¨RLé¡¹ç›®ä¸­å‡†ç¡®æ€§å¦‚ä½•ï¼Ÿé”™è¯¯éªŒè¯æ˜¯å¦ä¼šæ˜¾è‘—å½±å“RLæ€§èƒ½ï¼ŸåŒæ—¶ï¼Œä¸ºè§£å†³åŸºäºè§„åˆ™éªŒè¯å™¨çš„ä¸è¶³è€Œæ¢ç´¢çš„åŸºäºæ¨¡å‹çš„éªŒè¯å™¨ï¼Œå…¶åœ¨RLè®­ç»ƒä¸­åˆå­˜åœ¨æ€æ ·çš„é—®é¢˜ï¼Ÿè¿™äº›éƒ½æ˜¯æœ¬æ–‡è¦æ¢ç©¶çš„åŠ¨æœºæ‰€åœ¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢åˆ†æç°æœ‰åŸºäºè§„åˆ™çš„éªŒè¯å™¨
å¯¹å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„å¼€æºæ•°å­¦æ•°æ®é›†ä¸Šçš„ç°æœ‰åŸºäºè§„åˆ™éªŒè¯å™¨è¿›è¡Œå…¨é¢åˆ†æï¼Œæ¢ç©¶å…¶åœ¨é™æ€ã€åŸºäºåˆ†ç±»çš„è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºå…¶åœ¨è¯†åˆ«ä¸åŒæ ¼å¼ç­‰æ•ˆç­”æ¡ˆæ—¶å­˜åœ¨çš„ä¸è¶³ï¼Œå¦‚åœ¨å¸¸ç”¨æ•°å­¦æ•°æ®é›†ä¸Šå­˜åœ¨ä¸å¯å¿½è§†çš„å‡é˜´æ€§ç‡ï¼Œä¸”éšç€ç”Ÿæˆæ¨¡å‹å˜å¼ºå‡é˜´æ€§ç‡å‘ˆä¸Šå‡è¶‹åŠ¿ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¢ç´¢åŸºäºæ¨¡å‹çš„éªŒè¯å™¨å¹¶åˆ†æå…¶åœ¨RLè®­ç»ƒä¸­çš„é—®é¢˜
åˆ©ç”¨ç°æˆçš„å¼€æºæ¨¡å‹ä»¥åŠè®­ç»ƒæ–°æ¨¡å‹æ¥æ¢ç©¶åŸºäºæ¨¡å‹çš„éªŒè¯å™¨ï¼Œå¯¹æ¯”å…¶ä¸åŸºäºè§„åˆ™éªŒè¯å™¨åœ¨åˆ†ç±»è¯„ä¼°ä¸­çš„è¡¨ç°ï¼›åŒæ—¶åœ¨RLè®­ç»ƒå®éªŒä¸­è§‚å¯ŸåŸºäºæ¨¡å‹çš„éªŒè¯å™¨å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚æ˜¯å¦æ˜“å—â€œå¥–åŠ±é»‘å®¢æ”»å‡»â€ï¼Œå¹¶åˆ†æéªŒè¯å™¨åˆ†ç±»å‡†ç¡®ç‡ä¸æŠ—å¥–åŠ±é»‘å®¢æ”»å‡»èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼›è¿˜é€šè¿‡æ„å»ºå¯¹æŠ—æ¨¡å¼æ¥æ¢ç©¶åŸºäºæ¨¡å‹éªŒè¯å™¨çš„æ¼æ´ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. åŸºäºè§„åˆ™çš„éªŒè¯å™¨ï¼šåœ¨é™æ€åˆ†ç±»è¯„ä¼°ä¸­ï¼Œå½“å›ç­”ä¸çœŸå®ç­”æ¡ˆæ ¼å¼ç›¸è¿‘æ—¶è¯†åˆ«æ•ˆæœå¥½ï¼Œä½†åœ¨ç”Ÿæˆç­”æ¡ˆæ›´å…·å¤šæ ·æ€§æˆ–å¤„äºé•¿å°¾åˆ†å¸ƒæ—¶è¡¨ç°ä¸ä½³ï¼Œå¹³å‡å¬å›ç‡ä»…86%ï¼Œä¸”ç”Ÿæˆæ¨¡å‹è¶Šå¼ºå‡é˜´æ€§ç‡è¶Šé«˜ï¼Œå¯¹RLè®­ç»ƒæ€§èƒ½æœ‰ä¸åˆ©å½±å“ã€‚
2. åŸºäºæ¨¡å‹çš„éªŒè¯å™¨ï¼šé™æ€è¯„ä¼°ä¸­åˆ†ç±»å‡†ç¡®ç‡æ˜¾è‘—é«˜äºåŸºäºè§„åˆ™çš„éªŒè¯å™¨ï¼Œå¦‚åœ¨Skywork - OR1æ•°æ®é›†ä¸Šå¬å›ç‡ä»84%æå‡åˆ°92%ï¼›ä½†åœ¨RLè®­ç»ƒä¸­è¡¨ç°ä¸ä¸€ï¼Œéƒ¨åˆ†èƒ½æ¯”åŸºäºè§„åˆ™çš„éªŒè¯å™¨å¹³å‡æå‡è¶…3ä¸ªç»å¯¹ç‚¹ï¼Œéƒ¨åˆ†æ˜“å—é»‘å®¢æ”»å‡»å¯¼è‡´RLè®­ç»ƒç»“æœä¸ä½³ï¼Œä¸”è®­ç»ƒåçš„éªŒè¯å™¨è™½é™æ€åˆ†ç±»å‡†ç¡®ç‡é«˜ä½†åœ¨RLè®­ç»ƒä¸­æ›´æ˜“è¢«æ”»å‡»ï¼Œè¯´æ˜éªŒè¯å™¨åˆ†ç±»å‡†ç¡®ç‡ä¸èƒ½åæ˜ å…¶æŠ—å¥–åŠ±é»‘å®¢æ”»å‡»èƒ½åŠ›ã€‚
3. å¯¹æŠ—æ¨¡å¼æµ‹è¯•ï¼šæ„å»ºå¦‚æ’å…¥ç©ºå­—ç¬¦æˆ–ä¹±ç æ–‡æœ¬ç­‰å¯¹æŠ—æ¨¡å¼æµ‹è¯•åŸºäºæ¨¡å‹çš„éªŒè¯å™¨ï¼Œå‘ç°å¤šæ•°éªŒè¯å™¨æ˜“è¢«æ¬ºéª—ï¼Œåˆ¤åˆ«å¼éªŒè¯å™¨æ¯”ç”Ÿæˆå¼æ›´é²æ£’ï¼Œå¾®è°ƒè™½èƒ½æé«˜é™æ€è¯„ä¼°åˆ†æ•°ä½†ä¸ä¸€å®šå¢å¼ºé²æ£’æ€§ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼šé™ä½é²æ£’æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹äºç ”ç©¶å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±ç³»ç»Ÿçš„å­¦è€…ï¼Œæœ¬æ–‡æ­ç¤ºäº†åŸºäºè§„åˆ™å’ŒåŸºäºæ¨¡å‹çš„éªŒè¯å™¨åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸‹çš„å›ºæœ‰é£é™©ï¼Œä¸ºå¼€å‘æ›´é²æ£’çš„å¥–åŠ±ç³»ç»Ÿæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¦‚åœ¨è®¾è®¡éªŒè¯å™¨æ—¶éœ€è€ƒè™‘ä¸åŒæ ¼å¼ç­‰æ•ˆç­”æ¡ˆçš„è¯†åˆ«ä»¥åŠæŠ—æ”»å‡»èƒ½åŠ›ç­‰ã€‚
2. å¯¹äºä»äº‹å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†è®­ç»ƒçš„ä»ä¸šè€…ï¼Œäº†è§£åˆ°åŸºäºè§„åˆ™éªŒè¯å™¨çš„ä¸è¶³å’ŒåŸºäºæ¨¡å‹éªŒè¯å™¨çš„ä¼˜ç¼ºç‚¹ï¼Œåœ¨é€‰æ‹©éªŒè¯å™¨æ—¶å¯æ›´æœ‰é’ˆå¯¹æ€§ï¼ŒåŒæ—¶åœ¨ä¼˜åŒ–æ¨¡å‹è¿‡ç¨‹ä¸­è¦è­¦æƒ•å¥–åŠ±é»‘å®¢æ”»å‡»é—®é¢˜ã€‚
3. åœ¨éªŒè¯å™¨çš„æ”¹è¿›æ–¹å‘ä¸Šï¼Œæœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜è¿½æ±‚æ›´å‡†ç¡®çš„åŸºäºæ¨¡å‹çš„éªŒè¯å™¨æ˜¯æå‡RLæ€§èƒ½çš„æœ‰å‰æ™¯æ–¹å‘ï¼Œä½†éœ€è§£å†³å…¶æ˜“å—æ”»å‡»çš„é—®é¢˜ï¼Œä¸ºåç»­éªŒè¯å™¨çš„ç ”ç©¶å’Œæ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ï¼Œå¦‚é’ˆå¯¹å¯¹æŠ—æ¨¡å¼å¢å¼ºéªŒè¯å™¨çš„é²æ£’æ€§ç­‰ã€‚
```

## learning-to-route-queries-across-knowledge-bases-for-step-wise-retrieval-augmented-reasoning
### Abstract
Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in
mitigating hallucinations in Multimodal Large Language Models (MLLMs) by
incorporating external knowledge during generation. Existing MRAG methods
typically adopt a static retrieval pipeline that fetches relevant information
from multiple Knowledge Bases (KBs), followed by a refinement step. However,
these approaches overlook the reasoning and planning capabilities of MLLMs to
dynamically determine how to interact with different KBs during the reasoning
process. To address this limitation, we propose R1-Router, a novel MRAG
framework that learns to decide when and where to retrieve knowledge based on
the evolving reasoning state. Specifically, R1-Router can generate follow-up
queries according to the current reasoning step, routing these intermediate
queries to the most suitable KB, and integrating external knowledge into a
coherent reasoning trajectory to answer the original query. Furthermore, we
introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored
reinforcement learning algorithm that assigns step-specific rewards to optimize
the reasoning behavior of MLLMs. Experimental results on various open-domain QA
benchmarks across multiple modalities demonstrate that R1-Router outperforms
baseline models by over 7%. Further analysis shows that R1-Router can
adaptively and effectively leverage diverse KBs, reducing unnecessary
retrievals and improving both efficiency and accuracy.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤šæ¨¡æ€å¤§æ¨¡å‹â€œèªæ˜æ£€ç´¢â€ï¼šR1 - Router å®ç°åŠ¨æ€çŸ¥è¯†åº“è·¯ç”±ä¸åˆ†æ­¥æ¨ç†å¢å¼º

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢†åŸŸï¼Œå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰æœ¬æ˜¯ç¼“è§£æ¨¡å‹å¹»è§‰çš„æœ‰æ•ˆæ‰‹æ®µï¼Œé€šè¿‡ç”Ÿæˆæ—¶èå…¥å¤–éƒ¨çŸ¥è¯†æ¥æå‡è¡¨ç°ã€‚ä½†ç°æœ‰ MRAG æ–¹æ³•å­˜åœ¨å±€é™ï¼šé‡‡ç”¨â€œé™æ€æ£€ç´¢ pipeline + åç»­ç²¾ä¿®â€çš„æ¨¡å¼ï¼Œåªåšä¸€æ¬¡æ€§ä»å¤šçŸ¥è¯†åº“ï¼ˆKBsï¼‰å–ä¿¡æ¯ï¼Œå´å¿½ç•¥äº† MLLMs è‡ªèº«çš„æ¨ç†ä¸è§„åˆ’èƒ½åŠ›â€”â€”æ²¡æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­**åŠ¨æ€å†³å®š**è¯¥å’Œå“ªäº›çŸ¥è¯†åº“äº¤äº’ã€ä½•æ—¶äº¤äº’ã€‚æ¯”å¦‚é¢å¯¹å¤æ‚å¤šæ¨¡æ€é—®ç­”ï¼Œå›ºå®šä»æ–‡æœ¬åº“æˆ–å›¾åƒåº“å–ä¿¡æ¯ï¼Œå¯èƒ½æ— æ³•æŒ‰éœ€çµæ´»ç»„åˆå¤šæºçŸ¥è¯†ï¼Œæ—¢å½±å“ç­”æ¡ˆå‡†ç¡®æ€§ï¼Œä¹Ÿæ˜“åšæ— ç”¨æ£€ç´¢ã€‚æ­¤å¤–ï¼Œå·²æœ‰è¿­ä»£æ£€ç´¢ç­–ç•¥è™½å°è¯•åˆ†è§£æŸ¥è¯¢ã€å­æŸ¥è¯¢æ£€ç´¢ï¼Œä½†è¦ä¹ˆä¾èµ–å•æ¨¡æ€ä¼˜å…ˆçš„é¢„å®šä¹‰æµç¨‹ä¸å¤Ÿçµæ´»ï¼Œè¦ä¹ˆæ²¡å……åˆ†å‘æŒ¥æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼›è€Œç»“åˆå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æˆ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å·¥ä½œï¼Œåˆå¤šèšç„¦æ–‡æœ¬æ¨¡æ€ï¼Œç¼ºä¹å¤šæ¨¡æ€æ„ŸçŸ¥ï¼Œéš¾ä»¥åº”å¯¹å¤šæ¨¡æ€æŸ¥è¯¢é€‰åˆé€‚çŸ¥è¯†åº“çš„é—®é¢˜ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®© MLLMs ä¾æ®æ¨ç†çŠ¶æ€åŠ¨æ€é€‰â€œä½•æ—¶ã€å‘å“ªï¼ˆçŸ¥è¯†åº“ï¼‰â€æ£€ç´¢ï¼Œæˆäº†å…³é”®ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šR1 - Router æ¡†æ¶â€”â€”åŠ¨æ€è§„åˆ’å¤šçŸ¥è¯†åº“æ£€ç´¢è·¯å¾„  
R1 - Router æ˜¯å…¨æ–°çš„ MRAG æ¡†æ¶ï¼Œæ ¸å¿ƒæ˜¯è®© MLLMs åŸºäº**åŠ¨æ€å˜åŒ–çš„æ¨ç†çŠ¶æ€**ï¼Œè‡ªä¸»å†³å®šä½•æ—¶ã€å‘å“ªä¸ªçŸ¥è¯†åº“å»æ£€ç´¢çŸ¥è¯†ã€‚å…·ä½“æµç¨‹æ˜¯ï¼šå½“æ¨ç†è¿‡ç¨‹éœ€è¦è¡¥å……çŸ¥è¯†æ—¶ï¼Œç”Ÿæˆâ€œä¸­é—´æŸ¥è¯¢â€ï¼Œå†æŠŠè¿™ä¸ªä¸­é—´æŸ¥è¯¢è·¯ç”±åˆ°æœ€é€‚é…çš„çŸ¥è¯†åº“ï¼›ç„¶åæŠŠä»è¯¥çŸ¥è¯†åº“æ‹¿åˆ°çš„å¤–éƒ¨çŸ¥è¯†ï¼Œæ•´åˆè¿›è¿è´¯çš„æ¨ç†è½¨è¿¹ï¼Œæœ€ç»ˆå›ç­”åŸå§‹é—®é¢˜ã€‚æ‰“ä¸ªæ¯”æ–¹ï¼Œè‹¥è¦è§£å†³â€œå›¾ä¸­é¸Ÿç±»çš„æœ€è¿‘äº²åˆ†ç±»é˜¶å…ƒâ€ï¼ŒR1 - Router ä¼šå…ˆæ¨ç†å‡ºâ€œå¾—å…ˆçŸ¥é“è¿™é¸Ÿå«å•¥â€ï¼Œç”Ÿæˆå¯¹åº”ä¸­é—´æŸ¥è¯¢ï¼Œé€‰å›¾åƒ - æ–‡æœ¬æ··åˆçŸ¥è¯†åº“å»æŸ¥åç§°ï¼›æ‹¿åˆ°åç§°åï¼Œå†å†³å®šæ˜¯å¦è¦å»æ–‡æœ¬åº“æŸ¥åˆ†ç±»ä¿¡æ¯ï¼Œä¸€æ­¥æ­¥æ¨è¿›ç›´åˆ°äº§å‡ºæœ€ç»ˆç­”æ¡ˆã€‚è¿™ç§â€œæŒ‰éœ€ç”Ÿæˆå­æŸ¥è¯¢ + åŠ¨æ€é€‰åº“ + è¿­ä»£æ¨ç†â€çš„æ¨¡å¼ï¼Œçªç ´äº†é™æ€æ£€ç´¢çš„æ­»æ¿ï¼Œè®©çŸ¥è¯†è·å–å’Œæ¨ç†æ·±åº¦ç»‘å®šã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šStep - GRPO å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ä¸ºåˆ†æ­¥æ¨ç†å®šåˆ¶å¥–åŠ±ä¼˜åŒ–  
ä¸ºäº†é«˜æ•ˆè®­ç»ƒ R1 - Routerï¼Œè®ºæ–‡æå‡º Step - wise Group Relative Policy Optimizationï¼ˆStep - GRPOï¼‰ã€‚å®ƒçš„ç‰¹ç‚¹æ˜¯**ç»™â€œæ¯ä¸€æ­¥æ¨ç†â€åˆ†é…ç‰¹å®šå¥–åŠ±**ï¼Œå¼•å¯¼æ¨¡å‹åœ¨æ•´ä¸ªæ¨ç†è½¨è¿¹é‡Œä¼˜åŒ–è¡Œä¸ºã€‚ä¼ ç»Ÿ RL å¯èƒ½å¯¹é•¿æ¨ç†é“¾é‡Œçš„æ­¥éª¤åŒºåˆ†åº¦ä¸è¶³ï¼Œè€Œ Step - GRPO å…³æ³¨â€œåˆ†æ­¥â€å¥–åŠ±ï¼Œè®©æ¨¡å‹èƒ½æ›´ç²¾ç»†åœ°å­¦ä¹ ï¼šå“ªä¸€æ­¥è¯¥ç”Ÿæˆæ€æ ·çš„å­æŸ¥è¯¢ã€é€‰å“ªä¸ªçŸ¥è¯†åº“ï¼Œæ‰èƒ½è®©æ•´ä¸ªæ¨ç†è¿‡ç¨‹ï¼ˆä»åˆå§‹é—®é¢˜åˆ°æœ€ç»ˆç­”æ¡ˆï¼‰æ›´é«˜æ•ˆå‡†ç¡®ã€‚é€šè¿‡è¿™ç§å®šåˆ¶åŒ–å¼ºåŒ–å­¦ä¹ ï¼ŒMLLMs èƒ½é€æ­¥å­¦ä¼šâ€œèªæ˜æ¨ç† + èªæ˜æ£€ç´¢â€çš„ç­–ç•¥ï¼Œé¿å…æ— æ•ˆæ£€ç´¢ï¼Œæå‡æ¨ç†è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šæ¨¡æ€å¼€æ”¾åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆæ¶µç›–è§†è§‰é—®ç­”ã€è¡¨æ ¼é—®ç­”ç­‰ä»»åŠ¡ï¼‰ä¸ŠéªŒè¯äº† R1 - Routerã€‚ç»“æœæ˜¾ç¤ºï¼š  
- æ€§èƒ½ç¢¾å‹åŸºçº¿ï¼šR1 - Router æ¯”åŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æå‡è¶… 7%ï¼Œè¯æ˜åŠ¨æ€æ£€ç´¢ + åˆ†æ­¥æ¨ç†çš„æ¨¡å¼åœ¨å¤šæ¨¡æ€ QA ä»»åŠ¡é‡Œæ›´å…·ä¼˜åŠ¿ã€‚  
- æ•ˆç‡ä¸ç²¾å‡†åº¦åŒèµ¢ï¼šè¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒR1 - Router èƒ½â€œè‡ªé€‚åº”ä¸”é«˜æ•ˆâ€åˆ©ç”¨å¤šæ ·çŸ¥è¯†åº“ã€‚æ¯”å¦‚å‡å°‘ä¸å¿…è¦çš„æ£€ç´¢æ¬¡æ•°â€”â€”ä¸å†ç›²ç›®ä»æ‰€æœ‰åº“å–ä¿¡æ¯ï¼Œè€Œæ˜¯æŒ‰éœ€é€‰åº“ï¼›è¿™æ—¢æå‡äº†æ•ˆç‡ï¼ˆå°‘åšæ— ç”¨åŠŸï¼‰ï¼Œåˆæå‡äº†ç­”æ¡ˆç²¾å‡†åº¦ï¼ˆæ‹¿åˆ°çš„çŸ¥è¯†æ›´å¯¹ç—‡ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŠ¨æ€äº¤äº’ç†å¿µï¼šæ‰“ç ´â€œé™æ€æ£€ç´¢æµç¨‹â€æ€ç»´å®šå¼ï¼Œå¼ºè°ƒæ¨¡å‹æ¨ç†çŠ¶æ€å’ŒçŸ¥è¯†åº“äº¤äº’çš„åŠ¨æ€æ€§ã€‚åœ¨åšå¤šæ¨¡æ€æˆ–å¤æ‚çŸ¥è¯†ä»»åŠ¡æ—¶ï¼Œå¯å€Ÿé‰´â€œè®©æ¨¡å‹ä¾æ®å½“å‰æ¨ç†è¿›åº¦ï¼Œè‡ªä¸»å†³å®šä¸‹ä¸€æ­¥æ£€ç´¢åŠ¨ä½œâ€çš„æ€è·¯ï¼Œçµæ´»é€‚é…å¤šæºå¼‚æ„çŸ¥è¯†ã€‚  
2. åˆ†æ­¥å¼ºåŒ–å­¦ä¹ ï¼šStep - GRPO å±•ç¤ºäº†â€œä¸ºé•¿æ¨ç†é“¾çš„æ¯ä¸€æ­¥è®¾è®¡ç‰¹å®šå¥–åŠ±â€çš„ä»·å€¼ã€‚åœ¨éœ€è¦åˆ†æ­¥éª¤å®Œæˆçš„ä»»åŠ¡ï¼ˆå¦‚å¤šè½®å¯¹è¯ã€å¤æ‚å†³ç­–ï¼‰é‡Œï¼Œå¯è€ƒè™‘æ‹†è§£æ­¥éª¤ã€å®šåˆ¶å¥–åŠ±ä¿¡å·ï¼Œè®©æ¨¡å‹æ›´ç²¾ç»†å­¦ä¹ è¿‡ç¨‹ä¼˜åŒ–ï¼Œè€Œéåªçœ‹æœ€ç»ˆç»“æœã€‚  
3. å¤šçŸ¥è¯†åº“ååŒï¼šR1 - Router è¯æ˜äº†â€œä¸åŒçŸ¥è¯†åº“æŒ‰éœ€è°ƒç”¨â€åœ¨å¤šæ¨¡æ€åœºæ™¯çš„å¿…è¦æ€§ã€‚åç»­åšè·¨æ¨¡æ€ã€è·¨æ•°æ®æºçš„ AI åº”ç”¨æ—¶ï¼Œå¯æ€è€ƒå¦‚ä½•è®©ç³»ç»Ÿæ™ºèƒ½é€‰åº“ã€ç»„åˆå¤šåº“çŸ¥è¯†ï¼Œè€Œéä¾èµ–å•ä¸€çŸ¥è¯†åº“æˆ–å›ºå®šç»„åˆæ–¹å¼ã€‚  
4. å¼€æºä¸å¯å¤ç°ï¼šè®ºæ–‡ä»£ç å¼€æºï¼ˆhttps://github.com/OpenBMB/R1 - Routerï¼‰ï¼Œä¸ºç ”ç©¶è€…å¤ç°ã€æ”¹è¿›æ–¹æ³•æä¾›äº†ä¾¿åˆ©ï¼Œä¹Ÿé¼“åŠ±ç¤¾åŒºåŸºäºæ­¤æ¢ç´¢æ›´æ™ºèƒ½çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæ¨ç†æ–¹æ¡ˆã€‚  
```

## iterative-self-incentivization-empowers-large-language-models-as-agentic-searchers
### Abstract
Large language models (LLMs) have been widely integrated into information
retrieval to advance traditional techniques. However, effectively enabling LLMs
to seek accurate knowledge in complex tasks remains a challenge due to the
complexity of multi-hop queries as well as the irrelevant retrieved content. To
address these limitations, we propose EXSEARCH, an agentic search framework,
where the LLM learns to retrieve useful information as the reasoning unfolds
through a self-incentivized process. At each step, the LLM decides what to
retrieve (thinking), triggers an external retriever (search), and extracts
fine-grained evidence (recording) to support next-step reasoning. To enable LLM
with this capability, EXSEARCH adopts a Generalized Expectation-Maximization
algorithm. In the E-step, the LLM generates multiple search trajectories and
assigns an importance weight to each; the M-step trains the LLM on them with a
re-weighted loss function. This creates a self-incentivized loop, where the LLM
iteratively learns from its own generated data, progressively improving itself
for search. We further theoretically analyze this training process,
establishing convergence guarantees. Extensive experiments on four
knowledge-intensive benchmarks show that EXSEARCH substantially outperforms
baselines, e.g., +7.8% improvement on exact match score. Motivated by these
promising results, we introduce EXSEARCH-Zoo, an extension that extends our
method to broader scenarios, to facilitate future work.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¿­ä»£è‡ªæ¿€åŠ±è®©å¤§æ¨¡å‹åŒ–èº«æ™ºèƒ½æœç´¢ä»£ç†ï¼šEXSEARCHæ¡†æ¶æ­ç§˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ä½œä¸ºæ•°æ®æŒ–æ˜çš„åŸºç¡€æŠ€æœ¯ï¼Œæ—¨åœ¨ç†è§£å¤æ‚æŸ¥è¯¢å¹¶ä»å¤–éƒ¨æºæå–ç›¸å…³ä¿¡æ¯ã€‚å¦‚ä»Šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½å¹¿æ³›èå…¥IRä»¥å¢å¼ºä¼ ç»ŸæŠ€æœ¯ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­é«˜æ•ˆè·å–å‡†ç¡®çŸ¥è¯†ä»å­˜æŒ‘æˆ˜ï¼šä¸€æ–¹é¢å¤šè·³æŸ¥è¯¢ç­‰å¤æ‚ä»»åŠ¡éœ€è¿­ä»£åŠ¨æ€æ£€ç´¢ï¼Œç›´æ¥å‘å¤æ‚æŸ¥è¯¢æ˜“è‡´æ£€ç´¢è¦†ç›–ä¸è¶³ï¼›å¦ä¸€æ–¹é¢æ£€ç´¢ç»“æœå¸¸å«æ— å…³å†…å®¹ï¼Œå¼•å‘è¯¯å¯¼æ€§ä¸Šä¸‹æ–‡ã€‚æ­¤å‰å·¥ä½œå¤šä¸²è”ä¿¡æ¯æœç´¢ pipeline æˆ–ç‹¬ç«‹ç”¨åˆæˆæ•°æ®è®­ç»ƒç‰¹å®šé˜¶æ®µï¼Œç«¯åˆ°ç«¯å¯¹é½ä¸åŒæ£€ç´¢é˜¶æ®µä»å¾…æ¢ç´¢ã€‚å› æ­¤ï¼Œéœ€è®©LLMå­¦ä¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸æ£€ç´¢å™¨äº¤äº’å¹¶åæ€æ£€ç´¢å†…å®¹ï¼Œæœ¬æ–‡æå‡ºEXSEARCHæ¡†æ¶åº”å¯¹è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEXSEARCHæ™ºèƒ½æœç´¢æ¡†æ¶ï¼Œèµ‹äºˆLLMç»†ç²’åº¦æ¨ç†ä¸æœç´¢èƒ½åŠ›  
EXSEARCHå°†æœç´¢è¿‡ç¨‹å½¢å¼åŒ–ä¸ºä¸‰ä¸ªæ ¸å¿ƒåŠ¨ä½œå¾ªç¯ï¼šthinkingï¼ˆåŸºäºæ¼”è¿›çš„æœç´¢è½¨è¿¹ç”ŸæˆæŸ¥è¯¢ï¼‰ã€searchï¼ˆè§¦å‘å¤–éƒ¨æ£€ç´¢å™¨è·å–æ–‡æ¡£ï¼‰ã€recordingï¼ˆä»æ£€ç´¢æ–‡æ¡£æå–ç»†ç²’åº¦è¯æ®æ”¯æ’‘åç»­æ¨ç†ï¼‰ã€‚LLM interleavesï¼ˆäº¤æ›¿æ‰§è¡Œï¼‰è¿™äº›åŠ¨ä½œï¼Œé€æ­¥æ¢ç´¢ç”±å­æŸ¥è¯¢ã€æ£€ç´¢æ–‡æ¡£å’Œæ”¯æŒæ€§è¯æ®æ„æˆçš„æœç´¢è½¨è¿¹ï¼Œæœ€ç»ˆåŸºäºæ•´ä¸ªè½¨è¿¹ç”Ÿæˆç­”æ¡ˆï¼Œè®©å“åº”é”šå®šå¤–éƒ¨çŸ¥è¯†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡‡ç”¨å¹¿ä¹‰æœŸæœ›æœ€å¤§åŒ–ï¼ˆGEMï¼‰ç®—æ³•å®ç°è‡ªæ¿€åŠ±å­¦ä¹   
ä¸ºè®©LLMæŒæ¡ä¸Šè¿°æ™ºèƒ½æœç´¢èƒ½åŠ›ï¼ŒEXSEARCHå¼•å…¥GEMç®—æ³•ï¼Œäº¤æ›¿æ‰§è¡Œè½¨è¿¹æ¢ç´¢ï¼ˆE-stepï¼‰ä¸é‡åŠ æƒè½¨è¿¹å­¦ä¹ ï¼ˆM-stepï¼‰ï¼Œå°†æœç´¢è½¨è¿¹è§†ä¸ºéšå˜é‡ï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚E-step ç”¨é‡è¦æ€§é‡‡æ ·è¿‘ä¼¼æœç´¢è½¨è¿¹åˆ†å¸ƒï¼ŒLLMä¸ºæ¯ä¸ªè¾“å…¥ä»»åŠ¡ç”Ÿæˆå€™é€‰è½¨è¿¹å¹¶ä¾å…¶å¯¹æ­£ç¡®ç­”æ¡ˆçš„æ”¯æŒåº¦è‡ªåŠ¨åˆ†é…æƒé‡ï¼›M-step åˆ™é‡åŠ æƒè¿™äº›è½¨è¿¹æ„é€ è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰å¹¶æœ€å¤§åŒ–ä»¥æ›´æ–°LLMå‚æ•°ï¼Œä½¿æ¨¡å‹ä»è‡ªèº«ç”Ÿæˆæ•°æ®ä¸­å­¦ä¹ ï¼Œç”Ÿæˆæ›´å…·æ”¯æŒæ€§çš„è½¨è¿¹ä¸å‡†ç¡®ç­”æ¡ˆï¼Œå½¢æˆè‡ªæ¿€åŠ±å¾ªç¯æŒç»­ä¼˜åŒ–æœç´¢ä¸æ¨ç†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†è®ºåˆ†æä¿éšœæ”¶æ•›æ€§ & æ¨å‡ºEXSEARCH - Zooæ‹“å±•åº”ç”¨åœºæ™¯  
ä»ç†è®ºå±‚é¢åˆ†æè®­ç»ƒè¿‡ç¨‹ï¼Œè¯æ˜è‡ªæ¿€åŠ±æ¡†æ¶çš„ç¨³å®šæ”¶æ•›æ€§ï¼›åŸºäºè‰¯å¥½å®éªŒç»“æœï¼Œæ¨å‡ºEXSEARCH - Zooæ‰©å±•èµ„æºï¼Œä»ä¸¤ç»´åº¦æ‹“å±•ï¼šè¦†ç›–ä¸åŒæ¨¡å‹å®¶æ—ï¼ˆå¦‚LLaMAã€Qwenï¼‰ä¸è§„æ¨¡ï¼ˆ7Bã€24Bç­‰å‚æ•°ï¼‰çš„å¤šæ · backbone LLMsï¼›æ–°å¢æ–‡æ¡£é‡æ’åºç­‰æ‰©å±•åŠ¨ä½œï¼Œä¸°å¯ŒåŸæ¡†æ¶åŠ¨ä½œç©ºé—´ï¼ŒåŠ©åŠ›æœªæ¥ç ”ç©¶ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªçŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ï¼ˆå¦‚HotpotQAç­‰ï¼‰ä¸Šå¼€å±•å¤§é‡å®éªŒï¼Œç»“æœæ˜¾ç¤ºEXSEARCHæ˜¾è‘—è¶…è¶ŠåŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚åœ¨ç²¾ç¡®åŒ¹é…åˆ†æ•°ä¸Šå®ç° + 7.8% çš„æå‡ï¼›å›¾1å±•ç¤ºäº†åœ¨HotpotQAæ•°æ®é›†ä¸Šä¸åŒLLMåº”ç”¨EXSEARCHåçš„æ€§èƒ½è¡¨ç°ï¼Œéšè®­ç»ƒè¿­ä»£æ¨è¿›ï¼Œå„æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸è¯„ä¼°é›†ä¸Šçš„ç²¾ç¡®åŒ¹é…åˆ†æ•°å‡é€æ­¥æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ä¸æ”¶æ•›æ€§è¶‹åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šå°†å¤æ‚æœç´¢æ‹†è§£ä¸ºâ€œæ€è€ƒ - æœç´¢ - è®°å½•â€å¾ªç¯åŠ¨ä½œï¼Œä¸ºå¤§æ¨¡å‹åœ¨éœ€å¤–éƒ¨çŸ¥è¯†è¾…åŠ©çš„ä»»åŠ¡ä¸­è®¾è®¡äº¤äº’é€»è¾‘æä¾›å‚è€ƒï¼Œå¯å‘å¦‚ä½•è®©æ¨¡å‹åŠ¨æ€è¿­ä»£å¼åˆ©ç”¨å¤–éƒ¨å·¥å…·ï¼›  
2. è®­ç»ƒèŒƒå¼åˆ›æ–°ï¼šå€ŸåŠ©GEMç®—æ³•å®ç°è‡ªæ¿€åŠ±å­¦ä¹ ï¼ŒæŠŠæ¨¡å‹è‡ªèº«ç”Ÿæˆçš„è½¨è¿¹ä½œä¸ºå­¦ä¹ èµ„æºï¼Œä¸ºè§£å†³â€œæ— å……è¶³æ ‡æ³¨æ•°æ®ä¸‹å¦‚ä½•è®©æ¨¡å‹è‡ªæ”¹è¿›â€æä¾›äº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ€è·¯ï¼›  
3. æ‰©å±•æ€§å®è·µï¼šEXSEARCH - Zoo å±•ç¤ºäº†å¦‚ä½•ä»æ¨¡å‹å¤šæ ·æ€§ä¸åŠ¨ä½œä¸°å¯Œæ€§è§’åº¦æ‹“å±•æ–¹æ³•é€‚ç”¨åœºæ™¯ï¼Œä¸ºæŠ€æœ¯è½åœ°åˆ°æ›´å¹¿æ³›ä»»åŠ¡ä¸æ¨¡å‹ç”Ÿæ€æä¾›èŒƒå¼ï¼Œä¾¿äºåç»­ç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šå¿«é€Ÿè¯•éªŒä¸æ”¹è¿›ã€‚  
```

## search-wisely--mitigating-sub-optimal-agentic-searches-by-reducing-uncertainty
### Abstract
Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language
Models (LLMs) by enabling dynamic, multi-step reasoning and information
retrieval. However, these systems often exhibit sub-optimal search behaviors
like over-search (retrieving redundant information) and under-search (failing
to retrieve necessary information), which hinder efficiency and reliability.
This work formally defines and quantifies these behaviors, revealing their
prevalence across multiple QA datasets and agentic RAG systems (e.g., one model
could have avoided searching in 27.7% of its search steps). Furthermore, we
demonstrate a crucial link between these inefficiencies and the models'
uncertainty regarding their own knowledge boundaries, where response accuracy
correlates with model's uncertainty in its search decisions. To address this,
we propose $\beta$-GRPO, a reinforcement learning-based training method that
incorporates confidence threshold to reward high-certainty search decisions.
Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model
with better agentic RAG ability, outperforming other strong baselines with a 4%
higher average exact match score.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å‡å°‘ä¸ç¡®å®šæ€§ï¼Œè®©æ™ºèƒ½ä½“æœç´¢æ›´â€œèªæ˜â€â€”â€”Search Wisely è§£è¯»

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆAgentic RAGï¼‰æ¡†æ¶åï¼Œèƒ½å®ç°å¤šæ­¥æ¨ç†ä¸åŠ¨æ€ä¿¡æ¯æ£€ç´¢ï¼Œæ¨¡æ‹Ÿäººç±»å¤æ‚ç ”ç©¶è¿‡ç¨‹ã€‚ä½†ç°æœ‰Agentic RAGç³»ç»Ÿå¸¸å­˜åœ¨**æœç´¢ä½æ•ˆ**é—®é¢˜ï¼šä¸€æ˜¯â€œè¿‡åº¦æœç´¢ï¼ˆover - searchï¼‰â€ï¼Œå³æ£€ç´¢å·²æœ‰å†…éƒ¨çŸ¥è¯†çš„å†—ä½™ä¿¡æ¯ï¼›äºŒæ˜¯â€œæœç´¢ä¸è¶³ï¼ˆunder - searchï¼‰â€ï¼Œå³éœ€è¦å¤–éƒ¨çŸ¥è¯†æ—¶å´æœªæ£€ç´¢ï¼Œè¿™ä¸¤å¤§é—®é¢˜ä¸¥é‡å½±å“ç³»ç»Ÿæ•ˆç‡ä¸å¯é æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é‡åŒ–è¿™äº›é—®é¢˜ã€æ¢ç©¶å…¶æ ¹æºå¹¶æå‡ºæ”¹è¿›æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡åŒ–åˆ†ææœç´¢ä½æ•ˆè¡Œä¸º  
å¯¹2WikiMultiHopQAã€Bamboogleã€HotpotQAã€MuSiQueç­‰å¤šè·³é—®ç­”æ•°æ®é›†ï¼Œç»“åˆR1 - Searcherã€Search - R1ç­‰å½“ä»£å¤§è¯­è¨€æ¨¡å‹ï¼Œå¼€å±•ä¸‰æ­¥å®éªŒé‡åŒ–â€œè¿‡åº¦æœç´¢â€ä¸â€œæœç´¢ä¸è¶³â€ã€‚é€šè¿‡é€æ­¥éª¤åˆ†æï¼Œæ˜ç¡®æ¨¡å‹åœ¨å“ªäº›æœç´¢æ­¥éª¤å±äºå†—ä½™ï¼ˆè¿‡åº¦æœç´¢ç‡ï¼‰ã€å“ªäº›éæœç´¢æ­¥éª¤å› æœªæ£€ç´¢å¿…è¦ä¿¡æ¯å¯¼è‡´é”™è¯¯ï¼ˆæœç´¢ä¸è¶³ç‡ï¼‰ï¼Œæ­ç¤ºè¿™äº›ä½æ•ˆè¡Œä¸ºåœ¨å¤šæ•°æ®é›†å’Œæ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼ˆå¦‚Search - R1æœ‰27.7%çš„æœç´¢æ­¥éª¤æœ¬å¯é¿å…æœç´¢ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ­ç¤ºçŸ¥è¯†è¾¹ç•Œä¸ç¡®å®šæ€§ä¸æœç´¢ä½æ•ˆçš„å…³è”  
æ¢ç©¶æ¨¡å‹å¯¹è‡ªèº«â€œçŸ¥è¯†è¾¹ç•Œï¼ˆå·²çŸ¥ä¸éœ€æ£€ç´¢ä¿¡æ¯çš„ç•Œé™ï¼‰â€çš„è®¤çŸ¥ä¸æœç´¢æ•ˆç‡çš„è”ç³»ã€‚åˆ†æåŸºäºQwen2.5 - 3Bçš„ä¸åŒSearch - R1æ¨¡å‹ï¼ˆå«PPOã€GRPOè®­ç»ƒåŠBaseã€Instructå˜ä½“ï¼‰ï¼Œä»¥æœç´¢æŸ¥è¯¢tokençš„æœ€å°æ¦‚ç‡è¡¡é‡æ¨¡å‹å¯¹çŸ¥è¯†è¾¹ç•Œçš„â€œç¡®å®šæ€§â€ï¼Œå‘ç°ç”Ÿæˆæ—¶å¯¹æœç´¢å¿…è¦æ€§â€œç¡®å®šæ€§â€è¶Šé«˜ï¼ˆä¸ç¡®å®šæ€§è¶Šä½ï¼‰ï¼Œæœ€ç»ˆå›ç­”å‡†ç¡®ç‡è¶Šé«˜ï¼ˆå¦‚Bamboogleæ•°æ®é›†ä¸Šé«˜ç¡®å®šæ€§ç»„æ¯”ä½ç»„å‡†ç¡®ç‡é«˜6%ï¼‰ï¼Œè¯æ˜çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›æ˜¯æ”¹å–„æœç´¢ä½æ•ˆçš„å…³é”®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºÎ² - GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•  
åŸºäºGRPOï¼ˆShao et al., 2024ï¼‰æ”¹è¿›ï¼Œå°†æœç´¢è°ƒç”¨çš„ç½®ä¿¡åº¦å»ºæ¨¡ä¸ºæ¨¡å‹ç”Ÿæˆæœç´¢æŸ¥è¯¢çš„æœ€å°tokenæ¦‚ç‡ï¼Œå¹¶åœ¨å¥–åŠ±å‡½æ•°ä¸­åŠ å…¥ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä»…å¥–åŠ±é«˜ç¡®å®šæ€§ä¸”èƒ½å¯¼å‘æ­£ç¡®ç­”æ¡ˆçš„æœç´¢å†³ç­–ï¼Œè®­ç»ƒæ¨¡å‹æ›´ç²¾å‡†è¯„ä¼°çŸ¥è¯†çŠ¶æ€ã€å‡å°‘ä¸å¿…è¦ä¸ç¡®å®šæ€§ï¼Œä»è€Œç¼“è§£è¿‡åº¦ä¸ä¸è¶³æœç´¢ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨7ä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒÎ² - GRPOè®­ç»ƒçš„3Bè§„æ¨¡æ¨¡å‹å±•ç°æ›´å¼ºAgentic RAGèƒ½åŠ›ï¼šå¹³å‡ç²¾ç¡®åŒ¹é…åˆ†æ•°æ¯”å¼ºåŸºçº¿é«˜4%ï¼›è¿‡åº¦æœç´¢å‡å°‘1.21%ã€æœç´¢ä¸è¶³å‡å°‘7.33%ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨æå‡æœç´¢æ•ˆç‡ä¸å›ç­”å‡†ç¡®æ€§ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜é‡åŒ–æ€è·¯ï¼šå¯¹Agentic RAGä¸­æ¨¡ç³Šçš„â€œæœç´¢ä½æ•ˆâ€é—®é¢˜ï¼Œé€šè¿‡é€æ­¥éª¤æ‹†è§£ã€ç»“åˆå¤šæ•°æ®é›†å®éªŒå®ç°ç²¾å‡†é‡åŒ–ï¼Œä¸ºåŒç±»ç³»ç»Ÿæ€§èƒ½åˆ†ææä¾›èŒƒå¼ã€‚  
2. çŸ¥è¯†è¾¹ç•Œè§†è§’ï¼šä»æ¨¡å‹å¯¹è‡ªèº«çŸ¥è¯†çš„â€œè‡ªçŸ¥â€è§’åº¦è§£é‡Šæœç´¢é—®é¢˜ï¼Œå¯å‘åç»­ç ”ç©¶å…³æ³¨æ¨¡å‹å…ƒè®¤çŸ¥ï¼ˆmetacognitionï¼‰ä¸ä»»åŠ¡æ•ˆç‡çš„å…³è”ã€‚  
3. å¼ºåŒ–å­¦ä¹ æ”¹è¿›ï¼šåœ¨RLé©±åŠ¨çš„Agentic RAGè®­ç»ƒä¸­èå…¥ç½®ä¿¡åº¦æœºåˆ¶ï¼Œä¸ºæå‡æ™ºèƒ½ä½“å†³ç­–è´¨é‡æä¾›äº†ç®€å•æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ è°ƒä¼˜æ–¹å‘ã€‚  
```

## composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering
### Abstract
Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet
many suffer from monolithic designs that tightly couple core functions like
query reformulation, retrieval, reasoning, and verification. This limits their
interpretability, systematic evaluation, and targeted improvement, especially
for complex multi-hop question answering. We introduce ComposeRAG, a novel
modular abstraction that decomposes RAG pipelines into atomic, composable
modules. Each module, such as Question Decomposition, Query Rewriting,
Retrieval Decision, and Answer Verification, acts as a parameterized
transformation on structured inputs/outputs, allowing independent
implementation, upgrade, and analysis. To enhance robustness against errors in
multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that
iteratively revisits and refines earlier steps upon verification failure.
Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently
outperforms strong baselines in both accuracy and grounding fidelity.
Specifically, it achieves up to a 15% accuracy improvement over
fine-tuning-based methods and up to a 5% gain over reasoning-specialized
pipelines under identical retrieval conditions. Crucially, ComposeRAG
significantly enhances grounding: its verification-first design reduces
ungrounded answers by over 10% in low-quality retrieval settings, and by
approximately 3% even with strong corpora. Comprehensive ablation studies
validate the modular architecture, demonstrating distinct and additive
contributions from each component. These findings underscore ComposeRAG's
capacity to deliver flexible, transparent, scalable, and high-performing
multi-hop reasoning with improved grounding and interpretability.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ComposeRAGï¼šæ¨¡å—åŒ–å¯ç»„åˆçš„å¤šè·³é—®ç­”RAGæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç±»NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¾èµ–é™æ€é¢„è®­ç»ƒçŸ¥è¯†æ˜“äº§ç”Ÿé”™è¯¯ï¼ˆå¹»è§‰ï¼‰ä¸”éš¾è·å–æœ€æ–°æˆ–ç‰¹å®šé¢†åŸŸä¿¡æ¯ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è™½èƒ½æ•´åˆå¤–éƒ¨çŸ¥è¯†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œç„¶è€Œä¼ ç»ŸRAGåœ¨å¤æ‚å¤šè·³é—®ç­”æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¤šè·³é—®ç­”éœ€å¤šæ–‡æ¡£é—´åˆ†è§£ä¸é€æ­¥æ¨ç†ï¼Œç°æœ‰æ–¹æ³•å¸¸å› æ•´ä½“å¼æˆ–ä¸é€æ˜æ¶æ„ï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§ã€é€‚åº”æ€§ä¸ç³»ç»Ÿæ”¹è¿›ç©ºé—´ï¼Œå¦‚æ—©æœŸæ–¹æ¡ˆæ˜“å‡ºé”™ä¼ æ’­ï¼Œåç»­ç³»ç»Ÿåˆæœ‰ä¾èµ–å¾®è°ƒæˆ–éªŒè¯æœºåˆ¶ä¸è¶³ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´çµæ´»ã€é€æ˜ä¸”èƒ½åº”å¯¹å¤šè·³é—®ç­”çš„RAGæ¶æ„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¨¡å—åŒ–æ¶æ„è®¾è®¡
å°†RAG pipelineåˆ†è§£ä¸ºå¦‚é—®é¢˜åˆ†è§£ã€æŸ¥è¯¢é‡å†™ã€æ£€ç´¢å†³ç­–ã€ç­”æ¡ˆéªŒè¯ç­‰åŸå­åŒ–ã€å¯ç»„åˆæ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—å¯¹ç»“æ„åŒ–è¾“å…¥/è¾“å‡ºåšå‚æ•°åŒ–è½¬æ¢ï¼Œæ”¯æŒç‹¬ç«‹å®ç°ã€å‡çº§ä¸åˆ†æï¼Œä¸ºå¤šè·³é—®ç­”æä¾›çµæ´»ä¸”å¯è§£é‡Šçš„æ¨ç†ç»„ä»¶ï¼Œå®ç°â€œå³æ’å³ç”¨â€ä¸é€æ˜åŒ–åˆ†ææ”¹è¿›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¸¦è‡ªåæ€çš„ç¼–æ’ç­–ç•¥
å¼•å…¥è‡ªåæ€æœºåˆ¶å¢å¼ºå¤šæ­¥æ¨ç†é²æ£’æ€§ï¼Œå½“éªŒè¯å¤±è´¥æ—¶è¿­ä»£å›é¡¾å’Œä¼˜åŒ–æ—©æœŸæ­¥éª¤ã€‚é€šè¿‡è‡ªåæ€çš„é—®é¢˜åˆ†è§£ç­‰æ¨ç†æ­¥éª¤åè°ƒï¼Œä»æ—©æœŸé”™è¯¯ä¸­æ¢å¤ï¼Œæå‡æ•´ä½“é²æ£’æ€§ï¼Œä¸”åæ€æ­¥éª¤å¢åŠ èƒ½å¸¦æ¥æ€§èƒ½æå‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡å—ä»·å€¼éªŒè¯ä¸å¯å‡çº§æ€§
ä¸€æ–¹é¢é‡åŒ–æ ¸å¿ƒæ¨¡å—ï¼ˆå¦‚é—®é¢˜åˆ†è§£ã€æ®µè½é‡æ’ã€ç­”æ¡ˆéªŒè¯ï¼‰å„è‡ªç‹¬ç‰¹ä¸”å¯å åŠ çš„æ€§èƒ½è´¡çŒ®ï¼›å¦ä¸€æ–¹é¢éªŒè¯ç‹¬ç«‹æ¨¡å—å¯å‡çº§æ€§ï¼Œå•ç‹¬å¢å¼ºå•ä¸ªç»„ä»¶ï¼ˆå¦‚ç‰¹å®šä»»åŠ¡ç”¨æ›´å¼ºå¤§LLMï¼‰èƒ½å¸¦æ¥å¯è¡¡é‡æ”¹è¿›ï¼Œä½“ç°ç³»ç»Ÿæ‰©å±•æ€§ä¸é€‚åº”æ€§ï¼Œè¿˜æœ‰åƒæ£€ç´¢å†³ç­–è¿™ç±»æ³¨é‡æ•ˆç‡ç»„ä»¶å¯¹æ€§èƒ½å’Œèµ„æºåˆ©ç”¨æœ‰ç§¯æå½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨HotpotQAã€2WikiMultiHopQAã€MuSiQueã€Bamboogleå››ä¸ªå¤šè·³QAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒComposeRAGåœ¨å‡†ç¡®ç‡å’Œ grounding ä¿çœŸåº¦ä¸ŠæŒç»­è¶…è¶Šå¼ºåŸºçº¿ã€‚ç›¸æ¯”åŸºäºå¾®è°ƒæ–¹æ³•å‡†ç¡®ç‡æœ€å¤šæå‡15%ï¼Œç›¸åŒæ£€ç´¢æ¡ä»¶ä¸‹æ¯”æ¨ç†ä¸“ç”¨ pipeline æœ€å¤šé«˜5%ï¼›éªŒè¯ä¼˜å…ˆè®¾è®¡åœ¨ä½è´¨é‡æ£€ç´¢åœºæ™¯å‡å°‘è¶…10%æ— æ ¹æ®ç­”æ¡ˆï¼Œé«˜è´¨é‡è¯­æ–™ä¸‹ä¹Ÿçº¦é™3%ï¼›æ¶ˆèå®éªŒè¯å®æ¨¡å—åŒ–æ¶æ„ä»·å€¼ï¼Œå„ç»„ä»¶è´¡çŒ®ç‹¬ç‰¹ä¸”å¯å åŠ ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æ¶æ„è®¾è®¡ä¸Šï¼Œæ¨¡å—åŒ–æ€è·¯ä¸ºå¤æ‚ç³»ç»Ÿæ‹†è§£æä¾›èŒƒä¾‹ï¼Œä¾¿äºåˆ†æã€å‡çº§ä¸é€‚é…æ–°ä»»åŠ¡é¢†åŸŸï¼Œå¯åº”ç”¨äºéœ€å¤šç»„ä»¶åä½œçš„AIç³»ç»Ÿï¼›è‡ªåæ€æœºåˆ¶ä¸ºå¤„ç†å¤šæ­¥æ¨ç†ä¸­é”™è¯¯ä¼ æ’­é—®é¢˜æä¾›æ–°æ€è·¯ï¼Œåœ¨éœ€è¿­ä»£ä¼˜åŒ–çš„ä»»åŠ¡æµç¨‹é‡Œå€¼å¾—å‚è€ƒï¼›å¯¹æ¨¡å—ä»·å€¼é‡åŒ–ä¸å¯å‡çº§æ€§éªŒè¯ï¼Œå¼•å¯¼åç»­å·¥ä½œå…³æ³¨ç»„ä»¶çº§ä¼˜åŒ–ä¸ç³»ç»Ÿæ‰©å±•æ€§ï¼Œä¸ºæ„å»ºæ›´çµæ´»é«˜æ•ˆAIç³»ç»Ÿæä¾›äº†ä»è®¾è®¡åˆ°éªŒè¯çš„å®Œæ•´æ€è·¯å‚è€ƒã€‚
```

## cort--code-integrated-reasoning-within-thinking
### Abstract
Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | CoRTï¼šè®©å¤§æ¨ç†æ¨¡å‹åœ¨æ€è€ƒä¸­é«˜æ•ˆèåˆä»£ç æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è‡ªç„¶è¯­è¨€é•¿é“¾å¼æ€è€ƒï¼ˆCoTï¼‰ä»»åŠ¡ä¸­ï¼Œåƒ o1ã€DeepSeek - R1 è¿™ç±»å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶ï¼Œæ•ˆç‡å’Œå‡†ç¡®æ€§ä»å­˜åœ¨ä¸è¶³ã€‚è™½ç„¶å€ŸåŠ©è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—åº“ã€ç¬¦å·æ±‚è§£å™¨ï¼‰æœ‰æœ›è§£å†³è¿™äº›å±€é™ï¼Œä½†ä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰ä¼šå¼•å…¥æ¨¡å‹å†…éƒ¨æ–‡æœ¬è¡¨å¾ä¹‹å¤–çš„å¤–éƒ¨çŸ¥è¯†ï¼Œç›´æ¥ç»“åˆçš„æ•ˆç‡å¹¶ä¸é«˜ã€‚åŒæ—¶ï¼Œè¿˜é¢ä¸´ç€æ•°æ®åˆæˆå›°éš¾ï¼ˆå¦‚ o3ã€o4 - mini æœªå…¬å¼€è¯¦ç»†æ¨ç†è½¨è¿¹ï¼‰ã€åè°ƒè®¡ç®—ç²¾åº¦ä¸æŠ½è±¡æ¨ç†èƒ½åŠ›ã€è°ƒå’Œæ¨¡å‹è‡ªåæ€æœºåˆ¶ä¸å¤–éƒ¨ç²¾ç¡®çŸ¥è¯†ç­‰æŒ‘æˆ˜ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®© LRMs é«˜æ•ˆåˆ©ç”¨ CI è¿›è¡Œæ¨ç†æˆä¸ºå…³é”®é—®é¢˜ï¼Œè¿™ä¹Ÿæ˜¯æœ¬æ–‡çš„ç ”ç©¶åŠ¨æœºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šHint - Engineering è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜
ä¸ºåº”å¯¹ä»£ç é›†æˆæ¨ç†é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºéš¾é¢˜ï¼Œæœ¬æ–‡æå‡ºé€šè¿‡ Hint - Engineering åˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®ã€‚å…¶æ ¸å¿ƒæ˜¯åœ¨æ¨ç†è¿‡ç¨‹çš„åˆé€‚ä½ç½®æˆ˜ç•¥æ€§æ’å…¥ä¸åŒæç¤ºï¼Œä»¥æ­¤ä¼˜åŒ– LRM - CI äº¤äº’ã€‚ä¾‹å¦‚ï¼Œå‘ç°åœ¨æ¨¡å‹æ€è€ƒæ ‡è®°åæ’å…¥ç‰¹å®šæç¤ºï¼ˆå¦‚ â€œOkay, letâ€™s try to solve this problem step by step using multiple python code callsâ€ï¼‰èƒ½å°†ä»£ç è§¦å‘ç‡ä» 50% æå‡åˆ° 90%ã€‚å¹¶ä¸”éµå¾ª â€œè´¨é‡é‡äºæ•°é‡â€ åŸåˆ™ï¼Œæ‰‹åŠ¨åˆ›å»º 30 ä¸ªé«˜è´¨é‡ä¸”ç»è¿‡äººå·¥éªŒè¯çš„æ ·æœ¬ï¼Œä¸ºåç»­è®­ç»ƒæä¾›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé˜¶æ®µè®­ç»ƒ pipeline èµ‹èƒ½æ¨¡å‹èƒ½åŠ›
é’ˆå¯¹ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹è®¾è®¡äº†ç›¸åº”è®­ç»ƒæµç¨‹ã€‚å¯¹äº 32B å¤§å‚æ•°æ¨¡å‹ï¼Œé‡‡ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ‹’ç»å¾®è°ƒï¼ˆRFTï¼‰ï¼›å¯¹äº 1.5B å°å‚æ•°æ¨¡å‹ï¼Œå®ç°äº† SFT - RFT - å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å®Œæ•´ pipelineã€‚é€šè¿‡è¿™äº›é’ˆå¯¹æ€§çš„åè®­ç»ƒè¿‡ç¨‹ï¼Œè®©å¤§è¯­è¨€æ¨¡å‹è·å¾—å¤æ‚çš„ä»£ç é›†æˆæ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç²¾å¿ƒè®¾è®¡ç»“æœå¥–åŠ±æ¥é¼“åŠ±æ¨¡å‹æ­£ç¡®ç¼–å†™ä»£ç ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒHint - Engineering æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚DeepSeek - R1 - Distill - Qwen - 32B ç»å¯¹å‡†ç¡®ç‡æå‡ 4%ï¼ŒDeepSeek - R1 - Distill - Qwen - 1.5B æå‡ 8%ã€‚åœ¨ token æ•ˆç‡æ–¹é¢ï¼Œä¸è‡ªç„¶è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œ32B æ¨¡å‹ä½¿ç”¨çš„ token å‡å°‘çº¦ 30%ï¼Œ1.5B æ¨¡å‹å‡å°‘çº¦ 50%ã€‚ä»¥æå…·æŒ‘æˆ˜æ€§çš„ AIME åŸºå‡†æµ‹è¯•ä¸ºä¾‹ï¼Œè¯¥æ–¹æ³•åœ¨æå‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½äº† token æ¶ˆè€—ï¼Œå¦‚ 32B æ¨¡å‹åœ¨ AIME ä¸Š token æ¶ˆè€—å‡å°‘ 30%ï¼Œ1.5B æ¨¡å‹å‡å°‘ 50%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆæˆæ€è·¯ï¼šå½“é¢†åŸŸæ•°æ®ç¨€ç¼ºæ—¶ï¼Œå¯å€Ÿé‰´æœ¬æ–‡ â€œè´¨é‡ä¼˜å…ˆ + äººå·¥æ„å»º + ç­–ç•¥æ€§æç¤ºæ’å…¥â€ çš„æ–¹å¼æ¥åˆæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œä¸ºæ¨¡å‹è®­ç»ƒç­‘ç‰¢åŸºç¡€ã€‚
2. æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼šæ ¹æ®æ¨¡å‹å‚æ•°è§„æ¨¡å·®å¼‚è®¾è®¡ä¸åŒè®­ç»ƒæµç¨‹ï¼ˆå¤§æ¨¡å‹ä¾§é‡ SFTã€RFTï¼›å°æ¨¡å‹å°è¯•å®Œæ•´ SFT - RFT - RL æµç¨‹ï¼‰ï¼Œè¿™ç§æŒ‰è§„æ¨¡é€‚é…è®­ç»ƒæ–¹æ³•çš„æ€è·¯ï¼Œå¯¹ä¸åŒå¤§å°æ¨¡å‹çš„èƒ½åŠ›æå‡æœ‰å‚è€ƒä»·å€¼ã€‚
3. å·¥å…·èåˆæ¢ç´¢ï¼šåœ¨æ¨¡å‹ä¸å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœ¬æ–‡ä¸­ä»£ç è§£é‡Šå™¨ï¼‰ç»“åˆçš„åœºæ™¯ä¸‹ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹ç­‰æ‰‹æ®µä¼˜åŒ–äº¤äº’ï¼Œä¸ºè§£å†³æ¨¡å‹åˆ©ç”¨å¤–éƒ¨å·¥å…·æ—¶çš„æ•ˆç‡ã€å‡†ç¡®æ€§é—®é¢˜æä¾›äº†å®è·µèŒƒä¾‹ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–å·¥å…·ä¸æ¨¡å‹ç»“åˆçš„ç ”ç©¶ä¸­ã€‚
```

## interleaved-reasoning-for-large-language-models-via-reinforcement-learning
### Abstract
Long chain-of-thought (CoT) significantly enhances large language models'
(LLM) reasoning capabilities. However, the extensive reasoning traces lead to
inefficiencies and an increased time-to-first-token (TTFT). We propose a novel
training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs
to interleave thinking and answering for multi-hop questions. We observe that
models inherently possess the ability to perform interleaved reasoning, which
can be further enhanced through RL. We introduce a simple yet effective
rule-based reward to incentivize correct intermediate steps, which guides the
policy model toward correct reasoning paths by leveraging intermediate signals
generated during interleaved reasoning. Extensive experiments conducted across
five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)
demonstrate consistent improvements over traditional think-answer reasoning,
without requiring external tools. Specifically, our approach reduces TTFT by
over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,
our method, trained solely on question answering and logical reasoning
datasets, exhibits strong generalization ability to complex reasoning datasets
such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to
reveal several valuable insights into conditional reward modeling.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ é©±åŠ¨å¤§æ¨¡å‹çš„äº¤é”™æ¨ç†ï¼šæ•ˆç‡ä¸èƒ½åŠ›åŒæå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å€ŸåŠ©é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½æ˜¾è‘—å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†ä¼ ç»Ÿâ€œæ€è€ƒ - å›ç­”â€èŒƒå¼å­˜åœ¨ä¸¤å¤§å…³é”®ç¼ºé™·ï¼šä¸€æ˜¯ç”Ÿæˆç­”æ¡ˆå‰è¦å®Œæˆå®Œæ•´æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´é¦– token ç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰å¤§å¹…å¢åŠ ï¼Œåœ¨å®æ—¶äº¤äº’åœºæ™¯ï¼ˆå¦‚å¯¹è¯åŠ©æ‰‹ï¼‰ä¸­å½±å“ç”¨æˆ·ä½“éªŒï¼›äºŒæ˜¯å»¶è¿Ÿåˆ°æ¨ç†ç»“æŸæ‰ç”Ÿæˆç­”æ¡ˆï¼Œæ˜“è®©é”™è¯¯ä¸­é—´æ­¥éª¤ä¼ æ’­ï¼Œå¼•å‘æœ€ç»ˆç­”æ¡ˆä¸å‡†ç¡®ä¸æ¨ç†ä½æ•ˆï¼ˆå¦‚è¿‡åº¦/ä¸è¶³æ€è€ƒï¼‰ã€‚åŒæ—¶ï¼Œç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ¨ç†å‹ LLM æ—¶ï¼Œå¸¸æŠŠä¸­é—´æ¨ç†è½¨è¿¹å½“é™„å±å“ï¼Œæœªå……åˆ†åˆ©ç”¨å…¶ä¸­é—´ä¿¡å·è¾…åŠ©è®­ç»ƒä¸äº¤äº’ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©æ¨¡å‹åœ¨æ¨ç†ä¸­äº¤é”™â€œæ€è€ƒâ€ä¸â€œå›ç­”â€ã€åˆ©ç”¨ä¸­é—´ä¿¡å·ä¼˜åŒ–è®­ç»ƒå’Œäº¤äº’ï¼Œæˆä¸ºå¾…è§£éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºäº¤é”™æ¨ç† RL è®­ç»ƒèŒƒå¼  
æå‡ºâ€œäº¤é”™æ¨ç†â€è¿™ä¸€å…¨æ–° RL è®­ç»ƒèŒƒå¼ï¼Œè®© LLM åœ¨å¤šè·³é—®é¢˜æ¨ç†ä¸­äº¤æ›¿è¿›è¡Œæ€è€ƒä¸å›ç­”ï¼Œæ— éœ€å¤–éƒ¨å·¥å…·ã€‚è¯¥èŒƒå¼ä½¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆæœ‰ä¿¡æ¯é‡çš„ä¸­é—´ç­”æ¡ˆï¼Œæ—¢ç»™ç”¨æˆ·åŠæ—¶åé¦ˆï¼ˆé™ä½ TTFTï¼‰ï¼Œåˆä¸ºè‡ªèº«åç»­æ¨ç†æä¾›å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼å‘æ­£ç¡®æœ€ç»ˆç­”æ¡ˆæ¨è¿›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶  
å¼•å…¥ç®€å•æœ‰æ•ˆçš„åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œæ¿€åŠ±æ¨¡å‹ç”Ÿæˆæ­£ç¡®ä¸­é—´æ­¥éª¤ã€‚åˆ©ç”¨äº¤é”™æ¨ç†ä¸­äº§ç”Ÿçš„ä¸­é—´ä¿¡å·ï¼Œä¸ºç­–ç•¥æ¨¡å‹æŒ‡æ˜æ­£ç¡®æ¨ç†è·¯å¾„ï¼Œåœ¨è®­ç»ƒæ—¶æä¾›å¯†é›†ä¸”ä¸€è‡´çš„åé¦ˆï¼Œè§£å†³ä¼ ç»Ÿè®­ç»ƒä¸­ä¸­é—´æ­¥éª¤éš¾ credit assignmentï¼ˆ credit assignment æŒ‡è®­ç»ƒä¸­å¦‚ä½•æŠŠæœ€ç»ˆç»“æœçš„å¥–æƒ©åˆ†é…åˆ°å„ä¸­é—´æ­¥éª¤ï¼‰çš„é—®é¢˜ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ•ˆç‡ä¸å‡†ç¡®ç‡æå‡ï¼šåœ¨ 5 ä¸ªä¸åŒæ•°æ®é›†ä¸Šï¼Œç”¨ PPOã€GRPOã€REINFORCE++ ä¸‰ç§ RL ç®—æ³•å®éªŒï¼Œç›¸æ¯”ä¼ ç»Ÿâ€œæ€è€ƒ - å›ç­”â€æ¨ç†ï¼Œå¹³å‡ TTFT é™ä½è¶… 80%ï¼›Pass@1 å‡†ç¡®ç‡æœ€å¤šæå‡ 19.3%ã€‚  
2. æ³›åŒ–èƒ½åŠ›éªŒè¯ï¼šä»…åœ¨é—®ç­”å’Œé€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ¨¡å‹å¯¹ MATHã€GPQAã€MMLU ç­‰å¤æ‚æ¨ç†æ•°æ®é›†å±•ç°å¼ºæ³›åŒ–èƒ½åŠ›ã€‚  
3. å¥–åŠ±å»ºæ¨¡æ´å¯Ÿï¼šæ·±å…¥åˆ†æ conditional reward modelingï¼Œå¾—åˆ°å…³äºå¥–åŠ±å»ºæ¨¡ã€ç¨³å®š RL è®­ç»ƒå’Œæ¨¡å‹æ¨ç†åŠ¨æ€çš„æœ‰ä»·å€¼è§è§£ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è®­ç»ƒèŒƒå¼åˆ›æ–°ï¼šâ€œäº¤é”™æ¨ç†â€èŒƒå¼ä¸ºä¼˜åŒ–å¤§æ¨¡å‹æ¨ç†æ—¶çš„äº¤äº’æ•ˆç‡ä¸è®­ç»ƒæœ‰æ•ˆæ€§æä¾›æ–°æ€è·¯ï¼Œæ‰“ç ´â€œå…ˆå®Œæ•´æ€è€ƒå†å›ç­”â€çš„å›ºå®šé¡ºåºï¼Œå¯å‘åç»­æ¢ç´¢æ›´çµæ´»çš„æ¨ç†äº¤äº’æ¨¡å¼ã€‚  
2. å¥–åŠ±æœºåˆ¶è®¾è®¡ï¼šåŸºäºè§„åˆ™çš„ç®€å•å¥–åŠ±åœ¨é¿å…å¤æ‚ reward model è®­ç»ƒçš„åŒæ—¶ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸­é—´ä¿¡å·å¼•å¯¼è®­ç»ƒï¼Œè¯æ˜æ— éœ€å¤æ‚æ¨¡å‹ä¹Ÿèƒ½æŒ–æ˜ä¸­é—´æ­¥éª¤ä»·å€¼ï¼Œä¸ºè½»é‡åŒ–å¥–åŠ±è®¾è®¡æä¾›å‚è€ƒã€‚  
3. æ³›åŒ–æ€§æ¢ç´¢ï¼šä»…ç”¨åŸºç¡€æ¨ç†ç±»æ•°æ®é›†è®­ç»ƒå´èƒ½æ³›åŒ–åˆ°å¤æ‚ä»»åŠ¡ï¼Œè¯´æ˜è¯¥æ–¹æ³•æŠ“å‡†äº†æ¨ç†èƒ½åŠ›çš„å…±æ€§æœ¬è´¨ï¼Œä¸ºå¤§æ¨¡å‹è·¨ä»»åŠ¡æ¨ç†èƒ½åŠ›åŸ¹å…»æä¾›å®è·µèŒƒä¾‹ã€‚  
```

