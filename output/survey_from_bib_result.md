# Paper List from BIB File: tmpqqiiq0uq.bib
- [25/05] **T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT**  
[[Paper](http://arxiv.org/pdf/2505.00703v1)] [[Code/Page](https://github.com/CaraJ7/T2I-R1)] [[TLDR/Notes](#t2i-r1--reinforcing-image-generation-with-collaborative-semantic-level-and-token-level-cot)]

- [25/02] **Atom of Thoughts for Markov LLM Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2502.12018v2)] [[Code/Page](https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.)] [[TLDR/Notes](#atom-of-thoughts-for-markov-llm-test-time-scaling)]

- [25/04] **TTRL: Test-Time Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.16084v2)] [[Code/Page](https://github.com/PRIME-RL/TTRL)] [[TLDR/Notes](#ttrl--test-time-reinforcement-learning)]

- [25/04] **A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility**  
[[Paper](http://arxiv.org/pdf/2504.07086v1)] [[Code/Page]()] [[TLDR/Notes](#a-sober-look-at-progress-in-language-model-reasoning--pitfalls-and-paths-to-reproducibility)]

- [25/04] **ToolRL: Reward is All Tool Learning Needs**  
[[Paper](http://arxiv.org/pdf/2504.13958v1)] [[Code/Page]()] [[TLDR/Notes](#toolrl--reward-is-all-tool-learning-needs)]



# TLDR/Notes
## t2i-r1--reinforcing-image-generation-with-collaborative-semantic-level-and-token-level-cot
### Abstract
Recent advancements in large language models have demonstrated how
chain-of-thought (CoT) and reinforcement learning (RL) can improve performance.
However, applying such reasoning strategies to the visual generation domain
remains largely unexplored. In this paper, we present T2I-R1, a novel
reasoning-enhanced text-to-image generation model, powered by RL with a
bi-level CoT reasoning process. Specifically, we identify two levels of CoT
that can be utilized to enhance different stages of generation: (1) the
semantic-level CoT for high-level planning of the prompt and (2) the
token-level CoT for low-level pixel processing during patch-by-patch
generation. To better coordinate these two levels of CoT, we introduce
BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes
both generation CoTs within the same training step. By applying our reasoning
strategies to the baseline model, Janus-Pro, we achieve superior performance
with 13% improvement on T2I-CompBench and 19% improvement on the WISE
benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available
at: https://github.com/CaraJ7/T2I-R1
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œT2I-R1ï¼šååŒè¯­ä¹‰çº§ä¸æ ‡è®°çº§é“¾å¼æ€ç»´å¢å¼ºçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰å¤šä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç»“åˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ¨ç†ç­–ç•¥åº”ç”¨äºè§†è§‰ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶è¿˜ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•å°†CoTå’ŒRLåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ï¼Œä»¥æå‡å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†T2I-R1æ¨¡å‹ï¼Œä¸€ç§åŸºäºRLçš„åŒçº§åˆ«CoTæ¨ç†çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹åˆ©ç”¨ä¸¤ä¸ªçº§åˆ«çš„CoTæ¥å¢å¼ºä¸åŒé˜¶æ®µçš„ç”Ÿæˆè¿‡ç¨‹ï¼šè¯­ä¹‰çº§CoTç”¨äºç”Ÿæˆå‰çš„é«˜çº§åˆ«è§„åˆ’ï¼Œè€Œæ ‡è®°çº§CoTåˆ™ç”¨äºç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½çº§åˆ«åƒç´ å¤„ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†æ›´å¥½åœ°åè°ƒè¿™ä¸¤ä¸ªçº§åˆ«çš„CoTï¼Œæœ¬æ–‡å¼•å…¥äº†BiCoT-GRPOæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å¤šç§ç”Ÿæˆå¥–åŠ±çš„RLæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨åŒä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ— ç¼ä¼˜åŒ–ä¸¤ä¸ªç”ŸæˆCoTã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å°†æ¨ç†ç­–ç•¥åº”ç”¨äºåŸºçº¿æ¨¡å‹Janus-Proï¼ŒT2I-R1æ¨¡å‹åœ¨T2I-CompBenchåŸºå‡†ä¸Šå®ç°äº†13%çš„æ€§èƒ½æå‡ï¼Œåœ¨WISEåŸºå‡†ä¸Šå®ç°äº†19%çš„æ€§èƒ½æå‡ï¼Œç”šè‡³è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹FLUXã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºè§†è§‰ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ¨ç†å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¯­ä¹‰çº§å’Œæ ‡è®°çº§çš„CoTï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå‡å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒBiCoT-GRPOæ–¹æ³•çš„æå‡ºï¼Œä¸ºå¦‚ä½•åŒæ—¶ä¼˜åŒ–ä¸åŒçº§åˆ«çš„æ¨ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºè¿›ä¸€æ­¥æ¢ç´¢è§†è§‰ç”Ÿæˆé¢†åŸŸçš„æ¨ç†ç­–ç•¥å…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ã€‚

## atom-of-thoughts-for-markov-llm-test-time-scaling
### Abstract
Large Language Models (LLMs) achieve superior performance through
training-time scaling, and test-time scaling further enhances their
capabilities by conducting effective reasoning during inference. However, as
the scale of reasoning increases, existing test-time scaling methods suffer
from accumulated historical information, which not only wastes computational
resources but also interferes with effective reasoning. To address this issue,
we observe that complex reasoning can be achieved by solving a series of
independent and self-contained subquestions. These subquestions are essentially
\textit{atomic questions}, exhibiting the memoryless property similar to Markov
processes. Based on this observation, we propose Atom of Thoughts (\our), where
each state transition consists of decomposing the current question into a
dependency-based directed acyclic graph and contracting its subquestions,
forming a simplified question that maintains answer equivalence with the
original problem. This answer preservation enables the iterative
\textit{decomposition-contraction} process to naturally form a meaningful
Markov reasoning process. Furthermore, these atomic states can be seamlessly
integrated into existing test-time scaling methods, enabling \our to serve as a
plug-in enhancement for improving reasoning capabilities. Experiments across
six benchmarks demonstrate the effectiveness of \our both as a standalone
framework and a plug-in enhancement. Notably, on HotpotQA, when applied to
gpt-4o-mini, \our achieves an \textbf{80.6\%} F1 score, surpassing o3-mini by
\textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}. The code is available at
\href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œåŸå­æ€ç»´â€åŠ©åŠ›å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡æå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡è®­ç»ƒæ—¶çš„è§„æ¨¡æ‰©å±•å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè€Œæµ‹è¯•æ—¶çš„è§„æ¨¡æ‰©å±•åˆ™é€šè¿‡æœ‰æ•ˆçš„æ¨ç†è¿›ä¸€æ­¥å¢å¼ºäº†å…¶èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€æ¨ç†è§„æ¨¡çš„å¢åŠ ï¼Œç°æœ‰çš„æµ‹è¯•æ—¶è§„æ¨¡æ‰©å±•æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿‡åº¦ç»´æŠ¤å†å²ä¿¡æ¯ï¼Œè¿™ä¸ä»…æµªè´¹äº†è®¡ç®—èµ„æºï¼Œè¿˜å¹²æ‰°äº†æœ‰æ•ˆçš„æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†â€œåŸå­æ€ç»´â€ï¼ˆAtom of Thoughtsï¼Œç®€ç§°AOTï¼‰æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
è§‚å¯Ÿåˆ°å¤æ‚æ¨ç†å¯ä»¥é€šè¿‡è§£å†³ä¸€ç³»åˆ—ç‹¬ç«‹ä¸”è‡ªåŒ…å«çš„å­é—®é¢˜æ¥å®ç°ã€‚è¿™äº›å­é—®é¢˜æœ¬è´¨ä¸Šæ˜¯å…·æœ‰é©¬å°”å¯å¤«æ€§è´¨çš„åŸå­é—®é¢˜ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼ŒAOTæ¡†æ¶é€šè¿‡å°†å½“å‰é—®é¢˜åˆ†è§£ä¸ºä¾èµ–å…³ç³»çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œç„¶åå°†å…¶å­é—®é¢˜å‹ç¼©ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„ç®€åŒ–é—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜ä¸åŸå§‹é—®é¢˜åœ¨ç­”æ¡ˆä¸Šç­‰ä»·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
AOTæ¡†æ¶ä¸­çš„æ¯ä¸ªçŠ¶æ€è½¬æ¢éƒ½åŒ…æ‹¬é—®é¢˜çš„åˆ†è§£å’Œå‹ç¼©è¿‡ç¨‹ï¼Œè¿™æ ·è¿­ä»£è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°å¯ä»¥ç›´æ¥è§£å†³çš„åŸå­é—®é¢˜ã€‚è¿™ç§è®¾è®¡ä½¿å¾—AOTåœ¨æ‰©å±•è®¡ç®—èµ„æºæ—¶æ— éœ€ç»´æŠ¤å†å²ä¿¡æ¯ï¼ŒåŒæ—¶è¿™äº›åŸå­çŠ¶æ€å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æµ‹è¯•æ—¶è§„æ¨¡æ‰©å±•æ–¹æ³•ä¸­ï¼Œä½œä¸ºæé«˜æ¨ç†èƒ½åŠ›çš„æ’ä»¶å¢å¼ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAOTæ¡†æ¶ä½œä¸ºç‹¬ç«‹æ¡†æ¶æˆ–æ’ä»¶å¢å¼ºéƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨HotpotQAæ•°æ®é›†ä¸Šï¼Œå½“åº”ç”¨äºgpt-4o-miniæ—¶ï¼ŒAOTå®ç°äº†80.6%çš„F1åˆ†æ•°ï¼Œè¶…è¿‡äº†o3-miniçš„3.4%å’ŒDeepSeek-R1çš„10.6%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„AOTæ¡†æ¶ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå³é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºåŸå­é—®é¢˜ï¼Œå‡å°‘å†å²ä¿¡æ¯çš„ç»´æŠ¤ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAOTæ¡†æ¶çš„è®¾è®¡å…è®¸å…¶ä½œä¸ºæ’ä»¶è½»æ¾é›†æˆåˆ°ç°æœ‰çš„æ¨ç†æ–¹æ³•ä¸­ï¼Œä¸ºç°æœ‰æ–¹æ³•å¸¦æ¥æ€§èƒ½æå‡ã€‚è¿™ä¸€æ¡†æ¶çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯¹äºæœªæ¥çš„è¯­è¨€æ¨¡å‹æ¨ç†ç ”ç©¶å’Œåº”ç”¨å…·æœ‰å¯å‘å’Œå‚è€ƒä»·å€¼ã€‚

## ttrl--test-time-reinforcement-learning
### Abstract
This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the maj@n metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model maj@n,
and approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
### ğŸŒŸ è®ºæ–‡è§£è¯» | "æ¢ç´¢æ— æ ‡æ³¨æ•°æ®çš„å¼ºåŒ–å­¦ä¹ ï¼šTTRLæ–¹æ³•å¼•é¢†AIè‡ªæˆ‘è¿›åŒ–æ–°ç¯‡ç« "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºé•¿é“¾å¼æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºé‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¤§å¤šä¾èµ–äºæ ‡æ³¨æ•°æ®ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨è¾ƒå¤§å±€é™æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ç§åœ¨æ— æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œå³TTRLï¼ˆTest-Time Reinforcement Learningï¼‰ï¼Œä»¥æ¨åŠ¨AIç³»ç»Ÿçš„è‡ªæˆ‘è¿›åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
TTRLé€šè¿‡åœ¨æµ‹è¯•æ—¶å¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨é‡å¤é‡‡æ ·ç­–ç•¥æ¥å‡†ç¡®ä¼°è®¡æ ‡ç­¾å¹¶è®¡ç®—åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œä»è€Œå®ç°åœ¨æ— æ ‡æ³¨æ•°æ®ä¸Šçš„å¼ºåŒ–å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
TTRLé‡‡ç”¨å¤šæ•°æŠ•ç¥¨æ–¹æ³•æ¥ä¼°è®¡å¥–åŠ±ï¼Œè¿™ç§æ–¹æ³•åœ¨ç¼ºä¹çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æä¾›å¥–åŠ±ä¿¡å·ï¼Œä½¿å¾—å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹æ—¢é«˜æ•ˆåˆç¨³å®šã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œåº”ç”¨TTRLæ–¹æ³•å¯¹Qwen2.5-Math-7Bæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œåœ¨AIME 2024ä»»åŠ¡ä¸Šçš„é€šè¿‡ç‡æé«˜äº†211%ï¼ˆä»12.9æå‡åˆ°40.2ï¼‰ï¼Œåœ¨AIME 2024ã€AMCã€MATH-500å’ŒGPQAä»»åŠ¡ä¸Šçš„å¹³å‡æå‡è¾¾åˆ°äº†76%ã€‚è¿™äº›æ”¹è¿›å®Œå…¨æ˜¯é€šè¿‡æ¨¡å‹çš„è‡ªæˆ‘è¿›åŒ–å®ç°çš„ï¼Œæ²¡æœ‰ä½¿ç”¨ä»»ä½•æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TTRLæ–¹æ³•å±•ç¤ºäº†åœ¨æ— ç›‘ç£ç¯å¢ƒä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„å¯èƒ½æ€§ï¼Œä¸ºå‡å°‘å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒTTRLåœ¨ä¸åŒè§„æ¨¡å’Œç±»å‹çš„æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚

## a-sober-look-at-progress-in-language-model-reasoning--pitfalls-and-paths-to-reproducibility
### Abstract
Reasoning has emerged as the next major frontier for language models (LMs),
with rapid advances from both academic and industrial labs. However, this
progress often outpaces methodological rigor, with many evaluations relying on
benchmarking practices that lack transparency, robustness, or statistical
grounding. In this work, we conduct a comprehensive empirical study and find
that current mathematical reasoning benchmarks are highly sensitive to subtle
implementation choices - including decoding parameters, random seeds, prompt
formatting, and even hardware and software-framework configurations.
Performance gains reported in recent studies frequently hinge on unclear
comparisons or unreported sources of variance. To address these issues, we
propose a standardized evaluation framework with clearly defined best practices
and reporting standards. Using this framework, we reassess recent methods and
find that reinforcement learning (RL) approaches yield only modest improvements
- far below prior claims - and are prone to overfitting, especially on
small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)
methods show consistently stronger generalization. To foster reproducibility,
we release all code, prompts, and model outputs, for reasoning benchmarks,
establishing more rigorous foundations for future work.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦è¯­è¨€æ¨¡å‹æ¨ç†è¿›å±•çš„å†·é™è§‚å¯Ÿï¼šè¯¯åŒºä¸å¯é‡å¤æ€§è·¯å¾„

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ·±åº¦è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„å¿«é€Ÿå‘å±•ï¼Œå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›å±•å¾€å¾€ä¼´éšç€æ–¹æ³•è®ºä¸Šçš„ä¸å¤Ÿä¸¥è°¨ï¼Œè®¸å¤šè¯„ä¼°ä¾èµ–äºç¼ºä¹é€æ˜åº¦ã€é²æ£’æ€§æˆ–ç»Ÿè®¡åŸºç¡€çš„åŸºå‡†æµ‹è¯•å®è·µã€‚æœ¬æ–‡æ—¨åœ¨å¯¹å½“å‰è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†é¢†åŸŸçš„åŸºå‡†æµ‹è¯•è¿›è¡Œå…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæ­ç¤ºå…¶ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶æå‡ºæ”¹è¿›å»ºè®®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å‘ç°å½“å‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å¯¹ç»†å¾®çš„å®ç°é€‰æ‹©é«˜åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬è§£ç å‚æ•°ã€éšæœºç§å­ã€æç¤ºæ ¼å¼ï¼Œç”šè‡³ç¡¬ä»¶å’Œè½¯ä»¶æ¡†æ¶é…ç½®ã€‚è¿™äº›å› ç´ å¯èƒ½å¯¼è‡´æ€§èƒ½æŒ‡æ ‡çš„æ˜¾è‘—æ³¢åŠ¨ï¼ŒæŒ‘æˆ˜äº†å·²å‘è¡¨ç»“æœçš„å¯é æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€å¥—æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ˜ç¡®å®šä¹‰çš„æœ€ä½³å®è·µå’ŒæŠ¥å‘Šæ ‡å‡†ã€‚ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œä½œè€…é‡æ–°è¯„ä¼°äº†æœ€è¿‘çš„æ–¹æ³•ï¼Œå¹¶å‘ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä»…å¸¦æ¥æœ‰é™çš„æ”¹è¿›ï¼Œä¸”å®¹æ˜“åœ¨å°è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸Šè¿‡æ‹Ÿåˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•è¡¨ç°å‡ºæ›´ä¸€è‡´çš„ä¸€èˆ¬åŒ–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶é‡æ–°è¯„ä¼°äº†æœ€è¿‘çš„æ–¹æ³•ï¼Œå‘ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æ”¹è¿›è¿œä½äºä¹‹å‰çš„å®£ç§°ï¼Œå¹¶ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚è€Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨å„ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†æ›´ç¨³å®šå’Œä¸€èˆ¬åŒ–çš„æ”¹è¿›ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§ï¼Œä½œè€…å…¬å¼€äº†æ‰€æœ‰ä»£ç ã€æç¤ºå’Œæ¨¡å‹è¾“å‡ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æé†’æˆ‘ä»¬ï¼Œåœ¨è¯„ä¼°æ·±åº¦è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ—¶ï¼Œéœ€è¦æ›´åŠ ä¸¥è°¨å’Œé€æ˜çš„æ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- åœ¨è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œåº”ä»”ç»†æ§åˆ¶å®éªŒæ¡ä»¶ï¼ŒåŒ…æ‹¬è§£ç å‚æ•°ã€éšæœºç§å­ç­‰ã€‚
- å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„åº”ç”¨éœ€è¦è°¨æ…è¯„ä¼°ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚
- ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸Šå…·æœ‰ç¨³å®šæ€§å’Œä¸€èˆ¬åŒ–ä¼˜åŠ¿ã€‚
- ä¸ºäº†æé«˜ç ”ç©¶çš„å¯é‡å¤æ€§ï¼Œåº”å…¬å¼€æ‰€æœ‰å®éªŒä»£ç å’Œç»“æœã€‚

## toolrl--reward-is-all-tool-learning-needs
### Abstract
Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å·¥å…·å­¦ä¹ ï¼Œå¥–åŠ±æœºåˆ¶è‡³å…³é‡è¦ï¼šæ¢ç´¢LLMå·¥å…·ä½¿ç”¨çš„æ–°è·¯å¾„

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è·å–å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSFTåœ¨åº”å¯¹ä¸ç†Ÿæ‚‰æˆ–å¤æ‚çš„å·¥å…·ä½¿ç”¨åœºæ™¯æ—¶å­˜åœ¨æ³›åŒ–éš¾é¢˜ã€‚å°½ç®¡æœ€è¿‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯R1-likeæ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æ¨ç†å’Œæ³›åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œä½†å·¥å…·ä½¿ç”¨çš„å¥–åŠ±è®¾è®¡å´é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼šå¤šç§å·¥å…·å¯èƒ½è¢«è°ƒç”¨ï¼Œä¸”å‚æ•°å„ä¸ç›¸åŒï¼Œè€Œç²—ç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼Œå¦‚ç­”æ¡ˆåŒ¹é…ï¼Œæ— æ³•æä¾›æœ‰æ•ˆå­¦ä¹ æ‰€éœ€çš„ç»†ç²’åº¦åé¦ˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹å·¥å…·é€‰æ‹©å’Œåº”ç”¨ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡ï¼Œå¹¶æ¢ç´¢å…¶åœ¨RLæ¡†æ¶ä¸‹çš„æ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æ˜¯é¦–ä¸ªå…³äºå·¥å…·é€‰æ‹©å’Œåº”ç”¨ä»»åŠ¡å¥–åŠ±è®¾è®¡çš„å…¨é¢ç ”ç©¶ã€‚ä½œè€…ç³»ç»Ÿåœ°æ¢ç´¢äº†å¤šç§å¥–åŠ±ç­–ç•¥ï¼Œåˆ†æäº†å®ƒä»¬çš„ç±»å‹ã€å°ºåº¦ã€ç²’åº¦å’Œæ—¶é—´åŠ¨æ€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åŸºäºè¿™äº›æ´å¯Ÿï¼Œä½œè€…æå‡ºäº†ä¸€ç§é’ˆå¯¹å·¥å…·ä½¿ç”¨ä»»åŠ¡çš„åŸç†æ€§å¥–åŠ±è®¾è®¡ï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMçš„è®­ç»ƒï¼Œä½¿ç”¨äº†ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•èƒ½å¤Ÿå®ç°ç¨³å¥ã€å¯æ‰©å±•å’Œç¨³å®šçš„è®­ç»ƒï¼Œæ¯”åŸºç¡€æ¨¡å‹æé«˜äº†17%ï¼Œæ¯”SFTæ¨¡å‹æé«˜äº†15%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±æœºåˆ¶åœ¨å¢å¼ºLLMå·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºå·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä»»åŠ¡çš„RLè®­ç»ƒæä¾›äº†é¦–ä¸ªå®è¯è·¯çº¿å›¾ï¼Œä¸ºæœªæ¥LLMä»£ç†è®­ç»ƒçš„ç ”ç©¶æä¾›äº†ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- é•¿çš„æ¨ç†è½¨è¿¹å¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼Œé•¿åº¦å¥–åŠ±å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚
- åŠ¨æ€å¥–åŠ±å°ºåº¦æœ‰åŠ©äºæ¨¡å‹å¹³æ»‘åœ°ä»ç®€å•è¡Œä¸ºè¿‡æ¸¡åˆ°å¤æ‚è¡Œä¸ºã€‚
- ç»†ç²’åº¦çš„å¥–åŠ±åˆ†è§£å¯ä»¥å¸¦æ¥æ›´ç¨³å®šå’Œæœ‰æ•ˆçš„å­¦ä¹ ã€‚

æ­¤å¤–ï¼Œæ‰€æœ‰ä»£ç éƒ½å·²å…¬å¼€ï¼Œä»¥ä¾¿æœªæ¥çš„ç ”ç©¶èƒ½å¤Ÿåœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ¢ç´¢ã€‚

