# Paper List from BIB File: tmppqmmjdk9.bib
- [24/05] **Instruction-Guided Visual Masking**  
[[Paper](http://arxiv.org/pdf/2405.19783v2)] [[Code/Page](https://github.com/2toinf/IVM.)] [[TLDR/Notes](#instruction-guided-visual-masking)]

- [25/05] **OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.08617v1)] [[Code/Page]()] [[TLDR/Notes](#openthinkimg--learning-to-think-with-images-via-visual-tool-reinforcement-learning)]

- [25/05] **Visual Agentic Reinforcement Fine-Tuning**  
[[Paper](http://arxiv.org/pdf/2505.14246v1)] [[Code/Page]()] [[TLDR/Notes](#visual-agentic-reinforcement-fine-tuning)]

- [25/05] **Grounded Reinforcement Learning for Visual Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.23678v1)] [[Code/Page]()] [[TLDR/Notes](#grounded-reinforcement-learning-for-visual-reasoning)]

- [25/02] **MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs**  
[[Paper](http://arxiv.org/pdf/2502.17422v1)] [[Code/Page]()] [[TLDR/Notes](#mllms-know-where-to-look--training-free-perception-of-small-visual-details-with-multimodal-llms)]

- [25/05] **DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.14362v2)] [[Code/Page](https://github.com/Visual-Agent/DeepEyes.)] [[TLDR/Notes](#deepeyes--incentivizing--thinking-with-images--via-reinforcement-learning)]

- [25/05] **Visual Abstract Thinking Empowers Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.20164v2)] [[Code/Page]()] [[TLDR/Notes](#visual-abstract-thinking-empowers-multimodal-reasoning)]

- [25/05] **Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought**  
[[Paper](http://arxiv.org/pdf/2505.15510v1)] [[Code/Page]()] [[TLDR/Notes](#visual-thoughts--a-unified-perspective-of-understanding-multimodal-chain-of-thought)]

- [24/11] **ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration**  
[[Paper](http://arxiv.org/pdf/2411.16044v1)] [[Code/Page](https://github.com/om-ai-lab/ZoomEye}{https://github.com/om-ai-lab/ZoomEye}.)] [[TLDR/Notes](#zoomeye--enhancing-multimodal-llms-with-human-like-zooming-capabilities-through-tree-based-image-exploration)]

- [24/06] **Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models**  
[[Paper](http://arxiv.org/pdf/2406.09403v3)] [[Code/Page](https://visualsketchpad.github.io/.)] [[TLDR/Notes](#visual-sketchpad--sketching-as-a-visual-chain-of-thought-for-multimodal-language-models)]

- [25/03] **Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG**  
[[Paper](http://arxiv.org/pdf/2503.01222v2)] [[Code/Page]()] [[TLDR/Notes](#retrieval-augmented-perception--high-resolution-image-perception-meets-visual-rag)]

- [25/06] **Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing**  
[[Paper](http://arxiv.org/pdf/2506.09965v2)] [[Code/Page]()] [[TLDR/Notes](#reinforcing-spatial-reasoning-in-vision-language-models-with-interwoven-thinking-and-visual-drawing)]

- [25/04] **DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding**  
[[Paper](http://arxiv.org/pdf/2504.14920v1)] [[Code/Page](https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025)] [[TLDR/Notes](#dyfo--a-training-free-dynamic-focus-visual-search-for-enhancing-lmms-in-fine-grained-visual-understanding)]

- [23/12] **V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs**  
[[Paper](http://arxiv.org/pdf/2312.14135v2)] [[Code/Page](https://github.com/penghao-wu/vstar.)] [[TLDR/Notes](#v*--guided-visual-search-as-a-core-mechanism-in-multimodal-llms)]

- [24/06] **From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis**  
[[Paper](http://arxiv.org/pdf/2406.19934v2)] [[Code/Page](https://github.com/steven-ccq/VisualReasoner.)] [[TLDR/Notes](#from-the-least-to-the-most--building-a-plug-and-play-visual-reasoner-via-data-synthesis)]

- [25/05] **Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL**  
[[Paper](http://arxiv.org/pdf/2505.15436v1)] [[Code/Page]()] [[TLDR/Notes](#chain-of-focus--adaptive-visual-search-and-zooming-for-multimodal-reasoning-via-rl)]

- [25/05] **VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought**  
[[Paper](http://arxiv.org/pdf/2505.16192v2)] [[Code/Page]()] [[TLDR/Notes](#vlm-r$^3$--region-recognition--reasoning--and-refinement-for-enhanced-multimodal-chain-of-thought)]



# TLDR/Notes
## instruction-guided-visual-masking
### Abstract
Instruction following is crucial in contemporary LLM. However, when extended
to multimodal setting, it often suffers from misalignment between specific
textual instruction and targeted local region of an image. To achieve more
accurate and nuanced multimodal instruction following, we introduce
Instruction-guided Visual Masking (IVM), a new versatile visual grounding model
that is compatible with diverse multimodal models, such as LMM and robot model.
By constructing visual masks for instruction-irrelevant regions, IVM-enhanced
multimodal models can effectively focus on task-relevant image regions to
better align with complex instructions. Specifically, we design a visual
masking data generation pipeline and create an IVM-Mix-1M dataset with 1
million image-instruction pairs. We further introduce a new learning technique,
Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training
that prioritizes high-quality data samples. Experimental results on generic
multimodal tasks such as VQA and embodied robotic control demonstrate the
versatility of IVM, which as a plug-and-play tool, significantly boosts the
performance of diverse multimodal models, yielding new state-of-the-art results
across challenging multimodal benchmarks. Code, model and data are available at
https://github.com/2toinf/IVM.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšæ–°çªç ´ï¼šInstruction-Guided Visual Masking

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å½“ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼ŒæŒ‡ä»¤è·Ÿéšè‡³å…³é‡è¦ã€‚ä½†å½“æ‹“å±•åˆ°å¤šæ¨¡æ€åœºæ™¯æ—¶ï¼Œå¸¸å¸¸é¢ä¸´ç‰¹å®šæ–‡æœ¬æŒ‡ä»¤ä¸å›¾åƒç›®æ ‡å±€éƒ¨åŒºåŸŸä¸åŒ¹é…çš„é—®é¢˜ã€‚ä¸ºäº†å®ç°æ›´ç²¾å‡†ã€ç»†è‡´çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšï¼Œè§£å†³æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒåŒºåŸŸçš„å¯¹é½éš¾é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Instruction - guided Visual Maskingï¼ˆIVMï¼‰æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºInstruction - guided Visual Maskingï¼ˆIVMï¼‰æ¨¡å‹
IVMæ˜¯ä¸€ç§å…¨æ–°çš„é€šç”¨è§†è§‰å®šä½æ¨¡å‹ï¼Œèƒ½ä¸è¯¸å¦‚å¤§è¯­è¨€ - è§†è§‰æ¨¡å‹ï¼ˆLMMï¼‰ã€æœºå™¨äººæ¨¡å‹ç­‰å¤šç§å¤šæ¨¡æ€æ¨¡å‹å…¼å®¹ã€‚é€šè¿‡ä¸ºä¸æŒ‡ä»¤æ— å…³çš„åŒºåŸŸæ„å»ºè§†è§‰æ©ç ï¼Œç»IVMå¢å¼ºçš„å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆèšç„¦äºä¸ä»»åŠ¡ç›¸å…³çš„å›¾åƒåŒºåŸŸï¼Œä»è€Œæ›´å¥½åœ°ä¸å¤æ‚æŒ‡ä»¤å¯¹é½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºæ•°æ®ç”Ÿæˆ pipeline å’Œ IVM - Mix - 1M æ•°æ®é›†
è®¾è®¡äº†è§†è§‰æ©ç æ•°æ®ç”Ÿæˆ pipelineï¼Œåˆ›å»ºäº†åŒ…å«100ä¸‡å›¾åƒ - æŒ‡ä»¤å¯¹çš„IVM - Mix - 1Mæ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æ•°æ®æ”¯æ’‘ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºDiscriminator Weighted Supervised Learningï¼ˆDWSLï¼‰å­¦ä¹ æŠ€æœ¯
å¼•å…¥äº†ä¸€ç§æ–°çš„å­¦ä¹ æŠ€æœ¯DWSLç”¨äºä¼˜å…ˆIVMè®­ç»ƒï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿä¼˜å…ˆè€ƒè™‘é«˜è´¨é‡çš„æ•°æ®æ ·æœ¬ï¼Œæå‡è®­ç»ƒæ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨é€šç”¨å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œå…·èº«æœºå™¨äººæ§åˆ¶ï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†IVMçš„å¤šåŠŸèƒ½æ€§ã€‚ä½œä¸ºå³æ’å³ç”¨å·¥å…·ï¼ŒIVMæ˜¾è‘—æå‡äº†å¤šç§å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹å…¼å®¹æ€§æ€è·¯ï¼šIVMå…¼å®¹å¤šç§å¤šæ¨¡æ€æ¨¡å‹çš„è®¾è®¡æ€è·¯ï¼Œä¸ºåç»­å¼€å‘é€šç”¨å‹å¤šæ¨¡æ€å¢å¼ºå·¥å…·æä¾›äº†å‚è€ƒï¼Œå¯æ€è€ƒå¦‚ä½•è®©å·¥å…·åœ¨ä¸åŒå¤šæ¨¡æ€æ¨¡å‹é—´é«˜æ•ˆé€‚é…ã€‚
2. æ•°æ®æ„å»ºæ–¹æ³•ï¼šå…¶è®¾è®¡çš„è§†è§‰æ©ç æ•°æ®ç”Ÿæˆ pipeline å’Œå¤§è§„æ¨¡æ•°æ®é›†æ„å»ºæ–¹å¼ï¼Œå¯¹äºéœ€è¦å¤§é‡æ•°æ®æ”¯æ’‘çš„å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼Œæä¾›äº†æ•°æ®å±‚é¢çš„æ„å»ºèŒƒä¾‹ï¼Œå¯å­¦ä¹ å¦‚ä½•é«˜æ•ˆç”Ÿæˆé’ˆå¯¹æ€§æ•°æ®ã€‚
3. è®­ç»ƒæŠ€æœ¯åˆ›æ–°ï¼šDWSLè¿™ç§ä¼˜å…ˆè€ƒè™‘é«˜è´¨é‡æ•°æ®æ ·æœ¬çš„è®­ç»ƒæŠ€æœ¯ï¼Œä¸ºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æ•°æ®åˆ©ç”¨æ•ˆç‡çš„æå‡æä¾›äº†æ–°æ–¹å‘ï¼Œåœ¨åç»­æ¨¡å‹è®­ç»ƒä¼˜åŒ–ä¸­å¯å€Ÿé‰´è¯¥æ€è·¯æ¥å¤„ç†æ•°æ®æ ·æœ¬çš„ä¼˜å…ˆçº§é—®é¢˜ã€‚
4. å³æ’å³ç”¨å·¥å…·å±æ€§ï¼šIVMä½œä¸ºå³æ’å³ç”¨å·¥å…·æå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½çš„æ¨¡å¼ï¼Œä¸ºå¤šæ¨¡æ€é¢†åŸŸå·¥å…·å¼€å‘æä¾›äº†æ–°çš„äº§å“å½¢æ€å‚è€ƒï¼Œå¯æ¢ç´¢æ›´å¤šæ­¤ç±»èƒ½å¿«é€Ÿèµ‹èƒ½ç°æœ‰æ¨¡å‹çš„å·¥å…·å‹æ–¹æ³•ã€‚
```

## openthinkimg--learning-to-think-with-images-via-visual-tool-reinforcement-learning
### Abstract
While humans can flexibly leverage interactive visual cognition for complex
problem-solving, enabling Large Vision-Language Models (LVLMs) to learn
similarly adaptive behaviors with visual tools remains challenging. A
significant hurdle is the current lack of standardized infrastructure, which
hinders integrating diverse tools, generating rich interaction data, and
training robust agents effectively. To address these gaps, we introduce
OpenThinkIMG, the first open-source, comprehensive end-to-end framework for
tool-augmented LVLMs. It features standardized vision tool interfaces, scalable
trajectory generation for policy initialization, and a flexible training
environment. Furthermore, considering supervised fine-tuning (SFT) on static
demonstrations offers limited policy generalization for dynamic tool
invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL
to train LVLMs to learn adaptive policies for invoking external vision tools.
V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies
by directly optimizing for task success using feedback from tool interactions.
We empirically validate V-ToolRL on challenging chart reasoning tasks. Our
RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its
SFT-initialized counterpart (+28.83 points) and surpasses established
supervised tool-learning baselines like Taco and CogCom by an average of +12.7
points. Notably, it also surpasses prominent closed-source models like GPT-4.1
by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational
framework for advancing dynamic, tool-augmented visual reasoning, helping the
community develop AI agents that can genuinely "think with images".
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | OpenThinkIMGï¼šè®©å¤§è§†è§‰è¯­è¨€æ¨¡å‹å€ŸåŠ©è§†è§‰å·¥å…·â€œåƒäººç±»ä¸€æ ·æ€è€ƒå›¾åƒâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»èƒ½å¤Ÿçµæ´»è¿ç”¨äº¤äº’å¼è§†è§‰è®¤çŸ¥æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä½†è®©å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä»¥ç±»ä¼¼çš„è‡ªé€‚åº”æ–¹å¼åˆ©ç”¨è§†è§‰å·¥å…·å´é¢‡å…·æŒ‘æˆ˜ã€‚å½“å‰ç¼ºä¹æ ‡å‡†åŒ–åŸºç¡€è®¾æ–½æ˜¯ä¸€å¤§éšœç¢ï¼Œè¿™é˜»ç¢äº†å¤šæ ·å·¥å…·çš„æ•´åˆã€ä¸°å¯Œäº¤äº’æ•°æ®çš„ç”Ÿæˆä»¥åŠé²æ£’æ™ºèƒ½ä½“çš„é«˜æ•ˆè®­ç»ƒã€‚åŒæ—¶ï¼Œä»…ä¾é é™æ€æ¼”ç¤ºçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨åŠ¨æ€å·¥å…·è°ƒç”¨æ—¶æ”¿ç­–æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºå¡«è¡¥è¿™äº›ç©ºç™½ï¼Œè®ºæ–‡æå‡ºäº†ç›¸åº”è§£å†³æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºOpenThinkIMGæ¡†æ¶  
OpenThinkIMGæ˜¯é¦–ä¸ªç”¨äºå·¥å…·å¢å¼ºå‹å¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å¼€æºã€å…¨é¢çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚å®ƒå…·å¤‡æ ‡å‡†åŒ–è§†è§‰å·¥å…·æ¥å£ï¼Œèƒ½å®ç°ç­–ç•¥åˆå§‹åŒ–çš„å¯æ‰©å±•è½¨è¿¹ç”Ÿæˆï¼Œè¿˜æœ‰çµæ´»çš„è®­ç»ƒç¯å¢ƒï¼Œä¸ºå·¥å…·å¢å¼ºLVLMsæä¾›äº†åŸºç¡€æ”¯æ’‘ï¼Œè§£å†³äº†åŸºç¡€è®¾æ–½ç¼ºå¤±çš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºV - ToolRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶  
è€ƒè™‘åˆ°é™æ€æ¼”ç¤ºçš„æœ‰ç›‘ç£å¾®è°ƒå¯¹åŠ¨æ€å·¥å…·è°ƒç”¨çš„ç­–ç•¥æ³›åŒ–ä¸è¶³ï¼Œæå‡ºV - ToolRLã€‚è¯¥æ¡†æ¶è®©LVLMsé€šè¿‡åˆ©ç”¨å·¥å…·äº¤äº’çš„åé¦ˆç›´æ¥ä¼˜åŒ–ä»»åŠ¡æˆåŠŸç‡ï¼Œè‡ªä¸»å‘ç°æœ€ä¼˜å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä»¥æ­¤è®­ç»ƒLVLMså­¦ä¹ è°ƒç”¨å¤–éƒ¨è§†è§‰å·¥å…·çš„è‡ªé€‚åº”ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸Šå¯¹V - ToolRLè¿›è¡Œäº†å®è¯éªŒè¯ã€‚åŸºäºQwen2 - VL - 2Bæ„å»ºçš„ç»RLè®­ç»ƒçš„æ™ºèƒ½ä½“ï¼Œæ˜¾è‘—è¶…è¶Šå…¶SFTåˆå§‹åŒ–çš„å¯¹åº”ç‰ˆæœ¬ï¼ˆæå‡28.83åˆ†ï¼‰ï¼›å¹³å‡è¶…è¿‡åƒTacoå’ŒCogComè¿™æ ·çš„æœ‰ç›‘ç£å·¥å…·å­¦ä¹ åŸºçº¿12.7åˆ†ï¼›ç”šè‡³è¶…è¿‡äº†å¦‚GPT - 4.1è¿™æ ·çš„çŸ¥åé—­æºæ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡8.68åˆ†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŸºç¡€è®¾æ–½å»ºè®¾æ–¹é¢ï¼šOpenThinkIMGæä¾›äº†æ ‡å‡†åŒ–è§†è§‰å·¥å…·æ¥å£ç­‰åŸºç¡€è®¾æ–½æ„å»ºæ€è·¯ï¼Œä¸ºåç»­å·¥å…·å¢å¼ºå‹è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶æä¾›äº†å¯å‚è€ƒçš„æ¡†æ¶èŒƒå¼ï¼Œä¾¿äºç ”ç©¶è€…æ•´åˆå·¥å…·ã€ç”Ÿæˆæ•°æ®å’Œè®­ç»ƒæ™ºèƒ½ä½“ã€‚
2. è®­ç»ƒæ–¹æ³•åˆ›æ–°ï¼šV - ToolRLé’ˆå¯¹æœ‰ç›‘ç£å¾®è°ƒåœ¨åŠ¨æ€å·¥å…·è°ƒç”¨æ³›åŒ–ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œä¸ºè§£å†³ç±»ä¼¼çš„åŠ¨æ€åœºæ™¯ä¸‹æ¨¡å‹ç­–ç•¥å­¦ä¹ é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œå³åˆ©ç”¨äº¤äº’åé¦ˆä¼˜åŒ–ä»»åŠ¡ç›®æ ‡æ¥è®©æ¨¡å‹è‡ªä¸»å­¦ä¹ ç­–ç•¥ã€‚
3. å®éªŒéªŒè¯è§’åº¦ï¼šåœ¨å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸Šçš„å…¨é¢å¯¹æ¯”å®éªŒï¼Œä¸ºéªŒè¯å·¥å…·å¢å¼ºå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§æä¾›äº†èŒƒä¾‹ï¼Œåç»­ç ”ç©¶åœ¨éªŒè¯æ–°æ–¹æ³•æ—¶å¯å€Ÿé‰´è¿™ç§å¤šç»´åº¦å¯¹æ¯”ï¼ˆä¸è‡ªèº«SFTç‰ˆæœ¬ã€å·²æœ‰ç›‘ç£åŸºçº¿ã€é—­æºå¼ºæ¨¡å‹å¯¹æ¯”ï¼‰çš„æ–¹å¼ã€‚
```

## visual-agentic-reinforcement-fine-tuning
### Abstract
A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£é”å¤šæ¨¡æ€æ™ºèƒ½æ–°å§¿åŠ¿ï¼šVisual-ARFTè®©å¤§æ¨¡å‹â€œçœ‹å›¾æ€è€ƒ+å·¥å…·ååŒâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§æ¨¡å‹æµªæ½®ä¸­ï¼ŒClosedAIç­‰æœºæ„çš„å¤§æ¨¡å‹ï¼ˆå¦‚o3ï¼‰å·²å±•ç°å‡º**åŸç”Ÿæ™ºèƒ½ä½“èƒ½åŠ›**â€”â€”èƒ½ç”¨æµè§ˆå™¨æœç´¢ã€å†™/æ‰§è¡Œå›¾åƒæ“ä½œä»£ç æ¥å®ç°â€œä»¥å›¾æ€è€ƒâ€ã€‚ä½†å¼€æºç¤¾åŒºé‡Œï¼Œçº¯è¯­è¨€ç±»æ™ºèƒ½ä½“èƒ½åŠ›ï¼ˆå¦‚å‡½æ•°è°ƒç”¨ï¼‰è™½æœ‰è¿›å±•ï¼Œ**çœŸæ­£èåˆå›¾åƒæ€ç»´çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“èƒ½åŠ›åŠå¯¹åº”è¯„æµ‹åŸºå‡†**å´é²œæœ‰äººæ·±å…¥æ¢ç´¢ã€‚LVLMsï¼ˆå¤§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰è¦æˆä¸ºçµæ´»çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œéœ€è¦çªç ´â€œåªä¼šçœ‹å›¾æ–‡ã€ä¸ä¼šä¸»åŠ¨ç”¨å·¥å…·å¤„ç†/è·å–ä¿¡æ¯â€çš„å±€é™ï¼Œè¿™å°±æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šVisual-ARFTæŠ€æœ¯èŒƒå¼  
æå‡º**è§†è§‰æ™ºèƒ½ä½“å¼ºåŒ–å¾®è°ƒï¼ˆVisual Agentic Reinforcement Fine-Tuningï¼‰**ï¼Œè®©å¼€æºLVLMså…·å¤‡ä¸»åŠ¨è°ƒç”¨å¤–éƒ¨å·¥å…·çš„æ™ºèƒ½ä½“èƒ½åŠ›ï¼šä¸€æ˜¯**ç½‘é¡µæµè§ˆ**è·å–å®æ—¶ä¿¡æ¯ï¼ŒäºŒæ˜¯**ç¼–å†™å›¾åƒæ“ä½œä»£ç **ï¼ˆè£å‰ªã€æ—‹è½¬ç­‰å¤„ç†ä¸åˆ†æï¼‰ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ æ€è·¯ï¼Œè®©æ¨¡å‹å­¦ä¼šâ€œä½•æ—¶è¯¥è°ƒç”¨å·¥å…·ã€æ€ä¹ˆç”¨å·¥å…·ã€å¦‚ä½•ç»“åˆå·¥å…·è¾“å‡ºåšå†³ç­–â€ï¼Œå®ç°â€œä»¥å›¾æ€è€ƒ+å·¥å…·ååŒâ€çš„çµæ´»æ¨ç†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMATå¤šæ¨¡æ€æ™ºèƒ½ä½“å·¥å…·åŸºå‡†  
æ„å»º**Multi-modal Agentic Tool Benchï¼ˆMATï¼‰**è¯„æµ‹å¥—ä»¶ï¼ŒåŒ…å«ä¸¤å¤§åœºæ™¯ï¼š  
- MAT-Searchï¼šè¯„ä¼°æ¨¡å‹â€œç”¨å·¥å…·æœç´¢ä¿¡æ¯è¾…åŠ©å¤šæ¨¡æ€ä»»åŠ¡â€çš„èƒ½åŠ›ï¼›  
- MAT-Codingï¼šè¯„ä¼°æ¨¡å‹â€œç¼–å†™å›¾åƒæ“ä½œä»£ç å¹¶æ‰§è¡Œâ€çš„èƒ½åŠ›ã€‚  
å¡«è¡¥äº†å¤šæ¨¡æ€æ™ºèƒ½ä½“å·¥å…·èƒ½åŠ›è¯„æµ‹çš„ç©ºç™½ï¼Œä¸ºåç»­ç ”ç©¶æä¾›ç»Ÿä¸€â€œè€ƒåœºâ€ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è‡ªè®¾åŸºå‡†MATä¸Šï¼ŒVisual-ARFTå¯¹æ¯”åŸºçº¿æ¨¡å‹ä¼˜åŠ¿æ˜¾è‘—ï¼š  
- MAT-Codingä»»åŠ¡ï¼šF1æå‡+18.6%ã€EMï¼ˆç²¾ç¡®åŒ¹é…ï¼‰æå‡+13.0%ï¼›  
- MAT-Searchä»»åŠ¡ï¼šF1æå‡+10.3%ã€EMæå‡+8.7%ï¼›  
ç”šè‡³**è¶…è¶ŠGPT-4o**ï¼Œè¯æ˜æŠ€æœ¯åœ¨å¤šæ¨¡æ€å·¥å…·èƒ½åŠ›ä¸Šçš„ç«äº‰åŠ›ã€‚  

åœ¨è·¨é¢†åŸŸæ³›åŒ–æ€§æµ‹è¯•ï¼ˆ2Wikiã€HotpotQAç­‰å¤šè·³é—®ç­”åŸºå‡†ï¼‰ä¸­ï¼ŒVisual-ARFTä¹Ÿæ–©è·F1+29.3%ã€EM+25.9%çš„å¢ç›Šï¼ŒéªŒè¯â€œå·¥å…·ååŒå¼å¤šæ¨¡æ€æ¨ç†â€å…·å¤‡å¼ºæ³›åŒ–æ½œåŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æŠ€æœ¯è·¯çº¿å¯å¤ç”¨**ï¼šVisual-ARFTçš„â€œå¼ºåŒ–å¾®è°ƒ+å·¥å…·è°ƒç”¨â€èŒƒå¼ï¼Œä¸ºå¼€æºLVLMså‘å¤šæ¨¡æ€æ™ºèƒ½ä½“è¿›åŒ–æä¾›äº†è½åœ°è·¯å¾„ï¼Œå…¶ä»–ç ”ç©¶å¯å‚è€ƒè¯¥æ€è·¯æ‰©å±•åˆ°éŸ³é¢‘ã€è§†é¢‘ç­‰æ›´å¤šæ¨¡æ€ï¼›  
2. **è¯„æµ‹åŸºå‡†ä»·å€¼**ï¼šMATå¡«è¡¥å¤šæ¨¡æ€æ™ºèƒ½ä½“å·¥å…·èƒ½åŠ›è¯„æµ‹ç©ºç™½ï¼Œåç»­ç ”ç©¶å¯åŸºäºMATåšæ›´ç»†ç²’åº¦çš„èƒ½åŠ›æ‹†è§£æˆ–æ‰©å±•åœºæ™¯ï¼›  
3. **å·¥ä¸šè½åœ°å¯å‘**ï¼šè‹¥è¦è®©å¤§æ¨¡å‹åœ¨å®é™…ä¸šåŠ¡ï¼ˆå¦‚ç”µå•†å›¾æœã€åŒ»ç–—å½±åƒåˆ†æï¼‰ä¸­â€œä¸»åŠ¨å¹²æ´»â€ï¼Œâ€œæ•™æ¨¡å‹ç”¨å·¥å…·â€+â€œå¼ºåŒ–å­¦ä¹ å¼•å¯¼å†³ç­–â€æ˜¯å€¼å¾—å°è¯•çš„å·¥ç¨‹æ€è·¯ã€‚  
```

## grounded-reinforcement-learning-for-visual-reasoning
### Abstract
While reinforcement learning (RL) over chains of thought has significantly
advanced language models in tasks such as mathematics and coding, visual
reasoning introduces added complexity by requiring models to direct visual
attention, interpret perceptual inputs, and ground abstract reasoning in
spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement
Learning), a vision-language model trained with RL to explicitly anchor each
reasoning step to specific visual coordinates. Inspired by human visual
decision-making, ViGoRL learns to produce spatially grounded reasoning traces,
guiding visual attention to task-relevant regions at each step. When
fine-grained exploration is required, our novel multi-turn RL framework enables
the model to dynamically zoom into predicted coordinates as reasoning unfolds.
Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK
for spatial reasoning, V*bench for visual search, and ScreenSpot and
VisualWebArena for web-based grounding--ViGoRL consistently outperforms both
supervised fine-tuning and conventional RL baselines that lack explicit
grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual
feedback significantly improves ViGoRL's performance on localizing small GUI
elements and visual search, achieving 86.4% on V*Bench. Additionally, we find
that grounding amplifies other visual behaviors such as region exploration,
grounded subgoal setting, and visual verification. Finally, human evaluations
show that the model's visual references are not only spatially accurate but
also helpful for understanding model reasoning steps. Our results show that
visually grounded RL is a strong paradigm for imbuing models with
general-purpose visual reasoning.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§†è§‰æ¨ç†æ–°èŒƒå¼ï¼šViGoRLçš„è§†è§‰é”šå®šå¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨åŠ¨è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç ç­‰ä»»åŠ¡å–å¾—è¿›å±•çš„åŒæ—¶ï¼Œè§†è§‰æ¨ç†å› éœ€å¼•å¯¼è§†è§‰æ³¨æ„åŠ›ã€è§£è¯»æ„ŸçŸ¥è¾“å…¥å¹¶å°†æŠ½è±¡æ¨ç†é”šå®šåœ¨ç©ºé—´è¯æ®ä¸Šï¼Œé¢ä¸´é¢å¤–å¤æ‚æ€§ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æ˜¾å¼çš„è§†è§‰é”šå®šæœºåˆ¶ï¼Œåœ¨éœ€è¦ç»†ç²’åº¦æ¢ç´¢çš„è§†è§‰æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ç©ºé—´æ¨ç†ã€è§†è§‰æœç´¢ã€ç½‘é¡µå…ƒç´ å®šä½ç­‰ï¼‰ä¸­è¡¨ç°å—é™ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©æ¨¡å‹åœ¨æ¯ä¸€æ­¥æ¨ç†ä¸­æ˜¾å¼åœ°å°†æ¨ç†æ­¥éª¤é”šå®šåˆ°ç‰¹å®šè§†è§‰åæ ‡ï¼Œæˆä¸ºè§†è§‰æ¨ç†é¢†åŸŸäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºViGoRLï¼ˆVisually Grounded Reinforcement Learningï¼‰æ¡†æ¶  
ViGoRLæ˜¯ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ƒæ˜¾å¼åœ°å°†æ¯ä¸ªæ¨ç†æ­¥éª¤é”šå®šåˆ°ç‰¹å®šè§†è§‰åæ ‡ã€‚å—äººç±»è§†è§‰å†³ç­–è¿‡ç¨‹å¯å‘ï¼ŒViGoRLå­¦ä¹ ç”Ÿæˆç©ºé—´é”šå®šçš„æ¨ç†è½¨è¿¹ï¼Œåœ¨æ¯ä¸€æ­¥å¼•å¯¼è§†è§‰æ³¨æ„åŠ›åˆ°ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸï¼Œè®©æŠ½è±¡æ¨ç†æœ‰äº†ç©ºé—´ç»´åº¦çš„â€œè½è„šç‚¹â€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šè½®å¼ºåŒ–å­¦ä¹ +åŠ¨æ€ç¼©æ”¾æœºåˆ¶  
å½“ä»»åŠ¡éœ€è¦ç»†ç²’åº¦æ¢ç´¢æ—¶ï¼ŒViGoRLçš„å¤šè½®RLæ¡†æ¶å…è®¸æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°æ”¾å¤§åˆ°é¢„æµ‹åæ ‡ã€‚è¿™ç§æœºåˆ¶è®©æ¨¡å‹èƒ½æ ¹æ®æ¨ç†è¿›å±•ï¼Œé€æ­¥èšç„¦åˆ°æ›´ç²¾ç¡®çš„åŒºåŸŸï¼Œç‰¹åˆ«é€‚ç”¨äºå°GUIå…ƒç´ å®šä½ã€è§†è§‰æœç´¢ç­‰å¯¹å±€éƒ¨ç»†èŠ‚è¦æ±‚é«˜çš„ä»»åŠ¡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
1. å¤šåŸºå‡†æµ‹è¯•é¢†å…ˆï¼šåœ¨ç©ºé—´æ¨ç†ï¼ˆå¦‚SAT - 2ã€BLINKï¼‰ã€è§†è§‰æœç´¢ï¼ˆV*benchï¼‰ã€ç½‘é¡µé”šå®šï¼ˆScreenSpotã€VisualWebArenaï¼‰ç­‰å¤šæ ·åŒ–è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViGoRLæŒç»­è¶…è¶Šæœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•å’Œç¼ºä¹æ˜¾å¼é”šå®šæœºåˆ¶çš„ä¼ ç»ŸRLåŸºçº¿ã€‚  
2. ç»†åˆ†ä»»åŠ¡çªç ´ï¼šåœ¨V*Benchè§†è§‰æœç´¢ä»»åŠ¡ä¸Šï¼Œç»“åˆå¤šè½®RLä¸ç¼©æ”¾è§†è§‰åé¦ˆçš„ViGoRLè¾¾åˆ°86.4%çš„å‡†ç¡®ç‡ï¼Œåœ¨å°GUIå…ƒç´ å®šä½ç­‰ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚  
3. è¡Œä¸ºå¢å¼ºä¸äººç±»è¯„ä¼°ï¼šé”šå®šæœºåˆ¶è¿˜æ”¾å¤§äº†åŒºåŸŸæ¢ç´¢ã€é”šå®šå­ç›®æ ‡è®¾å®šã€è§†è§‰éªŒè¯ç­‰å…¶ä»–è§†è§‰è¡Œä¸ºï¼›äººç±»è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹çš„è§†è§‰å‚è€ƒä¸ä»…ç©ºé—´å‡†ç¡®ï¼Œè¿˜èƒ½å¸®åŠ©ç†è§£æ¨ç†æ­¥éª¤ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§†è§‰é”šå®šèŒƒå¼ï¼šå°†æŠ½è±¡æ¨ç†ä¸ç©ºé—´è¯æ®æ˜¾å¼é”šå®šçš„æ€è·¯ï¼Œä¸ºé€šç”¨è§†è§‰æ¨ç†æ¨¡å‹çš„æ„å»ºæä¾›äº†å¼ºèŒƒå¼ï¼Œå¯å¯å‘åç»­è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­â€œæ¨ç†-è§†è§‰â€å…³è”æœºåˆ¶çš„è®¾è®¡ã€‚  
2. å¤šè½®äº¤äº’ä¸åŠ¨æ€èšç„¦ï¼šå¤šè½®RLç»“åˆåŠ¨æ€ç¼©æ”¾çš„æœºåˆ¶ï¼Œä¸ºéœ€è¦ç»†ç²’åº¦æ¢ç´¢çš„è§†è§‰ä»»åŠ¡ï¼ˆå¦‚ç•Œé¢å…ƒç´ äº¤äº’ã€å¤æ‚åœºæ™¯æœç´¢ï¼‰æä¾›äº†äº¤äº’å¼ã€æ¸è¿›å¼çš„è§£å†³æ€è·¯ï¼Œå¯è¿ç§»åˆ°ç±»ä¼¼éœ€é€æ­¥èšç„¦çš„è§†è§‰åœºæ™¯ä»»åŠ¡ä¸­ã€‚  
3. è¡Œä¸ºä¸å¯è§£é‡Šæ€§ï¼šé”šå®šå¸¦æ¥çš„è§†è§‰è¡Œä¸ºå¢å¼ºï¼ˆå¦‚åŒºåŸŸæ¢ç´¢ã€å­ç›®æ ‡è®¾å®šï¼‰ä»¥åŠäººç±»å¯ç†è§£çš„æ¨ç†å‚è€ƒï¼Œä¸ºæå‡æ¨¡å‹å¯è§£é‡Šæ€§å’Œè¡Œä¸ºå¯åˆ†ææ€§æä¾›äº†æ–¹å‘ï¼Œåœ¨è¿½æ±‚æ¨¡å‹é€æ˜æ€§çš„ç ”ç©¶ä¸­å€¼å¾—å€Ÿé‰´ã€‚  
```

## mllms-know-where-to-look--training-free-perception-of-small-visual-details-with-multimodal-llms
### Abstract
Multimodal Large Language Models (MLLMs) have experienced rapid progress in
visual recognition tasks in recent years. Given their potential integration
into many critical applications, it is important to understand the limitations
of their visual perception. In this work, we study whether MLLMs can perceive
small visual details as effectively as large ones when answering questions
about images. We observe that their performance is very sensitive to the size
of the visual subject of the question, and further show that this effect is in
fact causal by conducting an intervention study. Next, we study the attention
patterns of MLLMs when answering visual questions, and intriguingly find that
they consistently know where to look, even when they provide the wrong answer.
Based on these findings, we then propose training-free visual intervention
methods that leverage the internal knowledge of any MLLM itself, in the form of
attention and gradient maps, to enhance its perception of small visual details.
We evaluate our proposed methods on two widely-used MLLMs and seven visual
question answering benchmarks and show that they can significantly improve
MLLMs' accuracy without requiring any training. Our results elucidate the risk
of applying MLLMs to visual recognition tasks concerning small details and
indicate that visual intervention using the model's internal state is a
promising direction to mitigate this risk.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | MLLMsèƒ½å¦ç²¾å‡†æ•æ‰å¾®å°è§†è§‰ç»†èŠ‚ï¼Ÿæ— è®­ç»ƒå¹²é¢„æ–¹æ³•ç»™å‡ºæ–°æ–¹å‘

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå…¶å¯èƒ½è¢«é›†æˆåˆ°è¯¸å¤šå…³é”®åº”ç”¨åœºæ™¯ä¸­ï¼Œäº†è§£å®ƒä»¬åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„å±€é™æ€§å°±å˜å¾—è‡³å…³é‡è¦ã€‚å…¶ä¸­ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯ï¼šå½“å›ç­”å…³äºå›¾åƒçš„é—®é¢˜æ—¶ï¼ŒMLLMsèƒ½å¦åƒæ„ŸçŸ¥å¤§çš„è§†è§‰å¯¹è±¡é‚£æ ·æœ‰æ•ˆåœ°æ„ŸçŸ¥å¾®å°è§†è§‰ç»†èŠ‚ï¼Ÿè¿™ä¸€é—®é¢˜å…³ä¹MLLMsåœ¨æ¶‰åŠå¾®å°ç»†èŠ‚çš„è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­çš„å¯é æ€§ï¼Œä¹Ÿæ˜¯æœ¬æ–‡ç ”ç©¶çš„æ ¸å¿ƒåŠ¨æœºâ€”â€”æ¢ç©¶MLLMså¯¹å¾®å°è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥èƒ½åŠ›ã€èƒŒååŸå› ï¼Œå¹¶å°è¯•è§£å†³å…¶å­˜åœ¨çš„ä¸è¶³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºè§†è§‰ä¸»ä½“å¤§å°å¯¹MLLMsæ€§èƒ½çš„å› æœå½±å“
é€šè¿‡å¼€å±•å¹²é¢„æ€§ç ”ç©¶ï¼Œæ·±å…¥åˆ†æMLLMsåœ¨å›ç­”è§†è§‰é—®é¢˜æ—¶æ€§èƒ½ä¸è§†è§‰ä¸»ä½“å¤§å°çš„å…³ç³»ï¼Œæ˜ç¡®äº†å…¶æ€§èƒ½å¯¹é—®é¢˜ä¸­è§†è§‰ä¸»ä½“å¤§å°ååˆ†æ•æ„Ÿè¿™ä¸€ç°è±¡å­˜åœ¨å› æœæ€§ï¼Œä¸æ˜¯å¶ç„¶å› ç´ å¯¼è‡´ï¼Œä¸ºåç»­é’ˆå¯¹æ€§æ”¹è¿›æä¾›äº†åŸºç¡€è®¤çŸ¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå‘ç°MLLMsæ³¨æ„åŠ›æ¨¡å¼çš„â€œçŸ¥è€Œä¸å‡†â€ç°è±¡
ç ”ç©¶MLLMså›ç­”è§†è§‰é—®é¢˜æ—¶çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œæœ‰è¶£åœ°å‘ç°å³ä¾¿æ¨¡å‹ç»™å‡ºé”™è¯¯ç­”æ¡ˆï¼Œå®ƒä»¬ä¹Ÿå§‹ç»ˆâ€œçŸ¥é“è¯¥çœ‹å“ªé‡Œâ€ï¼ˆå³æ³¨æ„åŠ›èƒ½èšç„¦åˆ°å¯¹åº”åŒºåŸŸï¼‰ã€‚è¿™ä¸€å‘ç°ä¸ºåˆ©ç”¨æ¨¡å‹å†…éƒ¨çŠ¶æ€æ¥æ”¹è¿›æ€§èƒ½æä¾›äº†å…³é”®ä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºæ— è®­ç»ƒçš„è§†è§‰å¹²é¢„æ–¹æ³•
åŸºäºå‰é¢å‘ç°çš„MLLMsæ³¨æ„åŠ›å’Œæ¢¯åº¦å›¾ç­‰å†…éƒ¨çŸ¥è¯†ï¼Œæå‡ºæ— éœ€è®­ç»ƒçš„è§†è§‰å¹²é¢„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å€ŸåŠ©æ¨¡å‹è‡ªèº«å†…éƒ¨çŠ¶æ€ï¼Œå¢å¼ºå…¶å¯¹å¾®å°è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºæå‡MLLMsåœ¨å¾®å°ç»†èŠ‚è§†è§‰ä»»åŠ¡è¡¨ç°å¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒè¿‡ç¨‹ï¼Œå¯¹å·²æœ‰æ¨¡å‹æ˜“ç”¨æ€§å¼ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„MLLMsï¼ˆæœªæ˜ç¡®æåŠå…·ä½“æ¨¡å‹ä½†å±äºä¸»æµå¤šæ¨¡æ€å¤§æ¨¡å‹èŒƒç•´ï¼‰ä»¥åŠä¸ƒä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•é›†ä¸Šå¯¹æ‰€ææ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ— è®­ç»ƒçš„è§†è§‰å¹²é¢„æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜MLLMsçš„å‡†ç¡®ç‡ï¼Œæœ‰åŠ›è¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯´æ˜åˆ©ç”¨æ¨¡å‹å†…éƒ¨çŠ¶æ€è¿›è¡Œè§†è§‰å¹²é¢„åœ¨ç¼“è§£å¾®å°ç»†èŠ‚æ„ŸçŸ¥ä¸è¶³é£é™©æ–¹é¢æ˜¯å¯è¡Œä¸”æœ‰æ½œåŠ›çš„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç ”ç©¶æ€è·¯ä¸Šï¼Œå…ˆæ·±å…¥å‰–ææ¨¡å‹åœ¨ç‰¹å®šè§†è§‰ä»»åŠ¡ï¼ˆå¾®å°ç»†èŠ‚æ„ŸçŸ¥ï¼‰ä¸­çš„å±€é™æ€§åŠèƒŒåè§„å¾‹ï¼ˆå¦‚è§†è§‰ä¸»ä½“å¤§å°å½±å“ã€æ³¨æ„åŠ›æ¨¡å¼ç‰¹ç‚¹ï¼‰ï¼Œè¿™ç§ä»é—®é¢˜æœ¬è´¨å’Œæ¨¡å‹å†…åœ¨æœºåˆ¶å‡ºå‘çš„ç ”ç©¶æ–¹å¼ï¼Œä¸ºæ¢ç´¢å…¶ä»–AIæ¨¡å‹èƒ½åŠ›è¾¹ç•Œå’Œæ”¹è¿›æ–¹å‘æä¾›äº†èŒƒä¾‹ï¼Œå¯å€Ÿé‰´æ¥åˆ†ææ¨¡å‹åœ¨ä¸åŒç»†åˆ†ä»»åŠ¡ä¸‹çš„è¡¨ç°ä¸å†…åœ¨é€»è¾‘ã€‚
2. æ–¹æ³•åˆ›æ–°ä¸Šï¼Œæå‡ºçš„æ— è®­ç»ƒè§†è§‰å¹²é¢„æ–¹æ³•ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«å†…éƒ¨çŸ¥è¯†ï¼ˆæ³¨æ„åŠ›ã€æ¢¯åº¦å›¾ï¼‰æ¥å¢å¼ºæ€§èƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæ•°æ®å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œè¿™ç§è½»é‡çº§ä¸”åŸºäºæ¨¡å‹å†…åœ¨çŠ¶æ€çš„ä¼˜åŒ–æ€è·¯ï¼Œå¯¹äºå·²æœ‰éƒ¨ç½²çš„å¤§æ¨¡å‹å¿«é€Ÿæå‡ç‰¹å®šèƒ½åŠ›å…·æœ‰å‚è€ƒä»·å€¼ï¼Œåœ¨å®é™…å·¥ä¸šåº”ç”¨æˆ–æ¨¡å‹ä¼˜åŒ–è¿­ä»£ä¸­ï¼Œå¯æ€è€ƒå¦‚ä½•æŒ–æ˜æ¨¡å‹å†…éƒ¨ä¿¡æ¯æ¥åšé’ˆå¯¹æ€§å¢å¼ºã€‚
3. å®éªŒéªŒè¯ç»´åº¦ï¼Œé€‰æ‹©å¤šä¸ªä¸»æµMLLMså’Œå¤šä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œä¿è¯äº†ç»“æœçš„é€šç”¨æ€§å’Œè¯´æœåŠ›ï¼Œåœ¨åšç›¸å…³æ¨¡å‹æ”¹è¿›æˆ–æ–¹æ³•éªŒè¯æ—¶ï¼Œå¯å­¦ä¹ è¿™ç§å¤šæ¨¡å‹ã€å¤šæ•°æ®é›†çš„å…¨é¢è¯„ä¼°æ–¹å¼ï¼Œä»¥å……åˆ†è¯æ˜æ–¹æ³•æœ‰æ•ˆæ€§ã€‚ 
```

## deepeyes--incentivizing--thinking-with-images--via-reinforcement-learning
### Abstract
Large Vision-Language Models (VLMs) have shown strong capabilities in
multimodal understanding and reasoning, yet they are primarily constrained by
text-based reasoning processes. However, achieving seamless integration of
visual and textual reasoning which mirrors human cognitive processes remains a
significant challenge. In particular, effectively incorporating advanced visual
input processing into reasoning mechanisms is still an open question. Thus, in
this paper, we explore the interleaved multimodal reasoning paradigm and
introduce DeepEyes, a model with "thinking with images" capabilities
incentivized through end-to-end reinforcement learning without the need for
cold-start SFT. Notably, this ability emerges natively within the model itself,
leveraging its inherent grounding ability as a tool instead of depending on
separate specialized models. Specifically, we propose a tool-use-oriented data
selection mechanism and a reward strategy to encourage successful tool-assisted
reasoning trajectories. DeepEyes achieves significant performance gains on
fine-grained perception and reasoning benchmarks and also demonstrates
improvement in grounding, hallucination, and mathematical reasoning tasks.
Interestingly, we observe the distinct evolution of tool-calling behavior from
initial exploration to efficient and accurate exploitation, and diverse
thinking patterns that closely mirror human visual reasoning processes. Code is
available at https://github.com/Visual-Agent/DeepEyes.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | DeepEyesï¼šç”¨å¼ºåŒ–å­¦ä¹ èµ‹èƒ½â€œä»¥å›¾æ€è€ƒâ€çš„å¤šæ¨¡æ€æ¨ç†æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£ä¸æ¨ç†æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†ç›®å‰ä¸»è¦å—é™äºåŸºäºæ–‡æœ¬çš„æ¨ç†æµç¨‹ã€‚è€Œå®ç°ç±»äººè®¤çŸ¥è¿‡ç¨‹ä¸­è§†è§‰ä¸æ–‡æœ¬æ¨ç†çš„æ— ç¼èåˆä»æ˜¯é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æŠŠå…ˆè¿›è§†è§‰è¾“å…¥å¤„ç†æœ‰æ•ˆèå…¥æ¨ç†æœºåˆ¶è¿™ä¸€é—®é¢˜å°šæœªè§£å†³ã€‚æ‰€ä»¥æœ¬æ–‡æ¢ç´¢äº¤é”™å¼å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼ŒæœŸæœ›è®©æ¨¡å‹å…·å¤‡â€œä»¥å›¾æ€è€ƒâ€èƒ½åŠ›ï¼Œçªç ´ç°æœ‰VLMsåœ¨æ¨ç†æ–¹å¼ä¸Šçš„å±€é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ— éœ€å†·å¯åŠ¨SFTçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¿€åŠ±æ–¹æ¡ˆ  
DeepEyesä¸ä¾èµ–å†·å¯åŠ¨çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¥æ¿€å‘æ¨¡å‹â€œä»¥å›¾æ€è€ƒâ€çš„èƒ½åŠ›ã€‚è¿™ç§èƒ½åŠ›æ˜¯æ¨¡å‹åŸç”Ÿæ¶Œç°çš„ï¼Œå€ŸåŠ©è‡ªèº«å›ºæœ‰çš„æ¥åœ°ï¼ˆgroundingï¼‰èƒ½åŠ›å½“ä½œå·¥å…·ï¼Œè€Œéä¾èµ–å•ç‹¬çš„ä¸“ç”¨æ¨¡å‹ï¼Œè®©è§†è§‰ä¿¡æ¯å¤„ç†è‡ªç„¶èå…¥æ¨ç†è¿‡ç¨‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢å‘å·¥å…·ä½¿ç”¨çš„æ•°æ®é€‰æ‹©æœºåˆ¶ä¸å¥–åŠ±ç­–ç•¥  
è®¾è®¡äº†é¢å‘å·¥å…·ä½¿ç”¨çš„æ•°æ®é€‰æ‹©æœºåˆ¶ï¼Œèƒ½é’ˆå¯¹æ€§åœ°ç­›é€‰æ•°æ®æ¥åŠ©åŠ›æ¨¡å‹å­¦ä¹ å·¥å…·è¾…åŠ©æ¨ç†ï¼›åŒæ—¶æ­é…å¥–åŠ±ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹äº§ç”ŸæˆåŠŸçš„å·¥å…·è¾…åŠ©æ¨ç†è½¨è¿¹ï¼Œä»æ•°æ®å’Œåé¦ˆå±‚é¢åŒç®¡é½ä¸‹ï¼Œæ¨åŠ¨æ¨¡å‹æŒæ¡é«˜æ•ˆæ¨ç†æ–¹å¼ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
DeepEyesåœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ï¼Œåœ¨æ¥åœ°ï¼ˆgroundingï¼‰ã€å¹»è§‰ï¼ˆhallucinationï¼‰ä»¥åŠæ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºæ”¹è¿›æ•ˆæœã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œè§‚æµ‹åˆ°æ¨¡å‹çš„å·¥å…·è°ƒç”¨è¡Œä¸ºä»åˆå§‹æ¢ç´¢é˜¶æ®µå‘é«˜æ•ˆç²¾å‡†åˆ©ç”¨é˜¶æ®µçš„æ˜æ˜¾è¿›åŒ–ï¼Œè¿˜å‡ºç°äº†å¤šç§è´´è¿‘äººç±»è§†è§‰æ¨ç†è¿‡ç¨‹çš„æ€ç»´æ¨¡å¼ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å¡‘é€ ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­æ¿€åŠ±ç‰¹æ®Šèƒ½åŠ›ï¼ˆå¦‚â€œä»¥å›¾æ€è€ƒâ€ï¼‰çš„æ€è·¯ï¼Œä¸ºçªç ´çº¯æ–‡æœ¬æ¨ç†é™åˆ¶æä¾›äº†æ–°èŒƒå¼å‚è€ƒï¼Œå¯å¯å‘åç»­æ¢ç´¢ç±»äººè®¤çŸ¥å¼çš„å¤šæ¨¡æ€èåˆæ¨ç†ã€‚  
2. é¢å‘å·¥å…·ä½¿ç”¨çš„æ•°æ®é€‰æ‹©ä¸å¥–åŠ±è®¾è®¡ç­–ç•¥ï¼Œä¸ºæ¨¡å‹åœ¨å·¥å…·è¾…åŠ©ç±»ä»»åŠ¡ä¸Šçš„è®­ç»ƒæä¾›äº†å¯å¤ç”¨çš„æ–¹æ³•è®ºï¼Œåœ¨éœ€è¦å·¥å…·åä½œå®Œæˆå¤æ‚æ¨ç†çš„åœºæ™¯ä¸­å…·æœ‰å€Ÿé‰´ä»·å€¼ã€‚  
3. å¯¹å·¥å…·è°ƒç”¨è¡Œä¸ºæ¼”åŒ–å’Œç±»äººæ€ç»´æ¨¡å¼çš„è§‚æµ‹åˆ†æï¼Œä¸ºç†è§£æ¨¡å‹å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å½¢æˆè¿‡ç¨‹æä¾›äº†è§†è§’ï¼Œæœ‰åŠ©äºåç»­æ›´æ·±å…¥æ¢ç©¶æ¨¡å‹è¡Œä¸ºæœºç†ä¸èƒ½åŠ›æˆé•¿è§„å¾‹ã€‚  
```

## visual-abstract-thinking-empowers-multimodal-reasoning
### Abstract
Images usually convey richer detail than text, but often include redundant
information which potentially downgrades multimodal reasoning performance. When
faced with lengthy or complex messages, humans tend to employ abstract thinking
to convert them into simple and concise abstracts. Inspired by this cognitive
strategy, we introduce Visual Abstract Thinking (VAT), a novel thinking
paradigm that prompts Multimodal Large Language Models (MLLMs) with visual
abstract instead of explicit verbal thoughts or elaborate guidance, permitting
a more concentrated visual reasoning mechanism. Explicit thinking, such as
Chain-of-thought (CoT) or tool-augmented approaches, increases the complexity
of reasoning process via inserting verbose intermediate steps, external
knowledge or visual information. In contrast, VAT reduces redundant visual
information and encourages models to focus their reasoning on more essential
visual elements. Experimental results show that VAT consistently empowers
different models, and achieves an average gain of 17% over GPT-4o baseline by
employing diverse types of visual abstracts, demonstrating that VAT can enhance
visual reasoning abilities for MLLMs regarding conceptual, structural and
relational reasoning tasks. VAT is also compatible with CoT in
knowledge-intensive multimodal reasoning tasks. These findings highlight the
effectiveness of visual reasoning via abstract thinking and encourage further
exploration of more diverse reasoning paradigms from the perspective of human
cognition.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§†è§‰æŠ½è±¡æ€ç»´ï¼šä¸ºå¤šæ¨¡æ€æ¨ç†æ³¨å…¥æ–°æ´»åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šæ¨¡æ€æ¨ç†åœºæ™¯ä¸­ï¼Œå›¾åƒè™½èƒ½ä¼ é€’æ¯”æ–‡æœ¬æ›´ä¸°å¯Œçš„ç»†èŠ‚ï¼Œä½†ä¹Ÿå­˜åœ¨å†—ä½™ä¿¡æ¯ï¼Œè¿™äº›å†—ä½™ä¿¡æ¯å¯èƒ½ä¼šé™ä½å¤šæ¨¡æ€æ¨ç†çš„æ€§èƒ½ã€‚è€Œäººç±»åœ¨é¢å¯¹å†—é•¿æˆ–å¤æ‚ä¿¡æ¯æ—¶ï¼Œä¼šè¿ç”¨æŠ½è±¡æ€ç»´å°†å…¶è½¬åŒ–ä¸ºç®€æ´çš„æŠ½è±¡å†…å®¹ã€‚å—æ­¤è®¤çŸ¥ç­–ç•¥å¯å‘ï¼Œè®ºæ–‡å¸Œæœ›æ¢ç´¢ä¸€ç§æ–°çš„æ–¹å¼ï¼Œè®©å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½æ›´é«˜æ•ˆåœ°è¿›è¡Œè§†è§‰æ¨ç†ï¼Œé¿å…ä¼ ç»Ÿæ˜¾å¼æ€ç»´ï¼ˆå¦‚æ€ç»´é“¾CoTæˆ–å·¥å…·å¢å¼ºæ–¹æ³•ï¼‰å› å¼•å…¥å†—é•¿ä¸­é—´æ­¥éª¤ã€å¤–éƒ¨çŸ¥è¯†æˆ–è§†è§‰ä¿¡æ¯è€Œå¢åŠ æ¨ç†å¤æ‚åº¦çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè§†è§‰æŠ½è±¡æ€ç»´ï¼ˆVATï¼‰èŒƒå¼  
å¼•å…¥Visual Abstract Thinkingï¼ˆVATï¼‰è¿™ä¸€æ–°é¢–çš„æ€ç»´èŒƒå¼ï¼Œä¸å†ç”¨æ˜¾å¼çš„è¯­è¨€æ€è€ƒæˆ–å¤æ‚æŒ‡å¯¼ï¼Œè€Œæ˜¯ä»¥è§†è§‰æŠ½è±¡æ¥æç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œè®©æ¨¡å‹èƒ½èšç„¦åˆ°æ›´å…³é”®çš„è§†è§‰å…ƒç´ ä¸Šï¼Œå‡å°‘å†—ä½™è§†è§‰ä¿¡æ¯å¯¹æ¨ç†çš„å¹²æ‰°ï¼Œæ„å»ºæ›´é›†ä¸­çš„è§†è§‰æ¨ç†æœºåˆ¶ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šVATä¸CoTçš„å…¼å®¹æ€§æ¢ç´¢  
åœ¨çŸ¥è¯†å¯†é›†å‹å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­ï¼ŒéªŒè¯äº†VATä¸CoTç»“åˆçš„å¯è¡Œæ€§ï¼Œå±•ç°å‡ºè¯¥èŒƒå¼åœ¨ä¸åŒåœºæ™¯ä¸‹çš„çµæ´»åº”ç”¨æ½œåŠ›ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æä¾›äº†æ›´ä¸°å¯Œçš„è§£å†³æ€è·¯ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜VATèƒ½æŒç»­èµ‹èƒ½ä¸åŒçš„æ¨¡å‹ï¼Œé€šè¿‡é‡‡ç”¨å¤šç§ç±»å‹çš„è§†è§‰æŠ½è±¡ï¼Œç›¸æ¯”GPT - 4oåŸºçº¿å¹³å‡æå‡äº†17%ã€‚å¹¶ä¸”åœ¨æ¦‚å¿µã€ç»“æ„å’Œå…³ç³»æ¨ç†ç­‰ä»»åŠ¡ä¸Šï¼ŒVATå¢å¼ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæœ‰åŠ›åœ°è¯æ˜äº†é€šè¿‡æŠ½è±¡æ€ç»´è¿›è¡Œè§†è§‰æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»äººç±»è®¤çŸ¥è§’åº¦ä¸ºå¤šæ¨¡æ€æ¨ç†èŒƒå¼æä¾›äº†æ–°æ–¹å‘ï¼Œå¯å‘ç ”ç©¶è€…æ¢ç´¢æ›´å¤šæ ·åŒ–çš„æ¨ç†èŒƒå¼ï¼›VATæ‰€ä½“ç°çš„â€œç®€åŒ–å†—ä½™ã€èšç„¦å…³é”®â€æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è¦å¤„ç†å†—ä½™ä¿¡æ¯çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼›å…¶ä¸CoTå…¼å®¹çš„ç‰¹æ€§ï¼Œä¹Ÿä¸ºç°æœ‰å¤šæ¨¡æ€æ¨ç†æŠ€æœ¯æ ˆçš„ä¼˜åŒ–å’Œæ‹“å±•æä¾›äº†å‚è€ƒï¼Œè®©å¼€å‘è€…åœ¨æ„å»ºå¤šæ¨¡æ€åº”ç”¨æ—¶èƒ½è€ƒè™‘ç»“åˆè¿™ç§æŠ½è±¡æ€ç»´æ–¹å¼æ¥æå‡æ€§èƒ½ã€‚
```

## visual-thoughts--a-unified-perspective-of-understanding-multimodal-chain-of-thought
### Abstract
Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing
performance and interpretability. Recent MCoT methods fall into two categories:
(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual
output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved
image-text outputs. Despite advances in both approaches, the mechanisms driving
these improvements are not fully understood. To fill this gap, we first reveal
that MCoT boosts LVLMs by incorporating visual thoughts, which convey image
information to the reasoning process regardless of the MCoT format, depending
only on clarity and conciseness of expression. Furthermore, to explore visual
thoughts systematically, we define four distinct forms of visual thought
expressions and analyze them comprehensively. Our findings demonstrate that
these forms differ in clarity and conciseness, yielding varying levels of MCoT
improvement. Additionally, we explore the internal nature of visual thoughts,
finding that visual thoughts serve as intermediaries between the input image
and reasoning to deeper transformer layers, enabling more advanced visual
information transmission. We hope that the visual thoughts can inspire further
breakthroughs for future MCoT research.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜å¤šæ¨¡æ€æ€ç»´é“¾èƒŒåçš„â€œè§†è§‰æ€ç»´â€ï¼šç»Ÿä¸€è§†è§’ä¸‹çš„ç†è§£ä¸æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆMCoTï¼‰è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ä¸å¯è§£é‡Šæ€§ã€‚ç°æœ‰çš„MCoTæ–¹æ³•ä¸»è¦åˆ†ä¸ºæ–‡æœ¬å‹ï¼ˆT-MCoTï¼Œè¾“å…¥å¤šæ¨¡æ€ã€è¾“å‡ºæ–‡æœ¬ï¼‰å’Œäº¤é”™å‹ï¼ˆI-MCoTï¼Œç”Ÿæˆå›¾æ–‡äº¤é”™è¾“å‡ºï¼‰ä¸¤ç±»ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç±»æ–¹æ³•æ€§èƒ½æå‡èƒŒåçš„é©±åŠ¨æœºåˆ¶å°šæœªè¢«å……åˆ†ç†è§£ã€‚ä¸ºå¡«è¡¥è¿™ä¸€è®¤çŸ¥ç©ºç™½ï¼Œè®ºæ–‡èšç„¦äºâ€œè§†è§‰æ€ç»´ï¼ˆvisual thoughtsï¼‰â€è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µï¼Œæ¢ç´¢å…¶åœ¨ä¸åŒMCoTå½¢å¼ä¸­å¦‚ä½•å‘æŒ¥ä½œç”¨ï¼Œè¿›è€Œæ¨åŠ¨LVLMsèƒ½åŠ›è¿›é˜¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºâ€œè§†è§‰æ€ç»´â€æ˜¯MCoTæ€§èƒ½æå‡çš„å…³é”®  
è®ºæ–‡é¦–æ¬¡æŒ‡å‡ºï¼Œæ— è®ºMCoTé‡‡ç”¨T-MCoTè¿˜æ˜¯I-MCoTå½¢å¼ï¼Œ**è§†è§‰æ€ç»´**æ‰æ˜¯æ€§èƒ½æå‡çš„æ ¸å¿ƒâ€”â€”å®ƒèƒ½å°†å›¾åƒä¿¡æ¯ä¼ é€’åˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸”è¿™ä¸€ä¼ é€’æ•ˆæœä»…å–å†³äºè¡¨è¾¾çš„â€œæ¸…æ™°æ€§â€ä¸â€œç®€æ´æ€§â€ï¼Œè€Œéå…·ä½“MCoTæ ¼å¼ã€‚è¿™æ‰“ç ´äº†å¯¹ä¸åŒMCoTå½¢å¼â€œè¡¨é¢å·®å¼‚â€çš„å…³æ³¨ï¼Œç›´æŒ‡åº•å±‚ç»Ÿä¸€çš„æå‡é€»è¾‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç³»ç»Ÿå®šä¹‰å¹¶åˆ†æå››ç§è§†è§‰æ€ç»´è¡¨è¾¾å½¢å¼  
ä¸ºæ·±å…¥ç ”ç©¶è§†è§‰æ€ç»´ï¼Œè®ºæ–‡æ˜ç¡®æå‡ºå››ç§æˆªç„¶ä¸åŒçš„è§†è§‰æ€ç»´è¡¨è¾¾å½¢å¼ï¼Œå¹¶ä»æ¸…æ™°æ€§ã€ç®€æ´æ€§ç»´åº¦å±•å¼€å…¨é¢åˆ†æã€‚é€šè¿‡å¯¹æ¯”å‘ç°ï¼Œä¸åŒå½¢å¼åœ¨è¿™ä¸¤ä¸ªç»´åº¦çš„è¡¨ç°å·®å¼‚ï¼Œä¼šç›´æ¥å¯¼è‡´MCoTæ€§èƒ½æå‡å¹…åº¦çš„ä¸åŒã€‚è¿™ä¸ºåç»­é’ˆå¯¹æ€§è®¾è®¡MCoTç­–ç•¥æä¾›äº†åˆ†ç±»ä¾æ®ä¸ä¼˜åŒ–æ–¹å‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå‰–æè§†è§‰æ€ç»´çš„å†…åœ¨æœ¬è´¨  
è®ºæ–‡è¿›ä¸€æ­¥æ¢ç´¢è§†è§‰æ€ç»´çš„â€œå†…éƒ¨å±æ€§â€ï¼Œå‘ç°å®ƒåœ¨è¾“å…¥å›¾åƒä¸æ›´æ·±å±‚Transformeræ¨ç†ä¹‹é—´æ‰®æ¼”â€œä¸­ä»‹â€è§’è‰²â€”â€”èƒ½è®©æ›´å¤æ‚çš„è§†è§‰ä¿¡æ¯åœ¨æ¨¡å‹ä¸­é«˜æ•ˆä¼ é€’ï¼Œæ”¯æ’‘æ›´æ·±å…¥çš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ä»æ¨¡å‹å†…éƒ¨æœºåˆ¶å±‚é¢ï¼Œè§£é‡Šäº†è§†è§‰æ€ç»´ä¸ºä½•èƒ½æˆä¸ºMCoTçš„â€œåŠ¨åŠ›æºâ€ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡ç³»åˆ—å®éªŒéªŒè¯äº†ä¸Šè¿°ç»“è®ºï¼š  
1. ä¸åŒè§†è§‰æ€ç»´è¡¨è¾¾å½¢å¼åœ¨æ¸…æ™°æ€§ã€ç®€æ´æ€§ä¸Šçš„å·®å¼‚ï¼Œç¡®å®å¯¹åº”ç€MCoTæ€§èƒ½æå‡çš„ä¸åŒå¹…åº¦â€”â€”æ¸…æ™°ä¸”ç®€æ´çš„è¡¨è¾¾å½¢å¼ï¼Œèƒ½æ›´æ˜¾è‘—åœ°æ¨åŠ¨ä»»åŠ¡è¡¨ç°ï¼›  
2. å¯¹è§†è§‰æ€ç»´â€œä¸­ä»‹ä½œç”¨â€çš„éªŒè¯å®éªŒï¼Œä¹Ÿä½è¯äº†å…¶åœ¨è§†è§‰ä¿¡æ¯å‘æ·±å±‚Transformerä¼ é€’æ—¶çš„å…³é”®ä»·å€¼ï¼Œä»æ¨¡å‹å†…éƒ¨æµç¨‹è§’åº¦å¼ºåŒ–äº†ç†è®ºè¯´æœåŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **è®¤çŸ¥å‡çº§**ï¼šè·³å‡ºâ€œT-MCoT vs I-MCoTâ€çš„å½¢å¼ä¹‹äº‰ï¼Œèšç„¦â€œè§†è§‰æ€ç»´â€è¿™ä¸€ç»Ÿä¸€è§†è§’ï¼Œä¸ºå¤šæ¨¡æ€æ€ç»´é“¾ç ”ç©¶æä¾›äº†å…¨æ–°è®¤çŸ¥æ¡†æ¶ï¼›  
2. **æ–¹æ³•è®ºæŒ‡å¯¼**ï¼šå››ç§è§†è§‰æ€ç»´è¡¨è¾¾å½¢å¼çš„åˆ†ç±»ä¸åˆ†æï¼Œä¸ºåç»­è®¾è®¡æ›´é«˜æ•ˆçš„MCoTç­–ç•¥æä¾›äº†â€œæ¸…æ™°æ€§+ç®€æ´æ€§â€çš„ä¼˜åŒ–é”šç‚¹ï¼›  
3. **æœºåˆ¶ç†è§£**ï¼šå¯¹è§†è§‰æ€ç»´å†…åœ¨æœ¬è´¨çš„å‰–æï¼Œå¸®åŠ©ç ”ç©¶è€…ä»æ¨¡å‹å±‚çº§ç†è§£å¤šæ¨¡æ€æ¨ç†çš„ä¿¡æ¯æµåŠ¨é€»è¾‘ï¼Œä¸ºæœªæ¥LVLMsæ¶æ„ä¼˜åŒ–ã€è®­ç»ƒç­–ç•¥è®¾è®¡ç­‰æä¾›äº†æœºåˆ¶å±‚é¢çš„å¯å‘ã€‚  

æ€»ä¹‹ï¼Œè¿™ç¯‡è®ºæ–‡ä»¥â€œè§†è§‰æ€ç»´â€ä¸ºçº¿ç´¢ï¼Œä¸²è”èµ·å¤šæ¨¡æ€æ€ç»´é“¾çš„å½¢å¼ã€æ•ˆæœä¸å†…åœ¨æœºåˆ¶ï¼Œä¸ºMCoTä¹ƒè‡³æ•´ä¸ªå¤šæ¨¡æ€æ¨ç†é¢†åŸŸçš„ç ”ç©¶æ‰“å¼€äº†æ›´å…·æ·±åº¦çš„æ¢ç´¢ç»´åº¦ã€‚
```

## zoomeye--enhancing-multimodal-llms-with-human-like-zooming-capabilities-through-tree-based-image-exploration
### Abstract
An image, especially with high-resolution, typically consists of numerous
visual elements, ranging from dominant large objects to fine-grained detailed
objects. When perceiving such images, multimodal large language models~(MLLMs)
face limitations due to the restricted input resolution of the pretrained
vision encoder and the cluttered, dense context of the image, resulting in a
focus on primary objects while easily overlooking detailed ones. In this paper,
we propose Zoom Eye, a tree search algorithm designed to navigate the
hierarchical and visual nature of images to capture relevant information. Zoom
Eye conceptualizes an image as a tree, with each children node representing a
zoomed sub-patch of the parent node and the root represents the overall image.
Moreover, Zoom Eye is model-agnostic and training-free, so it enables any MLLMs
to simulate human zooming actions by searching along the image tree from root
to leaf nodes, seeking out pertinent information, and accurately responding to
related queries. We experiment on a series of elaborate high-resolution
benchmarks and the results demonstrate that Zoom Eye not only consistently
improves the performance of a series base MLLMs with large margin~(e.g.,
LLaVA-v1.5-7B increases by 34.57\% on $V^*$ Bench and 17.88\% on HR-Bench), but
also enables small 7B MLLMs to outperform strong large models such as GPT-4o.
Our code is available at
\href{https://github.com/om-ai-lab/ZoomEye}{https://github.com/om-ai-lab/ZoomEye}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ZoomEyeï¼šä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹èµ‹äºˆç±»äººâ€œå˜ç„¦è§‚å¯Ÿâ€èƒ½åŠ›ï¼Œçªç ´é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´ç€æ˜¾è‘—æŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼Œé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„è¾“å…¥åˆ†è¾¨ç‡å­˜åœ¨é™åˆ¶ï¼›å¦ä¸€æ–¹é¢ï¼Œé«˜åˆ†è¾¨ç‡å›¾åƒä¸­å…ƒç´ å¯†é›†ä¸”ç¹æ‚ï¼Œå¯¼è‡´æ¨¡å‹å¾€å¾€åªèƒ½å…³æ³¨åˆ°ä¸»è¦ç‰©ä½“ï¼Œè€Œå®¹æ˜“å¿½ç•¥ç»†èŠ‚ä¿¡æ¯ã€‚è¿™ç§â€œé¡¾å¤§å¤±ç»†â€çš„é—®é¢˜æå¤§é™åˆ¶äº†MLLMså¯¹å¤æ‚å›¾åƒåœºæ™¯çš„ç†è§£ä¸å“åº”èƒ½åŠ›ï¼Œå› æ­¤äºŸéœ€ä¸€ç§èƒ½è®©æ¨¡å‹åƒäººç±»è§‚å¯Ÿå›¾åƒæ—¶â€œå˜ç„¦æŸ¥çœ‹ç»†èŠ‚â€çš„æœºåˆ¶æ¥çªç ´ç“¶é¢ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ‘å½¢å›¾åƒæ¢ç´¢æ¶æ„è®¾è®¡  
ZoomEye å°†å›¾åƒæ¦‚å¿µåŒ–ä¸ºä¸€æ£µâ€œæ ‘â€ç»“æ„ï¼šæ ¹èŠ‚ç‚¹ä»£è¡¨å®Œæ•´çš„å›¾åƒï¼Œæ¯ä¸ªå­èŠ‚ç‚¹å¯¹åº”çˆ¶èŠ‚ç‚¹åŒºåŸŸçš„ä¸€ä¸ªâ€œå˜ç„¦å­å—â€ï¼ˆzoomed sub - patchï¼‰ã€‚è¿™ç§ç»“æ„å¤©ç„¶å¥‘åˆå›¾åƒä»æ•´ä½“åˆ°å±€éƒ¨çš„å±‚çº§åŒ–è§†è§‰ç‰¹æ€§ï¼Œä¸ºæ¨¡å‹æ¢ç´¢å›¾åƒä¿¡æ¯æä¾›äº†æ¸…æ™°çš„å±‚çº§å¯¼èˆªè·¯å¾„ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å‹æ— å…³ä¸”æ— è®­ç»ƒå¼€é”€çš„æ ‘æœç´¢ç®—æ³•  
ZoomEye æ˜¯â€œæ¨¡å‹æ— å…³â€ï¼ˆmodel - agnosticï¼‰ä¸”â€œæ— è®­ç»ƒâ€ï¼ˆtraining - freeï¼‰çš„ã€‚å®ƒä¸ä¾èµ–ç‰¹å®šMLLMsçš„å†…éƒ¨ç»“æ„ï¼Œä¹Ÿæ— éœ€é¢å¤–è®­ç»ƒè¿‡ç¨‹ï¼Œèƒ½è®©ä»»æ„MLLMsæ¨¡æ‹Ÿäººç±»â€œå˜ç„¦æŸ¥çœ‹â€çš„åŠ¨ä½œâ€”â€”ä»æ ¹èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹æ²¿ç€å›¾åƒæ ‘æœç´¢ï¼Œä¸»åŠ¨æŒ–æ˜ç›¸å…³ä¿¡æ¯ä»¥å“åº”æŸ¥è¯¢ã€‚è¿™å¤§å¤§é™ä½äº†æŠ€æœ¯è½åœ°é—¨æ§›ï¼Œä»»ä½•ç°æœ‰MLLMséƒ½èƒ½å¿«é€Ÿé›†æˆè¯¥èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡çš„é«˜åˆ†è¾¨ç‡å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒZoomEye å±•ç°å‡ºå¼ºå¤§æ•ˆèƒ½ï¼š  
- å¯¹åŸºç¡€MLLMsæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå¦‚ LLaVA - v1.5 - 7B åœ¨ $V^*$ Bench ä¸Šæ€§èƒ½æå‡ 34.57%ï¼Œåœ¨ HR - Bench ä¸Šæå‡ 17.88%ï¼›  
- è®©7Bè§„æ¨¡çš„å°å‹MLLMså®ç°å¯¹GPT - 4oè¿™ç±»å¼ºå¤§å‹æ¨¡å‹çš„è¶…è¶Šï¼Œå……åˆ†éªŒè¯äº†æ–¹æ³•åœ¨æ€§èƒ½å¢ç›Šä¸Šçš„å¹…åº¦ä¸è·¨æ¨¡å‹ä¼˜åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å±‚çº§åŒ–ç»“æ„å»ºæ¨¡æ€è·¯ï¼šå°†å¤æ‚è§†è§‰ä¿¡æ¯ä»¥æ ‘ç»“æ„ç»„ç»‡ï¼Œä¸ºå¤„ç†å…·æœ‰å±‚çº§ç‰¹æ€§çš„ä»»åŠ¡ï¼ˆä¸æ­¢å›¾åƒï¼ŒéŸ³é¢‘ã€æ–‡æ¡£ç­‰ä¹Ÿå¯å°è¯•ï¼‰æä¾›äº†ç»“æ„åŒ–æ¢ç´¢çš„å‚è€ƒèŒƒå¼ï¼›  
2. æ— è®­ç»ƒã€æ¨¡å‹æ— å…³çš„æ’ä»¶å¼å¢å¼ºï¼šè¿™ç§â€œå³æ’å³ç”¨â€çš„è®¾è®¡ç†å¿µï¼Œåœ¨ä¸æ”¹å˜åŸæœ‰å¤§æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²é€»è¾‘ä¸‹æå‡èƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹ç”Ÿæ€ä¸­å·¥å…·ä¸æ’ä»¶å¼€å‘æä¾›äº†ä¼˜ç§€èŒƒæœ¬ï¼›  
3. ç±»äººæ„ŸçŸ¥æœºåˆ¶æ¨¡æ‹Ÿï¼šä»äººç±»è§†è§‰â€œå˜ç„¦è§‚å¯Ÿâ€ä¸­æ±²å–çµæ„Ÿï¼Œä¸ºAIç³»ç»Ÿæ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥ä¸è®¤çŸ¥æ¨¡å¼æä¾›äº†æ–°çš„æ€è€ƒè§’åº¦ï¼Œåç»­åœ¨å¤šæ¨¡æ€äº¤äº’ã€äººæœºååŒç­‰æ–¹å‘å¯å»¶ä¼¸æ¢ç´¢ã€‚  
```

## visual-sketchpad--sketching-as-a-visual-chain-of-thought-for-multimodal-language-models
### Abstract
Humans draw to facilitate reasoning: we draw auxiliary lines when solving
geometry problems; we mark and circle when reasoning on maps; we use sketches
to amplify our ideas and relieve our limited-capacity working memory. However,
such actions are missing in current multimodal language models (LMs). Current
chain-of-thought and tool-use paradigms only use text as intermediate reasoning
steps. In this work, we introduce Sketchpad, a framework that gives multimodal
LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts
planning and reasoning according to the visual artifacts it has drawn.
Different from prior work, which uses text-to-image models to enable LMs to
draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is
closer to human sketching and better facilitates reasoning. Sketchpad can also
use specialist vision models during the sketching process (e.g., draw bounding
boxes with object detection models, draw masks with segmentation models), to
further enhance visual perception and reasoning. We experiment with a wide
range of math tasks (including geometry, functions, graphs, and chess) and
complex visual reasoning tasks. Sketchpad substantially improves performance on
all tasks over strong base models with no sketching, yielding an average gain
of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a
new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial
reasoning (83.9%), and visual correspondence (80.8%). All codes and data are in
https://visualsketchpad.github.io/.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤šæ¨¡æ€å¤§æ¨¡å‹æ‹¥æœ‰â€œè§†è§‰è‰ç¨¿æœ¬â€ï¼šSketchpad å¼€å¯ç±»äººæ¨ç†æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»åœ¨æ¨ç†æ—¶å¸¸å¸¸å€ŸåŠ©ç»˜å›¾è¾…åŠ©ï¼šè§£å‡ ä½•é¢˜ç”»è¾…åŠ©çº¿ã€çœ‹åœ°å›¾åšæ ‡è®°ã€ç”¨è‰å›¾æ‹“å±•æ€è·¯å¹¶å‡è½»å·¥ä½œè®°å¿†è´Ÿæ‹…â€¦â€¦ä½†å½“å‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å´ç¼ºå¤±è¿™ç±»â€œç»˜å›¾æ¨ç†â€èƒ½åŠ›ã€‚ç°æœ‰æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰å’Œå·¥å…·è°ƒç”¨èŒƒå¼ä»…ä¾èµ–æ–‡æœ¬ä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæ— æ³•åƒäººç±»ä¸€æ ·é€šè¿‡â€œç”»çº¿æ¡ã€æ ‡æ¡†ã€åšæ ‡è®°â€ç­‰æ›´ç›´è§‚çš„è§†è§‰æ‰‹æ®µè¾…åŠ©æ€è€ƒã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè®ºæ–‡æå‡º **Sketchpad** æ¡†æ¶ï¼Œè®©å¤šæ¨¡æ€å¤§æ¨¡å‹æ‹¥æœ‰â€œè§†è§‰è‰ç¨¿æœ¬â€å¹¶èƒ½åœ¨ä¸Šé¢ç»˜å›¾æ¨ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ‰“é€ â€œè§†è§‰è‰ç¨¿æœ¬â€ï¼Œè´´è¿‘äººç±»æ‰‹ç»˜æ¨ç†  
ä¸åŒäºä»¥å¾€ç”¨â€œæ–‡æœ¬åˆ°å›¾åƒï¼ˆText-to-Imageï¼‰â€æ¨¡å‹è®©å¤§æ¨¡å‹â€œç”»ç”»â€çš„æ€è·¯ï¼ŒSketchpad è®©æ¨¡å‹èƒ½ç›´æ¥ç»˜åˆ¶**çº¿æ¡ã€æ¡†é€‰ã€æ ‡è®°**ç­‰æ›´æ¥è¿‘äººç±»â€œè‰ç¨¿å¼â€çš„è§†è§‰å…ƒç´ ã€‚è¿™ç§æ–¹å¼æ›´è´´åˆçœŸå®æ¨ç†åœºæ™¯ï¼ˆå¦‚å‡ ä½•é¢˜æ ‡è¾…åŠ©çº¿ã€åœ°å›¾æ ‡é‡ç‚¹ï¼‰ï¼Œä¸ºæ¨¡å‹æä¾›æ›´è‡ªç„¶çš„è§†è§‰æ¨ç†è½½ä½“ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèåˆä¸“ä¸šè§†è§‰æ¨¡å‹ï¼Œå¼ºåŒ–æ„ŸçŸ¥ä¸æ¨ç†  
Sketchpad å…è®¸åœ¨ç»˜å›¾è¿‡ç¨‹ä¸­è°ƒç”¨â€œä¸“ç²¾è§†è§‰æ¨¡å‹â€ï¼šæ¯”å¦‚ç”¨ç›®æ ‡æ£€æµ‹æ¨¡å‹ç”» bounding boxã€ç”¨åˆ†å‰²æ¨¡å‹ç”»æ©ç ï¼ˆmaskï¼‰ç­‰ã€‚é€šè¿‡æ•´åˆè¿™äº›å·¥å…·ï¼Œæ¨¡å‹èƒ½åœ¨â€œç”»è‰ç¨¿â€æ—¶æ›´ç²¾å‡†åœ°æ„ŸçŸ¥è§†è§‰ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æå‡æ¨ç†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»¥â€œè§†è§‰äº§ç‰©â€é©±åŠ¨è§„åˆ’ä¸æ¨ç†  
æ¨¡å‹ä¸å†åªä¾èµ–æ–‡æœ¬æ€ç»´é“¾ï¼Œè€Œæ˜¯**æ ¹æ®è‡ªå·±ç”»å‡ºçš„è§†è§‰å†…å®¹**æ¥æ¨è¿›è§„åˆ’ä¸æ¨ç†ã€‚è‰ç¨¿æœ¬ä¸Šçš„æ¯ä¸€ç¬”ã€æ¯ä¸€ä¸ªæ ‡è®°éƒ½æˆä¸ºæ¨ç†è¿‡ç¨‹çš„â€œä¸­é—´çº¿ç´¢â€ï¼Œè®©å¤šæ¨¡æ€æ¨ç†æ›´æ¥è¿‘äººç±»â€œè¾¹ç”»è¾¹æƒ³â€çš„è®¤çŸ¥æ¨¡å¼ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨**æ•°å­¦ä»»åŠ¡ï¼ˆå‡ ä½•ã€å‡½æ•°ã€å›¾è¡¨ã€å›½é™…è±¡æ£‹ç­‰ï¼‰**å’Œ**å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡**ä¸­å±•å¼€å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼š  
- å¯¹æ¯”æ— ç»˜å›¾èƒ½åŠ›çš„å¼ºåŸºçº¿æ¨¡å‹ï¼ŒSketchpad åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—æå‡â€”â€”æ•°å­¦ä»»åŠ¡å¹³å‡å¢ç›Š 12.7%ï¼Œè§†è§‰ä»»åŠ¡å¹³å‡å¢ç›Š 8.6%ã€‚  
- ç»“åˆ Sketchpad çš„ GPT-4o åœ¨å¤šä¸ªæƒå¨åŸºå‡†ä¸Šåˆ·æ–° SOTAï¼šV*Bench è¾¾ 80.3%ã€BLINK ç©ºé—´æ¨ç†è¾¾ 83.9%ã€è§†è§‰å¯¹åº”ä»»åŠ¡è¾¾ 80.8%ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **è®¤çŸ¥æ¨¡æ‹Ÿè§†è§’**ï¼šä»â€œäººç±»å¦‚ä½•ç”¨ç»˜å›¾è¾…åŠ©æ¨ç†â€ä¸­æ±²å–çµæ„Ÿï¼Œä¸ºå¤§æ¨¡å‹è®¾è®¡æ›´è´´åˆç”Ÿç‰©è®¤çŸ¥çš„æ¨ç†èŒƒå¼ï¼Œè·³å‡ºçº¯æ–‡æœ¬æ€ç»´é“¾çš„å±€é™ã€‚  
2. **å·¥å…·é“¾æ•´åˆ**ï¼šå±•ç¤ºäº†â€œå¤§æ¨¡å‹ + ä¸“ç²¾å·¥å…·ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²æ¨¡å‹ï¼‰â€çš„åä½œæ¨¡å¼ï¼Œä¸ºå¤šæ¨¡æ€å·¥å…·ç”Ÿæ€æ„å»ºæä¾›å‚è€ƒã€‚  
3. **ä»»åŠ¡æ‹“å±•æ½œåŠ›**ï¼šåœ¨æ•°å­¦ã€ç©ºé—´æ¨ç†ç­‰é¢†åŸŸçš„æˆåŠŸï¼Œè¯æ˜è§†è§‰è‰ç¨¿å¼æ¨ç†å¯¹â€œéœ€å¯è§†åŒ–è¾…åŠ©çš„å¤æ‚ä»»åŠ¡â€æœ‰æ™®é€‚ä»·å€¼ï¼Œæœªæ¥å¯å»¶ä¼¸åˆ°æ•™è‚²ã€è®¾è®¡ã€ç§‘ç ”ç­‰åœºæ™¯ã€‚  
4. **å¼€æºç”Ÿæ€**ï¼šé¡¹ç›®ä»£ç ä¸æ•°æ®å¼€æºï¼ˆhttps://visualsketchpad.github.io/ï¼‰ï¼Œä¸ºç ”ç©¶è€…å¤ç°ã€æ”¹è¿›æä¾›äº†ä¾¿åˆ©ï¼Œæ¨åŠ¨é¢†åŸŸå¿«é€Ÿè¿­ä»£ã€‚  
```

## retrieval-augmented-perception--high-resolution-image-perception-meets-visual-rag
### Abstract
High-resolution (HR) image perception remains a key challenge in multimodal
large language models (MLLMs). To overcome the limitations of existing methods,
this paper shifts away from prior dedicated heuristic approaches and revisits
the most fundamental idea to HR perception by enhancing the long-context
capability of MLLMs, driven by recent advances in long-context techniques like
retrieval-augmented generation (RAG) for general LLMs. Towards this end, this
paper presents the first study exploring the use of RAG to address HR
perception challenges. Specifically, we propose Retrieval-Augmented Perception
(RAP), a training-free framework that retrieves and fuses relevant image crops
while preserving spatial context using the proposed Spatial-Awareness Layout.
To accommodate different tasks, the proposed Retrieved-Exploration Search
(RE-Search) dynamically selects the optimal number of crops based on model
confidence and retrieval scores. Experimental results on HR benchmarks
demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving
a 43% improvement on $V^*$ Bench and 19% on HR-Bench.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜åˆ†è¾¨ç‡å›¾åƒæ„ŸçŸ¥æ–°çªç ´ï¼šRetrieval - Augmented Perception å¼€å¯è§†è§‰RAGæ–°æ€è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œé«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒæ„ŸçŸ¥å§‹ç»ˆæ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å¤šä¸ºä¸“ç”¨çš„å¯å‘å¼æ–¹æ³•ï¼Œå­˜åœ¨ä¸€å®šå±€é™æ€§ã€‚è€Œéšç€é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é•¿ä¸Šä¸‹æ–‡æŠ€æœ¯ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ŒRAGï¼‰çš„å‘å±•ï¼Œæœ¬æ–‡è¯•å›¾ä»å¢å¼ºMLLMsçš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›è¿™ä¸€æœ€åŸºç¡€çš„æ€è·¯å‡ºå‘ï¼Œæ¥è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒæ„ŸçŸ¥é—®é¢˜ï¼Œè¿™ä¹Ÿæ˜¯é¦–æ¬¡æ¢ç´¢ä½¿ç”¨RAGæ¥åº”å¯¹é«˜åˆ†è¾¨ç‡æ„ŸçŸ¥æŒ‘æˆ˜çš„ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRetrieval - Augmented Perceptionï¼ˆRAPï¼‰æ¡†æ¶
RAPæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿæ£€ç´¢å¹¶èåˆç›¸å…³çš„å›¾åƒè£å‰ªå—ï¼ŒåŒæ—¶ä½¿ç”¨æ‰€æå‡ºçš„ç©ºé—´æ„ŸçŸ¥å¸ƒå±€ï¼ˆSpatial - Awareness Layoutï¼‰æ¥ä¿ç•™ç©ºé—´ä¸Šä¸‹æ–‡ã€‚è¿™ä¸€æ¡†æ¶æ‘†è„±äº†ä¼ ç»Ÿçš„ä¸“ç”¨å¯å‘å¼æ€è·¯ï¼Œå€ŸåŠ©RAGç›¸å…³æŠ€æœ¯æ€è·¯æ¥æå‡é«˜åˆ†è¾¨ç‡å›¾åƒæ„ŸçŸ¥èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡Retrieved - Exploration Searchï¼ˆRE - Searchï¼‰æœºåˆ¶
ä¸ºäº†é€‚åº”ä¸åŒä»»åŠ¡ï¼ŒRE - SearchåŸºäºæ¨¡å‹ç½®ä¿¡åº¦å’Œæ£€ç´¢åˆ†æ•°åŠ¨æ€é€‰æ‹©æœ€ä¼˜çš„è£å‰ªå—æ•°é‡ã€‚è¿™ç§åŠ¨æ€é€‰æ‹©æœºåˆ¶è®©æ¨¡å‹åœ¨å¤„ç†ä¸åŒé«˜åˆ†è¾¨ç‡å›¾åƒæ„ŸçŸ¥ä»»åŠ¡æ—¶æ›´å…·çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨é«˜åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAPå±•ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚ä»¥LLaVA - v1.5 - 13Bä¸ºä¾‹ï¼Œåœ¨$V^*$ Benchä¸Šå®ç°äº†43%çš„æ€§èƒ½æå‡ï¼Œåœ¨HR - Benchä¸Šä¹Ÿæœ‰19%çš„æ€§èƒ½æå‡ï¼Œæœ‰åŠ›åœ°è¯æ˜äº†RAPæ¡†æ¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šå°†é’ˆå¯¹é€šç”¨LLMsçš„RAGæŠ€æœ¯æ€è·¯è¿ç§»åˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒæ„ŸçŸ¥ä»»åŠ¡ä¸­ï¼Œä¸ºè·¨é¢†åŸŸæŠ€æœ¯è¿ç§»æä¾›äº†å¾ˆå¥½çš„èŒƒä¾‹ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨ä¸åŒé¢†åŸŸæŠ€æœ¯é—´çš„è”ç³»ä¸å€Ÿé‰´ã€‚
2. æ— è®­ç»ƒæ¡†æ¶ï¼šRAPä½œä¸ºæ— è®­ç»ƒæ¡†æ¶ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥é™ä½è®­ç»ƒæˆæœ¬å’Œèµ„æºæ¶ˆè€—ï¼Œå¯¹äºèµ„æºæœ‰é™æˆ–è€…è¿½æ±‚å¿«é€Ÿåº”ç”¨æ–°æ–¹æ³•çš„åœºæ™¯å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
3. åŠ¨æ€é€‰æ‹©æœºåˆ¶ï¼šRE - Searchçš„åŠ¨æ€é€‰æ‹©è£å‰ªå—æ•°é‡çš„æœºåˆ¶ï¼Œä¸ºå¤„ç†ä¸åŒä»»åŠ¡æ—¶å¦‚ä½•æ ¹æ®ä»»åŠ¡ç‰¹æ€§å’Œæ¨¡å‹çŠ¶æ€æ¥ä¼˜åŒ–è¾“å…¥å¤„ç†æ–¹å¼æä¾›äº†å‚è€ƒï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦åŠ¨æ€è°ƒæ•´è¾“å…¥å…ƒç´ æ•°é‡çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ã€‚
```

## reinforcing-spatial-reasoning-in-vision-language-models-with-interwoven-thinking-and-visual-drawing
### Abstract
As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨äº¤ç»‡æ€ç»´ä¸è§†è§‰ç»˜å›¾å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå¢å¼ºå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å—åˆ°è¶Šæ¥è¶Šå¤šå…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€æ¨ç†æ—¶ï¼Œä¸»è¦é‡‡ç”¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ç›´æ¥æ–¹å¼ï¼Œæ¨ç†å’Œç­”æ¡ˆæ¨å¯¼éƒ½çº¯é€šè¿‡æ–‡æœ¬è¿›è¡Œï¼Œä»…è¾“å…¥æ˜¯å¤šæ¨¡æ€çš„ã€‚è¿™ä½¿å¾—è¿™äº›æ–¹æ³•åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æ ¹æœ¬å±€é™ï¼Œå› ä¸ºç©ºé—´æ¨ç†ä»»åŠ¡éœ€è¦ç²¾ç¡®çš„å‡ ä½•ç†è§£å’Œè¿ç»­çš„ç©ºé—´è·Ÿè¸ªèƒ½åŠ›ï¼Œè€Œäººç±»æ˜¯é€šè¿‡å¿ƒç†å¯è§†åŒ–å’Œæ“ä½œæ¥å®ç°è¿™äº›èƒ½åŠ›çš„ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæœ¬æ–‡æå‡ºæ–°èŒƒå¼æ¥è®©LVLMsåœ¨è§†è§‰ç©ºé—´ä¸­é€šè¿‡åŸºæœ¬ç»˜å›¾æ“ä½œè¿›è¡Œæ¨ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºâ€œç»˜å›¾ç©ºé—´æ¨ç†â€æ–°èŒƒå¼
è®©å¤§è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨è§†è§‰ç©ºé—´ä¸­é€šè¿‡åŸºæœ¬ç»˜å›¾æ“ä½œæ¥æ¨ç†ã€‚ä¸ºæ¨¡å‹é…å¤‡åŒ…æ‹¬æ ‡æ³¨è¾¹ç•Œæ¡†ã€ç»˜åˆ¶è¾…åŠ©çº¿ç­‰åŸºæœ¬ç»˜å›¾æ“ä½œï¼Œä½¿æ¨¡å‹èƒ½é€šè¿‡ç›´æ¥è§†è§‰æ“ä½œè¡¨è¾¾å’Œåˆ†æç©ºé—´å…³ç³»ï¼ŒåŒæ—¶é¿å…äº†ä¹‹å‰å·¥å…·é›†æˆæ¨ç†æ–¹æ³•ä¸­ä¸“ä¸šæ„ŸçŸ¥å·¥å…·å¸¦æ¥çš„æ€§èƒ½ä¸Šé™é—®é¢˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶
å¼€å‘äº†ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶æ¥åŸ¹å…»æ¨¡å‹èƒ½åŠ›ï¼šé¦–å…ˆåˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒä»¥å»ºç«‹åŸºæœ¬ç»˜å›¾èƒ½åŠ›ï¼›ç„¶åé€šè¿‡åå°„æ‹’ç»é‡‡æ ·å¢å¼ºè‡ªæˆ‘åæ€è¡Œä¸ºï¼›æœ€åç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–ç›®æ ‡å¥–åŠ±ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸åŒçš„ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œåä¸ºVILASRçš„æ¨¡å‹æŒç»­è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚è¿™äº›åŸºå‡†æµ‹è¯•æ¶µç›–è¿·å®«å¯¼èˆªã€é™æ€ç©ºé—´æ¨ç†ã€åŸºäºè§†é¢‘çš„æ¨ç†å’ŒåŸºäºå¤šè§†å›¾çš„æ¨ç†ä»»åŠ¡ï¼Œå¹³å‡æå‡å¹…åº¦è¾¾åˆ°18.4%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨å¤„ç†å¤šæ¨¡æ€ç‰¹åˆ«æ˜¯ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¯è€ƒè™‘å¼•å…¥ç±»ä¼¼çš„â€œè§†è§‰æ“ä½œâ€èŒƒå¼ï¼Œçªç ´çº¯æ–‡æœ¬æ¨ç†çš„å±€é™ï¼›ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶çš„è®¾è®¡æ€è·¯ï¼Œä»åŸºç¡€èƒ½åŠ›å»ºç«‹åˆ°è¡Œä¸ºå¢å¼ºå†åˆ°å¥–åŠ±ä¼˜åŒ–ï¼Œä¸ºåŸ¹å…»æ¨¡å‹ç‰¹å®šå¤æ‚èƒ½åŠ›æä¾›äº†åˆ†é˜¶æ®µæ¨è¿›çš„å‚è€ƒæ¨¡å¼ï¼›é’ˆå¯¹ä»»åŠ¡ç“¶é¢ˆï¼ˆå¦‚ç©ºé—´æ¨ç†å¯¹å‡ ä½•å’Œè·Ÿè¸ªèƒ½åŠ›çš„éœ€æ±‚ï¼‰ï¼Œæ€è€ƒé€šè¿‡èµ‹äºˆæ¨¡å‹æ›´è´´åˆäººç±»è®¤çŸ¥æ–¹å¼ï¼ˆå¦‚å¿ƒç†å¯è§†åŒ–å¯¹åº”è§†è§‰ç»˜å›¾æ“ä½œï¼‰çš„æ‰‹æ®µæ¥çªç ´æ€§èƒ½ä¸Šé™ï¼Œè¿™ç§ä»äººç±»è®¤çŸ¥å€Ÿé‰´è§£å†³æ¨¡å‹å±€é™çš„æ€è·¯å€¼å¾—åœ¨å…¶ä»–ä»»åŠ¡åœºæ™¯ä¸­å°è¯•ã€‚
```

## dyfo--a-training-free-dynamic-focus-visual-search-for-enhancing-lmms-in-fine-grained-visual-understanding
### Abstract
Humans can effortlessly locate desired objects in cluttered environments,
relying on a cognitive mechanism known as visual search to efficiently filter
out irrelevant information and focus on task-related regions. Inspired by this
process, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing
visual search method that enhances fine-grained visual understanding in large
multimodal models (LMMs). Unlike existing approaches which require additional
modules or data collection, Dyfo leverages a bidirectional interaction between
LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to
simulate human-like focus adjustments. This enables LMMs to focus on key visual
regions while filtering out irrelevant content, without introducing additional
training caused by vocabulary expansion or the integration of specialized
localization modules. Experimental results demonstrate that Dyfo significantly
improves fine-grained visual understanding and reduces hallucination issues in
LMMs, achieving superior performance across both fixed and dynamic resolution
models. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | DyFoï¼šæ— éœ€è®­ç»ƒï¼ŒåŠ©åŠ›å¤§æ¨¡å‹å®ç°åŠ¨æ€èšç„¦çš„ç»†ç²’åº¦è§†è§‰ç†è§£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»åœ¨æ‚ä¹±ç¯å¢ƒä¸­èƒ½è½»æ¾å®šä½ç›®æ ‡ç‰©ä½“ï¼Œä¾é çš„æ˜¯â€œè§†è§‰æœç´¢â€è¿™ä¸€è®¤çŸ¥æœºåˆ¶ï¼Œé«˜æ•ˆè¿‡æ»¤æ— å…³ä¿¡æ¯å¹¶èšç„¦ä»»åŠ¡ç›¸å…³åŒºåŸŸã€‚ä½†å½“å‰å¤§çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç»†ç²’åº¦è§†è§‰ç†è§£ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç°æœ‰æå‡æ–¹æ³•å¾€å¾€éœ€è¦é¢å¤–æ¨¡å—æˆ–æ•°æ®æ”¶é›†ï¼Œè¿˜å¯èƒ½å› è¯æ±‡æ‰©å±•ã€æ•´åˆä¸“é—¨å®šä½æ¨¡å—å¸¦æ¥é¢å¤–è®­ç»ƒæˆæœ¬ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸‹ç²¾å‡†èšç„¦å…³é”®åŒºåŸŸä¸å‡å°‘å¹»è§‰é—®é¢˜çš„èƒ½åŠ›ã€‚å—äººç±»è§†è§‰æœç´¢æœºåˆ¶å¯å‘ï¼Œæœ¬æ–‡å¸Œæœ›æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•æ¥å¢å¼ºLMMsçš„ç»†ç²’åº¦è§†è§‰ç†è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDyFoï¼ˆDynamic Focusï¼‰æ–¹æ³•  
DyFoæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ¨æ€èšç„¦è§†è§‰æœç´¢æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„ç»†ç²’åº¦è§†è§‰ç†è§£ã€‚å®ƒåŒºåˆ«äºéœ€è¦é¢å¤–æ¨¡å—æˆ–æ•°æ®æ”¶é›†çš„ç°æœ‰æ–¹æ³•ï¼Œä»¥æ›´è½»é‡åŒ–ã€æ— è®­ç»ƒæˆæœ¬çš„æ–¹å¼æ¥ä¼˜åŒ–LMMsæ€§èƒ½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒå‘äº¤äº’ä¸MCTSæ¨¡æ‹Ÿäººç±»èšç„¦  
DyFoåˆ©ç”¨LMMså’Œè§†è§‰ä¸“å®¶ä¹‹é—´çš„åŒå‘äº¤äº’ï¼Œå€ŸåŠ©è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç®—æ³•æ¨¡æ‹Ÿç±»äººçš„ç„¦ç‚¹è°ƒæ•´è¿‡ç¨‹ã€‚è¿™ä¸€è¿‡ç¨‹è®©LMMsåœ¨è¿‡æ»¤æ— å…³å†…å®¹çš„åŒæ—¶ï¼Œèšç„¦å…³é”®è§†è§‰åŒºåŸŸï¼Œä¸”ä¸ä¼šå› è¯æ±‡æ‰©å±•æˆ–æ•´åˆä¸“é—¨å®šä½æ¨¡å—è€Œå¼•å…¥é¢å¤–è®­ç»ƒè´Ÿæ‹…ï¼Œåœ¨æ¨¡å‹è¿è¡Œé€»è¾‘å±‚é¢å®ç°äº†å¯¹äººç±»è§†è§‰æœç´¢æœºåˆ¶çš„æ¨¡æ‹Ÿä¸å€Ÿé‰´ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒDyFoåœ¨æå‡LMMsç»†ç²’åº¦è§†è§‰ç†è§£ä¸å‡å°‘å¹»è§‰é—®é¢˜ä¸Šæ•ˆæœæ˜¾è‘—ï¼Œåœ¨å›ºå®šåˆ†è¾¨ç‡å’ŒåŠ¨æ€åˆ†è¾¨ç‡çš„æ¨¡å‹ä¸­éƒ½å®ç°äº†æ›´ä¼˜æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸è§†è§‰åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ä¸æ™®é€‚æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ— è®­ç»ƒèŒƒå¼çš„æ¢ç´¢ï¼šDyFoé‡‡ç”¨æ— éœ€è®­ç»ƒçš„æ–¹å¼ä¼˜åŒ–æ¨¡å‹èƒ½åŠ›ï¼Œä¸ºåç»­åœ¨ä¸å¢åŠ è®­ç»ƒæˆæœ¬ï¼ˆå¦‚æ•°æ®ã€ç®—åŠ›ã€æ—¶é—´ï¼‰çš„æƒ…å†µä¸‹æå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½æä¾›äº†æ–°æ€è·¯ï¼Œå¯å¯å‘ç ”ç©¶è€…å…³æ³¨â€œæ— è®­ç»ƒâ€â€œå³æ’å³ç”¨â€ç±»æ–¹æ³•çš„è®¾è®¡ã€‚  
2. ç±»äººè®¤çŸ¥æœºåˆ¶å€Ÿé‰´ï¼šä»äººç±»è§†è§‰æœç´¢çš„è®¤çŸ¥æœºåˆ¶å‡ºå‘è®¾è®¡ç®—æ³•ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹ç»“åˆè®¤çŸ¥ç§‘å­¦ç†è®ºæä¾›äº†ä¼˜ç§€èŒƒä¾‹ï¼Œåç»­å¯æ¢ç´¢æ›´å¤šäººç±»è®¤çŸ¥è¿‡ç¨‹ï¼ˆå¦‚è®°å¿†ã€æ¨ç†ç­‰ï¼‰åœ¨AIæ¨¡å‹ä¼˜åŒ–ä¸­çš„åº”ç”¨ã€‚  
3. åŒå‘äº¤äº’ä¸MCTSç»“åˆï¼šå…¶åˆ©ç”¨åŒå‘äº¤äº’ä¸MCTSæ¨¡æ‹Ÿç„¦ç‚¹è°ƒæ•´çš„æŠ€æœ¯è·¯çº¿ï¼Œä¸ºæ¨¡å‹åœ¨å¤æ‚è§†è§‰ä¿¡æ¯ä¸­é«˜æ•ˆèšç„¦å…³é”®åŒºåŸŸæä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯å‚è€ƒï¼Œåœ¨éœ€è¦è§†è§‰å®šä½ã€ä¿¡æ¯è¿‡æ»¤çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­éƒ½æœ‰æ½œåœ¨å€Ÿé‰´ä»·å€¼ã€‚  
4. å¼€æºä¼˜åŠ¿ï¼šä»£ç å¼€æºï¼ˆhttps://github.com/PKU-ICST-MIPL/DyFo_CVPR2025ï¼‰æ–¹ä¾¿ç ”ç©¶è€…å¤ç°ä¸äºŒæ¬¡å¼€å‘ï¼Œåˆ©äºè¯¥æ–¹å‘æŠ€æœ¯ç”Ÿæ€çš„æ„å»ºä¸å‘å±•ã€‚  
```

## v*--guided-visual-search-as-a-core-mechanism-in-multimodal-llms
### Abstract
When we look around and perform complex tasks, how we see and selectively
process what we see is crucial. However, the lack of this visual search
mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on
important visual details, especially when handling high-resolution and visually
crowded images. To address this, we introduce V*, an LLM-guided visual search
mechanism that employs the world knowledge in LLMs for efficient visual
querying. When combined with an MLLM, this mechanism enhances collaborative
reasoning, contextual understanding, and precise targeting of specific visual
elements. This integration results in a new MLLM meta-architecture, named Show,
sEArch, and TelL (SEAL). We further create V*Bench, a benchmark specifically
designed to evaluate MLLMs in their ability to process high-resolution images
and focus on visual details. Our study highlights the necessity of
incorporating visual search capabilities into multimodal systems. The code is
available https://github.com/penghao-wu/vstar.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | V*ï¼šä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹æ³¨å…¥è§†è§‰æœç´¢â€œçµé­‚â€ï¼Œçªç ´ç»†èŠ‚å¤„ç†ç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººç±»æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶ï¼Œâ€œçœ‹ä»€ä¹ˆã€æ€ä¹ˆçœ‹â€çš„è§†è§‰æœç´¢èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå®ƒèƒ½å¸®åŠ©æˆ‘ä»¬åœ¨çº·ç¹çš„è§†è§‰ä¿¡æ¯ä¸­èšç„¦å…³é”®ç»†èŠ‚ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å´ç¼ºå°‘è¿™æ ·çš„è§†è§‰æœç´¢æœºåˆ¶ï¼Œé¢å¯¹é«˜åˆ†è¾¨ç‡ã€è§†è§‰å…ƒç´ å¯†é›†çš„å›¾åƒæ—¶ï¼Œå¾ˆéš¾ç²¾å‡†æ•æ‰é‡è¦è§†è§‰ç»†èŠ‚ï¼Œè¿™æå¤§é™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ä¸ºå¡«è¡¥è¿™ä¸€æŠ€æœ¯ç¼ºå£ï¼Œè®ºæ–‡æå‡ºä¸ºMLLMså¼•å…¥è§†è§‰æœç´¢èƒ½åŠ›çš„å…¨æ–°æ€è·¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºL LMå¼•å¯¼çš„è§†è§‰æœç´¢æœºåˆ¶V*  
V* å€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å­˜å‚¨çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œè®©å¤šæ¨¡æ€æ¨¡å‹èƒ½æ›´é«˜æ•ˆåœ°å‘èµ·â€œè§†è§‰æŸ¥è¯¢â€ã€‚ä¸å†æ˜¯æ— å·®åˆ«å¤„ç†å›¾åƒå†…å®¹ï¼Œè€Œæ˜¯åƒäººç±»ä¸»åŠ¨æœç´¢å…³é”®ä¿¡æ¯ä¸€æ ·ï¼Œç”¨LLMçš„è®¤çŸ¥æ¥æŒ‡å¯¼è§†è§‰ç³»ç»Ÿèšç„¦ã€ç­›é€‰ï¼Œè®©æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„å¤„ç†æ›´æœ‰â€œç›®çš„æ€§â€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºSEALå…ƒæ¶æ„ï¼ˆShow, sEArch, and TelLï¼‰  
å°†V* ä¸MLLMç»“åˆåï¼Œå½¢æˆäº†SEALè¿™ä¸€å…¨æ–°å¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„ã€‚å®ƒè®©å¤šæ¨¡æ€åä½œæ¨ç†ã€ä¸Šä¸‹æ–‡ç†è§£ä»¥åŠå¯¹ç‰¹å®šè§†è§‰å…ƒç´ çš„ç²¾å‡†å®šä½éƒ½å¾—åˆ°å¢å¼ºâ€”â€”â€œShowâ€å‘ˆç°è§†è§‰å†…å®¹ï¼Œâ€œsEArchâ€ç”¨V* å®šå‘æœç´¢å…³é”®ç»†èŠ‚ï¼Œâ€œTelLâ€å†ç”¨è¯­è¨€å®Œæˆè¾“å‡ºï¼Œä¸‰æ­¥åä½œè®©å¤šæ¨¡æ€èƒ½åŠ›æ›´æ™ºèƒ½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ‰“é€ V*Benchä¸“å±åŸºå‡†æµ‹è¯•é›†  
ä¸ºäº†è¡¡é‡MLLMsåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç»†èŠ‚å¤„ç†ã€è§†è§‰æœç´¢æ–¹é¢çš„èƒ½åŠ›ï¼Œè®ºæ–‡å›¢é˜Ÿä¸“é—¨æ„å»ºäº†V*Benchã€‚å®ƒä¸ºè¡Œä¸šæä¾›äº†ä¸€å¥—é’ˆå¯¹æ€§æå¼ºçš„è¯„ä¼°æ ‡å‡†ï¼Œèƒ½æ¸…æ™°æ£€éªŒæ¨¡å‹æ˜¯å¦çœŸæ­£å…·å¤‡ç²¾å‡†å¤„ç†å¤æ‚è§†è§‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡åœ¨V*Benchä¸Šçš„æµ‹è¯•ç­‰å®éªŒï¼Œæœ‰åŠ›éªŒè¯äº†ï¼šå¼•å…¥V* è§†è§‰æœç´¢æœºåˆ¶åï¼Œå¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡ã€è§†è§‰æ‹¥æŒ¤å›¾åƒæ—¶ï¼Œå¯¹å…³é”®ç»†èŠ‚çš„èšç„¦å’Œç†è§£èƒ½åŠ›æ˜¾è‘—æå‡ï¼›SEALæ¶æ„ä¹Ÿå±•ç°å‡ºæ›´ä¼˜çš„åä½œæ¨ç†ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç°ã€‚è¿™ä¸€ç³»åˆ—å®éªŒå……åˆ†è¯´æ˜è§†è§‰æœç´¢èƒ½åŠ›å¯¹å¤šæ¨¡æ€ç³»ç»Ÿçš„å¿…è¦æ€§ï¼Œä¸ºåç»­æŠ€æœ¯è¿­ä»£æä¾›äº†æœ‰åŠ›ä½è¯ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æœºåˆ¶åˆ›æ–°è§’åº¦ï¼šä¸ºå¤šæ¨¡æ€æ¨¡å‹è¡¥ä¸Šâ€œè§†è§‰æœç´¢â€è¿™ä¸€äººç±»æ„ŸçŸ¥ç³»ç»Ÿçš„å…³é”®èƒ½åŠ›ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨â€œæ¨¡æ€åä½œä¸­æ›´ç²¾ç»†çš„æ§åˆ¶é€»è¾‘â€ï¼Œæœªæ¥å¯æ¢ç´¢æ›´å¤šç±»ä¼¼â€œè®©è¯­è¨€å¼•å¯¼è§†è§‰èšç„¦â€çš„è·¨æ¨¡æ€ååŒèŒƒå¼ã€‚  
2. æ¶æ„è®¾è®¡è§’åº¦ï¼šSEALçš„â€œShow - sEArch - TelLâ€åˆ†å±‚åä½œæ€è·¯ï¼Œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¨¡å—åŒ–è®¾è®¡æä¾›äº†æ–°å‚è€ƒï¼Œä¸åŒæ¨¡æ€ä¸å†æ˜¯ç®€å•æ‹¼æ¥ï¼Œè€Œæ˜¯æœ‰åˆ†å·¥ã€æœ‰å¼•å¯¼çš„æ·±åº¦ååŒã€‚  
3. è¯„ä¼°ä½“ç³»è§’åº¦ï¼šV*Benchçš„æ„å»ºå‡¸æ˜¾â€œé’ˆå¯¹æ€§ benchmark å¯¹æŠ€æœ¯æ–¹å‘çš„æ¨åŠ¨ä½œç”¨â€ï¼Œå½“æ–°èƒ½åŠ›ï¼ˆå¦‚è§†è§‰æœç´¢ï¼‰å‡ºç°æ—¶ï¼Œå®šåˆ¶åŒ–è¯„ä¼°åŸºå‡†èƒ½æ›´ç²¾å‡†è¡¡é‡è¿›å±•ï¼Œè¿™ä¸ºé¢†åŸŸå†…æ‰“é€ ç»†åˆ†åœºæ™¯è¯„ä¼°å·¥å…·æä¾›äº†èŒƒä¾‹ã€‚  
4. è½åœ°ä»·å€¼è§’åº¦ï¼šè®©å¤šæ¨¡æ€æ¨¡å‹æ›´æ“…é•¿å¤„ç†å¤æ‚è§†è§‰ç»†èŠ‚ï¼Œèƒ½ç›´æ¥èµ‹èƒ½æ™ºèƒ½é©¾é©¶ã€æ™ºèƒ½å®‰é˜²ã€å›¾åƒç¼–è¾‘ç­‰å¯¹è§†è§‰ç²¾å‡†åº¦è¦æ±‚é«˜çš„ä¸‹æ¸¸åœºæ™¯ï¼Œæ‹“å®½MLLMsçš„å®ç”¨è¾¹ç•Œã€‚  
```

## from-the-least-to-the-most--building-a-plug-and-play-visual-reasoner-via-data-synthesis
### Abstract
We explore multi-step reasoning in vision-language models (VLMs). The problem
is challenging, as reasoning data consisting of multiple steps of visual and
language processing are barely available. To overcome the challenge, we first
introduce a least-to-most visual reasoning paradigm, which interleaves steps of
decomposing a question into sub-questions and invoking external tools for
resolving sub-questions. Based on the paradigm, we further propose a novel data
synthesis approach that can automatically create questions and multi-step
reasoning paths for an image in a bottom-up manner. Our approach divides the
complex synthesis task into a few simple sub-tasks, and (almost entirely)
relies on open-sourced models to accomplish the sub-tasks. Therefore, the
entire synthesis process is reproducible and cost-efficient, and the
synthesized data is quality guaranteed. With the approach, we construct $50$k
visual reasoning examples. Then, we develop a visual reasoner through
supervised fine-tuning, which is capable of generally enhancing the reasoning
abilities of a wide range of existing VLMs in a plug-and-play fashion.
Extensive experiments indicate that the visual reasoner can consistently and
significantly improve four VLMs on four VQA benchmarks. Our code and dataset
are available at https://github.com/steven-ccq/VisualReasoner.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»ç®€åˆ°ç¹ï¼šç”¨æ•°æ®åˆæˆæ‰“é€ å³æ’å³ç”¨çš„è§†è§‰æ¨ç†å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­æ¢ç´¢å¤šæ­¥æ¨ç†æ˜¯æå…·æŒ‘æˆ˜æ€§çš„ï¼ŒåŸå› åœ¨äºåŒ…å«å¤šæ­¥è§†è§‰å’Œè¯­è¨€å¤„ç†çš„æ¨ç†æ•°æ®æä¸ºç¨€ç¼ºã€‚ç°æœ‰çš„æ•°æ®éš¾ä»¥æ”¯æ’‘æ¨¡å‹è¿›è¡Œå¤æ‚çš„å¤šæ­¥è§†è§‰è¯­è¨€æ¨ç†å­¦ä¹ ï¼Œè¿™é™åˆ¶äº†VLMsåœ¨éœ€è¦æ·±åº¦æ¨ç†çš„è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ‰€ä»¥è¿«åˆ‡éœ€è¦ä¸€ç§æ–¹æ³•æ¥è§£å†³æ¨ç†æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œè¿›è€Œæå‡VLMsçš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºç”±æµ…å…¥æ·±çš„è§†è§‰æ¨ç†èŒƒå¼
å¼•å…¥äº†least - to - mostè§†è§‰æ¨ç†èŒƒå¼ï¼Œè¯¥èŒƒå¼å°†æŠŠä¸€ä¸ªé—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ä»¥åŠè°ƒç”¨å¤–éƒ¨å·¥å…·è§£å†³å­é—®é¢˜çš„æ­¥éª¤è¿›è¡Œäº¤é”™å¤„ç†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œèƒ½å¤Ÿé€æ­¥å¤„ç†å¤æ‚çš„è§†è§‰ - è¯­è¨€æ¨ç†ä»»åŠ¡ï¼ŒæŠŠå¤§çš„æ¨ç†é—®é¢˜æ‹†è§£ä¸ºå¯é€æ­¥è§£å†³çš„å­é—®é¢˜åºåˆ—ï¼Œä¸ºåç»­çš„æ¨ç†è¿‡ç¨‹æä¾›äº†æ¸…æ™°çš„é€»è¾‘æ¡†æ¶ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–°é¢–çš„æ•°æ®åˆæˆæ–¹æ³•
åŸºäºä¸Šè¿°èŒƒå¼ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®åˆæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•ä»¥è‡ªåº•å‘ä¸Šçš„æ–¹å¼ä¸ºå›¾åƒè‡ªåŠ¨ç”Ÿæˆé—®é¢˜å’Œå¤šæ­¥æ¨ç†è·¯å¾„ã€‚å®ƒå°†å¤æ‚çš„åˆæˆä»»åŠ¡åˆ†è§£ä¸ºå‡ ä¸ªç®€å•çš„å­ä»»åŠ¡ï¼Œå¹¶ä¸”ï¼ˆå‡ ä¹å®Œå…¨ï¼‰ä¾èµ–å¼€æºæ¨¡å‹æ¥å®Œæˆè¿™äº›å­ä»»åŠ¡ã€‚è¿™ä½¿å¾—æ•´ä¸ªåˆæˆè¿‡ç¨‹å…·æœ‰å¯é‡å¤æ€§ä¸”æˆæœ¬æ•ˆç›Šé«˜ï¼ŒåŒæ—¶ä¿è¯äº†åˆæˆæ•°æ®çš„è´¨é‡ã€‚åˆ©ç”¨è¯¥æ–¹æ³•æ„å»ºäº†5ä¸‡ä¸ªè§†è§‰æ¨ç†ç¤ºä¾‹ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æ•°æ®æ”¯æ’‘ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå³æ’å³ç”¨çš„è§†è§‰æ¨ç†å™¨å¼€å‘
é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒå¼€å‘äº†ä¸€ä¸ªè§†è§‰æ¨ç†å™¨ï¼Œè¯¥æ¨ç†å™¨èƒ½å¤Ÿä»¥å³æ’å³ç”¨çš„æ–¹å¼æ™®éå¢å¼ºå¤§é‡ç°æœ‰VLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹å¼ä¸éœ€è¦å¯¹ç°æœ‰VLMsè¿›è¡Œå¤§è§„æ¨¡çš„ç»“æ„æ”¹é€ ï¼Œåªéœ€æ¥å…¥è¯¥æ¨ç†å™¨å°±èƒ½æå‡æ¨ç†æ€§èƒ½ï¼Œå¤§å¤§æé«˜äº†æ–¹æ³•çš„å®ç”¨æ€§å’Œé€šç”¨æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥è§†è§‰æ¨ç†å™¨èƒ½å¤Ÿåœ¨å››ä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¸”æ˜¾è‘—åœ°æå‡å››ä¸ªVLMsçš„æ€§èƒ½ã€‚è¿™å……åˆ†è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨æå‡VLMså¤šæ­¥æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæ— è®ºæ˜¯æ•°æ®åˆæˆæ–¹æ³•è¿˜æ˜¯åŸºäºæ­¤è®­ç»ƒçš„è§†è§‰æ¨ç†å™¨ï¼Œéƒ½åœ¨å®é™…çš„åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨ç†èŒƒå¼çš„è®¾è®¡æ€è·¯ï¼šå°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºå­ä»»åŠ¡å¹¶äº¤é”™å¤„ç†çš„èŒƒå¼ï¼Œä¸ºè§£å†³å…¶ä»–å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æä¾›äº†æ€è·¯ï¼Œå¯å€Ÿé‰´è¿™ç§ä»»åŠ¡åˆ†è§£ä¸æ­¥éª¤äº¤é”™çš„æ€æƒ³æ¥å¤„ç†ä¸åŒé¢†åŸŸçš„å¤æ‚é—®é¢˜ã€‚
2. æ•°æ®åˆæˆçš„æ–¹æ³•ï¼šæŠŠå¤æ‚åˆæˆä»»åŠ¡åˆ†è§£ä¸ºç®€å•å­ä»»åŠ¡å¹¶åˆ©ç”¨å¼€æºæ¨¡å‹å®Œæˆçš„ç­–ç•¥ï¼Œåœ¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸä¸­ï¼Œä¸ºè·å–é«˜è´¨é‡è®­ç»ƒæ•°æ®æä¾›äº†å¯å‚è€ƒçš„èŒƒä¾‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€é¢†åŸŸï¼Œå¯å­¦ä¹ è¿™ç§ä½æˆæœ¬ã€å¯å¤ç°çš„æ•°æ®é›†æ„å»ºæ–¹å¼ã€‚
3. å³æ’å³ç”¨çš„æ¨¡å‹å¢å¼ºæ–¹å¼ï¼šå¼€å‘å³æ’å³ç”¨çš„æ¨¡å—æ¥å¢å¼ºç°æœ‰æ¨¡å‹æ€§èƒ½çš„åšæ³•ï¼Œå¯¹äºæå‡å·²æœ‰æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„èƒ½åŠ›å…·æœ‰å¾ˆå¥½çš„å€Ÿé‰´æ„ä¹‰ï¼Œé¿å…äº†å¯¹ç°æœ‰æˆç†Ÿæ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡é‡æ„ï¼ŒèŠ‚çœäº†å¼€å‘æˆæœ¬å’Œæ—¶é—´ã€‚
```

## chain-of-focus--adaptive-visual-search-and-zooming-for-multimodal-reasoning-via-rl
### Abstract
Vision language models (VLMs) have achieved impressive performance across a
variety of computer vision tasks. However, the multimodal reasoning capability
has not been fully explored in existing models. In this paper, we propose a
Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and
zooming in on key image regions based on obtained visual cues and the given
questions, achieving efficient multimodal reasoning. To enable this CoF
capability, we present a two-stage training pipeline, including supervised
fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we
construct the MM-CoF dataset, comprising 3K samples derived from a visual agent
designed to adaptively identify key regions to solve visual tasks with
different image resolutions and questions. We use MM-CoF to fine-tune the
Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome
accuracies and formats as rewards to update the Qwen2.5-VL model, enabling
further refining the search and reasoning strategy of models without human
priors. Our model achieves significant improvements on multiple benchmarks. On
the V* benchmark that requires strong visual reasoning capability, our model
outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to
4K, demonstrating the effectiveness of the proposed CoF method and facilitating
the more efficient deployment of VLMs in practical applications.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Chain-of-Focusï¼šç”¨å¼ºåŒ–å­¦ä¹ å®ç°å¤šæ¨¡æ€æ¨ç†çš„è‡ªé€‚åº”è§†è§‰æœç´¢ä¸ç¼©æ”¾

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¼—å¤šè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººç©ç›®çš„æˆç»©ï¼Œä½†ç°æœ‰æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æŒ–æ˜ã€‚åœ¨å¤„ç†ä¸åŒå›¾åƒåˆ†è¾¨ç‡å’Œé—®é¢˜æ—¶ï¼Œå¦‚ä½•è®©VLMsè‡ªé€‚åº”åœ°èšç„¦å’Œæ”¾å¤§å…³é”®å›¾åƒåŒºåŸŸä»¥å®ç°é«˜æ•ˆå¤šæ¨¡æ€æ¨ç†ï¼Œæ˜¯å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºChain-of-Focusï¼ˆCoFï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡VLMsåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºChain-of-Focusï¼ˆCoFï¼‰æ–¹æ³•  
è¯¥æ–¹æ³•è®©è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤ŸåŸºäºè·å–çš„è§†è§‰çº¿ç´¢å’Œç»™å®šé—®é¢˜ï¼Œè‡ªé€‚åº”åœ°èšç„¦å¹¶æ”¾å¤§å…³é”®å›¾åƒåŒºåŸŸï¼Œä»¥æ­¤å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ï¼Œä¸ºVLMsåœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡æ—¶æä¾›äº†æ›´æ™ºèƒ½çš„åŒºåŸŸå…³æ³¨ç­–ç•¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ä¸¤é˜¶æ®µè®­ç»ƒ pipeline  
ç¬¬ä¸€é˜¶æ®µæ˜¯æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼šæ„å»ºäº†MM - CoFæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«3Kä¸ªæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ¥è‡ªä¸€ä¸ªè§†è§‰æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“å¯è‡ªé€‚åº”è¯†åˆ«å…³é”®åŒºåŸŸä»¥è§£å†³ä¸åŒå›¾åƒåˆ†è¾¨ç‡å’Œé—®é¢˜ä¸‹çš„è§†è§‰ä»»åŠ¡ã€‚åˆ©ç”¨MM - CoFæ•°æ®é›†å¯¹Qwen2.5 - VLæ¨¡å‹è¿›è¡Œå†·å¯åŠ¨å¾®è°ƒï¼Œä¸ºæ¨¡å‹æ‰“ä¸‹åŸºç¡€ã€‚  
ç¬¬äºŒé˜¶æ®µæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼šå°†ç»“æœçš„å‡†ç¡®æ€§å’Œæ ¼å¼ä½œä¸ºå¥–åŠ±æ¥æ›´æ–°Qwen2.5 - VLæ¨¡å‹ï¼Œä½¿æ¨¡å‹åœ¨æ²¡æœ‰äººç±»å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æœç´¢å’Œæ¨ç†ç­–ç•¥ï¼Œè®©æ¨¡å‹èƒ½è‡ªä¸»åœ°æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ã€‚åœ¨å¯¹è§†è§‰æ¨ç†èƒ½åŠ›è¦æ±‚è¾ƒé«˜çš„V*åŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨ä»224åˆ°4Kçš„8ç§å›¾åƒåˆ†è¾¨ç‡ä¸‹ï¼Œè¯¥æ¨¡å‹æ¯”ç°æœ‰VLMsè¡¨ç°å¥½5%ï¼Œå……åˆ†è¯æ˜äº†æ‰€æå‡ºçš„CoFæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºVLMsåœ¨å®é™…åº”ç”¨ä¸­çš„æ›´é«˜æ•ˆéƒ¨ç½²æä¾›äº†åŠ©åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€ä»»åŠ¡å¤„ç†æ€è·¯ï¼šCoFæ–¹æ³•ä¸ºå¤„ç†å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å¦‚ä½•èšç„¦å…³é”®åŒºåŸŸæä¾›äº†åˆ›æ–°æ€è·¯ï¼Œå…¶ä»–ç ”ç©¶è€…å¯å€Ÿé‰´è¿™ç§è®©æ¨¡å‹è‡ªé€‚åº”å…³æ³¨å…³é”®ä¿¡æ¯çš„æ–¹å¼ï¼Œæ‹“å±•åˆ°æ›´å¤šå¤šæ¨¡æ€ä»»åŠ¡åœºæ™¯ã€‚  
2. ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼šå…ˆé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒåˆ©ç”¨æ„å»ºçš„ç‰¹å®šæ•°æ®é›†è®©æ¨¡å‹å†·å¯åŠ¨ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ ä¾æ®ä»»åŠ¡ç»“æœåé¦ˆä¼˜åŒ–æ¨¡å‹ï¼Œè¿™ç§åˆ†é˜¶æ®µè®­ç»ƒä¸”ç»“åˆä¸åŒè®­ç»ƒèŒƒå¼ä¼˜åŠ¿çš„æ–¹å¼ï¼Œå¯ä¸ºæ¨¡å‹è®­ç»ƒæµç¨‹è®¾è®¡æä¾›å‚è€ƒã€‚  
3. æ•°æ®é›†æ„å»ºç†å¿µï¼šMM - CoFæ•°æ®é›†ä»è‡ªé€‚åº”è¯†åˆ«å…³é”®åŒºåŸŸä»¥è§£å†³è§†è§‰ä»»åŠ¡çš„è§’åº¦æ„å»ºï¼Œè¿™ç§é’ˆå¯¹ç‰¹å®šèƒ½åŠ›åŸ¹å…»æ„å»ºæ•°æ®é›†çš„æ€è·¯ï¼Œå¯¹åç»­æ‰“é€ é’ˆå¯¹æ€§è®­ç»ƒæ•°æ®æœ‰å¯å‘æ„ä¹‰ã€‚
```

## vlm-r$^3$--region-recognition--reasoning--and-refinement-for-enhanced-multimodal-chain-of-thought
### Abstract
Recently, reasoning-based MLLMs have achieved a degree of success in
generating long-form textual reasoning chains. However, they still struggle
with complex tasks that necessitate dynamic and iterative focusing on and
revisiting of visual regions to achieve precise grounding of textual reasoning
in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual
\textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and
\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)
decide \emph{when} additional visual evidence is needed, (ii) determine
\emph{where} to ground within the image, and (iii) seamlessly weave the
relevant sub-image content back into an interleaved chain-of-thought. The core
of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization
(R-GRPO)}, a training paradigm that rewards the model for selecting informative
regions, formulating appropriate transformations (e.g.\ crop, zoom), and
integrating the resulting visual context into subsequent reasoning steps. To
bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual
Interleaved Rationale (VLIR) corpus that provides step-level supervision on
region selection and textual justification. Extensive experiments on MathVista,
ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art
in zero-shot and few-shot settings, with the largest gains appearing on
questions demanding subtle spatial reasoning or fine-grained visual cue
extraction.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | VLM-RÂ³ï¼šè®©å¤šæ¨¡æ€æ€ç»´é“¾æ›´æ™ºèƒ½çš„åŒºåŸŸè¯†åˆ«ã€æ¨ç†ä¸ä¼˜åŒ–æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåŸºäºæ¨ç†çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”Ÿæˆé•¿ç¯‡æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å–å¾—äº†ä¸€å®šæˆæœã€‚ä½†é¢å¯¹å¤æ‚ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»éš¾ä»¥åŠ¨æ€ä¸”è¿­ä»£åœ°èšç„¦ã€å›é¡¾è§†è§‰åŒºåŸŸï¼Œæ¥å®ç°æ–‡æœ¬æ¨ç†åœ¨è§†è§‰è¯æ®ä¸Šçš„ç²¾å‡†é”šå®šï¼ˆgroundingï¼‰ã€‚æ¯”å¦‚éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†æˆ–æå–ç»†ç²’åº¦è§†è§‰çº¿ç´¢çš„ä»»åŠ¡ï¼Œç°æœ‰æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©MLLMæ›´æ™ºèƒ½åœ°åˆ©ç”¨è§†è§‰åŒºåŸŸä¿¡æ¯è¾…åŠ©æ¨ç†ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºVLM-RÂ³æ¡†æ¶  
VLM-RÂ³ï¼ˆVisual Language Model with Region Recognition, Reasoning, and Refinementï¼‰ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èµ‹äºˆä¸‰é¡¹å…³é”®èƒ½åŠ›ï¼šï¼ˆiï¼‰åˆ¤æ–­â€œä½•æ—¶â€éœ€è¦é¢å¤–è§†è§‰è¯æ®ï¼›ï¼ˆiiï¼‰ç¡®å®šåœ¨å›¾åƒä¸­â€œå“ªé‡Œâ€è¿›è¡Œé”šå®šï¼›ï¼ˆiiiï¼‰å°†ç›¸å…³å­å›¾åƒå†…å®¹æ— ç¼ç¼–ç»‡åˆ°äº¤é”™çš„æ€ç»´é“¾ä¸­ã€‚é€šè¿‡è¿™ä¸‰ç‚¹ï¼Œæ¨¡å‹èƒ½æ›´çµæ´»åœ°åˆ©ç”¨è§†è§‰åŒºåŸŸä¿¡æ¯æ”¯æ’‘æ¨ç†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šRegion-Conditioned Reinforcement Policy Optimizationï¼ˆR-GRPOï¼‰è®­ç»ƒèŒƒå¼  
è¿™æ˜¯æ–¹æ³•çš„æ ¸å¿ƒï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶å¼•å¯¼æ¨¡å‹ï¼šé€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€åˆ¶å®šåˆé€‚çš„å˜æ¢ï¼ˆå¦‚è£å‰ªã€ç¼©æ”¾ï¼‰ã€å¹¶å°†å¾—åˆ°çš„è§†è§‰ä¸Šä¸‹æ–‡æ•´åˆåˆ°åç»­æ¨ç†æ­¥éª¤ä¸­ã€‚è¿™ç§å¼ºåŒ–å­¦ä¹ å¼çš„è®­ç»ƒï¼Œè®©æ¨¡å‹å­¦ä¼šä¸»åŠ¨ä¸”åˆç†åœ°åˆ©ç”¨è§†è§‰åŒºåŸŸè¾…åŠ©æ¨ç†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºVisuo-Lingual Interleaved Rationaleï¼ˆVLIRï¼‰è¯­æ–™åº“  
ä¸ºäº†å¼•å¯¼R-GRPOç­–ç•¥çš„è®­ç»ƒï¼Œå›¢é˜Ÿæ„å»ºäº†VLIRè¯­æ–™åº“ã€‚å®ƒè™½è§„æ¨¡ä¸å¤§ï¼Œä½†ç»è¿‡ç²¾å¿ƒç­–åˆ’ï¼Œèƒ½åœ¨â€œåŒºåŸŸé€‰æ‹©â€å’Œâ€œæ–‡æœ¬è®ºè¯â€å±‚é¢æä¾›æ­¥éª¤çº§ç›‘ç£ï¼Œä¸ºæ¨¡å‹å­¦ä¹ å¦‚ä½•ç»“åˆè§†è§‰åŒºåŸŸæ¨ç†æ‰“ä¸‹æ•°æ®åŸºç¡€ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MathVistaã€ScienceQAç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLM-RÂ³åœ¨é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰å’Œå°‘æ ·æœ¬ï¼ˆfew-shotï¼‰è®¾ç½®ä¸‹éƒ½è¾¾åˆ°äº†å…¨æ–°çš„SOTAï¼ˆstate-of-the-artï¼‰æ°´å¹³ã€‚å°¤å…¶åœ¨éœ€è¦å¾®å¦™ç©ºé—´æ¨ç†æˆ–ç»†ç²’åº¦è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æå‡æœ€ä¸ºæ˜¾è‘—ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è§†è§‰-æ–‡æœ¬æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€æ¨ç†çš„â€œä¸»åŠ¨è§†è§‰åˆ©ç”¨â€æ€è·¯ï¼šVLM-RÂ³å¼ºè°ƒæ¨¡å‹ä¸»åŠ¨åˆ¤æ–­ä½•æ—¶ã€ä½•å¤„è·å–è§†è§‰ä¿¡æ¯ï¼Œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹è®¾è®¡æ¨ç†é€»è¾‘æä¾›äº†â€œä¸»åŠ¨äº¤äº’å¼â€çš„æ–°è§†è§’ã€‚  
2. å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€ç»“åˆï¼šR-GRPOå±•ç¤ºäº†å¦‚ä½•ç”¨å¼ºåŒ–å­¦ä¹ èŒƒå¼å¼•å¯¼æ¨¡å‹å­¦ä¹ è§†è§‰åŒºåŸŸé€‰æ‹©ä¸æ¨ç†æ•´åˆï¼Œä¸ºå¤šæ¨¡æ€ä»»åŠ¡çš„ç­–ç•¥å­¦ä¹ æä¾›äº†å‚è€ƒã€‚  
3. å°è€Œç²¾çš„è¯­æ–™åº“æ„å»ºï¼šVLIRè¯´æ˜åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œç²¾å¿ƒè®¾è®¡çš„å°è§„æ¨¡è¯­æ–™ä¹Ÿèƒ½æœ‰æ•ˆå¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œä¸ºæ•°æ®èµ„æºæœ‰é™æ—¶çš„å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæä¾›äº†æ€è·¯ã€‚  
```

