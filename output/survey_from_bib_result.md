# Paper List from BIB File: tmp8c4fwrj2.bib
- [25/05] **Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability**  
[[Paper](http://arxiv.org/pdf/2505.13258v1)] [[Code/Page]()] [[TLDR/Notes](#effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability)]

- [25/05] **Hybrid Latent Reasoning via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.18454v1)] [[Code/Page]()] [[TLDR/Notes](#hybrid-latent-reasoning-via-reinforcement-learning)]

- [25/05] **EvolveSearch: An Iterative Self-Evolving Search Agent**  
[[Paper](http://arxiv.org/pdf/2505.22501v1)] [[Code/Page]()] [[TLDR/Notes](#evolvesearch--an-iterative-self-evolving-search-agent)]

- [25/04] **Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration**  
[[Paper](http://arxiv.org/pdf/2504.04915v1)] [[Code/Page](https://github.com/ritaranx/Collab-RAG/.)] [[TLDR/Notes](#collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration)]

- [25/03] **Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.09516v3)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning)]

- [25/01] **Search-o1: Agentic Search-Enhanced Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2501.05366v1)] [[Code/Page](https://github.com/sunnynexus/Search-o1}.)] [[TLDR/Notes](#search-o1--agentic-search-enhanced-large-reasoning-models)]

- [25/03] **Agent models: Internalizing Chain-of-Action Generation into Reasoning models**  
[[Paper](http://arxiv.org/pdf/2503.06580v1)] [[Code/Page](https://github.com/ADaM-BJTU/AutoCoA)] [[TLDR/Notes](#agent-models--internalizing-chain-of-action-generation-into-reasoning-models)]

- [25/05] **Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging**  
[[Paper](http://arxiv.org/pdf/2505.09316v1)] [[Code/Page]()] [[TLDR/Notes](#scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging)]

- [25/06] **ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering**  
[[Paper](http://arxiv.org/pdf/2506.00232v1)] [[Code/Page]()] [[TLDR/Notes](#composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering)]

- [24/11] **Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models**  
[[Paper](http://arxiv.org/pdf/2411.19443v1)] [[Code/Page](https://github.com/ictnlp/Auto-RAG}.)] [[TLDR/Notes](#auto-rag--autonomous-retrieval-augmented-generation-for-large-language-models)]

- [25/05] **LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization**  
[[Paper](http://arxiv.org/pdf/2505.17447v1)] [[Code/Page]()] [[TLDR/Notes](#lets--learning-to-think-and-search-via-process-and-outcome-reward-hybridization)]

- [25/03] **R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.05592v2)] [[Code/Page]()] [[TLDR/Notes](#r1-searcher--incentivizing-the-search-capability-in-llms-via-reinforcement-learning)]

- [25/03] **ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.19470v2)] [[Code/Page]()] [[TLDR/Notes](#research--learning-to-reason-with-search-for-llms-via-reinforcement-learning)]

- [25/06] **Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source Search in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.08352v1)] [[Code/Page](https://github.com/wentao0429/Reasoning-search.)] [[TLDR/Notes](#reinforcement-fine-tuning-for-reasoning-towards-multi-step-multi-source-search-in-large-language-models)]

- [25/05] **ZeroSearch: Incentivize the Search Capability of LLMs without Searching**  
[[Paper](http://arxiv.org/pdf/2505.04588v2)] [[Code/Page]()] [[TLDR/Notes](#zerosearch--incentivize-the-search-capability-of-llms-without-searching)]

- [25/06] **Constructing and Evaluating Declarative RAG Pipelines in PyTerrier**  
[[Paper](http://arxiv.org/pdf/2506.10802v1)] [[Code/Page]()] [[TLDR/Notes](#constructing-and-evaluating-declarative-rag-pipelines-in-pyterrier)]

- [25/05] **Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization**  
[[Paper](http://arxiv.org/pdf/2505.15444v1)] [[Code/Page]()] [[TLDR/Notes](#single-llm--multiple-roles--a-unified-retrieval-augmented-generation-framework-using-role-specific-token-optimization)]

- [25/06] **R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.04185v1)] [[Code/Page](https://github.com/QingFei1/R-Search.)] [[TLDR/Notes](#r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning)]

- [23/05] **Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy**  
[[Paper](http://arxiv.org/pdf/2305.15294v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-retrieval-augmented-large-language-models-with-iterative-retrieval-generation-synergy)]

- [25/05] **Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs**  
[[Paper](http://arxiv.org/pdf/2505.11277v3)] [[Code/Page]()] [[TLDR/Notes](#search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms)]

- [24/10] **SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback**  
[[Paper](http://arxiv.org/pdf/2410.18141v2)] [[Code/Page]()] [[TLDR/Notes](#smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback)]

- [25/05] **Neuro-Symbolic Query Compiler**  
[[Paper](http://arxiv.org/pdf/2505.11932v1)] [[Code/Page]()] [[TLDR/Notes](#neuro-symbolic-query-compiler)]

- [25/05] **Interleaved Reasoning for Large Language Models via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.19640v1)] [[Code/Page]()] [[TLDR/Notes](#interleaved-reasoning-for-large-language-models-via-reinforcement-learning)]

- [25/01] **Chain-of-Retrieval Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2501.14342v2)] [[Code/Page]()] [[TLDR/Notes](#chain-of-retrieval-augmented-generation)]

- [25/03] **OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query Processing**  
[[Paper](http://arxiv.org/pdf/2503.02603v2)] [[Code/Page]()] [[TLDR/Notes](#okralong--a-flexible-retrieval-augmented-framework-for-long-text-query-processing)]

- [24/12] **RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation**  
[[Paper](http://arxiv.org/pdf/2412.11919v1)] [[Code/Page](https://github.com/sunnynexus/RetroLLM}.)] [[TLDR/Notes](#retrollm--empowering-large-language-models-to-retrieve-fine-grained-evidence-within-generation)]

- [25/05] **StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.15107v2)] [[Code/Page](https://github.com/Zillwang/StepSearch.)] [[TLDR/Notes](#stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization)]

- [25/04] **ToolRL: Reward is All Tool Learning Needs**  
[[Paper](http://arxiv.org/pdf/2504.13958v1)] [[Code/Page]()] [[TLDR/Notes](#toolrl--reward-is-all-tool-learning-needs)]

- [25/04] **DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments**  
[[Paper](http://arxiv.org/pdf/2504.03160v4)] [[Code/Page](https://github.com/GAIR-NLP/DeepResearcher.)] [[TLDR/Notes](#deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments)]

- [25/05] **An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents**  
[[Paper](http://arxiv.org/pdf/2505.15117v1)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents)]

- [25/05] **R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.17005v1)] [[Code/Page](https://github.com/RUCAIBox/R1-Searcher-plus.)] [[TLDR/Notes](#r1-searcher++--incentivizing-the-dynamic-knowledge-acquisition-of-llms-via-reinforcement-learning)]

- [25/05] **Removal of Hallucination on Hallucination: Debate-Augmented RAG**  
[[Paper](http://arxiv.org/pdf/2505.18581v1)] [[Code/Page](https://github.com/Huenao/Debate-Augmented-RAG.)] [[TLDR/Notes](#removal-of-hallucination-on-hallucination--debate-augmented-rag)]

- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.22019v2)] [[Code/Page](https://github.com/Alibaba-NLP/VRAG.)] [[TLDR/Notes](#vrag-rl--empower-vision-perception-based-rag-for-visually-rich-information-understanding-via-iterative-reasoning-with-reinforcement-learning)]

- [25/01] **AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation using Tree-based Search**  
[[Paper](http://arxiv.org/pdf/2501.10053v2)] [[Code/Page]()] [[TLDR/Notes](#airrag--activating-intrinsic-reasoning-for-retrieval-augmented-generation-using-tree-based-search)]

- [24/11] **AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning**  
[[Paper](http://arxiv.org/pdf/2411.16495v3)] [[Code/Page]()] [[TLDR/Notes](#atomr--atomic-operator-empowered-large-language-models-for-heterogeneous-knowledge-reasoning)]

- [25/05] **SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis**  
[[Paper](http://arxiv.org/pdf/2505.16834v2)] [[Code/Page](https://github.com/RUCAIBox/SimpleDeepSearcher.)] [[TLDR/Notes](#simpledeepsearcher--deep-information-seeking-via-web-powered-reasoning-trajectory-synthesis)]

- [25/05] **ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision**  
[[Paper](http://arxiv.org/pdf/2505.21250v1)] [[Code/Page](https://leeds1219.github.io/ReSCORE.)] [[TLDR/Notes](#rescore--label-free-iterative-retriever-training-for-multi-hop-question-answering-with-relevance-consistency-supervision)]

- [24/07] **Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach**  
[[Paper](http://arxiv.org/pdf/2407.13101v2)] [[Code/Page]()] [[TLDR/Notes](#retrieve--summarize--plan--advancing-multi-hop-question-answering-with-an-iterative-approach)]

- [25/02] **RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2502.13957v2)] [[Code/Page](https://rag-gym.github.io.)] [[TLDR/Notes](#rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation)]

- [24/05] **FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research**  
[[Paper](http://arxiv.org/pdf/2405.13576v2)] [[Code/Page](https://github.com/RUC-NLPIR/FlashRAG.)] [[TLDR/Notes](#flashrag--a-modular-toolkit-for-efficient-retrieval-augmented-generation-research)]



# TLDR/Notes
## effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability
### Abstract
Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.
```
### 🌟 论文解读 | 解决RAG两大痛点！ARENA用强化学习让生成更有效、更透明

### 📌 背景痛点/本文动机
Retrieval - Augmented Generation（RAG）在提升大语言模型（LLMs）处理知识密集型任务表现上效果显著，但仍存在两大未解决的挑战：
1. **有效性不足**：现有研究多聚焦于打造更强的RAG检索器，却忽略了如何提升生成器（LLM）利用检索信息进行推理和生成的能力。实际测试发现，在多跳问答基准测试中，相同检索上下文下7B规模的模型，回答准确率比OpenAI - o1、DeepSeek - R1等推理模型低15 - 35%，这表明生成器的推理能力是当前RAG pipeline的关键瓶颈。
2. **透明度缺失**：多数RAG方法未关注哪些检索内容真正对推理过程有贡献，导致可解释性和可见性不足。很多生成器输出无结构答案，隐藏了决策过程，降低了可信度，在处理多文档多跳推理时，小模型因推理能力有限更难应对噪声或冗余上下文。
同时，现有基于强化学习（RL）的方法，奖励设计通用、输出格式未考虑多跳QA结构，且KL正则化不稳定易导致训练发散，限制了在检索类任务中的应用。基于此，论文提出ARENA框架来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ARENA框架，实现透明且有效的RAG生成
ARENA（Adaptive - Rewarded Evidence Navigation Agent）是一个基于强化学习训练的透明RAG生成器框架。它引入结构化输出格式，包含选定的参考资料、明确的推理轨迹和最终答案，实现端到端的可解释性。通过这种结构化生成，模型能识别关键证据、进行结构化推理并生成带有可解释决策轨迹的答案。
💡 创新点2：设计自适应任务特定奖励与稳定优化策略
为多跳问答任务定制了一套自适应的、特定任务的奖励机制，从格式、准确性、相关性和额外奖励等多个维度评估模型输出，提供可解释且细粒度的训练信号。同时改进KL公式实现KL稳定化，解决了现有RL方法中KL正则化不稳定导致训练发散的问题，让训练更稳定适用于检索类任务。

### 📈 实验结果
将ARENA应用于Qwen2.5 - 7B - Instruct和Llama3.1 - 8B - Instruct等开源模型，在多个RAG基线的大量实验表明：
1. 在所有多跳QA数据集上，模型准确率提升了10 - 30%，性能可与OpenAI - o1、DeepSeek - R1等商业开发的SOTA大语言模型媲美。
2. 进一步分析显示，ARENA在新数据集上无需额外训练就能很好地适配，具有很强的灵活性，在数据集和模型 backbone 上泛化性良好。

### 💬 可借鉴之处
1. **关注生成器优化**：论文指出当前RAG系统中生成器推理能力不足是关键问题，提醒研究者们除了检索器，要重视生成器的优化，为RAG研究提供了新的关注方向。
2. **强化学习在RAG的应用模式**：展示了通过强化学习，结合结构化生成、自适应奖励设计和稳定训练来提升RAG推理的透明性和有效性的模式，为后续利用RL优化RAG生成器提供了参考框架。
3. **开源资源贡献**：公开了模型和代码，方便后续研究者在此基础上对RAG生成器优化进行研究，推动该领域发展。
```

## hybrid-latent-reasoning-via-reinforcement-learning
### Abstract
Recent advances in large language models (LLMs) have introduced latent
reasoning as a promising alternative to autoregressive reasoning. By performing
internal computation with hidden states from previous steps, latent reasoning
benefit from more informative features rather than sampling a discrete
chain-of-thought (CoT) path. Yet latent reasoning approaches are often
incompatible with LLMs, as their continuous paradigm conflicts with the
discrete nature of autoregressive generation. Moreover, these methods rely on
CoT traces for training and thus fail to exploit the inherent reasoning
patterns of LLMs. In this work, we explore latent reasoning by leveraging the
intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we
introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid
latent reasoning approach that (1) integrates prior hidden states into sampled
tokens with a learnable gating mechanism, and (2) initializes training with
predominantly token embeddings while progressively incorporating more hidden
features. This design maintains LLMs' generative capabilities and incentivizes
hybrid reasoning using both discrete and continuous representations. In
addition, the hybrid HRPO introduces stochasticity into latent reasoning via
token sampling, thereby enabling RL-based optimization without requiring CoT
trajectories. Extensive evaluations across diverse benchmarks show that HRPO
outperforms prior methods in both knowledge- and reasoning-intensive tasks.
Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing
behaviors like cross-lingual patterns and shorter completion lengths,
highlighting the potential of our RL-based approach and offer insights for
future work in latent reasoning.
```
### 🌟 论文解读 | 强化学习驱动的混合潜在推理：解锁大模型内在推理能力

### 📌 背景痛点/本文动机
在大语言模型（LLMs）领域，潜在推理作为 autoregressive（自回归）推理的替代方案展现出潜力。传统自回归推理依赖离散的思维链（CoT）解码与采样，而潜在推理借助前序步骤的连续隐藏状态实现内部推理，能利用更丰富信息。但现有潜在推理方法存在诸多问题：一是与LLMs兼容性差，连续范式和自回归生成的离散性冲突，将隐藏状态输入下一步解码会降低生成质量（如重复、不连贯）；二是依赖CoT轨迹训练，既忽视LLMs固有推理能力，又带来高昂训练成本（如多阶段训练、从头训多块模型），限制了适用范围。因此，亟需一种能无缝整合连续表示、依托预训练LLMs泛化性、减少CoT依赖的潜在推理方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出HRPO框架，首推基于强化学习的混合推理方案  
Hybrid Reasoning Policy Optimization（HRPO）是首个基于强化学习的混合潜在推理优化框架，把策略学习和潜在推理统一起来，无需依赖CoT轨迹就能利用LLMs内在推理模式。通过强化学习让大模型自主发展潜在推理能力，为潜在推理提供了高效且可扩展的新路径。

💡 创新点2：设计门控机制，平衡生成能力与连续推理  
为在保留LLMs生成能力的同时，引导模型在连续空间推理，HRPO引入**可学习门控机制**。训练初期，输入主要来自采样token的嵌入，保证生成质量；随训练推进，门控学习融入前序隐藏状态中更丰富的信息，助力内部推理。这种渐进式融合，让离散token和连续隐藏表示协同作用，实现混合推理。

💡 创新点3：借助采样随机性，实现无CoT的RL优化  
HRPO通过token采样给潜在推理引入随机性，使得rollout（轨迹展开）能像标准RL方法一样执行。混合输出（token + 潜在表示）存入rollout buffer用于策略更新，基于简单的“结果导向型奖励”计算对数概率，实现策略梯度更新，自适应整合token级和潜在表示信息，解锁现有LLMs的潜在推理能力。

### 📈 实验结果
在多个知识密集型和推理密集型任务基准测试中，HRPO表现优于现有方法（包括其他潜在推理基线），在不同场景下持续提升性能。此外，经HRPO训练的LLMs保留了可解释性，还展现出跨语言模式、更短生成长度等有趣行为，验证了基于RL的混合潜在推理方案的潜力，也为后续潜在推理研究提供了新视角。

### 💬 可借鉴之处
1. 方法创新角度：将强化学习与潜在推理结合，跳出“依赖CoT标注和多阶段训练”的传统思路，为大模型推理能力提升开辟新范式，证明了RL在解锁LLMs内在推理模式上的价值。  
2. 工程设计角度：门控机制的“渐进式融合”思路，为平衡模型既有能力（如生成）与新能力（如连续空间推理）提供了可参考的技术路线，在需兼顾“存量能力”和“增量能力”的模型优化场景中具借鉴性。  
3. 实验与分析角度：不仅验证性能，还关注模型行为（可解释性、跨语言等），这种从“效果”到“特性”的全面分析，能帮助研究者更深入理解方法对模型的塑造，为后续优化指明方向。  
```

## evolvesearch--an-iterative-self-evolving-search-agent
### Abstract
The rapid advancement of large language models (LLMs) has transformed the
landscape of agentic information seeking capabilities through the integration
of tools such as search engines and web browsers. However, current mainstream
approaches for enabling LLM web search proficiency face significant challenges:
supervised fine-tuning struggles with data production in open-search domains,
while RL converges quickly, limiting their data utilization efficiency. To
address these issues, we propose EvolveSearch, a novel iterative self-evolution
framework that combines SFT and RL to enhance agentic web search capabilities
without any external human-annotated reasoning data. Extensive experiments on
seven multi-hop question-answering (MHQA) benchmarks demonstrate that
EvolveSearch consistently improves performance across iterations, ultimately
achieving an average improvement of 4.7\% over the current state-of-the-art
across seven benchmarks, opening the door to self-evolution agentic
capabilities in open web search domains.
```
### 🌟 论文解读 | EvolveSearch：开启智能体网络搜索自进化新篇章

### 📌 背景痛点/本文动机
大语言模型（LLMs）的飞速发展，借助搜索引擎、网页浏览器等工具革新了智能体信息获取能力，但当前主流让LLM具备网络搜索能力的方法存在明显挑战：有监督微调（SFT）在开放搜索领域面临数据生成难题，强化学习（RL）收敛过快导致数据利用效率受限。为解决这些问题，论文提出EvolveSearch框架，旨在无需外部人工标注推理数据的情况下增强智能体网络搜索能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：首创迭代式自进化框架
EvolveSearch是首个将强化学习与有监督微调迭代结合，用于提升大语言模型网络搜索场景能力的框架。它通过交替进行RL探索阶段与SFT优化阶段，让模型从自身经验中学习鲁棒有效的搜索行为，无需人工干预。
- RL探索阶段：模型与网络搜索环境交互，利用工具能力并接收混合奖励信号，从高奖励的rollouts（执行轨迹）中学习。
- SFT优化阶段：依据三个标准筛选RL阶段中表现最佳的rollouts，用于SFT优化模型，为下一个RL循环提供更强的初始化（冷启动策略）。

💡 创新点2：无人工标注推理数据依赖
EvolveSearch不依赖外部人工标注的推理数据，而是利用强化学习模型产生的高质量rollouts，通过自我生成的监督信号实现持续自我改进。在迭代过程中，将RL得到的优质rollouts融入数据池，经筛选后用于后续SFT，让模型不断进化。

### 📈 实验结果
论文在七个多跳问答（MHQA）基准测试上开展大量实验。结果显示，EvolveSearch在迭代过程中性能持续提升，最终在七个基准测试上较当前最先进方法平均提升4.7%，有力证明了在自进化框架中结合有监督微调与强化学习的优势，也展现了从高奖励rollouts中迭代学习、不依赖人工标注数据就能实现性能大幅提升的能力。

### 💬 可借鉴之处
- 框架设计思路：其迭代结合RL与SFT的思路为解决开放领域任务中数据稀缺和模型性能提升难题提供了新范式，可启发后续在智能体能力增强、工具使用优化等方向的研究。
- 数据利用方式：不依赖人工标注数据，转而从模型自身交互经验中挖掘优质数据用于迭代优化，这种自监督式的改进路径为数据获取困难场景下的模型训练提供了参考。
- 多任务验证模式：在多个MHQA数据集上验证有效性与通用性，这种全面的实验验证方式值得科研工作中借鉴，以充分说明方法的普适价值。
```

## collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration
### Abstract
Retrieval-Augmented Generation (RAG) systems often struggle to handle
multi-hop question-answering tasks accurately due to irrelevant context
retrieval and limited complex reasoning capabilities. We introduce Collab-RAG,
a collaborative training framework that leverages mutual enhancement between a
white-box small language model (SLM) and a blackbox large language model (LLM)
for RAG. Specifically, the SLM decomposes complex queries into simpler
sub-questions, thus enhancing the accuracy of the retrieval and facilitating
more effective reasoning by the black-box LLM. Concurrently, the black-box LLM
provides feedback signals to improve the SLM's decomposition capability. We
observe that Collab-RAG relies solely on supervision from an affordable
black-box LLM without additional distillation from frontier LLMs, yet
demonstrates strong generalization across multiple black-box LLMs. Experimental
evaluations across five multi-hop QA datasets demonstrate that Collab-RAG
substantially outperforms existing black-box-only and SLM fine-tuning baselines
by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a
frozen 32B LLM in question decomposition, highlighting the efficiency of
Collab-RAG in improving reasoning and retrieval for complex questions. The code
of Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.
```
### 🌟 论文解读 | Collab-RAG：白盒小模型与黑盒大模型协作，突破复杂问答场景下的RAG瓶颈

### 📌 背景痛点/本文动机
大语言模型（LLM）虽在众多语言任务中表现出色，但存在幻觉、难适配领域知识等问题。检索增强生成（RAG）技术通过整合外部知识缓解这些问题，然而在复杂多跳问答任务中，RAG常因检索到无关上下文、复杂推理能力受限而表现不佳。现有提升检索质量的方法多聚焦单步检索优化，难应对需迭代证据收集的复杂问答；无训练的LLM查询分解能力有限；小模型微调方法对黑盒LLM参数更新又低效昂贵。因此，充分释放黑盒LLM在复杂问答的能力仍具挑战，这催生了Collab - RAG的研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Collab - RAG框架，实现白盒小语言模型（SLM）与黑盒大语言模型（LLM）动态协作  
SLM作为分解器，将复杂查询拆解为更简单的子问题，提升检索相关上下文的准确性，也为黑盒LLM更有效推理铺路；黑盒LLM作为阅读器，为每个子问题生成中间答案并合成最终响应，借助子问题检索来逐步解答复杂问题。

💡 创新点2：基于黑盒LLM反馈的自改进训练策略  
直接用SLM做问题分解因推理能力有限且高质量标注成本高而效果不佳。Collab - RAG仅依赖黑盒LLM（如GPT - 4o - mini）的反馈来优化，将黑盒LLM建模为能生成响应的环境，SLM与之多轮交互迭代优化分解策略。设计迭代偏好优化方法，依据黑盒LLM反馈（通过基于规则的评估方法，从子问题格式和最终答案准确性判断分解是否有效）提升SLM分解能力，无需昂贵人工标注或前沿LLM蒸馏。

### 📈 实验结果
在五个多跳问答数据集上评估，Collab - RAG平均比现有仅黑盒LLM和SLM微调基线方法性能高出1.8% - 14.2%。在问题分解任务上，微调后的3B参数SLM表现超过冻结的32B参数LLM，有力证明了Collab - RAG在提升复杂问题推理和检索方面的高效性，且仅用GPT - 4o - mini监督训练却能在多个黑盒LLM上有强泛化性。

### 💬 可借鉴之处
1. 模型协作思路：在RAG等需多组件配合的任务中，探索不同“能力互补”模型（如白盒小模型与黑盒大模型）的协作模式，发挥各自优势（小模型可训练优化、大模型强推理等）。
2. 训练优化方式：利用现有黑盒大模型反馈来优化小模型，避免高成本标注与前沿大模型蒸馏，为资源有限情况下提升模型特定能力（如查询分解）提供思路。
3. 复杂任务处理：针对复杂多跳问答这类需分步推理、多证据整合的任务，通过“分解 - 子任务处理 - 合成”的 pipeline 设计，为突破任务难点提供了可参考的架构范式。
```

## search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning
### Abstract
Efficiently acquiring external knowledge and up-to-date information is
essential for effective reasoning and text generation in large language models
(LLMs). Prompting advanced LLMs with reasoning capabilities to use search
engines during inference is often suboptimal, as the LLM might not fully
possess the capability on how to interact optimally with the search engine.
This paper introduces Search-R1, an extension of reinforcement learning (RL)
for reasoning frameworks where the LLM learns to autonomously generate
(multiple) search queries during step-by-step reasoning with real-time
retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn
search interactions, leveraging retrieved token masking for stable RL training
and a simple outcome-based reward function. Experiments on seven
question-answering datasets show that Search-R1 improves performance by 41%
(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same
setting. This paper further provides empirical insights into RL optimization
methods, LLM choices, and response length dynamics in retrieval-augmented
reasoning. The code and model checkpoints are available at
https://github.com/PeterGriffinJin/Search-R1.
```
### 🌟 论文解读 | Search-R1：用强化学习让大模型学会推理与搜索引擎协作

### 📌 背景痛点/本文动机
大语言模型（LLMs）在自然语言理解与生成方面表现卓越，但面对复杂推理任务和获取外部实时信息时仍存在不足。现有将LLMs与搜索引擎结合的方式（如检索增强生成RAG、把搜索引擎当工具）存在局限：RAG虽能利用外部知识，但LLMs在训练中未被优化以高效与搜索引擎交互；工具类方法里，基于提示的方式泛化性差，基于训练的方式又因依赖大规模高质量标注轨迹和搜索操作不可微分，难以有效扩展。同时，把强化学习（RL）应用于“搜索 + 推理”场景也面临框架稳定性、多轮交错推理与搜索、奖励设计三大挑战。因此，如何让LLMs在推理时自主且高效地利用搜索引擎，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建含搜索引擎的RL环境与稳定优化机制  
Search - R1将搜索引擎建模为环境的一部分，让轨迹序列能交错生成LLM tokens和执行搜索引擎检索。它兼容PPO、GRPO等多种RL算法，还通过“检索token掩码”技术保障RL训练的稳定性，让模型在整合检索到的上下文时也能稳定优化。  

💡 创新点2：支持多轮检索与推理的交互流程  
模型可在<search>和</search>标记触发下调用搜索，检索内容被包裹在<information>和</information>间，LLM推理步骤用<think>和<think>封装，最终答案也以特定格式输出，以此实现结构化、迭代式的决策过程，让大模型能依据问题复杂度动态调整检索策略，完成多轮推理与搜索的交错协作。  

💡 创新点3：简洁的基于结果的奖励函数设计  
摒弃复杂的过程型奖励，采用简单的结果导向奖励函数。实验证明这种极简设计在“搜索 + 推理”场景中能有效引导模型学习，助力Search - R1成为DeepSeek - R1 Zero的扩展，为检索驱动决策引入搜索增强的RL训练。  

### 📈 实验结果
在七个问答数据集上开展实验，同一设置下（相同检索模型、训练数据、预训练LLM），Search - R1让Qwen2.5 - 7B和Qwen2.5 - 3B分别比各类RAG基线模型性能提升41%和20%。同时，论文还在RL优化方法、LLM选择、检索增强推理中的响应长度动态等方面给出了实证性见解。  

### 💬 可借鉴之处
1. 技术思路层面：为解决大模型“搜索 + 推理”难题提供了RL框架新思路，把搜索引擎纳入环境、设计多轮交互流程等做法，为后续优化大模型外部知识利用方式提供了参考范式。  
2. 实验与分析层面：系统的实验不仅验证了方法有效性，还对RL方法选择、不同LLM适配、响应长度等维度展开研究，这些实证洞察能辅助研究者在类似“检索 + 推理”任务中做更优决策。  
3. 工程落地层面：代码和模型 checkpoint 开源（https://github.com/PeterGriffinJin/Search - R1），便于社区基于该工作进一步探索大模型与搜索引擎协作的更多可能。  
```

## search-o1--agentic-search-enhanced-large-reasoning-models
### Abstract
Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive
long stepwise reasoning capabilities through large-scale reinforcement
learning. However, their extended reasoning processes often suffer from
knowledge insufficiency, leading to frequent uncertainties and potential
errors. To address this limitation, we introduce \textbf{Search-o1}, a
framework that enhances LRMs with an agentic retrieval-augmented generation
(RAG) mechanism and a Reason-in-Documents module for refining retrieved
documents. Search-o1 integrates an agentic search workflow into the reasoning
process, enabling dynamic retrieval of external knowledge when LRMs encounter
uncertain knowledge points. Additionally, due to the verbose nature of
retrieved documents, we design a separate Reason-in-Documents module to deeply
analyze the retrieved information before injecting it into the reasoning chain,
minimizing noise and preserving coherent reasoning flow. Extensive experiments
on complex reasoning tasks in science, mathematics, and coding, as well as six
open-domain QA benchmarks, demonstrate the strong performance of Search-o1.
This approach enhances the trustworthiness and applicability of LRMs in complex
reasoning tasks, paving the way for more reliable and versatile intelligent
systems. The code is available at
\url{https://github.com/sunnynexus/Search-o1}.
```
### 🌟 论文解读 | Search-o1：让大推理模型拥有智能搜索增强能力

### 📌 背景痛点/本文动机
近年来，以OpenAI - o1、Qwen - QwQ、DeepSeek - R1为代表的大推理模型（LRMs）通过大规模强化学习展现出强大的长步骤推理能力，能在数学、编码等复杂任务中模拟人类解决问题的思路。但这类模型在延长推理链时，常因**知识不足**引发不确定性与错误。比如在复杂推理场景中，模型会频繁出现“perhaps”“alternatively”这类体现不确定的词汇（如图1左所示）；传统面向问题的检索增强生成（RAG）技术也难以有效填补知识缺口（如图1右对比实验）。而且人工验证推理过程成本高，自动补充o1类推理所需知识成了难题，限制了LRMs实现更可靠推理的发展。于是，论文团队希望通过让模型自主检索来增强LRMs的o1式推理能力，提出Search - o1框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：首个集成智能搜索工作流的大推理模型增强框架
Search - o1是首个把**智能检索增强生成（agentic RAG）机制**融入LRMs的o1式推理过程的框架，实现知识自主补充。不同于传统RAG只在问题层面单次检索，Search - o1让模型在遇到知识短缺时，主动生成搜索查询来触发检索，且单轮推理中能多次触发迭代检索，满足各推理步骤的知识需求。

💡 创新点2：文档内推理（Reason - in - Documents）模块解决知识整合难题
考虑到检索文档存在冗余信息、LRMs长文档理解能力有限这两个整合知识的难点，设计独立于主推理链的Reason - in - Documents模块。它会结合当前搜索查询和之前推理步骤，深度分析检索到的文档，提炼出能无缝融入推理链的信息，减少噪声还能保持推理连贯性。

### 📈 实验结果
论文在科学、数学、编码等5个复杂推理领域，以及6个开放域问答基准测试中验证Search - o1。结果显示，Search - o1在推理领域表现卓越，同时在通用知识方面也有大幅提升；进一步定量分析也证实了它的效率与可扩展性，为LRMs实现可信推理提供了实用指导。

### 💬 可借鉴之处
1. 解决知识不足新思路：当模型类方法在知识覆盖上面临瓶颈时，可参考Search - o1引入智能检索机制，让模型动态获取外部知识，为长推理链补全信息。
2. 模块解耦与协作：把知识检索、知识精炼和主推理过程做合理解耦（如单独的Reason - in - Documents模块），再让各部分协作，能有效处理外部知识整合时的冗余、理解受限等问题，这种模块化设计思路可迁移到其他需外部知识注入的AI系统。
3. 多领域验证范式：论文在复杂推理多领域和开放域QA基准测试的实验验证方式，为评估模型在“推理 + 知识补充”类改进的有效性提供了全面验证的范例，后续类似增强型模型研究可借鉴这种多场景测试思路。
```

## agent-models--internalizing-chain-of-action-generation-into-reasoning-models
### Abstract
Traditional agentic workflows rely on external prompts to manage interactions
with tools and the environment, which limits the autonomy of reasoning models.
We position \emph{Large Agent Models (LAMs)} that internalize the generation of
\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when
and how to use external tools. Our proposed AutoCoA framework combines
supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the
model to seamlessly switch between reasoning and action while efficiently
managing environment interactions. Main components include step-level action
triggering, trajectory-level CoA optimization, and an internal world model to
reduce real-environment interaction costs. Evaluations on open-domain QA tasks
demonstrate that AutoCoA-trained agent models significantly outperform
ReAct-based workflows in task completion, especially in tasks that require
long-term reasoning and multi-step actions. Code and dataset are available at
https://github.com/ADaM-BJTU/AutoCoA
```
### 🌟 论文解读 | 从推理到行动：Large Agent Models 如何重塑智能体范式

### 📌 背景痛点/本文动机
在人工智能迈向通用智能的进程中，OpenAI 勾勒出从 Chatbot（如 GPT - 3.5）、Reasoner（如 o1）到 Agent（如 Operator、Deep Research）的演进路径。传统智能体工作流（如 ReAct）依赖外部提示来管理工具与环境交互，限制了推理模型的自主性——思维与行动的切换常由预设流程触发，属于“被动”行为。而新一代智能体需具备主动决策何时、如何使用工具的能力，实现思维（Chain - of - Thought, CoT）与行动（Chain - of - Action, CoA）的深度融合。在此背景下，论文提出**Large Agent Models (LAMs)** 这一概念，旨在让推理模型内化为能自主生成 Chain - of - Action 的智能体模型，突破传统工作流的局限。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：定义 Large Agent Models（LAMs）范式  
LAMs 是在推理模型基础上，通过面向任务的工具增强训练得到的生成式模型。它能生成交错的“思维链（CoT）”与“行动链（CoA）”序列：CoT 支撑内部推理规划，CoA 则通过调用工具与外部环境交互。与依赖外部提示的传统智能体工作流不同，LAMs 把工具使用能力（即 CoA 生成）内化为模型固有行为，实现“主动”决定何时、如何行动，构建“人 - 模型 - 环境”三元交互结构。  

💡 创新点2：提出 AutoCoA 训练框架  
AutoCoA 结合**有监督微调（SFT）**与**强化学习（RL）**，让模型在推理与行动间无缝切换，同时高效管理环境交互。其核心组件包括：  
- 步骤级行动触发：模型能自主决定何时从“思考（⟨think⟩）”切换到“行动（⟨action⟩）”；  
- 轨迹级 CoA 优化：从整个思维 - 行动轨迹层面优化行动链，确保多步行动的连贯性与任务导向性；  
- 内部世界模型：减少与真实环境直接交互的成本，通过内部模拟或预学习来预估行动反馈，提升效率。  

💡 创新点3：形式化 Agent Model 推理过程  
将 Agent Model 的推理建模为**部分可观测马尔可夫决策过程（POMDP）**，状态由初始环境状态 \( s_0 \)、任务上下文 \( t_c \) 和已生成序列 \( x_{1:n} \) 组成。模型的策略 \( \pi_\theta \) 决定每一步生成的 token 类型（是触发思考的 ⟨think⟩ 还是触发行动的 ⟨action⟩），行动时还需指定工具类型（如搜索）与参数（如搜索查询），环境反馈再引导后续推理与行动，直至任务完成。  


### 📈 实验结果
论文在**开放域问答（QA）任务**上评估 AutoCoA 训练的智能体模型：  
- 对比基于 ReAct 的传统工作流，LAMs 在任务完成度上显著更优；  
- 尤其在需要长期推理、多步行动的复杂任务中，内化 CoA 生成的模型展现出更强的连贯性与执行能力，验证了让模型主动管理“思维 - 行动”循环的价值。  


### 💬 可借鉴之处
1. 范式升级视角：从“外部提示驱动”到“模型内部能力内化”是智能体发展的关键趋势，类似推理模型从 CoT Prompting 到内置 CoT 生成的演进，为构建更自主的智能体提供了方向；  
2. 训练框架复用：AutoCoA 结合 SFT + RL 的思路可迁移到多工具、多任务场景，为后续开发更复杂的智能体系统提供技术参考；  
3. 环境交互优化：内部世界模型的设计思路，对降低真实环境交互成本（如减少 API 调用、物理世界试错）具有启发，可用于资源受限或高成本交互场景的智能体开发。  

总之，这篇论文为智能体从“被动响应流程”迈向“主动规划行动”提供了理论与技术蓝图，是理解 AGI 路径上“Agent 阶段”技术突破的重要参考～ 
```

## scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging
### Abstract
Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.
```
### 🌟 论文解读 | 用“信息觅食”优化搜索增强推理：InForage框架如何让LLM更智能？

### 📌 背景痛点/本文动机
大语言模型（LLM）的“知识截断”问题一直是行业痛点——模型无法获取训练后更新的知识，也难以应对模糊、多步骤或动态变化的复杂任务。传统的“检索增强生成”方法采用**静态的预推理检索策略**，把检索到的信息一股脑塞进prompt就完事，面对需要迭代推理、逐步挖掘证据的复杂场景（比如“我要去NeurIPS参会，需要签证吗？”这类要先查会议地点、再查签证政策的问题），就显得力不从心。  

好在“测试时扩展技术”的发展让LLM能动态调用外部工具了，这推动大家把思路从“静态预推理检索”转向“自适应的推理时检索”。但动态检索也有两大挑战：一是复杂任务的信息需求是隐含且动态变化的，单次检索只能拿到局部信息“补丁”，得靠迭代积累；二是信息补丁的价值不取决于自身，而要看它对最终推理的贡献。  

人类面对这类信息搜索任务时，却能高效用几次迭代搜索解决问题——这背后是“信息觅食理论（IFT）”：人类会权衡信息补丁的价值和获取成本，用“信息气味（scent）”指引搜索方向。受此启发，论文提出InForage框架，想让LLM也学会这种“动态信息觅食”的能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于信息觅食理论的强化学习框架  
InForage把“检索增强推理”形式化为**动态信息搜索过程**，用强化学习（RL）来优化。和传统只看最终答案是否正确的方法不同，它关注**中间检索步骤的质量**：参考IFT里“信息气味”的概念，让模型在推理过程中，通过自适应搜索行为迭代收集、整合信息。  

💡 创新点2：三重奖励机制引导推理行为  
为了激励模型学习“高效信息觅食”，InForage设计了三类奖励：  
- 结果奖励（Outcome Reward）：奖励最终答案正确的推理轨迹；  
- 信息增益奖励（Information Gain Reward）：奖励那些能挖掘出有价值证据的中间检索步骤；  
- 效率惩罚（Efficiency Penalty）：避免无意义的冗长推理，鼓励“高效信息觅食”。  

💡 创新点3：构建人类引导的细粒度数据集  
现有QA数据集大多只有“问题-答案”对，缺少中间推理/检索步骤的记录，也很难支撑复杂多步推理的训练。为此，论文团队构建了一个**人类引导的数据集**：标注者从“种子问题”出发，模拟真实网页浏览时的迭代搜索、选文档、提炼子问题的过程，还要求每个案例至少包含4次信息跳跃或交叉条件，保证任务复杂度。数据集记录了搜索和推理的每一步，能为“最终答案正确性、中间检索质量、整体推理效率”提供监督信号。  


### 📈 实验结果
论文在三类任务上做了评估：通用问答、多跳推理任务，以及团队新开发的**实时网络QA数据集**。结果显示，InForage在所有任务中都**持续超越基线方法**，证明了从“丰富监督的搜索增强推理数据”中学习的有效性，也验证了InForage构建“鲁棒、自适应、高效推理智能体”的潜力。  


### 💬 可借鉴之处
1. **理论跨界融合**：把认知科学里的“信息觅食理论”引入LLM的检索增强推理，为动态检索提供了新颖的理论视角；  
2. **强化学习细粒度监督**：跳出“只看最终结果”的思维，设计对中间步骤的奖励机制，更贴合复杂任务里“迭代积累证据”的逻辑；  
3. **数据集构建思路**：针对“复杂多步推理缺数据”的痛点，人工模拟真实信息搜索轨迹，记录细粒度步骤——这种构建“带中间过程的复杂任务数据集”的思路，对后续研究很有参考价值；  
4. **动态检索的落地潜力**：InForage展示了LLM在“模糊、多步骤、信息需求动态变化”场景下的推理能力提升，为检索增强从“静态”转向“动态自适应”提供了可行方案。  
```

## composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering
### Abstract
Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet
many suffer from monolithic designs that tightly couple core functions like
query reformulation, retrieval, reasoning, and verification. This limits their
interpretability, systematic evaluation, and targeted improvement, especially
for complex multi-hop question answering. We introduce ComposeRAG, a novel
modular abstraction that decomposes RAG pipelines into atomic, composable
modules. Each module, such as Question Decomposition, Query Rewriting,
Retrieval Decision, and Answer Verification, acts as a parameterized
transformation on structured inputs/outputs, allowing independent
implementation, upgrade, and analysis. To enhance robustness against errors in
multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that
iteratively revisits and refines earlier steps upon verification failure.
Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently
outperforms strong baselines in both accuracy and grounding fidelity.
Specifically, it achieves up to a 15% accuracy improvement over
fine-tuning-based methods and up to a 5% gain over reasoning-specialized
pipelines under identical retrieval conditions. Crucially, ComposeRAG
significantly enhances grounding: its verification-first design reduces
ungrounded answers by over 10% in low-quality retrieval settings, and by
approximately 3% even with strong corpora. Comprehensive ablation studies
validate the modular architecture, demonstrating distinct and additive
contributions from each component. These findings underscore ComposeRAG's
capacity to deliver flexible, transparent, scalable, and high-performing
multi-hop reasoning with improved grounding and interpretability.
```
### 🌟 论文解读 | ComposeRAG：模块化可组合的多跳问答RAG框架

### 📌 背景痛点/本文动机
大语言模型（LLMs）在各类NLP任务中表现出色，但依赖静态预训练知识易产生错误（幻觉）且难获取最新或特定领域信息。检索增强生成（RAG）虽能整合外部知识缓解这些问题，然而传统RAG在复杂多跳问答时面临挑战，多跳问答需多文档间分解与逐步推理，现有方法常因整体式或不透明架构，限制了可解释性、适应性与系统改进空间，如早期方案易出错传播，后续系统又有依赖微调或验证机制不足等问题。因此，需要一种更灵活、透明且能应对多跳问答的RAG架构。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：模块化架构设计
将RAG pipeline分解为如问题分解、查询重写、检索决策、答案验证等原子化、可组合模块。每个模块对结构化输入/输出做参数化转换，支持独立实现、升级与分析，为多跳问答提供灵活且可解释的推理组件，实现“即插即用”与透明化分析改进。

💡 创新点2：带自反思的编排策略
引入自反思机制增强多步推理鲁棒性，当验证失败时迭代回顾和优化早期步骤。通过自反思的问题分解等推理步骤协调，从早期错误中恢复，提升整体鲁棒性，且反思步骤增加能带来性能提升。

💡 创新点3：模块价值验证与可升级性
一方面量化核心模块（如问题分解、段落重排、答案验证）各自独特且可叠加的性能贡献；另一方面验证独立模块可升级性，单独增强单个组件（如特定任务用更强大LLM）能带来可衡量改进，体现系统扩展性与适应性，还有像检索决策这类注重效率组件对性能和资源利用有积极影响。

### 📈 实验结果
在HotpotQA、2WikiMultiHopQA、MuSiQue、Bamboogle四个多跳QA基准测试中，ComposeRAG在准确率和 grounding 保真度上持续超越强基线。相比基于微调方法准确率最多提升15%，相同检索条件下比推理专用 pipeline 最多高5%；验证优先设计在低质量检索场景减少超10%无根据答案，高质量语料下也约降3%；消融实验证实模块化架构价值，各组件贡献独特且可叠加。

### 💬 可借鉴之处
在架构设计上，模块化思路为复杂系统拆解提供范例，便于分析、升级与适配新任务领域，可应用于需多组件协作的AI系统；自反思机制为处理多步推理中错误传播问题提供新思路，在需迭代优化的任务流程里值得参考；对模块价值量化与可升级性验证，引导后续工作关注组件级优化与系统扩展性，为构建更灵活高效AI系统提供了从设计到验证的完整思路参考。
```

## auto-rag--autonomous-retrieval-augmented-generation-for-large-language-models
### Abstract
Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.
```
### 🌟 论文解读 | Auto-RAG：让大模型自主迭代检索，释放推理决策潜能

### 📌 背景痛点/本文动机
在大语言模型（LLM）的检索增强生成（RAG）领域，尽管RAG能提升输出质量、减少幻觉，但仍存在检索内容噪声大、单次检索难满足复杂查询需求等问题。为解决这些，迭代检索应运而生，它能动态更新检索结果以匹配生成过程中的信息需求。然而现有迭代检索方法多依赖少样本提示或人工规则，既增加推理开销，又忽视了LLM强大的推理决策能力，未能充分利用LLM来决定“何时检索、检索什么”。因此，本文提出Auto - RAG，旨在依托LLM的决策能力实现自主迭代检索。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：以LLM决策能力为核心的自主迭代检索架构
Auto - RAG将LLM与检索器的交互建模为多轮对话形式的迭代检索过程。在迭代中，LLM基于当前状态进行推理，判断是否需要继续检索以及要获取何种信息，持续与检索器对话、规划检索和优化查询，直到收集到足够信息来生成最终答案，充分发挥LLM推理决策能力来驱动迭代检索。
💡 创新点2：自主合成推理型决策指令的方法
为让LLM在迭代检索中具备自主决策能力，本文研发了在迭代检索中自主合成基于推理的决策指令的方法，并对如Llama - 3 - 8B - Instruct等最新开源LLM进行微调，使LLM能在迭代检索里自主决定检索时机与内容，摆脱对人工规则和大量少样本提示的依赖。

### 📈 实验结果
在涵盖开放域问答和多跳问答的六个代表性基准测试中，Auto - RAG即便在训练数据有限的情况下，仍展现出卓越性能。此外，分析表明Auto - RAG能依据问题复杂度和检索知识的相关性动态调整迭代次数，无需人工干预；还能用自然语言表达迭代检索过程，提升可解释性与用户直观体验。

### 💬 可借鉴之处
1. 架构设计层面：将迭代检索建模为多轮对话交互，为利用LLM推理能力优化RAG系统提供了新的架构思路，后续研究可借鉴这种交互模式来增强系统对复杂任务的处理能力。
2. 方法创新层面：自主合成推理型决策指令并微调LLM的方法，为释放LLM在迭代任务中的自主决策潜能提供了可行路径，其他需让模型自主决策的迭代类任务可参考该方法来设计指令与微调策略。
3. 效果提升与体验优化层面：Auto - RAG依据任务动态调整迭代次数以及用自然语言解释过程的特性，为提升AI系统的效率与用户体验提供了借鉴，在构建智能系统时可考虑让系统具备自适应调整和过程可解释性的能力。
```

## lets--learning-to-think-and-search-via-process-and-outcome-reward-hybridization
### Abstract
Large language models (LLMs) have demonstrated impressive capabilities in
reasoning with the emergence of reasoning models like OpenAI-o1 and
DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into
the realm of retrieval-augmented generation (RAG) via outcome-supervised
reinforcement learning (RL) approaches, while the correctness of intermediate
think-and-search steps is usually neglected. To address this issue, we design a
process-level reward module to mitigate the unawareness of intermediate
reasoning steps in outcome-level supervision without additional annotation.
Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel
framework that hybridizes stepwise process reward and outcome-based reward to
current RL methods for RAG. Extensive experiments demonstrate the
generalization and inference efficiency of LeTS across various RAG benchmarks.
In addition, these results reveal the potential of process- and outcome-level
reward hybridization in boosting LLMs' reasoning ability via RL under other
scenarios. The code will be released soon.
```
### 🌟 论文解读 | LeTS：通过过程与结果奖励混合，让大模型学会“思考-搜索”

### 📌 背景痛点/本文动机
大语言模型（LLMs）在推理任务中展现出强大能力，但仅依赖模型内部参数知识存在生成幻觉、信息过时等问题。检索增强生成（RAG）成为缓解这些问题的有效范式，然而传统RAG在复杂查询下的多跳推理能力不足。后续基于结果监督的强化学习（RL）方法虽尝试优化RAG的“思考-搜索”过程，却忽略了中间步骤的正确性，导致**冗余搜索**（重复获取相似/无用信息）与**无关搜索**（中间步骤引入无关内容误导模型）等问题，限制了性能提升。因此，如何在不额外标注的情况下，对中间推理步骤进行有效监督，成为关键挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：设计过程级奖励模块  
为弥补结果级监督对中间步骤的“忽视”，论文提出两个基于规则的过程级奖励模块：  
- **知识冗余奖励**：惩罚那些重复检索已被更优轨迹（rollouts）覆盖信息的步骤，减少冗余搜索；  
- **知识匹配奖励**：通过对比高表现轨迹，识别并奖励弱轨迹中正确的动作，引导模型聚焦有效搜索。  

💡 创新点2：提出LeTS框架，融合过程与结果奖励  
基于过程级奖励模块，论文提出“Learning to Think-and-Search（LeTS）”框架。它通过**优势重缩放（advantage rescaling）**机制，将“逐步过程奖励”与“结果导向奖励”混合到现有RAG强化学习方法中。该框架能更精细地对轨迹分类（区分“表现优”和“表现差”轨迹），并在策略更新时实现更精准的“功劳分配”，从而优化大模型的“思考-搜索”行为。  


### 📈 实验结果
论文在多个RAG基准测试中验证LeTS的效果：  
- **性能提升**：在各类RAG任务上实现平均2.61%的性能增益；  
- **效率优化**：生成token数和搜索次数分别减少11.15%、30.85%，推理效率显著提升；  
- **泛化性与鲁棒性**：在不同场景下（基础模型与指令微调模型）均展现强泛化能力，同时揭示了“过程+结果”奖励混合在其他场景下提升LLMs推理能力的潜力。  


### 💬 可借鉴之处
1. **中间步骤监督思路**：传统RL在RAG中只看“结果”，LeTS则关注“过程”，为解决多步骤任务中中间环节失控问题提供了新视角——通过轻量规则设计过程奖励，无需额外标注即可约束中间行为；  
2. **奖励混合范式**：将细粒度过程奖励与粗粒度结果奖励结合，证明了多维度奖励在强化学习中对复杂任务（如多跳推理+检索）的优化价值，可启发其他需要分步决策的LLM应用；  
3. **效率与性能平衡**：实验中同时实现性能与推理效率提升，说明LeTS不是“为了精度牺牲速度”，而是真正优化了“思考-搜索”的决策逻辑，这对实际落地的工业化大模型应用极具参考意义。  
```

## r1-searcher--incentivizing-the-search-capability-in-llms-via-reinforcement-learning
### Abstract
Existing Large Reasoning Models (LRMs) have shown the potential of
reinforcement learning (RL) to enhance the complex reasoning capabilities of
Large Language Models~(LLMs). While they achieve remarkable performance on
challenging tasks such as mathematics and coding, they often rely on their
internal knowledge to solve problems, which can be inadequate for
time-sensitive or knowledge-intensive questions, leading to inaccuracies and
hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel
two-stage outcome-based RL approach designed to enhance the search capabilities
of LLMs. This method allows LLMs to autonomously invoke external search systems
to access additional knowledge during the reasoning process. Our framework
relies exclusively on RL, without requiring process rewards or distillation for
a cold start. % effectively generalizing to out-of-domain datasets and
supporting both Base and Instruct models. Our experiments demonstrate that our
method significantly outperforms previous strong RAG methods, even when
compared to the closed-source GPT-4o-mini.
```
### 🌟 论文解读 | R1-Searcher：用强化学习激发大模型搜索能力，突破知识局限

### 📌 背景痛点/本文动机
大语言模型（LLMs）在数学、编码等复杂推理任务中表现出色，但它们依赖内部知识解决问题，面对时效性强或知识密集型问题时，容易出现不准确甚至幻觉（hallucinations）。为解决这一问题，研究聚焦于用外部信息增强LLMs（即检索增强生成RAG），但现有方法存在依赖闭源大模型、蒸馏导致泛化差、推理开销大等不足。因此，本文提出R1 - Searcher，通过强化学习（RL）激励LLMs自主调用外部搜索系统，提升搜索能力与推理效果。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：两阶段基于结果的强化学习框架
设计两阶段RL方法逐步提升LLMs搜索能力。第一阶段用“检索奖励 + 格式奖励”，让模型先学会正确调用外部检索系统的格式，不考虑答案正确性；第二阶段引入“答案奖励”，激励模型利用检索到的知识准确解题，全程仅依赖基于结果的RL，无需蒸馏或冷启动时的监督微调（SFT）。

💡 创新点2：适配训练的改进RL训练方法
为支持训练中LLMs与外部检索环境的交互探索，基于Reinforce++提出改进方法，结合RAG - 基于的rollout和检索掩码的损失计算，让模型在训练时能自主探索如何用检索解决问题。

💡 创新点3：针对性的数据选择策略
从HotpotQA和2WikiMultiHopQA等多跳问答数据集选训练数据，按模型正确回答问题所需rollouts数量，将数据分为易、中、难三个难度等级，分阶段构建训练集，解决检索环境查询范围外等训练效率问题。

### 📈 实验结果
在HotpotQA、2WikiMultiHopQA、Bamboogle、Musique等多跳问答基准测试中，R1 - Searcher表现优异：用Qwen - 2.5 - 7B - Base时，在HotpotQA上比强基线（含GPT - 4o - mini的ReARTeR）提升达48.22%，在2Wiki上提升21.72%；在训练未见过的Bamboogle在线搜索场景中，比32B参数的Search - o1提升11.4%；整体在四个数据集上持续超越现有RAG方法，甚至超过闭源的GPT - 4o - mini。

### 💬 可借鉴之处
1. 强化学习驱动搜索能力：纯RL方式训练，摆脱蒸馏和SFT冷启动依赖，为大模型能力增强提供新范式，且对Base和Instruct模型都有效。
2. 两阶段训练思路：分阶段聚焦“学会调用”和“用好检索解题”，这种分步强化的思路可迁移到其他需分步骤学习的大模型能力训练场景。
3. 数据与训练适配：按任务难度分层选数据、改进RL训练适配检索环境，为处理外部工具交互类训练的效率与效果问题提供了实践参考。
```

## research--learning-to-reason-with-search-for-llms-via-reinforcement-learning
### Abstract
Large Language Models (LLMs) have shown remarkable capabilities in reasoning,
exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating
reasoning with external search processes remains challenging, especially for
complex multi-hop questions requiring multiple retrieval steps. We propose
ReSearch, a novel framework that trains LLMs to Reason with Search via
reinforcement learning without using any supervised data on reasoning steps.
Our approach treats search operations as integral components of the reasoning
chain, where when and how to perform searches is guided by text-based thinking,
and search results subsequently influence further reasoning. We train ReSearch
on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct
extensive experiments. Despite being trained on only one dataset, our models
demonstrate strong generalizability across various benchmarks. Analysis reveals
that ReSearch naturally elicits advanced reasoning capabilities such as
reflection and self-correction during the reinforcement learning process.
```
### 🌟 论文解读 | ReSearch：用强化学习让大模型学会“边搜索边推理”

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）在推理任务上展现出强大能力，像OpenAI - o1、DeepSeek - R1等模型的成功就是例证。不过，将推理与外部搜索流程结合仍颇具挑战，尤其是面对复杂多跳问题（需多轮检索的问题）时。现有多步检索增强生成（RAG）方法多依赖人工设计提示或启发式规则，不仅耗时耗力，对复杂问题也缺乏扩展性；而且给多步RAG框架标注推理步骤成本高、不现实。同时，当下基于强化学习（RL）提升大模型推理能力的工作，大多聚焦内部推理，鲜少探索如何把推理与外部知识检索有效结合。于是，本文提出ReSearch框架，旨在用强化学习让大模型学会“边搜索边推理”，无需推理步骤的有监督数据。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ReSearch框架，将搜索作为推理链的一部分
ReSearch把搜索操作视为链式推理过程的有机组成，推理链不仅包含基于文本的思考（用`[]`包裹），还有搜索查询（用`<search>` `</search>`包裹）和检索结果（用`<result>` `</result>`包裹）。何时、如何执行搜索由之前的文本思考引导，搜索结果又会影响后续文本思考。并且，该框架不提供推理步骤的有监督数据让模型模仿，而是借助强化学习（GRPO算法）激励模型“边搜索边推理”。
💡 创新点2：基于强化学习从头训练模型
在Qwen2.5 - 7B（- Instruct）和Qwen2.5 - 32B（- Instruct）模型上从头训练ReSearch。利用强化学习的思路，采样多个“边搜索边推理”的链（即rollouts），优化大模型策略，让生成高奖励rollouts的概率最大化，以此让模型学会在推理中合理运用搜索。
💡 创新点3：Reward建模与GRPO算法结合
采用Group Relative Policy Optimization（GRPO）作为学习算法，它从一组rollouts中估计基线，而非像近端策略优化（PPO）那样训练单独的 critic 模型。通过设计奖励建模来引导强化学习的优化过程，让模型在训练中逐步掌握“边搜索边推理”的能力。

### 📈 实验结果
在多跳问答基准测试（需多步推理和多次信息检索）上开展大量实验。训练后的ReSearch模型相比基线模型，绝对性能提升8.9% - 22.4%不等。而且仅在一个特定训练集上训练，却能在多个基准测试上评估，展现出良好的泛化性。此外，分析训练过程发现，ReSearch在强化学习过程中能自然激发反射、自我修正等高级推理能力。

### 💬 可借鉴之处
1. 框架设计思路：将外部搜索深度融入推理链，为解决大模型“推理 + 外部工具”结合难题提供了新范式，不再依赖人工设计的繁琐提示或启发式规则，而是让模型自主学习何时、如何用搜索辅助推理。
2. 强化学习应用：证明了强化学习在无需推理步骤有监督数据的情况下，能有效训练大模型获得“边搜索边推理”能力，为大模型推理能力提升开辟了新路径，后续工作可借鉴这种“无监督数据依赖的RL训练”思路拓展模型能力。
3. 泛化性验证：仅用单一训练集训练却能在多基准测试表现良好，说明该框架在模型泛化能力培养上有优势，为打造更通用的大模型推理 + 工具使用能力提供了参考，后续可围绕如何进一步提升泛化性、适配更多工具展开研究。
```

## reinforcement-fine-tuning-for-reasoning-towards-multi-step-multi-source-search-in-large-language-models
### Abstract
Large language models (LLMs) can face factual limitations when responding to
time-sensitive queries about recent events that arise after their knowledge
thresholds in the training corpus. Existing search-augmented approaches fall
into two categories, each with distinct limitations: multi-agent search
frameworks incur substantial computational overhead by separating search
planning and response synthesis across multiple LLMs, while single-LLM
tool-calling methods restrict themselves to sequential planned, single-query
searches from sole search sources. We present Reasoning-Search (R-Search), a
single-LLM search framework that unifies multi-step planning, multi-source
search execution, and answer synthesis within one coherent inference process.
Innovatively, it structure the output into four explicitly defined components,
including reasoning steps that guide the search process (<think>), a
natural-language directed acyclic graph that represents the search plans with
respect to diverse sources (<search>), retrieved results from executing the
search plans (<result>), and synthesized final answers (<answer>). To enable
effective generation of these structured outputs, we propose a specialized
Reinforcement Fine-Tuning (ReFT) method based on GRPO, together with a
multi-component reward function that optimizes LLM's answer correctness,
structural validity of the generated DAG, and adherence to the defined output
format. Experimental evaluation on FinSearchBench-24, SearchExpertBench-25, and
seven Q and A benchmarks demonstrates that R-Search outperforms
state-of-the-art methods, while achieving substantial efficiency gains through
70% reduction in context token usage and approximately 50% decrease in
execution latency. Code is available at
https://github.com/wentao0429/Reasoning-search.
```
### 🌟 论文解读 | 大模型搜索增强新范式：R-Search 如何用单模型统一多步多源搜索？

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽在诸多任务展现强大泛化能力，但面对训练语料知识阈值之后出现的**时效性查询（如近期事件、动态事实）**时，会因训练语料静态性导致事实性局限。为弥补知识缺口，现有搜索增强方法分两类却各有缺陷：  
- **多智能体搜索框架**（如 MindSearch、FinSearch 等）：将“搜索规划→执行→结果合成”拆分到多个 LLM 完成，虽能做全局规划，但多阶段调用带来**巨量计算开销**（token 消耗随搜索复杂度倍增），且“规划 - 执行”分离易导致合成阶段与原始规划逻辑脱节，出现“有规划无推理”问题。  
- **单 LLM 工具调用方法**（如 Search-R1、ZeroSearch 等）：虽效率较高，但依赖离线知识库/模拟环境，难获取真实实时信息；且多为**单查询、单源顺序搜索**，无法利用多查询依赖与多源互补性，应对复杂搜索需求时战略深度不足。  

因此，论文提出 **Reasoning-Search（R-Search）**，目标是用**单个 LLM** 统一“多步规划→多源搜索执行→答案合成”全流程，同时解决效率与能力瓶颈。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：单 LLM 框架下的结构化输出设计  
R-Search 把推理过程的输出拆分为**四个显式组件**，让单 LLM 一次推理完成多任务：  
- `<reason>`：指导搜索过程的推理步骤（解释“为什么这么搜”）；  
- `<search>`：自然语言有向无环图（NL-DAG），表示跨多源的搜索计划（定义“搜什么、怎么搜、源之间咋配合”）；  
- `<result>`：执行搜索计划后获取的结果；  
- `<answer>`：最终合成的答案。  
这种设计让“推理 - 规划 - 执行 - 合成”在**同一 LLM 推理流程**完成，既避免多智能体的碎片化架构，又突破单工具调用的单源/单查询限制。  

💡 创新点2：基于 GRPO 的强化微调（ReFT）与多维度奖励函数  
为让 LLM 高效生成上述结构化输出，论文提出**专门面向搜索任务的强化微调方法**：  
- 基于 GRPO（一种强化学习算法）设计训练范式，让 LLM 学会生成合规且有效的搜索逻辑；  
- 构建**多组件奖励函数**，同时优化三个维度：  
  - 答案正确性（最终回答是否准确）；  
  - DAG 结构有效性（NL-DAG 是否能被可靠解析、执行）；  
  - 输出格式合规性（是否严格遵循 `<reason>/<search>/<result>/<answer>` 结构）。  
  三者结合确保 LLM 既输出准确答案，又生成可执行、易解析的搜索计划。  


### 📈 实验结果
论文在 **FinSearchBench-24、SearchExpertBench-25** 等金融/复杂搜索基准，以及 7 个通用 QA 基准上测试，验证 R-Search 优势：  
- **效果领先**：超越现有 SOTA 方法；  
- **效率暴增**：上下文 token 消耗减少 70%，执行延迟降低约 50%；  
- 多源/多步搜索能力验证：在需跨源、多轮查询的复杂任务中，结构化输出与强化微调让 R-Search 比单工具调用更具策略深度。  


### 💬 可借鉴之处
1. **架构设计思路**：用“单模型 + 结构化输出”统一多阶段任务，减少多智能体碎片化开销，为复杂任务的端到端推理提供范式参考；  
2. **强化学习在 LLM 工具增强的落地**：通过多维度奖励函数，把“答案质量、结构有效性、格式合规性”纳入训练目标，解决结构化输出的可控生成难题；  
3. **多源搜索能力的释放**：NL-DAG 设计让单 LLM 能协调多源（如学术库、实时新闻、垂直搜索引擎），挖掘不同信息库的互补价值，启发未来多源融合类任务的解法。  

代码已开源（https://github.com/wentao0429/Reasoning-search），对大模型工具增强、搜索规划方向感兴趣的同学可深入研究～
```

## zerosearch--incentivize-the-search-capability-of-llms-without-searching
### Abstract
Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a novel RL
framework that incentivizes the capabilities of LLMs to use a real search
engine with simulated searches during training. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both useful and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.
```
### 🌟 论文解读 | ZeroSearch：无需真实搜索，激发大模型搜索能力新范式

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽在诸多下游任务表现卓越，但知识静态且易生成幻觉或过时信息，需外接信息增强可靠性。检索增强生成（RAG）是常用方案，不过早期基于提示工程、后续监督微调或推理时缩放技术等存在工程复杂、计算开销大等问题。近年强化学习（RL）用于提升大模型搜索能力，却面临两大挑战：一是搜索引擎返回文档质量不可控，给训练引入噪声与不稳定；二是RL训练需频繁调用搜索API，成本高昂限制扩展性。为此，论文提出ZeroSearch框架，旨在训练时用模拟搜索来激发大模型使用真实搜索引擎的能力，解决上述痛点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：轻量监督微调打造检索模块  
通过轻量级监督微调，把大语言模型转化为能响应查询、生成“有用”和“含噪声”文档的检索模块。利用prompt设计区分不同质量文档，让模拟用的大模型学会生成不同质量文档，替代真实搜索引擎返回内容，既规避API成本，又能控制文档质量。  

💡 创新点2：课程式rollout策略渐进训练  
RL训练阶段采用基于课程的rollout策略，让生成文档的质量逐步降低，模拟越来越具挑战性的检索场景，循序渐进激发模型推理能力。先让策略模型学习基础输出格式与任务要求，再适应更复杂带噪声的检索场景，实现能力阶梯式提升。  

### 📈 实验结果
大量实验验证ZeroSearch有效性：用3B参数大模型作检索模块时，能有效激发策略模型使用真实搜索引擎的能力；7B参数模型作模拟检索模块，训练出的策略模型性能与真实搜索引擎训练的相当；14B参数模型作模拟时，性能甚至超越真实搜索引擎。此外，该框架在不同参数规模的基础模型和指令微调模型上泛化性好，还能兼容REINFORCE、PPO、GRPO等多种RL算法。  

### 💬 可借鉴之处
1. 思路创新：巧妙利用大模型预训练积累的世界知识，用模型模拟搜索引擎，绕开真实搜索API调用成本与质量不可控问题，为RL结合搜索场景训练开辟新路径。  
2. 训练策略：课程式rollout机制为处理“难度递增任务”类训练提供参考，可迁移到需逐步提升模型能力的场景；轻量监督微调快速构建功能模块的思路，在打造特定能力子模块时值得借鉴。  
3. 兼容性：对不同类型（基础/指令微调）、不同参数规模大模型以及多种RL算法的良好兼容性，让该框架在工业界或研究中更易落地拓展，减少适配成本。  
```

## constructing-and-evaluating-declarative-rag-pipelines-in-pyterrier
### Abstract
Search engines often follow a pipeline architecture, where complex but
effective reranking components are used to refine the results of an initial
retrieval. Retrieval augmented generation (RAG) is an exciting application of
the pipeline architecture, where the final component generates a coherent
answer for the users from the retrieved documents. In this demo paper, we
describe how such RAG pipelines can be formulated in the declarative PyTerrier
architecture, and the advantages of doing so. Our PyTerrier-RAG extension for
PyTerrier provides easy access to standard RAG datasets and evaluation
measures, state-of-the-art LLM readers, and using PyTerrier's unique operator
notation, easy-to-build pipelines. We demonstrate the succinctness of indexing
and RAG pipelines on standard datasets (including Natural Questions) and how to
build on the larger PyTerrier ecosystem with state-of-the-art sparse,
learned-sparse, and dense retrievers, and other neural rankers.
```
### 🌟 论文解读 | 用PyTerrier构建与评估声明式RAG流水线：简化检索增强生成实验

### 📌 背景痛点/本文动机
在信息检索领域，多阶段架构（如排序学习、神经重排器、密集检索等）愈发重要；而检索增强生成（RAG）作为热门多阶段架构，能结合检索文档让大语言模型（LLM）生成更可靠回答，但传统RAG实验存在诸多不便：现有框架要么需用户自行实现流水线（如DSPy）、要么仅支持单一顺序式RAG（如BERGEN）、要么依赖配置文件且组件有限（如FlashRAG）。同时，PyTerrier作为支持多阶段信息检索实验的声明式平台，尚未针对RAG场景做专项拓展。因此，本文动机是基于PyTerrier打造PyTerrier - RAG扩展，简化RAG流水线构建、数据集与评估接入，让研究者更高效开展RAG实验。

### 🚀 核心方法
💡 创新点1：基于PyTerrier声明式架构封装RAG流水线  
PyTerrier本身支持将索引、检索等操作以声明式表达式组合成流水线，PyTerrier - RAG在此基础上，为RAG场景定制数据模型与算子，让RAG流水线（从文档检索到LLM生成回答）能像搭积木一样用直观算子快速构建，无需关注查询层面细节，代码可读性强。例如借助PyTerrier特有的算子符号，能把检索器、重排器、LLM阅读器等组件流畅串联成RAG流程。  

💡 创新点2：丰富生态与工具链整合  
一方面，PyTerrier - RAG提供标准RAG数据集（如Natural Questions）的便捷访问方式，还包含预构建索引，省去研究者手动处理数据的繁琐；另一方面，整合前沿LLM阅读器、多种检索器（稀疏、习得稀疏、密集检索等）与神经排序器，让实验能轻松对接学界最新技术。此外，内置QA任务评估指标（如EM%、F1），一站式满足RAG从数据准备、流水线搭建到结果评估的全流程需求。  

💡 创新点3：支持多元RAG架构  
不同于部分仅支持“检索器 + 阅读器”顺序式RAG的框架（如BERGEN），PyTerrier - RAG还能支持迭代式/自适应RAG（如IR - CoT这类需多轮检索推理的场景）。通过扩展PyTerrier数据模型适配多轮交互逻辑，让复杂RAG实验也能在声明式范式下高效实现，助力研究者探索多步推理等更具挑战性的RAG任务。  

### 📈 实验结果
论文在标准数据集（如Natural Questions）上展示了PyTerrier - RAG的简洁性：从索引构建到RAG流水线搭建，代码表达极为紧凑。同时，借助PyTerrier生态中各类先进检索、排序组件，验证了在不同RAG架构下（包括迭代式）能高效整合技术模块，完成从文档检索到答案生成的全流程实验，充分体现其在RAG研究中“低代码负担、高组件兼容性”的优势，证明该扩展能有效降低RAG实验门槛、提升研究效率。  

### 💬 可借鉴之处
1. 声明式架构思维：在构建复杂多阶段系统（如RAG）时，采用声明式范式能让代码更易读、易维护，聚焦逻辑本身而非执行细节，这一思路可迁移到其他AI流水线类项目（如多模态检索、推荐系统等）。  
2. 生态化工具链打造：通过整合数据集、评估指标、前沿模型等资源，形成“一站式”研究工具，为领域内工具开发提供参考——让研究者减少重复造轮子，把精力放在算法创新上。  
3. 多元架构支持：对RAG这类仍在快速演进的技术，工具需前瞻性支持不同流派（顺序式、迭代式等）架构，PyTerrier - RAG的设计思路启示我们，平台类项目要预留拓展性，适配学术研究的多样性需求。  
```

## single-llm--multiple-roles--a-unified-retrieval-augmented-generation-framework-using-role-specific-token-optimization
### Abstract
Existing studies have optimized retrieval-augmented generation (RAG) across
various sub-tasks, such as query understanding and retrieval refinement, but
integrating these optimizations into a unified framework remains challenging.
To tackle this problem, this work proposes RoleRAG, a unified RAG framework
that achieves efficient multi-task processing through role-specific token
optimization. RoleRAG comprises six modules, each handling a specific sub-task
within the RAG process. Additionally, we introduce a query graph to represent
the decomposition of the query, which can be dynamically resolved according to
the decomposing state. All modules are driven by the same underlying LLM,
distinguished by task-specific role tokens that are individually optimized.
This design allows RoleRAG to dynamically activate different modules within a
single LLM instance, thereby streamlining deployment and reducing resource
consumption. Experimental results on five open-domain question-answering
datasets demonstrate the effectiveness, generalizability, and flexibility of
our framework.
```
### 🌟 论文解读 | 单LLM玩转多角色：RoleRAG用角色专属Token打造统一RAG框架

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽在众多任务中表现卓越，但在准确性、可靠性和时效性上仍存挑战。检索增强生成（RAG）为这些问题提供了有效解法，它能让LLM结合外部知识生成更优响应。然而现有RAG优化研究存在两大困境：一是多数研究聚焦单个子任务优化（如查询理解、检索精修等），却难整合到统一框架；二是少数尝试在单LLM内整合多组件的工作，面临额外数据收集、重训成本高，以及自反思机制复杂度上升导致性能和泛化能力下降的问题。因此，打造能高效整合多任务、低资源消耗的统一RAG框架成为关键诉求。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RoleRAG统一框架，基于角色专属Token优化实现多任务高效处理  
RoleRAG包含六个模块（查询图构建器、检索判断器、子答案生成器、总结器、新查询生成器、答案推理器），各模块对应RAG流程中特定子任务。通过引入任务专属的“角色Token”并优化其嵌入，让同一基础LLM能在不同模块间动态切换功能。训练时仅优化角色Token，推理时单LLM实例靠不同角色Token激活对应模块，简化部署且降低资源消耗。  

💡 创新点2：设计查询图动态分解与优化机制  
引入查询图来表示查询的分解结构，能依据分解状态动态解析。比如将复杂原始查询拆分为多个子查询并构建有向无环图，过程中还会动态剔除冗余子查询、按需生成新子查询，提升检索效率与相关性，更好处理复杂查询场景。  

💡 创新点3：发布覆盖RAG全流程的数据集  
为训练不同RAG模块，团队发布了首个涵盖RAG系统完整 pipeline 的数据集，为相关模块训练提供全面数据支撑，助力整个框架性能提升。  

### 📈 实验结果
在五个开放域问答数据集上开展实验，RoleRAG在精确匹配分数（exact match score）指标上，较现有SOTA RAG方法实现了16% - 64%的性能提升。额外实验也验证了框架在泛化性与鲁棒性上的优势，充分展现其在不同场景下稳定高效工作的能力。  

### 💬 可借鉴之处
1. 多任务统一框架思路：将RAG各子任务整合到单LLM驱动的统一框架，通过角色Token解耦不同功能模块，为复杂任务流程的工程化落地提供了“单模型多能力”的高效范式，减少多模型协作的资源开销。  
2. 动态查询处理机制：查询图的动态构建与优化，为处理复杂、多步骤推理类查询提供了结构化分解思路，可借鉴到需分步拆解问题的智能问答、知识推理等场景。  
3. 轻量化训练策略：仅优化角色Token而非全模型参数，在保证模块功能实现的同时大幅降低训练成本，这种“软提示 + 局部调优”的方式对资源有限的研发场景很有参考价值。  
4. 全流程数据集建设：打造覆盖RAG全流程的数据集，填补领域数据空白，为后续研究中模块训练、性能对比等提供了基础资源，启发研究者重视垂直领域全链路数据的构建。  
```

## r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning
### Abstract
Large language models (LLMs) have notably progressed in multi-step and
long-chain reasoning. However, extending their reasoning capabilities to
encompass deep interactions with search remains a non-trivial challenge, as
models often fail to identify optimal reasoning-search interaction
trajectories, resulting in suboptimal responses. We propose R-Search, a novel
reinforcement learning framework for Reasoning-Search integration, designed to
enable LLMs to autonomously execute multi-step reasoning with deep search
interaction, and learn optimal reasoning search interaction trajectories via
multi-reward signals, improving response quality in complex logic- and
knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when
to retrieve or reason, while globally integrating key evidence to enhance deep
knowledge interaction between reasoning and search. During RL training,
R-Search provides multi-stage, multi-type rewards to jointly optimize the
reasoning-search trajectory. Experiments on seven datasets show that R-Search
outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%
(out-of-domain). The code and data are available at
https://github.com/QingFei1/R-Search.
```
### 🌟 论文解读 | R-Search：用多奖励强化学习赋能LLM推理与搜索深度交互

### 📌 背景痛点/本文动机
大语言模型（LLMs）在多步骤和长链推理任务中取得了显著进展，但要让其推理能力与搜索深度交互仍面临挑战。现有模型常难以找到最优的“推理 - 搜索”交互轨迹，导致输出质量欠佳。在复杂的逻辑密集型和知识密集型任务（如多跳问答）中，传统方法存在两大局限：一是模型内部决定的检索时机未必契合实际需求；二是推理与搜索的模块化设计限制了外部知识与推理链的深度交互，易让模型基于局部信息做决策，最终影响输出质量。因此，如何让LLM动态整合外部知识、学习最优交互轨迹成为关键问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：R - Search框架设计——强化学习驱动推理与搜索深度整合  
提出R - Search这一基于强化学习（RL）的新颖框架，让LLM能动态交错执行多步骤推理与搜索。LLM可在任意token级推理步骤触发检索，将检索内容无缝融入推理过程，实现推理与外部知识的深度耦合；交互后，LLM还能通过推理把检索文档提炼为证据，从全局视角重新评估和构建关键知识，聚焦解决任务的核心事实。该框架能引导LLM保障中间推理的合理性与检索知识的完整性，联合优化RAG中复杂的“推理 - 搜索”轨迹。  

💡 创新点2：多阶段多类型奖励机制——引导学习最优交互序列  
设计了包含答案质量、证据质量、格式正确性等的多阶段、多类型奖励机制。这些互补的奖励信号推动模型学习最优“推理 - 搜索”交互序列，其中证据奖励能促使模型关注关键中间推理步骤的事实质量，助力构建更稳健的推理路径，减少走捷径或臆测行为的风险。  

💡 创新点3：R - Search - as - a - Tool（RSTool）——模块化与实用拓展  
提出RSTool，将推理中高质量证据模块化封装为可迁移组件，把复杂且耗时的“推理 - 搜索”交互卸载到本地部署，具备很强的实际可扩展性。  

### 📈 实验结果
在7个涵盖多跳和单跳问答任务的数据集上开展实验，结果显示R - Search在域内（in - domain）比先进RAG基线最多领先32.2%，在域外（out - of - domain）最多领先25.1%。此外，消融实验和训练动态分析等进一步验证了证据整合与多奖励建模的有效性，还揭示了不同RL算法下的性能趋势与检索行为等洞察。  

### 💬 可借鉴之处
1. 框架设计思路：将强化学习引入RAG场景来优化“推理 - 搜索”轨迹，为解决复杂任务中模型与外部知识深度交互问题提供了新范式，启示在需动态交互的任务中可探索RL驱动的框架设计。  
2. 奖励机制构建：多维度、多阶段的奖励设计思路，能为强化学习中引导智能体学习提供更全面的信号参考，可借鉴到需多目标优化的任务场景。  
3. 工具化落地：RSTool的模块化思路，为复杂AI能力向实用工具转化、降低部署成本提供了参考，利于推动技术在实际场景的落地应用。  
```

## enhancing-retrieval-augmented-large-language-models-with-iterative-retrieval-generation-synergy
### Abstract
Large language models are powerful text processors and reasoners, but are
still subject to limitations including outdated knowledge and hallucinations,
which necessitates connecting them to the world. Retrieval-augmented large
language models have raised extensive attention for grounding model generation
on external knowledge. However, retrievers struggle to capture relevance,
especially for queries with complex information needs. Recent work has proposed
to improve relevance modeling by having large language models actively involved
in retrieval, i.e., to improve retrieval with generation. In this paper, we
show that strong performance can be achieved by a method we call Iter-RetGen,
which synergizes retrieval and generation in an iterative manner. A model
output shows what might be needed to finish a task, and thus provides an
informative context for retrieving more relevant knowledge which in turn helps
generate a better output in the next iteration. Compared with recent work which
interleaves retrieval with generation when producing an output, Iter-RetGen
processes all retrieved knowledge as a whole and largely preserves the
flexibility in generation without structural constraints. We evaluate
Iter-RetGen on multi-hop question answering, fact verification, and commonsense
reasoning, and show that it can flexibly leverage parametric knowledge and
non-parametric knowledge, and is superior to or competitive with
state-of-the-art retrieval-augmented baselines while causing fewer overheads of
retrieval and generation. We can further improve performance via
generation-augmented retrieval adaptation.
```
### 🌟 论文解读 | 迭代检索-生成协同，增强检索增强型大语言模型

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽强大但存在知识过时、易产生幻觉等局限，检索增强型大语言模型为将模型生成锚定外部知识提供了思路。然而传统检索增强方法存在不足：一次性检索难以满足复杂信息需求任务；近期将检索与生成交错的方法，存在无法整体处理检索知识、增加检索和生成开销等问题。因此，本文旨在通过迭代式的检索 - 生成协同（Iter - RetGen）来增强检索增强型大语言模型性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Iter - RetGen方法，实现迭代式检索 - 生成协同
Iter - RetGen迭代执行检索增强生成和生成增强检索。首先基于任务输入（初始用任务输入作查询）进行检索增强生成得到输出，该输出展示了完成任务所需信息，可作为生成增强检索的上下文来获取更相关知识，新检索知识又能助力下一轮检索增强生成，整体处理检索到的知识且保留生成灵活性，无结构约束。
💡 创新点2：生成增强的检索适配
利用模型生成来适配检索，从能访问模型生成的重排序器中提取知识到仅能访问任务输入的密集检索器，在用户输入易收集但相关知识或理想输出无标注场景下有益，可进一步提升性能并减少迭代次数。

### 📈 实验结果
在多跳问答、事实验证和常识推理三项任务上评估Iter - RetGen。在少样本设置下让大语言模型生成推理链和最终答案，结果显示：在六个数据集中四个数据集上，相较之前最先进的检索增强方法，最多实现8.6%的绝对性能提升，另外两个数据集上表现具竞争力；生成通常从更多迭代中受益，两次迭代性能提升最明显，可通过选择合适迭代次数定制性能 - 成本权衡；经生成增强的检索适配后能进一步提升性能。同时发现精确匹配等自动评估指标可能低估大语言模型在问答任务中的性能，用大语言模型评估更可靠；Iter - RetGen在问答任务上始终优于Self - Ask，无论上下文非参数知识是否提及答案。

### 💬 可借鉴之处
方法设计上，Iter - RetGen的迭代协同思路为处理复杂信息需求任务提供了更优范式，避免传统交错方法的局限，简化流程且减少检索生成开销；评估视角上，提醒研究者关注自动评估指标对大语言模型性能评估的不足，考虑用大语言模型评估更可靠；技术拓展上，生成增强的检索适配为在数据标注不足场景下优化检索提供了新途径，可借鉴该思路拓展模型与外部知识交互的方式，提升模型利用参数知识和非参数知识的能力。
```

## search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms
### Abstract
Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.
```
### 🌟 论文解读 | 让大模型边“思考-搜索-精炼”边推理：AutoRefine 革新检索增强范式

### 📌 背景痛点/本文动机
大语言模型（LLMs）在推理任务中展现了强大能力，但受限于训练语料的知识储备，在需要实时或精准知识的任务中表现不足。检索增强生成（RAG）通过让LLM调用外部工具查询知识库来缓解这一问题，然而现有方法存在两大核心缺陷：  
1. **缺乏对检索文档的精炼**：传统“思考时搜索（search-during-think）”范式直接基于检索到的（可能包含噪声、无关信息的）文档生成答案，没有先提炼关键信息，限制了模型识别缺失知识、基于不完整证据推理或迭代优化检索的能力；  
2. **检索专属奖励未充分探索**：多数检索增强推理方法仅依赖“结果导向”奖励（如最终答案是否正确），对“如何提升检索过程本身”缺乏直接指导，导致模型难学如何获取更相关、更有信息量的文档。  

为解决这些问题，论文提出 **AutoRefine** —— 一个基于强化学习（RL）的后训练框架，重塑检索增强推理范式。

### 🚀 核心方法
💡 创新点1：“思考时搜索+精炼（search-and-refine-during-think）”新范式  
传统RAG是“思考→搜索→直接生成答案”，AutoRefine则在“搜索”与“生成答案”之间插入**显式的知识精炼步骤**。具体通过 `<search>...</search>[documents]<refine>...</refine>` 这样的模板，强制模型先对检索到的文档进行“过滤噪声、提炼关键、组织证据”的迭代操作，再生成最终答案。这一步让模型在回答前先“消化”检索到的信息，避免被无关内容干扰。  

💡 创新点2：融合“结果奖励+检索专属奖励”的强化学习训练  
AutoRefine采用 **Group Relative Policy Optimization（GRPO）** 算法，同时优化两种奖励：  
- **结果导向奖励**：评估最终答案的正确性；  
- **检索专属奖励**：基于 `<refine>` 块中提炼内容的质量计算，直接指导“如何更好检索与利用文档”。  
训练时，模型会生成多条包含“思考、搜索、精炼、回答”的推理轨迹，再用GRPO对这些轨迹做优化，让模型学会在推理中更智能地调用检索工具、更高效地处理信息。  

### 📈 实验结果
论文在单跳（single-hop）和多跳（multi-hop）问答基准测试中验证AutoRefine：  
- **性能超越现有方法**：在复杂的多跳推理场景下提升尤为显著，证明“搜索+精炼”范式对长链条、多步骤推理的有效性；  
- **检索质量更高频**：分析显示AutoRefine更频繁发起高质量搜索，且能有效整合、合成证据，减少噪声干扰。  

### 💬 可借鉴之处
1. **范式创新**：“搜索后精炼再回答”的流程设计，为解决“检索信息噪声/无关”问题提供了新视角，可启发后续RAG类工作优化推理流程；  
2. **奖励设计**：将“过程性奖励（检索质量）”与“结果性奖励（答案正确）”结合，展示了强化学习在引导模型“过程优化”上的潜力，为RL与LLM结合的奖励机制设计提供参考；  
3. **多跳推理适配**：在多跳任务中表现突出，说明该方法对需要多次检索、信息拼接的复杂任务友好，可迁移到知识密集型的长文本推理、多步骤决策等场景。  

总之，AutoRefine通过“流程范式+奖励机制”的双重创新，让大模型在检索增强推理中更自主、更智能，为突破“知识储备限制+检索信息噪声”两大痛点提供了一套简洁而有力的方案~
```

## smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback
### Abstract
RAG systems consist of multiple modules to work together. However, these
modules are usually separately trained. We argue that a system like RAG that
incorporates multiple modules should be jointly optimized to achieve optimal
performance. To demonstrate this, we design a specific pipeline called
\textbf{SmartRAG} that includes a policy network and a retriever. The policy
network can serve as 1) a decision maker that decides when to retrieve, 2) a
query rewriter to generate a query most suited to the retriever, and 3) an
answer generator that produces the final response with/without the
observations. We then propose to jointly optimize the whole system using a
reinforcement learning algorithm, with the reward designed to encourage the
system to achieve the best performance with minimal retrieval cost. When
jointly optimized, all the modules can be aware of how other modules are
working and thus find the best way to work together as a complete system.
Empirical results demonstrate that the jointly optimized SmartRAG can achieve
better performance than separately optimized counterparts.
```
### 🌟 论文解读 | SmartRAG：用环境反馈端到端优化RAG系统，突破模块分离训练瓶颈

### 📌 背景痛点/本文动机
尽管大语言模型（LLMs）在诸多领域展现出强大能力，但处理模型参数之外的知识类问题仍颇具挑战。检索增强生成（RAG）通过从外部工具检索信息有效提升了模型在这类场景的表现。然而，传统RAG系统的多个模块（如检索器、决策器、查询重写器等）往往是**分离训练**的。这带来两大问题：一是中间模块的“黄金答案”（最优输出）通常难以获取，甚至依赖特定模型或检索器；二是分离优化会导致各模块缺乏对整体系统的协同感知，难以达到全局最优性能。因此，论文提出要对RAG这类多模块系统进行**端到端的联合优化**，让各模块在协作中相互适配。

### 🚀 核心方法（介绍本文的几个创新点）
#### 💡 创新点1：设计SmartRAG系统架构，让Policy Network身兼三职  
SmartRAG的核心由**策略网络（Policy Network）**和**检索器（Retriever）**构成。其中，策略网络承担三个关键角色：  
- 决策器（Decision Maker）：根据输入问题与已有观测，决定是否需要发起检索；  
- 查询重写器（Query Rewriter）：若决定检索，生成更适配检索器的查询语句；  
- 答案生成器（Answer Generator）：若认为当前信息足够，直接生成最终回答。  
这种“一专多能”的设计让单个策略网络串联起RAG的核心流程，为后续端到端优化打下基础。  

#### 💡 创新点2：基于强化学习（RL）的端到端联合优化  
论文采用**近端策略优化（PPO）**算法对SmartRAG进行联合训练，用**环境反馈**替代传统“黄金答案”作为监督信号。设计奖励函数时，兼顾两大目标：一是“正确回答问题”，二是“最小化检索成本”（减少不必要的检索次数）。通过强化学习，策略网络能在与环境（如外部知识库、检索工具）的交互中，学习到各模块间的最优协作方式——何时检索、检索什么、如何回答，让整个系统形成有机整体。  


### 📈 实验结果
论文在多个数据集上验证了SmartRAG的有效性，核心结论是：**联合优化的SmartRAG显著优于模块分离训练的基线系统**。此外，通过系统性分析，论文还展示了SmartRAG如何学习“何时检索、检索什么、如何回答”这三项关键能力，证明了模块间协同感知对系统性能的提升作用。  


### 💬 可借鉴之处
1. **架构设计思路**：将RAG多模块功能收敛到“策略网络+检索器”的简洁架构，用单一网络承载决策、重写、生成，为复杂系统的模块化整合提供了参考；  
2. **优化范式创新**：用强化学习做端到端联合优化，摆脱对“黄金答案”的强依赖，更贴合真实场景中模块输出难标注的痛点；  
3. **奖励函数设计**：平衡“任务效果（回答正确率）”与“资源成本（检索次数）”，这种多目标权衡的思路可迁移到其他需资源约束的AI系统设计中。  

总之，SmartRAG为RAG系统的工程化落地提供了“协同优化”的新范式，让多模块系统从“各自为战”走向“全局最优”，值得从事检索增强、大模型应用的研究者与工程师深入参考~
```

## neuro-symbolic-query-compiler
### Abstract
Precise recognition of search intent in Retrieval-Augmented Generation (RAG)
systems remains a challenging goal, especially under resource constraints and
for complex queries with nested structures and dependencies. This paper
presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar
rules and compiler design, to bridge this gap. It theoretically designs a
minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize
complex queries. Unlike previous methods, this grammar maintains completeness
while minimizing redundancy. Based on this, QCompiler includes a Query
Expression Translator, a Lexical Syntax Parser, and a Recursive Descent
Processor to compile queries into Abstract Syntax Trees (ASTs) for execution.
The atomicity of the sub-queries in the leaf nodes ensures more precise
document retrieval and response generation, significantly improving the RAG
system's ability to address complex queries.
```
### 🌟 论文解读 | 神经符号查询编译器：精准破解RAG复杂查询难题

### 📌 背景痛点/本文动机
在人工智能领域，检索增强生成（RAG）系统中精准识别搜索意图仍是一大挑战，尤其面对资源约束以及带有嵌套结构和依赖关系的复杂查询时。传统RAG系统处理复杂查询时，一次性检索到所有相关文档的概率大幅降低，性能受限。同时，人工神经网络（ANNs）虽拟合能力强，但在需显式符号推理的任务上表现不佳；现有处理复杂查询的方法要么存在冗余、资源浪费问题，要么过度依赖大语言模型（LLM）性能，在性能与效率间难以平衡。此外，如何复刻人类大脑中神经计算与符号推理的协同机制来处理复杂查询，也是待解难题。基于此，论文提出QCompiler框架来填补这些空白。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：设计极简且充分的BNF语法G[q]  
从理论层面设计了最小却足够用的Backus - Naur Form（BNF）语法G[q]来形式化复杂查询。该语法与以往方法不同，在最小化冗余的同时保持完整性，为精准识别搜索意图奠定基础，能将复杂查询纳入规范的语法框架内处理。  

💡 创新点2：提出神经符号框架QCompiler  
QCompiler受语言学语法规则和编译器设计启发，融合神经计算与符号推理。它包含查询表达式翻译器、词法语法解析器和递归下降处理器，能将查询编译成抽象语法树（AST）执行。其中，查询表达式翻译器可将自然语言查询转成基于BNF的表达式；词法语法解析器负责解析成AST；递归下降处理器借助语法规则在节点递归推理。AST能捕捉原始复杂查询的隐式搜索意图、嵌套结构和依赖关系，叶节点子查询的原子性保障了文档检索和响应生成更精准，提升RAG处理复杂查询能力。  

### 📈 实验结果
论文在四个多跳基准测试中验证了QCompiler的效果，展现出对复杂查询出色的理解能力。通过将复杂查询编译为AST结构，能以更少但更精准的文档检索，提升响应准确性，在处理嵌套结构与依赖关系的复杂查询场景下表现优异，验证了这种树结构表示复杂查询的准确性，以及对RAG系统效率和精度的提升作用。  

### 💬 可借鉴之处
1. 语法设计思路：在处理需形式化表示的任务时，可借鉴这种“极简且充分”的语法设计理念，平衡表达能力与冗余度，为后续处理建立高效规范的基础。  
2. 神经 - 符号融合：将神经网络的感知处理能力与符号系统的规则推理能力结合，为解决需复杂推理、结构解析的AI任务提供了范式参考，比如在智能问答、复杂指令理解等场景可尝试类似融合思路。  
3. 工程实用性：QCompiler允许生产环境开发者验证子查询节点正确性并干预推理过程，这种可解释、可干预的设计思路，对打造可靠的工业级AI系统很有借鉴意义，能提升系统在实际部署中的稳定性与可维护性。  
```

## interleaved-reasoning-for-large-language-models-via-reinforcement-learning
### Abstract
Long chain-of-thought (CoT) significantly enhances large language models'
(LLM) reasoning capabilities. However, the extensive reasoning traces lead to
inefficiencies and an increased time-to-first-token (TTFT). We propose a novel
training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs
to interleave thinking and answering for multi-hop questions. We observe that
models inherently possess the ability to perform interleaved reasoning, which
can be further enhanced through RL. We introduce a simple yet effective
rule-based reward to incentivize correct intermediate steps, which guides the
policy model toward correct reasoning paths by leveraging intermediate signals
generated during interleaved reasoning. Extensive experiments conducted across
five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)
demonstrate consistent improvements over traditional think-answer reasoning,
without requiring external tools. Specifically, our approach reduces TTFT by
over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,
our method, trained solely on question answering and logical reasoning
datasets, exhibits strong generalization ability to complex reasoning datasets
such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to
reveal several valuable insights into conditional reward modeling.
```
### 🌟 论文解读 | 强化学习驱动大模型的交错推理：效率与能力双提升

### 📌 背景痛点/本文动机
大语言模型（LLM）借助长思维链（CoT）能显著增强推理能力，但传统“思考 - 回答”范式存在两大关键缺陷：一是生成答案前要完成完整推理轨迹，导致首 token 生成时间（TTFT）大幅增加，在实时交互场景（如对话助手）中影响用户体验；二是延迟到推理结束才生成答案，易让错误中间步骤传播，引发最终答案不准确与推理低效（如过度/不足思考）。同时，现有强化学习（RL）训练推理型 LLM 时，常把中间推理轨迹当附属品，未充分利用其中间信号辅助训练与交互。因此，如何让模型在推理中交错“思考”与“回答”、利用中间信号优化训练和交互，成为待解难题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出交错推理 RL 训练范式  
提出“交错推理”这一全新 RL 训练范式，让 LLM 在多跳问题推理中交替进行思考与回答，无需外部工具。该范式使模型在推理过程中生成有信息量的中间答案，既给用户及时反馈（降低 TTFT），又为自身后续推理提供可验证的奖励信号，引导向正确最终答案推进。  

💡 创新点2：设计基于规则的奖励机制  
引入简单有效的基于规则的奖励，激励模型生成正确中间步骤。利用交错推理中产生的中间信号，为策略模型指明正确推理路径，在训练时提供密集且一致的反馈，解决传统训练中中间步骤难 credit assignment（ credit assignment 指训练中如何把最终结果的奖惩分配到各中间步骤）的问题。  


### 📈 实验结果
1. 效率与准确率提升：在 5 个不同数据集上，用 PPO、GRPO、REINFORCE++ 三种 RL 算法实验，相比传统“思考 - 回答”推理，平均 TTFT 降低超 80%；Pass@1 准确率最多提升 19.3%。  
2. 泛化能力验证：仅在问答和逻辑推理数据集上训练，模型对 MATH、GPQA、MMLU 等复杂推理数据集展现强泛化能力。  
3. 奖励建模洞察：深入分析 conditional reward modeling，得到关于奖励建模、稳定 RL 训练和模型推理动态的有价值见解。  


### 💬 可借鉴之处
1. 训练范式创新：“交错推理”范式为优化大模型推理时的交互效率与训练有效性提供新思路，打破“先完整思考再回答”的固定顺序，启发后续探索更灵活的推理交互模式。  
2. 奖励机制设计：基于规则的简单奖励在避免复杂 reward model 训练的同时，有效利用中间信号引导训练，证明无需复杂模型也能挖掘中间步骤价值，为轻量化奖励设计提供参考。  
3. 泛化性探索：仅用基础推理类数据集训练却能泛化到复杂任务，说明该方法抓准了推理能力的共性本质，为大模型跨任务推理能力培养提供实践范例。  
```

## chain-of-retrieval-augmented-generation
### Abstract
This paper introduces an approach for training o1-like RAG models that
retrieve and reason over relevant information step by step before generating
the final answer. Conventional RAG methods usually perform a single retrieval
step before the generation process, which limits their effectiveness in
addressing complex queries due to imperfect retrieval results. In contrast, our
proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the
model to dynamically reformulate the query based on the evolving state. To
train CoRAG effectively, we utilize rejection sampling to automatically
generate intermediate retrieval chains, thereby augmenting existing RAG
datasets that only provide the correct final answer. At test time, we propose
various decoding strategies to scale the model's test-time compute by
controlling the length and number of sampled retrieval chains. Experimental
results across multiple benchmarks validate the efficacy of CoRAG, particularly
in multi-hop question answering tasks, where we observe more than 10 points
improvement in EM score compared to strong baselines. On the KILT benchmark,
CoRAG establishes a new state-of-the-art performance across a diverse range of
knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to
understand the scaling behavior of CoRAG, laying the groundwork for future
research aimed at developing factual and grounded foundation models.
```
### 🌟 论文解读 | 突破单步检索限制：CoRAG开启多步检索增强生成新范式

### 📌 背景痛点/本文动机
在企业应用等场景中，检索增强生成（RAG）是核心技术之一，它能让大模型结合专有数据源生成可靠且基于事实的响应。但传统RAG存在局限：常规RAG在生成前仅执行**单步检索**，面对复杂查询时，因检索结果可能不完善，难以有效应对；而且检索模型为效率采用的固定大小向量表示等架构，限制了处理复杂查询的能力，在多跳推理任务中，也难确定初始该检索什么信息。为打破检索质量瓶颈，让模型能像人类解决复杂问题那样迭代检索信息，本文提出CoRAG方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：CoRAG多步动态检索与查询重构范式  
传统RAG单步检索后生成，CoRAG则允许模型**基于状态演化动态重构查询**，在生成最终答案前，逐步检索和推理相关信息。这种方式模拟人类解决复杂问题时迭代找信息的过程，能在检索器没返回有用信息时，尝试不同查询重写策略，探索查询的多方面信息。  

💡 创新点2：拒绝采样增强数据集与显式训练  
现有RAG数据集常只给最终正确答案，为有效训练CoRAG，本文用**拒绝采样**自动生成中间检索链，扩充数据集。之后用开源语言模型在这些增强数据集上，以标准下一个token预测目标微调，让模型显式学习逐步检索的能力，而非仅依赖模型上下文学习或专有模型蒸馏。  

💡 创新点3：多样测试时解码策略调控计算资源  
测试阶段，提出贪心解码、best - of - N采样、树搜索等**多种解码策略**，通过控制采样检索链的长度和数量，调节模型测试时的计算开销（如token消耗、检索器调用频率等），实现测试时计算资源的灵活缩放。  

### 📈 实验结果
- 多跳问答任务：在多跳推理类QA任务中，相比强基线，CoRAG在EM分数上提升超10个点，验证了其处理需多步检索推理复杂任务的有效性。  
- KILT基准测试：在涵盖多样知识密集型任务的KILT基准上，CoRAG在几乎所有任务的隐藏测试集上取得全新SOTA成绩。  
- 缩放行为分析：不同解码策略下，总token消耗和模型性能近似呈对数线性关系（系数因数据集而异）；还发现对不同任务类型CoRAG缩放行为不同，如NQ这类现有SOTA检索器已高召回的数据集，测试时缩放收益有限，为依查询复杂度和检索器质量动态分配测试计算资源提供依据。  

### 💬 可借鉴之处
- 技术范式层面：CoRAG提出的多步动态检索 + 查询重构思路，为RAG突破单步限制、处理复杂任务提供了新范式，后续研究可在此基础上探索更灵活的检索 - 生成交互逻辑。  
- 数据增强层面：用拒绝采样自动生成中间检索链来增强数据集，为解决RAG数据集缺少中间过程标注的问题提供了可行方法，可借鉴到需中间步骤数据的模型训练场景。  
- 测试优化层面：通过多样解码策略调控测试时计算，这种根据任务和检索器情况灵活分配资源的思路，对大模型实际部署时平衡性能与成本很有启发，可用于优化模型推理阶段的资源利用。  
- 研究方向层面：CoRAG为开发事实性、有依据的基础模型（foundation models）奠定基础，后续在减少模型幻觉、提升内容可靠性方面，可延续其多步检索推理的思路深入探索。  
```

## okralong--a-flexible-retrieval-augmented-framework-for-long-text-query-processing
### Abstract
Large Language Models (LLMs) encounter challenges in efficiently processing
long-text queries, as seen in applications like enterprise document analysis
and financial report comprehension. While conventional solutions employ
long-context processing or Retrieval-Augmented Generation (RAG), they suffer
from prohibitive input expenses or incomplete information. Recent advancements
adopt context compression and dynamic retrieval loops, but still sacrifice
critical details or incur iterative costs. To address these limitations, we
propose OkraLong, a novel framework that flexibly optimizes the entire
processing workflow. Unlike prior static or coarse-grained adaptive strategies,
OkraLong adopts fine-grained orchestration through three synergistic
components: analyzer, organizer and executor. The analyzer characterizes the
task states, which guide the organizer in dynamically scheduling the workflow.
The executor carries out the execution and generates the final answer.
Experimental results demonstrate that OkraLong not only enhances answer
accuracy but also achieves cost-effectiveness across a variety of datasets.
```
### 🌟 论文解读 | OkraLong：灵活应对长文本查询的检索增强新框架

### 📌 背景痛点/本文动机
在企业文档分析、财务报告理解等实际应用场景中，大语言模型（LLMs）处理长文本查询时面临诸多挑战。传统的长上下文（LC）处理方案虽能利用模型全局语境感知能力，但输入成本过高；检索增强生成（RAG）方案虽通过检索减少输入长度，却存在信息遗漏风险。近年的上下文压缩和动态RAG等改进方法，要么丢失关键细节，要么带来迭代成本。为解决这些问题，论文提出OkraLong框架，旨在灵活优化长文本查询处理全流程。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：细粒度编排的三组件架构  
OkraLong由分析器（Analyzer）、编排器（Organizer）和执行器（Executor）三个协同组件构成。分析器基于查询语义和初步检索上下文，精准刻画任务状态；编排器依据分析结果动态生成优化执行计划；执行器则负责执行多样的处理流程与策略，实现对全处理流程的细粒度优化，区别于以往静态或粗粒度的自适应策略。  

💡 创新点2：任务适配与成本感知的灵活优化  
一方面，OkraLong能根据不同任务需求定制策略，比如比较类查询需单独实体检索、桥接类查询触发分步推理链，提升回答准确性；另一方面，成本感知调度可在不同场景动态分配token预算与信息资源，如语义密集查询分配多聚合上下文、特定信息查询用靶向上下文切片，减少冗余开销。同时还开发创新执行算子支撑定制策略。  

### 📈 实验结果
论文使用涵盖多领域、从简单事实查询到复杂多步推理任务的长文本理解数据集评估OkraLong。结果表明，相比现有先进方法，OkraLong不仅提升了回答准确率，还在成本效益方面表现更优，验证了框架在 accuracy 和 cost-effectiveness 上的双重优势。  

### 💬 可借鉴之处
从技术设计角度，其细粒度拆分系统组件（分析 - 编排 - 执行）的思路，为复杂NLP任务流程优化提供了模块化、可扩展的参考范式；在实际落地层面，成本感知与任务适配的结合思路，对于企业级大模型应用（如文档检索、报告分析）平衡性能与成本极具借鉴价值，能指导开发者在不同业务场景下更智能地调度资源、设计流程。
```

## retrollm--empowering-large-language-models-to-retrieve-fine-grained-evidence-within-generation
### Abstract
Large language models (LLMs) exhibit remarkable generative capabilities but
often suffer from hallucinations. Retrieval-augmented generation (RAG) offers
an effective solution by incorporating external knowledge, but existing methods
still face several limitations: additional deployment costs of separate
retrievers, redundant input tokens from retrieved text chunks, and the lack of
joint optimization of retrieval and generation. To address these issues, we
propose \textbf{RetroLLM}, a unified framework that integrates retrieval and
generation into a single, cohesive process, enabling LLMs to directly generate
fine-grained evidence from the corpus with constrained decoding. Moreover, to
mitigate false pruning in the process of constrained evidence generation, we
introduce (1) hierarchical FM-Index constraints, which generate
corpus-constrained clues to identify a subset of relevant documents before
evidence generation, reducing irrelevant decoding space; and (2) a
forward-looking constrained decoding strategy, which considers the relevance of
future sequences to improve evidence accuracy. Extensive experiments on five
open-domain QA datasets demonstrate RetroLLM's superior performance across both
in-domain and out-of-domain tasks. The code is available at
\url{https://github.com/sunnynexus/RetroLLM}.
```
### 🌟 论文解读 | RetroLLM：让大模型在生成中直接获取细粒度证据，革新检索增强范式

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽生成能力强大，但易出现“幻觉”问题。检索增强生成（RAG）通过引入外部知识缓解该问题，然而现有方法存在诸多局限：需额外部署独立检索器增加成本、检索文本块带来冗余输入token、检索与生成缺乏联合优化等。此外，生成式检索（GR）虽尝试整合但仍需映射文档内容，无法无缝衔接检索与生成。同时，直接基于FM - Index的约束生成证据易出现“错误剪枝”，即早期解码步骤的错误导致正确证据序列被剪枝。这些痛点推动了RetroLLM的提出，旨在打造统一框架，让检索与生成在自回归解码中无缝融合，同时解决错误剪枝问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RetroLLM统一框架
RetroLLM将检索与生成整合到单一自回归解码过程，让大模型能在约束解码下从语料库直接生成细粒度证据与最终答案。无需独立嵌入模型，模型可自主决定检索多少证据及何时生成最终响应，实现RAG任务的联合优化，提升系统灵活性与整体性能。

💡 创新点2：分层FM - Index约束应对错误剪枝
构建分层FM - Index，先生成语料库约束线索以识别候选文档子集，再在该子集的FM - Index约束下生成证据，大幅减少早期解码步骤中无关解码空间，缓解错误剪枝问题，让证据生成更聚焦于相关文档范围。

💡 创新点3：前瞻性约束解码策略提升证据准确性
在证据生成时引入前瞻性约束解码，利用文档FM - Index依据线索识别候选文档内的未来窗口，再通过相关性模型对这些窗口打分，引导模型生成更相关的证据，考虑未来序列相关性以提升证据准确性。

### 📈 实验结果
在五个开放域QA数据集上开展大量实验，涵盖域内与域外任务、不同基础大模型及不同参数规模。结果表明，RetroLLM相比传统RAG和复杂RAG策略表现更优，验证了其在整合检索生成与提升证据生成质量方面的有效性。

### 💬 可借鉴之处
1. 架构设计层面：RetroLLM的统一框架思路为打破检索与生成的分离壁垒提供范例，启示后续工作思考如何更深度整合不同模块以降低部署成本与提升协同性。
2. 技术优化层面：分层FM - Index与前瞻性约束解码针对约束生成中的错误剪枝问题提出创新解法，为处理大规模语料下的生成约束与相关性保障提供了技术参考，可迁移到其他需约束生成且面临类似剪枝问题的场景。
3. 实验验证层面：在多数据集、多任务场景下的全面实验设计，为评估模型泛化性与鲁棒性提供了示范，后续研究可借鉴这种多维度的实验验证方式。
```

## stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization
### Abstract
Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our code
will be released on https://github.com/Zillwang/StepSearch.
```
### 🌟 论文解读 | StepSearch：用分步近端策略优化点燃大模型搜索能力

### 📌 背景痛点/本文动机
大语言模型（LLMs）在多跳推理任务中需要迭代获取高价值外部知识，但现有基于强化学习（RL）训练LLMs进行搜索式文档检索的工作，仅依赖全局信号带来的稀疏奖励，在复杂多跳问答任务中表现不佳。此前方法存在对中间查询和多步检索缺乏细粒度监督、多数多跳问答框架在查询轨迹建模上存在关键缺口等问题，为填补这些研究空白，本文提出StepSearch框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建通用多跳搜索数据  
基于MuSiQue数据集开发新颖的数据 pipeline，生成6万条经过筛选的子问题搜索关键词，这些关键词可在不同检索数据集上泛化。具体流程为利用GPT - 4o丰富分解后的MuSiQue问题，得到连贯的子问题 - 答案对并生成每一步的搜索查询；将增强后的步骤问题重新表述为搜索查询集；向多个来源发出查询并筛选出有效结果。

💡 创新点2：提出StepSearch分步强化学习框架  
在近端策略优化（PPO）基础上增加基于token级别的奖励（信息增益和冗余惩罚），用于查询构建和文档检索。将每一轮交互划分为思考→搜索→回答阶段，为每个token分配信息增益信号和冗余惩罚，引导模型将多跳查询分解为聚焦的搜索子任务，动态调整检索策略，更有效地整合外部证据。同时设计简约的提示模板，在训练时解耦参数更新与检索产物，聚焦模型内部推理和搜索策略参数学习。

### 📈 实验结果
在标准多跳QA基准测试中，StepSearch显著超越全局奖励基线。使用仅19k的训练数据，对于3B和7B规模的模型，相比各种基于RL的搜索基线分别实现了11.2%和4.2%的绝对性能提升；在不同多跳QA基准上，相比标准RL基线分别有5.7%、9.1%、10.0%和15.2%的绝对提升，证明了细粒度、分步监督在优化深度搜索LLMs方面的有效性。

### 💬 可借鉴之处
1. 数据构建方面：通过数据 pipeline 生成包含子问题级搜索轨迹的细粒度问答数据集，为模型训练提供更丰富的多跳推理相关数据支撑，这种从已有开源数据集拓展构建专用数据的思路值得借鉴。
2. 强化学习优化方面：引入分步的、token级别的奖励机制，关注中间步骤的信息增益与冗余问题，为解决复杂任务中强化学习奖励稀疏、监督不细的问题提供了新方向，可用于其他需要多步迭代、细粒度监督的LLMs应用场景优化。
3. 训练策略方面：在训练时对特定模块（如检索产物相关部分）进行梯度屏蔽，聚焦关键学习目标，这种解耦学习目标的训练技巧在融合外部工具或信息的LLMs训练中具有参考价值。
```

## toolrl--reward-is-all-tool-learning-needs
### Abstract
Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.
```
### 🌟 论文解读 | ToolRL：工具学习，奖励设计是关键

### 📌 背景痛点/本文动机
大语言模型（LLMs）常通过有监督微调（SFT）来获得工具使用能力，但SFT在陌生或复杂工具使用场景下泛化能力不足。强化学习（RL）虽在推理和泛化方面展现潜力，然而工具使用的奖励设计存在挑战：多工具调用参数多样，粗粒度奖励（如答案匹配）无法提供有效学习所需的细粒度反馈。因此，本文聚焦RL范式下工具选择与应用任务的奖励设计，探索如何通过合理奖励设计提升LLMs工具使用和泛化能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：首次系统研究RL范式下工具选择与应用的奖励设计  
全面探索奖励策略的类型、规模、粒度和时间动态等维度，分析不同奖励策略对工具学习的影响，为工具集成推理（TIR）任务的奖励设计提供系统认知。  

💡 创新点2：提出针对性奖励设计框架并结合GRPO训练LLMs  
基于对奖励策略的探索洞察，设计适用于工具使用任务的原则性奖励方案，并利用Group Relative Policy Optimization（GRPO）训练大语言模型，兼顾奖励设计与训练算法，提升工具使用性能。  

### 📈 实验结果
在多个基准测试中，该方法表现出色：相较于基础模型性能提升17%，较SFT模型提升15%；且训练鲁棒、可扩展且稳定，奖励曲线在训练中快速上升，展现良好学习效果；同时模型对未见过的场景和任务目标泛化能力强，还出现主动性、元认知推理等涌现行为。  

### 💬 可借鉴之处
1. 奖励设计维度的全面探索为后续LLM - agent训练提供参考，如认识到过长推理轨迹奖励未必好、动态奖励规模助力行为过渡、细粒度奖励分解提升学习稳定性等结论，可指导后续奖励机制设计。  
2. 首次将RL应用于通用TIR任务并给出奖励设计实证路线，为打造更强大自主的LLM智能体铺路，后续研究可基于此框架拓展工具学习和RL结合的方向。  
3. 代码开源便于复现与进一步研究，推动该领域快速发展，为科研社区提供了可复用的资源与研究基础。  
```

## deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments
### Abstract
Large Language Models (LLMs) equipped with web search capabilities have
demonstrated impressive potential for deep research tasks. However, current
approaches predominantly rely on either manually engineered prompts (prompt
engineering-based) with brittle performance or reinforcement learning within
controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that
fail to capture the complexities of real-world interaction. In this paper, we
introduce DeepResearcher, the first comprehensive framework for end-to-end
training of LLM-based deep research agents through scaling reinforcement
learning (RL) in real-world environments with authentic web search
interactions. Unlike RAG-based approaches that assume all necessary information
exists within a fixed corpus, our method trains agents to navigate the noisy,
unstructured, and dynamic nature of the open web. We implement a specialized
multi-agent architecture where browsing agents extract relevant information
from various webpage structures and overcoming significant technical
challenges. Extensive experiments on open-domain research tasks demonstrate
that DeepResearcher achieves substantial improvements of up to 28.9 points over
prompt engineering-based baselines and up to 7.2 points over RAG-based RL
agents. Our qualitative analysis reveals emergent cognitive behaviors from
end-to-end RL training, including the ability to formulate plans,
cross-validate information from multiple sources, engage in self-reflection to
redirect research, and maintain honesty when unable to find definitive answers.
Our results highlight that end-to-end training in real-world web environments
is not merely an implementation detail but a fundamental requirement for
developing robust research capabilities aligned with real-world applications.
We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.
```
### 🌟 论文解读 | DeepResearcher：在真实环境中用强化学习赋能深度研究

### 📌 背景痛点/本文动机
大语言模型（LLMs）结合网络搜索能力后，在深度研究任务中展现出了潜力，但现有方法存在明显不足：一类是依赖人工设计提示词的方式，性能脆弱不稳定；另一类是在受控的检索增强生成（RAG）环境内做强化学习，无法捕捉真实世界交互的复杂性。为了突破这些局限，论文提出要打造能在真实网络环境下，通过规模化强化学习来端到端训练基于LLM的深度研究智能体的框架，也就是DeepResearcher。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：真实环境下的端到端强化学习训练  
不同于RAG类方法假设所需信息都在固定语料库，DeepResearcher聚焦开放网络“嘈杂、无结构、动态”的特性，让智能体学习在真实网页交互里导航，实现对研究智能体的端到端训练，不再受限于受控环境的假设。  

💡 创新点2：专用多智能体架构  
搭建了特殊的多智能体架构，其中浏览智能体负责从各类网页结构里提取相关信息，克服了网页结构多样带来的技术挑战，让智能体在真实网页场景下能有效获取信息用于研究任务。  

### 📈 实验结果
在开放域研究任务的大量实验中，DeepResearcher对比基于提示词工程的基线方法，性能提升最高可达28.9个百分点；对比基于RAG的强化学习智能体，也能提升最高7.2个百分点。此外，定性分析还发现了端到端强化学习训练带来的“涌现认知行为”，比如制定计划、多源信息交叉验证、自我反思调整研究方向、找不到明确答案时保持诚实等。  

### 💬 可借鉴之处
从研究价值来看，论文证明了真实网络环境下的端到端训练不只是“实现细节”，更是打造贴合真实应用的鲁棒研究能力的根本要求，这为后续研究智能体的开发指明了方向——要重视真实场景交互训练。从工程实践角度，其开源的DeepResearcher框架（https://github.com/GAIR-NLP/DeepResearcher）能为想探索“强化学习+真实网页交互”做深度研究的团队或个人提供参考，多智能体处理网页信息的思路也值得在复杂信息抽取类任务中借鉴。而且对“涌现行为”的分析，也启发研究者关注训练过程中智能体能力的自然成长与展现~
```

## an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents
### Abstract
Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.
```
### 🌟 论文解读 | 强化学习驱动推理-搜索交织型大模型智能体的实证研究

### 📌 背景痛点/本文动机
大语言模型（LLMs）在自然语言处理诸多任务中表现卓越，但在需与外部环境交互、借助工具的场景（如搜索任务）中存在局限。强化学习（RL）为训练LLM成为能交织推理与搜索的智能体提供了潜力，然而这类智能体的最优设计仍不明晰，如奖励设计、基础LLM选择与特性、搜索引擎在RL过程中角色等关键因素待深入探究。此前方法在训练搜索智能体时，基于提示或有监督微调存在难扩展等问题，而RL虽有前景但相关关键问题研究不足，本文旨在通过全面实证研究填补这些空白。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：系统探究奖励设计对搜索智能体训练的影响 
区分格式奖励（反映对智能体动作格式的遵循）与中间检索奖励（迭代激励与结果相关的检索），实证分析不同奖励在训练中作用，明确格式奖励对性能提升更有效，中间检索奖励效用有限。

💡 创新点2：深入分析基础LLM特性的影响 
对比通用型与推理专用型LLM，以及不同规模模型在RL训练中的表现，揭示模型规模、初始化类型（通用或推理专用）对RL训练结果的显著影响，为选择合适基础模型提供依据。

💡 创新点3：剖析搜索引擎选择的作用 
研究训练时不同质量搜索引擎（从随机噪声到强密集检索器）如何塑造RL训练动态，以及推理时检索系统变化对智能体鲁棒性影响，明确搜索引擎选择在训练动态与推理鲁棒性上的关键作用。

### 📈 实验结果
1. 奖励设计方面：加入格式奖励能显著提升性能，尤其从基础LLM开始训练时；中间检索奖励未带来持续性能提升，实用价值有限。
2. 基础LLM层面：通用型LLM在RL场景中表现优于推理专用型，可能因后者训练初期指令遵循能力较弱；扩大模型规模通常提升最终性能，但收益递减。
3. 搜索引擎选择维度：训练时搜索引擎质量强烈影响RL动态，用无信息引擎（如随机噪声）会让智能体完全回避检索，弱引擎（如BM25）导致频繁但低效搜索调用，强引擎（如密集检索器）学习更稳定；推理时搜索智能体对不同检索系统普遍鲁棒，更强搜索引擎持续带来更好下游性能。

### 💬 可借鉴之处
1. 奖励设计：在构建RL驱动的LLM搜索智能体时，优先考虑格式奖励来提升性能，可谨慎评估中间检索奖励投入。
2. 模型选择：根据应用场景权衡通用型与推理专用型LLM，若追求RL训练效果初期通用型更具优势，同时合理考虑模型规模与性能收益平衡。
3. 搜索引擎集成：训练阶段选择高质量搜索引擎保障学习稳定性，推理阶段可基于场景灵活更换检索系统，且利用强搜索引擎提升下游表现，为实际部署搜索智能体提供了清晰的选型与优化方向。 
```

## r1-searcher++--incentivizing-the-dynamic-knowledge-acquisition-of-llms-via-reinforcement-learning
### Abstract
Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.
```
### 🌟 论文解读 | R1-Searcher++：用强化学习驱动大模型动态获取知识

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽具备强大能力，但依赖静态内部知识易产生幻觉，在开放性任务中表现受限。检索增强生成（RAG）通过注入外部信息缓解此问题，然而现有方法存在成本高、泛化差或忽视模型内部知识等缺陷。同时，基于监督微调（SFT）的蒸馏易让模型记忆固定路径，限制泛化；测试时扩展方法（如MCTS）推理开销大；基于结果的强化学习（RL）训练虽能让模型自主探索外部检索，但易过度依赖外部引擎而忽略内部知识。人类解决问题时会先调用内部知识、缺信息再搜且会记忆新知识，大模型经大规模预训练已有丰富内部知识，因此需让模型动态切换内外知识源并记忆有用信息，R1 - Searcher++应运而生。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：两阶段训练策略
采用SFT Cold - start和RL for Dynamic Knowledge Acquisition两阶段。第一阶段用拒绝采样收集符合格式要求数据，通过SFT让模型完成初步格式学习；第二阶段基于结果的强化学习引导模型动态获取知识，依据精心设计的奖励机制，让模型在有信心时依赖内部知识、不确定时调用外部搜索机制。
💡 创新点2：激励内部知识利用与记忆机制
设计奖励机制鼓励模型利用内部知识，同时引入记忆机制，将检索到的内容转换并记忆，使模型在训练中保留遇到的知识，持续丰富内部知识，通过自主探索和及时记忆平衡内部推理与外部检索。

### 📈 实验结果
基于Qwen - 2.5 - 7B - Instruct开展大量实验，R1 - Searcher++相比强基线最多提升4.3%；与基于原始RL的方法相比，检索次数减少42.9%，证明其在性能和检索效率上的优势，且优于现有RAG方法。

### 💬 可借鉴之处
从方法设计看，两阶段训练策略为模型分阶段学习（先格式后动态知识获取）提供了思路，可借鉴于需分步骤提升能力的模型训练任务；奖励与记忆机制结合，为平衡模型内外知识利用、持续进化提供了范式，在需结合内外资源且要知识积累的场景（如智能问答系统持续优化）有参考价值；从实验验证角度，基于特定模型（Qwen - 2.5 - 7B - Instruct）的详细对比实验，展示了新方法在性能和效率上的提升，为后续类似方法的实验设计提供了对照范例，让研究者明白如何通过对比凸显方法优势。
```

## removal-of-hallucination-on-hallucination--debate-augmented-rag
### Abstract
Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.
```
### 🌟 论文解读 | 消除“幻觉上的幻觉”：辩论增强的RAG框架DRAG

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽具备强大的自然语言理解与推理能力，但存在“幻觉”问题，即生成内容偏离事实正确性，在知识密集型任务中可靠性受限。为缓解幻觉，检索增强生成（RAG）框架应运而生，它通过整合外部知识检索来提升输出的事实准确性。然而，RAG也带来新挑战：有偏差或错误的检索结果会误导生成过程，使幻觉问题加剧，这种现象被作者们称为“幻觉上的幻觉（Hallucination on Hallucination）”。现有方法未系统优化RAG全流程，且依赖单智能体决策易受固有偏差影响。而多智能体辩论（MAD）在增强LLM鲁棒性、事实准确性等方面展现潜力，因此本文受此启发，提出辩论增强的RAG框架（DRAG）来应对RAG面临的挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出“幻觉上的幻觉”新视角  
首次明确指出RAG中存在的关键问题——当检索提供不完整、有偏差或误导性信息时，LLM固有的幻觉会被进一步放大，即“幻觉上的幻觉”，为后续解决RAG全流程问题锚定了新视角。  

💡 创新点2：提出DRAG训练无关框架，融合多智能体辩论到检索与生成阶段  
在**检索阶段**，DRAG引入支持者（proponents）、反对者（opponents）和裁判（judges）进行结构化辩论：支持者倡导检索策略，反对者质疑查询充分性，裁判评估完整性，以此维护动态查询池，确保知识覆盖更全面、减少事实偏差，提升检索质量与可靠性。  
在**生成阶段**，DRAG在智能体间建立非对称信息角色，减少LLM对检索内容的过度依赖，同时推动结构化对抗辩论，增强推理鲁棒性、缓解事实不一致问题。整个DRAG无需额外训练，借助多智能体辩论机制优化RAG全流程。  

### 📈 实验结果
文中在多个任务上对DRAG进行评估，结果表明：DRAG提升了检索可靠性，减少了RAG引发的幻觉问题，同时显著增强了整体事实准确性，验证了该框架在优化RAG流程、解决“幻觉上的幻觉”方面的有效性。  

### 💬 可借鉴之处
1. 问题定义层面：提出“幻觉上的幻觉”精准刻画RAG新问题，为领域内后续研究锚定新的问题视角与方向，启发研究者关注RAG全流程中检索与生成的联动性问题。  
2. 方法创新层面：将多智能体辩论机制创新性融入RAG的检索和生成双阶段，且无需训练，为提升RAG性能提供了轻量且有效的框架思路，后续在构建多智能体协作系统优化生成或检索任务时，可借鉴这种角色分工、辩论交互的模式。  
3. 实践价值层面：开源代码为行业提供了可复现、可拓展的基础，便于研究者或开发者在此基础上探索多智能体与RAG结合的更多可能性，推动技术落地与迭代。  
```

## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### 🌟 论文解读 | s3：小数据量下用强化学习训练搜索智能体的新范式

### 📌 背景痛点/本文动机
在检索增强生成（RAG）系统中，大语言模型（LLMs）能借助外部知识提升推理能力，而近期用强化学习（RL）让LLM扮演搜索智能体的进展，可通过多轮交互优化信息获取。但现有方法存在缺陷：要么用仅关注搜索的指标（如NDCG）优化检索，忽略下游效用；要么微调整个LLM来联合推理与检索，把检索和生成耦合，限制了实际搜索效用且难适配冻结或闭源模型。因此，需要一种解耦搜索与生成、轻量且模型无关的框架，在小数据量下提升下游性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出s3框架，解耦搜索器与生成器  
s3是轻量、模型无关的框架，将搜索器（Searcher）从生成器（Generator）中解耦。训练时固定生成器（可复用任意冻结的LLM），仅专注于训练搜索器，让搜索优化聚焦在对下游生成质量的提升上，避免了生成与检索的耦合问题，也能适配闭源或冻结的LLM。  

💡 创新点2：定义“Gain Beyond RAG（GBR）”奖励信号  
GBR作为全新的奖励指标，衡量了s3检索到的文档让生成器表现，相比“朴素RAG（naïve RAG）检索”在生成准确率上的提升幅度。通过该奖励训练搜索器，能直接针对下游生成效用优化检索组件，摆脱仅关注搜索指标或易过拟合的精确匹配（EM）类指标的局限，让检索优化更贴合实际生成需求。  

### 📈 实验结果
在6个通用QA基准和5个医疗QA基准测试中，s3展现出强大性能：仅用2.4k训练样本，就超越了使用超70倍数据（如70k甚至更多）训练的基线方法（像DeepRetrieval、Search - R1等）。在通用和医疗领域的平均得分上，s3对比其他经典RAG、Active RAG、RL - Zero阶段的方法，都实现了更优的下游生成表现，验证了小数据量下高效训练搜索智能体的能力。  

### 💬 可借鉴之处
1. 模块化设计思路：将复杂的“检索 + 生成”任务解耦为搜索器和生成器独立优化，为构建更灵活的RAG系统提供了模块化思路，后续可针对不同模块分别迭代升级。  
2. 奖励信号设计：GBR奖励将下游生成效用作为检索优化的核心依据，跳出了传统仅看搜索环节指标的思维定式，为强化学习在检索增强场景下的奖励函数设计提供了新范式，可启发其他需要跨环节优化任务的奖励设计。  
3. 小数据高效训练：证明了在少量优质样本下也能训练出高性能搜索智能体，降低了大规模数据标注与训练成本，对资源有限但需构建RAG系统的场景（如垂直领域小团队）有很强的借鉴意义。

## vrag-rl--empower-vision-perception-based-rag-for-visually-rich-information-understanding-via-iterative-reasoning-with-reinforcement-learning
### Abstract
Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
https://github.com/Alibaba-NLP/VRAG.
```
### 🌟 论文解读 | VRAG-RL：用强化学习赋能视觉感知RAG，突破富视觉信息理解瓶颈

### 📌 背景痛点/本文动机
在信息理解领域，传统基于文本的检索增强生成（RAG）方法难以处理视觉相关信息；而当前基于视觉的RAG方法又受限于固定流程，且常因未能充分激活模型基础能力，导致有效推理能力不足。同时，强化学习（RL）已被证明对模型推理有帮助，但在多模态RAG场景下还存在两大关键局限：一是现有多模态RAG往往仅把图像塞进上下文，推理token分配不足且忽视视觉专属感知；二是模型与搜索引擎交互时，因表述不清难以精准检索信息，性能欠佳。在此背景下，论文提出VRAG - RL框架，旨在解决富视觉信息下的复杂推理难题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：设计视觉感知动作空间  
为富视觉输入量身定制动作空间，包含裁剪、缩放等动作。借助视觉感知token，让视觉语言模型（VLMs）能从粗到细地收集信息。比如处理文档里的图像或图表时，模型可通过感知token更关注信息密集区域，在有限上下文长度内激活推理能力，避免遗漏细节。

💡 创新点2：构建全面奖励结构  
为弥合用户原始查询与检索器间的差距，采用整合查询重写、检索性能与基于模型奖励的简单有效奖励。将检索过程有效性纳入奖励结构，在模型与搜索引擎交互时，及时检索相关图像能助力模型有效答题，反之则添噪干扰推理，以此为检索增强生成框架提供全面指导。

💡 创新点3：打造强化学习驱动的迭代推理框架  
受“思考再回答”和ReAct范式启发，把VLMs与搜索引擎交互及视觉感知动作空间建模为迭代推理和工具调用过程。支持自动采样并整合GRPO算法，精心设计采样策略（含每次交互后处理），用基于模型的奖励和检索奖励引导模型训练，保障多轮采样与训练的稳定性。还重新标注富视觉文档数据集、构建数据 pipeline 来为RL和SFT高效扩容数据。

### 📈 实验结果
在多样且具挑战性的基准测试中，VRAG - RL展现出强大性能。基于Qwen2.5 - VL - 7B模型时，相较现有方法提升超20%；基于Qwen2.5 - VL - 3B模型时，提升超30%，有力证明了方法的有效性。

### 💬 可借鉴之处
1. 模态感知动作空间设计：在多模态场景下，针对特定模态（如视觉）设计专属动作空间，引导模型聚焦关键信息区域，为提升多模态理解精细度提供思路。
2. 奖励机制融合实践：将过程指标（如检索性能）与结果指标（模型输出结果）融合进奖励，让模型训练更贴合真实应用场景，这种奖励设计思路可迁移到其他需多环节协作的任务。
3. 强化学习与多模态RAG结合：借助强化学习优化多模态模型在RAG任务的推理与交互能力，为多模态大模型在复杂信息处理场景的优化提供了新范式参考。
```

## airrag--activating-intrinsic-reasoning-for-retrieval-augmented-generation-using-tree-based-search
### Abstract
Leveraging the autonomous decision-making capabilities of large language
models (LLMs) has demonstrated superior performance in reasoning tasks.
However, despite the success of iterative or recursive retrieval-augmented
generation (RAG) techniques, these methods are often constrained to a single
solution space when confronted with complex problems. In this paper, we propose
a novel thinking pattern in RAG that integrates system analysis with efficient
reasoning actions, significantly activating intrinsic reasoning capabilities
and expanding the solution space of specific tasks via Monte Carlo Tree Search
(MCTS), which we refer to as AirRAG. Specifically, our approach designs five
fundamental reasoning actions, which are expanded to a broad tree-based
reasoning space using MCTS. The approach also incorporates self-consistency
verification to explore potential reasoning paths and inference scaling law.
Additionally, computationally optimal strategies are employed to allocate more
inference resources to key actions, thereby enhancing overall performance.
Experimental results demonstrate the effectiveness of AirRAG, showing
significant performance gains on complex question-answering datasets.
Furthermore, AirRAG is flexible and lightweight, making it easy to integrate
with other advanced technologies.
```
### 🌟 论文解读 | AirRAG：基于树搜索激活检索增强生成的内在推理能力

### 📌 背景痛点/本文动机
检索增强生成（RAG）在解决大语言模型（LLM）生成事实性错误内容问题上展现出潜力，尤其在特定领域或知识密集型任务中表现突出。但随着任务复杂度提升，现有方法面临诸多挑战：单一查询难以获取足够知识、难以理解问题内在复杂推理逻辑；此前针对复杂查询场景的研究多依赖人工规则与提示工程，灵活性不足，且递归检索等方法在复杂任务下易受限于单一解决方案空间，无法充分激活LLM决策能力，链式推理过程也易陷入狭窄解空间，小模型自探索时更难引导。因此，需设计合理推理动作以全面探索广阔且深入的解空间，提升RAG性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：设计五种基础推理动作  
提出系统分析、直接回答、检索回答、查询转换、总结回答这五种基础推理动作，能有效应对RAG场景下多种问题类型，涵盖渐进式或并行查询等场景，且在相对较小语言模型上也能高效执行，保障推理过程可控性。  

💡 创新点2：引入MCTS与自一致性扩展解空间  
借助蒙特卡洛树搜索（MCTS）将五种基础推理动作扩展为基于树的广阔推理空间，结合自一致性验证实现可控推理路径生成与高效推理缩放；同时结合投票机制与过程监督奖励模型从多条推理路径选答案，通过全面的推理缩放与可插拔架构提升泛化性与性能。  

### 📈 实验结果
实验表明AirRAG在复杂问答数据集上表现出色，相比现有迭代或递归RAG方法性能显著提升；且随着推理计算量增加（如输出序列数、token总量变化），能借助生成多样性与自一致性探索潜在解空间，整体性能得到明显增强（如图1展示不同输出序列数下多数据集平均性能对比，体现推理过程中token消耗与性能指标的变化趋势）。  

### 💬 可借鉴之处
1. 推理动作设计思路：为特定任务场景定制基础推理动作，覆盖多类问题，可启发后续针对不同领域任务设计针对性推理原语，提升任务处理全面性。  
2. 树搜索与自一致性结合：将MCTS引入RAG扩展解空间，搭配自一致性验证优化推理路径，为解决复杂任务下解空间受限问题提供了结合经典搜索算法与大模型特性的新思路。  
3. 灵活轻量架构：AirRAG架构灵活，易于整合其他先进技术作为额外动作分支，这种模块化、可扩展设计理念便于后续技术迭代与多方法融合，降低与其他技术结合门槛。  
```

## atomr--atomic-operator-empowered-large-language-models-for-heterogeneous-knowledge-reasoning
### Abstract
Despite the outstanding capabilities of large language models (LLMs),
knowledge-intensive reasoning still remains a challenging task due to LLMs'
limitations in compositional reasoning and the hallucination problem. A
prevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented
generation (RAG), which first formulates a reasoning plan by decomposing
complex questions into simpler sub-questions, and then applies iterative RAG at
each sub-question. However, prior works exhibit two crucial problems:
inadequate reasoning planning and poor incorporation of heterogeneous
knowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct
accurate heterogeneous knowledge reasoning at the atomic level. Inspired by how
knowledge graph query languages model compositional reasoning through combining
predefined operations, we propose three atomic knowledge operators, a unified
set of operators for LLMs to retrieve and manipulate knowledge from
heterogeneous sources. First, in the reasoning planning stage, AtomR decomposes
a complex question into a reasoning tree where each leaf node corresponds to an
atomic knowledge operator, achieving question decomposition that is highly
fine-grained and orthogonal. Subsequently, in the reasoning execution stage,
AtomR executes each atomic knowledge operator, which flexibly selects,
retrieves, and operates atomic level knowledge from heterogeneous sources. We
also introduce BlendQA, a challenging benchmark specially tailored for
heterogeneous knowledge reasoning. Experiments on three single-source and two
multi-source datasets show that AtomR outperforms state-of-the-art baselines by
a large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on
BlendQA. We release our code and datasets.
```
### 🌟 论文解读 | AtomR：原子算子赋能大模型，突破异构知识推理困境

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽能力出众，但在知识密集型推理任务中，受限于组合推理能力不足与幻觉问题，表现仍有欠缺。现有结合思维链（CoT）与检索增强生成（RAG）的方法，虽先分解复杂问题为子问题再迭代检索，但存在两大关键缺陷：一是推理规划不够精细，子问题分解缺乏原子性与语义功能约束，易生成粗粒度或功能不准确的子问题；二是对异构知识源（如网页、文本语料、知识图谱）的整合能力弱，多数仅固定单源检索，少数支持多源也难在步骤级跨源检索与有效知识操作；此外，异构知识源上的优质基准数据集也很匮乏。为解决这些问题，论文提出AtomR框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出原子知识算子  
受知识图谱查询语言通过组合预定义操作建模组合推理的启发，提炼出**Search、Relate、Filter**这三种原子知识算子。它们具备不可分割性与正交性，功能无重叠，能覆盖知识密集型推理的复杂流程，且统一应用于文本语料（Text）、在线网页（Web）、结构化知识图谱（KG）三类异构知识源，让大模型能动态操作异构源的原子级知识。  

💡 创新点2：原子推理规划与执行双阶段流程  
- **原子推理规划（Atomic Reasoning Planning）**：将复杂问题分解为**原子推理树（ART）**，树的每个叶节点对应一种原子知识算子。这迫使大模型把问题分解到原子粒度，实现细粒度且功能正交的推理规划，解决推理规划不足问题。  
- **原子推理执行（Atomic Reasoning Execution）**：自底向上递归执行推理树。叶节点执行时，动态从多异构源选、检索和操作知识，解决异构知识整合难题；非叶节点则通过“子答案推理（Child Answer Reasoning）”或“直接RAG推理（Direct RAG Reasoning）”生成输出，前者无需额外检索，后者仅在前者失败时触发，兼顾鲁棒性与成本效率。  

💡 创新点3：构建BlendQA基准数据集  
针对异构知识源优质数据集缺失问题，构建了跨Text、Web、KG三源的**BlendQA**基准，为异构知识推理研究提供更具挑战性的测试平台。  


### 📈 实验结果
在三个单源数据集和两个多源数据集（含自研BlendQA）上开展实验，AtomR大幅超越现有SOTA基线。其中，在2WikiMultihop数据集上F1分数提升9.4%，在BlendQA数据集上F1分数提升9.5%，充分验证了框架在异构知识推理任务中的有效性。  


### 💬 可借鉴之处
1. **原子化思路**：将复杂推理拆解为原子级操作，为大模型处理知识密集型任务提供了更精细的“操作单元”参考，可启发后续推理框架设计中对任务分解粒度的思考。  
2. **异构知识整合**：统一算子适配多类异构知识源，为跨源知识检索与操作提供了通用范式，有助于推动多源知识融合类任务的研究。  
3. **基准数据集构建**：针对领域痛点构建BlendQA，为学界提供了更贴合真实异构知识推理场景的测试基准，彰显了填补数据空白对领域发展的推动价值。  
4. **分层执行策略**：推理执行阶段的分层（叶节点与非叶节点不同执行逻辑）设计，平衡了鲁棒性与成本，这种“按需检索”思路在资源受限场景下的大模型应用中值得借鉴。  
```

## simpledeepsearcher--deep-information-seeking-via-web-powered-reasoning-trajectory-synthesis
### Abstract
Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.
```
### 🌟 论文解读 | SimpleDeepSearcher：用网页驱动的推理轨迹合成实现深度信息检索

### 📌 背景痛点/本文动机
近年来，检索增强生成（RAG）方法通过引入外部知识检索机制极大增强了大语言模型（LLMs）能力，在复杂深度搜索场景（需多步推理与迭代信息检索）中也有进展。但现有方法存在关键局限：一方面，基于强化学习（RL）的方法多在人工环境（静态文档库）运行，与真实网络动态存在分布不匹配问题，且和实时搜索API交互时计算成本极高；另一方面，有监督微调（SFT）虽训练流程简洁，但深度搜索场景下缺乏高质量训练数据——输入侧现有QA数据集缺少网页上问题的多样性与复杂性，输出侧传统答案标注遗漏了搜索集成推理策略所需的关键推理痕迹（如搜索操作、证据合成、高效决策路径等）。因此，本文希望通过策略性的数据工程而非复杂训练范式，搭建轻量且有效的框架SimpleDeepSearcher来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于真实网页搜索环境的数据合成框架  
搭建了以真实网络搜索环境为基础的数据合成框架，模拟真实用户搜索行为来生成多轮推理轨迹。通过“推理 - 搜索 - 总结 - 生成”的迭代循环，利用商业搜索API直接处理原始HTML内容，捕捉从结构化数据片段到非结构化叙事话语等多样的网络信息特征，让模型能接触到真实的搜索产物与噪声模式。  

💡 创新点2：多准则的数据精选策略  
设计了多准则的数据精选策略，从输入问题选择和输出响应过滤两方面协同优化。输入侧，用领域异质性、关键词多样性、知识单元复杂度来过滤查询，构建信息丰富的训练基础且确保贴合真实网页搜索场景；输出侧，对大语言模型合成的响应施加四维质量过滤（格式标准化、推理路径控制、问题难度、搜索有效性），保障响应质量。  

### 📈 实验结果
在五个不同领域的基准测试中，仅用871个精心挑选的样本进行有监督微调（SFT），SimpleDeepSearcher就展现出显著性能提升：相比基于提示的方法，性能提升了48.3%；相比基于强化学习的RAG方法，性能提升了24.9%。这表明该框架在性能和效率间实现了有效平衡，能以简单却有力的方式增强深度搜索能力，且框架扩展性强，还可与其他类型训练数据结合，也适用于基于RL的训练。  

### 💬 可借鉴之处
1. 数据工程思路：当复杂训练范式（如RL）面临成本、环境匹配等问题时，可从数据层面入手，通过模拟真实场景合成数据、多维度精选数据来为模型训练提供优质“养料”，为解决数据稀缺瓶颈提供了思路。  
2. 模块化设计优势：框架的模块化设计能让模型接触真实搜索 artifact 和噪声，还能以极小的SFT数据集实现优异性能，摆脱对资源密集型RL训练的依赖，这种解耦数据合成与模型约束的架构值得在类似需和外部环境交互的任务系统设计中参考。  
3. 小样本SFT价值验证：验证了在深度搜索任务中，高质量小样本SFT也能超越强基线（尤其是RL基线），为后续在资源有限场景下探索高效训练方式提供了实践依据。  
```

## rescore--label-free-iterative-retriever-training-for-multi-hop-question-answering-with-relevance-consistency-supervision
### Abstract
Multi-hop question answering (MHQA) involves reasoning across multiple
documents to answer complex questions. Dense retrievers typically outperform
sparse methods like BM25 by leveraging semantic embeddings; however, they
require labeled query-document pairs for fine-tuning. This poses a significant
challenge in MHQA due to the high variability of queries (reformulated)
questions throughout the reasoning steps. To overcome this limitation, we
introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a
novel method for training dense retrievers for MHQA without labeled documents.
ReSCORE leverages large language models to capture each documents relevance to
the question and consistency with the correct answer and use them to train a
retriever within an iterative question-answering framework. Experiments on
three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with
significant improvements in retrieval, and in turn, the state-of-the-art MHQA
performance. Our implementation is available at:
https://leeds1219.github.io/ReSCORE.
```
### 🌟 论文解读 | ReSCORE：无标注文档下多跳问答的迭代检索器训练新范式

### 📌 背景痛点/本文动机
多跳问答（MHQA）需要跨多个文档推理来回答复杂问题，当前主流的密集检索器虽比BM25等稀疏方法更优，但需标注的查询 - 文档对来微调。而MHQA中查询（迭代中的改写问题）变异性大，标注成本高、难度大。同时，现有多跳问答系统常依赖稀疏检索器，密集检索器因领域适配需标注数据受限；且现有针对检索器训练的方法多聚焦单跳、忽视迭代推理与多跳场景，迭代RAG工作也未重视检索器训练。因此，需一种无需标注文档就能训练多跳问答密集检索器的方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ReSCORE方法
ReSCORE是一种无需标注文档训练MHQA密集检索器的新方法。其核心直觉是文档对回答问题的重要性与大语言模型（LLM）在给定文档时生成问题和正确答案的概率成正比，以此联合建模文档与答案的一致性和与问题的相关性，并将该概率作为伪标注（pseudo - GT）在迭代RAG框架中训练检索器。通过LLM捕捉文档对问题的相关性和与正确答案的一致性，为无标注训练提供监督信号。

💡 创新点2：构建IQATR系统
提出Iterative Question Answerer with Trained Retriever（IQATR）多跳问答系统，其检索器由ReSCORE训练而来。该系统在迭代RAG框架下运作，通过训练后的检索器迭代检索文档并生成答案，直至得到最终答案，将ReSCORE训练的检索器融入实际多跳问答系统，验证方法有效性。

💡 创新点3：深入分析伪标注与查询改写
对不同伪 - GT标签和查询改写方法的效果进行了深入分析，探究不同因素对检索器训练和多跳问答性能的影响，为后续相关研究提供了分析思路与参考依据。

### 📈 实验结果
在三个流行的MHQA基准数据集MuSiQue、2WikiMHQA和HotpotQA上进行实验。结果表明，ReSCORE结合一致性和相关性为无标注文档训练MHQA密集检索器提供了有效监督。经ReSCORE训练的检索器不仅提升了检索质量，当集成到IQATR迭代RAG框架中时，还在多跳问答任务上取得了当前最优（SOTA）性能，证明了方法在检索和问答整体流程中的有效性。

### 💬 可借鉴之处
1. 无监督/弱监督训练思路：在数据标注昂贵或困难的任务中，可借鉴ReSCORE利用大语言模型生成伪标注来训练模型的思路，减少对人工标注的依赖。
2. 迭代框架下的任务适配：针对多步骤、迭代性的任务（如多跳推理、多轮对话等），可参考ReSCORE在迭代RAG框架内训练检索器以适配任务特性的方式，让模型更好地处理任务中的动态变化。
3. 多因素分析方法：文中对伪标注和查询改写等因素的深入分析，为后续研究中探究不同组件对系统性能的影响提供了范例，有助于更全面地理解模型工作机制与优化方向。
```

## retrieve--summarize--plan--advancing-multi-hop-question-answering-with-an-iterative-approach
### Abstract
Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.
```
### 🌟 论文解读 | 迭代式RAG新范式ReSP：解决多跳问答难题的高效路径

### 📌 背景痛点/本文动机
多跳问答是开放域问答中极具挑战性且有重要产业价值的子任务，需系统整合信息完成多步推理。基于大语言模型（LLMs）的检索增强生成（RAG）是主流解法，但传统单轮RAG处理多跳问答时，单次检索难集齐所有关键信息。迭代式RAG虽通过多轮检索+子问题规划提升了性能，却面临两大核心挑战：一是多轮检索导致上下文过载，引入更多噪声且易让模型遗漏关键信息；二是缺乏检索轨迹记录，引发“过度规划”（信息够了还不停迭代）和“重复规划”（重复生成已检索子问题）问题。因此，如何高效处理多轮检索信息、优化迭代过程，成为迭代式RAG突破的关键。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出迭代式RAG方法ReSP，引入双功能 summarizer  
ReSP在迭代RAG框架中集成基于LLM的 summarizer，该 summarizer 承担双重任务：一方面为顶层目标问题汇总佐证信息，形成“全局证据记忆（global evidence memory）”；另一方面针对当前子问题，从检索文档生成回应，构建“局部路径记忆（local pathway memory）”。通过同时压缩面向顶层问题和当前子问题的信息，既缓解多轮检索的上下文过载，又为后续迭代提供清晰的信息轨迹参考。  

💡 创新点2：优化迭代逻辑，避免重复与过度规划  
每次迭代开始时，模型会结合积累的全局证据记忆和局部路径记忆来评估信息是否充足。若充足则生成最终答案；若不足则生成新子问题，且确保不重复生成已检索过的子问题。这种设计让迭代过程更“聪明”，既解决了因无轨迹记录导致的重复规划，也避免了信息足够却不停迭代的过度规划问题。  

### 📈 实验结果
在多跳问答基准数据集HotpotQA和2WikiMultihopQA上，ReSP展现出显著优势：  
- HotpotQA 上，F1 分数较当前最优方法提升 4.1；  
- 2WikiMultihopQA 上，F1 分数较当前最优提升 4.4；  
此外，对比研究验证了 ReSP 对上下文长度变化具备出色的鲁棒性，在不同上下文长度下表现稳定，远超其他RAG方法。  

### 💬 可借鉴之处
1. 功能分解式的信息压缩思路：将 summarizer 功能拆分为全局和局部维度，为处理多轮检索信息提供了分层压缩的新范式，可启发后续长文本、多轮交互场景下的信息处理方案；  
2. 迭代过程的轨迹化管理：通过记录“全局+局部”记忆来引导迭代决策，有效解决重复与过度规划，这种“记忆+约束”的迭代控制逻辑，对需多步推理的任务（如复杂问答、任务规划）有参考价值；  
3. 产业场景落地潜力：多跳问答在智能助手、生成式搜索引擎等领域需求强烈，ReSP 提升性能与鲁棒性的同时，为工业级多轮推理应用提供了更可靠的技术路径。  
```

## rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation
### Abstract
Retrieval-augmented generation (RAG) has shown great promise for
knowledge-intensive tasks and recently advanced with agentic RAG, where
language agents engage in multi-round interactions with external knowledge
sources for adaptive information retrieval. However, existing agentic RAG
methods often depend on ad-hoc prompt engineering and lack a unified
optimization framework. We introduce RAG-Gym, a comprehensive platform that
systematically explores three optimization dimensions: (1) prompt engineering,
(2) actor tuning, and (3) critic training. For prompt engineering, we propose
Re$^2$Search, a novel agent incorporating reasoning reflection that
significantly outperforms standard prompts. In actor tuning, we evaluate three
popular post-training algorithms with fine-grained process supervision and
identify direct preference optimization as the most effective. We further
demonstrate that a trained critic can enhance inference by selecting
higher-quality intermediate reasoning steps. Together, these findings lead to
the optimized Re$^2$Search++ agent, which surpasses most recent methods like
Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we
examine the impact of different reward sources and analyze scaling properties
in training and inference, offering practical insights for agentic RAG
optimization. The project homepage is available at https://rag-gym.github.io.
```
### 🌟 论文解读 | RAG-Gym：为检索增强生成打造语言智能体的系统优化平台

### 📌 背景痛点/本文动机
大语言模型（LLMs）在面对知识密集型问题时，若缺乏足够或最新的领域知识，容易给出不准确回答甚至产生幻觉。检索增强生成（RAG）通过结合信息检索（IR）系统的相关信息来改善这一情况，而智能体化的RAG（agentic RAG）让语言智能体与外部知识源多轮交互以实现自适应信息检索，进一步提升了效果。但现有agentic RAG方法存在依赖临时prompt工程、缺乏统一优化框架的问题；同时，虽有LLM后训练算法，却难直接适配agentic RAG动态调整token生成策略的需求，且对中间步骤的细粒度监督不足。因此，本文旨在构建系统框架来优化agentic RAG。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RAG-Gym综合平台  
RAG-Gym从prompt工程、actor调优、critic训练三个维度系统探索agentic RAG的优化。将知识密集型问答建模为高层马尔可夫决策过程（MDP），明确中间动作，为语言智能体优化提供模块化方法，涵盖了智能体与IR系统交互的状态、动作、环境和奖励等要素定义，支撑细粒度过程监督与优化方法的系统评估。

💡 创新点2：Prompt工程层面提出Re²Search智能体  
设计了融入推理反思（reasoning reflection）的Re²Search智能体，相比标准prompt在性能上有显著超越，让智能体在与IR系统交互过程中能更智能地生成查询等动作来辅助回答问题。

💡 创新点3：Actor调优与Critic训练的探索  
在actor调优中，用细粒度过程监督评估三种流行后训练算法，发现直接偏好优化（Direct Preference Optimization）效果最佳；同时证明训练critic来选择更高质量中间推理步骤能提升推理效果。整合这些成果得到优化后的Re²Search++智能体。

### 📈 实验结果
优化后的Re²Search++智能体在具有挑战性的知识密集型任务上超越现有方法，平均F1相对提升3.2% - 11.6%，在未见过的数据集上甚至有8.5% - 24.7%的提升；此外还分析了不同奖励源影响以及训练和推理时的缩放特性等，验证了各优化维度和方法的有效性。

### 💬 可借鉴之处
1. 多维度系统优化思路：从prompt、actor、critic三个维度系统优化agentic RAG，为复杂智能体系统优化提供了“分模块+协同”的范例，可启发其他需多环节协作的AI系统优化。  
2. 细粒度监督价值：强调对agentic RAG中间步骤的细粒度过程监督，证明其对性能提升的重要性，在后续类似需动态交互、多步推理的任务型智能体研发中，可重视中间过程的监督设计。  
3. 算法选型与融合：对不同后训练算法在agentic RAG场景下的测试与优选（如发现直接偏好优化更优），以及critic训练辅助推理的思路，为模型训练策略选择和模块配合提供了实践参考。 
4. 实践指导意义：对奖励源、训练与推理缩放特性的分析，给后续agentic RAG优化提供了可落地的实用洞见，便于研究者和工程师在实际项目中权衡决策。
```

## flashrag--a-modular-toolkit-for-efficient-retrieval-augmented-generation-research
### Abstract
With the advent of large language models (LLMs) and multimodal large language
models (MLLMs), the potential of retrieval-augmented generation (RAG) has
attracted considerable research attention. Various novel algorithms and models
have been introduced to enhance different aspects of RAG systems. However, the
absence of a standardized framework for implementation, coupled with the
inherently complex RAG process, makes it challenging and time-consuming for
researchers to compare and evaluate these approaches in a consistent
environment. Existing RAG toolkits, such as LangChain and LlamaIndex, while
available, are often heavy and inflexibly, failing to meet the customization
needs of researchers. In response to this challenge, we develop \ours{}, an
efficient and modular open-source toolkit designed to assist researchers in
reproducing and comparing existing RAG methods and developing their own
algorithms within a unified framework. Our toolkit has implemented 16 advanced
RAG methods and gathered and organized 38 benchmark datasets. It has various
features, including a customizable modular framework, multimodal RAG
capabilities, a rich collection of pre-implemented RAG works, comprehensive
datasets, efficient auxiliary pre-processing scripts, and extensive and
standard evaluation metrics. Our toolkit and resources are available at
https://github.com/RUC-NLPIR/FlashRAG.
```
### 🌟 论文解读 | FlashRAG：高效助力检索增强生成研究的模块化工具包

### 📌 背景痛点/本文动机
在大语言模型（LLMs）和多模态大语言模型（MLLMs）时代，检索增强生成（RAG）凭借缓解模型幻觉等优势成为研究热点，众多新算法和模型不断涌现以优化RAG系统不同环节。但当前存在诸多阻碍研究推进的问题：一是缺乏标准化实现框架，RAG流程本身复杂，导致研究者在统一环境下对比评估各类方法困难且耗时；二是现有工具包（如LangChain、LlamaIndex）往往厚重、灵活性不足，无法满足研究者定制化需求；三是很多方法不开源或需特殊配置，数据集和检索语料零散且预处理繁琐，RAG系统涉及的索引、检索、生成等环节技术实现门槛高。因此，亟需一个面向研究、统一且灵活的RAG工具包来简化方法开发与对比研究，FlashRAG应运而生。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：全面灵活的模块化RAG框架  
FlashRAG在组件和 pipeline 层面实现高度模块化，包含5个核心模块与16个不同RAG子组件，各组件可独立集成或组合成流程。同时提供9种标准化RAG流程和辅助脚本（如维基百科下载、分块构建语料库、构建检索索引、准备检索结果等任务），打造高效且易用的端到端RAG框架，让研究者能按需定制工作流。

💡 创新点2：预实现前沿RAG算法并统一评估  
目前FlashRAG实现了16种先进RAG算法（如Self - RAG、FLARE等），覆盖顺序、条件、分支、循环等RAG类别。这些方法在统一框架内评估且有基准报告，支持透明化的评估与对比，后续还会持续纳入更多方法，为研究提供丰富且规范的算法参考。

💡 创新点3：支持多模态RAG场景  
集成主流多模态大语言模型（如Qwen、InternVL、LLaVA）和多种基于CLIP的检索器，为研究者在纯文本和多模态场景下都提供广泛技术支持，还配备多个常用MRAG基准数据集用于系统评估，拓展了RAG研究的模态边界。

💡 创新点4：丰富规范的基准数据集  
收集38个常用数据集并标准化格式，部分数据集（如WikiAsp、NarrativeQA）针对RAG场景做特定调整以保证一致性，且这些数据集在HuggingFace平台易获取，提升RAG研究中数据集的一致性与实用性，减少研究者数据准备成本。

💡 创新点5：可视化网页界面辅助实验  
具备直观的网页界面，可可视化完整RAG pipeline。用户能在检索到答案生成各步骤查看中间结果，还能一键调参和自动基准评估，支持语料加载、组件实时可视化与 pipeline 全面评估，让RAG实验更透明高效。

### 📈 实验结果
文中通过与LangChain、LlamaIndex、Haystack等现有工具包从自动评估、多模态支持、语料辅助、提供数据集数量、支持方法数量等维度对比（如表1所示），FlashRAG在自动评估（支持）、多模态（支持）、语料辅助（支持）、提供数据集数量（38个）、支持方法数量（16种）等方面展现出显著优势，证明其在功能丰富度、对研究支持的全面性上远超同类工具包。

### 💬 可借鉴之处
1. 模块化设计思路：将复杂系统拆分为可独立组合的模块，既方便研究者理解各环节功能，又能快速定制符合需求的工作流，这种解耦思想在工具类项目开发中值得借鉴。  
2. 生态化资源整合：不仅实现算法，还整合大量数据集并标准化，同时提供辅助脚本，从研究全流程降低门槛，为领域工具构建提供了“算法 + 数据 + 工具链”的完整生态参考。  
3. 多模态与可视化方向：紧跟多模态趋势支持MRAG，且通过可视化界面降低技术门槛、提升实验透明度，在工具易用性和前沿方向适配性上给出了优秀范例，对打造用户友好型研究工具很有启发。  
4. 统一评估框架：为不同RAG算法搭建统一评估环境并生成基准报告，让方法间对比公平透明，这种推动领域研究标准化的做法，有助于加速领域内知识积累与方法迭代。 
```

