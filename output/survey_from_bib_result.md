# Paper List from BIB File: tmpgeucql8d.bib
- [25/01] **rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking**  
[[Paper](http://arxiv.org/pdf/2501.04519v1)] [[Code/Page](https://github.com/microsoft/rStar.)] [[TLDR/Notes](#rstar-math--small-llms-can-master-math-reasoning-with-self-evolved-deep-thinking)]

- [23/04] **ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation**  
[[Paper](http://arxiv.org/pdf/2304.05977v4)] [[Code/Page](https://github.com/THUDM/ImageReward}.)] [[TLDR/Notes](#imagereward--learning-and-evaluating-human-preferences-for-text-to-image-generation)]

- [25/05] **R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.02835v2)] [[Code/Page]()] [[TLDR/Notes](#r1-reward--training-multimodal-reward-model-through-stable-reinforcement-learning)]

- [24/11] **Self-Generated Critiques Boost Reward Modeling for Language Models**  
[[Paper](http://arxiv.org/pdf/2411.16646v3)] [[Code/Page]()] [[TLDR/Notes](#self-generated-critiques-boost-reward-modeling-for-language-models)]

- [23/04] **RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment**  
[[Paper](http://arxiv.org/pdf/2304.06767v4)] [[Code/Page]()] [[TLDR/Notes](#raft--reward-ranked-finetuning-for-generative-foundation-model-alignment)]

- [24/08] **Generative Verifiers: Reward Modeling as Next-Token Prediction**  
[[Paper](http://arxiv.org/pdf/2408.15240v3)] [[Code/Page]()] [[TLDR/Notes](#generative-verifiers--reward-modeling-as-next-token-prediction)]

- [25/01] **InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model**  
[[Paper](http://arxiv.org/pdf/2501.12368v2)] [[Code/Page](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward)] [[TLDR/Notes](#internlm-xcomposer2-5-reward--a-simple-yet-effective-multi-modal-reward-model)]

- [24/01] **WARM: On the Benefits of Weight Averaged Reward Models**  
[[Paper](http://arxiv.org/pdf/2401.12187v1)] [[Code/Page]()] [[TLDR/Notes](#warm--on-the-benefits-of-weight-averaged-reward-models)]

- [25/05] **Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.07263v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-vl-reward--an-effective-reward-model-for-multimodal-understanding-and-reasoning)]

- [25/05] **RM-R1: Reward Modeling as Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.02387v3)] [[Code/Page](https://github.com/RM-R1-UIUC/RM-R1.)] [[TLDR/Notes](#rm-r1--reward-modeling-as-reasoning)]

- [23/12] **Diffusion Reward: Learning Rewards via Conditional Video Diffusion**  
[[Paper](http://arxiv.org/pdf/2312.14134v3)] [[Code/Page](https://diffusion-reward.github.io.)] [[TLDR/Notes](#diffusion-reward--learning-rewards-via-conditional-video-diffusion)]

- [24/09] **RRM: Robust Reward Model Training Mitigates Reward Hacking**  
[[Paper](http://arxiv.org/pdf/2409.13156v2)] [[Code/Page]()] [[TLDR/Notes](#rrm--robust-reward-model-training-mitigates-reward-hacking)]

- [25/06] **RewardAnything: Generalizable Principle-Following Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.03637v2)] [[Code/Page]()] [[TLDR/Notes](#rewardanything--generalizable-principle-following-reward-models)]

- [25/03] **VisualPRM: An Effective Process Reward Model for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2503.10291v1)] [[Code/Page](https://internvl.github.io/blog/2025-03-13-VisualPRM/.)] [[TLDR/Notes](#visualprm--an-effective-process-reward-model-for-multimodal-reasoning)]

- [24/10] **Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs**  
[[Paper](http://arxiv.org/pdf/2410.18451v1)] [[Code/Page]()] [[TLDR/Notes](#skywork-reward--bag-of-tricks-for-reward-modeling-in-llms)]

- [24/10] **On Designing Effective RL Reward at Training Time for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2410.15115v3)] [[Code/Page]()] [[TLDR/Notes](#on-designing-effective-rl-reward-at-training-time-for-llm-reasoning)]

- [25/06] **GRAM: A Generative Foundation Reward Model for Reward Generalization**  
[[Paper](http://arxiv.org/pdf/2506.14175v2)] [[Code/Page]()] [[TLDR/Notes](#gram--a-generative-foundation-reward-model-for-reward-generalization)]

- [25/04] **Inference-Time Scaling for Generalist Reward Modeling**  
[[Paper](http://arxiv.org/pdf/2504.02495v2)] [[Code/Page]()] [[TLDR/Notes](#inference-time-scaling-for-generalist-reward-modeling)]

- [24/06] **ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search**  
[[Paper](http://arxiv.org/pdf/2406.03816v3)] [[Code/Page](https://github.com/THUDM/ReST-MCTS.)] [[TLDR/Notes](#rest-mcts*--llm-self-training-via-process-reward-guided-tree-search)]

- [25/07] **Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy**  
[[Paper](http://arxiv.org/pdf/2507.01352v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy)]



# TLDR/Notes
## rstar-math--small-llms-can-master-math-reasoning-with-self-evolved-deep-thinking
### Abstract
We present rStar-Math to demonstrate that small language models (SLMs) can
rival or even surpass the math reasoning capability of OpenAI o1, without
distillation from superior models. rStar-Math achieves this by exercising "deep
thinking" through Monte Carlo Tree Search (MCTS), where a math policy SLM
performs test-time search guided by an SLM-based process reward model.
rStar-Math introduces three innovations to tackle the challenges in training
the two SLMs: (1) a novel code-augmented CoT data sythesis method, which
performs extensive MCTS rollouts to generate step-by-step verified reasoning
trajectories used to train the policy SLM; (2) a novel process reward model
training method that avoids na\"ive step-level score annotation, yielding a
more effective process preference model (PPM); (3) a self-evolution recipe in
which the policy SLM and PPM are built from scratch and iteratively evolved to
improve reasoning capabilities. Through 4 rounds of self-evolution with
millions of synthesized solutions for 747k math problems, rStar-Math boosts
SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it
improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to
86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad
(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among
the top 20% the brightest high school math students. Code and data will be
available at https://github.com/microsoft/rStar.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°æ¨¡å‹ä¹Ÿèƒ½ç©è½¬æ•°å­¦æ¨ç†ï¼ŸrStar-Math é â€œæ·±åº¦æ€è€ƒâ€æŒ‘æˆ˜å¤§æ¨¡å‹éœ¸æƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä¼ ç»Ÿâ€œä¸€æ­¥å‡ºç­”æ¡ˆâ€çš„æ¨ç†æ–¹å¼ï¼ˆç±»ä¼¼äººç±»â€œç›´è§‰å¼æ€è€ƒâ€System 1ï¼‰å®¹æ˜“å‡ºé”™ã€‚è€Œâ€œæ·±åº¦æ€è€ƒâ€ï¼ˆç±»ä¼¼äººç±»â€œå®¡æ…å¼æ€è€ƒâ€System 2ï¼‰çš„èŒƒå¼è™½è¢«æå‡ºâ€”â€”è®©æ¨¡å‹åˆ†æ­¥éª¤æ¨ç†å¹¶ç”±å¥–åŠ±æ¨¡å‹è¯„ä¼°ï¼Œä½†é¢ä¸´ä¸¤å¤§æ ¸å¿ƒéš¾é¢˜ï¼š  
1. **ä¼˜è´¨æ•°æ®ç¨€ç¼º**ï¼šé«˜è´¨é‡æ•°å­¦æ¨ç†æ•°æ®æœ¬èº«éš¾è·å–ï¼Œåˆæˆæ•°æ®æ—¶åˆå®¹æ˜“æ··å…¥é”™è¯¯ä¸­é—´æ­¥éª¤ï¼›  
2. **å¥–åŠ±æ¨¡å‹éš¾è®­**ï¼šç»™æ¯ä¸€æ­¥æ¨ç†ç²¾å‡†æ‰“åˆ†éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ï¼Œè‡ªåŠ¨æ ‡æ³¨åˆå®¹æ˜“å¼•å…¥å™ªå£°ï¼Œå¯¼è‡´è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ•ˆæœå—é™ã€‚  
æ­¤å¤–ï¼Œç°æœ‰ä¾èµ–å¤§æ¨¡å‹è’¸é¦çš„æ–¹æ³•â€œååŠ²ä¸è¶³â€ï¼Œæ— æ³•çªç ´æ•™å¸ˆæ¨¡å‹çš„èƒ½åŠ›ä¸Šé™ã€‚äºæ˜¯å¾®è½¯äºšæ´²ç ”ç©¶é™¢å›¢é˜Ÿæå‡º **rStar-Math**ï¼Œæ¢ç´¢è®©å°è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼Œå¦‚7Bã€3.8Bé‡çº§ï¼‰ä¸é å¤§æ¨¡å‹è’¸é¦ï¼Œä¹Ÿèƒ½åœ¨æ•°å­¦æ¨ç†ä¸Šæ¯”è‚©ç”šè‡³è¶…è¶ŠOpenAI o1ç­‰æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
rStar-Math å›´ç»•â€œè‡ªè¿›åŒ–çš„æ·±åº¦æ€è€ƒâ€å±•å¼€ï¼Œç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è®©å°æ¨¡å‹å®ç°System 2å¼æ¨ç†ï¼Œå¹¶é’ˆå¯¹â€œç­–ç•¥æ¨¡å‹ï¼ˆç”Ÿæˆæ¨ç†æ­¥éª¤ï¼‰â€å’Œâ€œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆè¯„ä¼°æ­¥éª¤ï¼‰â€çš„è®­ç»ƒéš¾é¢˜ï¼Œæå‡ºä¸‰å¤§åˆ›æ–°ï¼š  

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»£ç å¢å¼ºçš„CoTæ•°æ®åˆæˆæ³•  
è¦è®­ç»ƒèƒ½ç”Ÿæˆä¼˜è´¨æ¨ç†æ­¥éª¤çš„â€œç­–ç•¥SLMâ€ï¼Œéœ€å¤§é‡å¸¦æ­£ç¡®ä¸­é—´æ­¥éª¤çš„æ¨ç†è½¨è¿¹ã€‚rStar-Math æŠŠæ•°å­¦è§£é¢˜æ‹†è§£ä¸ºMCTSå†…çš„å¤šæ­¥ç”Ÿæˆï¼šæ¯ä¸€æ­¥è®©ç­–ç•¥SLMç”Ÿæˆå€™é€‰æ¨ç†èŠ‚ç‚¹ï¼ˆå«ä¸€æ­¥CoTå’Œå¯¹åº”Pythonä»£ç ï¼‰ï¼Œ**åªä¿ç•™ä»£ç èƒ½æˆåŠŸæ‰§è¡Œçš„èŠ‚ç‚¹**ï¼ˆè¿‡æ»¤ä¸­é—´é”™è¯¯ï¼‰ï¼›åŒæ—¶ï¼Œé€šè¿‡å¤§è§„æ¨¡MCTS rolloutsï¼Œä¾æ®â€œæ­¥éª¤å¯¹æœ€ç»ˆæ­£ç¡®è§£çš„è´¡çŒ®åº¦â€è‡ªåŠ¨ç»™ä¸­é—´æ­¥åˆ†é…Qå€¼â€”â€”è´¡çŒ®å¤§çš„æ­¥éª¤Qå€¼é«˜ã€è´¨é‡é«˜ã€‚è¿™æ ·å°±ç”¨MCTSè‡ªåŠ¨åˆæˆäº†å¸¦è‡ªæ ‡æ³¨Qå€¼çš„ã€æ­¥éª¤å¯é çš„æ¨ç†è½¨è¿¹ï¼Œä¾›ç­–ç•¥SLMè®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¿‡ç¨‹åå¥½æ¨¡å‹ï¼ˆPPMï¼‰çš„è®­ç»ƒæ–°æ³•  
è¿‡ç¨‹å¥–åŠ±æ¨¡å‹éœ€è¦ç»™æ¯ä¸€æ­¥æ¨ç†ç²¾å‡†æ‰“åˆ†ï¼Œä½†ç›´æ¥ç”¨Qå€¼å½“åˆ†æ•°å™ªå£°å¤§ã€ä¸ç²¾å‡†ã€‚rStar-Math æ¢æ€è·¯ï¼šæ—¢ç„¶Qå€¼è™½ä¸å®Œç¾ï¼Œä½†èƒ½åŒºåˆ†â€œæ­£å‘æ­¥éª¤ï¼ˆæ­£ç¡®/ç›¸å…³ï¼‰â€å’Œâ€œè´Ÿå‘æ­¥éª¤ï¼ˆé”™è¯¯/æ— å…³ï¼‰â€ï¼Œé‚£å°±**åŸºäºQå€¼æ„å»ºâ€œæ­¥éª¤åå¥½å¯¹â€**ï¼Œç”¨ pairwise ranking loss è®­ç»ƒPPMï¼Œè®©å®ƒå­¦ä¼šé¢„æµ‹æ­¥éª¤çš„å¥–åŠ±å€¾å‘ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥æ‹¿Qå€¼å½“æ ‡ç­¾çš„ç²—ç³™åšæ³•ï¼Œè®©PPMæ›´å¯é ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå››è½®è‡ªè¿›åŒ–æµç¨‹  
ä»74.7ä¸‡é“æ•°å­¦é¢˜çš„å…¬å¼€æ•°æ®é›†èµ·æ­¥ï¼ŒrStar-Math è®©â€œç­–ç•¥SLMâ€å’Œâ€œPPMâ€ä»0å¼€å§‹ã€è¿­ä»£è¿›åŒ–ï¼šæ¯ä¸€è½®ç”¨å½“å‰æœ€æ–°çš„ç­–ç•¥æ¨¡å‹å’ŒPPMåšMCTSï¼Œç”¨å‰ä¸¤ä¸ªåˆ›æ–°ç‚¹ç”Ÿæˆæ›´é«˜è´¨é‡çš„æ•°æ®ï¼Œå†è®­ç»ƒä¸‹ä¸€è½®æ›´å¼ºçš„ç­–ç•¥æ¨¡å‹å’ŒPPMã€‚æ¯ä¸€è½®éƒ½å®ç°â€œæ›´å¼ºçš„ç­–ç•¥æ¨¡å‹â†’æ›´å¯é çš„PPMâ†’ç”¨PPMå¢å¼ºçš„MCTSç”Ÿæˆæ›´å¥½è½¨è¿¹â†’è¦†ç›–æ›´éš¾æ•°å­¦é¢˜â€çš„æ­£å‘å¾ªç¯ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒrStar-Math å±•ç°äº†å°æ¨¡å‹â€œæ·±åº¦æ€è€ƒâ€çš„æ½œåŠ›ï¼š  
- **MATHåŸºå‡†**ï¼šç”¨64æ¡æ¨ç†è½¨è¿¹æ—¶ï¼ŒQwen2.5-Math-7Bä»58.8%æå‡åˆ°90.0%ï¼ŒPhi3-mini-3.8Bä»41.4%æå‡åˆ°86.4%ï¼Œ**è¶…è¿‡OpenAI o1-previewï¼ˆ85.5%ï¼‰**ï¼›  
- **ç¾å›½æ•°å­¦å¥¥èµ›ï¼ˆAIME 2024ï¼‰**ï¼šå¹³å‡è§£å‡º53.3%ï¼ˆ8/15ï¼‰çš„é¢˜ç›®ï¼Œè¶…è¿‡o1-previewï¼ˆ44.6%ï¼‰ï¼Œæˆç»©èƒ½æ’è¿›å…¨ç¾æœ€ä¼˜ç§€é«˜ä¸­ç”Ÿçš„å‰20%ï¼›  
- åœ¨å…¶ä»–å¦‚Olympiad Benchã€College Mathç­‰ä»»åŠ¡ä¸­ï¼Œä¹Ÿæ™®éè¶…è¶Šæˆ–è¿½å¹³OpenAI o1ç³»åˆ—ï¼Œä¸”æ‰€æœ‰å®éªŒå‡åŸºäºâ€œå°æ¨¡å‹ï¼ˆâ‰¤7Bï¼‰â€å®Œæˆï¼Œè¯æ˜äº†ä¸ä¾èµ–å¤§æ¨¡å‹è’¸é¦ä¹Ÿèƒ½çªç ´ä¸Šé™ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å°æ¨¡å‹çš„â€œæ·±åº¦æ€è€ƒâ€èŒƒå¼**ï¼šç”¨MCTSæ¨¡æ‹Ÿäººç±»åˆ†æ­¥æ¨ç†+è¯„ä¼°çš„è¿‡ç¨‹ï¼Œè¯æ˜å°æ¨¡å‹ä¹Ÿèƒ½é€šè¿‡â€œæµ‹è¯•æ—¶æœç´¢â€å®ç°å¼ºæ¨ç†ï¼Œä¸å¿…ä¾èµ–æ¨¡å‹å‚æ•°è§„æ¨¡ï¼›  
2. **è‡ªç›‘ç£çš„æ•°æ®åˆæˆ**ï¼šç”¨MCTSè‡ªåŠ¨ç”Ÿæˆå¸¦éªŒè¯ï¼ˆä»£ç æ‰§è¡Œï¼‰å’Œè‡ªæ ‡æ³¨ï¼ˆQå€¼ï¼‰çš„æ¨ç†è½¨è¿¹ï¼Œè§£å†³ä¼˜è´¨æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸ºå°æ¨¡å‹è®­ç»ƒæä¾›æ–°æ€è·¯ï¼›  
3. **è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„è½»é‡åŒ–è®­ç»ƒ**ï¼šç”¨â€œåå¥½å¯¹+pairwise lossâ€æ›¿ä»£ç›´æ¥æ‰“åˆ†ï¼Œé™ä½å¯¹ç²¾å‡†æ ‡æ³¨çš„ä¾èµ–ï¼Œè®©å¥–åŠ±æ¨¡å‹æ›´æ˜“è®­ç»ƒï¼›  
4. **è‡ªè¿›åŒ–è¿­ä»£æ€è·¯**ï¼šä»0å¼€å§‹è¿­ä»£ä¼˜åŒ–ç­–ç•¥æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ï¼Œé æ•°æ®å’Œæ¨¡å‹çš„â€œåŒå‘å¢å¼ºâ€å®ç°èƒ½åŠ›è·ƒè¿ï¼Œè¿™ç§é—­ç¯è¿›åŒ–æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€è¦åˆ†æ­¥æ¨ç†çš„ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ï¼‰ã€‚  

æ€»ä¹‹ï¼ŒrStar-Math æ’•å¼€äº†â€œåªæœ‰å¤§æ¨¡å‹èƒ½åšå¤æ‚æ¨ç†â€çš„å›ºæœ‰è®¤çŸ¥ï¼Œä¸ºå°æ¨¡å‹çš„èƒ½åŠ›çªç ´æä¾›äº†ä¸€å¥—å®Œæ•´çš„â€œæ·±åº¦æ€è€ƒ+è‡ªè¿›åŒ–â€æ–¹æ³•è®ºï¼Œæœªæ¥åœ¨æ•™è‚²ã€ç«èµ›è¾…åŠ©ç­‰åœºæ™¯ä¹Ÿæœ‰æ½œåœ¨åº”ç”¨ç©ºé—´~
```

## imagereward--learning-and-evaluating-human-preferences-for-text-to-image-generation
### Abstract
We present a comprehensive solution to learn and improve text-to-image models
from human preference feedback. To begin with, we build ImageReward -- the
first general-purpose text-to-image human preference reward model -- to
effectively encode human preferences. Its training is based on our systematic
annotation pipeline including rating and ranking, which collects 137k expert
comparisons to date. In human evaluation, ImageReward outperforms existing
scoring models and metrics, making it a promising automatic metric for
evaluating text-to-image synthesis. On top of it, we propose Reward Feedback
Learning (ReFL), a direct tuning algorithm to optimize diffusion models against
a scorer. Both automatic and human evaluation support ReFL's advantages over
compared methods. All code and datasets are provided at
\url{https://github.com/THUDM/ImageReward}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ImageRewardï¼šè®©æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ›´æ‡‚äººç±»åå¥½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚è‡ªå›å½’å’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼‰å‘å±•è¿…çŒ›ï¼Œèƒ½ä¾æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜ä¿çœŸã€è¯­ä¹‰ç›¸å…³çš„å›¾åƒã€‚ä½†ç°æœ‰æ¨¡å‹ä»å­˜åœ¨è¯¸å¤šé—®é¢˜ï¼Œæ¯”å¦‚æ–‡æœ¬ - å›¾åƒå¯¹é½ä¸ä½³ï¼ˆæ— æ³•å‡†ç¡®å‘ˆç°æ–‡æœ¬ä¸­å¯¹è±¡çš„æ•°é‡ã€å±æ€§ç­‰ï¼‰ã€äººä½“éƒ¨ä½ç•¸å½¢ã€ä¸ç¬¦åˆäººç±»å®¡ç¾åå¥½ã€å­˜åœ¨æ¯’æ€§å’Œåè§ç­‰ã€‚è¿™äº›é—®é¢˜éš¾ä»¥ä»…é€šè¿‡æ”¹è¿›æ¨¡å‹æ¶æ„å’Œé¢„è®­ç»ƒæ•°æ®è§£å†³ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œä»äººç±»åé¦ˆä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰èƒ½å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹è´´åˆäººç±»åå¥½ï¼Œå—æ­¤å¯å‘ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸä¹Ÿéœ€è¦ç±»ä¼¼æ–¹æ³•æ¥å­¦ä¹ äººç±»åå¥½ï¼Œäºæ˜¯æœ¬æ–‡æå‡ºç›¸å…³è§£å†³æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºImageRewardæ¨¡å‹
æ„å»ºäº†é¦–ä¸ªé€šç”¨çš„æ–‡æœ¬åˆ°å›¾åƒäººç±»åå¥½å¥–åŠ±æ¨¡å‹ImageRewardã€‚å…¶è®­ç»ƒåŸºäºç³»ç»Ÿçš„æ ‡æ³¨æµç¨‹ï¼ŒåŒ…æ‹¬è¯„çº§å’Œæ’åºã€‚è¯¥æµç¨‹å…ˆä»DiffusionDBé€‰å–å¤šæ ·çš„çœŸå®ç”¨æˆ·æç¤ºè¯ï¼Œæ”¶é›†å€™é€‰å›¾åƒå¯¹ï¼›ç„¶åè®¾è®¡åŒ…å«æç¤ºè¯æ ‡æ³¨ã€æ–‡æœ¬ - å›¾åƒè¯„çº§ã€å›¾åƒæ’åºçš„æ ‡æ³¨é˜¶æ®µï¼Œè®©æ ‡æ³¨å‘˜æŒ‰å¯¹é½åº¦ã€ä¿çœŸåº¦ã€æ— å®³æ€§ç­‰å¯¹å›¾åƒè¯„çº§å¹¶æ’åºï¼›æœ€ç»ˆæ”¶é›†åˆ°137kçš„ä¸“å®¶æ¯”è¾ƒæ•°æ®ç”¨äºè®­ç»ƒï¼Œæœ‰æ•ˆç¼–ç äººç±»åå¥½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºReward Feedback Learningï¼ˆReFLï¼‰ç®—æ³•
æå‡ºReFLç›´æ¥è°ƒä¼˜æ‰©æ•£æ¨¡å‹ã€‚åŸºäºImageRewardåœ¨æ‰©æ•£æ¨¡å‹åæœŸå»å™ªæ­¥éª¤å¯¹å›¾åƒè´¨é‡çš„å¯è¯†åˆ«æ€§ï¼Œåˆ©ç”¨ImageRewardçš„åé¦ˆåœ¨éšæœºçš„åæœŸå»å™ªæ­¥éª¤ç›´æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ— ä¼¼ç„¶æ€§éš¾ä»¥ç›´æ¥ä¼˜åŒ–çš„é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äººç±»è¯„ä¼°ä¸­ï¼ŒImageRewardåœ¨ç†è§£æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­çš„äººç±»åå¥½æ–¹é¢ï¼Œè¶…è¶Šäº†ç°æœ‰è¯„åˆ†æ¨¡å‹å’ŒæŒ‡æ ‡ï¼ˆå¦‚æ¯”CLIPé«˜38.6%ã€æ¯”Aestheticé«˜39.6%ã€æ¯”BLIPé«˜31.6%ï¼‰ï¼Œèƒ½æ˜¾è‘—ç¼“è§£æ–‡æœ¬ - å›¾åƒç”Ÿæˆä¸­çš„è¯¸å¤šé—®é¢˜ï¼›ä½œä¸ºè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œä¸FIDå’ŒCLIPåˆ†æ•°ç›¸æ¯”ï¼Œåœ¨çœŸå®ç”¨æˆ·æç¤ºå’ŒMS - COCO 2014æ•°æ®ä¸Šï¼ŒImageRewardä¸äººç±»åå¥½æ’åæ›´ä¸€è‡´ï¼Œæ¨¡å‹å’Œæ ·æœ¬é—´åŒºåˆ†åº¦æ›´é«˜ã€‚å¯¹äºReFLï¼Œè‡ªåŠ¨å’Œäººç±»è¯„ä¼°éƒ½è¯æ˜å…¶æ¯”æ•°æ®å¢å¼ºã€æŸå¤±é‡åŠ æƒç­‰ç°æœ‰æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ ‡æ³¨æµç¨‹è®¾è®¡ï¼šæœ¬æ–‡ç³»ç»Ÿåœ°è¯†åˆ«æ–‡æœ¬åˆ°å›¾åƒäººç±»åå¥½æ ‡æ³¨çš„æŒ‘æˆ˜ï¼Œè®¾è®¡é’ˆå¯¹æ€§æ ‡æ³¨æµç¨‹ï¼ŒåŒ…æ‹¬ç¡®å®šé‡åŒ–è¯„ä¼°æ ‡å‡†ã€è®­ç»ƒæ ‡æ³¨å‘˜ã€ä¼˜åŒ–æ ‡æ³¨ä½“éªŒå’Œè´¨é‡éªŒè¯ç­‰ï¼Œä¸ºé¢†åŸŸå†…æ”¶é›†äººç±»åå¥½æ•°æ®æä¾›äº†å¯å‚è€ƒçš„å®Œæ•´æµç¨‹èŒƒå¼ã€‚
2. å¥–åŠ±æ¨¡å‹ä¸ç”Ÿæˆæ¨¡å‹ç»“åˆï¼šå°†äººç±»åå¥½ç¼–ç åˆ°å¥–åŠ±æ¨¡å‹ï¼ˆImageRewardï¼‰ï¼Œå†åŸºäºå¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ï¼ˆReFLï¼‰çš„æ€è·¯ï¼Œä¸ºå…¶ä»–ç”Ÿæˆç±»ä»»åŠ¡ï¼ˆå¦‚éŸ³é¢‘ç”Ÿæˆã€è§†é¢‘ç”Ÿæˆç­‰ï¼‰æä¾›äº†ä»äººç±»åé¦ˆä¸­å­¦ä¹ ä¼˜åŒ–çš„å‚è€ƒæ¡†æ¶ã€‚
3. è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ¢ç´¢ï¼šImageRewardä½œä¸ºæœ‰å‰æ™¯çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œå…¶æ„å»ºå’ŒéªŒè¯è¿‡ç¨‹ä¸ºæ–‡æœ¬åˆ°å›¾åƒé¢†åŸŸæ‰“é€ æ›´è´´åˆäººç±»æ„ŸçŸ¥çš„è‡ªåŠ¨è¯„ä¼°å·¥å…·æä¾›äº†ç»éªŒï¼Œä¹Ÿå¯å‘åœ¨å…¶ä»–ç”Ÿæˆä»»åŠ¡ä¸­æ€è€ƒå¦‚ä½•æ„å»ºæœ‰æ•ˆè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚
```

## r1-reward--training-multimodal-reward-model-through-stable-reinforcement-learning
### Abstract
Multimodal Reward Models (MRMs) play a crucial role in enhancing the
performance of Multimodal Large Language Models (MLLMs). While recent
advancements have primarily focused on improving the model structure and
training data of MRMs, there has been limited exploration into the
effectiveness of long-term reasoning capabilities for reward modeling and how
to activate these capabilities in MRMs. In this paper, we explore how
Reinforcement Learning (RL) can be used to improve reward modeling.
Specifically, we reformulate the reward modeling problem as a rule-based RL
task. However, we observe that directly applying existing RL algorithms, such
as Reinforce++, to reward modeling often leads to training instability or even
collapse due to the inherent limitations of these algorithms. To address this
issue, we propose the StableReinforce algorithm, which refines the training
loss, advantage estimation strategy, and reward design of existing RL methods.
These refinements result in more stable training dynamics and superior
performance. To facilitate MRM training, we collect 200K preference data from
diverse datasets. Our reward model, R1-Reward, trained using the
StableReinforce algorithm on this dataset, significantly improves performance
on multimodal reward modeling benchmarks. Compared to previous SOTA models,
R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$
improvement on the Multimodal Reward Bench. Moreover, with more inference
compute, R1-Reward's performance is further enhanced, highlighting the
potential of RL algorithms in optimizing MRMs.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼šR1 - Reward çš„åˆ›æ–°ä¹‹è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ€§èƒ½æ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰å¯¹ MRMs çš„ç ”ç©¶å¤šé›†ä¸­åœ¨æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ•°æ®ä¸Šï¼Œå¯¹åˆ©ç”¨é•¿æœŸæ¨ç†èƒ½åŠ›æ”¹è¿›å¥–åŠ±å»ºæ¨¡ä»¥åŠå¦‚ä½•æ¿€æ´»è¯¥èƒ½åŠ›æ¢ç´¢ä¸è¶³ã€‚åŒæ—¶ï¼Œå°†ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ç›´æ¥åº”ç”¨äºå¥–åŠ±å»ºæ¨¡æ—¶ï¼Œå¸¸å› ç®—æ³•å›ºæœ‰å±€é™å¯¼è‡´è®­ç»ƒä¸ç¨³å®šç”šè‡³å´©æºƒã€‚å› æ­¤ï¼Œæœ¬æ–‡æ¢ç´¢å¦‚ä½•ç”¨ RL æ”¹è¿›å¥–åŠ±å»ºæ¨¡ï¼Œè§£å†³ç°æœ‰ RL ç®—æ³•åœ¨å¥–åŠ±å»ºæ¨¡ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º StableReinforce ç®—æ³•
å°†å¥–åŠ±å»ºæ¨¡é—®é¢˜è½¬åŒ–ä¸ºåŸºäºè§„åˆ™çš„ RL ä»»åŠ¡åï¼Œé’ˆå¯¹ç°æœ‰ RL ç®—æ³•ï¼ˆå¦‚ Reinforce++ï¼‰åœ¨å¥–åŠ±å»ºæ¨¡ä¸­è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼ŒStableReinforce ä»è®­ç»ƒæŸå¤±ã€ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥å’Œå¥–åŠ±è®¾è®¡ä¸‰æ–¹é¢æ”¹è¿›ã€‚æ”¹è¿›å‰ªè¾‘æ“ä½œä»¥å‡è½»å¤§æ›´æ–°å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šæ€§ï¼›å¼•å…¥é²æ£’çš„ä¼˜åŠ¿å½’ä¸€åŒ–æŠ€æœ¯ï¼Œé™åˆ¶å¼‚å¸¸å€¼å½±å“ï¼›åœ¨å¥–åŠ±å‡½æ•°è®¾è®¡ä¸Šï¼Œå¼•å…¥ MLLM ä½œä¸ºè£åˆ¤ï¼Œè¯„ä¼°æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸æœ€ç»ˆç»“æœçš„ä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨ç†å’Œè¾“å‡ºå¯¹é½ï¼Œä¿ƒè¿›æ›´å‡†ç¡®ä¸”é€»è¾‘è¿è´¯çš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¸è¿›éš¾åº¦è®­ç»ƒç­–ç•¥ä¸æ•°æ®æ”¶é›†
ä¸ºåŠ©åŠ› MRM è®­ç»ƒï¼Œæ”¶é›†æ¥è‡ªä¸åŒæ•°æ®é›†çš„ 20 ä¸‡åå¥½æ•°æ®ã€‚è®­ç»ƒæ—¶é‡‡ç”¨æ¸è¿›éš¾åº¦ç­–ç•¥ï¼šå…ˆç”¨ GPT - 4o ç”Ÿæˆå¯¹åº”æ€è€ƒè¿‡ç¨‹ä½œä¸ºå†·å¯åŠ¨ SFT æ•°æ®ï¼Œè®°å½• GPT - 4o æ¨æ–­å‡ºä¸çœŸå®ç»“æœåŒ¹é…ç»“è®ºæ‰€éœ€é‡‡æ ·å°è¯•æ¬¡æ•°ä½œä¸ºæ ·æœ¬éš¾åº¦ï¼›å¼ºåŒ–å­¦ä¹ é˜¶æ®µé€‰å– GPT - 4o è‡³å°‘ä¸¤æ¬¡é‡‡æ ·æ‰å¾—åˆ°æ­£ç¡®ç­”æ¡ˆæˆ–ä¸‰æ¬¡å°è¯•éƒ½å¤±è´¥çš„æ ·æœ¬è®­ç»ƒæ¨¡å‹ï¼Œæå‡è®­ç»ƒæ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
R1 - Rewardï¼ˆç”¨ StableReinforce è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼‰åœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚åœ¨ VL Reward - Bench ä¸Šæ¯”ä¹‹å‰çš„ SOTA æ¨¡å‹æå‡ 8.4%ï¼Œåœ¨ Multimodal Reward Bench ä¸Šæå‡ 14.3%ï¼›åœ¨ MM - RLHF Reward Benchã€VL Reward - Bench å’Œ Multimodal Reward Bench ä¸Šåˆ†åˆ«æ¯” SOTA æå‡ 3.5%ã€13.5% å’Œ 14.6%ã€‚ä¸”æ¨ç†æ—¶é€šè¿‡å¤šæ¬¡é‡‡æ ·ï¼ˆå¦‚ Voting@5/15 ç­–ç•¥ï¼‰æ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ›´å¤šæ¨ç†è®¡ç®—èƒ½è¿›ä¸€æ­¥å¢å¼ºå…¶æ€§èƒ½ï¼Œä½“ç°äº† RL ç®—æ³•ä¼˜åŒ– MRMs çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒStableReinforce ç›¸æ¯” Reinforce++ï¼Œæ”¿ç­–æŸå¤±æ”¶æ•›æ›´å¿«æ›´ç¨³å®šï¼Œè®­ç»ƒä¸­èƒ½æŒç»­è¿›è¡Œé•¿åº¦å‹ç¼©æå‡æ•ˆç‡ï¼Œè®­ç»ƒåå¹³å‡å“åº”é•¿åº¦æ¯”åŸºç¡€æ¨¡å‹å‡å°‘çº¦ 15%ï¼Œæ¨ç† token æ•ˆç‡æœ‰æœ›æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç®—æ³•æ”¹è¿›æ€è·¯ï¼šå½“ç°æœ‰ç®—æ³•åœ¨ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚å¥–åŠ±å»ºæ¨¡ï¼‰ä¸­å­˜åœ¨ç¼ºé™·æ—¶ï¼Œå¯ä»æŸå¤±å‡½æ•°ã€ä¼˜åŠ¿ä¼°è®¡ã€å¥–åŠ±è®¾è®¡ç­‰å¤šæ–¹é¢é’ˆå¯¹æ€§æ”¹è¿›ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ä¸æ€§èƒ½ã€‚
2. æ•°æ®ä¸è®­ç»ƒç­–ç•¥ï¼šæ”¶é›†å¤§è§„æ¨¡å¤šæ ·æœ¬æ•°æ®ï¼Œå¹¶é‡‡ç”¨æ¸è¿›éš¾åº¦è®­ç»ƒç­–ç•¥ï¼Œæ ¹æ®æ•°æ®éš¾åº¦åˆ†å±‚è®­ç»ƒï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ›´åˆç†çš„è®­ç»ƒè·¯å¾„ã€‚
3. å¤šæ¨¡æ€ä»»åŠ¡ä¸­ RL çš„åº”ç”¨ï¼šå±•ç¤ºäº† RL åœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œä¸ºå¤šæ¨¡æ€é¢†åŸŸç»“åˆ RL æŠ€æœ¯æä¾›äº†å®è·µå‚è€ƒï¼Œåç»­å¯æ¢ç´¢ RL åœ¨æ›´å¤šå¤šæ¨¡æ€å­ä»»åŠ¡ä¸­çš„åº”ç”¨ä¸æ”¹è¿›ã€‚
```

## self-generated-critiques-boost-reward-modeling-for-language-models
### Abstract
Reward modeling is crucial for aligning large language models (LLMs) with
human preferences, especially in reinforcement learning from human feedback
(RLHF). However, current reward models mainly produce scalar scores and
struggle to incorporate critiques in a natural language format. We hypothesize
that predicting both critiques and the scalar reward would improve reward
modeling ability. Motivated by this, we propose Critic-RM, a framework that
improves reward models using self-generated critiques without extra
supervision. Critic-RM employs a two-stage process: generating and filtering
high-quality critiques, followed by joint fine-tuning on reward prediction and
critique generation. Experiments across benchmarks show that Critic-RM improves
reward modeling accuracy by 3.7%-7.3% compared to standard reward models and
LLM judges, demonstrating strong performance and data efficiency. Additional
studies further validate the effectiveness of generated critiques in rectifying
flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨è‡ªç”Ÿæˆ critique æå‡å¤§æ¨¡å‹å¥–åŠ±å»ºæ¨¡ï¼šCritic-RM æ¡†æ¶æ¥è¢­

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯è®©å¤§æ¨¡å‹å¯¹é½äººç±»åå¥½çš„æ ¸å¿ƒç¯èŠ‚ã€‚ä½†ç°æœ‰å¥–åŠ±æ¨¡å‹å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯è¾“å‡ºâ€œéš¾ä»¥è§£é‡Šâ€çš„æ ‡é‡åˆ†æ•°ï¼Œæ²¡å……åˆ†åˆ©ç”¨å¤§æ¨¡å‹æœ¬èº«çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œå¯¼è‡´æ•°æ®æ•ˆç‡ä½ã€æ˜“å‡ºç°â€œå¥–åŠ±é»‘å®¢â€ç­‰é²æ£’æ€§é—®é¢˜ï¼›äºŒæ˜¯å¾ˆéš¾è‡ªç„¶åœ°èå…¥â€œè‡ªç„¶è¯­è¨€å½¢å¼çš„ critiqueï¼ˆ critique å¯ç†è§£ä¸ºå¯¹æ¨¡å‹è¾“å‡ºçš„åˆ†æã€ç‚¹è¯„ï¼‰â€ã€‚è€Œâ€œLLM ä½œè¯„å§”ï¼ˆLLM-as-a-judgeï¼‰â€èŒƒå¼è™½èƒ½ç”Ÿæˆ critique ä½†åˆç¼ºä¹æ ‡é‡ä¼˜åŒ–çš„ä¼˜åŠ¿ã€‚è‹¥èƒ½ç»“åˆäºŒè€…ï¼ŒæŠŠ critique çš„å¯è§£é‡Šæ€§å’Œæ ‡é‡ä¼˜åŒ–æ¡†æ¶ç»“åˆï¼Œæœ‰æœ›æ‰“é€ æ›´é²æ£’æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚ä½†éš¾ç‚¹åœ¨äºï¼š**ç›®æ ‡å†²çª**ï¼ˆç”Ÿæˆ critique æ˜¯è¯­è¨€å»ºæ¨¡ï¼Œå¥–åŠ±æ¨¡å‹æ˜¯æ ‡é‡è¾“å‡ºï¼Œæ•´åˆå›°éš¾ï¼‰å’Œ**è¯„ä¼°å™¨å±€é™**ï¼ˆç°æˆå¤§æ¨¡å‹å½“è¯„å§”ä¸å¤Ÿå¥½ï¼Œäººå·¥æ ‡æ³¨ critique æˆæœ¬é«˜ï¼‰ã€‚äºæ˜¯ï¼Œè®ºæ–‡æå‡º Critic-RM æ¡†æ¶ï¼Œè¯•å›¾ç”¨â€œè‡ªç”Ÿæˆ critiqueâ€æ¥å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼Œä¸”ä¸ä¾èµ–æ›´å¼ºçš„å¤–éƒ¨æ•™å¸ˆæ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªç”Ÿæˆ+ç­›é€‰é«˜è´¨é‡ critique  
Critic-RM ä»¥â€œæŒ‡ä»¤å¾®è°ƒåçš„å¤§æ¨¡å‹â€ä¸ºåŸºç¡€ï¼Œå…ˆè®©æ¨¡å‹ä¸ºå•ä¸ªå“åº”ç”Ÿæˆå¤šä¸ªå€™é€‰ critiqueï¼Œæ¯ä¸ª critique è¿˜ä¼šé™„å¸¦ä¸€ä¸ªç¦»æ•£åˆ†æ•°ï¼ˆä»…ç”¨äºç­›é€‰ï¼Œéæœ€ç»ˆå¥–åŠ±ï¼‰ã€‚ä¸ºè§£å†³ critique è´¨é‡å‚å·®ä¸é½çš„é—®é¢˜ï¼Œå…ˆç”¨**ä¸€è‡´æ€§å¼•å¯¼è¿‡æ»¤**ï¼šåªä¿ç•™é‚£äº›ç¦»æ•£åˆ†æ•°å’Œäººç±»æ ‡æ³¨åå¥½æ ‡ç­¾ä¸€è‡´çš„ critiqueã€‚æ­¤å¤–è¿˜åŠ äº†â€œæ€»ç»“â€å’Œâ€œæ’åºâ€ç­–ç•¥è¿›ä¸€æ­¥æçº¯ critique è´¨é‡ï¼Œç¡®ä¿ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹çš„ critique è¶³å¤Ÿä¼˜è´¨ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒä»»åŠ¡è”åˆå¾®è°ƒ+æƒé‡å¹³è¡¡ç­–ç•¥  
æ‹¿åˆ°é«˜è´¨é‡ critique åï¼Œè¦åŒæ—¶è®­ç»ƒâ€œç”Ÿæˆ critiqueâ€å’Œâ€œé¢„æµ‹æ ‡é‡å¥–åŠ±â€ä¸¤ä¸ªä»»åŠ¡ã€‚ä½†å¥–åŠ±å»ºæ¨¡æ˜“è¿‡æ‹Ÿåˆï¼Œè€Œå¤§æ¨¡å‹ä»å¤šæ · critique å­¦ä¹ åˆéœ€è¦ä¸°å¯Œæ€§ï¼ŒäºŒè€…å­˜åœ¨çŸ›ç›¾ã€‚ä¸ºæ­¤è®¾è®¡**æƒé‡å¹³è¡¡ç­–ç•¥**ï¼šè®­ç»ƒåˆæœŸä¾§é‡â€œcritique å»ºæ¨¡æŸå¤±â€ï¼Œä¹‹åé€æ¸è¿‡æ¸¡åˆ°ç»“åˆâ€œå“åº”+ critiqueâ€æ¥é¢„æµ‹å¥–åŠ±ã€‚å¹³è¡¡ä¸¤ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œè®©æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ critique å’Œç²¾å‡†é¢„æµ‹å¥–åŠ±ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸ä¾èµ–å¼ºå¤–éƒ¨æ•™å¸ˆæ¨¡å‹çš„è‡ªæå‡èŒƒå¼  
ä»¥å¾€ç±»ä¼¼å·¥ä½œä¾èµ–æ›´å¼ºçš„æ•™å¸ˆ LLM ç”Ÿæˆ critiqueï¼Œæˆæœ¬é«˜ä¸”éš¾è§„æ¨¡åŒ–ï¼Œç”šè‡³æ²¡æ›´å¼ºæ•™å¸ˆæ—¶æ— æ³•ç”¨ã€‚Critic-RM å€Ÿé‰´â€œè‡ªæ”¹è¿›è¯­è¨€æ¨¡å‹â€æ€è·¯ï¼Œè®©æ¨¡å‹ç”¨è‡ªå·±ç”Ÿæˆçš„æ•°æ®è¿­ä»£ä¼˜åŒ–ï¼Œä¸ä¾èµ–é¢å¤–å¼ºç›‘ç£ï¼Œåœ¨æå‡å¥–åŠ±å»ºæ¨¡èƒ½åŠ›åŒæ—¶è¿˜èƒ½ä¿è¯ç”Ÿæˆè´¨é‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. å¥–åŠ±å»ºæ¨¡ç²¾åº¦æå‡ï¼šåœ¨ RewardBench ç­‰åå¥½æ’åºåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCritic-RM æ¯”æ ‡å‡†å¥–åŠ±æ¨¡å‹ã€LLM è¯„å§”ç­‰åŸºçº¿æ–¹æ³•ï¼Œå¥–åŠ±å»ºæ¨¡å‡†ç¡®ç‡æå‡ 3.7% - 7.3%ï¼Œä½“ç°äº†å¼ºæ€§èƒ½ä¸æ•°æ®æ•ˆç‡ã€‚  
2. æ¨ç†çº é”™èƒ½åŠ›éªŒè¯ï¼šåœ¨é¢å¤–ç ”ç©¶ä¸­ï¼Œç”Ÿæˆçš„ critique èƒ½æœ‰æ•ˆä¿®æ­£å¤§æ¨¡å‹æœ‰ç¼ºé™·çš„æ¨ç†æ­¥éª¤ï¼Œæ¨ç†å‡†ç¡®ç‡æå‡ 2.5% - 3.2%ï¼ŒéªŒè¯äº† critique å¯¹æ¨¡å‹æ¨ç†çº é”™çš„ä»·å€¼ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªç”Ÿæˆ+è‡ªç­›é€‰çš„â€œé—­ç¯â€æ€è·¯ï¼šåˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆæ•°æ®å¹¶ç­›é€‰ï¼Œæ‘†è„±å¯¹å¤–éƒ¨å¼ºæ•™å¸ˆæˆ–å¤§é‡äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä¸ºèµ„æºæœ‰é™åœºæ™¯ä¸‹çš„æ¨¡å‹ä¼˜åŒ–æä¾›æ€è·¯ã€‚  
2. å¤šä»»åŠ¡è”åˆè®­ç»ƒçš„æƒé‡è°ƒåº¦ï¼šé¢å¯¹â€œè¯­è¨€ç”Ÿæˆ+æ ‡é‡é¢„æµ‹â€è¿™ç±»å¤šç›®æ ‡å­¦ä¹ çŸ›ç›¾æ—¶ï¼Œç”¨åˆ†é˜¶æ®µæƒé‡è°ƒæ•´å¹³è¡¡ç›®æ ‡ï¼Œè¿™ç§â€œåŠ¨æ€ä¾§é‡â€çš„è®­ç»ƒç­–ç•¥å€¼å¾—å€Ÿé‰´åˆ°å…¶ä»–å¤šä»»åŠ¡åœºæ™¯ã€‚  
3. æŠŠâ€œå¯è§£é‡Šæ€§ critiqueâ€å’Œâ€œæ ‡é‡å¥–åŠ±â€ç»“åˆï¼šè®©å¥–åŠ±æ¨¡å‹æ—¢æœ‰æ•°å€¼ä¼˜åŒ–çš„ä¼˜åŠ¿ï¼Œåˆæœ‰è¯­è¨€å±‚é¢çš„å¯è§£é‡Šæ€§ï¼Œä¸ºåç»­ RLHF ä¸­æ›´é€æ˜ã€é²æ£’çš„åé¦ˆä¿¡å·è®¾è®¡æä¾›äº†æ–°èŒƒå¼ã€‚  
```

## raft--reward-ranked-finetuning-for-generative-foundation-model-alignment
### Abstract
Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RAFTï¼šé©æ–°ç”Ÿæˆå¼åŸºç¡€æ¨¡å‹æ ¡å‡†çš„å¥–åŠ±æ’åºå¾®è°ƒæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”Ÿæˆå¼åŸºç¡€æ¨¡å‹åœ¨å¤§é‡æ— ç›‘ç£è®­ç»ƒæ•°æ®çš„ä½œç”¨ä¸‹ï¼Œæ˜“å—åˆ°éšå«åå·®çš„å½±å“ã€‚è¿™äº›åå·®å¯èƒ½å¯¼è‡´ç”Ÿæˆçš„æ ·æœ¬å¹¶éæœ€ä¼˜ã€ç»“æœå‡ºç°åå·®ä»¥åŠäº§ç”Ÿä¸å…¬å¹³ç°è±¡ï¼Œç”šè‡³å¸¦æ¥ä¸¥é‡åæœã€‚å› æ­¤ï¼Œä½¿è¿™äº›æ¨¡å‹ç¬¦åˆäººç±»ä¼¦ç†å’Œåå¥½ï¼Œæ˜¯ç¡®ä¿å…¶åœ¨ç°å®åº”ç”¨ä¸­å¾—ä»¥è´Ÿè´£ä¸”æœ‰æ•ˆéƒ¨ç½²çš„å…³é”®ä¸€æ­¥ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦é‡‡ç”¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå³é€šè¿‡ç”±äººç±»åé¦ˆæä¾›ä¿¡æ¯çš„å¥–åŠ±æ¨¡å‹å¼•å¯¼å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ ç®—æ³•å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œä¸ç¨³å®šçš„é—®é¢˜ï¼Œå¸¸å¸¸ç»™æ¨¡å‹çš„æˆåŠŸæ ¡å‡†å¸¦æ¥é‡å¤§é˜»ç¢ï¼Œæ•…è€Œéœ€è¦å¼€å‘ä¸€ç§æ›´å¼ºå¤§ä¸”é«˜æ•ˆçš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå…¨æ–°æ¡†æ¶
å¼•å…¥äº†å¥–åŠ±æ’åºå¾®è°ƒï¼ˆReward rAnked FineTuningï¼ŒRAFTï¼‰è¿™ä¸€å…¨æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆæ ¡å‡†ç”Ÿæˆæ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ ·æœ¬ç­›é€‰ä¸å¾®è°ƒæœºåˆ¶
åˆ©ç”¨å¥–åŠ±æ¨¡å‹å’Œè¶³å¤Ÿæ•°é‡çš„æ ·æœ¬ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæŒ‘é€‰å‡ºé«˜è´¨é‡æ ·æœ¬ï¼Œæ‘’å¼ƒè¡¨ç°å‡ºä¸è‰¯è¡Œä¸ºçš„æ ·æœ¬ï¼Œç„¶åé€šè¿‡å¯¹è¿™äº›ç»è¿‡ç­›é€‰çš„æ ·æœ¬è¿›è¡Œå¾®è°ƒæ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ç ”ç©¶è¡¨æ˜ï¼ŒRAFTåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ— è®ºæ˜¯åœ¨å¥–åŠ±å­¦ä¹ æ–¹é¢ï¼Œè¿˜æ˜¯åœ¨å…¶ä»–è‡ªåŠ¨åŒ–æŒ‡æ ‡ä¸Šï¼Œéƒ½èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºè‡´åŠ›äºè§£å†³ç”Ÿæˆå¼åŸºç¡€æ¨¡å‹éšå«åå·®é—®é¢˜ã€æå‡æ¨¡å‹æ ¡å‡†æ•ˆæœçš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…è€Œè¨€ï¼ŒRAFTæä¾›äº†ä¸€ç§å…¨æ–°ä¸”æœ‰æ•ˆçš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶æ ·æœ¬ç­›é€‰ä¸å¾®è°ƒçš„æœºåˆ¶å¯ä½œä¸ºä¸€ç§å‚è€ƒèŒƒå¼ï¼Œåº”ç”¨äºä¸åŒç±»å‹çš„ç”Ÿæˆå¼æ¨¡å‹ä¸­ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç¬¦åˆäººç±»ä¼¦ç†å’Œåå¥½æ–¹é¢çš„è¡¨ç°ï¼ŒåŒæ—¶é¿å…å¼ºåŒ–å­¦ä¹ ç®—æ³•å¸¦æ¥çš„æ•ˆç‡å’Œç¨³å®šæ€§é—®é¢˜ã€‚
``` 

## generative-verifiers--reward-modeling-as-next-token-prediction
### Abstract
Verifiers or reward models are often used to enhance the reasoning
performance of large language models (LLMs). A common approach is the Best-of-N
method, where N candidate solutions generated by the LLM are ranked by a
verifier, and the best one is selected. While LLM-based verifiers are typically
trained as discriminative classifiers to score solutions, they do not utilize
the text generation capabilities of pretrained LLMs. To overcome this
limitation, we instead propose training verifiers using the ubiquitous
next-token prediction objective, jointly on verification and solution
generation. Compared to standard verifiers, such generative verifiers (GenRM)
can benefit from several advantages of LLMs: they integrate seamlessly with
instruction tuning, enable chain-of-thought reasoning, and can utilize
additional test-time compute via majority voting for better verification. We
demonstrate that GenRM outperforms discriminative, DPO verifiers, and
LLM-as-a-Judge, resulting in large performance gains with Best-of-N, namely 5%
$\rightarrow$ 45.3% on algorithmic tasks and 73% $\rightarrow$ 93.4% on GSM8K.
In easy-to-hard generalization settings, we observe improvements of 28%
$\rightarrow$ 44.6% on MATH, and 37.9% $\rightarrow$ 53.5% on MMLU abstract
algebra. Furthermore, we find that training GenRM with synthetic verification
rationales is sufficient to pick out subtle errors on math problems. Finally,
we demonstrate that GenRM scales favorably with model size and test-time
compute.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”Ÿæˆå¼éªŒè¯å™¨ï¼šæŠŠå¥–åŠ±å»ºæ¨¡å˜æˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼Œè®©å¤§æ¨¡å‹æ¨ç†æ›´èªæ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶èƒ½åŠ›å¾ˆå¼ºï¼Œä½†æ¨ç†æ—¶ç»å¸¸çŠ¯é€»è¾‘æˆ–äº‹å®é”™è¯¯ï¼Œä¸€ä¸ªå°é”™è¯¯å°±å¯èƒ½è®©æ•´ä¸ªè§£å†³æ–¹æ¡ˆå¤±æ•ˆã€‚ä¸ºäº†è§£å†³è¿™é—®é¢˜ï¼Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯ â€œBest - of - Nâ€ï¼šè®©LLMç”ŸæˆNä¸ªå€™é€‰ç­”æ¡ˆï¼Œå†ç”¨â€œéªŒè¯å™¨ï¼ˆverifierï¼‰â€ ç»™è¿™äº›ç­”æ¡ˆæ’åºé€‰æœ€ä¼˜ã€‚ä½†ä¼ ç»ŸåŸºäºLLMçš„éªŒè¯å™¨æ˜¯åˆ¤åˆ«å¼åˆ†ç±»å™¨ï¼Œåªç»™ç­”æ¡ˆæ‰“åˆ†ï¼Œæ²¡ç”¨åˆ°LLMæœ¬èº«çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œæµªè´¹äº†å¤§æ¨¡å‹åœ¨æŒ‡ä»¤å¾®è°ƒã€æ€ç»´é“¾æ¨ç†ç­‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ‰€ä»¥ï¼Œè¿™ç¯‡è®ºæ–‡æƒ³æ¢ä¸ªæ€è·¯ï¼Œè®©éªŒè¯å™¨ä¹Ÿèƒ½åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¼éªŒè¯å™¨ï¼ˆGenRMï¼‰ï¼Œç”¨â€œä¸‹ä¸€ä¸ªè¯é¢„æµ‹â€è®­ç»ƒéªŒè¯å™¨
ä»¥å¾€éªŒè¯å™¨æ˜¯åˆ¤åˆ«å¼çš„ï¼Œç°åœ¨æ”¹æˆè®©éªŒè¯å™¨ç”¨â€œä¸‹ä¸€ä¸ªè¯é¢„æµ‹â€çš„ç›®æ ‡æ¥è®­ç»ƒã€‚æ¯”å¦‚ç»™ä¸ªæç¤ºâ€œç­”æ¡ˆæ­£ç¡®å—ï¼Ÿï¼ˆYes/Noï¼‰â€ï¼ŒéªŒè¯å™¨è¾“å‡ºçš„åˆ†æ•°å°±æ˜¯ç”Ÿæˆâ€œYesâ€æˆ–â€œNoâ€è¿™ä¸ªè¯çš„æ¦‚ç‡ã€‚è¿™æ ·å°±æŠŠéªŒè¯ä»»åŠ¡å˜æˆäº†å¤§æ¨¡å‹æ“…é•¿çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œèƒ½è‡ªç„¶åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šGenRM - CoTï¼Œç»“åˆæ€ç»´é“¾æ¨ç†+å¤šæ•°æŠ•ç¥¨æå‡éªŒè¯ç²¾åº¦
GenRM - CoTåœ¨éªŒè¯æ—¶ï¼Œå…ˆç”Ÿæˆâ€œéªŒè¯æ€ç»´é“¾ï¼ˆCoTï¼‰â€çš„ç†ç”±ï¼Œå†é¢„æµ‹â€œYes/Noâ€ã€‚è®­ç»ƒæ—¶å¦‚æœæœ‰æ€ç»´é“¾çš„ç†ç”±æ•°æ®ï¼Œæ¨¡å‹å°±èƒ½æ˜¾å¼åœ°å»æ¨ç†ç­”æ¡ˆå¯¹é”™ã€‚æ¨ç†æ—¶è¿˜èƒ½é‡‡æ ·å¤šä¸ªæ€ç»´é“¾ç†ç”±ï¼Œç”¨å¤šæ•°æŠ•ç¥¨ç®—â€œYesâ€çš„å¹³å‡æ¦‚ç‡ï¼Œåˆ©ç”¨æ¨ç†æ—¶çš„è®¡ç®—èµ„æºæ¥æå‡éªŒè¯æ•ˆæœã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»Ÿä¸€â€œç”Ÿæˆç­”æ¡ˆâ€å’Œâ€œéªŒè¯ç­”æ¡ˆâ€ï¼Œå€ŸåŠ©ç”Ÿæˆèƒ½åŠ›æ­£å‘è¿ç§»
GenRMç”¨ä¸‹ä¸€ä¸ªè¯é¢„æµ‹è®­ç»ƒï¼Œèƒ½æŠŠâ€œç”Ÿæˆè§£å†³æ–¹æ¡ˆâ€å’Œâ€œéªŒè¯è§£å†³æ–¹æ¡ˆâ€ç»Ÿä¸€èµ·æ¥ï¼Œè¿™åœ¨ä¹‹å‰çš„DPOéªŒè¯å™¨é‡Œå¾ˆéš¾åšåˆ°ï¼Œæœ‰å¯èƒ½é€šè¿‡ç”Ÿæˆä»»åŠ¡çš„æ­£å‘è¿ç§»æ¥æå‡éªŒè¯æ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- åœ¨ç®—æ³•ä»»åŠ¡ï¼ˆLast Letter Concatã€BBH Word Sortingï¼‰ä¸Šï¼ŒBest - of - 32è®¾ç½®ä¸‹ï¼ŒGenRMæŠŠè§£å†³ç‡ä»5%æå‡åˆ°45.3%ï¼›åœ¨GSM8Kæ•°å­¦é¢˜ä¸Šï¼ŒBest - of - 16ä¸‹ä»73%æå‡åˆ°93.4%ï¼Œè¶…è¿‡äº†GPT - 4å’ŒGemini 1.5 Proã€‚
- ä»ç®€å•åˆ°éš¾çš„æ³›åŒ–ä»»åŠ¡é‡Œï¼ŒMATH500ä¸ŠBest - of - 32ä»28%æå‡åˆ°44.6%ï¼›MMLUæŠ½è±¡ä»£æ•°ä»»åŠ¡ä»37.9%æå‡åˆ°53.5%ã€‚
- ç”¨åˆæˆçš„éªŒè¯ç†ç”±è®­ç»ƒGenRMï¼Œèƒ½å‘ç°æ•°å­¦é¢˜é‡Œå¾ˆç»†å¾®çš„é”™è¯¯ï¼ˆæ¯”å¦‚GSM8Ké‡Œå¿½ç•¥â€œeachâ€è¿™ç§ç»†èŠ‚é”™è¯¯ï¼Œåˆ¤åˆ«å¼RMæ²¡å‘ç°ï¼ŒGenRM - CoTèƒ½æ£€æµ‹åˆ°ï¼‰ã€‚
- æ¨¡å‹è§„æ¨¡å’Œæ¨ç†æ—¶è®¡ç®—èµ„æºå¢åŠ æ—¶ï¼ŒGenRMè¡¨ç°æ›´ä¼˜ã€‚æ¯”å¦‚æ¨ç†æ—¶ç”¨å¤šæ•°æŠ•ç¥¨å¢åŠ è®¡ç®—ï¼ŒGenRMæ¯”LLM - as - a - Judgeæ•ˆæœå¥½ï¼›æ¨¡å‹å˜å¤§ï¼ŒGenRMä¹Ÿæ¯”åˆ¤åˆ«å¼éªŒè¯å™¨ scaling å¾—æ›´å¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- éªŒè¯å™¨è®¾è®¡æ–°æ€è·¯ï¼šåˆ«åªæŠŠéªŒè¯å™¨å½“åˆ¤åˆ«å¼åˆ†ç±»å™¨ï¼Œè¦åˆ©ç”¨å¤§æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç”¨â€œä¸‹ä¸€ä¸ªè¯é¢„æµ‹â€è¿™ç§å’Œé¢„è®­ç»ƒä¸€è‡´çš„ç›®æ ‡æ¥è®­ç»ƒï¼Œèƒ½è§£é”å¾ˆå¤šå¤§æ¨¡å‹ç”Ÿæˆä¾§çš„ä¼˜åŠ¿ï¼ˆæŒ‡ä»¤å¾®è°ƒã€æ€ç»´é“¾ç­‰ï¼‰ã€‚
- æ€ç»´é“¾+å¤šæ•°æŠ•ç¥¨çš„éªŒè¯å¢å¼ºï¼šåœ¨éªŒè¯ç¯èŠ‚åŠ å…¥æ€ç»´é“¾æ¨ç†ï¼Œè¿˜èƒ½åœ¨æ¨ç†æ—¶ç”¨å¤šæ•°æŠ•ç¥¨åˆ©ç”¨é¢å¤–è®¡ç®—èµ„æºï¼Œè¿™ä¸ªæ€è·¯å¯ä»¥è¿ç§»åˆ°å…¶ä»–éœ€è¦ç²¾ç»†éªŒè¯ã€é”™è¯¯æ£€æµ‹çš„ä»»åŠ¡é‡Œã€‚
- ä»»åŠ¡ç»Ÿä¸€çš„æ½œåŠ›ï¼šæŠŠç”Ÿæˆå’ŒéªŒè¯ä»»åŠ¡ç”¨ç”Ÿæˆå¼ç›®æ ‡ç»Ÿä¸€èµ·æ¥ï¼Œå¯èƒ½åœ¨æ›´å¤šâ€œç”Ÿæˆ+è¯„ä¼°â€çš„åœºæ™¯é‡Œå‘æŒ¥ä½œç”¨ï¼Œæ¯”å¦‚ä»£ç ç”ŸæˆåéªŒè¯ã€æ–‡æ¡ˆç”Ÿæˆåå®¡æ ¸ç­‰ï¼Œå€ŸåŠ©ç”Ÿæˆçš„æ­£å‘è¿ç§»æå‡è¯„ä¼°æ•ˆæœã€‚
```

## internlm-xcomposer2-5-reward--a-simple-yet-effective-multi-modal-reward-model
### Abstract
Despite the promising performance of Large Vision Language Models (LVLMs) in
visual understanding, they occasionally generate incorrect outputs. While
reward models (RMs) with reinforcement learning or test-time scaling offer the
potential for improving generation quality, a critical gap remains: publicly
available multi-modal RMs for LVLMs are scarce, and the implementation details
of proprietary models are often unclear. We bridge this gap with
InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective
multi-modal reward model that aligns LVLMs with human preferences. To ensure
the robustness and versatility of IXC-2.5-Reward, we set up a high-quality
multi-modal preference corpus spanning text, image, and video inputs across
diverse domains, such as instruction following, general understanding,
text-rich documents, mathematical reasoning, and video understanding.
IXC-2.5-Reward achieves excellent results on the latest multi-modal reward
model benchmark and shows competitive performance on text-only reward model
benchmarks. We further demonstrate three key applications of IXC-2.5-Reward:
(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward
with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows
consistent improvements in instruction following and multi-modal open-ended
dialogue; (2) Selecting the best response from candidate responses for
test-time scaling; and (3) Filtering outlier or noisy samples from existing
image and video instruction tuning training data. To ensure reproducibility and
facilitate further research, we have open-sourced all model weights and
training recipes at
https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward
### ğŸŒŸ è®ºæ–‡è§£è¯» | InternLM-XComposer2.5-Rewardï¼šå¡«è¡¥å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ç©ºç™½çš„é«˜æ•ˆæ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¶å°”ä¹Ÿä¼šç”Ÿæˆé”™è¯¯è¾“å‡ºã€‚å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ç»“åˆå¼ºåŒ–å­¦ä¹ æˆ–æµ‹è¯•æ—¶ç¼©æ”¾è™½æœ‰æå‡ç”Ÿæˆè´¨é‡çš„æ½œåŠ›ï¼Œç„¶è€Œå…¬å¼€å¯ç”¨çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ç¨€ç¼ºï¼Œä¸”ä¸“æœ‰æ¨¡å‹å®ç°ç»†èŠ‚ä¸æ˜ã€‚é’ˆå¯¹æ­¤ï¼Œè®ºæ–‡æå‡ºInternLM - XComposer2.5 - Rewardï¼ˆIXC - 2.5 - Rewardï¼‰æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè®©LVLMsä¸äººç±»åå¥½å¯¹é½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºé«˜è´¨é‡å¤šæ¨¡æ€åå¥½è¯­æ–™åº“
ä¸ºç¡®ä¿IXC - 2.5 - Rewardçš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œæ„å»ºäº†æ¶µç›–æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘è¾“å…¥ï¼Œè·¨æŒ‡ä»¤éµå¾ªã€é€šç”¨ç†è§£ã€æ–‡æœ¬ä¸°å¯Œæ–‡æ¡£ã€æ•°å­¦æ¨ç†ã€è§†é¢‘ç†è§£ç­‰å¤šæ ·é¢†åŸŸçš„é«˜è´¨é‡å¤šæ¨¡æ€åå¥½è¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“æ„å»º pipeline ä¼šä¸ºä¸åŒæ¨¡æ€è¾“å…¥é€‰æ‹©å¤šæ ·é¢†åŸŸçš„æç¤ºï¼Œç”Ÿæˆå¯¹åº”å“åº”ï¼Œå†ç”¨GPT - 4oæˆ–éªŒè¯å™¨åšåå¥½åˆ¤æ–­ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ç®€å•æœ‰æ•ˆçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ¶æ„
ä¸ç›´æ¥å°†å•æ¨¡æ€ï¼ˆæ–‡æœ¬ï¼‰å¥–åŠ±æ¨¡å‹è¿ç§»åˆ°è§†è§‰æ¨¡æ€ï¼Œè€Œæ˜¯åœ¨ç°æœ‰LVLMï¼ˆInternLM - XComposer2.5ï¼‰åŸºç¡€ä¸Šå¢åŠ ä¸€ä¸ªè¯„åˆ†å¤´æ¥é¢„æµ‹å¥–åŠ±åˆ†æ•°ï¼Œä½¿å…¶èƒ½æœ‰æ•ˆè¯„ä¼°è§†è§‰ï¼ˆå›¾åƒå’Œè§†é¢‘ï¼‰å’Œæ–‡æœ¬è¾“å…¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå±•ç¤ºå¥–åŠ±æ¨¡å‹ä¸‰å¤§å…³é”®åº”ç”¨
ä¸€æ˜¯ä¸ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›ç›‘ç£ä¿¡å·ï¼Œå°†IXC - 2.5 - Rewardä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç»“åˆå¾—åˆ°IXC - 2.5 - Chatï¼Œæå‡æŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡æ€å¼€æ”¾å¼å¯¹è¯èƒ½åŠ›ï¼›äºŒæ˜¯åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­ä»å€™é€‰å“åº”é‡Œé€‰æœ€ä½³å“åº”ï¼›ä¸‰æ˜¯ä»ç°æœ‰å›¾åƒå’Œè§†é¢‘æŒ‡ä»¤è°ƒä¼˜è®­ç»ƒæ•°æ®ä¸­è¿‡æ»¤å¼‚å¸¸æˆ–å™ªå£°æ ·æœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
IXC - 2.5 - Rewardåœ¨æœ€æ–°å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹åŸºå‡†VL - RewardBenchä¸Šè¡¨ç°å‡ºè‰²ï¼Œå–å¾—70.0%çš„æˆç»©ï¼Œå‡»è´¥åŒ…æ‹¬Gemini - 1.5 - Proï¼ˆ62.5%ï¼‰å’ŒGPT - 4oï¼ˆ62.4%ï¼‰åœ¨å†…çš„ä¹‹å‰æ‰€æœ‰ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼›åœ¨çº¯æ–‡æœ¬å¥–åŠ±æ¨¡å‹åŸºå‡†ä¸Šä¹Ÿæœ‰ç«äº‰åŠ›ï¼Œåœ¨Reward - Benchä¸Šå¹³å‡å¾—åˆ†88.6%ï¼Œåœ¨RM - Benchä¸Šå¾—åˆ†68.8%ã€‚åŒæ—¶ï¼Œåœ¨RLè®­ç»ƒã€æµ‹è¯•æ—¶ç¼©æ”¾ã€æ•°æ®æ¸…ç†ä¸‰å¤§åº”ç”¨åœºæ™¯çš„å®éªŒä¹ŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¦‚IXC - 2.5 - Chatåœ¨å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªå’Œé‡å¤–èŠå¤©åŸºå‡†ä¸Šæœ‰æ˜æ˜¾æ”¹è¿›ç­‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€æ•°æ®æ„å»ºæ€è·¯ï¼šè®ºæ–‡æ„å»ºè·¨æ¨¡æ€ã€è·¨é¢†åŸŸå¤šæ¨¡æ€åå¥½è¯­æ–™åº“çš„æ–¹å¼ï¼Œä¸ºåç»­å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ•°æ®æ„å»ºæä¾›äº†å‚è€ƒï¼Œå¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å’Œé«˜è´¨é‡æ ‡æ³¨çš„é‡è¦æ€§ã€‚
2. æ¨¡å‹æ‰©å±•ä¸åº”ç”¨æ–¹å‘ï¼šä»å•æ¨¡æ€å¥–åŠ±æ¨¡å‹æ‰©å±•åˆ°å¤šæ¨¡æ€çš„æ€è·¯ï¼Œä»¥åŠå±•ç¤ºçš„å¥–åŠ±æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ã€æµ‹è¯•æ—¶ä¼˜åŒ–ã€æ•°æ®æ¸…ç†ç­‰å¤šåœºæ™¯åº”ç”¨ï¼Œä¸ºå¥–åŠ±æ¨¡å‹ä¹ƒè‡³å¤§æ¨¡å‹ç”Ÿæ€ä¸­ä¸åŒæ¨¡å—çš„åä½œå’ŒåŠŸèƒ½æ‹“å±•æä¾›äº†èŒƒä¾‹ã€‚
3. å¼€æºä¸å¯å¤ç°æ€§ï¼šå°†æ¨¡å‹æƒé‡å’Œè®­ç»ƒæ–¹æ¡ˆå¼€æºï¼Œåˆ©äºç¤¾åŒºåŸºäºæ­¤è¿›ä¸€æ­¥ç ”ç©¶ï¼Œè¿™ç§å¼€æºå…±äº«çš„åšæ³•ä¹Ÿå€¼å¾—ç›¸å…³ç ”ç©¶å€Ÿé‰´ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## warm--on-the-benefits-of-weight-averaged-reward-models
### Abstract
Aligning large language models (LLMs) with human preferences through
reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit
failures in the reward model (RM) to achieve seemingly high rewards without
meeting the underlying objectives. We identify two primary challenges when
designing RMs to mitigate reward hacking: distribution shifts during the RL
process and inconsistencies in human preferences. As a solution, we propose
Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then
averaging them in the weight space. This strategy follows the observation that
fine-tuned weights remain linearly mode connected when sharing the same
pre-training. By averaging weights, WARM improves efficiency compared to the
traditional ensembling of predictions, while improving reliability under
distribution shifts and robustness to preference inconsistencies. Our
experiments on summarization tasks, using best-of-N and RL methods, shows that
WARM improves the overall quality and alignment of LLM predictions; for
example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy
RL fine-tuned with a single RM.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | WARMï¼šæƒé‡å¹³å‡å¥–åŠ±æ¨¡å‹å¦‚ä½•ç ´è§£å¤§æ¨¡å‹å¥–åŠ±é»‘å®¢éš¾é¢˜ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹é½å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‡ç¨‹ä¸­ï¼Œ**å¥–åŠ±é»‘å®¢ï¼ˆReward Hackingï¼‰** é—®é¢˜æ—¥ç›Šå‡¸æ˜¾ï¼šæ¨¡å‹ä¼šåˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰çš„æ¼æ´ï¼Œåœ¨æœªçœŸæ­£æ»¡è¶³ç›®æ ‡çš„æƒ…å†µä¸‹éª—å–é«˜å¥–åŠ±ã€‚è¿™èƒŒåå­˜åœ¨ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ç­–ç•¥æ¼‚ç§»å¼•å‘çš„**åˆ†å¸ƒåç§»**ï¼ˆå¥–åŠ±æ¨¡å‹è¦è¯„ä¼°çš„ç”Ÿæˆåˆ†å¸ƒå’Œè®­ç»ƒæ—¶å·®å¼‚å˜å¤§ï¼‰ï¼›äºŒæ˜¯äººç±»åå¥½æœ¬èº«çš„**ä¸ä¸€è‡´æ€§**ï¼ˆæ ‡æ³¨å™ªå£°ã€è¯„åˆ¤æ ‡å‡†ä¸ç»Ÿä¸€ç­‰ï¼‰ã€‚ä¼ ç»Ÿçš„é¢„æµ‹é›†æˆï¼ˆENSï¼‰è™½èƒ½ä¸€å®šç¨‹åº¦ç¼“è§£ï¼Œä½†å­˜åœ¨å†…å­˜ä¸æ¨ç†å¼€é”€å¤§ã€å¯¹æ ‡æ³¨å™ªå£°é²æ£’æ€§ä¸è¶³ç­‰ç¼ºé™·ã€‚å› æ­¤ï¼ŒäºŸéœ€æ›´é«˜æ•ˆå¯é çš„å¥–åŠ±æ¨¡å‹è®¾è®¡æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºWARMï¼ˆWeight Averaged Reward Modelsï¼‰æ¡†æ¶  
ä»å…±äº«é¢„è®­ç»ƒçš„å¤§æ¨¡å‹å‡ºå‘ï¼Œå…ˆå¯¹å¤šä¸ªå¥–åŠ±æ¨¡å‹åšå¾®è°ƒï¼ˆæ¯”å¦‚ç”¨ä¸åŒè¶…å‚æ•°ã€ä¸åŒæ•°æ®é¡ºåºç­‰æ–¹å¼å¾—åˆ°å¤šæ ·åŒ–RMï¼‰ï¼Œå†åœ¨**æƒé‡ç©ºé—´**å¯¹è¿™äº›RMåšçº¿æ€§å¹³å‡ã€‚è¿™ä¸€æ€è·¯æºäºâ€œçº¿æ€§æ¨¡å¼è¿é€šæ€§ï¼ˆLMCï¼‰â€å‘ç°ï¼šå…±äº«é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¾®è°ƒåçš„æƒé‡å¯è¢«çº¿æ€§æ’å€¼è¿æ¥ã€‚é€šè¿‡æƒé‡å¹³å‡è€Œéé¢„æµ‹ç»“æœå¹³å‡ï¼ŒWARMåœ¨æ¨ç†æ—¶åªéœ€å•ä¸ªæ¨¡å‹ï¼Œå¤§å¹…é™ä½ä¼ ç»Ÿé›†æˆæ–¹æ³•çš„å†…å­˜ä¸è®¡ç®—å¼€é”€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¼é¡¾å¯é æ€§ä¸é²æ£’æ€§çš„ä¸‰é‡ä¼˜åŠ¿  
- **æ•ˆç‡**ï¼šæ¨ç†ä»…éœ€å•æ¨¡å‹ï¼Œæ‘†è„±ä¼ ç»Ÿé¢„æµ‹é›†æˆçš„å†—ä½™è´Ÿæ‹…ï¼›  
- **åˆ†å¸ƒåç§»ä¸‹çš„å¯é æ€§**ï¼šç»§æ‰¿æƒé‡å¹³å‡åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–çš„ä¼˜åŠ¿ï¼Œè®©å¥–åŠ±æ¨¡å‹æ›´ç¨³åœ°è¯„ä¼°ç­–ç•¥æ¼‚ç§»åçš„ç”Ÿæˆï¼›  
- **æ ‡æ³¨å™ªå£°é²æ£’æ€§**ï¼šæƒé‡å¹³å‡èƒ½ç­›é€‰ä¸åŒè®­ç»ƒä¸­â€œä¸å˜çš„é¢„æµ‹æœºåˆ¶â€ï¼Œå‡å°‘å¯¹é”™è¯¯æ ‡æ³¨æ ·æœ¬çš„è®°å¿†ï¼ˆä¼ ç»Ÿé›†æˆæ˜“ç›´æ¥è®°ä½å™ªå£°ï¼‰ï¼Œè¿›è€Œæå‡RLè¿‡ç¨‹ç¨³å®šæ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨**æ‘˜è¦ä»»åŠ¡**ä¸ŠéªŒè¯WARMï¼š  
- å¯¹æ¯”ä¼ ç»Ÿå•RMã€é¢„æµ‹é›†æˆï¼ˆENSï¼‰ç­‰æ–¹æ³•ï¼ŒWARMåœ¨best-of-Nå’ŒRLä¸¤ç±»è®­ç»ƒèŒƒå¼ä¸­ï¼Œå‡æå‡äº†LLMç”Ÿæˆçš„æ•´ä½“è´¨é‡ä¸å¯¹é½åº¦ã€‚ä¾‹å¦‚ï¼Œç”¨WARMå¾®è°ƒçš„RLç­–ç•¥ï¼Œå¯¹å•RMå¾®è°ƒçš„ç­–ç•¥èƒœç‡è¾¾79.4%ï¼›  
- æ¶ˆèå®éªŒæ˜¾ç¤ºï¼Œå¢åŠ æƒé‡å¹³å‡çš„RMæ•°é‡ï¼ˆå³Må¢å¤§ï¼‰ï¼Œèƒ½å»¶ç¼“â€œå¥–åŠ±é»‘å®¢â€å¯¼è‡´çš„æ€§èƒ½å´©æºƒï¼Œè®©è®­ç»ƒä¸­å¥–åŠ±æ›´æŒä¹…åœ°ä¿æŒé«˜ä½ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æƒé‡å¹³å‡çš„èŒƒå¼è¿ç§»**ï¼šå°†â€œæƒé‡å¹³å‡æå‡æ³›åŒ–ä¸é²æ£’æ€§â€çš„æ€è·¯ä» supervised learning æ‹“å±•åˆ°RLHFçš„å¥–åŠ±å»ºæ¨¡ï¼Œä¸ºè§£å†³åˆ†å¸ƒåç§»ã€æ ‡æ³¨å™ªå£°ç­‰éš¾é¢˜æä¾›æ–°èŒƒå¼ï¼›  
2. **é«˜æ•ˆé›†æˆçš„å®è·µè·¯å¾„**ï¼šè¯æ˜â€œæƒé‡ç©ºé—´åˆå¹¶â€æ¯”â€œé¢„æµ‹ç»“æœåˆå¹¶â€æ›´è½»é‡é«˜æ•ˆï¼Œåœ¨å·¥ä¸šçº§å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œå¯é™ä½å¤šæ¨¡å‹é›†æˆçš„æ¨ç†æˆæœ¬ï¼›  
3. **é²æ£’å¥–åŠ±æ¨¡å‹çš„è®¾è®¡é€»è¾‘**ï¼šä»â€œå¯¹æŠ—å¥–åŠ±é»‘å®¢â€çš„è§’åº¦ï¼Œæ‹†è§£åˆ†å¸ƒåç§»ã€åå¥½ä¸ä¸€è‡´ä¸¤å¤§ç—›ç‚¹ï¼Œå¹¶é€šè¿‡æƒé‡å¹³å‡åŒæ—¶å¢å¼ºå¯é æ€§ä¸é²æ£’æ€§ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹ä¼˜åŒ–æŒ‡æ˜æ–¹å‘ã€‚  
```

## skywork-vl-reward--an-effective-reward-model-for-multimodal-understanding-and-reasoning
### Abstract
We propose Skywork-VL Reward, a multimodal reward model that provides reward
signals for both multimodal understanding and reasoning tasks. Our technical
approach comprises two key components: First, we construct a large-scale
multimodal preference dataset that covers a wide range of tasks and scenarios,
with responses collected from both standard vision-language models (VLMs) and
advanced VLM reasoners. Second, we design a reward model architecture based on
Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage
fine-tuning using pairwise ranking loss on pairwise preference data.
Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art
results on multimodal VL-RewardBench and exhibits competitive performance on
the text-only RewardBench benchmark. Furthermore, preference data constructed
based on our Skywork-VL Reward proves highly effective for training Mixed
Preference Optimization (MPO), leading to significant improvements in
multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as
a significant advancement toward general-purpose, reliable reward models for
multimodal alignment. Our model has been publicly released to promote
transparency and reproducibility.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-VL Rewardï¼šå¤šæ¨¡æ€ç†è§£ä¸æ¨ç†çš„é«˜æ•ˆå¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®©å®ƒä»¬çš„è¡Œä¸ºä¸äººç±»åå¥½å¯¹é½ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æ˜¯è§£å†³è¯¥é—®é¢˜çš„å…³é”®ç»„ä»¶ï¼Œç„¶è€Œé’ˆå¯¹çº¯æ–‡æœ¬LLMsçš„å¥–åŠ±æ¨¡å‹ç ”ç©¶è¾ƒå¤šï¼Œå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„å‘å±•è¿˜å¤„äºæ—©æœŸé˜¶æ®µã€‚ç°æœ‰å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹å­˜åœ¨ä¸¤å¤§å±€é™ï¼šç¼ºä¹è·¨å¤šæ ·ä»»åŠ¡çš„é€šç”¨æ€§ï¼Œä¸”éš¾ä»¥æœ‰æ•ˆè¯„ä¼°å…·æœ‰å¤æ‚æ¨ç†çš„å…ˆè¿›VLMæ¨ç†å™¨ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦èƒ½åœ¨å¤šæ ·é¢†åŸŸå’Œä»»åŠ¡ä¸­è¯„ä¼°æ ‡å‡†VLMså’Œå…ˆè¿›VLMæ¨ç†å™¨è¾“å‡ºçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œè¿™å°±æ˜¯æœ¬æ–‡æå‡ºSkywork - VL Rewardçš„åŠ¨æœºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€åå¥½æ•°æ®é›†
æ•´åˆå¤šä¸ªå¼€æºåå¥½æ•°æ®é›†ï¼ˆå¦‚LLaVA - Critic - 113kã€Skywork - Reward - Preference - 80K - v0.2ã€RLAIF - V - Datasetï¼‰å’Œå†…éƒ¨æ ‡æ³¨æ¥æ„å»ºè®­ç»ƒæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–ä»åŸºç¡€å›¾åƒæè¿°åˆ°å¤æ‚æ¨ç†åœºæ™¯çš„å¹¿æ³›ä»»åŠ¡ï¼Œæ”¶é›†çš„åå¥½å¯¹åŒ…å«å›¾åƒï¼ˆé€‚ç”¨æ—¶ï¼‰ã€æ–‡æœ¬æç¤ºå’Œæ¥è‡ªæ ‡å‡†VLMsä¸å…ˆè¿›VLMæ¨ç†å™¨çš„å€™é€‰å“åº”ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›ä¸°å¯Œå¤šæ ·çš„æ•°æ®æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡åŸºäºQwen2.5 - VL - 7B - Instructçš„å¥–åŠ±æ¨¡å‹æ¶æ„ä¸è®­ç»ƒèŒƒå¼
åŸºäºQwen2.5 - VL - 7B - Instructè®¾è®¡å¥–åŠ±æ¨¡å‹æ¶æ„ï¼Œé›†æˆå¥–åŠ±å¤´ï¼Œå¹¶åœ¨æˆå¯¹åå¥½æ•°æ®ä¸Šä½¿ç”¨æˆå¯¹æ’åºæŸå¤±è¿›è¡Œå¤šé˜¶æ®µå¾®è°ƒã€‚é‡‡ç”¨ç»“åˆçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®çš„ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œå¢å¼ºæ¨¡å‹åœ¨å¹¿æ³›å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„æ³›åŒ–æ€§å’Œæ€§èƒ½ï¼Œä½¿æ¨¡å‹èƒ½è¾“å‡ºä¸äººç±»åå¥½å¯¹é½çš„æ ‡é‡åˆ†æ•°ï¼Œæœ‰æ•ˆè¯„ä¼°VLMè¾“å‡ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Skywork - VL Rewardåœ¨å¤šæ¨¡æ€VL - RewardBenchä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨çº¯æ–‡æœ¬çš„RewardBenchåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå±•ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒåŸºäºSkywork - VL Rewardæ„å»ºçš„åå¥½æ•°æ®åœ¨è®­ç»ƒæ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ—¶è¢«è¯æ˜éå¸¸æœ‰æ•ˆï¼Œèƒ½æ˜¾è‘—æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†è¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€å¯¹é½é€šç”¨ã€å¯é å¥–åŠ±æ¨¡å‹æ–¹å‘ä¸Šçš„é‡å¤§è¿›å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºæ–¹é¢ï¼šé€šè¿‡æ•´åˆå¤šæ¥æºæ•°æ®ï¼ˆå¼€æº+å†…éƒ¨æ ‡æ³¨ï¼‰æ¥è¦†ç›–å¹¿æ³›ä»»åŠ¡å’Œåœºæ™¯çš„æ€è·¯ï¼Œä¸ºåç»­å¤šæ¨¡æ€ç›¸å…³æ•°æ®é›†æ„å»ºæä¾›äº†å‚è€ƒï¼Œèƒ½è®©æ¨¡å‹å­¦ä¹ åˆ°æ›´å…¨é¢çš„åå¥½ä¿¡æ¯ã€‚
2. æ¨¡å‹æ¶æ„ä¸è®­ç»ƒæ–¹é¢ï¼šåŸºäºç°æœ‰å¼ºåŸºç¡€æ¨¡å‹ï¼ˆå¦‚Qwen2.5 - VL - 7B - Instructï¼‰è¿›è¡Œæ”¹è¿›ï¼Œæ·»åŠ å¥–åŠ±å¤´å¹¶è®¾è®¡å¤šé˜¶æ®µå¾®è°ƒèŒƒå¼ï¼Œè¿™ç§åˆ©ç”¨å·²æœ‰èµ„æºå¹¶é’ˆå¯¹æ€§æ”¹è¿›ä»¥é€‚é…ä»»åŠ¡çš„æ–¹å¼å€¼å¾—å€Ÿé‰´ï¼Œæœ‰åŠ©äºæå‡æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. åº”ç”¨æ‹“å±•æ–¹é¢ï¼šå±•ç¤ºäº†å¥–åŠ±æ¨¡å‹ç”Ÿæˆçš„åå¥½æ•°æ®åœ¨MPOè®­ç»ƒä¸­æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„ä»·å€¼ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹åç»­çš„ä¼˜åŒ–å’Œèƒ½åŠ›æå‡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ï¼Œå³åˆ©ç”¨ä¼˜è´¨å¥–åŠ±æ¨¡å‹æ¥è¾…åŠ©å…¶ä»–è®­ç»ƒèŒƒå¼ä»¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚
```

## rm-r1--reward-modeling-as-reasoning
### Abstract
Reward modeling is essential for aligning large language models with human
preferences through reinforcement learning from human feedback. To provide
accurate reward signals, a reward model (RM) should stimulate deep thinking and
conduct interpretable reasoning before assigning a score or a judgment.
Inspired by recent advances of long chain-of-thought on reasoning-intensive
tasks, we hypothesize and validate that integrating reasoning capabilities into
reward modeling significantly enhances RMs interpretability and performance. To
this end, we introduce a new class of generative reward models - Reasoning
Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task.
We propose a reasoning-oriented training pipeline and train a family of
ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -
self-generating sample-level chat rubrics or math/code solutions, and
evaluating candidate responses against them. The training of RM-R1 consists of
two key stages: (1) distillation of high-quality reasoning chains and (2)
reinforcement learning with verifiable rewards. Empirically, our models achieve
state-of-the-art performance across three reward model benchmarks on average,
outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and
proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we
perform thorough empirical analyses to understand the key ingredients of
successful ReasRM training. To facilitate future research, we release six
REASRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RM-R1ï¼šå°†å¥–åŠ±å»ºæ¨¡é‡å¡‘ä¸ºæ¨ç†ä»»åŠ¡ï¼Œè§£é”å¥–åŠ±æ¨¡å‹æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼ŒRMï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œæ˜¯äººç±»è¯„ä¼°è€…çš„å¯æ‰©å±•ä»£ç†ã€‚ç°æœ‰å¥–åŠ±å»ºæ¨¡ç ”ç©¶åˆ†ä¸ºåŸºäºæ ‡é‡çš„å¥–åŠ±æ¨¡å‹ï¼ˆScalarRMï¼‰å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ä¸¤ç±»ã€‚ScalarRM æŠŠå¥–åŠ±å»ºæ¨¡å½“åˆ†ç±»é—®é¢˜ï¼Œè™½ç›´æ¥æœ‰æ•ˆä½†ç¼ºä¹é€æ˜åº¦ï¼Œæ— ä¸­é—´æ¨ç†æ­¥éª¤è§£é‡Šå†³ç­–ï¼›GenRM ä¿ç•™ç”Ÿæˆèƒ½åŠ›è¾“å‡ºè‡ªç”±å½¢å¼ pairwise åˆ¤æ–­ï¼Œå´æ¨ç†æµ…æ˜¾ï¼Œéš¾åº”å¯¹å¤æ‚æ¨ç†å¯†é›†å‹åå¥½ä»»åŠ¡ã€‚ç°å®ä¸­å‡†ç¡®å¥–åŠ±å»ºæ¨¡éœ€è”åˆæ¨ç†ä¸å¥–åŠ±åˆ†é…ï¼Œå› åå¥½åˆ¤æ–­æ¶‰åŠå¤šè®¤çŸ¥è€ƒé‡ï¼ˆå¦‚æ¨æ–­æ½œåœ¨è¯„ä¼°æ ‡å‡†ã€æƒè¡¡å¤šæ ‡å‡†ã€æ¨¡æ‹Ÿåæœç­‰ï¼‰ã€‚ç”±æ­¤ï¼Œæœ¬æ–‡æ ¸å¿ƒé—®é¢˜æ˜¯ï¼šèƒ½å¦å°†å¥–åŠ±å»ºæ¨¡è½¬åŒ–ä¸ºæ¨ç†ä»»åŠ¡ï¼Ÿ

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasoning Reward Modelsï¼ŒReasRMsï¼‰æ–°ç±»åˆ«  
å°†å¥–åŠ±å»ºæ¨¡æ„å»ºä¸ºæ¨ç†ä»»åŠ¡ï¼Œå¼ºè°ƒåœ¨åˆ¤æ–­è¿‡ç¨‹åˆ©ç”¨é•¿ä¸”è¿è´¯çš„æ¨ç†é“¾ï¼Œæå‡æ¨¡å‹å‡†ç¡®è¯„ä¼°åŒºåˆ†å¤æ‚è¾“å‡ºçš„èƒ½åŠ›ï¼ŒåŒºåˆ«äºä¼ ç»Ÿ GenRMs æ¨ç†æµ…æ˜¾çš„é—®é¢˜ï¼Œé‡Šæ”¾å¥–åŠ±æ¨¡å‹æ¨ç†æ½œåŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡é¢å‘æ¨ç†çš„è®­ç»ƒ pipeline ä¸ RM - R1 æ¨¡å‹  
è®­ç»ƒåˆ†ä¸¤å¤§å…³é”®é˜¶æ®µï¼šä¸€æ˜¯é«˜è´¨é‡æ¨ç†é“¾è’¸é¦ï¼Œç”¨ä¼˜è´¨åˆæˆæ•°æ®å¼•å¯¼æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼›äºŒæ˜¯å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚åŒæ—¶ï¼ŒRM - R1 å…·å¤‡ Chain - of - Rubricsï¼ˆCoRï¼‰æœºåˆ¶ï¼šé’ˆå¯¹è¾“å…¥æ ·æœ¬åˆ†ç±»ä¸ºèŠå¤©æˆ–æ¨ç†ä»»åŠ¡ï¼ŒèŠå¤©ä»»åŠ¡ç”Ÿæˆè¯„ä¼°å‡†åˆ™ã€ç†ç”±ä¸å®šåˆ¶åŒ–è¯„ä¼°ï¼›æ¨ç†ä»»åŠ¡å…ˆè‡ªä¸»è§£é¢˜å†è¯„ä¼°ï¼Œè®©æ¨¡å‹é€‚é…ä¸åŒä»»åŠ¡ç­–ç•¥ï¼Œç”Ÿæˆæ›´å¯¹é½æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚è¿˜æ¢ç´¢ä»ç°æœ‰æ¨ç†æ¨¡å‹é€‚é…ä¸ºå¥–åŠ±æ¨¡å‹ï¼Œå¯¹å·²åšæ¨ç†è’¸é¦çš„æ¨¡å‹ç”¨ RLVR å¾®è°ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ RewardBenchã€RM - Benchã€RMB ä¸‰å¤§å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRM - R1 å¹³å‡è¾¾å½“å‰æœ€ä¼˜æ€§èƒ½ï¼Œæœ€å¤šè¶…è¿‡ 70Bã€340B ç­‰å¤§å¼€æºæ¨¡å‹åŠ GPT - 4oã€Claude ç­‰é—­æºæ¨¡å‹ 4.9%ã€‚æ­¤å¤–ï¼Œå¯¹ RM - R1 åšäº†è®­ç»ƒé…æ–¹æ¶ˆèã€ç¼©æ”¾æ•ˆåº”ç ”ç©¶ã€ä¸éæ¨ç†åŸºçº¿å¯¹æ¯”ã€è¯¦ç»†æ¡ˆä¾‹åˆ†æå’Œè®­ç»ƒåŠ¨æ€ç­‰å¹¿æ³›å®è¯åˆ†æï¼Œæ·±å…¥ç†è§£æˆåŠŸè®­ç»ƒ ReasRM çš„å…³é”®è¦ç´ ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç†å¿µå±‚é¢ï¼šéªŒè¯äº†æ¨ç†èƒ½åŠ›å¯¹å¥–åŠ±æ¨¡å‹çš„å…³é”®ä»·å€¼ï¼Œä¸ºå¥–åŠ±å»ºæ¨¡å¼€è¾Ÿâ€œæ¨ç†ä¼˜å…ˆâ€æ–°è§†è§’ï¼Œå¯å‘åç»­æ¢ç´¢æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸å¥–åŠ±ä¿¡å·è´¨é‡çš„å…³è”ã€‚  
2. æ–¹æ³•å±‚é¢ï¼šè®¾è®¡çš„ä¸¤é˜¶æ®µè®­ç»ƒ pipelineï¼ˆæ¨ç†é“¾è’¸é¦ + RLVRï¼‰ã€CoR ä»»åŠ¡æ„ŸçŸ¥æ¨ç†æœºåˆ¶ï¼Œä¸ºæ‰“é€ é«˜æ€§èƒ½å¯è§£é‡Šå¥–åŠ±æ¨¡å‹æä¾›äº†æ¨¡å—åŒ–ã€å¯å¤ç°çš„æŠ€æœ¯è·¯çº¿ã€‚  
3. å¼€æºå±‚é¢ï¼šå‘å¸ƒ 6 ä¸ª ReasRM æ¨¡å‹åŠä»£ç æ•°æ®ï¼Œé™ä½é¢†åŸŸç ”ç©¶é—¨æ§›ï¼Œåˆ©äºç¤¾åŒºåŸºäºæ­¤è¿­ä»£åˆ›æ–°ï¼Œæ¨åŠ¨å¥–åŠ±å»ºæ¨¡ä¸ RLHF æŠ€æœ¯å‘å±•ã€‚  
```

## diffusion-reward--learning-rewards-via-conditional-video-diffusion
### Abstract
Learning rewards from expert videos offers an affordable and effective
solution to specify the intended behaviors for reinforcement learning (RL)
tasks. In this work, we propose Diffusion Reward, a novel framework that learns
rewards from expert videos via conditional video diffusion models for solving
complex visual RL problems. Our key insight is that lower generative diversity
is exhibited when conditioning diffusion on expert trajectories. Diffusion
Reward is accordingly formalized by the negative of conditional entropy that
encourages productive exploration of expert behaviors. We show the efficacy of
our method over robotic manipulation tasks in both simulation platforms and the
real world with visual input. Moreover, Diffusion Reward can even solve unseen
tasks successfully and effectively, largely surpassing baseline methods.
Project page and code: https://diffusion-reward.github.io.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ã€ŠDiffusion Rewardï¼šé€šè¿‡æ¡ä»¶è§†é¢‘æ‰©æ•£å­¦ä¹ å¥–åŠ±ã€‹ï¼šå¼€å¯è§†è§‰å¼ºåŒ–å­¦ä¹ æ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œå¥–åŠ±è®¾å®šæ˜¯ä¸€é¡¹æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œå®ƒå†³å®šäº†æ™ºèƒ½ä½“å­¦ä¹ è¡Œä¸ºä¸é¢„æœŸç›®æ ‡çš„æœ‰æ•ˆæ€§å’Œä¸€è‡´æ€§ã€‚æ‰‹åŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°æ—¢è€—è´¹äººåŠ›ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³ä¸å¯è¡Œï¼Œæ¯”å¦‚åœ¨æœºå™¨äººæ“ä½œè¿™ç±»ç°å®ä»»åŠ¡ä¸­ï¼Œè·å–ç‰¹æƒçŠ¶æ€ä¿¡æ¯ååˆ†å›°éš¾ã€‚ä½¿ç”¨ç¨€ç–å¥–åŠ±è™½åœ¨è¿™äº›åœºæ™¯ä¸­å› äººå·¥æŠ•å…¥ä½è€Œå—åˆ°é’çï¼Œä½†å®ƒæä¾›çš„ç›‘ç£ä¸è¶³ï¼Œé˜»ç¢äº†RLçš„æ ·æœ¬æ•ˆç‡ï¼Œå¯èƒ½ç»™æ•°æ®æ”¶é›†å¸¦æ¥éš¾ä»¥æ‰¿å—çš„è´Ÿæ‹…ã€‚ä»ä¸“å®¶è§†é¢‘ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºè§†é¢‘æ”¶é›†å·¥ä½œé‡å°ä¸”åŒ…å«å¯†é›†çš„ä»»åŠ¡æ‰§è¡Œä¿¡æ¯ï¼Œä½†ç°æœ‰çš„ä¸€äº›æ–¹æ³•å­˜åœ¨å¯¹æ—¶é—´ä¿¡æ¯åˆ©ç”¨ä¸è¶³ã€å¯¹æŠ—è®­ç»ƒæ€§èƒ½è„†å¼±ï¼Œä»¥åŠåœ¨å»ºæ¨¡å¤æ‚ä¸“å®¶è§†é¢‘åˆ†å¸ƒæ—¶é‡åˆ°å›°éš¾ç­‰é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDiffusion Rewardæ¡†æ¶ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ•æ‰ä¸“å®¶è§†é¢‘åˆ†å¸ƒå¹¶é¢„è®­ç»ƒå¯†é›†å¥–åŠ±å‡½æ•°ç”¨äºè§†è§‰RLã€‚å…¶å…³é”®è§è§£æ˜¯ï¼ŒåŸºäºç±»ä¼¼ä¸“å®¶è½¨è¿¹çš„æ‰©æ•£æ¯”åŸºäºéä¸“å®¶è½¨è¿¹çš„æ‰©æ•£å…·æœ‰æ›´ä½çš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œé€šè¿‡ä¼°è®¡åŸºäºå†å²å¸§çš„æ¡ä»¶ç†µæ¥æŒ‡å¯¼RLæ¢ç´¢ç±»ä¼¼ä¸“å®¶çš„è¡Œä¸ºã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°†æ¡ä»¶ç†µä¸å¯»æ±‚æ–°å¥‡å¥–åŠ±å’Œå¤‡ç”¨ç¯å¢ƒå¥–åŠ±ç›¸ç»“åˆï¼Œå½¢æˆå¯†é›†å¥–åŠ±ä»¥å®ç°é«˜æ•ˆRLã€‚æ­¤å¤–ï¼Œä¸ºåŠ é€Ÿå¥–åŠ±æ¨æ–­ï¼Œåˆ©ç”¨çŸ¢é‡é‡åŒ–ä»£ç è¿›è¡Œæ½œåœ¨æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥å‹ç¼©é«˜ç»´è§‚æµ‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡åœ¨10ä¸ªè§†è§‰æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒï¼ŒéªŒè¯äº†Diffusion Rewardæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬MetaWorldçš„7ä¸ªæŠ“å–æ“ä½œä»»åŠ¡å’ŒAdroitçš„3ä¸ªçµå·§æ“ä½œä»»åŠ¡ï¼Œä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½åˆ†åˆ«æé«˜äº†38%å’Œ35%ã€‚åŒæ—¶ï¼Œåœ¨æœªè§ä»»åŠ¡ä¸Šè§‚å¯Ÿåˆ°äº†è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ï¼Œåœ¨çœŸå®æœºå™¨äººä»»åŠ¡ä¸­äº§ç”Ÿçš„åˆç†å¥–åŠ±ä»¥åŠç›¸å…³çš„è‰¯å¥½ç¦»çº¿RLæ€§èƒ½ï¼Œä¹Ÿè¯æ˜äº†Diffusion Rewardåœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å¥–åŠ±å­¦ä¹ æ–°æ€è·¯**ï¼šåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä»ä¸“å®¶è§†é¢‘ä¸­å­¦ä¹ å¥–åŠ±ï¼Œä¸ºè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±è®¾å®šéš¾é¢˜æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå¯å¯å‘åœ¨å…¶ä»–å¤æ‚ä»»åŠ¡ä¸­çš„å¥–åŠ±å­¦ä¹ ç ”ç©¶ã€‚
2. **æ¨¡å‹ç»“åˆåº”ç”¨**ï¼šå°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå±•ç¤ºäº†ä¸åŒæ¨¡å‹èåˆåœ¨è§£å†³å®é™…é—®é¢˜æ—¶çš„æ½œåŠ›ï¼Œä¸ºå¤šæ¨¡å‹è”åˆåº”ç”¨æä¾›äº†å‚è€ƒã€‚
3. **é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨æœªè§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ï¼Œè¿™å¯¹äºåº”å¯¹ç°å®ä¸­ä¸æ–­å˜åŒ–çš„ä»»åŠ¡éœ€æ±‚å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç›¸å…³æ€è·¯å¯ç”¨äºæå‡æ¨¡å‹åœ¨æœªçŸ¥åœºæ™¯ä¸‹çš„é€‚åº”æ€§ã€‚
``` 

## rrm--robust-reward-model-training-mitigates-reward-hacking
### Abstract
Reward models (RMs) play a pivotal role in aligning large language models
(LLMs) with human preferences. However, traditional RM training, which relies
on response pairs tied to specific prompts, struggles to disentangle
prompt-driven preferences from prompt-independent artifacts, such as response
length and format. In this work, we expose a fundamental limitation of current
RM training methods, where RMs fail to effectively distinguish between
contextual signals and irrelevant artifacts when determining preferences. To
address this, we introduce a causal framework that learns preferences
independent of these artifacts and propose a novel data augmentation technique
designed to eliminate them. Extensive experiments show that our approach
successfully filters out undesirable artifacts, yielding a more robust reward
model (RRM). Our RRM improves the performance of a pairwise reward model
trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to
84.15%. Additionally, we train two DPO policies using both the RM and RRM,
demonstrating that the RRM significantly enhances DPO-aligned policies,
improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in
AlpacaEval-2 from 33.46% to 52.49%.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RRMï¼šå¯¹æŠ—Reward Hackingçš„é²æ£’å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»åå¥½çš„æŠ€æœ¯æ ˆä¸­ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯å…³é”®ç¯èŠ‚ï¼Œè€Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åˆ™æ˜¯RLHFé‡Œçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ã€‚ä¼ ç»ŸRMè®­ç»ƒä¾èµ–ä¸ç‰¹å®špromptç»‘å®šçš„å“åº”å¯¹ï¼Œä½†å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§ç¼ºé™·ï¼š**éš¾ä»¥åŒºåˆ†â€œç”±prompté©±åŠ¨çš„çœŸå®åå¥½ä¿¡å·â€å’Œâ€œä¸promptæ— å…³çš„å¹²æ‰°é¡¹ï¼ˆå¦‚å“åº”é•¿åº¦ã€æ ¼å¼ã€ç‰¹å®šn - gramæˆ–è¡¨æƒ…ç­‰ï¼‰â€**ã€‚è¿™å°±å¯¼è‡´äº†â€œå¥–åŠ±é»‘å®¢ï¼ˆReward Hackingï¼‰â€é—®é¢˜â€”â€”æ¨¡å‹ä¸ºäº†æœ€å¤§åŒ–å¥–åŠ±ï¼Œä¼šåˆ©ç”¨è¿™äº›å¹²æ‰°é¡¹â€œèµ°æ·å¾„â€ï¼Œè€ŒéçœŸæ­£å¯¹é½äººç±»æ„å›¾ã€‚æ¯”å¦‚LLMå¯èƒ½é€šè¿‡ç”Ÿæˆæ›´é•¿çš„å›ç­”æ¥éª—å–é«˜åˆ†ï¼Œå› ä¸ºäººç±»è¯„ä¼°è€…å¾€å¾€å¯¹é•¿å†…å®¹å­˜åœ¨åå‘æ€§ï¼›é™¤äº†é•¿åº¦ï¼Œæ ¼å¼ï¼ˆmarkdownã€åŠ ç²—ï¼‰ç­‰ä¹Ÿæ˜¯å¸¸è§çš„è¢«åˆ©ç”¨ç‚¹ã€‚ç°æœ‰ä¸€äº›é’ˆå¯¹é•¿åº¦åè§çš„æ”¹è¿›ï¼ˆå¦‚ç»™é•¿åº¦åŠ æƒ©ç½šã€å­¦ä¹ ä¸é•¿åº¦æ­£äº¤çš„è´¨é‡å¥–åŠ±ï¼‰ï¼Œä½†æ— æ³•è¦†ç›–æ‰€æœ‰æ½œåœ¨çš„å¹²æ‰°æ¨¡å¼ï¼Œä¸”å¾ˆéš¾æ˜¾å¼æ§åˆ¶æ‰€æœ‰å¹²æ‰°é¡¹ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©RMæ›´é²æ£’åœ°å­¦ä¹ çœŸå®åå¥½ã€æŠµå¾¡å„ç±»Reward Hackingï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå› æœæ¡†æ¶ä¸‹çš„åå¥½å»ºæ¨¡  
è®ºæ–‡ä»**å› æœè§†è§’**é‡æ–°å®¡è§†RMè®­ç»ƒé—®é¢˜ã€‚å°†äººç±»å¯¹ï¼ˆprompt xï¼Œå“åº”å¯¹(yâ‚,yâ‚‚)ï¼‰çš„åå¥½æ‹†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šä¸€æ˜¯ä¸promptç›¸å…³çš„çœŸå®è´¨é‡ä¿¡å·s(x, yâ‚, yâ‚‚)ï¼ŒäºŒæ˜¯ä¸promptæ— å…³çš„å¹²æ‰°é¡¹a(yâ‚, yâ‚‚)ï¼ˆå¦‚é•¿åº¦ã€æ ¼å¼ç­‰ï¼‰ã€‚ä¼ ç»ŸRMè®­ç»ƒæ— æ³•åŒºåˆ†è¿™ä¸¤ä¸ªå› ç´ ï¼Œè€Œè®ºæ–‡æå‡ºçš„å› æœæ¡†æ¶ï¼Œæ—¨åœ¨è®©RMå­¦ä¹ â€œå‰¥ç¦»å¹²æ‰°é¡¹åçš„çœŸå®åå¥½â€ã€‚é€šè¿‡å¼•å…¥åäº‹å®promptï¼ˆå³æ¥è‡ªå…¶ä»–ç¤ºä¾‹çš„promptï¼‰ç­‰æ€è·¯ï¼Œä¼°è®¡åå¥½æ•°æ®é›†ä¸­çš„â€œå¹²æ‰°é¡¹åè§â€ï¼Œä»è€Œä¸ºåç»­æ•°æ®å¢å¼ºå’Œæ¨¡å‹è®­ç»ƒæä¾›ç†è®ºæ”¯æ’‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå› æœè§„åˆ™çš„æ•°æ®å¢å¼ºæŠ€æœ¯  
ä¸ºäº†è®©RMåœ¨è®­ç»ƒæ—¶æ›´èšç„¦äºçœŸå®åå¥½ã€è¿‡æ»¤å¹²æ‰°é¡¹ï¼Œè®ºæ–‡è®¾è®¡äº†**æ–°é¢–çš„æ•°æ®å¢å¼ºæ–¹æ³•**ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ„å»ºRMè®­ç»ƒæ•°æ®æ—¶ï¼Œå¼•å…¥å…¶ä»–ç¤ºä¾‹çš„å“åº”æ¥â€œå¹³è¡¡â€é€‰ä¸­ï¼ˆchosenï¼‰å’Œæ‹’ç»ï¼ˆrejectedï¼‰å“åº”ä¸­çš„å¹²æ‰°é¡¹ã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œé€šè¿‡è¿™ç§æ–¹å¼ï¼Œè®©è®­ç»ƒæ•°æ®é‡Œçš„é€‰ä¸­å’Œæ‹’ç»å“åº”åœ¨å¹²æ‰°é¡¹åˆ†å¸ƒä¸Šæ›´å‡è¡¡ï¼Œè¿«ä½¿RMä¸èƒ½å†ä¾èµ–è¿™äº›å¹²æ‰°é¡¹æ¥åˆ¤æ–­åå¥½ï¼Œåªèƒ½å­¦ä¹ ä¸promptç›¸å…³çš„çœŸå®è´¨é‡ä¿¡å·ã€‚æœ€ç»ˆè®­ç»ƒå‡ºé²æ£’å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡å¤šç»´åº¦å®éªŒéªŒè¯RRMçš„æœ‰æ•ˆæ€§ï¼š  
- **RewardBenchåŸºå‡†æµ‹è¯•**ï¼šåœ¨åŸºäºGemma - 2 - 9b - itè®­ç»ƒçš„ pairwise reward model ä¸Šï¼ŒRRMå°†å‡†ç¡®ç‡ä»80.61%æå‡è‡³84.15%ï¼Œè¯æ˜RRMèƒ½æ›´å‡†ç¡®åŒºåˆ†ä¼˜è´¨å“åº”ã€‚  
- **DPOç­–ç•¥è®­ç»ƒå¯¹æ¯”**ï¼šåˆ†åˆ«ç”¨åŸå§‹RMå’ŒRRMè®­ç»ƒDPOç­–ç•¥ï¼ŒRRMè®­ç»ƒçš„ç­–ç•¥åœ¨MT - Benchä¸Šçš„åˆ†æ•°ä»7.27æå‡åˆ°8.31ï¼›åœ¨AlpacaEval - 2çš„é•¿åº¦æ§åˆ¶èƒœç‡ä»33.46%æå‡è‡³52.49%ã€‚è¿™è¡¨æ˜RRMèƒ½æ˜¾è‘—å¢å¼ºå¯¹é½åçš„ç­–ç•¥æ€§èƒ½ï¼Œè®©æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„ç»“æœã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å› æœè§†è§’çš„é—®é¢˜å»ºæ¨¡**ï¼šå½“å¤„ç†å¤æ‚çš„â€œä¿¡å· - å¹²æ‰°â€åˆ†ç¦»é—®é¢˜æ—¶ï¼Œå¼•å…¥å› æœæ¡†æ¶æ˜¯ä¸€ç§å¾ˆæœ‰æ½œåŠ›çš„æ€è·¯ã€‚å®ƒèƒ½å¸®åŠ©æˆ‘ä»¬æ›´æ¸…æ™°æ‹†è§£é—®é¢˜æœ¬è´¨ï¼Œä¸ºè§£å†³æ–¹æ¡ˆè®¾è®¡æä¾›ç†è®ºé”šç‚¹ã€‚  
2. **æ•°æ®å¢å¼ºçš„é’ˆå¯¹æ€§è®¾è®¡**ï¼šé’ˆå¯¹â€œå¹²æ‰°é¡¹å¯¼è‡´æ¨¡å‹èµ°æ·å¾„â€çš„é—®é¢˜ï¼Œè®ºæ–‡æ²¡æœ‰ç›²ç›®å¢åŠ æ•°æ®é‡ï¼Œè€Œæ˜¯**åŸºäºé—®é¢˜æœ¬è´¨ï¼ˆå¹²æ‰°é¡¹å‡è¡¡ï¼‰è®¾è®¡æ•°æ®å¢å¼º**ï¼Œè¿™ç§â€œç²¾å‡†æ‰“å‡»â€å¼çš„æ•°æ®å¤„ç†æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚åœ¨å…¶ä»–éœ€è¦æŠµå¾¡æ¨¡å‹â€œæŠ•æœºå–å·§â€çš„ä»»åŠ¡ï¼ˆå¦‚å¯¹æŠ—é²æ£’æ€§ã€å»åå­¦ä¹ ç­‰ï¼‰ä¸­ï¼Œå¯æ€è€ƒå¦‚ä½•æ„é€ ç±»ä¼¼çš„å¢å¼ºç­–ç•¥ã€‚  
3. **å¤šç»´åº¦çš„å®éªŒéªŒè¯**ï¼šä»å¥–åŠ±æ¨¡å‹æœ¬èº«çš„å‡†ç¡®ç‡ï¼Œåˆ°ä¸‹æ¸¸å¯¹é½ç­–ç•¥çš„æ€§èƒ½ï¼ˆMT - Benchã€AlpacaEval - 2ç­‰ä¸åŒåŸºå‡†ï¼‰ï¼Œå…¨é¢éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ã€‚è¿™ç§â€œä»ç»„ä»¶åˆ°ç³»ç»Ÿâ€çš„å®éªŒè®¾è®¡é€»è¾‘ï¼Œèƒ½æ›´æœ‰åŠ›åœ°è¯æ˜æŠ€æœ¯ä»·å€¼ï¼Œåœ¨ç§‘ç ”å’Œå·¥ç¨‹å®è·µä¸­éƒ½å€¼å¾—å‚è€ƒã€‚  
```

## rewardanything--generalizable-principle-following-reward-models
### Abstract
Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RewardAnythingï¼šè®©å¥–åŠ±æ¨¡å‹çªç ´å›ºå®šåå¥½ï¼Œçµæ´»éµå¾ªè‡ªç„¶è¯­è¨€åŸåˆ™

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½äººç±»åå¥½æ˜¯å…³é”®æŒ‘æˆ˜ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ä½œä¸ºRLHFç­‰å¯¹é½æŠ€æœ¯çš„æ ¸å¿ƒï¼Œå½“å‰è®­ç»ƒæ–¹å¼å­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼šä¸€æ˜¯æ³›åŒ–ä¸é€‚åº”æ€§å—é™ï¼Œä¾èµ–å›ºå®šåå¥½æ•°æ®é›†è®­ç»ƒçš„RMséš¾ä»¥é€‚é…ä¸åŒçœŸå®åœºæ™¯ï¼ˆå¦‚å®¢æœéœ€ç®€æ´ã€ç§‘ç ”åŠ©æ‰‹éœ€è¯¦ç»†ï¼‰ï¼Œé‡æ–°æ”¶é›†æ•°æ®å†è®­ç»ƒæˆæœ¬é«˜ï¼›äºŒæ˜¯éšå¼åå¥½å­¦ä¹ å¸¦æ¥åå·®ä¸å¯è§£é‡Šæ€§éš¾é¢˜ï¼Œç°æœ‰RMsä»äººç±»æ ‡æ³¨åå¥½æ•°æ®å­¦ä¹ ï¼Œå¤šä»…ä¿ç•™ç»“æœç›‘ç£ï¼Œæ˜“è®©æ¨¡å‹é€šè¿‡è™šå‡å…³è”æ¨æ–­éšå¼ä»·å€¼ï¼Œå¯¼è‡´å¥–åŠ±åå·®ä¸”å¯è§£é‡Šæ€§å·®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºè½¬å‘**éµå¾ªåŸåˆ™çš„å¥–åŠ±æ¨¡å‹**ï¼Œè®©RMsèƒ½åŸºäºåŠ¨æ€è‡ªç„¶è¯­è¨€åŸåˆ™è°ƒæ•´å¥–åŠ±æ ‡å‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºâ€œéµå¾ªåŸåˆ™çš„å¥–åŠ±æ¨¡å‹â€æ¦‚å¿µèŒƒå¼  
æ˜ç¡®RMsåº”åƒLLMséµå¾ªæŒ‡ä»¤ä¸€æ ·ï¼Œç†è§£å¹¶éµå¾ªåŠ¨æ€æä¾›çš„è‡ªç„¶è¯­è¨€å¥–åŠ±åŸåˆ™æè¿°ï¼Œæ— éœ€ä¸ºæ¯ä¸ªåå¥½åœºæ™¯è®­ç»ƒæ–°æ¨¡å‹ï¼Œä½¿RMsæˆä¸ºè·¨å¤šæ ·åå¥½åœºæ™¯æ³›åŒ–çš„çµæ´»å·¥å…·ï¼Œä¸ºæ„å»ºé€‚é…å¤šå˜äººç±»åå¥½çš„AIç³»ç»Ÿç­‘ç‰¢èƒ½åŠ›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºRABenchåŸºå‡†æµ‹è¯•é›†  
æ‰“é€ å…¨é¢çš„RMsåŸºå‡†RABenchï¼Œèšç„¦è¯„ä¼°RMså¯¹æ–°é¢–è‡ªç„¶è¯­è¨€åŸåˆ™çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦†ç›–å¤šé¢†åŸŸã€æš´éœ²å½“å‰RMså±€é™ï¼Œä¸ºè¡¡é‡â€œéµå¾ªåŸåˆ™â€èƒ½åŠ›è¿›å±•æä¾›æ ‡å°ºï¼Œå¡«è¡¥é¢†åŸŸå†…ç³»ç»Ÿè¯„ä¼°ç©ºç™½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç ”å‘RewardAnythingæ¨¡å‹  
è®¾è®¡å¹¶è®­ç»ƒRewardAnythingè¿™ä¸€ç”Ÿæˆå¼RMï¼Œç»“åˆGRPOä¸Group Relative Preference LearningæŠ€æœ¯ï¼Œè®©æ¨¡å‹èƒ½åœ¨æ¨ç†æ—¶é«˜æ•ˆè§£è¯»ã€åº”ç”¨å¤šæ ·åå¥½åŸåˆ™ï¼Œå®ç°å¯¹å“åº”ç»„çš„é«˜æ•ˆæ’åºä¸æ‰“åˆ†ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šé‡è®­å°±èƒ½ä¿æŒé«˜è´¨é‡åå¥½åˆ¤æ–­ï¼Œè¿˜èƒ½é«˜æ•ˆæ”¯æ’‘PPOã€GRPOç­‰RLè®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¼ ç»ŸRMåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…é€šè¿‡æŒ‡å®šæ¸…æ™°åŸåˆ™ï¼ŒRewardAnythingå°±èƒ½å–å¾—SOTAæ€§èƒ½ï¼›åœ¨RABenchä¸Šï¼Œå±•ç°å‡ºæ— éœ€é‡è®­å°±èƒ½é€‚é…æ–°åŸåˆ™çš„å“è¶Šèƒ½åŠ›ï¼›æ­¤å¤–ï¼Œæ¡ˆä¾‹ç ”ç©¶éªŒè¯å…¶èƒ½ä»…ä»¥è‡ªç„¶è¯­è¨€åŸåˆ™ä¸ºæŒ‡å¯¼è‡ªåŠ¨é«˜æ•ˆå¯¹é½LLMï¼Œåœ¨ç»†å¾®å®‰å…¨ã€å¸®åŠ©æ€§ã€å“åº”è´¨é‡ç­‰ç»´åº¦å®ç°æ˜¾è‘—æå‡ï¼Œæœ‰åŠ›è¯æ˜â€œéµå¾ªåŸåˆ™â€èŒƒå¼åœ¨LLMé«˜æ•ˆçµæ´»å¯¹é½ä¸Šçš„ä»·å€¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°å±‚é¢ï¼šå°†â€œéµå¾ªæŒ‡ä»¤â€æ€è·¯è¿ç§»åˆ°å¥–åŠ±æ¨¡å‹ï¼Œæå‡ºâ€œéµå¾ªè‡ªç„¶è¯­è¨€åŸåˆ™â€çš„æ–°èŒƒå¼ï¼Œä¸ºæ‰“ç ´RMså›ºå®šåå¥½æŸç¼šã€é€‚é…å¤šæ ·çœŸå®åœºæ™¯æä¾›æ–¹å‘ï¼Œå¯å‘åç»­å¯¹é½æŠ€æœ¯ä»â€œé™æ€åå¥½æ‹Ÿåˆâ€è½¬å‘â€œåŠ¨æ€åŸåˆ™éµå¾ªâ€ã€‚  
2. åŸºå‡†æ„å»ºå±‚é¢ï¼šRABenchä¸ºè¯„ä¼°RMsæ³›åŒ–åˆ°æ–°åŸåˆ™çš„èƒ½åŠ›æä¾›äº†æ ‡å‡†åŒ–å·¥å…·ï¼Œæ¨åŠ¨é¢†åŸŸå†…å¯¹RMsâ€œçµæ´»æ€§â€â€œé€šç”¨æ€§â€çš„é‡åŒ–ç ”ç©¶ï¼Œåç»­å¯åŸºäºæ­¤æŒç»­è¿­ä»£ä¼˜åŒ–æ¨¡å‹ã€‚  
3. æ¨¡å‹è®¾è®¡å±‚é¢ï¼šRewardAnythingç»“åˆç‰¹å®šè®­ç»ƒæ–¹æ³•å®ç°æ¨ç†æ—¶åŸåˆ™éµå¾ªï¼Œå…¶æŠ€æœ¯è·¯çº¿ï¼ˆå¦‚GRPOç»“åˆã€ç”Ÿæˆå¼RMè®¾è®¡ï¼‰ä¸ºæ‰“é€ çµæ´»å¥–åŠ±æ¨¡å‹æä¾›äº†å¯å‚è€ƒçš„å·¥ç¨‹å®è·µï¼ŒåŠ©åŠ›åç»­é«˜æ•ˆRLHFæµç¨‹æ­å»ºã€‚

## visualprm--an-effective-process-reward-model-for-multimodal-reasoning
### Abstract
We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)
with 8B parameters, which improves the reasoning abilities of existing
Multimodal Large Language Models (MLLMs) across different model scales and
families with Best-of-N (BoN) evaluation strategies. Specifically, our model
improves the reasoning performance of three types of MLLMs and four different
model scales. Even when applied to the highly capable InternVL2.5-78B, it
achieves a 5.9-point improvement across seven multimodal reasoning benchmarks.
Experimental results show that our model exhibits superior performance compared
to Outcome Reward Models and Self-Consistency during BoN evaluation. To
facilitate the training of multimodal PRMs, we construct a multimodal process
supervision dataset VisualPRM400K using an automated data pipeline. For the
evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with
human-annotated step-wise correctness labels, to measure the abilities of PRMs
to detect erroneous steps in multimodal reasoning tasks. We hope that our work
can inspire more future research and contribute to the development of MLLMs.
Our model, data, and benchmark are released in
https://internvl.github.io/blog/2025-03-13-VisualPRM/.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | VisualPRMï¼šå¼€å¯å¤šæ¨¡æ€æ¨ç†æ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰ - è¯­è¨€ä»»åŠ¡ä¸­ä¹Ÿæœ‰äº†é‡å¤§è¿›å±•ï¼Œä½†å¼€æºå’Œä¸“æœ‰æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šä»å­˜åœ¨è¾ƒå¤§å·®è·ã€‚ä¸€ç³»åˆ—ç ”ç©¶æ¢ç´¢äº†æå‡æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œåˆ©ç”¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç ”ç©¶ä¹Ÿæœ‰å¼€å±•ï¼Œä½†MLLMsçš„TTSåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚å°†TTSåº”ç”¨äºMLLMsé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¼ºä¹æœ‰æ•ˆçš„æ‰¹è¯„æ¨¡å‹ï¼Œç°æœ‰å¼€æºMLLMséš¾ä»¥ä½œä¸ºæ‰¹è¯„æ¨¡å‹ï¼Œåœ¨BoNè¯„ä¼°ä¸­æå‡æ•ˆæœæœ‰é™ï¼›äºŒæ˜¯ç¼ºä¹é’ˆå¯¹å¤šæ¨¡æ€æ‰¹è¯„æ¨¡å‹çš„è¯„ä¼°åŸºå‡†ï¼ŒBoNè¯„ä¼°æˆæœ¬é«˜ä¸”å…¶æ€§èƒ½å—ç­–ç•¥æ¨¡å‹å½±å“ï¼Œéš¾ä»¥æ¯”è¾ƒä¸åŒæ‰¹è¯„æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤šæ¨¡æ€è¿‡ç¨‹ç›‘ç£æ•°æ®é›†VisualPRM400K
è¯¥æ•°æ®é›†åŒ…å«çº¦40ä¸‡å¤šæ¨¡æ€è¿‡ç¨‹ç›‘ç£æ•°æ®ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬å›¾åƒã€é—®é¢˜ã€é€æ­¥è§£å†³æ–¹æ¡ˆä»¥åŠæ¯ä¸ªæ­¥éª¤çš„æ­£ç¡®æ€§æ³¨é‡Šã€‚ä»MMPR v1.1æ”¶é›†é—®é¢˜æç¤ºï¼Œç„¶åä½¿ç”¨è‡ªåŠ¨æ•°æ®ç®¡é“ç”Ÿæˆè¿‡ç¨‹æ­£ç¡®æ€§ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºè¯„ä¼°åŸºå‡†VisualProcessBench
ç”¨äºè¯„ä¼°PRMså’ŒMLLMsåœ¨æ£€æµ‹å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­é”™è¯¯æ­¥éª¤çš„èƒ½åŠ›ï¼ŒåŒ…å«2866ä¸ªæ ·æœ¬ä»¥åŠ26950ä¸ªäººå·¥æ³¨é‡Šçš„åˆ†æ­¥æ­£ç¡®æ€§æ ‡ç­¾ã€‚ä¸å…ˆå‰åŸºå‡†ä¸åŒï¼Œå®ƒè¦æ±‚æ¨¡å‹æ£€æµ‹ç»™å®šè§£å†³æ–¹æ¡ˆä¸­çš„æ‰€æœ‰é”™è¯¯ï¼Œä»¥å‡å°‘è¯„ä¼°ä¸­çš„å‡é˜´æ€§ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼€å‘å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹VisualPRM
ä½œä¸ºBoNè¯„ä¼°ä¸­çš„æ‰¹è¯„æ¨¡å‹ï¼Œå…·æœ‰80äº¿å‚æ•°ã€‚æ¯ä¸ªè®­ç»ƒæ ·æœ¬è¢«è¡¨è¿°ä¸ºå¤šè½®å¯¹è¯ï¼Œæ¨¡å‹ç»è¿‡è®­ç»ƒä»¥é¢„æµ‹æ¯è½®ç»™å®šæ­¥éª¤çš„æ­£ç¡®æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
VisualPRMåœ¨ä¸åŒæ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šå¢å¼ºäº†MLLMæ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ä¸ƒä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šï¼ŒVisualPRMåˆ†åˆ«å°†MiniCPM - V2.6ã€QwenVL2.5 - 7Bã€InternVL2.5 - 8Bå’ŒInternVL2.5 - 78Bçš„æ•´ä½“æ¨ç†æ€§èƒ½æé«˜äº†8.0ã€3.7ã€8.4å’Œ5.9ä¸ªç™¾åˆ†ç‚¹ã€‚åœ¨BoNè¯„ä¼°ä¸­ï¼ŒVisualPRMè¡¨ç°ä¼˜äºç»“æœå¥–åŠ±æ¨¡å‹å’Œè‡ªä¸€è‡´æ€§ã€‚åœ¨VisualProcessBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç°æœ‰å¼€æºMLLMséš¾ä»¥å‡†ç¡®è¯„ä¼°æ¯ä¸ªæ­¥éª¤çš„æ­£ç¡®æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç ”ç©¶å…·æœ‰é‡è¦å€Ÿé‰´æ„ä¹‰ã€‚æ„å»ºçš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ä¸ºåç»­ç›¸å…³ç ”ç©¶æä¾›äº†å®è´µèµ„æºï¼ŒVisualPRMä½œä¸ºæ‰¹è¯„æ¨¡å‹çš„æˆåŠŸåº”ç”¨ï¼Œä¸ºMLLMsçš„æµ‹è¯•æ—¶ç¼©æ”¾æä¾›äº†æœ‰æ•ˆæ€è·¯ï¼Œæ¨åŠ¨å¤šæ¨¡æ€æ¨ç†é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚
```

## skywork-reward--bag-of-tricks-for-reward-modeling-in-llms
### Abstract
In this report, we introduce a collection of methods to enhance reward
modeling for LLMs, focusing specifically on data-centric techniques. We propose
effective data selection and filtering strategies for curating high-quality
open-source preference datasets, culminating in the Skywork-Reward data
collection, which contains only 80K preference pairs -- significantly smaller
than existing datasets. Using this curated dataset, we developed the
Skywork-Reward model series -- Skywork-Reward-Gemma-27B and
Skywork-Reward-Llama-3.1-8B -- with the former currently holding the top
position on the RewardBench leaderboard. Notably, our techniques and datasets
have directly enhanced the performance of many top-ranked models on
RewardBench, highlighting the practical impact of our contributions in
real-world preference learning applications.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-Rewardï¼šå¤§è¯­è¨€æ¨¡å‹å¥–åŠ±å»ºæ¨¡çš„å®ç”¨æŠ€å·§é›†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å…¶ä¸ç”¨æˆ·åå¥½å¯¹é½çš„ç ”ç©¶ï¼Œå¥–åŠ±å»ºæ¨¡æ˜¯å…¶ä¸­å…³é”®ä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚ç„¶è€Œè®­ç»ƒå¥–åŠ±æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼šäººç±»åå¥½å¤æ‚éš¾ç©·å°½è¡¨ç¤ºï¼›å¼€æºåå¥½æ•°æ®é›†å¸¸å«å™ªå£°ï¼ˆå¦‚åå¥½ä¸éåå¥½å“åº”å·®å¼‚ä¸æ˜æ˜¾æˆ–æ ‡æ³¨ä¸ä¸€è‡´ï¼‰ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚åŒæ—¶ï¼Œæ­¤å‰ç ”ç©¶å¤šå…³æ³¨æ¨¡å‹æ¶æ„ä¸æŸå¤±å‡½æ•°ï¼Œè€Œåå¥½æ•°æ®çš„è´¨é‡å’Œå¯ç”¨æ€§å¯¹å¥–åŠ±å»ºæ¨¡æˆåŠŸè‡³å…³é‡è¦å´æœªè¢«å……åˆ†é‡è§†ã€‚å› æ­¤ï¼Œæœ¬æ–‡èšç„¦ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æŠ€æœ¯ï¼Œæå‡LLMså¥–åŠ±å»ºæ¨¡èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜è´¨é‡åå¥½æ•°æ®ç²¾é€‰ç­–ç•¥  
æå‡ºæœ‰æ•ˆæ•°æ®é€‰æ‹©ä¸è¿‡æ»¤ç­–ç•¥æ¥æ„å»ºé«˜è´¨é‡å¼€æºåå¥½æ•°æ®é›†ã€‚é€šè¿‡ç­›é€‰å¯¹æ¨¡å‹æ€§èƒ½æå‡æœ€æœ‰æ•ˆçš„åå¥½å¯¹ï¼Œæ‰“é€ äº†ä»…å«8ä¸‡åå¥½å¯¹çš„Skywork - Rewardæ•°æ®é›†ï¼Œè¿œå°äºç°æœ‰æ•°æ®é›†è§„æ¨¡ï¼Œå´èƒ½ä¿è¯æ•°æ®è´¨é‡ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›ä¼˜è´¨æ•°æ®åŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæŸå¤±å‡½æ•°çš„æ¶ˆèå®éªŒä¸é€‰æ‹©  
å¯¹å¤šç§æŸå¤±å‡½æ•°å¼€å±•å¹¿æ³›æ¶ˆèå®éªŒï¼Œé‡ç‚¹ä¼˜åŒ–åå¥½ä¸éåå¥½å“åº”é—´çš„é—´éš”ã€‚å®éªŒè¡¨æ˜ç»å…¸çš„Bradley - TerryæŸå¤±åœ¨å¥–åŠ±å»ºæ¨¡ä»»åŠ¡ä¸­æŒç»­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå‡¸æ˜¾å…¶é²æ£’æ€§ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æŸå¤±å‡½æ•°é€‰æ‹©æä¾›æœ‰åŠ›ä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šSkywork - Rewardæ¨¡å‹ç³»åˆ—å¼€å‘  
åˆ©ç”¨ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼Œå¼€å‘Skywork - Rewardæ¨¡å‹ç³»åˆ—ï¼ˆSkywork - Reward - Gemma - 27Bå’ŒSkywork - Reward - Llama - 3.1 - 8Bï¼‰ï¼Œå°†æ•°æ®ä¸è®­ç»ƒæŠ€æœ¯ç»“åˆï¼Œæå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨RewardBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSkywork - Rewardæ¨¡å‹ç³»åˆ—è¡¨ç°ä¼˜å¼‚ï¼šæˆªè‡³2024å¹´10æœˆï¼ŒSkywork - Reward - Gemma - 27Bä½å±…RewardBenchæ’è¡Œæ¦œé¦–ä½ï¼ŒSkywork - Reward - Llama - 3.1 - 8Bä½åˆ—ç¬¬ä¸ƒã€‚æ­¤å¤–ï¼Œæ‰€æ„å»ºçš„Skywork - Rewardåå¥½æ•°æ®é›†è¢«åç»­è¯¸å¤šç ”ç©¶é‡‡ç”¨ï¼Œè¯æ˜äº†å…¶ä»·å€¼ä¸é€‚ç”¨æ€§ï¼›ä¸”æ–‡ä¸­æŠ€æœ¯å’Œæ•°æ®é›†ç›´æ¥æå‡äº†RewardBenchä¸Šå¤šä¸ªé¡¶å°–æ¨¡å‹çš„æ€§èƒ½ï¼Œå½°æ˜¾åœ¨çœŸå®åå¥½å­¦ä¹ åº”ç”¨ä¸­çš„å®é™…å½±å“ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®å±‚é¢ï¼šé‡è§†æ•°æ®è´¨é‡ï¼Œé€šè¿‡åˆç†é€‰æ‹©ä¸è¿‡æ»¤ç­–ç•¥ï¼Œç”¨æ›´å°è§„æ¨¡æ•°æ®æ‰“é€ é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸ºèµ„æºæœ‰é™æƒ…å†µä¸‹æ„å»ºæœ‰æ•ˆè®­ç»ƒæ•°æ®æä¾›æ€è·¯ï¼Œä¹Ÿè¯´æ˜æ•°æ®â€œè´¨â€æ¯”â€œé‡â€åœ¨æŸäº›åœºæ™¯æ›´å…³é”®ã€‚  
2. æ–¹æ³•å±‚é¢ï¼šå¯¹æŸå¤±å‡½æ•°å¼€å±•æ¶ˆèå®éªŒéªŒè¯å…¶åœ¨å¥–åŠ±å»ºæ¨¡ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œè¿™ç§ä¸¥è°¨çš„å®éªŒæ€åº¦å’Œæ–¹æ³•å€¼å¾—å€Ÿé‰´ï¼Œå¯å¸®åŠ©åœ¨æ¨¡å‹è®­ç»ƒä¸­é€‰æ‹©æ›´ä¼˜æŸå¤±å‡½æ•°ã€‚  
3. å¼€æºè´¡çŒ®ï¼šå…¬å¼€æ¨¡å‹ç³»åˆ—ä¸æ•°æ®é›†ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ï¼Œè¿™ç§å¼€æ”¾å…±äº«çš„ç§‘ç ”ç²¾ç¥åˆ©äºè¡Œä¸šæ•´ä½“è¿›æ­¥ï¼Œä¹Ÿä¸ºåç»­ç ”ç©¶æä¾›äº†åšå®åŸºç¡€ä¸å‚è€ƒèŒƒä¾‹ã€‚  
```

## on-designing-effective-rl-reward-at-training-time-for-llm-reasoning
### Abstract
Reward models have been increasingly critical for improving the reasoning
capability of LLMs. Existing research has shown that a well-trained reward
model can substantially improve model performances at inference time via
search. However, the potential of reward models during RL training time still
remains largely under-explored. It is currently unclear whether these reward
models can provide additional training signals to enhance the reasoning
capabilities of LLMs in RL training that uses sparse success rewards, which
verify the correctness of solutions. In this work, we evaluate popular reward
models for RL training, including the Outcome-supervised Reward Model (ORM) and
the Process-supervised Reward Model (PRM), and train a collection of LLMs for
math problems using RL by combining these learned rewards with success rewards.
Surprisingly, even though these learned reward models have strong
inference-time performances, they may NOT help or even hurt RL training,
producing worse performances than LLMs trained with the success reward only.
Our analysis reveals that an LLM can receive high rewards from some of these
reward models by repeating correct but unnecessary reasoning steps, leading to
a severe reward hacking issue. Therefore, we introduce two novel reward
refinement techniques, including Clipping and Delta. The key idea is to ensure
the accumulative reward of any reasoning trajectory is upper-bounded to keep a
learned reward model effective without being exploited. We evaluate our
techniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH
and GSM8K benchmarks and demonstrate that with a carefully designed reward
function, RL training without any additional supervised tuning can improve all
the evaluated LLMs, including the state-of-the-art 7B LLM
Qwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹æ¨ç†è®­ç»ƒæ—¶ï¼Œå¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸï¼Œå¥–åŠ±æ¨¡å‹å¯¹æå‡æ¨ç†èƒ½åŠ›æ„ˆå‘å…³é”®ã€‚å·²æœ‰ç ”ç©¶è¡¨æ˜è®­ç»ƒè‰¯å¥½çš„å¥–åŠ±æ¨¡å‹èƒ½åœ¨æ¨ç†æ—¶é€šè¿‡æœç´¢ç­‰æ–¹å¼å¤§å¹…æå‡æ€§èƒ½ï¼Œä½†å¥–åŠ±æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒé˜¶æ®µçš„æ½œåŠ›å´æœªå……åˆ†æŒ–æ˜ã€‚ç›®å‰ä¸æ¸…æ¥šè¿™äº›å¥–åŠ±æ¨¡å‹åœ¨ä½¿ç”¨ç¨€ç–æˆåŠŸå¥–åŠ±ï¼ˆéªŒè¯è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§ï¼‰çš„RLè®­ç»ƒä¸­ï¼Œèƒ½å¦æä¾›é¢å¤–è®­ç»ƒä¿¡å·æ¥å¢å¼ºLLMæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸€äº›å¼ºå¤§LLMè™½å°†å¥–åŠ±æ¨¡å‹èå…¥RLè®­ç»ƒç”¨äºæ•°å­¦æ¨ç†ï¼Œä½†ç¼ºä¹å¯¹å¥–åŠ±æ¨¡å‹çš„è¯¦ç»†åˆ†æï¼Œä¸ç¡®å®šå…¶èƒ½å¦åœ¨æˆåŠŸå¥–åŠ±ä¹‹å¤–æä¾›é¢å¤–ä¿¡å·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯„ä¼°ä¸»æµå¥–åŠ±æ¨¡å‹åœ¨RLè®­ç»ƒä¸­çš„è¡¨ç°  
å¯¹ä¸»æµå¥–åŠ±æ¨¡å‹ï¼ˆç»“æœç›‘ç£å¥–åŠ±æ¨¡å‹ORMä¸è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹PRMï¼‰åœ¨RLè®­ç»ƒä¸­è¿›è¡Œè¯„ä¼°ï¼Œå°†è¿™äº›å­¦ä¹ åˆ°çš„å¥–åŠ±ä¸æˆåŠŸå¥–åŠ±ç»“åˆï¼Œç”¨äºè®­ç»ƒè§£å†³æ•°å­¦é—®é¢˜çš„LLMã€‚å‘ç°å°½ç®¡è¿™äº›å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶è¡¨ç°å¼ºï¼Œä½†å¯èƒ½å¯¹RLè®­ç»ƒæ— å¸®åŠ©ç”šè‡³èµ·åä½œç”¨ï¼Œæ¯”ä»…ç”¨æˆåŠŸå¥–åŠ±è®­ç»ƒçš„LLMè¡¨ç°æ›´å·®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºä¸¤ç§å¥–åŠ±ä¼˜åŒ–æŠ€æœ¯åº”å¯¹å¥–åŠ±é»‘å®¢é—®é¢˜  
åˆ†æå‘ç°LLMèƒ½é€šè¿‡é‡å¤æ­£ç¡®ä½†ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ä»éƒ¨åˆ†å¥–åŠ±æ¨¡å‹è·å–é«˜å¥–åŠ±ï¼Œå¼•å‘ä¸¥é‡â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜ã€‚ä¸ºæ­¤æå‡ºClippingï¼ˆæˆªæ–­ï¼‰å’ŒDeltaï¼ˆå·®å€¼ï¼‰ä¸¤ç§å¥–åŠ±ä¼˜åŒ–æŠ€æœ¯ï¼šClippingæœºåˆ¶å°†å¥–åŠ±é™åˆ¶åœ¨ upper threshold å†…ï¼Œè®©RLè®­ç»ƒèšç„¦å‡å°‘é”™è¯¯æ¨ç†æ­¥éª¤ï¼›Deltaæœºåˆ¶é€šè¿‡å‡å»ç›¸é‚»æ­¥éª¤é—´å¥–åŠ±æ¥ç»´æŒæœ‰ç•Œç›®æ ‡ï¼ŒæŠ‘åˆ¶æ— æ„ä¹‰é‡å¤æ¨¡å¼ä»¥å®ç°é«˜å›æŠ¥å¹¶æå‡è®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MATHå’ŒGSM8KåŸºå‡†æµ‹è¯•é›†ä¸Šï¼Œç”¨å¤šä¸ªå¥–åŠ±æ¨¡å‹å¯¹1.5Bå’Œ7Bè§„æ¨¡çš„LLMï¼ˆæ¥è‡ªQwen2å’ŒQwen2.5ç³»åˆ—ï¼‰è¯„ä¼°æ‰€ææŠ€æœ¯ã€‚ç»“æœæ˜¾ç¤ºClippingå’ŒDeltaèƒ½æŒç»­ç¨³å®šRLè®­ç»ƒï¼›ä¸”ç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°åï¼Œæ— é¢å¤–ç›‘ç£å¾®è°ƒçš„çº¯RLè®­ç»ƒèƒ½æå‡æ‰€æœ‰è¯„ä¼°çš„LLMï¼ŒåŒ…æ‹¬åœ¨MATHå’ŒGSM8KåŸºå‡†ä¸Šçš„SOTAæ¨¡å‹Qwen2.5-Math-7B-Instructã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹å¥–åŠ±æ¨¡å‹åœ¨RLè®­ç»ƒé˜¶æ®µä½œç”¨çš„æ¢ç´¢æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œæé†’ç ”ç©¶è€…å…³æ³¨è®­ç»ƒä¸æ¨ç†é˜¶æ®µå¥–åŠ±æ¨¡å‹è¡¨ç°å·®å¼‚ï¼Œæ·±å…¥åˆ†æå¥–åŠ±æ¨¡å‹åœ¨è®­ç»ƒä¸­æ½œåœ¨é—®é¢˜ï¼ˆå¦‚å¥–åŠ±é»‘å®¢ï¼‰ã€‚
2. æå‡ºçš„Clippingå’ŒDeltaæŠ€æœ¯ä¸ºè§£å†³å¥–åŠ±é»‘å®¢ã€ç¨³å®šRLè®­ç»ƒæä¾›äº†æ–°é¢–ä¸”æœ‰æ•ˆçš„æ€è·¯ï¼Œåç»­åœ¨è®¾è®¡LLMçš„RLå¥–åŠ±å‡½æ•°æ—¶å¯å‚è€ƒè¿™ç±»çº¦æŸå¥–åŠ±ç´¯è®¡çš„æ–¹æ³•ã€‚
3. å®éªŒéªŒè¯äº†çº¯RLè®­ç»ƒåœ¨åˆç†å¥–åŠ±è®¾è®¡ä¸‹å¯¹æå‡LLMæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåç»­LLMæ¨ç†èƒ½åŠ›ä¼˜åŒ–çš„è®­ç»ƒèŒƒå¼é€‰æ‹©æä¾›äº†å®è·µä¾æ®ã€‚
```

## gram--a-generative-foundation-reward-model-for-reward-generalization
### Abstract
In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | GRAMï¼šé¢å‘å¥–åŠ±æ³›åŒ–çš„ç”Ÿæˆå¼åŸºç¡€å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹æ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¤šä½œä¸ºåˆ¤åˆ«å¼æ¨¡å‹è®­ç»ƒï¼Œä¸”é«˜åº¦ä¾èµ–å¸¦æ ‡ç­¾çš„äººç±»åå¥½æ•°æ®ï¼Œå­˜åœ¨æ ‡æ³¨æˆæœ¬é«˜ã€å¯¹æ— æ ‡ç­¾æ•°æ®åˆ©ç”¨ä¸è¶³ç­‰é—®é¢˜ã€‚åŒæ—¶ï¼Œç°æœ‰è®­ç»ƒæ–¹å¼éš¾ä»¥é«˜æ•ˆå¾—åˆ°èƒ½åœ¨å¤šä»»åŠ¡é—´æ³›åŒ–çš„åŸºç¡€å¥–åŠ±æ¨¡å‹ï¼Œé™åˆ¶äº†å¥–åŠ±æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„çµæ´»åº”ç”¨ä¸æ•ˆç‡æå‡ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨æ— æ ‡ç­¾å’Œæœ‰æ ‡ç­¾æ•°æ®æ¥è®­ç»ƒæ›´å…·æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼š**ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼**  
åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆå¼èƒ½åŠ›ï¼Œæå‡ºå…ˆé€šè¿‡å¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒã€å†åˆ©ç”¨ç›‘ç£å­¦ä¹ å¾®è°ƒçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹è®­ç»ƒæµç¨‹ã€‚é¢„è®­ç»ƒé˜¶æ®µåœ¨è¾“å…¥ - å“åº”æ•°æ®ä¸Šå­¦ä¹ è¾“å…¥ä¸å“åº”çš„å¯¹åº”å…³ç³»ï¼Œæ— éœ€åå¥½æ ‡æ³¨æ•°æ®ï¼Œå¯è§„æ¨¡åŒ–è·å–å“åº”æ¯”è¾ƒçš„é€šç”¨çŸ¥è¯†ï¼›å¾®è°ƒé˜¶æ®µåˆ©ç”¨äººç±»åå¥½æ•°æ®å­¦ä¹ é¢„æµ‹å“åº”é—´åå¥½ï¼Œæœ€ç»ˆå¾—åˆ°èƒ½ç›´æ¥æˆ–ç»å°‘é‡å¾®è°ƒåº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€å¥–åŠ±æ¨¡å‹ï¼Œå……åˆ†ç»“åˆæ— æ ‡ç­¾ä¸æœ‰æ ‡ç­¾æ•°æ®ä¼˜åŠ¿ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼š**æ ‡ç­¾å¹³æ»‘ä¸‹çš„æŸå¤±å‡½æ•°ç»Ÿä¸€è§†è§’**  
å¼•å…¥æ ‡ç­¾å¹³æ»‘æŠ€æœ¯å¹¶è¯æ˜ï¼Œå…¶ä½¿è®­ç»ƒç›®æ ‡å¯è½¬åŒ–ä¸ºæ­£åˆ™åŒ–çš„ pairwise ranking lossï¼ˆBradley - Terry æŸå¤±å½¢å¼ï¼‰ã€‚è¿™ä¸€ç»“æœåœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒå±‚é¢ï¼Œå°†ç”Ÿæˆå¼æ¨¡å‹ä¸åˆ¤åˆ«å¼æ¨¡å‹ç»Ÿä¸€åˆ°åŒä¸€ç±»è®­ç»ƒç›®æ ‡ä¸‹ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°è§†è§’ï¼Œä¸”æ ‡ç­¾å¹³æ»‘å¯¹ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹è®­ç»ƒæœ‰æ˜¾è‘—å¢ç›Šï¼ŒåŠ©åŠ›æ¨¡å‹æ³›åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å“åº”æ’åºã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€ä»»åŠ¡è‡ªé€‚åº”ç­‰å¤šä»»åŠ¡åœºæ™¯ä¸‹å¼€å±•å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒGRAM æ— éœ€æˆ–ä»…éœ€å°‘é‡å¾®è°ƒå°±èƒ½åœ¨å„ä»»åŠ¡ä¸­å±•ç°è‰¯å¥½æ³›åŒ–æ€§ï¼Œæ€§èƒ½è¿œè¶…å¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒåŸºäº LLaMA - 3.1 - 8B - Instruct è®­ç»ƒå¥–åŠ±æ¨¡å‹æ—¶ï¼ŒGRAM åœ¨ RewardBench å¹³å‡å‡†ç¡®ç‡ä¸Šï¼Œç›¸æ¯”æ™®é€šåˆ¤åˆ«å¼å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åˆ†åˆ«æå‡ 11.0 å’Œ 5.1 ä¸ªç™¾åˆ†ç‚¹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ•°æ®åˆ©ç”¨æ€è·¯**ï¼šæ‰“ç ´ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¯¹æœ‰æ ‡ç­¾æ•°æ®çš„å¼ºä¾èµ–ï¼Œç¤ºèŒƒäº†æ— æ ‡ç­¾æ•°æ®åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸ºæ¨¡å‹æ³¨å…¥é€šç”¨çŸ¥è¯†ã€æœ‰æ ‡ç­¾æ•°æ®åœ¨å¾®è°ƒé˜¶æ®µç²¾å‡†å¯¹é½åå¥½çš„é«˜æ•ˆæ•°æ®åˆ©ç”¨èŒƒå¼ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹æ•°æ®ç­–ç•¥è®¾è®¡æä¾›å‚è€ƒã€‚  
2. **æ¨¡å‹ç»Ÿä¸€è§†è§’**ï¼šé€šè¿‡æ ‡ç­¾å¹³æ»‘å®ç°ç”Ÿæˆå¼ä¸åˆ¤åˆ«å¼å¥–åŠ±æ¨¡å‹è®­ç»ƒç›®æ ‡çš„å…³è”ä¸ç»Ÿä¸€ï¼Œå¯å‘ç ”ç©¶è€…ä»æ›´å®è§‚çš„æŸå¤±å‡½æ•°è®¾è®¡ä¸æ¨¡å‹æœ¬è´¨è”ç³»è§’åº¦ï¼Œæ¢ç´¢å¥–åŠ±æ¨¡å‹ä¹ƒè‡³æ›´å¹¿æ³›çš„å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–è·¯å¾„ã€‚  
3. **åŸºç¡€æ¨¡å‹æ„å»º**ï¼šæ‰“é€ èƒ½è·¨å¤šä»»åŠ¡æ³›åŒ–çš„åŸºç¡€å¥–åŠ±æ¨¡å‹ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹å¯¹é½æµç¨‹æä¾›â€œé¢„è®­ç»ƒ - å°‘æ ·æœ¬å¾®è°ƒâ€çš„é«˜æ•ˆæ¨¡å¼ï¼Œå‡å°‘ä¸‹æ¸¸ä»»åŠ¡é‡å¤è®­ç»ƒæˆæœ¬ï¼Œæ¨åŠ¨å¥–åŠ±æ¨¡å‹å‘æ›´é€šç”¨ã€æ›´æ˜“ç”¨æ–¹å‘å‘å±•ï¼Œç›¸å…³æ€è·¯å¯è¿ç§»è‡³å…¶ä»–éœ€æ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹æ„å»ºåœºæ™¯ã€‚
```

## inference-time-scaling-for-generalist-reward-modeling
### Abstract
Reinforcement learning (RL) has been widely adopted in post-training for
large language models (LLMs) at scale. Recently, the incentivization of
reasoning capabilities in LLMs from RL indicates that $\textit{proper learning
methods could enable effective inference-time scalability}$. A key challenge of
RL is to obtain accurate reward signals for LLMs in various domains beyond
verifiable questions or artificial rules. In this work, we investigate how to
improve reward modeling (RM) with more inference compute for general queries,
i.e. the $\textbf{inference-time scalability of generalist RM}$, and further,
how to improve the effectiveness of performance-compute scaling with proper
learning methods. For the RM approach, we adopt pointwise generative reward
modeling (GRM) to enable flexibility for different input types and potential
for inference-time scaling. For the learning method, we propose Self-Principled
Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs
through online RL, to generate principles adaptively and critiques accurately,
resulting in $\textbf{DeepSeek-GRM}$ models. Furthermore, for effective
inference-time scaling, we use parallel sampling to expand compute usage, and
introduce a meta RM to guide voting process for better scaling performance.
Empirically, we show that SPCT significantly improves the quality and
scalability of GRMs, outperforming existing methods and models in various RM
benchmarks without severe biases, and could achieve better performance compared
to training-time scaling. DeepSeek-GRM still meets challenges in some tasks,
which we believe can be addressed by future efforts in generalist reward
systems. The models will be released and open-sourced.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šç”¨å¥–åŠ±å»ºæ¨¡çš„æ¨ç†æ—¶ç¼©æ”¾ï¼šè§£é”å¤§æ¨¡å‹å¥–åŠ±ä¿¡å·æ–°ç»´åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è“¬å‹ƒå‘å±•æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ç ”ç©¶çš„é‡å¤§å˜é©ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºå¤§æ¨¡å‹è®­ç»ƒåçš„å…³é”®æŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨ï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨äººç±»ä»·å€¼å¯¹é½ã€é•¿æœŸæ¨ç†ç­‰æ–¹é¢çš„è¡¨ç°ã€‚ä½†RLé¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯åœ¨é€šç”¨é¢†åŸŸï¼ˆéå¯éªŒè¯é—®é¢˜æˆ–äººå·¥è§„åˆ™åœºæ™¯ï¼‰ä¸ºLLMsè·å–å‡†ç¡®å¥–åŠ±ä¿¡å·ã€‚é€šç”¨å¥–åŠ±å»ºæ¨¡éœ€å…¼é¡¾è¾“å…¥çµæ´»æ€§ä¸è·¨åŸŸå¥–åŠ±å‡†ç¡®æ€§ï¼Œä¸”è¦å®ç°æ¨ç†æ—¶çš„æœ‰æ•ˆç®—åŠ›æ‰©å±•ã€‚ç°æœ‰å¥–åŠ±ç”ŸæˆèŒƒå¼ï¼ˆå¦‚æ ‡é‡ã€åŠæ ‡é‡ç­‰ï¼‰åœ¨è¾“å…¥çµæ´»æ€§å’Œæ¨ç†æ—¶æ‰©å±•æ€§ä¸Šå­˜åœ¨å±€é™ï¼Œå­¦ä¹ æ–¹æ³•ä¹Ÿè¾ƒå°‘å…³æ³¨æ¨ç†æ—¶æ‰©å±•æ€§ä¸å¥–åŠ±ç”Ÿæˆè¡Œä¸ºçš„å…³è”ï¼Œéš¾ä»¥æ»¡è¶³é€šç”¨åœºæ™¯éœ€æ±‚ã€‚å› æ­¤ï¼Œæ¢ç´¢å¦‚ä½•é€šè¿‡åˆé€‚æ–¹æ³•å®ç°é€šç”¨å¥–åŠ±å»ºæ¨¡çš„æ¨ç†æ—¶ç¼©æ”¾æˆä¸ºå…³é”®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡‡ç”¨é€ç‚¹ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡ï¼ˆGRMï¼‰  
é€‰æ‹©é€ç‚¹ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡ï¼ˆpointwise generative reward modeling, GRMï¼‰ä½œä¸ºå¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œå®ƒèƒ½åœ¨çº¯è¯­è¨€è¡¨å¾å†…ç»Ÿä¸€å•æ¡ã€é…å¯¹åŠå¤šæ¡å“åº”çš„è¯„åˆ†ï¼Œä¸ºä¸åŒè¾“å…¥ç±»å‹æä¾›çµæ´»æ€§ï¼ŒåŒæ—¶å…·å¤‡æ¨ç†æ—¶ç¼©æ”¾çš„æ½œåŠ›ï¼Œè§£å†³äº†é€šç”¨å¥–åŠ±å»ºæ¨¡å¯¹è¾“å…¥çµæ´»æ€§çš„éœ€æ±‚è¿™ä¸€æŒ‘æˆ˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºè‡ªåŸåˆ™æ€§æ‰¹åˆ¤è°ƒä¼˜ï¼ˆSPCTï¼‰  
è®¾è®¡Self - Principled Critique Tuningï¼ˆSPCTï¼‰å­¦ä¹ æ–¹æ³•ï¼Œå€ŸåŠ©åŸºäºè§„åˆ™çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œè®©GRMå­¦ä¼šæ ¹æ®è¾“å…¥æŸ¥è¯¢å’Œå“åº”è‡ªé€‚åº”ç”ŸæˆåŸåˆ™ä¸æ‰¹åˆ¤ï¼Œåœ¨é€šç”¨é¢†åŸŸæå‡å¥–åŠ±è´¨é‡ï¼ŒåŸ¹å…»GRMä¸­å¯æ‰©å±•çš„å¥–åŠ±ç”Ÿæˆè¡Œä¸ºï¼Œåº”å¯¹é€šç”¨é¢†åŸŸå‡†ç¡®ç”Ÿæˆå¥–åŠ±çš„æŒ‘æˆ˜ã€‚åŸºäºæ­¤è®­ç»ƒå¾—åˆ°DeepSeek - GRM - 27Bæ¨¡å‹ï¼ˆä»¥Gemma - 2 - 27Bä¸ºåŸºç¡€åè®­ç»ƒï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨ç†æ—¶ç¼©æ”¾çš„å·¥ç¨‹ä¸æ¨¡å‹è®¾è®¡  
ä¸ºå®ç°æœ‰æ•ˆæ¨ç†æ—¶ç¼©æ”¾ï¼Œé‡‡ç”¨å¹¶è¡Œé‡‡æ ·æ‰©å±•ç®—åŠ›ä½¿ç”¨ï¼ŒDeepSeek - GRMå¯ç”Ÿæˆä¸åŒåŸåˆ™å’Œæ‰¹åˆ¤é›†åˆåæŠ•ç¥¨å¾—åˆ°æœ€ç»ˆå¥–åŠ±ï¼›å¤§è§„æ¨¡é‡‡æ ·ä¸‹èƒ½åŸºäºæ›´ä¸°å¯Œå¤šæ ·çš„åŸåˆ™æ›´ç²¾å‡†åˆ¤æ–­ï¼Œè¾“å‡ºæ›´ç»†ç²’åº¦å¥–åŠ±ï¼Œè§£å†³æ¨ç†æ—¶éšç®—åŠ›å¢åŠ ç”Ÿæˆæ›´é«˜è´¨é‡å¥–åŠ±ä¿¡å·åŠå­¦ä¹ å¯æ‰©å±•è¡Œä¸ºçš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè®­ç»ƒå…ƒå¥–åŠ±æ¨¡å‹ï¼ˆmeta RMï¼‰è¾…åŠ©æŠ•ç¥¨ä»¥ä¼˜åŒ–ç¼©æ”¾æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜SPCTæ˜¾è‘—æå‡äº†GRMçš„è´¨é‡ä¸å¯æ‰©å±•æ€§ï¼Œåœ¨å¤šä¸ªç»¼åˆå¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•å’Œæ¨¡å‹ï¼Œä¸”æ— ä¸¥é‡é¢†åŸŸåå·®ï¼›ä¸è®­ç»ƒæ—¶ç¼©æ”¾ç›¸æ¯”ï¼Œèƒ½å–å¾—æ›´ä¼˜æ€§èƒ½ã€‚ä¸è¿‡DeepSeek - GRMåœ¨éƒ¨åˆ†ä»»åŠ¡ä»å­˜æŒ‘æˆ˜ï¼Œæœªæ¥é€šç”¨å¥–åŠ±ç³»ç»Ÿç ”ç©¶æˆ–å¯è§£å†³ã€‚åŒæ—¶ä»å›¾1æ¨ç†æ—¶ç¼©æ”¾æ€§èƒ½å¯¹æ¯”å¯è§ï¼ŒDeepSeek - GRMç›¸å…³æ–¹æ³•åœ¨é‡‡æ ·æ‰©å±•ä¸‹è¡¨ç°çªå‡ºã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼é€‰æ‹©è§’åº¦ï¼šGRMè¿™ç§é€ç‚¹ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡æ€è·¯ä¸ºå¤„ç†å¤šç±»å‹è¾“å…¥ã€å®ç°æ¨ç†æ—¶æ‰©å±•æä¾›äº†æ–°èŒƒå¼å‚è€ƒï¼Œåœ¨éœ€çµæ´»å¤„ç†ä¸åŒå“åº”è¾“å…¥çš„å¥–åŠ±å»ºæ¨¡åœºæ™¯å¯å€Ÿé‰´ã€‚  
2. å­¦ä¹ æ–¹æ³•è§’åº¦ï¼šSPCTå°†åœ¨çº¿RLä¸åŸåˆ™ã€æ‰¹åˆ¤ç”Ÿæˆç»“åˆï¼Œä¸ºå¼•å¯¼æ¨¡å‹å­¦ä¹ å¯æ‰©å±•å¥–åŠ±è¡Œä¸ºæä¾›äº†åˆ›æ–°æ€è·¯ï¼Œåœ¨ä¼˜åŒ–æ¨¡å‹æ¨ç†æ—¶æ€§èƒ½çš„å­¦ä¹ ç­–ç•¥è®¾è®¡ä¸Šæœ‰å¯å‘ã€‚  
3. å·¥ç¨‹å®ç°è§’åº¦ï¼šå¹¶è¡Œé‡‡æ · + å…ƒRMè¾…åŠ©æŠ•ç¥¨çš„æ¨ç†æ—¶ç¼©æ”¾æ–¹æ¡ˆï¼Œä¸ºå¤§æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µé«˜æ•ˆåˆ©ç”¨ç®—åŠ›æå‡æ€§èƒ½æä¾›äº†å·¥ç¨‹å®è·µæ–¹å‘ï¼Œåœ¨å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–ã€å¤šè½®å†³ç­–ç±»ä»»åŠ¡ä¸­å¯å‚è€ƒè¯¥æ‰©å±•ç®—åŠ›å¹¶å¼•å¯¼å†³ç­–çš„æ€è·¯ã€‚  
4. å¼€æºä¸ç”Ÿæ€è§’åº¦ï¼šè®ºæ–‡æåŠæ¨¡å‹å°†å¼€æºï¼Œè¿™ç§å¼€æ”¾ç”Ÿæ€çš„åšæ³•ä¹Ÿä¸ºé¢†åŸŸå†…çŸ¥è¯†å…±äº«ã€åŠ é€Ÿç ”ç©¶è¿­ä»£æä¾›èŒƒä¾‹ï¼Œå€¼å¾—å…³æ³¨å¼€æºåä½œçš„å›¢é˜Ÿå‚è€ƒã€‚
```

## rest-mcts*--llm-self-training-via-process-reward-guided-tree-search
### Abstract
Recent methodologies in LLM self-training mostly rely on LLM generating
responses and filtering those with correct output answers as training data.
This approach often yields a low-quality fine-tuning training set (e.g.,
incorrect plans or intermediate reasoning). In this paper, we develop a
reinforced self-training approach, called ReST-MCTS*, based on integrating
process reward guidance with tree search MCTS* for collecting higher-quality
reasoning traces as well as per-step value to train policy and reward models.
ReST-MCTS* circumvents the per-step manual annotation typically used to train
process rewards by tree-search-based reinforcement learning: Given oracle final
correct answers, ReST-MCTS* is able to infer the correct process rewards by
estimating the probability this step can help lead to the correct answer. These
inferred rewards serve dual purposes: they act as value targets for further
refining the process reward model and also facilitate the selection of
high-quality traces for policy model self-training. We first show that the
tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior
LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same
search budget. We then show that by using traces searched by this tree-search
policy as training data, we can continuously enhance the three language models
for multiple iterations, and outperform other self-training algorithms such as
ReST$^\text{EM}$ and Self-Rewarding LM. We release all code at
https://github.com/THUDM/ReST-MCTS.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReST-MCTS*ï¼šç”¨è¿‡ç¨‹å¥–åŠ±å¼•å¯¼æ ‘æœç´¢å®ç°å¤§æ¨¡å‹è‡ªè®­ç»ƒæ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªè®­ç»ƒé¢†åŸŸï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–æ¨¡å‹ç”Ÿæˆå›ç­”åç­›é€‰æ­£ç¡®è¾“å‡ºä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œä½†è¿™ç±»æ–¹æ³•æ˜“å¾—åˆ°ä½è´¨é‡å¾®è°ƒæ•°æ®é›†â€”â€”æ¯”å¦‚æ¨ç†è¿‡ç¨‹é‡Œå­˜åœ¨é”™è¯¯è§„åˆ’æˆ–ä¸­é—´æ¨ç†æ­¥éª¤æœ‰é—®é¢˜ï¼Œå¯æœ€ç»ˆç­”æ¡ˆå´ç¢°å·§æ­£ç¡®ï¼ˆå³â€œå‡é˜³æ€§â€æ¨ç†è½¨è¿¹ï¼‰ã€‚è¿™ä¼šé™åˆ¶LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å¾®è°ƒè¡¨ç°ã€‚åŒæ—¶ï¼Œè®­ç»ƒå¯é çš„â€œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰â€æ¥éªŒè¯æ¯ä¸€æ­¥æ¨ç†æ˜¯å¦æ­£ç¡®ï¼Œå¾€å¾€ä¾èµ–äººå·¥é€æ­¥éª¤æ ‡æ³¨ï¼Œæˆæœ¬é«˜ä¸”éš¾æ‰©å±•ã€‚äºæ˜¯ï¼Œæœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ï¼š**å¦‚ä½•è‡ªåŠ¨è·å–é«˜è´¨é‡æ¨ç†è½¨è¿¹ï¼Œå¹¶é«˜æ•ˆåˆ©ç”¨å¥–åŠ±ä¿¡å·å®ŒæˆéªŒè¯ä¸LLMè‡ªè®­ç»ƒ**ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReST-MCTS*æ¡†æ¶ï¼Œèåˆè¿‡ç¨‹å¥–åŠ±å¼•å¯¼ä¸MCTS*æ ‘æœç´¢  
ReST-MCTS*é‡‡ç”¨æ”¹è¿›çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTS*ï¼‰ä½œä¸ºæ¨ç†ç­–ç•¥ï¼Œç”±è®­ç»ƒå¥½çš„â€œé€æ­¥éª¤è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰â€å¼•å¯¼æœç´¢ã€‚å®ƒèƒ½åœ¨ç»™å®šæœ€ç»ˆæ­£ç¡®ç­”æ¡ˆï¼ˆoracle answerï¼‰æ—¶ï¼Œé€šè¿‡ä¼°è®¡æŸä¸€æ­¥éª¤å¯¼å‘æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œ**è‡ªåŠ¨æ¨æ–­æ¯ä¸€æ­¥çš„è¿‡ç¨‹å¥–åŠ±**â€”â€”æ— éœ€äººå·¥é€æ­¥éª¤æ ‡æ³¨ã€‚è¿™äº›æ¨æ–­å‡ºçš„å¥–åŠ±æœ‰ä¸¤å¤§ä½œç”¨ï¼šä¸€æ˜¯ä½œä¸ºâ€œå€¼ç›®æ ‡â€å»è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼›äºŒæ˜¯å¸®æˆ‘ä»¬ç­›é€‰é«˜è´¨é‡æ¨ç†è½¨è¿¹ï¼Œç”¨äºç­–ç•¥æ¨¡å‹çš„è‡ªè®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ ‡ç­¾ï¼Œæ‘†è„±äººå·¥ä¾èµ–  
ä»¥å¾€è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹éœ€è¦å¤§é‡äººå·¥é€æ­¥éª¤æ ‡æ³¨ï¼ŒReST-MCTS*åˆ™é€šè¿‡â€œå……åˆ†æ¬¡æ•°çš„rolloutï¼ˆæ¨æ¼”ï¼‰â€ï¼Œè‡ªåŠ¨ä¸ºæ¯ä¸ªä¸­é—´èŠ‚ç‚¹ç”Ÿæˆè¿‡ç¨‹å¥–åŠ±çš„æ ‡æ³¨ã€‚è¿™ä¸ªæ ‡æ³¨è¿‡ç¨‹èƒ½æœ‰æ•ˆè¿‡æ»¤å‡ºæœ€é«˜è´¨é‡çš„æ ·æœ¬å­é›†ï¼Œå…¨ç¨‹æ— éœ€é¢å¤–äººå·¥ä»‹å…¥ï¼Œè§£å†³äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æ ‡æ³¨ç“¶é¢ˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºæ ‘æœç´¢çš„å¼ºåŒ–å­¦ä¹ æ€è·¯ï¼Œæå‡æ¨ç†ä¸è‡ªè®­ç»ƒæ•ˆæœ  
ReST-MCTS*æŠŠæ ‘æœç´¢å’Œå¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œè®©MCTS*åœ¨ç›¸åŒæœç´¢é¢„ç®—ä¸‹ï¼Œæ¨ç†å‡†ç¡®ç‡è¶…è¶Šäº†Best-of-Nã€Tree-of-Thoughtç­‰ç»å…¸LLMæ¨ç†åŸºçº¿ï¼›åŒæ—¶ï¼Œç”¨MCTS*æœç´¢å¾—åˆ°çš„æ¨ç†è½¨è¿¹åšè®­ç»ƒæ•°æ®ï¼Œèƒ½åœ¨å¤šè½®è¿­ä»£ä¸­æŒç»­æå‡LLMæ€§èƒ½ï¼Œè¶…è¿‡ReST^EMã€Self-Rewarding LMç­‰è‡ªè®­ç»ƒç®—æ³•ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ¨ç†ç­–ç•¥å¯¹æ¯”ï¼šåœ¨SciBenchå’ŒMATHç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReST-MCTS*é‡Œçš„MCTS*æ ‘æœç´¢ç­–ç•¥ï¼Œ**åœ¨ç›¸åŒæœç´¢é¢„ç®—ä¸‹ï¼Œå‡†ç¡®ç‡è¶…è¿‡Self-Consistencyã€Best-of-Nç­‰åŸºçº¿æ–¹æ³•**ï¼ˆå¦‚å›¾2æ‰€ç¤ºï¼‰ã€‚  
2. è‡ªè®­ç»ƒæ•ˆæœï¼šç”¨MCTS*æœç´¢å‡ºçš„è½¨è¿¹åšè®­ç»ƒæ•°æ®ï¼Œå¤šè½®è¿­ä»£åèƒ½æŒç»­å¢å¼ºLLMæ€§èƒ½ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Š**è¶…è¶ŠReST^EMã€Self-Rewarding LMç­‰è‡ªè®­ç»ƒç®—æ³•**ï¼ˆå¦‚è¡¨2æ‰€ç¤ºï¼‰ã€‚  
3. è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è´¨é‡ï¼šReST-MCTS*é‡Œçš„å¥–åŠ±ç”Ÿæˆæœºåˆ¶ï¼Œç›¸æ¯”MATH-SHEPHERDç­‰ä»¥å¾€æŠ€æœ¯ï¼Œè®­ç»ƒå‡ºçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è´¨é‡æ›´ä¼˜ï¼ˆå¦‚è¡¨3æ‰€ç¤ºï¼‰ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§£å†³æ ‡æ³¨éš¾é¢˜çš„æ€è·¯ï¼šReST-MCTS*å±•ç¤ºäº†â€œå¦‚ä½•ç”¨ç®—æ³•è‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨â€æ¥è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä¸ºéœ€è¦å¯†é›†æ ‡æ³¨ä½†äººå·¥æˆæœ¬é«˜çš„ä»»åŠ¡æä¾›äº†â€œå»äººå·¥åŒ–â€çš„æŠ€æœ¯å‚è€ƒã€‚  
2. æ ‘æœç´¢+å¼ºåŒ–å­¦ä¹ åœ¨LLMæ¨ç†çš„åº”ç”¨ï¼šæŠŠMCTSè¿™ç±»ç»å…¸æ ‘æœç´¢ç®—æ³•ä¸è¿‡ç¨‹å¥–åŠ±å¼•å¯¼ç»“åˆï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†ç­–ç•¥çš„ä¼˜åŒ–æä¾›äº†æ–°èŒƒå¼ï¼Œè¯æ˜äº†æ¨¡å‹-based RLåœ¨è‡ªè®­ç»ƒé‡Œçš„æ½œåŠ›ã€‚  
3. è‡ªè®­ç»ƒæ•°æ®è´¨é‡çš„æå‡ï¼šé€šè¿‡â€œç­›é€‰é«˜è´¨é‡æ¨ç†è½¨è¿¹â€æ¥åšè‡ªè®­ç»ƒï¼Œç‚¹æ˜äº†â€œæ•°æ®è´¨é‡â€åœ¨å¤§æ¨¡å‹è¿­ä»£ä¼˜åŒ–ä¸­çš„å…³é”®ä½œç”¨ï¼Œåç»­å·¥ä½œå¯å€Ÿé‰´è¿™ç±»â€œå…ˆç­›åè®­â€çš„æ€è·¯æå‡å¾®è°ƒæ•ˆæœã€‚  
```

## skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy
### Abstract
Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-Reward-V2ï¼šäººæœºååŒè§£é”åå¥½æ•°æ®è§„æ¨¡ä¸è´¨é‡æ–°é«˜åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­è‡³å…³é‡è¦ï¼Œä½†å½“å‰å¼€æºå¥–åŠ±æ¨¡å‹åœ¨å¤šæ•°è¯„ä¼°åŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ•æ‰å¤æ‚ç²¾å¦™çš„äººç±»åå¥½ã€‚ç©¶å…¶åŸå› ï¼Œç°æœ‰åå¥½æ•°æ®é›†å­˜åœ¨èŒƒå›´ç‹­çª„ã€æ ‡ç­¾åˆæˆæ€§å¼ºæˆ–è´¨é‡æ§åˆ¶ä¸ä¸¥è°¨ç­‰å±€é™ï¼Œå³ä¾¿é‡‡ç”¨å…ˆè¿›è®­ç»ƒæŠ€æœ¯ä¹Ÿéš¾æœ‰å®è´¨æ€§æå‡ã€‚å› æ­¤ï¼Œæå‡åå¥½æ•°æ®è´¨é‡ä»¥æ¨åŠ¨å¼€æºå¥–åŠ±æ¨¡å‹å‘å±•æˆä¸ºå…³é”®è¯‰æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºè¶…å¤§è§„æ¨¡åå¥½æ•°æ®é›†SynPref - 40M  
æ‰“é€ äº†åŒ…å«4000ä¸‡åå¥½å¯¹çš„å¤§è§„æ¨¡åå¥½æ•°æ®é›†SynPref - 40Mï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æ•°æ®åŸºç¡€ï¼Œè¿™ä¹Ÿæ˜¯ç›®å‰å·²çŸ¥è§„æ¨¡æœ€å¤§çš„ç²¾å¿ƒæ•´ç†åå¥½æ··åˆæ•°æ®é›†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äººæœºååŒä¸¤é˜¶æ®µæ•°æ®æ•´ç† pipeline  
ç¬¬ä¸€é˜¶æ®µå€ŸåŠ©ä¸¥æ ¼åè®®ä¸‹çš„äººå·¥éªŒè¯ä¿éšœæ•°æ®è´¨é‡ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨äººç±»åå¥½å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºâ€œè£åˆ¤â€å®ç°è§„æ¨¡åŒ–æ•´ç†ï¼ŒåŒæ—¶ç»“åˆå¥–åŠ±æ¨¡å‹çš„è¿­ä»£è®­ç»ƒï¼ŒæŒç»­çº³å…¥äººå·¥æ ‡ç­¾åé¦ˆå¹¶å¬å›æ¨¡å‹è¡¨ç°å·®çš„åå¥½æ•°æ®ä»¥ä¿ƒè¿›å­¦ä¹ ï¼Œæœ€ç»ˆå¾—åˆ°2600ä¸‡é«˜è´¨é‡åå¥½å¯¹ç”¨äºæ¨¡å‹è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨å‡ºSkywork - Reward - V2ç³»åˆ—å¥–åŠ±æ¨¡å‹  
åŸºäºSynPref - 40Mç­›é€‰å‡ºçš„åå¥½æ•°æ®ï¼Œè®­ç»ƒå‡ºåŒ…å«8ä¸ªä»0.6Båˆ°8Bå‚æ•°è§„æ¨¡çš„Skywork - Reward - V2ç³»åˆ—å¥–åŠ±æ¨¡å‹ï¼Œä»…ç”¨Bradley - Terryç›®æ ‡å‡½æ•°è®­ç»ƒå´èƒ½åœ¨å¤šåŸºå‡†å±•ç°å“è¶Šæ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨7ä¸ªä¸»è¦å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSkywork - Reward - V2ç³»åˆ—è¡¨ç°äº®çœ¼ï¼Œ8Bè§„æ¨¡æ¨¡å‹åœ¨æ‰€æœ‰7ä¸ªåŸºå‡†ä¸Šå¤§å¹…è¶…è¶Šç°æœ‰å¼€æºå¥–åŠ±æ¨¡å‹ï¼›åœ¨äººç±»åå¥½å¯¹é½ã€å®¢è§‚æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€æŠ—é£æ ¼åå·®ã€best - of - Nç¼©æ”¾ç­‰å¤šå…³é”®ç»´åº¦ä¹Ÿå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒSynPref - 40Mçš„æˆåŠŸæ—¢æºäºè§„æ¨¡ä¹Ÿå¾—ç›Šäºé«˜è´¨é‡ï¼ŒåŒæ—¶äººæœºååŒ pipeline ä¸­äººå·¥æ ‡æ³¨ã€äººç±»åå¥½å¼•å¯¼çš„LLMæ ‡æ³¨åŠä¸¥è°¨æ ‡æ³¨åè®®éƒ½è‡³å…³é‡è¦ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ•°æ®å±‚é¢ï¼Œå¤§è§„æ¨¡ä¸”é«˜è´¨é‡çš„åå¥½æ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½æå‡ä½œç”¨æ˜¾è‘—ï¼ŒSynPref - 40Mçš„æ„å»ºæ€è·¯ä¸ºæ•°æ®é©±åŠ¨çš„æ¨¡å‹ä¼˜åŒ–æä¾›èŒƒä¾‹ï¼›æ–¹æ³•å±‚é¢ï¼ŒäººæœºååŒçš„ä¸¤é˜¶æ®µæ•°æ®æ•´ç† pipeline æœ‰æ•ˆç»“åˆäººç±»æ ‡æ³¨è´¨é‡ä¸AIå¯æ‰©å±•æ€§ä¼˜åŠ¿ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®é«˜è´¨é‡æ•´ç†æä¾›äº†å¯å‚è€ƒçš„æµç¨‹æ¡†æ¶ï¼›æ¨¡å‹å±‚é¢ï¼ŒSkywork - Reward - V2ç³»åˆ—è¯æ˜åˆç†åˆ©ç”¨æ•°æ®ä¸è®­ç»ƒç­–ç•¥ï¼Œèƒ½åœ¨å¼€æºå¥–åŠ±æ¨¡å‹é¢†åŸŸå®ç°æ€§èƒ½çªç ´ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹ç ”å‘æŒ‡æ˜æ–¹å‘ï¼Œå‡¸æ˜¾äº†æŒ–æ˜ç°æœ‰åå¥½æ•°æ®æ½œåŠ›ä¸äººæœºååŒåœ¨æå‡æ•°æ®è´¨é‡ä¸Šçš„ä»·å€¼ã€‚

