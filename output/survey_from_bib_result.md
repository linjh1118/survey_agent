# Paper List from BIB File: tmp9a7_b2by.bib
- [25/09] **ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models**  
[[Paper](https://arxiv.org/pdf/2509.21826v1)] [[Code/Page]()] [[TLDR/Notes](#rest--reshaping-token-level-policy-gradients-for-tool-use-large-language-models)]

- [25/06] **AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning**  
[[Paper](https://arxiv.org/pdf/2506.15651v1)] [[Code/Page](https://github.com/cxcscmu/AutoRule.)] [[TLDR/Notes](#autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning)]

- [25/07] **QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation**  
[[Paper](https://arxiv.org/pdf/2507.13266v3)] [[Code/Page](https://github.com/foreverlasting1202/QuestA.)] [[TLDR/Notes](#questa--expanding-reasoning-capacity-in-llms-via-question-augmentation)]

- [25/09] **When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training**  
[[Paper](https://arxiv.org/pdf/2509.24923v1)] [[Code/Page]()] [[TLDR/Notes](#when-greedy-wins--emergent-exploitation-bias-in-meta-bandit-llm-training)]

- [25/10] **PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity**  
[[Paper](https://arxiv.org/pdf/2510.04080v1)] [[Code/Page]()] [[TLDR/Notes](#poli-rl--a-point-to-list-reinforcement-learning-framework-for-conditional-semantic-textual-similarity)]

- [25/09] **Dynamic Speculative Agent Planning**  
[[Paper](https://arxiv.org/pdf/2509.01920v3)] [[Code/Page](https://github.com/guanyilin428/Dynamic-Speculative-Planning.)] [[TLDR/Notes](#dynamic-speculative-agent-planning)]

- [25/09] **Reasoning Scaffolding: Distilling the Flow of Thought from LLMs**  
[[Paper](https://arxiv.org/pdf/2509.23619v2)] [[Code/Page]()] [[TLDR/Notes](#reasoning-scaffolding--distilling-the-flow-of-thought-from-llms)]

- [25/10] **Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents**  
[[Paper](https://arxiv.org/pdf/2510.25694v1)] [[Code/Page]()] [[TLDR/Notes](#process-level-trajectory-evaluation-for-environment-configuration-in-software-engineering-agents)]

- [25/10] **MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games**  
[[Paper](https://arxiv.org/pdf/2510.15414v1)] [[Code/Page](https://github.com/thu-nics/MARS.)] [[TLDR/Notes](#mars--reinforcing-multi-agent-reasoning-of-llms-through-self-play-in-strategic-games)]

- [25/09] **Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization**  
[[Paper](https://arxiv.org/pdf/2509.22115v1)] [[Code/Page]()] [[TLDR/Notes](#learning-more-with-less--a-dynamic-dual-level-down-sampling-framework-for-efficient-policy-optimization)]

- [25/08] **Deep Think with Confidence**  
[[Paper](https://arxiv.org/pdf/2508.15260v1)] [[Code/Page]()] [[TLDR/Notes](#deep-think-with-confidence)]

- [25/09] **MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems**  
[[Paper](https://arxiv.org/pdf/2509.24323v1)] [[Code/Page](https://github.com/yeyeyeah2/MAS2.)] [[TLDR/Notes](#mas$^2$--self-generative--self-configuring--self-rectifying-multi-agent-systems)]

- [25/05] **Token-Importance Guided Direct Preference Optimization**  
[[Paper](https://arxiv.org/pdf/2505.19653v1)] [[Code/Page]()] [[TLDR/Notes](#token-importance-guided-direct-preference-optimization)]

- [25/09] **Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training**  
[[Paper](https://arxiv.org/pdf/2509.21500v1)] [[Code/Page](https://github.com/Jun-Kai-Zhang/rubrics.git)] [[TLDR/Notes](#chasing-the-tail--effective-rubric-based-reward-modeling-for-large-language-model-post-training)]

- [25/08] **Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks**  
[[Paper](https://arxiv.org/pdf/2508.18672v2)] [[Code/Page](https://github.com/rioyokotalab/optimal-sparsity.)] [[TLDR/Notes](#optimal-sparsity-of-mixture-of-experts-language-models-for-reasoning-tasks)]

- [25/05] **RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning**  
[[Paper](https://arxiv.org/pdf/2505.14140v2)] [[Code/Page](https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30)] [[TLDR/Notes](#rl-of-thoughts--navigating-llm-reasoning-with-inference-time-reinforcement-learning)]

- [25/11] **Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World**  
[[Paper](https://arxiv.org/pdf/2511.00041v1)] [[Code/Page]()] [[TLDR/Notes](#endowing-gpt-4-with-a-humanoid-body--building-the-bridge-between-off-the-shelf-vlms-and-the-physical-world)]

- [25/10] **Rethinking Reward Models for Multi-Domain Test-Time Scaling**  
[[Paper](https://arxiv.org/pdf/2510.00492v2)] [[Code/Page](https://github.com/db-Lee/Multi-RM}{\underline{\small\texttt{https://github.com/db-Lee/Multi-RM}}})] [[TLDR/Notes](#rethinking-reward-models-for-multi-domain-test-time-scaling)]

- [25/09] **OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds**  
[[Paper](https://arxiv.org/pdf/2509.02322v1)] [[Code/Page]()] [[TLDR/Notes](#omniactor--a-generalist-gui-and-embodied-agent-for-2d&3d-worlds)]

- [25/10] **Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents**  
[[Paper](https://arxiv.org/pdf/2510.22620v1)] [[Code/Page]()] [[TLDR/Notes](#breaking-agent-backbones--evaluating-the-security-of-backbone-llms-in-ai-agents)]

- [25/09] **Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes**  
[[Paper](https://arxiv.org/pdf/2509.06266v2)] [[Code/Page]()] [[TLDR/Notes](#spatial-reasoning-with-vision-language-models-in-ego-centric-multi-view-scenes)]

- [25/05] **From Single to Multi-Granularity: Toward Long-Term Memory Association and Selection of Conversational Agents**  
[[Paper](https://arxiv.org/pdf/2505.19549v2)] [[Code/Page](https://github.com/quqxui/MemGAS})] [[TLDR/Notes](#from-single-to-multi-granularity--toward-long-term-memory-association-and-selection-of-conversational-agents)]

- [25/09] **ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data**  
[[Paper](https://arxiv.org/pdf/2509.15221v2)] [[Code/Page](https://github.com/OpenGVLab/ScaleCUA.)] [[TLDR/Notes](#scalecua--scaling-open-source-computer-use-agents-with-cross-platform-data)]

- [25/09] **AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?**  
[[Paper](https://arxiv.org/pdf/2509.03312v2)] [[Code/Page]()] [[TLDR/Notes](#agentracer--who-is-inducing-failure-in-the-llm-agentic-systems-)]

- [25/02] **InSTA: Towards Internet-Scale Training For Agents**  
[[Paper](https://arxiv.org/pdf/2502.06776v2)] [[Code/Page](https://data-for-agents.github.io.)] [[TLDR/Notes](#insta--towards-internet-scale-training-for-agents)]

- [25/09] **Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents**  
[[Paper](https://arxiv.org/pdf/2509.23045v2)] [[Code/Page]()] [[TLDR/Notes](#kimi-dev--agentless-training-as-skill-prior-for-swe-agents)]

- [25/09] **Estimating the Empowerment of Language Model Agents**  
[[Paper](https://arxiv.org/pdf/2509.22504v2)] [[Code/Page]()] [[TLDR/Notes](#estimating-the-empowerment-of-language-model-agents)]

- [25/09] **Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning**  
[[Paper](https://arxiv.org/pdf/2509.26383v3)] [[Code/Page](https://github.com/Jinyeop3110/KG-R1.)] [[TLDR/Notes](#efficient-and-transferable-agentic-knowledge-graph-rag-via-reinforcement-learning)]

- [25/10] **AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework**  
[[Paper](https://arxiv.org/pdf/2510.04206v1)] [[Code/Page](https://github.com/THUDM/AgentRL.)] [[TLDR/Notes](#agentrl--scaling-agentic-reinforcement-learning-with-a-multi-turn--multi-task-framework)]

- [25/05] **Learning to Reason without External Rewards**  
[[Paper](https://arxiv.org/pdf/2505.19590v2)] [[Code/Page](https://github.com/sunblaze-ucb/Intuitor)] [[TLDR/Notes](#learning-to-reason-without-external-rewards)]

- [25/10] **Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead**  
[[Paper](https://arxiv.org/pdf/2510.01624v1)] [[Code/Page]()] [[TLDR/Notes](#quagmires-in-sft-rl-post-training--when-high-sft-scores-mislead-and-what-to-use-instead)]

- [25/10] **LightMem: Lightweight and Efficient Memory-Augmented Generation**  
[[Paper](https://arxiv.org/pdf/2510.18866v1)] [[Code/Page](https://github.com/zjunlp/LightMem.)] [[TLDR/Notes](#lightmem--lightweight-and-efficient-memory-augmented-generation)]

- [25/08] **Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR**  
[[Paper](https://arxiv.org/pdf/2508.14029v3)] [[Code/Page]()] [[TLDR/Notes](#beyond-pass@1--self-play-with-variational-problem-synthesis-sustains-rlvr)]

- [25/10] **Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents**  
[[Paper](https://arxiv.org/pdf/2510.24702v1)] [[Code/Page]()] [[TLDR/Notes](#agent-data-protocol--unifying-datasets-for-diverse--effective-fine-tuning-of-llm-agents)]

- [25/09] **Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE**  
[[Paper](https://arxiv.org/pdf/2509.24130v2)] [[Code/Page]()] [[TLDR/Notes](#beyond-magic-words--sharpness-aware-prompt-evolving-for-robust-large-language-models-with-tare)]

- [25/09] **MemGen: Weaving Generative Latent Memory for Self-Evolving Agents**  
[[Paper](https://arxiv.org/pdf/2509.24704v2)] [[Code/Page]()] [[TLDR/Notes](#memgen--weaving-generative-latent-memory-for-self-evolving-agents)]

- [25/05] **Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning**  
[[Paper](https://arxiv.org/pdf/2505.16122v2)] [[Code/Page](https://github.com/junhongmit/P-and-B.)] [[TLDR/Notes](#plan-and-budget--effective-and-efficient-test-time-scaling-on-large-language-model-reasoning)]

- [25/11] **Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph**  
[[Paper](https://arxiv.org/pdf/2511.00086v1)] [[Code/Page]()] [[TLDR/Notes](#generalizing-test-time-compute-optimal-scaling-as-an-optimizable-graph)]

- [25/10] **On Predictability of Reinforcement Learning Dynamics for Large Language Models**  
[[Paper](https://arxiv.org/pdf/2510.00553v2)] [[Code/Page]()] [[TLDR/Notes](#on-predictability-of-reinforcement-learning-dynamics-for-large-language-models)]

- [25/10] **In-the-Flow Agentic System Optimization for Effective Planning and Tool Use**  
[[Paper](https://arxiv.org/pdf/2510.05592v1)] [[Code/Page]()] [[TLDR/Notes](#in-the-flow-agentic-system-optimization-for-effective-planning-and-tool-use)]

- [25/07] **Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification**  
[[Paper](https://arxiv.org/pdf/2507.11662v1)] [[Code/Page]()] [[TLDR/Notes](#let-s-think-in-two-steps--mitigating-agreement-bias-in-mllms-with-self-grounded-verification)]

- [25/09] **Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play**  
[[Paper](https://arxiv.org/pdf/2509.25541v1)] [[Code/Page](https://github.com/wangqinsi1/Vision-Zero.)] [[TLDR/Notes](#vision-zero--scalable-vlm-self-improvement-via-strategic-gamified-self-play)]

- [25/05] **Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents**  
[[Paper](https://arxiv.org/pdf/2505.12065v1)] [[Code/Page](https://github.com/tiannuo-yang/SearchAgent-X.)] [[TLDR/Notes](#demystifying-and-enhancing-the-efficiency-of-large-language-model-based-search-agents)]

- [25/10] **EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems**  
[[Paper](https://arxiv.org/pdf/2510.13220v1)] [[Code/Page]()] [[TLDR/Notes](#evotest--evolutionary-test-time-learning-for-self-improving-agentic-systems)]

- [25/09] **OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment**  
[[Paper](https://arxiv.org/pdf/2509.24610v2)] [[Code/Page]()] [[TLDR/Notes](#orthalign--orthogonal-subspace-decomposition-for-non-interfering-multi-objective-alignment)]

- [25/05] **Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration**  
[[Paper](https://arxiv.org/pdf/2505.17621v5)] [[Code/Page]()] [[TLDR/Notes](#navigate-the-unknown--enhancing-llm-reasoning-with-intrinsic-motivation-guided-exploration)]

- [25/06] **Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions**  
[[Paper](https://arxiv.org/pdf/2506.07527v2)] [[Code/Page]()] [[TLDR/Notes](#learning-what-reinforcement-learning-can-t--interleaved-online-fine-tuning-for-hardest-questions)]

- [25/10] **Reinforced Preference Optimization for Recommendation**  
[[Paper](https://arxiv.org/pdf/2510.12211v1)] [[Code/Page]()] [[TLDR/Notes](#reinforced-preference-optimization-for-recommendation)]

- [25/09] **RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards**  
[[Paper](https://arxiv.org/pdf/2509.21319v2)] [[Code/Page](https://huggingface.co/collections/nvidia/reward-models-10-2025)] [[TLDR/Notes](#rlbff--binary-flexible-feedback-to-bridge-between-human-feedback-&-verifiable-rewards)]

- [25/03] **MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow**  
[[Paper](https://arxiv.org/pdf/2503.18968v3)] [[Code/Page](https://github.com/jinlab-imvr/MedAgent-Pro.)] [[TLDR/Notes](#medagent-pro--towards-evidence-based-multi-modal-medical-diagnosis-via-reasoning-agentic-workflow)]

- [25/10] **GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time**  
[[Paper](https://arxiv.org/pdf/2510.03777v1)] [[Code/Page]()] [[TLDR/Notes](#guidedsampling--steering-llms-towards-diverse-candidate-solutions-at-inference-time)]



# TLDR/Notes
## rest--reshaping-token-level-policy-gradients-for-tool-use-large-language-models
### Abstract
Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ResTï¼šé‡å¡‘å·¥å…·ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œçº§ç­–ç•¥æ¢¯åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä»Šä¸ä»…èƒ½è¢«åŠ¨ç”Ÿæˆå†…å®¹ï¼Œè¿˜èƒ½é€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·æˆä¸ºç›®æ ‡å¯¼å‘çš„æ™ºèƒ½ä½“ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºä¼˜åŒ–è¿™äº›æ–°å…´çš„å·¥å…·ä½¿ç”¨ç­–ç•¥æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ï¼Œä½†ç›®å‰ä¸»æµèŒƒå¼ä»…ä¾èµ–ç¨€ç–çš„ç»“æœå¥–åŠ±ï¼Œä¸”æœªè€ƒè™‘å·¥å…·ä½¿ç”¨ä»»åŠ¡çš„ç‰¹æ®Šæ€§ï¼Œè¿™å¯¼è‡´ç­–ç•¥æ¢¯åº¦æ–¹å·®å¢å¤§ï¼Œè®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚æ­¤å¤–ï¼Œå¤šè½®å·¥å…·ä½¿ç”¨ä»»åŠ¡å­˜åœ¨å¥–åŠ±è®¾è®¡å™ªå£°å¤§ã€ç³»ç»Ÿæ•ˆç‡ä½ç­‰é—®é¢˜ï¼Œå…ˆå‰å·¥ä½œè¿˜å¸¸å¿½è§†ä»¤ç‰Œåœ¨è®­ç»ƒæ—©æœŸè´¡çŒ®å·®å¼‚çš„é—®é¢˜ã€‚ä¸ºæœ‰æ•ˆä¸”é«˜æ•ˆåœ°è®­ç»ƒå·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“ï¼Œæœ¬æ–‡æå‡ºåŸºäºå•è½® RL è®­ç»ƒé‡å¡‘ä»¤ç‰Œçº§ç­–ç•¥æ¢¯åº¦ï¼ˆResTï¼‰çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå»ºç«‹ç†è®ºä¸å®è¯è”ç³»
å»ºç«‹äº†ç­–ç•¥ç†µä¸å·¥å…·ä½¿ç”¨ä»»åŠ¡è®­ç»ƒç¨³å®šæ€§ä¹‹é—´çš„ç†è®ºå’Œå®è¯è”ç³»ã€‚å…·ä½“è¡¨æ˜è¾ƒä½çš„å¹³å‡ç†µä¸ç­–ç•¥æ¢¯åº¦æ›´æ–°ä¸­çš„æ–¹å·®å‡å°‘ç›¸å…³ï¼Œæ­ç¤ºäº†è¯¸å¦‚å·¥å…·åç§°å’Œå‚æ•°ç­‰ç»“æ„åŒ–ã€ä½ç†µä»¤ç‰Œæ˜¯å¥–åŠ±çš„ä¸»è¦å†³å®šå› ç´ ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºç†µæ„ŸçŸ¥ä»¤ç‰Œçº§é‡å¡‘æœºåˆ¶
å¼•å…¥äº†ä¸€ç§ç»“åˆè¯¾ç¨‹å­¦ä¹ çš„ç†µæ„ŸçŸ¥ä»¤ç‰Œçº§é‡å¡‘æœºåˆ¶ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œå·¥å…·ä½¿ç”¨å’Œæ¨ç†ä»¤ç‰Œæ ¹æ®å…¶åŒºåŸŸçº§å¹³å‡ç†µè¿›è¡ŒåŠ æƒï¼Œè¿™ç›´æ¥å½±å“è®­ç»ƒç¨³å®šæ€§å¹¶è°ƒèŠ‚ä¸åŒä»¤ç‰Œç±»åˆ«å¯¹å¥–åŠ±ä¿¡å·çš„è´¡çŒ®ã€‚éšç€è®­ç»ƒè¿›è¡Œå’Œç†µé™ä½ï¼Œæ¨ç†ä»¤ç‰Œçš„æƒé‡é€æ¸å¢åŠ ï¼Œç¨³å®šæ”¶æ•›å¹¶ç³»ç»Ÿåœ°å¢å¼ºæ¨¡å‹åœ¨å¤æ‚å¤šè½®å·¥å…·è°ƒç”¨åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ BFCL å’Œ API - Bank ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒResT å–å¾—äº†é¢†å…ˆçš„ç»“æœï¼Œæ¯”å…ˆå‰æ–¹æ³•é«˜å‡º 8.76%ã€‚åœ¨ 4B åŸºç¡€ LLM ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼ŒResT åœ¨å•è½®ä»»åŠ¡ä¸Šæ¯” GPT - 4o é«˜å‡º 4.11%ï¼Œåœ¨å¤šè½®åŸºç¡€ä»»åŠ¡ä¸Šé«˜å‡º 1.50%ã€‚æ­¤å¤–ï¼Œå…¨é¢çš„æ¶ˆèç ”ç©¶æ˜¾ç¤ºï¼ŒåŸºäºè¯¾ç¨‹çš„é‡å¡‘æ¯”é™æ€å¥–åŠ±åŠ æƒé«˜å‡º 4.86%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºè”ç³»çš„å»ºç«‹**ï¼šè®ºæ–‡ä¸­å»ºç«‹ç­–ç•¥ç†µä¸è®­ç»ƒç¨³å®šæ€§è”ç³»çš„æ€è·¯å’Œæ–¹æ³•ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ä¸€ç§ä»ç†è®ºå±‚é¢åˆ†æå’Œä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹çš„æ–¹å‘ï¼Œæœ‰åŠ©äºæ·±å…¥ç†è§£è®­ç»ƒä¸­çš„å…³é”®å› ç´ ã€‚
2. **ä»¤ç‰Œçº§é‡å¡‘æœºåˆ¶**ï¼šç†µæ„ŸçŸ¥ä»¤ç‰Œçº§é‡å¡‘æœºåˆ¶ç»“åˆè¯¾ç¨‹å­¦ä¹ çš„æ–¹å¼ï¼Œä¸ºè§£å†³å¤šè½®å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸­çš„è®­ç»ƒé—®é¢˜æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œåœ¨è®¾è®¡è®­ç»ƒæœºåˆ¶æ—¶å¯å€Ÿé‰´è¿™ç§æ ¹æ®ä¸åŒé˜¶æ®µè°ƒæ•´ä»¤ç‰Œæƒé‡çš„æ€è·¯ã€‚
3. **å®éªŒéªŒè¯**ï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢å®éªŒå’Œæ¶ˆèç ”ç©¶çš„åšæ³•ï¼Œä¸ºéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§æä¾›äº†å¯é çš„ä¾æ®ï¼Œåœ¨å¼€å±•ç›¸å…³ç ”ç©¶æ—¶å¯å‚è€ƒè¿™ç§ä¸¥è°¨çš„å®éªŒéªŒè¯æ–¹å¼ã€‚
``` 

## autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning
### Abstract
Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | AutoRuleï¼šè‡ªåŠ¨æå–è§„åˆ™åŠ©åŠ›åå¥½å­¦ä¹ æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºä½¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½ã€æå‡å…¶éµå¾ªäººç±»æŒ‡ä»¤èƒ½åŠ›çš„å…³é”®æŠ€æœ¯ï¼Œè¢«å¹¿æ³›åº”ç”¨äºGPT - 4ã€Geminiç­‰é¡¶çº§æ¨¡å‹ã€‚åŸºäºè§„åˆ™çš„å¥–åŠ±åœ¨æ¨ç†ä»»åŠ¡ä¸­èƒ½æä¾›å®¢è§‚ã€å¯éªŒè¯æ ‡å‡†ï¼Œå¯¹RLHFæœ‰æå‡æ½œåŠ›ï¼Œä½†å½“å‰åˆ©ç”¨å…¶è¿›è¡Œè¯­è¨€æ¨¡å‹çš„åå¥½å¯¹é½å­˜åœ¨æŒ‘æˆ˜ã€‚å› ä¸ºäººç±»åå¥½å¾€å¾€æ¨¡ç³Šä¸”ä¸»è§‚ï¼Œç°æœ‰çš„è¡Œä¸šæ–¹æ³•é€šå¸¸ä¾èµ–ä¸“å®¶æ‰‹å·¥åˆ¶å®šè§„åˆ™æˆ–å¤§è§„æ¨¡ä¼—åŒ…æ³¨é‡Šï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºAutoRuleï¼Œæ—¨åœ¨è‡ªåŠ¨ä»åå¥½åé¦ˆä¸­æå–è§„åˆ™å¹¶åˆ¶å®šä¸ºåŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªåŠ¨è§„åˆ™æå–æ¡†æ¶
AutoRuleæ˜¯ä¸€ç§å…¨è‡ªåŠ¨æ–¹æ³•ï¼Œå…¶æå–è¿‡ç¨‹åˆ†ä¸‰ä¸ªé˜¶æ®µã€‚é¦–å…ˆåˆ©ç”¨æ¨ç†æ¨¡å‹è§£é‡Šç”¨æˆ·åå¥½ï¼Œæ¥ç€ä»è¿™äº›è§£é‡Šçš„æ¨ç†é“¾ä¸­è¯†åˆ«å€™é€‰è§„åˆ™ï¼Œæœ€åå°†å€™é€‰è§„åˆ™åˆæˆç»Ÿä¸€è§„åˆ™é›†ã€‚è¯¥æ¡†æ¶æ‘†è„±äº†å¯¹æ‰‹å·¥åˆ¶ä½œæˆ–ä¼—åŒ…è§„åˆ™çš„ä¾èµ–ï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºåˆ¤æ–­å™¨ï¼ˆLLM - as - a - judgeï¼‰çš„éªŒè¯å™¨ï¼Œæ ¹æ®æœ€ç»ˆè§„åˆ™é›†è®¡ç®—æ¯ä¸ªè¾“å‡ºæ»¡è¶³è§„åˆ™çš„æ¯”ä¾‹ï¼Œå¹¶å°†å…¶ä½œä¸ºè¾…åŠ©å¥–åŠ±ç”¨äºç­–ç•¥ä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºæ¨ç†é“¾çš„è§„åˆ™æå–
ç»™å®šæ¨¡å‹è¾“å‡ºå¯¹å’Œç›¸å…³åå¥½æ ‡ç­¾ï¼Œå…ˆä¿ƒä½¿æœ‰æ¨ç†èƒ½åŠ›çš„LLMä¸ºé¦–é€‰è¾“å‡ºç”Ÿæˆé€æ­¥çš„ç†ç”±ï¼Œç„¶åè®©LLMä»æ¨ç†è¿‡ç¨‹ä¸­æå–ç±»ä¼¼è§„åˆ™çš„æ˜ç¡®é™ˆè¿°ï¼Œå°†è®­ç»ƒé›†ä¸­çš„å€™é€‰è§„åˆ™èšåˆåï¼Œç”±LLMåˆæˆç»¼åˆè§„åˆ™é›†ã€‚åˆ©ç”¨æ¨ç†é“¾çš„é€»è¾‘ç»“æ„ï¼Œæœ‰æœ›æå–æ›´ç²¾ç¡®ã€å¯æ“ä½œä¸”èƒ½æ›´å¥½æ•æ‰æ½œåœ¨åå¥½æ ‡å‡†çš„è§„åˆ™ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- **è§„åˆ™ä¸åå¥½ä¸€è‡´æ€§**ï¼šä»¥Llama 3 8B Instructä¸ºéªŒè¯å™¨è®¡ç®—çš„åŸºäºè§„åˆ™çš„åˆ†æ•°ï¼Œæ— è®ºæ˜¯å•ç‹¬è¿˜æ˜¯ç´¯ç§¯è®¡ç®—ï¼Œéƒ½ä¸UltraFeedbackå’ŒMT - Bench Human Judgmentæ•°æ®é›†ä¸Šçš„åå¥½é«˜åº¦ä¸€è‡´ã€‚
- **æ¨¡å‹è®­ç»ƒæ€§èƒ½æå‡**ï¼šåœ¨UltraFeedbackæ•°æ®ä¸Šä½¿ç”¨æ ‡å‡†RLHFç®¡é“å¯¹åŸºç¡€Llama - 3 - 8Bæ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œå°†ä¼ ç»ŸPPOæ›¿æ¢ä¸ºGRPOå¹¶é›†æˆAutoRuleä½œä¸ºå¥–åŠ±æœºåˆ¶ã€‚åœ¨UltraFeedbackèƒœç‡ã€AlpacaEval 2.0å’ŒMT - Benchçš„è¯„ä¼°ä¸­ï¼ŒAutoRuleå§‹ç»ˆä¼˜äºåŒ…æ‹¬ä»…ä½¿ç”¨æ¨¡å‹å¥–åŠ±çš„æ™®é€šPPOå’ŒGRPOåœ¨å†…çš„å¤šä¸ªåŸºçº¿ã€‚ä¾‹å¦‚ï¼Œä¸ä½¿ç”¨ç›¸åŒå­¦ä¹ å¥–åŠ±æ¨¡å‹ä½†æ— åŸºäºè§„åˆ™è¾…åŠ©å¥–åŠ±çš„GRPOåŸºçº¿ç›¸æ¯”ï¼Œåœ¨AlpacaEval2.0ä¸Šé•¿åº¦æ§åˆ¶èƒœç‡ç›¸å¯¹æé«˜28.6%ï¼Œåœ¨ä¿ç•™çš„MT - Benchå­é›†ä¸Šç¬¬äºŒè½®æ€§èƒ½ç›¸å¯¹æé«˜6.1%ã€‚
- **ç¼“è§£å¥–åŠ±æ¨¡å‹è¿‡ä¼˜åŒ–**ï¼šå¥–åŠ±ç ´è§£å®éªŒè¡¨æ˜ï¼ŒAutoRuleçš„åŸºäºè§„åˆ™çš„å¥–åŠ±èƒ½å¤Ÿç¼“è§£å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚
- **è§„åˆ™æå–æœ‰æ•ˆæ€§éªŒè¯**ï¼šå¯¹æ¯”ä»æ¨ç†é“¾å’Œç†ç”±ä¸­æå–è§„åˆ™çš„æ¶ˆèç ”ç©¶ï¼Œæ”¯æŒäº†åœ¨AutoRuleä¸­åˆ©ç”¨æ¨ç†é“¾çš„æœ‰æ•ˆæ€§ã€‚
- **è§„åˆ™ç‰¹æ€§åˆ†æ**ï¼šå®šæ€§åˆ†ææ˜¾ç¤ºï¼Œä»UltraFeedbackä¸­æå–çš„è§„åˆ™ä¸»è¦å¼ºè°ƒå¯¹è¯è´¨é‡ï¼Œè€Œä»MT - Benchä¸­æå–çš„è§„åˆ™åˆ™æ›´æ³¨é‡æŒ‡ä»¤éµå¾ªä»¥åŠåœ¨æ›´å¤æ‚ä»»åŠ¡ä¸Šçš„ç¨³å¥æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- **è‡ªåŠ¨åŒ–è§„åˆ™æå–æ€è·¯**ï¼šAutoRuleæä¾›äº†ä¸€ç§ä»åå¥½æ•°æ®ä¸­è‡ªåŠ¨æå–è§„åˆ™çš„å…¨æ–°æ€è·¯ï¼Œæ‘†è„±äº†å¯¹äººå·¥è§„åˆ™å·¥ç¨‹çš„ä¾èµ–ï¼Œä¸ºè§£å†³åå¥½å¯¹é½ä¸­è§„åˆ™è·å–éš¾é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå…¶ä»–ç ”ç©¶åœ¨æ„å»ºè§„åˆ™æ—¶å¯å€Ÿé‰´å…¶è‡ªåŠ¨åŒ–æµç¨‹ã€‚
- **åˆ©ç”¨æ¨ç†é“¾æå–è§„åˆ™**ï¼šé€šè¿‡åˆ©ç”¨æ¨ç†é“¾çš„é€»è¾‘ç»“æ„æå–è§„åˆ™ï¼Œä¸ºæŒ–æ˜æ›´ç²¾å‡†ã€ç¬¦åˆæ½œåœ¨åå¥½æ ‡å‡†çš„è§„åˆ™æä¾›äº†æ–°æ–¹æ³•ï¼Œåœ¨å…¶ä»–éœ€è¦ä»å¤æ‚é€»è¾‘ä¸­æå–è§„åˆ™çš„åœºæ™¯ä¸­å…·æœ‰å‚è€ƒä»·å€¼ã€‚
- **ç¼“è§£å¥–åŠ±æ¨¡å‹è¿‡ä¼˜åŒ–**ï¼šå…¶åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶åœ¨ç¼“è§£å¥–åŠ±æ¨¡å‹è¿‡ä¼˜åŒ–æ–¹é¢çš„è¡¨ç°ï¼Œä¸ºè§£å†³RLHFä¸­å¸¸è§çš„å¥–åŠ±ç ´è§£é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æ”¹è¿›RLHFè®­ç»ƒè¿‡ç¨‹æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
``` 

## questa--expanding-reasoning-capacity-in-llms-via-question-augmentation
### Abstract
Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | QuestAï¼šé€šè¿‡é—®é¢˜å¢å¼ºæ‹“å±•å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ¨ç†ä»»åŠ¡çš„æ ¸å¿ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿‘æœŸç ”ç©¶å¯¹RLèƒ½å¦æ¿€åŠ±æ¨¡å‹åœ¨åŸºç¡€æ¨¡å‹ä¹‹å¤–æå‡æ¨ç†èƒ½åŠ›æå‡ºäº†è´¨ç–‘ã€‚åœ¨å¤„ç†é«˜éš¾åº¦ä»»åŠ¡æ—¶ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„RLæ–¹æ³•ä¹Ÿå­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œä¾‹å¦‚æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆæ­£ç¡®ç­”æ¡ˆï¼Œå¯¼è‡´ç†µåç¼©ï¼Œé™åˆ¶äº†æ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ç®€å•æç¤ºè®­ç»ƒä¼šä½¿æ¨¡å‹è¿‡åº¦æ‹Ÿåˆæµ…å±‚æ¨¡å¼ï¼Œè€Œä½¿ç”¨å›°éš¾æç¤ºè®­ç»ƒåˆ™å› å¥–åŠ±ä¿¡å·ç¨€ç–å¯¼è‡´å­¦ä¹ ç¼“æ…¢ä¸”æ ·æœ¬æ•ˆç‡ä½ã€‚å› æ­¤ï¼Œå¦‚ä½•è°ƒæ•´RLä»¥æ›´æœ‰æ•ˆåœ°è§£å†³æ›´éš¾çš„æ¨ç†é—®é¢˜æˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°RLVRä¸­æ¨¡å‹èƒ½åŠ›çš„æ¼”å˜å…³é”®å–å†³äºæ•°æ®é›†éš¾åº¦ï¼Œå¼ºè°ƒäº†ä½¿ç”¨å›°éš¾é—®é¢˜è¿›è¡Œè®­ç»ƒä»¥æ‰©å±•æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºQuestAæ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥éƒ¨åˆ†è§£å†³æ–¹æ¡ˆæ¥æ§åˆ¶é—®é¢˜éš¾åº¦ã€‚å°†å›°éš¾é—®é¢˜ä¸éƒ¨åˆ†è§£å†³æ–¹æ¡ˆè¿›è¡Œå¢å¼ºï¼Œä¸ºRLè®­ç»ƒæä¾›äº†å¹³æ»‘çš„è¯¾ç¨‹ï¼Œä½¿é«˜éš¾åº¦ä»»åŠ¡æ›´æ˜“äºå¤„ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤ï¼Œäº§ç”Ÿæ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·å¹¶æé«˜æ ·æœ¬æ•ˆç‡ï¼ŒåŒæ—¶ä»ä¿ƒä½¿æ¨¡å‹æŒæ¡æœ€éš¾çš„é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨1.5Bå‚æ•°æ¨¡å‹å–å¾—äº†æ–°çš„SOTAç»“æœï¼šåœ¨AIME24ä¸Šè¾¾åˆ°72.50%ï¼ˆæé«˜äº†10.73%ï¼‰ï¼Œåœ¨AIME25ä¸Šè¾¾åˆ°62.29%ï¼ˆæé«˜äº†12.79%ï¼‰ï¼Œåœ¨HMMT25ä¸Šè¾¾åˆ°41.67%ï¼ˆæé«˜äº†10.11%ï¼‰ã€‚ä¸å…¶ä»–æ¨¡å‹ï¼ˆå¦‚Qwen3 - 1.7Bã€Nemotron - 1.5Bç­‰ï¼‰ç›¸æ¯”ï¼ŒQuestA - Nemotron - 1.5Bå±•ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åŒæ—¶ï¼Œå®éªŒè¡¨æ˜QuestAæ–¹æ³•åœ¨æé«˜pass@1çš„åŒæ—¶ä¹Ÿæé«˜äº†pass@kï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡å‡†RLéš¾ä»¥å–å¾—è¿›å±•çš„é—®é¢˜ä¸Šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **é—®é¢˜å¢å¼ºæ€è·¯**ï¼šé€šè¿‡å¼•å…¥éƒ¨åˆ†è§£å†³æ–¹æ¡ˆæ¥è°ƒæ•´é—®é¢˜éš¾åº¦çš„æ–¹æ³•ä¸ºè§£å†³RLåœ¨å›°éš¾ä»»åŠ¡ä¸Šçš„ä½æ•ˆé—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦æå‡æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡å’Œæ¨¡å‹è®­ç»ƒä¸­ã€‚
2. **å¹³è¡¡è®­ç»ƒæ•ˆç‡ä¸æ¨ç†èƒ½åŠ›**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡ç®€å•é—®é¢˜å’Œå›°éš¾é—®é¢˜çš„ä½¿ç”¨ï¼Œé¿å…æ¨¡å‹é™·å…¥è¿‡æ‹Ÿåˆæˆ–å­¦ä¹ å›°éš¾çš„å›°å¢ƒï¼Œä¸ºä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹æä¾›äº†å€Ÿé‰´ã€‚
3. **ç†è®ºä¸å®è·µç»“åˆ**ï¼šè®ºæ–‡ä¸ä»…æå‡ºäº†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œè¿˜ä»ç†è®ºä¸Šåˆ†æäº†éƒ¨åˆ†è§£å†³æ–¹æ¡ˆå¢å¼ºåŠ é€ŸRLè®­ç»ƒçš„åŸå› ï¼Œè¿™ç§ç†è®ºä¸å®è·µç›¸ç»“åˆçš„ç ”ç©¶æ–¹å¼æœ‰åŠ©äºæ·±å…¥ç†è§£æ¨¡å‹è®­ç»ƒæœºåˆ¶ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ”¹è¿›æä¾›æ–¹å‘ã€‚
``` 

## when-greedy-wins--emergent-exploitation-bias-in-meta-bandit-llm-training
### Abstract
While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹åœ¨å…ƒå¤šè‡‚è€è™æœºè®­ç»ƒä¸­çš„è´ªå©ªä¸æ¢ç´¢ä¹‹è°œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨é¡ºåºå†³ç­–é—®é¢˜ä¸­ï¼Œæ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„æƒè¡¡æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œå¤šè‡‚è€è™æœºï¼ˆMABï¼‰é—®é¢˜æ˜¯ç ”ç©¶è¿™ä¸€å…³é”®è¡Œä¸ºçš„ç»å…¸æµ‹è¯•å¹³å°ã€‚å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›å¼ºå¤§ï¼Œä½†åœ¨MABé—®é¢˜ä¸­å¸¸è¡¨ç°å‡ºçŸ­è§†ã€è´ªå©ªçš„è¡Œä¸ºï¼Œè¿‡åº¦åˆ©ç”¨å·²çŸ¥å¥–åŠ±è€Œå¿½è§†æ¢ç´¢ã€‚ä¸ºæ”¹å–„LLMsåœ¨é¡ºåºå†³ç­–ä¸­çš„æ¢ç´¢èƒ½åŠ›ï¼Œå‡ºç°äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸¤ç§ä¸»è¦è®­ç»ƒèŒƒå¼ï¼Œä½†ç›®å‰ç¼ºä¹å¯¹è¿™ä¸¤ç§æ–¹æ³•å¦‚ä½•å¡‘é€ æ¢ç´¢ç­–ç•¥ä»¥åŠå…¶æ³›åŒ–èƒ½åŠ›çš„æ·±å…¥ç†è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ ·åŒ–å¥–åŠ±ä¿¡å·è®¾è®¡
é™¤äº†æ ‡å‡†çš„éšæœºå¥–åŠ±ï¼Œæå‡ºä¸¤ç§é¢å¤–å¥–åŠ±ä¿¡å·ã€‚ä¸€æ˜¯åŸºäºé—æ†¾æ¦‚å¿µçš„æˆ˜ç•¥å¥–åŠ±ï¼Œç”¨äºå‡å°‘è®­ç»ƒæ–¹å·®ï¼Œåœ¨é«˜æ–¹å·®ç¯å¢ƒä¸­æé«˜è®­ç»ƒæ•ˆç‡ï¼›äºŒæ˜¯ç®—æ³•å¥–åŠ±ï¼Œé€šè¿‡RLæ¿€åŠ±å¯¹å¦‚UCBè¿™æ ·çš„ç¥è°•ç­–ç•¥è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼Œå› å…¶æ˜“äºè¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œå§‹ç»ˆä¼˜äºå…¶ä»–å­¦ä¹ ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢çš„å®éªŒè¯„ä¼°
ä½¿ç”¨SFTåœ¨ä¸“å®¶è½¨è¿¹ä¸Šä»¥åŠRLç»“åˆä¸€ç³»åˆ—ç‰¹å®šä»»åŠ¡å¥–åŠ±ä¿¡å·æ¥è®­ç»ƒLLMsæ‰§è¡ŒMABä»»åŠ¡ã€‚åœ¨å¤šç§MABç¯å¢ƒä¸‹ï¼ŒåŒ…æ‹¬é•¿åº¦æ³›åŒ–å’Œè·¨åˆ†å¸ƒè½¬ç§»ï¼ˆå¦‚ä»é«˜æ–¯åˆ†å¸ƒåˆ°ä¼¯åŠªåˆ©åˆ†å¸ƒï¼‰ï¼Œè¯„ä¼°å­¦ä¹ åˆ°çš„ç­–ç•¥çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ†æä»£ç†çš„è¡ŒåŠ¨æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨åç¼€å¤±è´¥ç‡ç­‰æ›¿ä»£ç»Ÿè®¡é‡ï¼Œæ·±å…¥ç ”ç©¶ç­–ç•¥æ˜¯å¦çœŸæ­£è·å¾—äº†ç¨³å¥çš„æ¢ç´¢ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SFTå’ŒRLå‡æé«˜äº†åŸºç¡€æ¨¡å‹åœ¨MABä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå®ç°äº†æ›´ä½çš„é—æ†¾å€¼å’Œæ›´é«˜çš„å¥–åŠ±ï¼Œæ€§èƒ½å¯ä¸UCBå’Œæ±¤æ™®æ£®é‡‡æ ·ç­‰ç†è®ºæœ€ä¼˜åŸºçº¿ç›¸åª²ç¾ã€‚RLç­–ç•¥åœ¨è·¨ä¸åŒè€è™æœºå®¶æ—çš„æ³›åŒ–æ–¹é¢æ¯”SFTæ›´ç¨³å¥ï¼Œåœ¨æ¯”è®­ç»ƒé•¿åº¦é•¿6å€çš„ç¯å¢ƒä»¥åŠåˆ†å¸ƒå¤–ç¯å¢ƒä¸­ä¹Ÿè¡¨ç°å‡ºè¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¡Œä¸ºåˆ†æå‘ç°ï¼Œæ€§èƒ½æå‡æºäºå­¦ä¹ åˆ°æ›´å¤æ‚ä½†æ›´è´ªå©ªçš„åˆ©ç”¨è¡Œä¸ºï¼Œä¾‹å¦‚æ¨¡ä»¿UCBçš„RLè®­ç»ƒä»£ç†å¸¸é€šè¿‡å®æ–½UCBçš„å˜ä½“è¶…è¶Šå…¶â€œè€å¸ˆâ€ï¼Œä½†è¿™ç§å˜ä½“å¯èƒ½è¿‡æ—©åœæ­¢æ¢ç´¢ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„ä¸¤ç§å¥–åŠ±è®¾è®¡ä¸ºåœ¨é«˜æ–¹å·®è®¾ç½®ä¸­ç¨³å®šå­¦ä¹ ä»¥åŠé«˜æ•ˆæ¨¡ä»¿ç¥è°•ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ã€‚åŒæ—¶ï¼Œå¼ºè°ƒåœ¨è¯„ä¼°LLMsåœ¨MABä»»åŠ¡ä¸­çš„è¡¨ç°æ—¶ï¼Œä¸èƒ½ä»…ä¾èµ–å¹³å‡é—æ†¾å€¼è¿™ä¸€æ€»ä½“ç»Ÿè®¡é‡ï¼Œè¿˜åº”è¿›è¡Œè¡Œä¸ºåˆ†æä»¥æ­ç¤ºç­–ç•¥çš„æ½œåœ¨é—®é¢˜ï¼Œè¿™ä¸ºåç»­ç ”ç©¶æä¾›äº†æ›´å…¨é¢çš„è¯„ä¼°è§†è§’ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœæ˜ç¡®äº†ä¸åŒè®­ç»ƒèŒƒå¼çš„é€‚ç”¨åœºæ™¯ï¼Œæœ‰åŠ©äºæ ¹æ®åº”ç”¨éœ€æ±‚é€‰æ‹©åˆé€‚çš„è®­ç»ƒæ–¹æ³•ã€‚
```

## poli-rl--a-point-to-list-reinforcement-learning-framework-for-conditional-semantic-textual-similarity
### Abstract
Conditional Semantic Textual Similarity (C-STS) measures the semantic proximity between text segments under a specific condition, thereby overcoming the ambiguity inherent in traditional STS. However, existing methods are largely confined to discriminative models, failing to fully integrate recent breakthroughs in the NLP community concerning Large Language Models (LLMs) and Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this task, as it can directly optimize the non-differentiable Spearman ranking metric and guide the reasoning process required by C-STS. However, we find that naively applying listwise RL fails to produce meaningful improvements, as the model is overwhelmed by complex, coarse-grained reward signals. To address this challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning framework. PoLi-RL employs a two-stage curriculum: it first trains the model with simple pointwise rewards to establish fundamental scoring capabilities, then transitions to a hybrid reward that combines pointwise, pairwise, and listwise objectives to refine the model's ability to discern subtle semantic distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward (PSRR) mechanism that computes ranking rewards in parallel slices, where each slice comprises same-indexed completions from different samples. This provides a precise, differentiated learning signal for each individual completion, enabling granular credit assignment and effective optimization. On the official C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18, establishing a new SOTA for the cross-encoder architecture. As the first work to successfully apply RL to C-STS, our study introduces a powerful and precise paradigm for training LLMs on complex, ranking-based conditional judgment tasks.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | PoLi - RLï¼šå¼€å¯æ¡ä»¶è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦ä»»åŠ¡æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆSTSï¼‰æ˜¯è®¡ç®—è¯­è¨€å­¦çš„æ ¸å¿ƒç ”ç©¶é¢†åŸŸï¼Œåœ¨è¯¸å¤šåœºæ™¯æœ‰å¹¿æ³›åº”ç”¨ã€‚ä½†ä¼ ç»ŸSTSä»»åŠ¡å­˜åœ¨å›ºæœ‰çš„æ¨¡ç³Šæ€§ï¼Œå› ä¸ºå…¶ç›¸ä¼¼åº¦å®šä¹‰å¸¸å—è§‚å¯Ÿè€…åå·®å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ¡ä»¶è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆC - STSï¼‰ä»»åŠ¡åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡å¼•å…¥æ˜ç¡®çš„è‡ªç„¶è¯­è¨€æ¡ä»¶ï¼Œå®ç°æ›´ç²¾ç¡®å’Œå®¢è§‚çš„ç›¸ä¼¼åº¦åˆ¤æ–­ï¼Œä½†ä¹Ÿå¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚ç›®å‰C - STSçš„ç ”ç©¶ä¸»è¦æœ‰åŒç¼–ç å™¨ã€ä¸‰ç¼–ç å™¨å’Œäº¤å‰ç¼–ç å™¨ä¸‰ç§èŒƒå¼ï¼Œå…¶ä¸­äº¤å‰ç¼–ç å™¨æ¶æ„ä¸ç°ä»£ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹æœ€ä¸ºå…¼å®¹ï¼Œä½†C - STSä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é›†æˆå°šå¤„äºæ—©æœŸé˜¶æ®µã€‚ç°æœ‰çš„LLMåº”ç”¨ä¸»è¦æ˜¯å°‘æ ·æœ¬æç¤ºçš„ç›´æ¥æ¨ç†å’Œä½œä¸ºç‰¹å¾æå–å™¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼Œä¸”éƒ½æœªå°†åŸºäºLLMçš„ç«¯åˆ°ç«¯äº¤å‰ç¼–ç å™¨åº”ç”¨äºC - STSä»»åŠ¡ï¼Œä¹Ÿæœªä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­‰å…ˆè¿›è®­ç»ƒæŠ€æœ¯ç»“åˆã€‚æ­¤å¤–ï¼Œè™½ç„¶RLé€‚åˆC - STSä»»åŠ¡ï¼Œä½†ç®€å•åº”ç”¨åˆ—è¡¨å¼RLæ— æ³•äº§ç”Ÿæœ‰æ„ä¹‰çš„æ”¹è¿›ï¼Œå­˜åœ¨è®­ç»ƒåœæ»å’Œå¥–åŠ±ä¿¡å·ç²—ç³™ç­‰é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºPoLi - RLæ¡†æ¶
PoLi - RLæ˜¯ä¸€ç§æ–°é¢–çš„ç‚¹åˆ°åˆ—è¡¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè¯¾ç¨‹è®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨ç®€å•çš„é€ç‚¹å¥–åŠ±è®­ç»ƒæ¨¡å‹ï¼Œä»¥å»ºç«‹ä»»åŠ¡çš„åŸºæœ¬è¯„åˆ†èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µè¿‡æ¸¡åˆ°ç»“åˆé€ç‚¹ã€æˆå¯¹å’Œåˆ—è¡¨ç›®æ ‡çš„æ··åˆå¥–åŠ±ï¼Œä»¥ç»†åŒ–æ¨¡å‹è¾¨åˆ«ç»†å¾®è¯­ä¹‰å·®å¼‚çš„èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥Parallel Slice Ranking Rewardï¼ˆPSRRï¼‰æœºåˆ¶
è¯¥æœºåˆ¶é€šè¿‡ä¸¤çº§åˆ†è§£è§£å†³æ‰¹é‡æ’åäº§ç”Ÿçš„ç²—ç²’åº¦å¥–åŠ±ä¿¡å·é—®é¢˜ã€‚å¯¹äºä¸€æ‰¹è¾“å…¥æ ·æœ¬ï¼Œæ¨¡å‹ä¸ºæ¯ä¸ªæ ·æœ¬ç”ŸæˆGä¸ªå®Œæˆé¡¹ï¼Œå½¢æˆGä¸ªâ€œå¹³è¡Œåˆ‡ç‰‡â€ï¼Œæ¯ä¸ªåˆ‡ç‰‡ç”±ä¸åŒæ ·æœ¬çš„ç›¸åŒç´¢å¼•å®Œæˆé¡¹ç»„æˆã€‚åœ¨æ¯ä¸ªåˆ‡ç‰‡å†…ï¼Œè®¡ç®—æ¯ä¸ªå•ç‹¬å®Œæˆé¡¹ç›¸å¯¹äºå…¶ç†æƒ³æ’åçš„æ’åå·®å¼‚ï¼Œä¸ºæ¯ä¸ªå®Œæˆé¡¹æä¾›ç‹¬ç‰¹ä¸”ç²¾ç¡®çš„å¥–åŠ±ï¼Œå®ç°ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…å’Œæ›´æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®˜æ–¹C - STSåŸºå‡†ä¸Šï¼ŒPoLi - RLå–å¾—äº†48.18çš„æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°ï¼Œä¸ºäº¤å‰ç¼–ç å™¨æ¶æ„å»ºç«‹äº†æ–°çš„SOTAï¼Œè¶…è¶Šäº†åŒ…æ‹¬GPT - 4ï¼ˆ43.6ï¼‰åœ¨å†…çš„å¼ºé—­æºæ¨¡å‹ã€‚å®šæ€§åˆ†æè¿˜æ­ç¤ºäº†è¯¥æ–¹æ³•åœ¨ç†è§£å¤æ‚æ¡ä»¶æ–¹é¢çš„ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ä»»åŠ¡ä¸æ¨¡å‹ç»“åˆæ€è·¯**ï¼šå°†RLä¸åŸºäºLLMçš„äº¤å‰ç¼–ç å™¨èŒƒå¼ç›¸ç»“åˆçš„æ€è·¯ï¼Œä¸ºåœ¨å¤æ‚çš„åŸºäºæ’åçš„æ¡ä»¶åˆ¤æ–­ä»»åŠ¡ä¸­è®­ç»ƒLLMsæä¾›äº†æ–°çš„æ–¹å‘ï¼Œå¯¹äºå…¶ä»–ç±»ä¼¼éœ€è¦æ¨ç†å’Œç²¾ç¡®åˆ¤æ–­çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
2. **è®­ç»ƒæ¡†æ¶è®¾è®¡**ï¼šPoLi - RLçš„ä¸¤é˜¶æ®µè®­ç»ƒè¯¾ç¨‹è®¾è®¡ï¼Œä»ç®€å•åˆ°å¤æ‚çš„å¥–åŠ±è¿‡æ¸¡æ–¹å¼ï¼Œæœ‰åŠ©äºå¤„ç†å¤æ‚å­¦ä¹ ä»»åŠ¡ï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦é€æ­¥æå‡æ¨¡å‹èƒ½åŠ›çš„è®­ç»ƒåœºæ™¯ã€‚
3. **å¥–åŠ±æœºåˆ¶åˆ›æ–°**ï¼šPSRRæœºåˆ¶ä¸ºæ¶‰åŠå¤šä¸ªç”Ÿæˆå€™é€‰çš„æ’åå’Œæ£€ç´¢ä»»åŠ¡æä¾›äº†ä¸€ç§å¯æ¨å¹¿çš„ç­–ç•¥ï¼Œå…¶é€šè¿‡ç‹¬ç‰¹çš„å¥–åŠ±è®¡ç®—æ–¹å¼å®ç°ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ï¼Œå¯å¯å‘å…¶ä»–ç›¸å…³ä»»åŠ¡ä¼˜åŒ–å¥–åŠ±è®¾è®¡ã€‚
``` 

## dynamic-speculative-agent-planning
### Abstract
Despite their remarkable success in complex tasks propelling widespread adoption, large language-model-based agents still face critical deployment challenges due to prohibitive latency and inference costs. While recent work has explored various methods to accelerate inference, existing approaches suffer from significant limitations: they either fail to preserve performance fidelity, require extensive offline training of router modules, or incur excessive operational costs. Moreover, they provide minimal user control over the tradeoff between acceleration and other performance metrics. To address these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous online reinforcement learning framework that provides lossless acceleration with substantially reduced costs without requiring additional pre-deployment preparation. DSP explicitly optimizes a joint objective balancing end-to-end latency against dollar cost, allowing practitioners to adjust a single parameter that steers the system toward faster responses, cheaper operation, or any point along this continuum. Experiments on two standard agent benchmarks demonstrate that DSP achieves comparable efficiency to the fastest lossless acceleration method while reducing total cost by 30% and unnecessary cost up to 60%. Our code and data are available through https://github.com/guanyilin428/Dynamic-Speculative-Planning.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŠ¨æ€æ¨æµ‹å¼æ™ºèƒ½ä½“è§„åˆ’ï¼šè§£é”å¤§è¯­è¨€æ¨¡å‹åº”ç”¨æ–°æ•ˆèƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆåŠŸå¹¶è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†åœ¨éƒ¨ç½²æ—¶é¢ä¸´ç€ä¸¥é‡çš„å»¶è¿Ÿå’Œæ¨ç†æˆæœ¬é—®é¢˜ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶æ¢ç´¢å¤šç§åŠ é€Ÿæ¨ç†æ–¹æ³•ï¼Œä½†å­˜åœ¨è¯¸å¤šå±€é™ï¼šè¦ä¹ˆæ— æ³•ä¿è¯æ€§èƒ½ä¿çœŸåº¦ï¼Œè¦ä¹ˆéœ€è¦å¯¹è·¯ç”±å™¨æ¨¡å—è¿›è¡Œå¤§é‡ç¦»çº¿è®­ç»ƒï¼Œæˆ–è€…äº§ç”Ÿè¿‡é«˜çš„è¿è¥æˆæœ¬ï¼Œä¸”å‡ ä¹æ²¡æœ‰ä¸ºç”¨æˆ·æä¾›åœ¨åŠ é€Ÿå’Œå…¶ä»–æ€§èƒ½æŒ‡æ ‡ä¹‹é—´æƒè¡¡çš„æ§åˆ¶èƒ½åŠ›ã€‚ä¸ºå¡«è¡¥è¿™äº›ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€æ¨æµ‹è§„åˆ’ï¼ˆDSPï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢æ¨æµ‹å¼æ™ºèƒ½ä½“è§„åˆ’ä¸­å»¶è¿Ÿ - æˆæœ¬æƒè¡¡çš„å¸•ç´¯æ‰˜å‰æ²¿
ç ”ç©¶å‘ç°ï¼Œå¸¸è§çš„å›ºå®šæ¨æµ‹æ­¥éª¤æ–¹æ³•å­˜åœ¨æ ¹æœ¬å±€é™ï¼Œå› ä¸ºå¯¹äºå¤æ‚ä»»åŠ¡ï¼Œæ¿€è¿›çš„æ¨æµ‹ä¼šäº§ç”Ÿè¿‡å¤šå†—ä½™çš„æ™ºèƒ½ä½“è°ƒç”¨ä»è€Œå¤§å¹…å¢åŠ æˆæœ¬ï¼Œè€Œå¯¹äºç®€å•ä»»åŠ¡ï¼Œä¿å®ˆçš„æ¨æµ‹åˆæ— æ³•æä¾›è¶³å¤Ÿçš„åŠ é€Ÿã€‚ç”±äºæœ€ä¼˜æ¨æµ‹æ­¥éª¤é«˜åº¦ä¾èµ–ä¸Šä¸‹æ–‡ä¸”å…ˆéªŒæœªçŸ¥ï¼Œå›ºå®šæ­¥éª¤ä¸¥é‡é™åˆ¶äº†å®é™…æ•ˆç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘è½»é‡çº§è‡ªé€‚åº”æ¨æµ‹æ­¥éª¤é¢„æµ‹å™¨
è¯¥é¢„æµ‹å™¨é‡‡ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ— éœ€å¤–éƒ¨æ•°æ®é›†ã€é¢„å¤„ç†æ­¥éª¤æˆ–ä¸“é—¨çš„é¢„éƒ¨ç½²é˜¶æ®µã€‚ç³»ç»Ÿåœ¨å¤„ç†ä»»åŠ¡æ—¶æœ‰æœºåœ°å­¦ä¹ ä¼˜åŒ–æ¨æµ‹æ­¥éª¤ï¼Œéšç€æ—¶é—´æ¨ç§»å˜å¾—è¶Šæ¥è¶Šé«˜æ•ˆï¼Œä¸”æ— éœ€é¢å¤–åŸºç¡€è®¾æ–½æˆæœ¬ï¼ŒåŒæ—¶ç¡®ä¿å³æ—¶éƒ¨ç½²æ•ˆç›Šï¼Œèƒ½æœ‰æ•ˆæ¶ˆé™¤ä¸å¿…è¦çš„æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™åŠ é€Ÿä¼˜åŠ¿ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥ä¸¤ç§è°ƒèŠ‚å»¶è¿Ÿå’Œæˆæœ¬æƒè¡¡çš„æœºåˆ¶
å³æœ‰åæ­¥éª¤é¢„æµ‹å’Œå¸¦åç½®åç§»çš„æ— åæ­¥éª¤ï¼Œå…è®¸ä»ä¸šè€…åœ¨ä»ä½æˆæœ¬/ä½é€Ÿåº¦åˆ°é«˜æˆæœ¬/é«˜é€Ÿåº¦çš„æ•´ä¸ªæ“ä½œèŒƒå›´å†…ç²¾ç¡®æ ¡å‡†ç³»ç»Ÿè¡Œä¸ºï¼Œä¸ºç”¨æˆ·æä¾›å¯¹å»¶è¿Ÿ - æˆæœ¬å¹³è¡¡çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œä»¥é€‚åº”ä¸åŒç»„ç»‡çš„ä¼˜å…ˆçº§ï¼Œå¹¶é€‚åº”å¿«é€Ÿå‘å±•çš„LLMç”Ÿæ€ç³»ç»Ÿä¸­æ³¢åŠ¨çš„å®šä»·ç»“æ„å’Œæ¨ç†é€Ÿåº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸¤ä¸ªæ ‡å‡†æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDSPå®ç°äº†ä¸æœ€å¿«çš„æ— æŸåŠ é€Ÿæ–¹æ³•ç›¸å½“çš„æ•ˆç‡ï¼ŒåŒæ—¶å°†æ€»æˆæœ¬é™ä½äº†30%ï¼Œå°†ä¸å¿…è¦çš„æˆæœ¬å‰Šå‡äº†60%ï¼Œå¹¶ä¿æŒäº†ç›¸å½“çš„åŠ é€Ÿæ•ˆæœï¼Œè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **åŠ¨æ€è‡ªé€‚åº”ç­–ç•¥**ï¼šDSPçš„åŠ¨æ€æ¨æµ‹è§„åˆ’æ–¹æ³•ä¸ºè§£å†³å¤æ‚ç³»ç»Ÿä¸­çš„å»¶è¿Ÿå’Œæˆæœ¬å¹³è¡¡é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå…¶è‡ªé€‚åº”æ¨æµ‹æ­¥éª¤é¢„æµ‹å™¨å¯æ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ¨æµ‹è¡Œä¸ºï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹éƒ½èƒ½æœ‰æ•ˆä¼˜åŒ–èµ„æºåˆ©ç”¨ã€‚
2. **ç”¨æˆ·æ§åˆ¶ä¸çµæ´»æ€§**ï¼šæä¾›ç»†ç²’åº¦çš„ç”¨æˆ·æ§åˆ¶æœºåˆ¶ï¼Œå…è®¸æ ¹æ®ç»„ç»‡çš„ä¸åŒä¼˜å…ˆçº§å’Œå®é™…éœ€æ±‚åœ¨å»¶è¿Ÿå’Œæˆæœ¬ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œè¿™ç§çµæ´»æ€§åœ¨å¿«é€Ÿå˜åŒ–çš„æŠ€æœ¯ç”Ÿæ€ç³»ç»Ÿä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯å¯å‘å…¶ä»–ç›¸å…³ç³»ç»Ÿè®¾è®¡ä¸­å¯¹ç”¨æˆ·éœ€æ±‚çš„å…³æ³¨å’Œæ»¡è¶³ã€‚
3. **åœ¨çº¿å¼ºåŒ–å­¦ä¹ åº”ç”¨**ï¼šåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨è¿è¡Œè¿‡ç¨‹ä¸­ä¸æ–­è‡ªæˆ‘ä¼˜åŒ–ï¼Œæ— éœ€å¤§é‡é¢„éƒ¨ç½²å‡†å¤‡å·¥ä½œï¼Œä¸ºå®æ—¶ç³»ç»Ÿçš„æ€§èƒ½æå‡å’Œæˆæœ¬æ§åˆ¶æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„æ–¹æ³•ï¼Œå€¼å¾—åœ¨ç±»ä¼¼éœ€è¦å®æ—¶å†³ç­–å’Œä¼˜åŒ–çš„åœºæ™¯ä¸­å€Ÿé‰´ã€‚
``` 

## reasoning-scaffolding--distilling-the-flow-of-thought-from-llms
### Abstract
The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¨ç†è„šæ‰‹æ¶ï¼šä»å¤§è¯­è¨€æ¨¡å‹ä¸­æç‚¼æ€ç»´æµç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰ä»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æç‚¼æ¨ç†èƒ½åŠ›çš„ä¸»æµæ–¹æ³•æ˜¯åŸºäºæ–‡æœ¬ç†ç”±çš„è¡Œä¸ºå…‹éš†ï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨æ ¹æœ¬å±€é™æ€§ã€‚å®ƒåªæ˜¯è®©å°è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰æ¨¡ä»¿è¡¨é¢æ¨¡å¼ï¼Œè€Œéæ€ç»´çš„åº•å±‚ç®—æ³•ç»“æ„ï¼Œå¯¼è‡´é€»è¾‘ç¨³å¥æ€§ä¸¥é‡ç¼ºå¤±ã€‚é¢å¯¹æ–°é—®é¢˜æ—¶ï¼Œç”±æ­¤è®­ç»ƒå‡ºçš„å­¦ç”Ÿæ¨¡å‹å¾€å¾€è¡¨ç°è„†å¼±ï¼Œäº§ç”Ÿé€»è¾‘ä¸ä¸€è‡´æˆ–æ— æ„ä¹‰çš„è®ºè¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥æœ‰æ•ˆæç‚¼LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿SLMsæˆä¸ºçœŸæ­£çš„æ¨ç†è€…ï¼Œè€Œéä»…ä»…æ˜¯æµç•…çš„æ¨¡ä»¿è€…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ¨ç†è„šæ‰‹æ¶æ¡†æ¶
å¼•å…¥Reasoning Scaffoldingæ¡†æ¶ï¼Œå°†æ¨ç†é‡æ–°æ„å»ºä¸ºç»“æ„åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•æŠŠæ•™å¸ˆæ¨¡å‹çš„æ€ç»´è¿‡ç¨‹æŠ½è±¡ä¸ºä¸€ç³»åˆ—ç¦»æ•£ã€å¯è§£é‡Šçš„è¯­ä¹‰ä¿¡å·ï¼ˆå¦‚å¯¹æ¯”ã€æ·»åŠ ç­‰ï¼‰ï¼Œä»¥æ­¤ä½œä¸ºè„šæ‰‹æ¶ï¼Œä¸ºå­¦ç”Ÿæ¨¡å‹æ„å»ºè‡ªå·±çš„ç¨³å¥æ¨ç†æä¾›æ”¯æ’‘ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šä»»åŠ¡è®­ç»ƒæœºåˆ¶
é€šè¿‡å¤šä»»åŠ¡ç›®æ ‡è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œä½¿å…¶åŒæ—¶å­¦ä¹ ä¸¤é¡¹å…³é”®æŠ€èƒ½ï¼šä¸€æ˜¯é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯­ä¹‰ä¿¡å·æ¥é¢„æµ‹é€»è¾‘è®ºè¯çš„æµç¨‹ï¼›äºŒæ˜¯åŸºäºè¯¥ä¿¡å·ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬ï¼Œæ‰§è¡Œç‰¹å®šçš„æ¨ç†æ­¥éª¤ã€‚è¿™ç§åŒç›®æ ‡è®­ç»ƒæ–¹å¼ä»¥ä¿¡å·é¢„æµ‹ä»»åŠ¡ä½œä¸ºé€»è¾‘è¿è´¯æ€§çš„å¼ºå¤§æ­£åˆ™åŒ–å™¨ï¼Œä¿ƒä½¿å­¦ç”Ÿæ¨¡å‹å†…åŒ–æ¨ç†çš„è®¡ç®—æ¨¡å¼ï¼Œè€Œä¸ä»…ä»…æ˜¯å…‹éš†æ–‡æœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GSM8Kå’ŒStrategyQAç­‰ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„è’¸é¦æ–¹æ³•ã€‚å®éªŒè¿˜é€šè¿‡æ·±å…¥åˆ†æè¡¨æ˜ï¼Œå­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„é€»è¾‘è¡¨ç¤ºæ›´åŠ å¯¹é½ï¼Œæœ‰åŠ›è¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿè®­ç»ƒå‡ºçœŸæ­£çš„æ¨ç†è€…ï¼Œè€Œéä»…ä»…æ˜¯æµç•…çš„æ¨¡ä»¿è€…ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ–°çš„è’¸é¦è§†è§’**ï¼šä¸å†å±€é™äºæ–‡æœ¬å…‹éš†ï¼Œè€Œæ˜¯ç›´æ¥è½¬ç§»æ¨ç†çš„ç®—æ³•ç»“æ„ï¼Œä¸ºè¯­è¨€æ¨¡å‹è’¸é¦æä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæ”¹è¿›ç°æœ‰çš„è’¸é¦æ–¹æ³•ã€‚
2. **ç»“æ„åŒ–è®­ç»ƒä¿¡å·**ï¼šä»æ–‡æœ¬ç†ç”±ä¸­æå–å’Œåˆ†ç±»â€œæ¨ç†è„šæ‰‹æ¶â€çš„æ–¹æ³•ï¼Œä¸ºè®­ç»ƒä¿¡å·çš„ç»“æ„åŒ–å’Œå¯è§£é‡Šæ€§æä¾›äº†èŒƒä¾‹ï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦ç»“æ„åŒ–è®­ç»ƒä¿¡å·çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚
3. **å¤šä»»åŠ¡è®­ç»ƒçš„åº”ç”¨**ï¼šå¤šä»»åŠ¡è®­ç»ƒæœºåˆ¶åœ¨æ¨ç†èƒ½åŠ›è’¸é¦ä¸­çš„æˆåŠŸåº”ç”¨ï¼Œä¸ºæå‡æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œåœ¨å…¶ä»–éœ€è¦å¢å¼ºæ¨¡å‹é€»è¾‘èƒ½åŠ›çš„åœºæ™¯ä¸­ä¹Ÿå…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
``` 

## process-level-trajectory-evaluation-for-environment-configuration-in-software-engineering-agents
### Abstract
Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´è½¯ä»¶å·¥ç¨‹ç¯å¢ƒé…ç½®ç“¶é¢ˆï¼šEnConda - Benchå¼€å¯æ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç¯å¢ƒé…ç½®æˆä¸ºäº†å‘å±•çš„ç“¶é¢ˆã€‚ä¸€æ–¹é¢ï¼Œç¯å¢ƒé…ç½®å·¥ä½œéœ€è¦å¤§é‡çš„äººå·¥æŠ•å…¥ï¼›å¦ä¸€æ–¹é¢ï¼Œå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„ç›¸å…³æ•°æ®é›†ç¨€ç¼ºã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä»…è¯„ä¼°ç«¯åˆ°ç«¯çš„æ„å»º/æµ‹è¯•æ˜¯å¦æˆåŠŸï¼Œæ— æ³•æ˜ç¡®æ™ºèƒ½ä½“åœ¨é…ç½®è¿‡ç¨‹ä¸­æˆåŠŸæˆ–å¤±è´¥çš„å…·ä½“ä½ç½®å’ŒåŸå› ï¼Œéš¾ä»¥å®šä½é”™è¯¯å‘ç”Ÿçš„ç‰¹å®šé˜¶æ®µä»¥åŠæ™ºèƒ½ä½“æ‰€ç¼ºä¹çš„ç²¾ç¡®é…ç½®èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé«˜è´¨é‡ã€å¯æ­£ç¡®æ„å»ºçš„ä»£ç åº“ç¨€ç¼ºï¼Œæ•°æ®çš„é€‰æ‹©å’Œæ ‡æ³¨éœ€è¦ä¸“å®¶æŠ•å…¥å¤§é‡ç²¾åŠ›ï¼Œå¯¼è‡´ç ”ç©¶äººå‘˜éš¾ä»¥è·å–å¤§é‡é«˜è´¨é‡æ•°æ®ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„ç¯å¢ƒé…ç½®èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºç¯å¢ƒé…ç½®è¯Šæ–­åŸºå‡†EnConda - Bench
æä¾›äº†åœ¨ç¯å¢ƒè®¾ç½®è§„åˆ’ã€æ„ŸçŸ¥é©±åŠ¨çš„é”™è¯¯è¯Šæ–­ã€åé¦ˆé©±åŠ¨çš„ä¿®å¤ä»¥åŠæ‰§è¡Œæœ€ç»ˆç¯å¢ƒé…ç½®æ“ä½œç­‰è¿‡ç¨‹ä¸­ï¼Œå¯¹æ™ºèƒ½ä½“ç»†ç²’åº¦èƒ½åŠ›çš„è¿‡ç¨‹çº§è½¨è¿¹è¯„ä¼°ã€‚é€šè¿‡å‘çœŸå®çš„READMEæ–‡ä»¶æ³¨å…¥é”™è¯¯æ¥è‡ªåŠ¨æ„å»ºä»»åŠ¡å®ä¾‹ï¼Œå¹¶åœ¨Dockerä¸­è¿›è¡ŒéªŒè¯ï¼Œä»¥å®ç°å¯æ‰©å±•ã€é«˜è´¨é‡çš„è¯„ä¼°ã€‚å°†è¿‡ç¨‹çº§åˆ†æä¸ç«¯åˆ°ç«¯çš„å¯æ‰§è¡Œæ€§ç›¸ç»“åˆï¼Œèƒ½å¤Ÿè¿›è¡Œè¶…è¶Šæ€»ä½“æˆåŠŸç‡çš„èƒ½åŠ›è¯„ä¼°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è‡ªåŠ¨åŒ–æ•°æ®æ„å»ºæ¡†æ¶
é€šè¿‡ä¸¥æ ¼æ ‡å‡†é€‰æ‹©é«˜è´¨é‡çš„ä»£ç åº“ï¼Œåˆ©ç”¨å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ç¼–è¾‘å…³é”®ç¯å¢ƒREADMEæ–‡ä»¶ï¼Œæ³¨å…¥å¸¸è§é”™è¯¯ç±»å‹å¹¶æ ‡æ³¨ç±»åˆ«å’Œå»ºè®®ä¿®å¤æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ¡†æ¶éªŒè¯å’Œç­›é€‰æœ‰æ•ˆé”™è¯¯ï¼Œä»¥è·å–é«˜è´¨é‡çš„ä»»åŠ¡å®ä¾‹ï¼Œå‡å°‘äº†äººå·¥åŠ³åŠ¨ï¼Œå¹¶ä¸ºæ™ºèƒ½ä½“å’Œå¤§è¯­è¨€æ¨¡å‹æä¾›äº†å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¯¹æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½ä½“æ¡†æ¶è¿›è¡Œè¯„ä¼°åå‘ç°ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿå®šä½é”™è¯¯ï¼Œä½†åœ¨å°†åé¦ˆè½¬åŒ–ä¸ºæœ‰æ•ˆçº æ­£æªæ–½æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†ç«¯åˆ°ç«¯çš„æ€§èƒ½è¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„è¿‡ç¨‹çº§è½¨è¿¹è¯„ä¼°æ–¹æ³•ä¸ºæ·±å…¥äº†è§£æ™ºèƒ½ä½“åœ¨ç¯å¢ƒé…ç½®ä¸­çš„èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ï¼Œå¯¹äºæ”¹è¿›æ™ºèƒ½ä½“åœ¨ç¯å¢ƒé…ç½®æ–¹é¢çš„èƒ½åŠ›ä»¥åŠåç»­ç›¸å…³ç ”ç©¶å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚è‡ªåŠ¨åŒ–æ•°æ®æ„å»ºæ¡†æ¶ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…¶æ€è·¯å’Œæ–¹æ³•å¯è¢«å€Ÿé‰´åº”ç”¨äºå…¶ä»–éœ€è¦å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®çš„ç ”ç©¶é¢†åŸŸã€‚åŒæ—¶ï¼Œè®ºæ–‡å¼ºè°ƒçš„å¯¹æ™ºèƒ½ä½“è§„åˆ’ã€æ„ŸçŸ¥ã€åé¦ˆå’Œè¡ŒåŠ¨èƒ½åŠ›çš„è¯„ä¼°ï¼Œä¸ºå…¨é¢è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚
``` 

## mars--reinforcing-multi-agent-reasoning-of-llms-through-self-play-in-strategic-games
### Abstract
Developing Large Language Models (LLMs) to cooperate and compete effectively within multi-agent systems is a critical step towards more advanced intelligence. While reinforcement learning (RL) has proven effective for enhancing reasoning in single-agent tasks, its extension to multi-turn, multi-agent scenarios remains underexplored due to the challenges of long-horizon credit assignment and agent-specific advantage estimation. To address these challenges, we introduce MARS, an end-to-end RL framework that incentivizes Multi-Agent Reasoning of LLMs through Self-play in both cooperative and competitive games. MARS features a turn-level advantage estimator that aligns learning signals with each interaction for credit assignment, and an agent-specific advantage normalization to stabilize multi-agent training. By learning with self-play across cooperative and competitive games, the MARS agent trained from Qwen3-4B develops strong strategic abilities that generalize to held-out games with up to 28.7% performance improvements. More importantly, the capability acquired through self-play generalizes beyond games, yielding consistent performance gains of multi-agent systems in reasoning benchmarks. When integrated into leading multi-agent systems, our MARS agent achieves significant performance gains of 10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL training with self-play in strategic games as a powerful approach for developing generalizable multi-agent reasoning capabilities in LLMs. Our code and models are publicly available at https://github.com/thu-nics/MARS.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | MARSï¼šé€šè¿‡æˆ˜ç•¥æ¸¸æˆä¸­çš„è‡ªæˆ‘åšå¼ˆå¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šé¢†åŸŸå±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†åœ¨è®¸å¤šå¦‚è°ˆåˆ¤ã€æˆ˜ç•¥æ¸¸æˆã€ååŒè½¯ä»¶å¼€å‘ç­‰ç°å®åœºæ™¯ä¸­ï¼Œå¾€å¾€æ¶‰åŠå¤šä¸ªæ™ºèƒ½ä½“çš„é•¿æœŸäº¤äº’ï¼Œä½¿LLMsåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æœ‰æ•ˆåˆä½œä¸ç«äº‰æˆä¸ºäººå·¥æ™ºèƒ½å‘å±•çš„å…³é”®å‰æ²¿ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å•æ™ºèƒ½ä½“ä»»åŠ¡æ¨ç†èƒ½åŠ›ä¸Šæˆæ•ˆæ˜¾è‘—ï¼Œä½†æ‰©å±•åˆ°å¤šè½®ã€å¤šæ™ºèƒ½ä½“åœºæ™¯æ—¶ï¼Œé¢ä¸´é•¿æœŸä¿¡ç”¨åˆ†é…å’Œç‰¹å®šæ™ºèƒ½ä½“ä¼˜åŠ¿ä¼°è®¡çš„æŒ‘æˆ˜ï¼Œä¸”è®¸å¤šç°å®ä»»åŠ¡éœ€è¦å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åˆä½œä¸ç«äº‰ï¼Œå½“å‰ç ”ç©¶å¯¹æ­¤æ¢ç´¢ä¸è¶³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºç®€å•æœ‰æ•ˆçš„å›åˆçº§ä¼˜åŠ¿ä¼°è®¡å™¨
é€šè¿‡è¯¥ä¼°è®¡å™¨ä½¿å­¦ä¹ ä¿¡å·ä¸æ¯æ¬¡äº¤äº’å¯¹é½ä»¥è¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå°†é•¿æœŸç»“æœå‡†ç¡®å½’å› äºä¸ªä½“è¡ŒåŠ¨ï¼Œå¹¶åœ¨å¤šä¸ªå›åˆå’Œæ™ºèƒ½ä½“é—´æä¾›å­¦ä¹ ä¿¡å·ï¼Œå®ç°ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºç‰¹å®šæ™ºèƒ½ä½“ä¼˜åŠ¿å½’ä¸€åŒ–æ–¹æ³•
é€šè¿‡æ ¡å‡†ç›¸å¯¹äºæ¯ä¸ªæ™ºèƒ½ä½“æ€§èƒ½çš„ä¼˜åŠ¿ä¼°è®¡æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥å½’ä¸€åŒ–è€ƒè™‘äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å¼‚æ„è§’è‰²ï¼Œç¡®ä¿ç¨³å®šçš„ç­–ç•¥æ›´æ–°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡åœ¨å¤šç§æ¸¸æˆä¸­è®­ç»ƒQwen3 - 4Bå¯¹MARSæ™ºèƒ½ä½“è¿›è¡Œè¯„ä¼°ï¼ŒMARSæ™ºèƒ½ä½“åœ¨åˆä½œå’Œç«äº‰æ¸¸æˆä¸­å‡å±•ç°å‡ºå¼ºå¤§çš„æˆ˜ç•¥èƒ½åŠ›ï¼Œåœ¨ä¸‰ä¸ªæœªè§è¿‡çš„æ¸¸æˆä¸­æ€§èƒ½æå‡é«˜è¾¾28.7%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨æ¸¸æˆä¸­é€šè¿‡è‡ªæˆ‘åšå¼ˆè·å¾—çš„èƒ½åŠ›è¿›ä¸€æ­¥æ³›åŒ–ï¼Œä½¿å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æŒç»­æå‡ã€‚å½“é›†æˆåˆ°é¢†å…ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼ŒMARSæ™ºèƒ½ä½“åœ¨AIMEä¸Šå®ç°äº†10.0%çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œåœ¨GPQA - Diamondä¸Šæå‡äº†12.5%ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¶ˆèç ”ç©¶éªŒè¯äº†å…³é”®æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ†æäº†æ¶Œç°çš„æ¨ç†æ¨¡å¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ–¹æ³•åˆ›æ–°**ï¼šæå‡ºçš„å›åˆçº§ä¼˜åŠ¿ä¼°è®¡å™¨å’Œç‰¹å®šæ™ºèƒ½ä½“ä¼˜åŠ¿å½’ä¸€åŒ–æ–¹æ³•ï¼Œä¸ºè§£å†³å¤šè½®ã€å¤šæ™ºèƒ½ä½“RLè®­ç»ƒä¸­çš„ä¿¡ç”¨åˆ†é…å’Œä¼˜åŠ¿ä¼°è®¡é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå¯å¯å‘å…¶ä»–ç›¸å…³ç ”ç©¶åœ¨æ–¹æ³•ä¸Šçš„åˆ›æ–°ã€‚
2. **èƒ½åŠ›æ³›åŒ–**ï¼šå®éªŒè¡¨æ˜é€šè¿‡åœ¨æ¸¸æˆä¸­è‡ªæˆ‘åšå¼ˆè·å¾—çš„èƒ½åŠ›èƒ½å¤Ÿæ³›åŒ–åˆ°æ¨ç†åŸºå‡†æµ‹è¯•å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œè¿™æç¤ºåœ¨æå‡å¤šæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›æ—¶ï¼Œå¯ä»¥è€ƒè™‘é€šè¿‡ç‰¹å®šåœºæ™¯çš„è®­ç»ƒæ¥å®ç°èƒ½åŠ›çš„æ³›åŒ–ã€‚
3. **ç ”ç©¶æ¡†æ¶**ï¼šMARSä½œä¸ºä¸€ä¸ªç«¯åˆ°ç«¯çš„RLæ¡†æ¶ï¼Œå…¶æ•´ä½“æ¶æ„å’Œè®­ç»ƒæ–¹å¼ä¸ºå¼€å‘å…·æœ‰å¯æ³›åŒ–å¤šæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›çš„LLMsæä¾›äº†ä¸€ä¸ªå¯å‚è€ƒçš„ç ”ç©¶æ¡†æ¶ã€‚
``` 

## learning-more-with-less--a-dynamic-dual-level-down-sampling-framework-for-efficient-policy-optimization
### Abstract
Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´å¼ºåŒ–å­¦ä¹ ç“¶é¢ˆï¼šDÂ³Sæ¡†æ¶åŠ©åŠ›é«˜æ•ˆç­–ç•¥ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ åœ¨ä½¿å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½ä¿æŒä¸€è‡´æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œå…¶ä¸­æ— è¯„è®ºå®¶æ–¹æ³•ï¼ˆå¦‚GRPOå’ŒGSPOï¼‰é€šè¿‡ä»å¤šä¸ªé‡‡æ ·å“åº”ä¸­ä¼°è®¡ä¼˜åŠ¿æ¥å‡å°‘å†…å­˜éœ€æ±‚ï¼Œä½†å­˜åœ¨æ•ˆç‡æŒ‘æˆ˜ã€‚è®­ç»ƒä¸­ä¼˜åŠ¿ä¼°è®¡çš„ç²¾åº¦ä¾èµ–äºé‡‡æ ·ç»„çš„è´¨é‡ï¼Œè¾ƒå¤§çš„é‡‡æ ·ç»„ä¸­å…³é”®å­¦ä¹ ä¿¡å·ä¼šè¢«å¤§é‡æ— ä¿¡æ¯çš„æ ·æœ¬å’Œæ ‡è®°ç¨€é‡Šï¼Œè¾ƒå°çš„é‡‡æ ·ç»„åˆå¯èƒ½å› é‡‡æ ·ä¸è¶³è€Œéš¾ä»¥äº§ç”Ÿå¤šæ ·åŒ–çš„æ ·æœ¬ï¼Œè¿™ç§æƒè¡¡é™åˆ¶äº†æ— è¯„è®ºå®¶ç®—æ³•çš„ä¼˜åŒ–æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè™½ç„¶æé«˜å¥–åŠ±ä¿¡å·çš„æ–¹å·®å¯ä»¥åŠ é€Ÿæ”¶æ•›ï¼Œä½†åœ¨å…¸å‹çš„æ— è¯„è®ºå®¶æ–¹æ³•ä¸­ï¼Œä¼˜åŠ¿æ˜¯é€šè¿‡å¯¹é€‰å®šå­é›†è¿›è¡Œå½’ä¸€åŒ–è®¡ç®—çš„ï¼Œå¯¼è‡´ä¼˜åŠ¿æ–¹å·®å›ºå®šä¸º1ï¼Œé™åˆ¶äº†ç­–ç•¥æ¢¯åº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€åŒå±‚ä¸‹é‡‡æ ·æ¡†æ¶ï¼ˆDÂ³Sï¼‰çš„æ ·æœ¬çº§æ“ä½œ
DÂ³Såœ¨æ ·æœ¬çº§ï¼Œä¸æ˜¯æœ€å¤§åŒ–å¥–åŠ±æ–¹å·®Var(R)ï¼Œè€Œæ˜¯é¦–å…ˆä¼°è®¡æ•´ä¸ªæ‰¹æ¬¡çš„ç»„ç›¸å¯¹ä¼˜åŠ¿ï¼Œç„¶åæœ€å¤§åŒ–ä¼˜åŠ¿æ–¹å·®Var(A)æ¥é€‰æ‹©ç”¨äºä¼˜åŒ–çš„æ ¸å¿ƒå­é›†ã€‚ç†è®ºè¯æ˜è¿™ç§é€‰æ‹©ä¸ç­–ç•¥æ¢¯åº¦èŒƒæ•°çš„ä¸Šç•Œæ­£ç›¸å…³ï¼Œå¯äº§ç”Ÿæ›´é«˜çš„ç­–ç•¥æ¢¯åº¦ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDÂ³Sçš„æ ‡è®°çº§æ“ä½œä¸åŠ¨æ€ä¸‹é‡‡æ ·è°ƒåº¦
åœ¨æ ‡è®°çº§ï¼ŒDÂ³Sæå‡ºå°†ä¼˜åŠ¿å¹…åº¦å’Œç­–ç•¥ç†µçš„ä¹˜ç§¯|Ai,t| Ã— Hi,tä½œä¸ºæ ‡è®°é‡è¦æ€§çš„åº¦é‡ï¼Œå…³æ³¨ç­–ç•¥æ—¢ä¸ç¡®å®šåˆæœ‰å½±å“åŠ›çš„æ ‡è®°è¿›è¡Œæ›´æ–°ã€‚åŒæ—¶ï¼Œä¸ºé˜²æ­¢ç­–ç•¥è¿‡åº¦æ‹Ÿåˆé«˜ä¿¡å·æ•°æ®ï¼ŒDÂ³Sé‡‡ç”¨å—è¯¾ç¨‹å­¦ä¹ å¯å‘çš„åŠ¨æ€ä¸‹é‡‡æ ·è°ƒåº¦ï¼Œå¼€å§‹æ—¶ç§¯æä¸‹é‡‡æ ·ä»¥åŠ é€Ÿæ—©æœŸå­¦ä¹ ï¼Œéšåé€æ¸æ”¾å®½ä»¥ä¿ƒè¿›ç¨³å¥çš„æ³›åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å„ç§å¼ºåŒ–å­¦ä¹ è®¾ç½®ï¼ˆå³GRPOå’ŒGSPOï¼‰ä¸‹ï¼Œé’ˆå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚ä»¥Qwen2.5 - Math - 7Bä¸ºéª¨å¹²åœ¨AIME24ä¸Šè®­ç»ƒä¸ºä¾‹ï¼Œä¸åŸå§‹GRPOç›¸æ¯”ï¼ŒDÂ³Sä¼˜åŒ–çš„æ ‡è®°ä¸åˆ°20%ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„ç­–ç•¥æ¢¯åº¦ï¼Œæ”¶æ•›é€Ÿåº¦æ˜¾è‘—åŠ å¿«ï¼Œæµ‹è¯•é›†ä¸Šçš„Pass@1åˆ†æ•°æ›´ä¼˜ã€‚ä½¿ç”¨Qwen2.5 - Math - 7Bä½œä¸ºéª¨å¹²æ—¶ï¼Œå¸¦DÂ³Sçš„GRPOåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„Pass@1å¹³å‡æé«˜4.5ï¼ŒPass@8å¹³å‡æé«˜3.7ï¼›ä½¿ç”¨Llama3.1 - 8B - Instructä½œä¸ºéª¨å¹²æ—¶ï¼Œå¸¦DÂ³Sçš„GRPOåœ¨Pass@1ä¸Šä¼˜äºåŸå§‹æ–¹æ³•3.3ï¼Œåœ¨Pass@8ä¸Šä¼˜äºåŸå§‹æ–¹æ³•7.8ã€‚æ­¤å¤–ï¼Œåˆ†æè¿˜å‘ç°æ ·æœ¬çº§å’Œæ ‡è®°çº§ä¸‹é‡‡æ ·åœ¨è®­ç»ƒæ—©æœŸæœ‰æ•ˆæ¶ˆé™¤æ— å·®å¼‚ä¿¡å·ï¼ŒåŠ é€Ÿç­–ç•¥æ”¶æ•›ï¼›è®­ç»ƒåæœŸåŠ¨æ€ä¸‹é‡‡æ ·è°ƒåº¦å¯¹å¢å¼ºDÂ³Sçš„æ³›åŒ–æ€§è‡³å…³é‡è¦ï¼›DÂ³Sèƒ½æ›´å¥½åœ°ç®¡ç†ç†µæ³¢åŠ¨ï¼Œåæ˜ æ›´ç¨³å®šçš„ç­–ç•¥è®­ç»ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **åŒå±‚ä¸‹é‡‡æ ·æ€è·¯**ï¼šDÂ³Sä»æ ·æœ¬çº§å’Œæ ‡è®°çº§ä¸¤ä¸ªå±‚é¢è¿›è¡Œä¸‹é‡‡æ ·æ“ä½œï¼Œä¸ºä¼˜åŒ–ç­–ç•¥æä¾›äº†æ–°çš„è§†è§’ï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶å¯å€Ÿé‰´è¿™ç§åˆ†å±‚ç­›é€‰é‡è¦ä¿¡æ¯çš„æ–¹å¼ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚
2. **åŠ¨æ€è°ƒåº¦ç­–ç•¥**ï¼šå—è¯¾ç¨‹å­¦ä¹ å¯å‘çš„åŠ¨æ€ä¸‹é‡‡æ ·è°ƒåº¦ï¼Œå¹³è¡¡äº†æ—©æœŸå¿«é€Ÿå­¦ä¹ å’ŒåæœŸæ³›åŒ–èƒ½åŠ›çš„æå‡ï¼Œåœ¨å…¶ä»–éœ€è¦é˜²æ­¢è¿‡æ‹Ÿåˆã€æå‡æ¨¡å‹æ³›åŒ–æ€§çš„ä»»åŠ¡ä¸­ï¼Œè¿™ç§è°ƒåº¦ç­–ç•¥å…·æœ‰å‚è€ƒä»·å€¼ã€‚
3. **ç†è®ºåˆ†ææ”¯æ’‘**ï¼šè®ºæ–‡é€šè¿‡ä¸¥è°¨çš„ç†è®ºåˆ†ææ¨å¯¼ç­–ç•¥æ¢¯åº¦èŒƒæ•°çš„ä¸Šç•Œç­‰ï¼Œä¸ºæ–¹æ³•çš„æœ‰æ•ˆæ€§æä¾›äº†ç†è®ºä¾æ®ï¼Œåœ¨ç ”ç©¶æ–°æ–¹æ³•æ—¶ï¼Œç†è®ºåˆ†æä¸å®éªŒéªŒè¯ç›¸ç»“åˆçš„æ–¹å¼å€¼å¾—å€Ÿé‰´ã€‚
``` 

## deep-think-with-confidence
### Abstract
Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | DeepConfï¼šæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡ä¸æ€§èƒ½çš„æ–°åˆ©å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚å¸¦å¤šæ•°æŠ•ç¥¨çš„è‡ªä¸€è‡´æ€§ï¼‰å¯æå‡æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¸¸å¯¼è‡´å‡†ç¡®æ€§çš„æ”¶ç›Šé€’å‡ä»¥åŠè¾ƒé«˜çš„è®¡ç®—å¼€é”€ï¼Œä¾‹å¦‚åœ¨AIME 2025ä¸Šä½¿ç”¨Qwen3 - 8Bé€šè¿‡æ ‡å‡†å¤šæ•°æŠ•ç¥¨å°†pass@1å‡†ç¡®ç‡ä»68%æå‡åˆ°82%ï¼Œæ¯ä¸ªé—®é¢˜éœ€é¢å¤–ç”Ÿæˆ511æ¡æ¨ç†è½¨è¿¹ï¼Œæ¶ˆè€—1äº¿é¢å¤–çš„tokenã€‚å¹¶ä¸”ï¼Œå¸¦å¤šæ•°æŠ•ç¥¨çš„å¹¶è¡Œæ€ç»´å­˜åœ¨å±€é™æ€§ï¼Œéšç€æ¨ç†è½¨è¿¹æ•°é‡å¢åŠ ï¼Œæ€§èƒ½å¸¸é¥±å’Œæˆ–ä¸‹é™ï¼Œå› ä¸ºæ ‡å‡†å¤šæ•°æŠ•ç¥¨å¹³ç­‰å¯¹å¾…æ‰€æœ‰æ¨ç†è½¨è¿¹ï¼Œå¿½ç•¥äº†è´¨é‡å·®å¼‚ã€‚è™½ç„¶è¿‘æœŸæœ‰å·¥ä½œåˆ©ç”¨ä¸‹ä¸€ä¸ªtokenåˆ†å¸ƒç»Ÿè®¡è¯„ä¼°æ¨ç†è½¨è¿¹è´¨é‡ï¼Œä½†å…¨å±€ç½®ä¿¡åº¦åº¦é‡åœ¨å®è·µä¸­å­˜åœ¨é—®é¢˜ï¼Œå¦‚æ©ç›–å±€éƒ¨æ¨ç†æ­¥éª¤çš„ç½®ä¿¡åº¦æ³¢åŠ¨ï¼Œä¸”éœ€ç”Ÿæˆå®Œæ•´æ¨ç†è½¨è¿¹æ‰èƒ½è®¡ç®—ï¼Œæ— æ³•æå‰åœæ­¢ä½è´¨é‡è½¨è¿¹ç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDeep Think with Confidenceï¼ˆDeepConfï¼‰æ–¹æ³•ï¼Œç»“åˆå¹¶è¡Œæ€ç»´ä¸åŸºäºå±€éƒ¨ç½®ä¿¡åº¦æµ‹é‡çš„ç½®ä¿¡åº¦æ„ŸçŸ¥è¿‡æ»¤ã€‚è¯¥æ–¹æ³•å¯åœ¨ç¦»çº¿å’Œåœ¨çº¿ä¸¤ç§æ¨¡å¼ä¸‹è¿è¡Œï¼Œç¦»çº¿æ¨¡å¼ä¸‹ï¼Œé€šè¿‡è¯„ä¼°å’Œèšåˆæ¥è‡ªå·²å®Œæˆæ¨ç†è½¨è¿¹çš„ä¿¡æ¯ï¼Œåˆ©ç”¨ç½®ä¿¡åº¦æå‡æ¨ç†æ€§èƒ½ï¼›åœ¨çº¿æ¨¡å¼ä¸‹ï¼Œåœ¨tokenç”Ÿæˆè¿‡ç¨‹ä¸­çº³å…¥ç½®ä¿¡åº¦ï¼Œä»¥å®æ—¶æé«˜æ¨ç†æ€§èƒ½å’Œ/æˆ–è®¡ç®—æ•ˆç‡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥å¤šç§æ›¿ä»£çš„ç½®ä¿¡åº¦æµ‹é‡æ–¹å¼ã€‚å¦‚ç»„ç½®ä¿¡åº¦ï¼ˆGroup Confidenceï¼‰ï¼Œé€šè¿‡å¯¹æ¨ç†è½¨è¿¹ä¸­é‡å è·¨åº¦çš„tokenç½®ä¿¡åº¦è¿›è¡Œå¹³å‡ï¼Œé‡åŒ–ä¸­é—´æ¨ç†æ­¥éª¤çš„ç½®ä¿¡åº¦ï¼Œæ¯ä¸ªtokenä¸ä¸€ä¸ªç”±nä¸ªå…ˆå‰tokenç»„æˆçš„æ»‘åŠ¨çª—å£ç»„ç›¸å…³è”ï¼Œä¸ºä¸­é—´æ¨ç†æ­¥éª¤æä¾›æ›´å±€éƒ¨å’Œæ›´å¹³æ»‘çš„ä¿¡å·ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ï¼ˆAIME 2024/2025ã€HMMT 2025ã€BRUMO25ã€GPQA - Diamondï¼‰å’Œæ¨¡å‹ï¼ˆDeepSeek - 8Bã€Qwen3 - 8B/32Bã€GPT - OSS - 20B/120Bï¼‰ä¸Šå¯¹DeepConfè¿›è¡Œè¯„ä¼°ã€‚åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨GPT - OSS - 120Bï¼ˆæ— å·¥å…·ï¼‰æ—¶ï¼ŒDeepConf@512åœ¨AIME 2025ä¸Šè¾¾åˆ°99.9%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”cons@512ï¼ˆå¤šæ•°æŠ•ç¥¨ï¼‰çš„97.0%å’Œpass@1çš„91.8%ï¼Œä½¿è¯¥åŸºå‡†é¥±å’Œã€‚åœ¨åœ¨çº¿æ¨¡å¼ä¸‹ï¼Œä¸æ ‡å‡†å¹¶è¡Œæ€ç»´ç›¸æ¯”ï¼ŒDeepConfåœ¨ä¿æŒæˆ–è¶…è¿‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œtokenç”Ÿæˆé‡æœ€å¤šå¯å‡å°‘84.7%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DeepConfæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒæˆ–è¶…å‚æ•°è°ƒæ•´ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰æœåŠ¡æ¡†æ¶ä¸­ã€‚å…¶åˆ©ç”¨æ¨¡å‹å†…éƒ¨çš„ç½®ä¿¡åº¦ä¿¡å·åŠ¨æ€è¿‡æ»¤ä½è´¨é‡æ¨ç†è½¨è¿¹çš„æ€è·¯ï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡å’Œæ€§èƒ½æä¾›äº†æ–°çš„æ–¹å‘ï¼Œåœ¨å®é™…åº”ç”¨ä¸­æœ‰æœ›åœ¨å‡å°‘è®¡ç®—èµ„æºæ¶ˆè€—çš„åŒæ—¶ï¼Œç»´æŒç”šè‡³æå‡æ¨ç†ç»“æœçš„å‡†ç¡®æ€§ï¼Œå¯¹äºå¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å®é™…éƒ¨ç½²å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚
``` 

## mas$^2$--self-generative--self-configuring--self-rectifying-multi-agent-systems
### Abstract
The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | MASÂ²ï¼šå¼€å¯å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè‡ªæˆ‘è¿›åŒ–æ–°æ—¶ä»£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è¿‡å»ä¸¤å¹´é‡Œï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰è¿…é€Ÿå´›èµ·ï¼Œä»éœ€è¦æ‰‹åŠ¨é…ç½®æç¤ºã€å·¥å…·ã€è§’è‰²å’Œé€šä¿¡åè®®çš„ç³»ç»Ÿï¼Œå‘å±•åˆ°èƒ½å¤Ÿè‡ªåŠ¨ç¼–æ’çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œä¸»æµçš„è‡ªåŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ— è®ºæ˜¯ç”±å¤–éƒ¨æ¨¡å—è¿˜æ˜¯å•ä¸ªLLMæ™ºèƒ½ä½“ç”Ÿæˆï¼Œå¤§å¤šéµå¾ª â€œä¸€æ¬¡ç”Ÿæˆå¹¶éƒ¨ç½²â€ çš„åˆšæ€§èŒƒå¼ï¼Œè¿™ä½¿å¾—ç³»ç»Ÿåœ¨é¢å¯¹ç°å®ä¸–ç•Œç¯å¢ƒçš„åŠ¨æ€æ€§å’Œä¸ç¡®å®šæ€§æ—¶ååˆ†è„†å¼±ï¼Œéš¾ä»¥é€‚åº”ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™ï¼Œè®ºæ–‡æå‡ºäº†MASÂ²èŒƒå¼ï¼Œæ—¨åœ¨å®ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é€’å½’è‡ªæˆ‘ç”Ÿæˆï¼Œè®©å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»åœ°ä¸ºå„ç§é—®é¢˜æ„å»ºå®šåˆ¶çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMASÂ²èŒƒå¼
å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„èŒƒå¼ï¼Œå³ç”±å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè‡ªä¸»æ„å»ºå¦ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚é€šè¿‡åè°ƒä¸€ä¸ªç”±LLMæ”¯æŒçš„ä¸“é—¨å…ƒæ™ºèƒ½ä½“å›¢é˜Ÿï¼Œä»¥ä»»åŠ¡è‡ªé€‚åº”å’Œè¿›åº¦æ„ŸçŸ¥çš„æ–¹å¼ç”Ÿæˆã€é…ç½®å’Œçº æ­£ç³»ç»Ÿï¼Œå…‹æœäº†å¤–éƒ¨æ¨¡å—çš„åˆ›é€ æ€§é™åˆ¶ä»¥åŠä¼ ç»Ÿ â€œä¸€æ¬¡ç”Ÿæˆå¹¶éƒ¨ç½²â€ ç­–ç•¥çš„åˆšæ€§ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ â€œç”Ÿæˆå™¨ - å®ç°å™¨ - çº æ­£å™¨â€ ä¸‰å…ƒæ™ºèƒ½ä½“æ¡†æ¶
æ„å»ºäº†ä¸€ä¸ªåŸºäºä¸‰å…ƒæ™ºèƒ½ä½“æ¶æ„çš„å…ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆmeta MASï¼‰ã€‚ç”Ÿæˆå™¨æ™ºèƒ½ä½“ä¸ºç»™å®šæŸ¥è¯¢è®¾è®¡é«˜çº§å¤šæ™ºèƒ½ä½“å·¥ä½œæµæ¨¡æ¿ï¼›å®ç°å™¨æ™ºèƒ½ä½“é€šè¿‡ä¸ºæ¯ä¸ªæ­¥éª¤å¡«å……å…·ä½“çš„LLMä¸»å¹²æ¥å®ä¾‹åŒ–æ¨¡æ¿ï¼Œä½¿å·¥ä½œæµå¯æ‰§è¡Œï¼›çº æ­£å™¨æ™ºèƒ½ä½“åœ¨è¿è¡Œæ—¶ä¸»åŠ¨ç›‘æ§æ‰§è¡ŒçŠ¶æ€å’Œç¯å¢ƒåé¦ˆï¼ŒåŠæ—¶å¯¹ç³»ç»Ÿè¿›è¡Œçº æ­£ä»¥é€‚åº”åŠ¨æ€æ¡ä»¶ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡åä½œæ ‘ä¼˜åŒ–ï¼ˆCTOï¼‰ç¨‹åº
æå‡ºä¸€ç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç”¨äºè½¨è¿¹æ”¶é›†å’Œä¼˜åŒ–ã€‚åœ¨CTOæ¡†æ¶ä¸­ï¼Œä¸‰ä¸ªæ™ºèƒ½ä½“åä½œæ‰©å±•ä»£è¡¨ä¸åŒMASé…ç½®å’Œæ‰§è¡Œè·¯å¾„çš„å†³ç­–æ ‘ï¼Œé€šè¿‡è·¯å¾„ä¿¡ç”¨ä¼ æ’­æœºåˆ¶å°†æœ€ç»ˆç»“æœå½’å› äºæ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸Šæ¸¸å†³ç­–ï¼Œåˆ©ç”¨åŸºäºç›¸å¯¹å¥–åŠ±çš„åå¥½å¯¹é½ç®—æ³•æœ‰æ•ˆåœ°ä½¿æ¯ä¸ªæ™ºèƒ½ä½“çš„å…ƒç”ŸæˆåŠŸèƒ½ä¸“ä¸šåŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒMASÂ²åœ¨æ·±åº¦ç ”ç©¶å’Œä»£ç ç”Ÿæˆç­‰å¤æ‚åœºæ™¯ä¸­ï¼Œç›¸æ¯”æœ€å…ˆè¿›çš„MASæ€§èƒ½æå‡é«˜è¾¾19.6%ã€‚æ­¤å¤–ï¼ŒMASÂ²è¡¨ç°å‡ºå“è¶Šçš„è·¨ä¸»å¹²æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä»¥å‰æœªè§è¿‡çš„LLMï¼Œå®ç°é«˜è¾¾15.1%çš„æ€§èƒ½æå‡ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™äº›æ€§èƒ½æå‡å¹¶æœªå¸¦æ¥è¿‡å¤šçš„ä»¤ç‰Œæˆæœ¬ï¼ŒMASÂ²å§‹ç»ˆä½äºæˆæœ¬ - æ€§èƒ½æƒè¡¡çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **èŒƒå¼åˆ›æ–°æ€è·¯**ï¼šMASÂ²çš„é€’å½’è‡ªæˆ‘ç”ŸæˆèŒƒå¼ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°çš„æ–¹å‘ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„ç”Ÿæˆå’Œéƒ¨ç½²æ¨¡å¼ï¼Œå¯¹äºè¿½æ±‚æ›´é«˜é€‚åº”æ€§å’Œåˆ›é€ æ€§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºå…·æœ‰å¯å‘æ„ä¹‰ã€‚
2. **æ™ºèƒ½ä½“åä½œä¸è®­ç»ƒæ–¹æ³•**ï¼šä¸‰å…ƒæ™ºèƒ½ä½“æ¡†æ¶ä»¥åŠCTOç¨‹åºä¸­æ™ºèƒ½ä½“çš„åä½œæ–¹å¼å’Œè®­ç»ƒç­–ç•¥ï¼Œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æ™ºèƒ½ä½“çš„åˆ†å·¥ã€åä½œä»¥åŠè®­ç»ƒæä¾›äº†å¯å‚è€ƒçš„æ¨¡å¼ï¼Œæœ‰åŠ©äºæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œé€‚åº”æ€§ã€‚
3. **æˆæœ¬ - æ€§èƒ½å¹³è¡¡**ï¼šåœ¨å®ç°æ€§èƒ½æå‡çš„åŒæ—¶ä¿æŒæˆæœ¬ - æ€§èƒ½çš„å¹³è¡¡ï¼Œè¿™ä¸€æˆæœå¯¹äºå®é™…åº”ç”¨ä¸­èµ„æºæœ‰é™çš„åœºæ™¯å…·æœ‰é‡è¦çš„å€Ÿé‰´ä»·å€¼ï¼Œä¸ºåœ¨èµ„æºçº¦æŸä¸‹ä¼˜åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½æä¾›äº†æ€è·¯ã€‚
``` 

## token-importance-guided-direct-preference-optimization
### Abstract
Ensuring that large language models (LLMs) generate outputs aligned with human preferences is important for safe and effective AI interactions. While Direct Preference Optimization (DPO) employs an implicit reward function to optimize the policy model, however, it and its related variants overlook the differential importance of individual tokens and are sensitive to judgment noise in preference datasets during generation. Although recent methods attempt to assess the important weight of tokens via probability prediction or simplistic weighting schemes, these evaluation methods are prone to biases and still cannot fully address these issues. To solve this problem, we propose the Token-Importance Guided Direct Preference Optimization (TI-DPO), which introduces two key innovations: the gradient-based token-importance weights that dynamically prioritize critical tokens, and a triple loss that explicitly guides model outputs to approach human-preferred responses and stay away from non-preferred responses. Experimental results show that TI-DPO achieves higher accuracy and stronger generative diversity, providing more stable and computationally efficient solutions compared with DPO and other RLHF methods.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | TI - DPOï¼šå¼•é¢†å¤§è¯­è¨€æ¨¡å‹åå¥½ä¼˜åŒ–æ–°æ–¹å‘

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€é€»è¾‘æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å¯èƒ½ä¸é¢„æœŸç›®çš„æˆ–ä¼¦ç†æ ‡å‡†ä¸ä¸€è‡´ã€‚å› æ­¤ï¼Œä½¿LLMç”Ÿæˆç¬¦åˆäººç±»åå¥½çš„å†…å®¹è‡³å…³é‡è¦ï¼Œäººç±»åå¥½å¯¹é½æ—¨åœ¨ç¡®ä¿LLMéµå¾ªäººç±»ä»·å€¼è§‚ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å®ç°å¯¹é½çš„ä¸»æµæ–¹æ³•ï¼Œä½†å®ƒå­˜åœ¨è®­ç»ƒä¸ç¨³å®šå’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¶ˆé™¤äº†å¯¹å¥–åŠ±æ¨¡å‹çš„éœ€æ±‚ï¼Œä½†DPOåŠå…¶ç›¸å…³å˜ä½“åœ¨ç”Ÿæˆé¦–é€‰å“åº”æ—¶å¿½ç•¥äº†å•ä¸ªæ ‡è®°çš„ä¸åŒé‡è¦æ€§ï¼Œå¹¶ä¸”å¯¹åå¥½æ•°æ®é›†ä¸­çš„åˆ¤æ–­å™ªå£°æ•æ„Ÿã€‚è™½ç„¶è¿‘æœŸæ–¹æ³•å°è¯•è¯„ä¼°æ ‡è®°çš„é‡è¦æƒé‡ï¼Œä½†è¿™äº›è¯„ä¼°æ–¹æ³•å®¹æ˜“äº§ç”Ÿåå·®ï¼Œæ— æ³•å®Œå…¨è§£å†³é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºæ¢¯åº¦çš„æ ‡è®°é‡è¦æ€§æƒé‡
é€šè¿‡æ¢¯åº¦å½’å› åŠ¨æ€åœ°å¯¹å…³é”®æ ‡è®°è¿›è¡Œä¼˜å…ˆçº§æ’åºï¼Œæ ‡è®°é‡è¦æ€§æƒé‡å°±åƒä¸€ä¸ªâ€œèšå…‰ç¯â€ï¼Œå°†ä¼˜åŒ–é‡ç‚¹æ”¾åœ¨å¯¹äººç±»åˆ¤æ–­å½±å“æœ€å¤§çš„æ ‡è®°ä¸Šã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰å…ƒæŸå¤±
æ˜ç¡®å¼•å¯¼æ¨¡å‹è¾“å‡ºæ¥è¿‘äººç±»åå¥½çš„å“åº”ï¼Œå¹¶è¿œç¦»éåå¥½çš„å“åº”ï¼Œæ„å»ºäº†â€œå¥½ - åâ€ä¸‰å…ƒç»„å…³ç³»ï¼Œé€šè¿‡çº³å…¥ä¸­é—´ç”Ÿæˆçš„è¾“å‡ºï¼Œå®ç°ç»†ç²’åº¦çš„åå¥½å¯¹é½ï¼Œå¹¶ä¿ƒè¿›åå¥½å­¦ä¹ çš„è¿ç»­æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–äºäºŒå…ƒæ¯”è¾ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å°†LLMä¸äººç±»åå¥½å¯¹é½æ–¹é¢ï¼ŒTI - DPOè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚åœ¨åå¥½æ¯”è¾ƒä¸­ï¼ŒTI - DPOåœ¨TruthfulQAå’ŒIFEvalä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼Œé€šè¿‡é›·è¾¾å›¾åœ¨å¤šä¸ªä»»åŠ¡ç»´åº¦ä¸Šä¸ä¸‰ä¸ªåŸºç¡€æŒ‡ä»¤æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒè¯å®äº†å…¶æ ‡è®°é‡è¦æ€§æŒ‡å¯¼å’Œä¸‰å…ƒæŸå¤±ç»„ä»¶çš„å¿…è¦æ€§ã€‚ç†è®ºä¸Šï¼Œè¯æ˜äº†TI - DPOæ¯”DPOå®ç°äº†æ›´ç´§çš„æŸå¤±è¾¹ç•Œï¼Œç¡®ä¿äº†æ›´ç¨³å®šçš„ä¼˜åŒ–ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ ‡è®°é‡è¦æ€§æƒé‡çš„å¼•å…¥**ï¼šåœ¨æ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ä¸­è€ƒè™‘æ ‡è®°çš„ä¸åŒé‡è¦æ€§ï¼Œä¸ºè§£å†³æ¨¡å‹ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œåœ¨å¤„ç†å¯¹æ ‡è®°æƒé‡æ•æ„Ÿçš„ä»»åŠ¡ï¼ˆå¦‚åŒ»ç–—å»ºè®®ç”Ÿæˆç­‰ï¼‰æ—¶æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ã€‚
2. **ä¸‰å…ƒæŸå¤±çš„è®¾è®¡**ï¼šæ„å»ºä¸‰å…ƒç»„å…³ç³»æ¥å¼•å¯¼æ¨¡å‹è¾“å‡ºï¼Œçªç ´äº†ä¼ ç»Ÿçš„äºŒå…ƒæ¯”è¾ƒæ–¹å¼ï¼Œå¯¹äºå®ç°æ›´ç»†ç²’åº¦çš„åå¥½å¯¹é½å…·æœ‰åˆ›æ–°æ€§ï¼Œåœ¨éœ€è¦ç²¾ç¡®æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„é¢†åŸŸï¼ˆå¦‚å†…å®¹å®¡æ ¸ç­‰ï¼‰å¯è€ƒè™‘å€Ÿé‰´ã€‚
3. **ç†è®ºä¸å®éªŒçš„ç»“åˆ**ï¼šä¸ä»…ä»ç†è®ºä¸Šè¯æ˜äº†TI - DPOçš„ä¼˜åŠ¿ï¼Œè¿˜é€šè¿‡ä¸°å¯Œçš„å®éªŒè¿›è¡Œäº†éªŒè¯ï¼Œè¿™ç§ç†è®ºä¸å®è·µç›¸ç»“åˆçš„ç ”ç©¶æ–¹å¼ä¸ºå…¶ä»–ç›¸å…³ç ”ç©¶æä¾›äº†è‰¯å¥½çš„èŒƒä¾‹ã€‚
``` 

## chasing-the-tail--effective-rubric-based-reward-modeling-for-large-language-model-post-training
### Abstract
Reinforcement fine-tuning (RFT) often suffers from \emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå‡†åˆ™çš„å¥–åŠ±å»ºæ¨¡ï¼šç ´è§£å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒçš„å¥–åŠ±è¿‡åº¦ä¼˜åŒ–éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¿‡ç¨‹ä¸­ï¼Œç»å¸¸å‡ºç°å¥–åŠ±è¿‡åº¦ä¼˜åŒ–çš„é—®é¢˜ã€‚å³ç­–ç•¥æ¨¡å‹ä¼šåˆ©ç”¨å¥–åŠ±ä¿¡å·çš„æ¼æ´ï¼Œåœ¨äº§ç”Ÿä½è´¨é‡è¾“å‡ºçš„åŒæ—¶å´è·å¾—é«˜åˆ†ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œå…³é”®åœ¨äºé«˜å¥–åŠ±å°¾éƒ¨çš„å¥–åŠ±é”™è¯¯æŒ‡å®šï¼Œä¹Ÿå°±æ˜¯æ— æ³•å¯é åœ°åŒºåˆ†ä¼˜ç§€å’Œä»…ä»…æ˜¯è‰¯å¥½çš„å›å¤ã€‚è™½ç„¶é«˜å¥–åŠ±åŒºåŸŸå¯¹äºLLMåè®­ç»ƒè‡³å…³é‡è¦ï¼Œä½†åœ¨åŸºç¡€LLMä¸‹ï¼Œæ­¤ç±»å°¾éƒ¨ç¤ºä¾‹éå¸¸ç¨€ç¼ºã€‚ä½¿ç”¨éç­–ç•¥ç¤ºä¾‹ï¼ˆå¦‚æ¥è‡ªæ›´å¼ºæ¨¡å‹æˆ–é‡å†™çš„ç¤ºä¾‹ï¼‰è™½å®¹æ˜“è·å–ï¼Œä½†ç›´æ¥åœ¨è¿™äº›ç¤ºä¾‹ä¸Šè®­ç»ƒå¥–åŠ±æ¨¡å‹å¯èƒ½ä¼šå­¦åˆ°è¡¨é¢ç‰¹å¾ï¼Œè€ŒéçœŸæ­£çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•äº§ç”Ÿåœ¨LLMåè®­ç»ƒä¸­æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹æˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç†è®ºåˆ»ç”»å¥–åŠ±é”™è¯¯æŒ‡å®šå¯¹åè®­ç»ƒçš„å½±å“
é€šè¿‡å¼•å…¥ä»çœŸå®å¥–åŠ±åˆ°ä»£ç†å¥–åŠ±çš„é”™è¯¯æŒ‡å®šæ˜ å°„fï¼Œåˆ†æfçš„å‡ ä½•å½¢çŠ¶å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¾—å‡ºä¿æŒé«˜å¥–åŠ±åŒºåŸŸçš„å‡†ç¡®æ€§æ˜¯å¯¹é½è´¨é‡çš„å…³é”®å†³å®šå› ç´ ï¼Œæ˜ç¡®äº†é«˜å¥–åŠ±åŒºåŸŸåœ¨LLMåè®­ç»ƒä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºéç­–ç•¥æ•°æ®æ„å»ºæœ‰æ•ˆå¥–åŠ±å‡†åˆ™çš„æ–¹æ³•
åˆ©ç”¨éç­–ç•¥ç”Ÿæˆè·å–éå¸¸å¼ºçš„ç¤ºä¾‹å›å¤ï¼Œç„¶åä½¿ç”¨å¦ä¸€ä¸ªLLMä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆè¯„åˆ†å‡†åˆ™ï¼Œä»¥æ­¤æ„å»ºåŸºäºå‡†åˆ™çš„å¥–åŠ±æ¨¡å‹ã€‚åŒæ—¶ç»™å‡ºäº†ä¸¤ä¸ªå®ç°ç›®æ ‡çš„åŸåˆ™ï¼Œå¹¶äº§ç”Ÿäº†å®ç°è¿™äº›æƒ³æ³•çš„å·¥ä½œæµç¨‹ã€‚è¿™ç§åŸºäºå‡†åˆ™çš„å¥–åŠ±è®¾è®¡ä¸Šå¯¹å›å¤çš„æ— å…³æ–¹é¢ä¸æ•æ„Ÿï¼Œèƒ½å¾ˆå¥½åœ°åœ¨éç­–ç•¥æƒ…å†µä¸‹æ³›åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å®éªŒå®è¯è¯æ˜ï¼ŒåŸºäºå‡†åˆ™çš„å¥–åŠ±èƒ½å¤Ÿæ˜¾è‘—å‡è½»å¥–åŠ±è¿‡åº¦ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶åœ¨LLMåè®­ç»ƒä»»åŠ¡ä¸­å¸¦æ¥æœ‰æ•ˆçš„æ”¹è¿›ï¼ŒéªŒè¯äº†æ‰€æ„å»ºå‡†åˆ™åœ¨åè®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠé«˜å¥–åŠ±åŒºåŸŸé”™è¯¯æŒ‡å®šçš„å…³é”®ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºæŒ‡å¯¼å®è·µ**ï¼šè®ºæ–‡çš„ç†è®ºåˆ†æä¸ºç†è§£å¥–åŠ±é”™è¯¯æŒ‡å®šå¯¹LLMåè®­ç»ƒçš„å½±å“æä¾›äº†æ¸…æ™°æ¡†æ¶ï¼Œå¯¹äºåç»­ç ”ç©¶å¥–åŠ±æ¨¡å‹è®¾è®¡å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚
2. **æ„å»ºå¥–åŠ±å‡†åˆ™çš„æ€è·¯**ï¼šåˆ©ç”¨éç­–ç•¥æ•°æ®æ„å»ºå¥–åŠ±å‡†åˆ™çš„æ–¹æ³•ä¸ºè§£å†³å¥–åŠ±è¿‡åº¦ä¼˜åŒ–é—®é¢˜æä¾›äº†æ–°çš„é€”å¾„ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥å€Ÿé‰´è¿™ç§æ€è·¯æ¥æå‡å¥–åŠ±æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚
3. **å·¥ä½œæµç¨‹çš„å‚è€ƒ**ï¼šæå‡ºçš„å®ç°æ„å»ºæœ‰æ•ˆå¥–åŠ±å‡†åˆ™çš„å·¥ä½œæµç¨‹å…·æœ‰å¯æ“ä½œæ€§ï¼Œå¯¹äºå¼€å±•ç±»ä¼¼LLMåè®­ç»ƒä»»åŠ¡çš„ç ”ç©¶å’Œå®è·µäººå‘˜æ¥è¯´ï¼Œæ˜¯ä¸€ä¸ªå€¼å¾—å‚è€ƒçš„èŒƒä¾‹ã€‚
``` 

## optimal-sparsity-of-mixture-of-experts-language-models-for-reasoning-tasks
### Abstract
Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization skills and reasoning skills. By training MoE families that vary total parameters, active parameters, and top-$k$ routing under fixed compute budgets, we disentangle pre-training loss from downstream accuracy. Our results reveal two principles. First, Active FLOPs: models with identical training loss but greater active compute achieve higher reasoning accuracy. Second, Total tokens per parameter (TPP): memorization tasks improve with more parameters, while reasoning tasks benefit from optimal TPP, indicating that reasoning is data-hungry. Neither reinforcement learning post-training (GRPO) nor increased test-time compute alters these trends. We therefore argue that optimal MoE sparsity must be determined jointly by active FLOPs and TPP, revising the classical picture of compute-optimal scaling. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æœ€ä¼˜ç¨€ç–æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ç”±ç»éªŒç¼©æ”¾å®šå¾‹é©±åŠ¨ï¼Œç„¶è€Œè¿™äº›å®šå¾‹çš„ç³»æ•°ä¼šéšç€æ¨¡å‹æ¶æ„æˆ–æ•°æ®ç®¡é“çš„æ”¹å˜è€Œå˜åŒ–ã€‚æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ä½œä¸ºå½“å‰å…ˆè¿›ç³»ç»Ÿçš„æ ‡å‡†ï¼Œå¼•å…¥äº†å½“å‰å¯†é›†æ¨¡å‹å‰æ²¿æ‰€å¿½è§†çš„æ–°ç¨€ç–ç»´åº¦ã€‚åŒæ—¶ï¼Œè¯„ä¼°é¢„è®­ç»ƒåçš„æ¨ç†æ€§èƒ½å¾€å¾€å¿½ç•¥äº†è®­ç»ƒåè‡ªé€‚åº”çš„å¥½å¤„å’Œé¢å¤–æµ‹è¯•æ—¶è®¡ç®—çš„ä½œç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶MoEç¨€ç–æ€§å¦‚ä½•å½±å“è®°å¿†æŠ€èƒ½å’Œæ¨ç†æŠ€èƒ½è¿™ä¸¤ç§ä¸åŒçš„èƒ½åŠ›æœºåˆ¶ï¼Œä»¥ç¡®å®šåœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶MoEçš„æœ€ä¼˜ç¨€ç–æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ ·åŒ–è®­ç»ƒMoEæ¨¡å‹æ—
åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œè®­ç»ƒæ€»å‚æ•°ã€æ´»åŠ¨å‚æ•°å’Œtop - kè·¯ç”±æ•°é‡ä¸åŒçš„MoEæ¨¡å‹æ—ï¼Œé€šè¿‡æµ‹é‡é¢„è®­ç»ƒæ•°æ®ä¸Šçš„æŸå¤±ã€ä¸‹æ¸¸åŸºå‡†æµ‹è¯•çš„ä»»åŠ¡æŸå¤±å’Œå‡†ç¡®æ€§ï¼Œæ¥è§£å¼€è®­ç»ƒæŸå¤±ä¸æµ‹è¯•æŸå¤±ä¹‹é—´çš„æ³›åŒ–å·®è·ä»¥åŠæŸå¤±ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å·®è·ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºä¸¤ä¸ªå…³é”®åŸåˆ™
 - **Active FLOPs**ï¼šå…·æœ‰ç›¸åŒè®­ç»ƒæŸå¤±ä½†æ›´é«˜æ´»åŠ¨è®¡ç®—é‡çš„æ¨¡å‹å®ç°æ›´é«˜çš„æ¨ç†å‡†ç¡®æ€§ã€‚
 - **Total tokens per parameter (TPP)**ï¼šè®°å¿†ä»»åŠ¡éšç€å‚æ•°å¢åŠ è€Œæ”¹å–„ï¼Œè€Œæ¨ç†ä»»åŠ¡å—ç›Šäºæœ€ä¼˜TPPï¼Œè¡¨æ˜æ¨ç†å¯¹æ•°æ®éœ€æ±‚å¤§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
 - å¯¹äºè®°å¿†å’Œæ¨ç†åŸºå‡†æµ‹è¯•ï¼Œè®­ç»ƒæŸå¤±éšæ€»å‚æ•°æ•°é‡å¢åŠ è€Œå•è°ƒå‡å°‘ã€‚è®°å¿†åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»»åŠ¡æŸå¤±å’Œå‡†ç¡®æ€§ä¸è®­ç»ƒæŸå¤±éµå¾ªç›¸åŒçš„å•è°ƒè¶‹åŠ¿ï¼›è€Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œéšç€æ€»å‚æ•°å¢åŠ å’Œè®­ç»ƒæŸå¤±å‡å°‘ï¼Œä»»åŠ¡æŸå¤±å’Œå‡†ç¡®æ€§åç¦»è¯¥å•è°ƒè¶‹åŠ¿ã€‚
 - æ”¹å˜top - kè·¯ç”±ä¸­çš„kï¼Œè‹¥æ´»åŠ¨å‚æ•°æ•°é‡ä¿æŒä¸å˜ï¼Œå…¶å½±å“å¯å¿½ç•¥ä¸è®¡ã€‚
 - ç»å…¸çš„æ³›åŒ–å·®è·æ§åˆ¶ï¼ˆå¦‚è°ƒæ•´å­¦ä¹ ç‡å’Œåˆå§‹åŒ–ï¼‰çš„æ•ˆæœä¸ç¨€ç–æ€§å¯¼è‡´çš„æ³›åŒ–å·®è·æƒŠäººä¸€è‡´ã€‚
 - åº”ç”¨GRPOæˆ–å¢åŠ æµ‹è¯•æ—¶è®¡ç®—ï¼Œéƒ½æ— æ³•æ”¹å˜å› ç¨€ç–æ€§å¢åŠ å¯¼è‡´çš„è®°å¿†å’Œæ¨ç†æ€§èƒ½å·®è·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - åœ¨è®¾è®¡å’Œè®­ç»ƒMoEæ¨¡å‹æ—¶ï¼Œéœ€åŒæ—¶è€ƒè™‘Active FLOPså’ŒTPPæ¥ç¡®å®šæœ€ä¼˜ç¨€ç–æ€§ï¼Œä¸ºæ¨¡å‹æ¶æ„è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ã€‚
 - å¯¹äºä¸åŒç±»å‹çš„ä»»åŠ¡ï¼ˆè®°å¿†å’Œæ¨ç†ï¼‰ï¼Œæ¨¡å‹æ€§èƒ½ä¸å‚æ•°ã€è®¡ç®—é‡å’Œæ•°æ®é‡ä¹‹é—´çš„å…³ç³»æœ‰æ‰€ä¸åŒï¼Œè¿™ä¸ºä»»åŠ¡ç‰¹å®šçš„æ¨¡å‹ä¼˜åŒ–æä¾›äº†å‚è€ƒã€‚
 - å®éªŒè¡¨æ˜è®­ç»ƒåæ–¹æ³•ï¼ˆå¦‚GRPOï¼‰å’Œæµ‹è¯•æ—¶è®¡ç®—åœ¨å¼¥è¡¥å› ç¨€ç–æ€§å¯¼è‡´çš„æ¨ç†æ€§èƒ½ä¸‹é™æ–¹é¢ä½œç”¨æœ‰é™ï¼Œè¿™æç¤ºåœ¨é¢„è®­ç»ƒé˜¶æ®µç¡®å®šåˆé€‚çš„ç¨€ç–æ€§è‡³å…³é‡è¦ï¼Œä¸ºåç»­ç ”ç©¶åœ¨è®¡ç®—é¢„ç®—ä¸‹è®­ç»ƒé«˜æ•ˆæ¨ç†æ¨¡å‹æä¾›äº†æ–¹å‘ã€‚
``` 

## rl-of-thoughts--navigating-llm-reasoning-with-inference-time-reinforcement-learning
### Abstract
Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs' parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for reproducibility.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RLofThoughtsï¼šç”¨æ¨ç†æ—¶å¼ºåŒ–å­¦ä¹ ä¸ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†å¯¼èˆª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¸å¤šè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å› å…¶å›ºæœ‰çš„tokençº§è‡ªå›å½’æ€§è´¨ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™ï¼Œå¦‚è§£å†³æ•°å­¦é—®é¢˜æˆ–å›ç­”å¤æ‚é—®é¢˜æ—¶ï¼Œéš¾ä»¥æ»¡è¶³å¯¹å¤æ‚é€»è¾‘ç»“æ„å’Œé•¿æœŸä¾èµ–çš„è¦æ±‚ã€‚ä¸ºæå‡LLMæ¨ç†èƒ½åŠ›ï¼Œæ¨ç†æ—¶æŠ€æœ¯ï¼ˆå¦‚æ€ç»´é“¾ã€æ€ç»´æ ‘ã€æ€ç»´å›¾ç­‰ï¼‰é€šè¿‡å¤–éƒ¨é€»è¾‘ç»“æ„å¼•å¯¼æ¨ç†ä¸”æ— éœ€ä¿®æ”¹LLMå‚æ•°ï¼Œå…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œä½†è¿™äº›æ‰‹åŠ¨é¢„å®šä¹‰ã€ä»»åŠ¡æ— å…³çš„æ¡†æ¶ç¼ºä¹é€‚åº”æ€§ï¼Œéš¾ä»¥åº”å¯¹ä¸åŒé¢†åŸŸä»»åŠ¡çš„å¤šæ ·æ€§å’ŒåŠ¨æ€æ€§ã€‚ä¸€æ–¹é¢ï¼Œæ¨ç†ä»»åŠ¡è·¨å¤šä¸ªé¢†åŸŸä¸”å„é¢†åŸŸä»»åŠ¡ç‰¹ç‚¹å„å¼‚ï¼Œæ‰‹åŠ¨è®¾è®¡ç‰¹å®šé€»è¾‘ç»“æ„ä¸å¯è¡Œï¼›å¦ä¸€æ–¹é¢ï¼Œå¤æ‚æ¨ç†ä»»åŠ¡å¸¸éœ€å¤šæ­¥éª¤ï¼Œæ¯æ­¥åé—®é¢˜è§£å†³çŠ¶æ€ä¼šå˜åŒ–ï¼Œé¢„å®šä¹‰é€»è¾‘ç»“æ„æ— æ³•é€‚åº”è¿™ç§å˜åŒ–ã€‚å› æ­¤ï¼Œéœ€è¦æ›´å…·é€‚åº”æ€§çš„æ¨ç†æ—¶æŠ€æœ¯æ¥å¤„ç†å¤šæ ·åŒ–å’ŒåŠ¨æ€çš„æ¨ç†ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRL - of - Thoughtsï¼ˆRLoTï¼‰æ¡†æ¶ï¼Œåœ¨æ¨ç†æ—¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡LLMsæ¨ç†èƒ½åŠ›ã€‚å°†é•¿åºåˆ—æ¨ç†å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œè®¾è®¡äº”ä¸ªå—äººç±»è®¤çŸ¥å¯å‘çš„åŸºæœ¬é€»è¾‘å—ä½œä¸ºå†³ç­–çš„æ½œåœ¨è¡ŒåŠ¨ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒä¸€ä¸ªRLä»£ç†ï¼ˆå³å¯¼èˆªå™¨æ¨¡å‹ï¼‰ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©å’Œç»„åˆè¿™äº›åŸºæœ¬é€»è¾‘å—ï¼Œæ„å»ºç‰¹å®šäºä»»åŠ¡çš„é€»è¾‘ç»“æ„ï¼Œä»è€Œå¢å¼ºLLMå¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ï¼ˆå¦‚AIMEã€MATHã€GPQAç­‰ï¼‰ä¸Šï¼Œä½¿ç”¨å¤šä¸ªLLMsï¼ˆå¦‚GPTã€Llamaã€Qwenå’ŒDeepSeekï¼‰è¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜RLoTä¼˜äºå·²æœ‰çš„æ¨ç†æ—¶æŠ€æœ¯ï¼Œæ€§èƒ½æå‡é«˜è¾¾13.4%ã€‚RLå¯¼èˆªå™¨å‚æ•°å°‘äº3Kï¼Œå´èƒ½ä½¿å°äº100äº¿å‚æ•°çš„LLMsæ€§èƒ½ä¸1000äº¿å‚æ•°è§„æ¨¡çš„LLMsç›¸å½“ã€‚æ­¤å¤–ï¼ŒRLå¯¼èˆªå™¨å…·æœ‰å¾ˆå¼ºçš„è¿ç§»æ€§ï¼Œåœ¨ä¸€ä¸ªç‰¹å®šLLM - ä»»åŠ¡å¯¹ä¸Šè®­ç»ƒçš„æ¨¡å‹èƒ½æœ‰æ•ˆæ³›åŒ–åˆ°æœªè§çš„LLMså’Œä»»åŠ¡ï¼Œæ— éœ€å¾®è°ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„RLoTæ¡†æ¶ä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åŠ¨æ€æ„å»ºä»»åŠ¡ç‰¹å®šé€»è¾‘ç»“æ„çš„æ–¹å¼ï¼Œåœ¨ä¸æ”¹å˜LLMå‚æ•°çš„æƒ…å†µä¸‹æœ‰æ•ˆå¢å¼ºæ¨ç†æ€§èƒ½ã€‚å…¶è®¾è®¡çš„åŸºæœ¬é€»è¾‘å—ä»äººç±»è®¤çŸ¥è§’åº¦å‡ºå‘ï¼Œå…·æœ‰ä¸€å®šçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚åŒæ—¶ï¼Œå¯¼èˆªå™¨æ¨¡å‹çš„è½»é‡çº§å’Œå¼ºè¿ç§»æ€§ï¼Œä¸ºåœ¨èµ„æºå—é™æƒ…å†µä¸‹æå‡æ¨¡å‹æ€§èƒ½ä»¥åŠè·¨æ¨¡å‹å’Œä»»åŠ¡çš„åº”ç”¨æä¾›äº†å€Ÿé‰´ï¼Œå¯¹äºæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨å…·æœ‰ç§¯ææ„ä¹‰ã€‚
``` 

## endowing-gpt-4-with-a-humanoid-body--building-the-bridge-between-off-the-shelf-vlms-and-the-physical-world
### Abstract
Humanoid agents often struggle to handle flexible and diverse interactions in open environments. A common solution is to collect massive datasets to train a highly capable model, but this approach can be prohibitively expensive. In this paper, we explore an alternative solution: empowering off-the-shelf Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents, thereby leveraging their strong open-world generalization to mitigate the need for extensive data collection. To this end, we present \textbf{BiBo} (\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf VLMs). It consists of two key components: (1) an \textbf{embodied instruction compiler}, which enables the VLM to perceive the environment and precisely translate high-level user instructions (e.g., {\small\itshape ``have a rest''}) into low-level primitive commands with control parameters (e.g., {\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and (2) a diffusion-based \textbf{motion executor}, which generates human-like motions from these commands, while dynamically adapting to physical feedback from the environment. In this way, BiBo is capable of handling not only basic interactions but also diverse and complex motions. Experiments demonstrate that BiBo achieves an interaction task success rate of 90.2\% in open environments, and improves the precision of text-guided motion execution by 16.3\% over prior methods. The code will be made publicly available.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | èµ‹äºˆGPT - 4ç±»äººèº¯ä½“ï¼Œæ‰“é€šé€šç”¨VLMä¸ç‰©ç†ä¸–ç•Œçš„æ¡¥æ¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç±»äººæ™ºèƒ½ä½“åœ¨å¼€æ”¾ç¯å¢ƒä¸­å¤„ç†çµæ´»å¤šæ ·çš„äº¤äº’æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿå¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯æ”¶é›†å¤§è§„æ¨¡çš„äººç±» - åœºæ™¯äº¤äº’æ•°æ®æ¥è®­ç»ƒé«˜æ€§èƒ½æ¨¡å‹ï¼Œä½†ç”±äºç±»äººèº¯ä½“çš„ç»“æ„å¤æ‚æ€§ä»¥åŠç°å®ç‰©ç†ä¸–ç•Œçš„å¤šæ ·æ€§ï¼Œè¿™ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ³›åŒ–ã€‚è€Œç°æˆçš„é€šç”¨è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚GPT - 4ã€Geminiå’ŒQwenç­‰ï¼Œåœ¨æ— éœ€ç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹å°±å±•ç°å‡ºäº†å¯¹å¼€æ”¾ä¸–ç•Œçš„æ¨ç†å’Œé€‚åº”èƒ½åŠ›ã€‚å› æ­¤ï¼Œè®ºæ–‡ä½œè€…æ€è€ƒèƒ½å¦ç›´æ¥åˆ©ç”¨è¿™äº›å¼ºå¤§çš„ç°æˆVLMsæ¥æ§åˆ¶ç±»äººæ™ºèƒ½ä½“ï¼Œä»¥å®ç°æ›´çµæ´»çš„ç‰©ç†ä¸–ç•Œäº¤äº’ï¼Œä»è€Œé¿å…æ˜‚è´µçš„æ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºBiBoæ¡†æ¶åŠå…·èº«æŒ‡ä»¤ç¼–è¯‘å™¨
ä½œè€…æå‡ºäº†BiBoï¼ˆBuilding humanoId agent By Off - the - shelf VLMsï¼‰æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€æ˜¯VLMé©±åŠ¨çš„å…·èº«æŒ‡ä»¤ç¼–è¯‘å™¨ã€‚è¯¥ç¼–è¯‘å™¨å—è®¡ç®—æœºä¸­ç¼–è¯‘å™¨å°†é«˜çº§ç¼–ç¨‹è¯­è¨€ç¿»è¯‘æˆä½çº§æ±‡ç¼–è¯­è¨€çš„å¯å‘ï¼Œèƒ½å¤Ÿæ ¹æ®ç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œå°†é«˜çº§è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºä½çº§æ‰§è¡Œå™¨å‘½ä»¤ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒå…ˆå°†åŠ¨ä½œè¡¨ç¤ºä¸ºåŒ…å«è¿åŠ¨æè¿°ã€å…³é”®å…³èŠ‚é…ç½®å’Œå…¶ä»–ä¸Šä¸‹æ–‡ç»†èŠ‚çš„ç»“æ„åŒ–æè¿°ç¬¦é›†åˆï¼Œç„¶åé©±åŠ¨VLMä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼å¯¹æ¯ä¸ªæè¿°ç¬¦è¿›è¡Œæ¨ç†ï¼Œè¿›è€Œç”Ÿæˆå‡†ç¡®ä¸”ç»“æ„åŒ–çš„å‘½ä»¤æ¥æŒ‡å®šç”¨æˆ·æ„å›¾çš„åŠ¨ä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºæ‰©æ•£çš„è¿åŠ¨æ‰§è¡Œå™¨åŠå¯¹LDMçš„æ–°åº”ç”¨
BiBoçš„å¦ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶æ˜¯åŸºäºæ‰©æ•£çš„è¿åŠ¨æ‰§è¡Œå™¨ï¼Œå®ƒç±»ä¼¼äºè®¡ç®—æœºä¸­çš„æ±‡ç¼–å™¨ï¼Œå°†å‘½ä»¤è§£é‡Šä¸ºç±»äººå…¨èº«è¿åŠ¨ã€‚ä¸ä¼ ç»ŸåŸºäºè§„åˆ™çš„æ±‡ç¼–å™¨ä¸åŒï¼Œè¯¥æ‰§è¡Œå™¨åˆ©ç”¨æ‰©æ•£ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡æ”¶åˆ°å‘½ä»¤æ—¶ï¼Œä»å½“å‰è¿åŠ¨æ‰©å±•æœªæ¥å…³èŠ‚è½¨è¿¹ï¼Œå®ç°å¤šæ ·åŒ–çš„è¿åŠ¨é£æ ¼å’Œå³æ—¶æ§åˆ¶ã€‚æ­¤å¤–ï¼Œåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œä¸ºå¤„ç†å› ç¢°æ’æˆ–å¤–åŠ›å¯¼è‡´çš„å®é™…æ‰§è¡Œè¿åŠ¨ä¸åˆå§‹ç”Ÿæˆåºåˆ—çš„åå·®ï¼Œä½œè€…å¼€å‘äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æ–°åº”ç”¨ã€‚é€šè¿‡ä»å®é™…æ‰§è¡Œçš„è¿åŠ¨æ‰©å±•æœªæ¥æ½œåœ¨ï¼Œå®ç°å¯¹ç¯å¢ƒçš„æ„ŸçŸ¥ï¼ŒåŒæ—¶åˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰è”åˆè§£ç å…ˆå‰å’Œå½“å‰ç”Ÿæˆè¿åŠ¨çš„æ½œåœ¨ï¼Œç¡®ä¿è¿åŠ¨çš„å¹³æ»‘è¿‡æ¸¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä½¿ç”¨ç°æˆçš„VLMï¼ˆå³GPT - 4oï¼‰çš„éšæœºç”Ÿæˆç‰©ç†ç¯å¢ƒä¸­ï¼ŒBiBoå®ç°äº†90.2%çš„äº¤äº’ä»»åŠ¡æˆåŠŸç‡ã€‚ä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼ŒBiBoå°†æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨æ‰§è¡Œç²¾åº¦æé«˜äº†16.3%ã€‚å®ƒä¸ä»…èƒ½å¤Ÿå¤„ç†å¤æ‚çš„è¿åŠ¨æ‰§è¡Œï¼Œè¿˜èƒ½é€šè¿‡ç”¨æˆ·æŒ‡ä»¤å®ç°æ— é™é•¿åºåˆ—åˆæˆå’Œå®æ—¶äº¤äº’æ§åˆ¶ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- **æ¨¡å‹åº”ç”¨æ€è·¯**ï¼šè®ºæ–‡æ¢ç´¢äº†åˆ©ç”¨ç°æˆçš„å¼ºå¤§VLMsæ¥æ§åˆ¶ç±»äººæ™ºèƒ½ä½“çš„æ–°æ–¹å‘ï¼Œä¸ºè§£å†³ç±»äººæ™ºèƒ½ä½“åœ¨å¼€æ”¾ç¯å¢ƒäº¤äº’é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­æ˜‚è´µçš„æ•°æ®æ”¶é›†ï¼Œå¯¹äºå…¶ä»–ç±»ä¼¼éœ€è¦åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ™ºèƒ½äº¤äº’çš„ç ”ç©¶æœ‰å¯å‘ä½œç”¨ã€‚
- **æ–¹æ³•ç»„ä»¶è®¾è®¡**ï¼šå…·èº«æŒ‡ä»¤ç¼–è¯‘å™¨ä¸­ç»“æ„åŒ–çš„ç±»äººåŠ¨ä½œè¡¨ç¤ºåŠä»ç²—åˆ°ç»†çš„æ¨ç†æ–¹å¼ï¼Œä¸ºç±»äººè¡Œä¸ºè§„åˆ’å’Œå»ºæ¨¡æä¾›äº†æ–°çš„æ–¹æ³•å‚è€ƒï¼›åŸºäºæ‰©æ•£çš„è¿åŠ¨æ‰§è¡Œå™¨å¯¹LDMçš„åˆ›æ–°åº”ç”¨ï¼Œåœ¨å¤„ç†ç¯å¢ƒåé¦ˆå’Œå®ç°å¹³æ»‘è¿åŠ¨è¿‡æ¸¡æ–¹é¢çš„ç­–ç•¥ï¼Œå¯¹äºè¿åŠ¨ç”Ÿæˆç›¸å…³ç ”ç©¶å…·æœ‰å€Ÿé‰´ä»·å€¼ã€‚
``` 

## rethinking-reward-models-for-multi-domain-test-time-scaling
### Abstract
The reliability of large language models (LLMs) during test-time scaling is often assessed with \emph{external verifiers} or \emph{reward models} that distinguish correct reasoning from flawed logic. Prior work generally assumes that process reward models (PRMs), which score every intermediate reasoning step, outperform outcome reward models (ORMs) that assess only the final answer. This view is based mainly on evidence from narrow, math-adjacent domains. We present the first unified evaluation of four reward model variants, discriminative ORM and PRM (\DisORM, \DisPRM) and generative ORM and PRM (\GenORM, \GenPRM), across 14 diverse domains. Contrary to conventional wisdom, we find that (i) \DisORM performs on par with \DisPRM, (ii) \GenPRM is not competitive, and (iii) overall, \GenORM is the most robust, yielding significant and consistent gains across every tested domain. We attribute this to PRM-style stepwise scoring, which inherits label noise from LLM auto-labeling and has difficulty evaluating long reasoning trajectories, including those involving self-correcting reasoning. Our theoretical analysis shows that step-wise aggregation compounds errors as reasoning length grows, and our empirical observations confirm this effect. These findings challenge the prevailing assumption that fine-grained supervision is always better and support generative outcome verification for multi-domain deployment. We publicly release our code, datasets, and checkpoints at \href{https://github.com/db-Lee/Multi-RM}{\underline{\small\texttt{https://github.com/db-Lee/Multi-RM}}} to facilitate future research in multi-domain settings.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šé¢†åŸŸæµ‹è¯•æ—¶ç¼©æ”¾ä¸­å¥–åŠ±æ¨¡å‹çš„å…¨æ–°æ€è€ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æµ‹è¯•æ—¶ç¼©æ”¾è¿‡ç¨‹ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯é æ€§å¸¸é€šè¿‡å¤–éƒ¨éªŒè¯å™¨æˆ–å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹ç”¨äºåŒºåˆ†æ­£ç¡®æ¨ç†å’Œæœ‰ç¼ºé™·çš„é€»è¾‘ã€‚å…ˆå‰çš„å·¥ä½œæ™®éè®¤ä¸ºï¼Œå¯¹æ¯ä¸ªä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡Œè¯„åˆ†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä¼˜äºä»…è¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰ï¼Œä¸”è¿™ä¸€è§‚ç‚¹ä¸»è¦åŸºäºç‹­çª„çš„ã€ä¸æ•°å­¦ç›¸å…³é¢†åŸŸçš„è¯æ®ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…³äºä½¿ç”¨å¤–éƒ¨éªŒè¯å™¨è¿›è¡Œæµ‹è¯•æ—¶ç¼©æ”¾çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸æ•°å­¦ç›¸å…³çš„é¢†åŸŸï¼Œè¿™ç§ç‹­çª„çš„èŒƒå›´é™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©çš„ç°å®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚æ³•å¾‹å’ŒåŒ»ç–—é¢†åŸŸï¼‰ä¸­çš„éƒ¨ç½²æ½œåŠ›ã€‚å°½ç®¡è¿‘æœŸæœ‰ç ”ç©¶æå‡ºåœ¨æ¶µç›–14ä¸ªä¸åŒé¢†åŸŸçš„åŸºå‡†ä¸Šè®­ç»ƒå¤šé¢†åŸŸPRMså¯æ˜¾è‘—æé«˜æµ‹è¯•æ—¶ç¼©æ”¾æ€§èƒ½ï¼Œä½†ä¸åŒéªŒè¯å™¨ç±»å‹åœ¨å¤šé¢†åŸŸè®¾ç½®ä¸­çš„æ›´å¹¿æ³›æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡å¯¹å››ç§å¥–åŠ±æ¨¡å‹å˜ä½“è¿›è¡Œç»Ÿä¸€è¯„ä¼°ã€‚æ¶µç›–åˆ¤åˆ«å¼ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆdORMï¼‰ã€åˆ¤åˆ«å¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆdPRMï¼‰ã€ç”Ÿæˆå¼ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆgORMï¼‰å’Œç”Ÿæˆå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆgPRMï¼‰ï¼Œå¹¶åœ¨14ä¸ªä¸åŒé¢†åŸŸè¿›è¡Œè¯„ä¼°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä»ç†è®ºå’Œå®è¯ä¸¤æ–¹é¢åˆ†æç»“æœã€‚ç†è®ºä¸Šåˆ†æPRMé£æ ¼çš„é€æ­¥èšåˆå¦‚ä½•éšç€æ¨ç†é•¿åº¦çš„å¢åŠ è€Œå¯¼è‡´é”™è¯¯ç´¯ç§¯ï¼›å®è¯ä¸Šé€šè¿‡æ¨¡æ‹Ÿæ ‡ç­¾å™ªå£°åˆ†æç­‰æ–¹å¼ï¼Œæ­ç¤ºä¸åŒå¥–åŠ±æ¨¡å‹åœ¨æ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œä»¥åŠgPRMåœ¨å¤šé¢†åŸŸè®¾ç½®ä¸­æ€§èƒ½ä¸‹é™çš„åŸå› ï¼ˆå¦‚å…±è¯†è¿‡æ»¤å¯¼è‡´çš„CoTé•¿åº¦åˆ†å¸ƒçš„ä¸¥é‡åç§»ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦é¢†åŸŸï¼Œå››ç§å˜ä½“çš„è¶‹åŠ¿ä¸å…ˆå‰å·¥ä½œä¸€è‡´ï¼ŒdPRMä¼˜äºdORMï¼Œç”Ÿæˆå¼å˜ä½“ä¼˜äºåˆ¤åˆ«å¼å˜ä½“ã€‚ä½†åœ¨å¤šé¢†åŸŸè®¾ç½®ä¸­ï¼Œå‡ºç°äº†ä¸ä¼ ç»Ÿè®¤çŸ¥ç›¸åçš„ç»“æœï¼šdORMä¸dPRMè¡¨ç°ç›¸å½“ï¼ŒgPRMç¼ºä¹ç«äº‰åŠ›ï¼Œæ€»ä½“è€Œè¨€gORMåœ¨æ‰€æœ‰æµ‹è¯•é¢†åŸŸä¸­éƒ½æ¯”å…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- å¯¹äºå¤šé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ¥è¯´ï¼Œåœ¨é€‰æ‹©å¥–åŠ±æ¨¡å‹æ—¶ï¼Œä¸åº”ç›²ç›®éµå¾ªæ•°å­¦é¢†åŸŸçš„ä¼ ç»Ÿè§‚ç‚¹ï¼Œéœ€ç»¼åˆè€ƒè™‘ä¸åŒé¢†åŸŸçš„ç‰¹ç‚¹ä»¥åŠæ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚
- ç†è®ºåˆ†æå’Œå®è¯ç»“æœä¸ºç†è§£å¥–åŠ±æ¨¡å‹åœ¨ä¸åŒæ¨ç†é•¿åº¦å’Œæ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹çš„è¡Œä¸ºæä¾›äº†æ·±å…¥è§è§£ï¼Œæœ‰åŠ©äºåœ¨å®é™…åº”ç”¨ä¸­æ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–å¥–åŠ±æ¨¡å‹ï¼Œä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶ç¼©æ”¾è¿‡ç¨‹ä¸­çš„å¯é æ€§ã€‚
- è®ºæ–‡å…¬å¼€çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä¸ºåç»­å¤šé¢†åŸŸè®¾ç½®ä¸‹çš„ç ”ç©¶æä¾›äº†å®è´µèµ„æºï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶äººå‘˜åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„æ¢ç´¢å’Œåˆ›æ–°ã€‚
``` 

## omniactor--a-generalist-gui-and-embodied-agent-for-2d&3d-worlds
### Abstract
Multimodal large language models are evolving toward multimodal agents capable of proactively executing tasks. Most agent research focuses on GUI or embodied scenarios, which correspond to agents interacting with 2D virtual worlds or 3D real worlds, respectively. However, many complex tasks typically require agents to interleavely interact with these two types of environment. We initially mix GUI and embodied data to train, but find the performance degeneration brought by the data conflict. Further analysis reveals that GUI and embodied data exhibit synergy and conflict at the shallow and deep layers, respectively, which resembles the cerebrum-cerebellum mechanism in the human brain. To this end, we propose a high-performance generalist agent OmniActor, designed from both structural and data perspectives. First, we propose Layer-heterogeneity MoE to eliminate the conflict between GUI and embodied data by separating deep-layer parameters, while leverage their synergy by sharing shallow-layer parameters. By successfully leveraging the synergy and eliminating the conflict, OmniActor outperforms agents only trained by GUI or embodied data in GUI or embodied tasks. Furthermore, we unify the action spaces of GUI and embodied tasks, and collect large-scale GUI and embodied data from various sources for training. This significantly improves OmniActor under different scenarios, especially in GUI tasks. The code will be publicly available.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | OmniActorï¼šå¼€å¯2Dä¸3Dä¸–ç•Œé€šç”¨æ™ºèƒ½ä½“æ–°æ—¶ä»£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ­£æœç€èƒ½å¤Ÿä¸»åŠ¨æ‰§è¡Œä»»åŠ¡çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“å‘å±•ã€‚å½“å‰å¤§å¤šæ•°æ™ºèƒ½ä½“ç ”ç©¶èšç„¦äºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æˆ–å…·èº«æ™ºèƒ½åœºæ™¯ï¼Œåˆ†åˆ«å¯¹åº”æ™ºèƒ½ä½“ä¸2Dè™šæ‹Ÿä¸–ç•Œæˆ–3Dç°å®ä¸–ç•Œçš„äº¤äº’ã€‚ç„¶è€Œï¼Œè®¸å¤šå¤æ‚ä»»åŠ¡å¾€å¾€éœ€è¦æ™ºèƒ½ä½“åœ¨è¿™ä¸¤ç§ç¯å¢ƒä¸­äº¤æ›¿äº¤äº’ã€‚åœ¨ç»Ÿä¸€ä¸åŒç¯å¢ƒæ•°æ®åˆ°ä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œå­˜åœ¨æ•°æ®å†²çªä¸ååŒé—®é¢˜ã€‚å°†GUIå’Œå…·èº«æ•°æ®æ··åˆè®­ç»ƒä¼šå› æ•°æ®å†²çªå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸”ç°æœ‰é€šç”¨æ™ºèƒ½ä½“å¤„ç†å†²çªå’ŒååŒçš„æœºåˆ¶ä¸è¶³ï¼ŒåŒæ—¶å…¶ä½¿ç”¨çš„å•ç¯å¢ƒï¼ˆGUIæˆ–å…·èº«ï¼‰æ•°æ®ä¹Ÿä¸å……åˆ†ï¼Œåœ¨GUIæˆ–å…·èº«ä»»åŠ¡ä¸­çš„è¡¨ç°è¿œä½äºæœ€å…ˆè¿›çš„å•ç¯å¢ƒæ™ºèƒ½ä½“ï¼Œéš¾ä»¥åœ¨å·¥ä¸šä¸­éƒ¨ç½²ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„é€šç”¨æ™ºèƒ½ä½“æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLayer - heterogeneity MoE
é€šè¿‡ç ”ç©¶ä¸åŒæ•°æ®çš„å‚æ•°æ›´æ–°æ–¹å‘ï¼Œå‘ç°ä¸æ·±å±‚ç›¸æ¯”ï¼Œæµ…å±‚ä¸­GUIå’Œå…·èº«æ•°æ®çš„å‚æ•°æ›´æ–°æ–¹å‘æ›´ä¸ºä¸€è‡´ã€‚ç±»æ¯”äººç±»å¤§è„‘çš„å¤§è„‘ - å°è„‘æœºåˆ¶ï¼Œå¤§è„‘æ›´æ¥è¿‘è¾“å…¥ï¼Œå¯¹ç¯å¢ƒå’ŒæŒ‡ä»¤è¿›è¡Œå…¨é¢ç†è§£ï¼›å°è„‘æ›´æ¥è¿‘è¾“å‡ºï¼Œéœ€æ‰§è¡Œä¸åŒåŠ¨ä½œã€‚å› æ­¤ï¼Œåœ¨æµ…å±‚å…±äº«å‚æ•°ä»¥åˆ©ç”¨GUIå’Œå…·èº«æ•°æ®ä¹‹é—´çš„ååŒä½œç”¨ï¼Œåœ¨æ·±å±‚åˆ†ç¦»å‚æ•°ä»¥æ¶ˆé™¤å› åŠ¨ä½œå·®å¼‚å¯¼è‡´çš„å†²çªï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“æ€§èƒ½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤§è§„æ¨¡GUIå’Œå…·èº«æ•°æ®
ä»å¤šç§æ¥æºæ”¶é›†æ•°æ®ï¼ŒåŒ…æ‹¬GUIæ•°æ®æºOS - Atlasã€Ugroundã€Aguvisã€Aria - UIå’Œå…·èº«æ•°æ®æ¥æºLIBEROç­‰ã€‚å°†æ‰€æœ‰æ•°æ®ç»Ÿä¸€ä¸ºç›¸åŒæ ¼å¼ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ç³»ç»Ÿæç¤ºã€å›¾åƒï¼ˆç¯å¢ƒï¼‰ã€ä»»åŠ¡æŒ‡ä»¤å’Œè¾“å‡ºåŠ¨ä½œã€‚ä½¿ç”¨æ–‡æœ¬æ ‡è®°å™¨å’Œç‰¹æ®Šçš„å…·èº«æ ‡è®°å™¨åˆ†åˆ«å°†GUIåŠ¨ä½œå’Œå…·èº«åŠ¨ä½œè½¬æ¢ä¸ºåŒä¸€è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ã€‚é€šè¿‡ç»Ÿä¸€æ•°æ®æ ¼å¼å’ŒåŠ¨ä½œç©ºé—´ï¼Œåˆ©ç”¨å¤§è§„æ¨¡GUIå’Œå…·èº«æ•°æ®è®­ç»ƒæ™ºèƒ½ä½“ï¼Œå¢å¼ºå…¶æ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
OmniActoråœ¨GUIå’Œå…·èº«ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰é€šç”¨æ™ºèƒ½ä½“ï¼Œç”šè‡³æ¯”æœ€å…ˆè¿›çš„å•ç¯å¢ƒæ™ºèƒ½ä½“æ›´å…·ä¼˜åŠ¿ã€‚é€šè¿‡æ€§èƒ½åˆ†æå›¾å¯çŸ¥ï¼Œä½¿ç”¨Layer - heterogeneity MoEæ¥åˆ©ç”¨ååŒä½œç”¨å¹¶æ¶ˆé™¤å†²çªçš„OmniActorï¼Œå…¶æˆåŠŸç‡æ˜æ˜¾é«˜äºä»…åœ¨GUIæ•°æ®ä¸Šè®­ç»ƒçš„OmniActor - GUIã€ä»…åœ¨å…·èº«æ•°æ®ä¸Šè®­ç»ƒçš„OmniActor - EAä»¥åŠåœ¨GUIå’Œå…·èº«æ•°æ®ä¸Šç®€å•æ··åˆè®­ç»ƒçš„OmniActor - EA&GUIã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æ™ºèƒ½ä½“è®¾è®¡æ–¹é¢ï¼ŒLayer - heterogeneity MoEè¿™ç§ä»ç»“æ„ä¸Šå¤„ç†æ•°æ®å†²çªå’ŒååŒçš„æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œå¯åº”ç”¨äºå…¶ä»–æ¶‰åŠå¤šæºæ•°æ®èåˆçš„æ™ºèƒ½ä½“æ¨¡å‹ä¸­ã€‚åœ¨æ•°æ®å¤„ç†ä¸Šï¼Œç»Ÿä¸€æ•°æ®æ ¼å¼å’ŒåŠ¨ä½œç©ºé—´ï¼Œå¹¶æ”¶é›†å¤§è§„æ¨¡å¤šæºæ•°æ®è¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ï¼Œä¸ºæå‡æ™ºèƒ½ä½“åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¯¹äºæ„å»ºé€šç”¨æ™ºèƒ½ä½“å…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚åŒæ—¶ï¼Œå…¶å¯¹äººç±»å¤§è„‘æœºåˆ¶çš„ç±»æ¯”å¯å‘æˆ‘ä»¬ä»ç”Ÿç‰©å­¦åŸç†ä¸­å¯»æ‰¾æ™ºèƒ½ä½“è®¾è®¡çš„çµæ„Ÿã€‚
``` 

## breaking-agent-backbones--evaluating-the-security-of-backbone-llms-in-ai-agents
### Abstract
AI agents powered by large language models (LLMs) are being deployed at scale, yet we lack a systematic understanding of how the choice of backbone LLM affects agent security. The non-deterministic sequential nature of AI agents complicates security modeling, while the integration of traditional software with AI components entangles novel LLM vulnerabilities with conventional security risks. Existing frameworks only partially address these challenges as they either capture specific vulnerabilities only or require modeling of complete agents. To address these limitations, we introduce threat snapshots: a framework that isolates specific states in an agent's execution flow where LLM vulnerabilities manifest, enabling the systematic identification and categorization of security risks that propagate from the LLM to the agent level. We apply this framework to construct the $\operatorname{b}^3$ benchmark, a security benchmark based on 194331 unique crowdsourced adversarial attacks. We then evaluate 31 popular LLMs with it, revealing, among other insights, that enhanced reasoning capabilities improve security, while model size does not correlate with security. We release our benchmark, dataset, and evaluation code to facilitate widespread adoption by LLM providers and practitioners, offering guidance for agent developers and incentivizing model developers to prioritize backbone security improvements.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„AIä»£ç†å®‰å…¨ï¼šå¨èƒå¿«ç…§ä¸bÂ³åŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„AIä»£ç†æ­£å¤§è§„æ¨¡éƒ¨ç½²ï¼Œä½†æˆ‘ä»¬ç¼ºä¹å¯¹ä¸»å¹²LLMçš„é€‰æ‹©å¦‚ä½•å½±å“ä»£ç†å®‰å…¨æ€§çš„ç³»ç»Ÿç†è§£ã€‚AIä»£ç†å…·æœ‰éç¡®å®šæ€§çš„é¡ºåºæ€§è´¨ï¼Œè¿™ä½¿å¾—å®‰å…¨å»ºæ¨¡å˜å¾—å¤æ‚ï¼ŒåŒæ—¶ä¼ ç»Ÿè½¯ä»¶ä¸AIç»„ä»¶çš„é›†æˆï¼Œå°†LLMçš„æ–°æ¼æ´ä¸ä¼ ç»Ÿå®‰å…¨é£é™©çº ç¼ åœ¨ä¸€èµ·ã€‚ç°æœ‰çš„æ¡†æ¶ä»…éƒ¨åˆ†è§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œå®ƒä»¬è¦ä¹ˆåªæ•è·ç‰¹å®šçš„æ¼æ´ï¼Œè¦ä¹ˆéœ€è¦å¯¹å®Œæ•´çš„ä»£ç†è¿›è¡Œå»ºæ¨¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥å¨èƒå¿«ç…§æ¡†æ¶
æå‡ºå¨èƒå¿«ç…§è¿™ä¸€æ­£å¼æ¡†æ¶ï¼Œå®ƒéš”ç¦»äº†AIä»£ç†æ‰§è¡Œæµç¨‹ä¸­LLMæ¼æ´æ˜¾ç°çš„ç‰¹å®šçŠ¶æ€ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°è¯†åˆ«å’Œåˆ†ç±»ä»LLMä¼ æ’­åˆ°ä»£ç†çº§åˆ«çš„å®‰å…¨é£é™©ã€‚è¯¥æ¡†æ¶æ•è·äº†ç°å®ä¸–ç•ŒAIä»£ç†ä¸­LLMæ¼æ´çš„å…·ä½“å®ä¾‹ï¼Œæä¾›äº†å¯¹æœ€ç›¸å…³çš„ä»£ç†å®‰å…¨é£é™©çš„è¯¦å°½æ”»å‡»åˆ†ç±»ï¼Œå°†LLMç‰¹æœ‰çš„æ¼æ´ä¸ä¼ ç»Ÿç³»ç»Ÿç»§æ‰¿çš„æ›´ä¸€èˆ¬é£é™©ç±»åˆ«åŒºåˆ†å¼€æ¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºbÂ³åŸºå‡†æµ‹è¯•
åŸºäºå¨èƒå¿«ç…§ï¼Œé€šè¿‡å¤§è§„æ¨¡ä¼—åŒ…æ”¶é›†é«˜è´¨é‡ã€å¯¹æŠ—æ€§ä¸”ä¾èµ–ä¸Šä¸‹æ–‡çš„æ”»å‡»ï¼Œåˆ›å»ºäº†ä¸»å¹²ç ´åè€…åŸºå‡†æµ‹è¯•ï¼ˆbÂ³åŸºå‡†æµ‹è¯•ï¼‰ã€‚è¯¥åŸºå‡†æµ‹è¯•æ˜¯ä¸€ä¸ªç”¨äºä»£ç†å®‰å…¨çš„åŸºå‡†ï¼Œå¯ä¾›ç¤¾åŒºä½¿ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½¿ç”¨bÂ³åŸºå‡†æµ‹è¯•å¯¹31ä¸ªæµè¡Œçš„ä¸»å¹²LLMsè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå¢å¼ºçš„æ¨ç†èƒ½åŠ›å¯æé«˜å®‰å…¨æ€§ï¼Œè€Œæ¨¡å‹å¤§å°ä¸å®‰å…¨æ€§æ²¡æœ‰ç›¸å…³æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¨èƒå¿«ç…§æ¡†æ¶ä¸ºåˆ†æåµŒå…¥åœ¨ä»£ç†ä¸­çš„ä¸»å¹²LLMsçš„æ¼æ´æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼ï¼Œå…¶å°†LLMæ¼æ´ä¸ä¼ ç»Ÿé£é™©åŒºåˆ†å¼€æ¥çš„æ€è·¯ï¼Œæœ‰åŠ©äºæ›´æ¸…æ™°åœ°è®¤è¯†å®‰å…¨é£é™©ã€‚é€šè¿‡ä¼—åŒ…æ”¶é›†æ”»å‡»æ•°æ®çš„æ–¹æ³•ï¼Œä¸ºè·å–é«˜è´¨é‡çš„æ”»å‡»æ•°æ®æä¾›äº†æ–°çš„é€”å¾„ã€‚bÂ³åŸºå‡†æµ‹è¯•ä¸ºå°†å®‰å…¨æ€§ä½œä¸ºLLMè¯„ä¼°çš„é¦–è¦ç»´åº¦æä¾›äº†åŸºç¡€ï¼Œä¸ºLLMæä¾›è€…å’Œä»ä¸šè€…æä¾›äº†ä¸€ä¸ªå¯å‚è€ƒçš„æ ‡å‡†ï¼Œä¹Ÿä¸ºä»£ç†å¼€å‘è€…æä¾›äº†æŒ‡å¯¼ï¼Œæ¿€åŠ±æ¨¡å‹å¼€å‘è€…ä¼˜å…ˆæ”¹è¿›ä¸»å¹²å®‰å…¨æ€§ã€‚
``` 

## spatial-reasoning-with-vision-language-models-in-ego-centric-multi-view-scenes
### Abstract
Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¿ˆå‘çœŸå®ä¸–ç•Œå¤šè§†è§’ç©ºé—´ç†è§£ï¼šè§†è§‰ - è¯­è¨€æ¨¡å‹çš„æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£3Dç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨é‡å¤§å±€é™ã€‚å…ˆå‰ç›¸å…³å·¥ä½œé€šè¿‡åŸºäºå•å¼ å›¾åƒæˆ–å®¤å†…è§†é¢‘åˆ›å»ºç©ºé—´é—®ç­”æ•°æ®é›†æ¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†ç°å®ä¸–ç•Œä¸­çš„å…·èº«AIæ™ºèƒ½ä½“ï¼Œå¦‚æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œé€šå¸¸ä¾èµ–ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¤šè§†è§’è§‚å¯Ÿã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦èšç„¦äºå•å›¾åƒæˆ–å®¤å†…é™æ€åœºæ™¯è§†é¢‘çš„ç©ºé—´æ¨ç†ï¼Œæ— æ³•åæ˜ ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¤šè§†è§’è¾“å…¥çš„ç»“æ„åŒ–ã€æ–¹å‘æ€§å’Œæ—¶é—´æ¼”å˜ç‰¹æ€§ï¼Œä¹Ÿæœªåœ¨åŠ¨æ€çœŸå®åœºæ™¯ä¸­è¯„ä¼°VLMsåœ¨è¿™äº›ç©ºé—´åŸºç¡€è§†è§’ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦æ–°çš„åŸºå‡†æµ‹è¯•æ¥æ›´å¥½åœ°åŒ¹é…å…·èº«æ™ºèƒ½ä½“çš„ç©ºé—´æ¨ç†éœ€æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥Ego3D - BenchåŸºå‡†æµ‹è¯•
ç²¾å¿ƒä»3ä¸ªå…¬å¼€æ•°æ®é›†ï¼ˆNuScenesã€Waymo Open Datasetå’ŒArgoverse 1ï¼‰çš„éªŒè¯é›†ä¸­ç­–åˆ’äº†åŒ…å«è¶…è¿‡8600ä¸ªé—®ç­”å¯¹çš„Ego3D - Benchï¼Œåœ¨æ•°æ®é›†æ„å»ºå’Œä¸¥æ ¼è´¨é‡å®¡æŸ¥è¿‡ç¨‹ä¸­ï¼Œäººç±»æ³¨é‡Šè€…å‘æŒ¥äº†æ ¸å¿ƒä½œç”¨ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸“é—¨é’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¤šè§†è§’ä»»åŠ¡ï¼Œæ’é™¤äº†å¯åŸºäºå•ä¸ªè§†å›¾ç‹¬ç«‹å›ç­”æˆ–ä¾èµ–å¤§è¯­è¨€æ¨¡å‹ä¸€èˆ¬çŸ¥è¯†å›ç­”çš„é—®é¢˜ï¼Œæ˜¯é¦–ä¸ªè¯„ä¼°VLMsåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¤šè§†è§’è¾“å…¥ä¸‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºEgo3D - VLMåè®­ç»ƒæ¡†æ¶
ä¸ºå¢å¼ºVLMsçš„3Dç©ºé—´ç†è§£èƒ½åŠ›ï¼Œæå‡ºEgo3D - VLMã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯åˆ›å»ºå‘¨å›´ç¯å¢ƒçš„æ–‡æœ¬è®¤çŸ¥åœ°å›¾ï¼Œè¯¥åœ°å›¾ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå®šä¹‰åæ ‡ç³»ï¼Œå¹¶åœ¨3Dåæ ‡ç©ºé—´ä¸­å®šä½é‡è¦å¯¹è±¡ã€‚ç»™å®šå¤šè§†è§’å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå…ˆä½¿ç”¨æŒ‡ä»£è¡¨è¾¾ç†è§£ï¼ˆRECï¼‰æ¨¡å‹æ‰¾åˆ°æŒ‡ä»£è¡¨è¾¾åœ¨åƒç´ ç©ºé—´ä¸­çš„2Dä½ç½®ï¼Œå†ç”¨åº¦é‡æ·±åº¦ä¼°è®¡å™¨ä¼°è®¡æ·±åº¦å€¼ï¼Œå°†2Dç‚¹è½¬æ¢ä¸ºç›¸æœºåæ ‡ç©ºé—´ä¸­çš„3Dç‚¹ï¼Œå¹¶å°†æ‰€æœ‰è§†å›¾çš„3Dç‚¹è½¬æ¢ä¸ºå…¨å±€åæ ‡ç©ºé—´ï¼ˆå³å‰ç›¸æœºåæ ‡ç©ºé—´ï¼‰ï¼Œé€šè¿‡è®¤çŸ¥åœ°å›¾ç”Ÿæˆå‡½æ•°è¿”å›æ–‡æœ¬è®¤çŸ¥åœ°å›¾ï¼Œä»¥ç»„ç»‡æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼Œä»è€Œæå‡VLMsåœ¨ç©ºé—´æ¨ç†ä¸Šçš„è¡¨ç°ï¼Œä¸”è¯¥æ¡†æ¶å¯å³æ’å³ç”¨äºç°æœ‰VLMsã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Ego3D - Benchä¸Šå¯¹16ä¸ªåŒ…æ‹¬é€šç”¨å’Œ3Dç©ºé—´çš„SOTA VLMsï¼ˆå¦‚GPT - 4oã€Gemini1.5 - Proã€InternVL3å’ŒQwen2.5 - VLç­‰ï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºäººç±»è¡¨ç°ä¸å½“å‰VLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä½¿ç”¨Ego3D - VLMåï¼Œåœ¨å¤šé¡¹é€‰æ‹©é—®ç­”ä»»åŠ¡ä¸Šå¹³å‡æå‡12%ï¼Œåœ¨ç»å¯¹è·ç¦»ä¼°è®¡ä¸Šå¹³å‡æå‡56%ï¼Œæ˜¾è‘—æé«˜äº†SOTAé€šç”¨ä»¥åŠ3Dç©ºé—´VLMsåœ¨3Dæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ•°æ®é›†æ„å»ºæ€è·¯**ï¼šEgo3D - Benchçš„æ„å»ºè¿‡ç¨‹ä¸­ï¼Œäººç±»æ³¨é‡Šè€…æ·±åº¦å‚ä¸ï¼Œä¸”é’ˆå¯¹ç‰¹å®šä»»åŠ¡ç²¾å¿ƒç­›é€‰é—®ç­”å¯¹ï¼Œè¿™ç§æ„å»ºæ–¹å¼ä¿è¯äº†æ•°æ®é›†çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œä¸ºæ„å»ºç¬¦åˆç‰¹å®šéœ€æ±‚çš„æ•°æ®é›†æä¾›äº†å€Ÿé‰´ã€‚
2. **æ¨¡å‹ä¼˜åŒ–æ–¹å‘**ï¼šEgo3D - VLMé€šè¿‡åˆ›å»ºæ–‡æœ¬è®¤çŸ¥åœ°å›¾æ¥æå‡VLMsç©ºé—´æ¨ç†èƒ½åŠ›çš„æ€è·¯ï¼Œä¸ºä¼˜åŒ–VLMsåœ¨ç©ºé—´ç†è§£æ–¹é¢çš„æ€§èƒ½æä¾›äº†æ–°æ–¹å‘ï¼Œå…¶æ¨¡å—åŒ–ä¸”å¯é›†æˆçš„ç‰¹æ€§ä¹Ÿä¸ºæ¨¡å‹æ”¹è¿›æä¾›äº†çµæ´»æ€§ã€‚
3. **åŸºå‡†æµ‹è¯•è®¾è®¡**ï¼šEgo3D - Benchä½œä¸ºé¦–ä¸ªé’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¤šè§†è§’è¾“å…¥è¯„ä¼°VLMsç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶è®¾è®¡ç†å¿µå’Œæµ‹è¯•ä»»åŠ¡çš„é€‰æ‹©ï¼Œä¸ºåç»­ç›¸å…³åŸºå‡†æµ‹è¯•çš„å¼€å‘æä¾›äº†å‚è€ƒï¼Œæœ‰åŠ©äºæ¨åŠ¨VLMsåœ¨çœŸå®ä¸–ç•Œå¤šè§†è§’åœºæ™¯ä¸‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç ”ç©¶ã€‚
``` 

## from-single-to-multi-granularity--toward-long-term-memory-association-and-selection-of-conversational-agents
### Abstract
Large Language Models (LLMs) have recently been widely adopted in conversational agents. However, the increasingly long interactions between users and agents accumulate extensive dialogue records, making it difficult for LLMs with limited context windows to maintain a coherent long-term dialogue memory and deliver personalized responses. While retrieval-augmented memory systems have emerged to address this issue, existing methods often depend on single-granularity memory segmentation and retrieval. This approach falls short in capturing deep memory connections, leading to partial retrieval of useful information or substantial noise, resulting in suboptimal performance. To tackle these limits, we propose MemGAS, a framework that enhances memory consolidation by constructing multi-granularity association, adaptive selection, and retrieval. MemGAS is based on multi-granularity memory units and employs Gaussian Mixture Models to cluster and associate new memories with historical ones. An entropy-based router adaptively selects optimal granularity by evaluating query relevance distributions and balancing information completeness and noise. Retrieved memories are further refined via LLM-based filtering. Experiments on four long-term memory benchmarks demonstrate that MemGAS outperforms state-of-the-art methods on both question answer and retrieval tasks, achieving superior performance across different query types and top-K settings. \footnote{https://github.com/quqxui/MemGAS}
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | MemGASï¼šè¿ˆå‘å¯¹è¯ä»£ç†é•¿æœŸè®°å¿†å…³è”ä¸é€‰æ‹©çš„å¤šç²’åº¦æ–°ç¯‡ç« 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯ä»£ç†ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†éšç€ç”¨æˆ·ä¸ä»£ç†ä¹‹é—´äº¤äº’æ—¶é—´å¢é•¿ï¼Œå¤§é‡å¯¹è¯è®°å½•ä¸æ–­ç§¯ç´¯ã€‚ç”±äºLLMsä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œéš¾ä»¥ç»´æŒè¿è´¯çš„é•¿æœŸå¯¹è¯è®°å¿†å¹¶ç»™å‡ºä¸ªæ€§åŒ–å›å¤ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºè®°å¿†ç³»ç»Ÿå‡ºç°ä»¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–å•ç²’åº¦çš„è®°å¿†åˆ†å‰²ä¸æ£€ç´¢ï¼Œæ— æ³•æ•æ‰æ·±å±‚è®°å¿†è¿æ¥ï¼Œå¯¼è‡´æœ‰ç”¨ä¿¡æ¯éƒ¨åˆ†æ£€ç´¢æˆ–å¼•å…¥å¤§é‡å™ªå£°ï¼Œæ€§èƒ½æ¬ ä½³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®°å¿†å…³è”
åˆ©ç”¨LLMsç”Ÿæˆè®°å¿†æ‘˜è¦å’Œå…³é”®è¯ï¼Œæ„å»ºå¤šç²’åº¦è®°å¿†å•å…ƒã€‚å½“æ–°è®°å¿†æ›´æ–°æ—¶ï¼Œé‡‡ç”¨é«˜æ–¯æ··åˆæ¨¡å‹å°†å†å²è®°å¿†èšç±»ä¸ºæ¥å—é›†ï¼ˆç›¸å…³ï¼‰å’Œæ‹’ç»é›†ï¼ˆä¸ç›¸å…³ï¼‰ï¼Œæ¥å—é›†ä¸­çš„è®°å¿†ä¸æ–°è®°å¿†å…³è”ï¼Œç¡®ä¿è®°å¿†ç»“æ„çš„æ•´åˆä¸å®æ—¶æ›´æ–°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç²’åº¦é€‰æ‹©
åŸºäºç†µçš„è·¯ç”±å™¨é€šè¿‡è¯„ä¼°æŸ¥è¯¢ç›¸å…³æ€§åˆ†å¸ƒçš„ç¡®å®šæ€§ï¼Œè‡ªé€‚åº”åœ°ä¸ºä¸åŒç²’åº¦åˆ†é…æ£€ç´¢æƒé‡ã€‚æœ€åï¼Œä½¿ç”¨ä¸ªæ€§åŒ–PageRankæ£€ç´¢å…³é”®è®°å¿†ï¼Œå¹¶é€šè¿‡LLMsè¿‡æ»¤ä»¥å»é™¤å†—ä½™ï¼Œç¡®ä¿é«˜è´¨é‡çš„è®°å¿†ï¼Œæå‡å¯¹è¯ä»£ç†çš„ç†è§£èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªå¼€æºé•¿æœŸè®°å¿†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMemGASåœ¨é—®ç­”å’Œæ£€ç´¢ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•å’Œå•ç²’åº¦æ–¹æ³•ï¼Œåœ¨ä¸åŒæŸ¥è¯¢ç±»å‹å’Œtop - Kè®¾ç½®ä¸‹å‡å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„å¤šç²’åº¦è®°å¿†å…³è”ä¸è‡ªé€‚åº”é€‰æ‹©æ¡†æ¶ä¸ºè§£å†³å¯¹è¯ä»£ç†é•¿æœŸè®°å¿†é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå…¶åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹è¿›è¡Œè®°å¿†èšç±»å…³è”ä»¥åŠåŸºäºç†µçš„è·¯ç”±å™¨è¿›è¡Œç²’åº¦é€‰æ‹©çš„æ–¹æ³•ï¼Œå¯¹äºå…¶ä»–éœ€è¦å¤„ç†å¤§é‡å†å²ä¿¡æ¯å¹¶è¿›è¡Œæœ‰æ•ˆæ£€ç´¢çš„åœºæ™¯å…·æœ‰å€Ÿé‰´æ„ä¹‰ï¼Œå¦‚æ™ºèƒ½å®¢æœç³»ç»Ÿã€æ™ºèƒ½æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿç­‰ã€‚
``` 

## scalecua--scaling-open-source-computer-use-agents-with-cross-platform-data
### Abstract
Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ScaleCUAï¼šå¼€å¯å¼€æºè®¡ç®—æœºä½¿ç”¨ä»£ç†çš„è·¨å¹³å°æ•°æ®æ‹“å±•ä¹‹è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½¿èƒ½å¤Ÿè‡ªä¸»æ“ä½œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIsï¼‰çš„è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¼€å‘å¼ºå¤§çš„CUAséœ€è¦å¹¿æ³›çš„è½¯ä»¶æ¥å£å’Œæ“ä½œé¢†åŸŸå†…çŸ¥è¯†ã€‚ä¸äº’è”ç½‘ä¸Šå¹¿æ³›å¯ç”¨çš„å›¾åƒ - æ–‡æœ¬å¯¹ä¸åŒï¼Œè®¡ç®—æœºä½¿ç”¨æ•°æ®ï¼Œå°¤å…¶æ˜¯æ“ä½œè½¨è¿¹ç¨€ç¼ºã€æ”¶é›†æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œè¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°æ•°æ®è§„æ¨¡å’Œç°æœ‰VLMsæœ‰é™çš„å¯è¿ç§»æ€§çš„åŒé‡é™åˆ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºè·¨å¹³å°äº¤äº’å¼æ•°æ®ç®¡é“
æå‡ºç”±ä¸¤ä¸ªååŒå¾ªç¯ç»„æˆçš„è·¨å¹³å°äº¤äº’å¼æ•°æ®ç®¡é“ã€‚â€œä»£ç† - ç¯å¢ƒäº¤äº’å¾ªç¯â€ä½¿è‡ªåŠ¨åŒ–ä»£ç†èƒ½å¤Ÿä¸å„ç§GUIç¯å¢ƒäº¤äº’ï¼Œâ€œä»£ç† - äººç±»æ··åˆæ•°æ®é‡‡é›†å¾ªç¯â€æ•´åˆä¸“å®¶æ³¨é‡Šçš„è½¨è¿¹ä»¥ç¡®ä¿è¦†ç›–èŒƒå›´å’Œè´¨é‡ã€‚è¯¥ç®¡é“æ¶µç›–Windowsã€macOSã€Linuxã€Androidã€iOSå’ŒWebå…­å¤§å¹³å°ï¼Œä¾¿äºæ”¶é›†ä¸°å¯Œçš„å±å¹•çŠ¶æ€è§‚å¯Ÿã€å…ƒæ•°æ®å’ŒåŸå§‹è½¨è¿¹ï¼Œå¹¶è®¾è®¡ç»Ÿä¸€åŠ¨ä½œç©ºé—´ï¼Œä»¥å®ç°ä¸ç°å®ç¯å¢ƒæ›´ä¸€è‡´å’Œé«˜æ•ˆçš„äº¤äº’ã€‚åœ¨æ­¤åŸºç¡€ä¸Šç²¾å¿ƒç­–åˆ’å’Œæ³¨é‡Šäº†æ¶µç›–GUIç†è§£ï¼ˆ471Kç¤ºä¾‹ï¼‰ã€GUIå®šä½ï¼ˆ1710ä¸‡è®­ç»ƒæ ·æœ¬ï¼‰å’Œä»»åŠ¡å®Œæˆï¼ˆè¶…1.5ä¸‡å¼±è¯­ä¹‰è½¨è¿¹å’Œ4Ké«˜çº§ç›®æ ‡å¯¼å‘è½¨è¿¹ï¼‰ä¸‰å¤§ä»»åŠ¡æ—çš„å¼€æ”¾è®¡ç®—æœºä½¿ç”¨æ•°æ®é›†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒScaleCUAåŸºç¡€ä»£ç†æ¨¡å‹æ—
åŸºäºä¸Šè¿°è¯­æ–™åº“ï¼Œä½¿ç”¨Qwen2.5 - VLè®­ç»ƒäº†ä¸€ç³»åˆ—åä¸ºScaleCUAçš„åŸºç¡€ä»£ç†æ¨¡å‹ã€‚ScaleCUAæ”¯æŒä¸‰ç§ä¸åŒçš„æ¨ç†èŒƒå¼ï¼šå®šä½æ¨¡å¼ä¸“æ³¨äºåŸºäºæ–‡æœ¬æè¿°ç²¾ç¡®å®šä½UIå…ƒç´ ï¼Œä¾¿äºä¸æ›´å¼ºå¤§çš„æ¨ç†è§„åˆ’å™¨é›†æˆï¼›ç›´æ¥åŠ¨ä½œæ¨¡å¼å¯ç›´æ¥ç”Ÿæˆä½çº§å¯æ‰§è¡ŒåŠ¨ä½œï¼Œé«˜æ•ˆå®Œæˆä»»åŠ¡ï¼›æ¨ç†åŠ¨ä½œæ¨¡å¼å…ˆåŸºäºå½“å‰è§‚å¯Ÿå’Œå†å²ä¸Šä¸‹æ–‡ç”Ÿæˆæ€ç»´è¿‡ç¨‹ï¼Œå†ç”Ÿæˆåç»­åŠ¨ä½œï¼Œæé«˜ä»»åŠ¡è§„åˆ’å‡†ç¡®æ€§ã€‚ç»Ÿä¸€çš„åŠ¨ä½œç©ºé—´è®¾è®¡ä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡æ ‡å‡†åŒ–æ§åˆ¶æ¨¡å¼ä¸å¼‚æ„ç¯å¢ƒäº¤äº’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªä»¥GUIä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒScaleCUAè¡¨ç°å‡ºè‰²ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨WebArena - Lite - v2ä¸Šæ¯”åŸºçº¿æé«˜äº†26.6ï¼Œåœ¨ScreenSpot - Proä¸Šæé«˜äº†10.7ï¼›åœ¨MMBench - GUI L1 - Hardä¸Šè¾¾åˆ°94.4%ï¼Œåœ¨OSWorld - Gä¸Šè¾¾åˆ°60.6%ï¼Œåœ¨WebArena - Lite - v2ä¸Šè¾¾åˆ°47.4%ï¼Œå–å¾—äº†æ–°çš„å½“å‰æœ€ä¼˜ç»“æœã€‚å®éªŒè¿˜ç ”ç©¶äº†ä¸åŒæ•°æ®æºã€è®­ç»ƒä»»åŠ¡å’Œä»£ç†è®¾è®¡ç­‰å¯¹ä»£ç†æ€§èƒ½çš„å½±å“ï¼Œçªå‡ºäº†æ•°æ®å¢å¼ºã€å¼±è¯­ä¹‰è½¨è¿¹å’Œä¸€èˆ¬æ¨ç†æ•°æ®å¯¹å¢å¼ºè§„åˆ’èƒ½åŠ›çš„ç›Šå¤„ï¼Œå¹¶å¯¹å„ç§ä»£ç†èŒƒå¼è¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨ä»£ç†å·¥ä½œæµç¨‹å’ŒåŸç”Ÿæ¨¡å‹ä¹‹é—´è¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - **æ•°æ®æ„å»ºä¸æ ‡æ³¨æ–¹æ³•**ï¼šé€šè¿‡ç»“åˆè‡ªåŠ¨åŒ–ä»£ç†å’Œäººç±»ä¸“å®¶æ„å»ºè·¨å¹³å°äº¤äº’å¼æ•°æ®ç®¡é“çš„æ–¹å¼ï¼Œä¸ºæ”¶é›†å¤§è§„æ¨¡ã€é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è®¡ç®—æœºä½¿ç”¨æ•°æ®æä¾›äº†å¯å€Ÿé‰´çš„æµç¨‹ï¼Œå°¤å…¶æ˜¯ç»Ÿä¸€åŠ¨ä½œç©ºé—´çš„è®¾è®¡ï¼Œæœ‰åˆ©äºä¸ä¸åŒç¯å¢ƒçš„äº¤äº’ã€‚
 - **æ¨¡å‹è®¾è®¡æ€è·¯**ï¼šScaleCUAå°†æ„ŸçŸ¥ã€æ¨ç†å’ŒåŠ¨ä½œç»Ÿä¸€åˆ°å•ä¸ªæ¨¡å‹ä¸­ï¼Œå¹¶æ”¯æŒå¤šç§çµæ´»æ¨ç†èŒƒå¼ï¼Œè¿™ç§è®¾è®¡ä¸ºå¼€å‘é€šç”¨ä¸”åŠŸèƒ½å¼ºå¤§çš„è®¡ç®—æœºä½¿ç”¨ä»£ç†æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚
 - **å®éªŒè¯„ä¼°ç­–ç•¥**ï¼šå…¨é¢çš„å®éªŒè¯„ä¼°æ¶µç›–äº†ç†è§£ã€å®šä½å’Œç«¯åˆ°ç«¯ä»»åŠ¡å®Œæˆç­‰å¤šä¸ªæ–¹é¢ï¼Œåœ¨å¤šä¸ªè·¨å¹³å°åœ¨çº¿åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæµ‹è¯•å¹¶åˆ†æä¸åŒå› ç´ å¯¹æ€§èƒ½çš„å½±å“ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ç³»ç»Ÿçš„è¯„ä¼°ç­–ç•¥å‚è€ƒã€‚
``` 

## agentracer--who-is-inducing-failure-in-the-llm-agentic-systems-
### Abstract
Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | AgenTracerï¼šæ­ç§˜å¤§è¯­è¨€æ¨¡å‹ä»£ç†ç³»ç»Ÿä¸­çš„å¤±è´¥è¯±å› 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†ç³»ç»Ÿé€šå¸¸ç”±å¤šä¸ªæ¨¡å‹ã€å¤æ‚çš„å·¥å…·è°ƒç”¨å’Œç¼–æ’åè®®ç»„æˆï¼Œå…¶æ€§èƒ½å¤§å¹…è¶…è¶Šå•ä½“ä»£ç†ã€‚ç„¶è€Œï¼Œè¿™ç§å¤æ‚æ€§ä¹Ÿæ”¾å¤§äº†å®ƒä»¬çš„è„†å¼±æ€§ï¼Œä½¿å…¶æ›´å®¹æ˜“å‡ºç°ç³»ç»Ÿæ•…éšœã€‚åœ¨é•¿æ‰§è¡Œè½¨è¿¹ä¸­ç²¾ç¡®å®šä½å¯¼è‡´é”™è¯¯çš„ç‰¹å®šä»£ç†æˆ–æ­¥éª¤ï¼Œå³ä»£ç†ç³»ç»Ÿæ•…éšœå½’å› ä»»åŠ¡ã€‚ä½†å½“å‰æœ€å…ˆè¿›çš„æ¨ç†LLMåœ¨åº”å¯¹è¿™ä¸€æŒ‘æˆ˜æ—¶è¡¨ç°æ¬ ä½³ï¼Œå‡†ç¡®ç‡é€šå¸¸ä½äº10%ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAgenTracerï¼Œè¿™æ˜¯é¦–ä¸ªé€šè¿‡åäº‹å®é‡æ”¾å’Œç¼–ç¨‹æ•…éšœæ³¨å…¥æ¥æ³¨é‡Šå¤±è´¥çš„å¤šä»£ç†è½¨è¿¹çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”Ÿæˆäº†ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†TracerTrajã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨TracerTrajå¼€å‘äº†AgenTracer - 8Bï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¤šç²’åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è½»é‡çº§æ•…éšœè¿½è¸ªå™¨ï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯Šæ–­å†—é•¿çš„å¤šä»£ç†äº¤äº’ä¸­çš„é”™è¯¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Who&WhenåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgenTracer - 8Bçš„è¡¨ç°æ¯”Gemini - 2.5 - Proå’ŒClaude - 4 - Sonnetç­‰å¤§å‹ä¸“æœ‰LLMé«˜å‡º18.18%ï¼Œä¸ºLLMä»£ç†æ•…éšœå½’å› è®¾å®šäº†æ–°æ ‡å‡†ã€‚æ­¤å¤–ï¼ŒAgenTracer - 8Bä¸ºMetaGPTå’ŒMaASç­‰ç°æˆçš„å¤šä»£ç†ç³»ç»Ÿæä¾›äº†å¯æ“ä½œçš„åé¦ˆï¼Œæ€§èƒ½æå‡äº†4.8 - 14.2%ï¼Œèµ‹èƒ½äº†å…·æœ‰è‡ªæˆ‘çº æ­£å’Œè‡ªæˆ‘è¿›åŒ–èƒ½åŠ›çš„ä»£ç†AIã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„è‡ªåŠ¨åŒ–æ•…éšœå½’å› æ¡†æ¶å’Œè½»é‡çº§æ•…éšœè¿½è¸ªå™¨çš„æ€è·¯ï¼Œä¸ºè§£å†³å¤æ‚çš„å¤šä»£ç†ç³»ç»Ÿæ•…éšœè¯Šæ–­é—®é¢˜æä¾›äº†æ–°çš„æ–¹å‘ã€‚å¤šç²’åº¦å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ–¹å¼ä¹Ÿä¸ºæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½æä¾›äº†å€Ÿé‰´ï¼ŒåŒæ—¶å…¶åœ¨å®é™…å¤šä»£ç†ç³»ç»Ÿä¸­æå‡æ€§èƒ½çš„å®è·µï¼Œå¯¹äºæ¨åŠ¨ä»£ç†AIçš„è‡ªæˆ‘å®Œå–„å…·æœ‰å‚è€ƒä»·å€¼ã€‚
```

## insta--towards-internet-scale-training-for-agents
### Abstract
The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and reaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code, models and data at: https://data-for-agents.github.io.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | InSTAï¼šå¼€å¯äº’è”ç½‘è§„æ¨¡çš„æ™ºèƒ½ä½“è®­ç»ƒæ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç½‘ç»œå¯¼èˆªæ™ºèƒ½ä½“çš„ä¸»æµæ–¹æ³•æ˜¯æ”¶é›†äººç±»åœ¨ä¸€ç»„æ‰‹åŠ¨ç­–åˆ’çš„ç½‘ç«™å’Œä»»åŠ¡ä¸Šçš„æ¼”ç¤ºæ•°æ®ã€‚ç„¶è€Œï¼Œäººç±»æ•°æ®æ”¶é›†æ—¢è´¹åŠ›åˆæ˜‚è´µï¼Œä¸”éšç€ç”¨æˆ·å¯¹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æŠ€èƒ½è¦æ±‚çš„å¢åŠ ï¼Œæ‰©å±•æˆæœ¬ä¹Ÿè¶Šæ¥è¶Šé«˜ã€‚æ®The Common Crawl Foundationç»Ÿè®¡ï¼Œè¥¿æ–¹äº’è”ç½‘ä¸Šæœ‰è¶…è¿‡3äº¿ä¸ªç½‘ç«™ï¼Œä½†ç ”ç©¶äººå‘˜æ ‡æ³¨çš„ç½‘ç«™åªæ˜¯å…¶ä¸­æå°ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”ç°æœ‰çš„äººç±»æ•°æ®æ˜¯é™æ€çš„ã€‚å› æ­¤ï¼Œåœ¨åŠ¨æ€çš„äº’è”ç½‘è§„æ¨¡ç¯å¢ƒä¸­ï¼Œè‡ªåŠ¨åŒ–è®­ç»ƒä¸‹ä¸€ä»£è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ç®¡é“éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå‡å°‘å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºInSTAè‡ªåŠ¨åŒ–ç®¡é“
è¯¥ç®¡é“åŒ…å«ä¸‰ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹ä»»åŠ¡æè®®å™¨ä¸º15ä¸‡ä¸ªç½‘ç«™æ ‡æ³¨å®æ—¶ç½‘ç»œå¯¼èˆªä»»åŠ¡ï¼Œä»äº’è”ç½‘ä¸Šæœ€å—æ¬¢è¿çš„100ä¸‡ä¸ªç½‘ç«™å¼€å§‹ç­›é€‰ï¼Œæœ€ç»ˆå¾—åˆ°15ä¸‡ä¸ªå†…å®¹å®‰å…¨çš„ç½‘ç«™ï¼Œå¹¶ä¸ºå…¶ç”Ÿæˆä»»åŠ¡ï¼ŒåŒæ—¶ä»»åŠ¡æè®®å™¨èƒ½ä»¥97%çš„å‡†ç¡®ç‡æ£€æµ‹æœ‰å®³å†…å®¹ã€‚ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„LLMä½œä¸ºæ™ºèƒ½ä½“å®Œæˆä»»åŠ¡å¹¶ç”Ÿæˆè½¨è¿¹ã€‚ç¬¬ä¸‰é˜¶æ®µï¼Œç”±LLMä½œä¸ºè£åˆ¤å¯¹è½¨è¿¹è¿›è¡Œç­›é€‰ï¼Œåˆ¤æ–­å…¶æ˜¯å¦æˆåŠŸï¼Œå¹¶åœ¨0 - 1çš„è¿ç»­å°ºåº¦ä¸Šå¯¹æ™ºèƒ½ä½“è¿›è¡Œè¯„åˆ†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°†LLMç”¨ä½œæ•°æ®ç®¡ç†å·¥å…·
é€šè¿‡è¿™ç§æ–¹å¼åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨ç†æ•°æ®é›†ï¼ŒåŒ…æ‹¬220ä¸‡å¼ æˆªå›¾ã€220ä¸‡æ¡åŠ¨ä½œè½¨è¿¹å’Œ15ä¸‡æ¡è£åˆ¤è½¨è¿¹ã€‚å¹¶ä¸”åœ¨ä¸åŒè§„æ¨¡çš„InSTAç®¡é“æ•°æ®ä¸Šè®­ç»ƒåŸºäºQwen 3 1.7Bçš„ä¸€ç³»åˆ—æ¨¡å‹ï¼Œä»¥è¾ƒå°çš„é¢„ç®—è¾¾åˆ°äº†å‰æ²¿LLMæ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®­ç»ƒçš„é¡¶çº§æ™ºèƒ½ä½“æˆåŠŸç‡è¾¾åˆ°56.9%ï¼Œè¶…è¿‡äº†æ•°æ®æ”¶é›†ç­–ç•¥Qwen 3 235Bå’Œæ¯”å…¶å¤§235å€çš„Llama 4 Maverickï¼Œè¾¾åˆ°äº†Gemini 2.5 Flashæ€§èƒ½çš„94.7%ã€‚è¯æ˜äº†InSTAç®¡é“ç”Ÿæˆçš„æ•°æ®åœ¨å°å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä½“æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°é¢„ç®—ä¸‹ä½¿å°å‹æ¨¡å‹è¾¾åˆ°å‰æ²¿LLMæ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ•°æ®è‡ªåŠ¨åŒ–ç”Ÿæˆä¸ç®¡ç†**ï¼šåˆ©ç”¨LLMå®ç°ä»»åŠ¡ç”Ÿæˆã€æ™ºèƒ½ä½“æ‰§è¡Œå’Œæ•°æ®ç­›é€‰çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œå‡å°‘å¯¹äººç±»æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä¸ºè§£å†³æ•°æ®æ”¶é›†éš¾é¢˜æä¾›äº†æ–°æ€è·¯ã€‚
2. **å°å‹æ¨¡å‹çš„æ½œåŠ›æŒ–æ˜**ï¼šé€šè¿‡InSTAç®¡é“ç”Ÿæˆçš„æ•°æ®ï¼Œåœ¨å°å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†ä¸å‰æ²¿LLMæ™ºèƒ½ä½“ç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºèµ„æºæœ‰é™çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å€Ÿé‰´ï¼Œè¡¨æ˜å°å‹æ¨¡å‹åœ¨åˆé€‚çš„æ•°æ®å’Œè®­ç»ƒæ–¹æ³•ä¸‹ä¹Ÿèƒ½å‘æŒ¥å¼ºå¤§ä½œç”¨ã€‚
3. **äº’è”ç½‘è§„æ¨¡çš„æ•°æ®é£è½®**ï¼šæ„å»ºäº’è”ç½‘è§„æ¨¡çš„æ•°æ®é£è½®ç”¨äºè®­ç»ƒLLMæ™ºèƒ½ä½“ï¼Œä¸ºæ¨åŠ¨è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨çœŸå®äº’è”ç½‘ç¯å¢ƒä¸­çš„å‘å±•æä¾›äº†æ–°çš„æ–¹å‘å’Œå®è·µç»éªŒã€‚
``` 

## kimi-dev--agentless-training-as-skill-prior-for-swe-agents
### Abstract
Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Kimi-Devï¼šæ— ä»£ç†è®­ç»ƒå¼€å¯è½¯ä»¶å·¥ç¨‹å¤§æ¨¡å‹æ–°ç¯‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰é¢†åŸŸï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒSWE - benchæˆä¸ºå…³é”®åŸºå‡†ã€‚ç›®å‰é’ˆå¯¹SWE - benchä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šå…·æœ‰å¤šè½®äº¤äº’çš„SWE - Agentæ¡†æ¶å’ŒåŸºäºå·¥ä½œæµçš„å•æ­¥å¯éªŒè¯çš„æ— ä»£ç†ï¼ˆAgentlessï¼‰æ–¹æ³•ã€‚ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºè¿™ä¸¤ç§èŒƒå¼ç›¸äº’æ’æ–¥ï¼ŒSWE - Agentsè™½æ½œåŠ›é«˜ã€é€‚åº”æ€§å¥½ï¼Œä½†å› å…¶ç«¯åˆ°ç«¯ç‰¹æ€§è®­ç»ƒéš¾åº¦å¤§ï¼›Agentlessæ–¹æ³•æ¨¡å—åŒ–å¥½ã€ä¾¿äºç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯è®­ç»ƒï¼Œä½†æ¢ç´¢ç©ºé—´å’Œçµæ´»æ€§æœ‰é™ï¼Œè¡Œä¸ºç›‘æµ‹å›°éš¾ã€‚æœ¬æ–‡ä»è®­ç»ƒæ–¹æ³•çš„è§’åº¦æŒ‘æˆ˜è¿™ç§äºŒåˆ†æ³•ï¼Œè®¤ä¸ºAgentlessè®­ç»ƒä¸åº”æ˜¯æœ€ç»ˆæˆæœï¼Œè€Œæ˜¯è¯±å¯¼æŠ€èƒ½å…ˆéªŒçš„ä¸€ç§æ–¹å¼ï¼Œä»¥åŠ©åŠ›æ›´å¼ºå¤§ã€æ›´å…·é€šç”¨æ€§çš„SWE - Agentsçš„é«˜æ•ˆé€‚é…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç²¾å¿ƒåˆ¶å®šAgentlessè®­ç»ƒæ–¹æ³•
å¼€å‘äº†ä¸€å¥—åŒ…å«è®­ç»ƒä¸­æœŸã€å†·å¯åŠ¨ã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶è‡ªåšå¼ˆçš„Agentlessè®­ç»ƒæ–¹æ³•ï¼Œå¹¶åŸºäºæ­¤æ¨å‡ºå¼€æºSWE LLMâ€”â€”Kimi - Devã€‚è¯¥æ–¹æ³•ä½¿Kimi - Devåœ¨SWE - bench Verifiedä¸Šè¾¾åˆ°60.4%çš„å‡†ç¡®ç‡ï¼Œåœ¨åŸºäºå·¥ä½œæµçš„è§£å†³æ–¹æ¡ˆä¸­è¡¨ç°æœ€ä½³ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéªŒè¯æŠ€èƒ½å…ˆéªŒçš„è¿ç§»ä¸æ³›åŒ–
é€šè¿‡å®éªŒè¡¨æ˜Agentlessè®­ç»ƒèƒ½è¯±å¯¼æŠ€èƒ½å…ˆéªŒï¼ŒKimi - Devåœ¨5kå…¬å¼€å¯ç”¨è½¨è¿¹ä¸Šè¿›è¡Œé¢å¤–çš„SFTé€‚é…åï¼ŒåŠ©åŠ›SWE - Agentsè¾¾åˆ°48.6%çš„pass@1åˆ†æ•°ï¼Œä¸Claude 3.5 Sonnetï¼ˆ20241022ç‰ˆæœ¬ï¼‰ç›¸å½“ã€‚åŒæ—¶ï¼Œè¿™äº›è¯±å¯¼çš„æŠ€èƒ½èƒ½å¤Ÿä»éä»£ç†å·¥ä½œæµè¿ç§»åˆ°ä»£ç†æ¡†æ¶ï¼Œä¸”é€šè¿‡Agentlessè®­ç»ƒèå…¥çš„é•¿æ€ç»´é“¾ä¸­çš„è‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œä½¿ä»£ç†æ¨¡å‹èƒ½åˆ©ç”¨æ›´å¤šè½®äº¤äº’ï¼Œåœ¨æ›´é•¿çš„æ—¶é—´èŒƒå›´å†…å–å¾—æˆåŠŸã€‚æ­¤å¤–ï¼Œè¿™äº›æŠ€èƒ½è¿˜èƒ½ä»SWE - bench Verifiedæ³›åŒ–åˆ°æ›´å¹¿æ³›çš„åŸºå‡†ï¼Œå¦‚SWE - bench - liveå’ŒSWE - bench Multilingualã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Kimi - Devåœ¨SWE - bench Verifiedä¸Šå®ç°äº†60.4%çš„å‡†ç¡®ç‡ï¼Œåœ¨åŸºäºå·¥ä½œæµçš„æ–¹æ³•ä¸­å¤„äºé¢†å…ˆåœ°ä½ã€‚ç»è¿‡SFTé€‚é…åï¼ŒKimi - DevåŠ©åŠ›SWE - Agentsçš„pass@1åˆ†æ•°è¾¾åˆ°48.6%ï¼Œä¸Claude 3.5 Sonnetï¼ˆ20241022ç‰ˆæœ¬ï¼‰è¡¨ç°ç›¸å½“ã€‚åŒæ—¶ï¼Œå®éªŒéªŒè¯äº†Agentlessè®­ç»ƒè¯±å¯¼çš„æŠ€èƒ½åœ¨ä¸åŒåŸºå‡†ä¸Šçš„è¿ç§»æ€§å’Œæ³›åŒ–æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»è®­ç»ƒæ–¹æ³•çš„è§’åº¦æ‰“ç ´ä¼ ç»ŸèŒƒå¼äºŒåˆ†æ³•çš„å±€é™ï¼Œä¸ºæ„å»ºå¯è¿ç§»çš„ç¼–ç LLMsæä¾›äº†æ–°çš„æ€è·¯ï¼Œå³é€šè¿‡ç»“æ„åŒ–æŠ€èƒ½å…ˆéªŒçš„è®­ç»ƒæ¥æ”¯æ’‘è‡ªä¸»çš„ä»£ç†äº¤äº’ã€‚è¿™ç§å°†Agentlessè®­ç»ƒè§†ä¸ºæŠ€èƒ½å…ˆéªŒè¯±å¯¼æ–¹å¼çš„ç†å¿µï¼Œæœ‰æœ›ä¸ºå…¶ä»–ç›¸å…³é¢†åŸŸçš„æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–æä¾›å€Ÿé‰´ï¼Œä¿ƒè¿›ä¸åŒè®­ç»ƒèŒƒå¼é—´çš„èåˆä¸äº’è¡¥ï¼Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹åŠæ›´å¤šé¢†åŸŸçš„åº”ç”¨å’Œå‘å±•ã€‚
``` 

## estimating-the-empowerment-of-language-model-agents
### Abstract
As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent's actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨èµ‹æƒè¯„ä¼°è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œå¼€å¯è¯„ä¼°æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ™ºèƒ½ä½“èƒ½åŠ›ä¸æ–­å¢å¼ºï¼Œä¸”èƒ½æ›´å¹¿æ³›åœ°ä½¿ç”¨ç°å®ä¸–ç•Œå·¥å…·ï¼Œå¯¹å¯æ‰©å±•çš„æ™ºèƒ½ä½“èƒ½åŠ›è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿä»¥åŸºå‡†ä¸ºä¸­å¿ƒçš„è¯„ä¼°è®¾è®¡æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦äººç±»è®¾è®¡è€…æå‡ºæœ‰æ•ˆçš„ä»»åŠ¡ï¼Œæ‰èƒ½æ´å¯Ÿæ¨¡å‹çš„ä¸€èˆ¬èƒ½åŠ›ã€‚å¹¶ä¸”ï¼Œä¼ ç»Ÿè¯„ä¼°å¾ˆå°‘è€ƒè™‘æ™ºèƒ½ä½“äº¤äº’çš„åŠ¨æ€å’Œå¼€æ”¾æ€§æœ¬è´¨ï¼Œéš¾ä»¥æ£€æµ‹æ™ºèƒ½ä½“åœ¨æµ‹é‡èŒƒå›´ä¹‹å¤–è¿½æ±‚ç›®æ ‡çš„èƒ½åŠ›ï¼Œè¿™åœ¨äººå·¥æ™ºèƒ½å®‰å…¨æ–¹é¢å­˜åœ¨éšæ‚£ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè®ºæ–‡æå‡ºåˆ©ç”¨èµ‹æƒï¼ˆempowermentï¼‰è¿™ä¸€ä¿¡æ¯è®ºåº¦é‡æ¥é‡åŒ–LMæ™ºèƒ½ä½“çš„èƒ½åŠ›ï¼Œæ— éœ€æŒ‡å®šç›®æ ‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEELMAç®—æ³•
å¼€å‘äº†ä¸€ç§æ–°é¢–çš„ä¿¡æ¯è®ºä¼°è®¡å™¨EELMAï¼ˆEstimating Empowerment of Language Model Agentsï¼‰ï¼Œå®ƒæ˜¯é¦–ä¸ªèƒ½ç›´æ¥ä»å¤šè½®åŸºäºæ–‡æœ¬çš„äº¤äº’ä¸­è¿‘ä¼¼æœ‰æ•ˆèµ‹æƒçš„æ–¹æ³•ï¼Œå¯å®ç°å¯¹LMæ™ºèƒ½ä½“èƒ½åŠ›çš„å¯æ‰©å±•ã€æ— ç›®æ ‡æµ‹é‡ï¼Œæ— éœ€æ˜ç¡®çš„ä»»åŠ¡è§„èŒƒæˆ–å¥–åŠ±å‡½æ•°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®ç°æ— ç›®æ ‡è¯„ä¼°
é€šè¿‡EELMAæµ‹é‡çš„æœ‰æ•ˆèµ‹æƒï¼Œèƒ½å¤Ÿåœ¨ç®€å•ç¯å¢ƒå’Œå¤§è§„æ¨¡ç½‘é¡µæµè§ˆä»»åŠ¡ä¸­å¯¹LMæ™ºèƒ½ä½“çš„èƒ½åŠ›è¿›è¡Œæ— ç›®æ ‡è¯„ä¼°ï¼Œå±•ç¤ºäº†èµ‹æƒä½œä¸ºè¯„ä¼°LMæ™ºèƒ½ä½“èƒ½åŠ›çš„æœ‰æ•ˆæŒ‡æ ‡çš„æ½œåŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆ†ææ™ºèƒ½ä½“å­ç³»ç»Ÿå½±å“
åˆ†æäº†LMæ™ºèƒ½ä½“çš„å­ç³»ç»Ÿï¼Œå¦‚æ€ç»´é“¾ï¼ˆchain - of - thoughtï¼‰ã€å†…å­˜å®¹é‡å’Œä¸»å¹²LLMæ¶æ„ç­‰å¦‚ä½•æ”¹å˜æ™ºèƒ½ä½“çš„æœ‰æ•ˆèµ‹æƒï¼Œæ·±å…¥æ¢ç©¶äº†å½±å“æ™ºèƒ½ä½“èµ‹æƒçš„å› ç´ ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæä¾›å¼‚å¸¸è¡Œä¸ºç›‘æµ‹æœºåˆ¶
è¯æ˜èµ‹æƒèƒ½å¤Ÿåœ¨æ— éœ€äººå·¥æ³¨é‡Šçš„æƒ…å†µä¸‹çªå‡ºè½¨è¿¹ä¸­å…·æœ‰é«˜åº¦å½±å“åŠ›çš„çŠ¶æ€ï¼Œä¸ºå¼€æ”¾å¼ç›‘æµ‹å¼‚å¸¸è¡Œä¸ºæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æœºåˆ¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç»“æ„åŒ–æ¸¸æˆï¼ˆç½‘æ ¼ä¸–ç•Œå’Œæ±‰è¯ºå¡”ï¼‰ä»¥åŠå¤§è§„æ¨¡ç½‘é¡µæµè§ˆæ²™ç›’ï¼ˆWebArenaï¼‰ä¸­å¯¹EELMAè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œèµ‹æƒä¼°è®¡ä¸å¹³å‡ä»»åŠ¡æ€§èƒ½å¯†åˆ‡ç›¸å…³ï¼›è½¨è¿¹ä¸­çš„é«˜èµ‹æƒæ—¶åˆ»æ­ç¤ºäº†æ™ºèƒ½ä½“è¿…é€Ÿæ‰©å¤§å¯¹ç¯å¢ƒæ§åˆ¶çš„å…³é”®ç‚¹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **è¯„ä¼°æ–¹æ³•åˆ›æ–°**ï¼šEELMAæä¾›äº†ä¸€ç§å…¨æ–°çš„è¯„ä¼°è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“èƒ½åŠ›çš„è§†è§’å’Œæ–¹æ³•ï¼Œæ‘†è„±äº†ä¼ ç»Ÿä¾èµ–ç‰¹å®šä»»åŠ¡çš„è¯„ä¼°æ–¹å¼ï¼Œä¸ºæ™ºèƒ½ä½“èƒ½åŠ›è¯„ä¼°æä¾›äº†æ–°æ€è·¯ï¼Œåœ¨å…¶ä»–ç±»ä¼¼çš„æ™ºèƒ½ä½“è¯„ä¼°åœºæ™¯ä¸­å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
2. **å¼‚å¸¸è¡Œä¸ºç›‘æµ‹**ï¼šåˆ©ç”¨èµ‹æƒçªå‡ºè½¨è¿¹ä¸­çš„å…³é”®çŠ¶æ€ä»¥ç›‘æµ‹å¼‚å¸¸è¡Œä¸ºçš„æ–¹å¼ï¼Œä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨ç›‘æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æœºåˆ¶ï¼Œå¯åº”ç”¨äºä¿éšœäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨è¿è¡Œã€‚
3. **å­ç³»ç»Ÿåˆ†ææ€è·¯**ï¼šå¯¹æ™ºèƒ½ä½“å­ç³»ç»Ÿå¦‚ä½•å½±å“èµ‹æƒçš„åˆ†æï¼Œæœ‰åŠ©äºæ·±å…¥ç†è§£è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„å·¥ä½œæœºåˆ¶ï¼Œä¸ºä¼˜åŒ–æ™ºèƒ½ä½“è®¾è®¡æä¾›äº†æ–¹å‘ï¼Œåœ¨æ™ºèƒ½ä½“æ¶æ„è®¾è®¡å’Œæ”¹è¿›æ–¹é¢å…·æœ‰å‚è€ƒä»·å€¼ã€‚
``` 

## efficient-and-transferable-agentic-knowledge-graph-rag-via-reinforcement-learning
### Abstract
Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ åŠ©åŠ›çŸ¥è¯†å›¾è°±RAGï¼šé«˜æ•ˆä¸”å¯è¿ç§»çš„æ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG - RAGï¼‰å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç»“æ„åŒ–ã€å¯éªŒè¯çš„çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ç›¸ç»“åˆï¼Œä»¥å‡å°‘å¹»è§‰å¹¶æš´éœ²æ¨ç†ç—•è¿¹ã€‚ç„¶è€Œï¼Œè®¸å¤šKG - RAGç³»ç»Ÿç”±å¤šä¸ªLLMæ¨¡å—ï¼ˆå¦‚è§„åˆ’ã€æ¨ç†å’Œå“åº”ï¼‰ç»„æˆï¼Œè¿™ä¸ä»…å¢åŠ äº†æ¨ç†æˆæœ¬ï¼Œè¿˜å°†ç³»ç»Ÿè¡Œä¸ºç»‘å®šåˆ°ç‰¹å®šçš„ç›®æ ‡KGä¸Šã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•åå¤è°ƒç”¨å¤§å‹åŸºç¡€LLMsä¼šå¯¼è‡´æ¨ç†æˆæœ¬å¢åŠ ï¼Œå¹¶ä¸”è¿™äº›ç»è¿‡æç¤ºæˆ–å¾®è°ƒçš„æ¨¡å—é€šå¸¸é’ˆå¯¹ç‰¹å®šKGçš„é¢†åŸŸå’Œæ¨¡å¼è¿›è¡Œè°ƒæ•´ï¼Œåœ¨é¢†åŸŸè½¬ç§»ã€æ¨¡å¼å˜åŒ–æˆ–éƒ¨ç½²åœ¨æ–°KGä¸Šæ—¶ï¼Œæ€§èƒ½æ— æ³•å¯é è¿ç§»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†KG - R1ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šKG - R1æ¡†æ¶
å¼•å…¥äº†ä¸€ç§æ™ºèƒ½ä½“å¼çš„KG - RAGç³»ç»Ÿï¼Œç”¨å•ä¸ªæ™ºèƒ½ä½“å–ä»£å¤šæ¨¡å—ç®¡é“ï¼Œåœ¨è½»é‡çº§KGæœåŠ¡å™¨ä¸Šè¿è¡Œã€‚è¯¥æ™ºèƒ½ä½“åœ¨å¤šä¸ªå›åˆä¸­äº¤æ›¿è¿›è¡Œæ¨ç†å’Œæ£€ç´¢æ“ä½œï¼Œå…¶ç«¯åˆ°ç«¯çš„è½¨è¿¹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œä½¿ç”¨åŸºäºå›åˆå’ŒåŸºäºç»“æœçš„å¥–åŠ±ä¿¡å·ã€‚åŸºäºå›åˆçš„å¥–åŠ±è¯„ä¼°å•ä¸ªåŠ¨ä½œçš„æœ‰æ•ˆæ€§å’Œæ ¼å¼éµå¾ªæƒ…å†µï¼Œè€Œå…¨å±€å¥–åŠ±åˆ™è¡¡é‡ç­”æ¡ˆè´¨é‡å’Œæ£€ç´¢ç›¸å…³æ€§ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆæ¨ç†
é€šè¿‡å°†æ¨ç†å’Œæ£€ç´¢æ•´åˆåˆ°å•ä¸ªæ™ºèƒ½ä½“çš„è¿‘è¿ç»­å·¥ä½œæµç¨‹ä¸­ï¼ŒKG - R1ä½¿ç”¨å°å‚æ•°æ¨¡å‹å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ¨ç†å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨é‡ï¼Œé™ä½äº†å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ï¼Œä½¿å¾—åœ¨é¢„ç®—ç´§å¼ çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œéƒ¨ç½²ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰åŸºå‡†æµ‹è¯•çš„æ§åˆ¶å®éªŒä¸­ï¼Œä½¿ç”¨Qwen - 2.5 - 3Bæ¨¡å‹æ—¶ï¼ŒKG - R1ä¸ä¹‹å‰ä½¿ç”¨æ›´å¤§åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹çš„å¤šæ¨¡å—å·¥ä½œæµç¨‹æ–¹æ³•ç›¸æ¯”ï¼Œä»¥æ›´å°‘çš„ç”Ÿæˆä»¤ç‰Œæé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒKG - R1å®ç°äº†å³æ’å³ç”¨ï¼Œåœ¨è®­ç»ƒåï¼Œæ— éœ€ä¿®æ”¹å³å¯åœ¨æ–°çš„KGä¸Šä¿æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œåœ¨è·¨KGçš„å¯è¿ç§»æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶å®ç°äº†ä½ä»¤ç‰Œæˆæœ¬å’Œå¼ºè·¨KGå¯è¿ç§»æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºå¸Œæœ›åœ¨KG - RAGé¢†åŸŸå®ç°é«˜æ•ˆä¸”å¯è¿ç§»ç³»ç»Ÿçš„ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¥è¯´ï¼ŒKG - R1æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶å•æ™ºèƒ½ä½“ç»“åˆRLçš„æ–¹å¼ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶æå‡äº†ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™ç§ä¼˜åŒ–æ–¹å¼å¯åº”ç”¨äºå¤šç§æ¶‰åŠçŸ¥è¯†å›¾è°±ä¸å¤§è¯­è¨€æ¨¡å‹ç»“åˆçš„åœºæ™¯ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ¨èç³»ç»Ÿç­‰ã€‚æ­¤å¤–ï¼Œå…¶å³æ’å³ç”¨çš„ç‰¹æ€§ä¸ºå®é™…åº”ç”¨ä¸­çš„å¿«é€Ÿéƒ¨ç½²å’Œé€‚åº”ä¸åŒKGæä¾›äº†ä¾¿åˆ©ï¼Œåœ¨å®é™…é¡¹ç›®ä¸­å…·æœ‰è¾ƒé«˜çš„å‚è€ƒä»·å€¼ã€‚
``` 

## agentrl--scaling-agentic-reinforcement-learning-with-a-multi-turn--multi-task-framework
### Abstract
Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | AgentRLï¼šå¼€å¯å¤šè½®å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ–°å¢ƒç•Œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é£é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹æ„å»ºèƒ½å¤Ÿé€šè¿‡åœ¨çº¿äº¤äº’å­¦ä¹ çš„é€šç”¨æ™ºèƒ½ä½“çš„æµ“åšå…´è¶£ã€‚ç„¶è€Œï¼Œå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºåœ¨å¤šè½®ã€å¤šä»»åŠ¡è®¾ç½®ä¸­è®­ç»ƒLLMæ™ºèƒ½ä½“ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼Œç°æœ‰çš„RLåœ¨LLMä¸Šçš„åº”ç”¨å¤§å¤šå±€é™äºå•è½®å•ä»»åŠ¡è®¾ç½®ï¼Œè€Œå¤šè½®è®¾ç½®ä¸‹çš„æ™ºèƒ½ä½“ä»»åŠ¡éœ€è¦æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’æ”¶é›†åé¦ˆï¼Œè®­ç»ƒä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„LLMè¿›è¡Œå¤šè½®æ¨ç†ã€ä¸å·¥å…·æˆ–ç¯å¢ƒäº¤äº’å¹¶åœ¨æ‰©å±•è½¨è¿¹ä¸Šè°ƒæ•´è¡Œä¸ºï¼›å¦ä¸€æ–¹é¢ï¼Œæ„å»ºèƒ½å¤Ÿå¤„ç†å¤šæ ·åŒ–ä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“ä¸€ç›´æ˜¯RLçš„ç›®æ ‡ï¼Œä½†åœ¨å¤šè½®è®¾ç½®ä¸­æ‰©å±•åˆ°å¼‚æ„å¤šä»»åŠ¡ç¯å¢ƒï¼Œéœ€è¦åœ¨LLMè®­ç»ƒåŸºç¡€è®¾æ–½å’Œç®—æ³•è®¾è®¡ä¸Šå–å¾—è¿›å±•ï¼Œç›®å‰ç¼ºä¹å¯æ‰©å±•çš„åŸºç¡€è®¾æ–½å’Œç¨³å®šçš„è®­ç»ƒç®—æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1 **åŸºç¡€è®¾æ–½åˆ›æ–°**
æå‡ºäº†AgentRLæ¡†æ¶ï¼Œåœ¨åŸºç¡€è®¾æ–½æ–¹é¢ï¼Œå…·å¤‡å…¨å¼‚æ­¥ç”Ÿæˆ - è®­ç»ƒç®¡é“ï¼Œå¯æœ‰æ•ˆå‡å°‘GPUé—²ç½®æ—¶é—´ï¼Œæå‡å¤šè½®RLè®­ç»ƒæ•ˆç‡ã€‚åŒæ—¶ï¼Œä¸ºæ”¯æŒå¤šä»»åŠ¡RLä¸­çš„å¼‚æ„ç¯å¢ƒå¼€å‘ï¼Œè®¾è®¡äº†ç»Ÿä¸€çš„åŸºäºå‡½æ•°è°ƒç”¨çš„APIæ¥å£ã€å®¹å™¨åŒ–ç¯å¢ƒå¼€å‘ä»¥åŠé›†ä¸­æ§åˆ¶å™¨ï¼Œèƒ½å¤Ÿç®¡ç†æ•°åƒä¸ªå¹¶è¡Œè®­ç»ƒæƒ…èŠ‚çš„ç”Ÿå‘½å‘¨æœŸï¼Œå¹¶åœ¨æ§åˆ¶å™¨çº§åˆ«å¼•å…¥ä¸€è‡´æ¥å£ä»¥æ”¯æŒå¼‚æ„ç¯å¢ƒæ‰©å±•ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2 **ç®—æ³•åˆ›æ–°**
åœ¨ç®—æ³•æ–¹é¢ï¼Œæå‡ºäº†äº¤å‰ç­–ç•¥é‡‡æ ·ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å¤šè½®è®¾ç½®ä¸­è¿›è¡Œæ¢ç´¢ï¼Œä»¥åº”å¯¹å¤§çŠ¶æ€ç©ºé—´å¯¹æ¢ç´¢çš„è´Ÿé¢å½±å“ï¼›å¼•å…¥äº†ä»»åŠ¡ä¼˜åŠ¿å½’ä¸€åŒ–ï¼Œç¼“è§£ä¸åŒä»»åŠ¡çš„å¼‚è´¨æ€§å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å°†AgentRLåº”ç”¨äºå¼€æºLLMsï¼ˆQwen2.5å’ŒGLM - 4 - 9Bï¼‰ï¼Œåœ¨äº”ä¸ªæ™ºèƒ½ä½“ä»»åŠ¡ï¼ˆALFWorldã€DBã€KGã€OSå’ŒWebshopï¼‰ä¸Šè¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒAgentRLæ˜¾è‘—ä¼˜äºGPT - 5ã€Clause - Sonnet - 4ã€DeepSeek - R1ç­‰å¼€æºLLMæ™ºèƒ½ä½“ã€‚ä½¿ç”¨AgentRLè¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒçš„å•ä¸ªæ¨¡å‹å¯ä»¥è¾¾åˆ°ä¸ä¸ºå•ä¸ªä»»åŠ¡åˆ†åˆ«è®­ç»ƒçš„äº”ä¸ªæ¨¡å‹çš„æœ€ä½³æ€§èƒ½ç›¸åŒ¹é…çš„æ•ˆæœï¼Œå¹¶ä¸”è¿˜èƒ½æ³›åŒ–åˆ°æœªè§ä»»åŠ¡ï¼ˆå¦‚BFCL - v3ï¼‰ã€‚å¹¿æ³›çš„æ¶ˆèå®éªŒè¯æ˜äº†AgentRLä¸­ç®—æ³•è®¾è®¡é€‰æ‹©å¸¦æ¥çš„ä¸€è‡´æ€§èƒ½æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡ä¸­æå‡ºçš„å¼‚æ­¥ç”Ÿæˆ - è®­ç»ƒç®¡é“ä»¥åŠç»Ÿä¸€çš„ç¯å¢ƒå¼€å‘å’Œç®¡ç†æ–¹å¼ï¼Œä¸ºè§£å†³å¤šè½®RLè®­ç»ƒä¸­çš„è®¡ç®—æ•ˆç‡é—®é¢˜å’Œå¼‚æ„ç¯å¢ƒç®¡ç†é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚äº¤å‰ç­–ç•¥é‡‡æ ·å’Œä»»åŠ¡ä¼˜åŠ¿å½’ä¸€åŒ–ç­‰ç®—æ³•åˆ›æ–°æœ‰åŠ©äºåœ¨å¤šè½®å¤šä»»åŠ¡åœºæ™¯ä¸­æå‡æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œå¯¹äºå…¶ä»–ç±»ä¼¼çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ åœºæ™¯å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚æ­¤å¤–ï¼ŒAgentRLåœ¨å¤šä»»åŠ¡è®­ç»ƒä¸­çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ„å»ºé€šç”¨çš„LLMæ™ºèƒ½ä½“æä¾›äº†å®è·µå‚è€ƒï¼Œå¯å‘ç ”ç©¶è€…åœ¨å¤šä»»åŠ¡å­¦ä¹ å’Œæ™ºèƒ½ä½“æ„å»ºæ–¹é¢è¿›è¡Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚
```

## learning-to-reason-without-external-rewards
### Abstract
Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— éœ€å¤–éƒ¨å¥–åŠ±ï¼Œå¤§è¯­è¨€æ¨¡å‹å¦‚ä½•å­¦ä¼šæ¨ç†ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é€šè¿‡å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¤æ‚æ¨ç†è™½æœ‰æˆæ•ˆï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚RLVRä¾èµ–æ˜‚è´µä¸”ç‰¹å®šé¢†åŸŸçš„ç›‘ç£ï¼Œå¦‚åœ¨æ•°å­¦é—®é¢˜ä¸­éœ€ä¸“å®¶æ ‡æ³¨ç­”æ¡ˆï¼Œåœ¨ä»£ç ç”Ÿæˆä¸­éœ€å…¨é¢æµ‹è¯•å¥—ä»¶å’Œæ‰§è¡Œç¯å¢ƒã€‚æ­¤å¤–ï¼Œä»¥ç»“æœä¸ºå¯¼å‘çš„å¯éªŒè¯å¥–åŠ±é™åˆ¶äº†æ¨¡å‹åœ¨å…¶ä»–é¢†åŸŸçš„è¿ç§»èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæ—©æœŸåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜ä¸”å¯èƒ½å­˜åœ¨åå·®ã€‚éšç€æ¨¡å‹èƒ½åŠ›å‘å±•ï¼Œæœªæ¥å¯èƒ½å‡ºç°äººç±»éš¾ä»¥ç›´æ¥è¯„ä¼°çš„æƒ…å†µï¼Œå› æ­¤éœ€è¦æ¨¡å‹é€šè¿‡å†…åœ¨æœºåˆ¶å®ç°è‡ªæˆ‘æå‡ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä¸€ç§æ–°èŒƒå¼â€”â€”åŸºäºå†…éƒ¨åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLIFï¼‰ï¼Œæ—¨åœ¨è®©LLMsä»…ä¾é æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„å†…åœ¨ä¿¡å·æå‡æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨æˆ–ç‰¹å®šé¢†åŸŸçš„çœŸå®æ•°æ®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRLIFèŒƒå¼
å¼•å…¥åŸºäºå†…éƒ¨åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLIFï¼‰è¿™ä¸€å…¨æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è‡ªèº«ç”Ÿæˆçš„å†…åœ¨ä¿¡å·è¿›è¡Œå­¦ä¹ ï¼Œæ— éœ€å¤–éƒ¨å¥–åŠ±æˆ–ç›‘ç£ï¼Œä¸ºæ¨¡å‹åœ¨æœªæ¥å…·å¤‡è¶…äººç±»èƒ½åŠ›ä¸”éš¾ä»¥è¢«äººç±»ç›´æ¥è¯„ä¼°çš„åœºæ™¯ä¸‹å®ç°è‡ªæˆ‘æå‡æä¾›äº†å¯èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºINTUITORæ–¹æ³•
åœ¨RLIFèŒƒå¼ä¸‹ï¼Œæå‡ºINTUITORè¿™ä¸€æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ç½®ä¿¡åº¦ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼Œå…·ä½“ä½¿ç”¨è‡ªæˆ‘ç¡®å®šæ€§ï¼ˆself - certaintyï¼‰ï¼Œå³æ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¸å‡åŒ€åˆ†å¸ƒä¹‹é—´çš„å¹³å‡KLæ•£åº¦ä½œä¸ºç½®ä¿¡åº¦åº¦é‡ã€‚INTUITORé€šè¿‡å°†ç°æœ‰RLVRæ¡†æ¶ï¼ˆå…·ä½“ä¸ºGroup Relative Policy Optimizationï¼ŒGRPOï¼‰ä¸­çš„å¯éªŒè¯å¥–åŠ±ä¿¡å·æ›¿æ¢ä¸ºè‡ªæˆ‘ç¡®å®šæ€§åˆ†æ•°ï¼Œå®ç°å®Œå…¨æ— ç›‘ç£å­¦ä¹ ï¼Œå¼•å¯¼æ¨¡å‹é€šè¿‡è‡ªèº«ç”Ÿæˆçš„ä¿¡å·è¿›è¡Œå­¦ä¹ ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£æˆ–æ‰‹å·¥è®¾è®¡çš„å¥–åŠ±ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MATHæ•°æ®é›†ä¸Šä½¿ç”¨Qwen2.5 - 3BåŸºç¡€æ¨¡å‹è¿›è¡Œå®éªŒï¼ŒINTUITORåœ¨ä¸ä¾èµ–ä»»ä½•æ ‡å‡†ç­”æ¡ˆçš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸GRPOç›¸å½“ã€‚åœ¨ä»£ç ç”Ÿæˆç­‰é¢†åŸŸå¤–ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ä¸Šï¼ŒINTUITORè¡¨ç°æ›´ä¼˜ã€‚ä¾‹å¦‚ï¼Œåœ¨LiveCodeBenchä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè®­ç»ƒQwen2.5 - 3BåŸºç¡€æ¨¡å‹åœ¨MATHæ•°æ®é›†ä¸Šï¼ŒINTUITORç›¸å¯¹æå‡65%ï¼Œè€ŒGRPOæ— æå‡ï¼›åœ¨CRUXEval - Oä»»åŠ¡ä¸Šï¼ŒINTUITORæå‡76%ï¼ŒGRPOæå‡44%ã€‚å¯¹Qwen2.5 - 1.5BåŸºç¡€æ¨¡å‹åœ¨MATHè¯­æ–™åº“ä¸Šä½¿ç”¨INTUITORè¿›è¡Œå¾®è°ƒï¼ŒåŸæœ¬äº§ç”Ÿé‡å¤å†…å®¹ä¸”åœ¨LiveCodeBenchä¸Šå¾—åˆ†ä¸º0%çš„æ¨¡å‹å­¦ä¼šå‘å‡ºè¿è´¯æ¨ç†é“¾å’Œç»“æ„è‰¯å¥½çš„ä»£ç ï¼Œå¾®è°ƒåå‡†ç¡®ç‡è¾¾åˆ°9.9%ã€‚åœ¨Llamaå’ŒOLMoæ¨¡å‹ä¸Šçš„å®éªŒä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾è‘—æå‡ï¼Œè¯æ˜äº†INTUITORå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ–°çš„å­¦ä¹ èŒƒå¼**ï¼šRLIFèŒƒå¼ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ï¼Œåœ¨ç¼ºä¹å¤–éƒ¨ç›‘ç£å’Œæ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯é€šè¿‡å†…åœ¨ä¿¡å·å®ç°æœ‰æ•ˆå­¦ä¹ ï¼Œä¸ºæœªæ¥æ¨¡å‹çš„è‡ªä¸»å‘å±•æä¾›äº†æ–¹å‘ã€‚
2. **æ— ç›‘ç£å­¦ä¹ æ–¹æ³•**ï¼šINTUITORæ–¹æ³•ä»…éœ€æ¸…æ™°çš„æç¤ºï¼Œæ— éœ€å¯éªŒè¯å¥–åŠ±ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œè¡¨æ˜é¢„è®­ç»ƒçš„LLMså¯èƒ½å…·æœ‰æ¯”ä»¥å‰è®¤è¯†åˆ°çš„æ›´ä¸°å¯Œçš„æ½œåœ¨è¡Œä¸ºå…ˆéªŒï¼Œè¿™ä¸ºå…¶ä»–ç ”ç©¶åœ¨è®¾è®¡æ— ç›‘ç£æˆ–å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•æ—¶æä¾›äº†å€Ÿé‰´ã€‚
3. **åˆ©ç”¨æ¨¡å‹å†…åœ¨ä¿¡å·**ï¼šåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ç½®ä¿¡åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè¿™ä¸€æ€è·¯å¯ä»¥å¯å‘å…¶ä»–ç ”ç©¶æ¢ç´¢å¦‚ä½•æŒ–æ˜æ¨¡å‹çš„å†…åœ¨ç‰¹æ€§æ¥å¼•å¯¼å­¦ä¹ ï¼Œæå‡æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚
``` 

## quagmires-in-sft-rl-post-training--when-high-sft-scores-mislead-and-what-to-use-instead
### Abstract
In post-training for reasoning Large Language Models (LLMs), the current state of practice trains LLMs in two independent stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as ``RL'' below). In this work, we challenge whether high SFT scores translate to improved performance after RL. We provide extensive counter-examples where this is not true. We find high SFT scores can be biased toward simpler or more homogeneous data and are not reliably predictive of subsequent RL gains or scaled-up post-training effectiveness. In some cases, RL training on models with improved SFT performance could lead to substantially worse outcome compared to RL on the base model without SFT. We study alternative metrics and identify generalization loss on held-out reasoning examples and Pass@large k performance to provide strong proxies for the RL outcome. We trained hundreds of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL performance, prediction based on generalization loss and Pass@large k achieves substantial higher precision, improving $R^2$ coefficient and Spearman's rank correlation coefficient by up to 0.5 (2x). This provides strong utility for broad use cases. For example, in most experiments, we find SFT training on unique examples for a one epoch underperforms training on half examples for two epochs, either after SFT or SFT-then-RL; With the same SFT budget, training only on short examples may lead to better SFT performance, though, it often leads to worse outcome after RL compared to training on examples with varying lengths. Evaluation tool will be open-sourced.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒï¼šé«˜åˆ†SFTä¸ºä½•å¤±çµåŠæ›¿ä»£æŒ‡æ ‡æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•é«˜åº¦å…³æ³¨æ¨ç†èƒ½åŠ›çš„å¢å¼ºï¼Œè¿™ä¾èµ–äºåè®­ç»ƒé˜¶æ®µï¼Œè¯¥é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼Œç®€ç§°RLï¼‰ä¸¤ä¸ªç‹¬ç«‹é˜¶æ®µæ¥ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥é€‚åº”å¤æ‚ä»»åŠ¡ã€‚å½“å‰å®è·µä¸­ï¼Œäººä»¬æ™®éå‡è®¾SFTé˜¶æ®µè¡¨ç°æ›´å¥½çš„æ¨¡å‹åœ¨RLé˜¶æ®µä¹Ÿä¼šæœ‰æ›´å¥½çš„è¡¨ç°ï¼Œä¸”SFTå’ŒRLçš„è®­ç»ƒç­–ç•¥ä¸æ•°æ®é€šå¸¸åˆ†å¼€è®¾è®¡ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾å­˜åœ¨ç¼ºé™·ï¼Œä¾‹å¦‚SFTé˜¶æ®µçš„è¿‡åº¦è®­ç»ƒå¯èƒ½é™åˆ¶æ¨¡å‹è¡Œä¸ºï¼Œå½±å“RLé˜¶æ®µçš„æ¢ç´¢ï¼Œå¯¼è‡´æœ€ç»ˆRLVRæ€§èƒ½ä¸ä½³ã€‚æ­¤å¤–ï¼Œå½“RLVRæœ€ç»ˆæ€§èƒ½ä¸ç†æƒ³æ—¶ï¼Œéš¾ä»¥ç¡®å®šæ˜¯RLé˜¶æ®µçš„é—®é¢˜è¿˜æ˜¯SFTèµ·å§‹ç‚¹ä¸ç†æƒ³ï¼Œä¸”RLè®­ç»ƒè®¡ç®—æˆæœ¬é«˜ã€ç®¡é“é•¿ï¼Œéš¾ä»¥è¿›è¡Œç«¯åˆ°ç«¯è°ƒæ•´ï¼Œä¹Ÿéš¾ä»¥å¯é é¢„æµ‹RLVRç»“æœã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæ¢ç´¢é¢„RLæ€§èƒ½ä¸RLåç»“æœä¹‹é—´çš„å…³ç³»ï¼Œå¯»æ‰¾æœ‰æ•ˆçš„SFTèŒƒå¼å’Œæ•°æ®ç­–ç•¥ï¼Œä»¥åŠå¯é çš„RLæˆåŠŸé¢„æµ‹æŒ‡æ ‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæŒ‘æˆ˜ä¼ ç»Ÿå‡è®¾ï¼Œæä¾›åä¾‹ã€‚é€šè¿‡å¤§é‡å®éªŒå‘ç°ï¼Œé«˜SFTåˆ†æ•°å¯èƒ½åå‘ç®€å•æˆ–åŒè´¨æ•°æ®ï¼Œä¸èƒ½å¯é é¢„æµ‹åç»­RLçš„å¢ç›Šæˆ–åè®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒåŸºäºSFTæ€§èƒ½æ”¹è¿›çš„æ¨¡å‹è¿›è¡ŒRLè®­ç»ƒï¼Œå¯èƒ½æ¯”æ²¡æœ‰SFTçš„åŸºç¡€æ¨¡å‹è¿›è¡ŒRLè®­ç»ƒçš„ç»“æœæ›´å·®ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºæ›¿ä»£æŒ‡æ ‡ã€‚ç ”ç©¶å‘ç°éªŒè¯æŸå¤±çš„æœ€ç»ˆå¢åŠ ä¸åæœŸRLé˜¶æ®µçš„æ€§èƒ½æå‡å¯†åˆ‡ç›¸å…³ï¼ŒåŒæ—¶å°†å¤§kå€¼ä¸‹çš„Pass@kä½œä¸ºé¢„æµ‹æŒ‡æ ‡ã€‚é€šè¿‡å¯¹Llama3 - 8Bã€Mistral - Nemo - 12Bå’ŒQwen3 - 4B - baseç­‰æ¨¡å‹ï¼Œåœ¨Llama - Nemotronå’ŒAceReasoner1.1ç­‰SFTæ•°æ®é›†ä»¥åŠä¸åŒRLæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®è¯éªŒè¯ï¼Œç»“æœè¡¨æ˜è¿™äº›æ–°æŒ‡æ ‡èƒ½å¯é é¢„æµ‹RLVRç»“æœï¼Œæ˜¾è‘—æé«˜R2ç³»æ•°å’ŒSpearmanç§©ç›¸å…³ç³»æ•°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼€å‘è¯„ä¼°å·¥å…·ã€‚ä¸ºè§£å†³ç°æœ‰å·¥å…·çš„å±€é™æ€§ï¼Œå¼€å‘äº†ä¸€ä¸ªå¢å¼ºå·¥å…·ï¼Œç”¨äºæ›´æ–¹ä¾¿ã€å¯é åœ°è¯„ä¼°æ¨ç†æ¨¡å‹ï¼Œå¹¶å°†å¼€æºè´¡çŒ®ç»™ç¤¾åŒºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½œè€…è®­ç»ƒäº†æ•°ç™¾ä¸ªå‚æ•°é«˜è¾¾12Bçš„æ¨¡å‹ï¼Œé€šè¿‡GRPOè¿›è¡ŒSFTå’ŒRLVRï¼Œå¹¶åœ¨7ä¸ªæ•°å­¦åŸºå‡†ä¸Šè¿›è¡Œäº†å¤šè¾¾256æ¬¡é‡å¤çš„å¹¿æ³›è¯„ä¼°ï¼ŒèŠ±è´¹è¶…è¿‡100ä¸‡GPUå°æ—¶ã€‚å®éªŒæ¶µç›–Llama3ã€Mistral - Nemoã€Qwen3ç­‰æ¨¡å‹ä»¥åŠå¤šä¸ªå…ˆè¿›çš„SFT/RLæ•°æ®é›†ã€‚ä¸ç›´æ¥ä»é¢„RLæ€§èƒ½è¿›è¡Œé¢„æµ‹ç›¸æ¯”ï¼ŒåŸºäºæ³›åŒ–æŸå¤±å’ŒPass@large kçš„é¢„æµ‹ç²¾åº¦å¤§å¹…æé«˜ï¼ŒR2ç³»æ•°å’ŒSpearmanç§©ç›¸å…³ç³»æ•°æé«˜äº†é«˜è¾¾0.5ï¼ˆ2å€ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§å¤šæ•°å®éªŒä¸­å‘ç°ï¼Œå¯¹å”¯ä¸€ç¤ºä¾‹è¿›è¡Œä¸€ä¸ªepochçš„SFTè®­ç»ƒï¼Œåœ¨SFTåæˆ–SFT - ç„¶å - RLåï¼Œè¡¨ç°ä¸å¦‚å¯¹ä¸€åŠç¤ºä¾‹è¿›è¡Œä¸¤ä¸ªepochçš„è®­ç»ƒï¼›åœ¨ç›¸åŒSFTé¢„ç®—ä¸‹ï¼Œä»…å¯¹çŸ­ç¤ºä¾‹è¿›è¡Œè®­ç»ƒå¯èƒ½å¯¼è‡´æ›´å¥½çš„SFTæ€§èƒ½ï¼Œä½†ä¸å¯¹ä¸åŒé•¿åº¦ç¤ºä¾‹è¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼ŒRLåçš„ç»“æœå¾€å¾€æ›´å·®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç ”ç©¶äººå‘˜è€Œè¨€ï¼Œä¸èƒ½å•çº¯ä¾èµ–SFTåˆ†æ•°æ¥è¯„ä¼°æ¨¡å‹åœ¨RLé˜¶æ®µçš„æ½œåŠ›ï¼Œè€Œåº”è€ƒè™‘æ–°æå‡ºçš„æ³›åŒ–æŸå¤±å’ŒPass@large kç­‰æŒ‡æ ‡ï¼Œä»¥æ›´å‡†ç¡®åœ°é¢„æµ‹RLVRç»“æœã€‚åœ¨è®¾è®¡SFTèŒƒå¼å’Œæ•°æ®ç­–ç•¥æ—¶ï¼Œè¦é¿å…è¿‡åº¦åå‘ç®€å•æˆ–åŒè´¨æ•°æ®ï¼Œç»¼åˆè€ƒè™‘ç¤ºä¾‹çš„å¤šæ ·æ€§å’Œè®­ç»ƒepochç­‰å› ç´ ã€‚æ­¤å¤–ï¼Œå¼€å‘çš„å¢å¼ºè¯„ä¼°å·¥å…·ä¸ºæ¨ç†æ¨¡å‹çš„è¯„ä¼°æä¾›äº†æ›´ä¾¿æ·å¯é çš„æ–¹å¼ï¼Œå¯è¢«ç¤¾åŒºå€Ÿé‰´ä½¿ç”¨ï¼Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶çš„å‘å±•ã€‚
``` 

## lightmem--lightweight-and-efficient-memory-augmented-generation
### Abstract
Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | LightMemï¼šé©æ–°å¤§è¯­è¨€æ¨¡å‹è®°å¿†ç³»ç»Ÿçš„è½»é‡é«˜æ•ˆæ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°½ç®¡èƒ½åŠ›å“è¶Šï¼Œä½†åœ¨åŠ¨æ€å¤æ‚ç¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å†å²äº¤äº’ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®°å¿†ç³»ç»Ÿè¢«å¼•å…¥ï¼Œä½¿LLMsèƒ½è¶…è¶Šæ— çŠ¶æ€äº¤äº’ï¼Œé€šè¿‡æŒä¹…çš„ä¿¡æ¯å­˜å‚¨ã€æ£€ç´¢å’Œåˆ©ç”¨æœºåˆ¶æ¥æå‡è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è®°å¿†ç³»ç»Ÿå­˜åœ¨æ˜¾è‘—çš„æ•ˆç‡å’Œä¸€è‡´æ€§é—®é¢˜ï¼šåœ¨é•¿äº¤äº’åœºæ™¯ä¸­ï¼ŒåŸå§‹ä¿¡æ¯å¸¸åŒ…å«å¤§é‡å†—ä½™ï¼Œå½“å‰ä¸»æµç ”ç©¶å¤šç›´æ¥å¤„ç†è€ŒæœªåŠ è¿‡æ»¤ï¼Œå¯¼è‡´é«˜å¼€é”€ï¼›è®°å¿†æ„å»ºå¸¸å­¤ç«‹å¤„ç†æ¯ä¸ªå›åˆæˆ–ä¾èµ–å›ºå®šä¸Šä¸‹æ–‡çª—å£è¾¹ç•Œï¼Œæ— æ³•å»ºæ¨¡ä¸åŒå›åˆé—´è¯­ä¹‰è¿æ¥ï¼Œå½±å“è®°å¿†é¡¹è¡¨ç¤ºå‡†ç¡®æ€§ï¼›è®°å¿†æ›´æ–°å’Œé—å¿˜é€šå¸¸åœ¨æ¨ç†å’Œä»»åŠ¡æ‰§è¡Œæ—¶ç›´æ¥è¿›è¡Œï¼Œå¯¼è‡´é•¿æµ‹è¯•æ—¶é—´å»¶è¿Ÿä¸”æ— æ³•æ·±åº¦å¤„ç†è¿‡å¾€ç»éªŒã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡çš„è®°å¿†ç³»ç»Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¢„å‹ç¼©æ„Ÿè§‰è®°å¿†æ¨¡å—
å—äººç±»è®°å¿†å¯å‘ï¼ŒLightMemçš„æ„Ÿè§‰è®°å¿†æ¨¡å—å¯¹åŸå§‹è¾“å…¥è¿›è¡Œé¢„å‹ç¼©ï¼Œè¿‡æ»¤æ‰å†—ä½™æˆ–ä½ä»·å€¼çš„æ ‡è®°ï¼Œå¹¶ç¼“å†²æç‚¼åçš„å†…å®¹ä¾›ä¸‹æ¸¸å¤„ç†ã€‚è¿™ä¸€åˆå§‹è¿‡æ»¤æ­¥éª¤åœ¨ä¿¡æ¯è¿›å…¥è®°å¿†ç®¡é“å‰å‡å°‘äº†å™ªå£°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸»é¢˜æ„ŸçŸ¥çŸ­æœŸè®°å¿†
è¯¥æ¨¡å—åˆ©ç”¨è¯­ä¹‰å’Œä¸»é¢˜ç›¸ä¼¼æ€§ï¼ŒåŠ¨æ€åœ°å°†ç›¸å…³è¯è¯­åˆ†ç»„ä¸ºè¿è´¯çš„ç‰‡æ®µã€‚å®ƒæ ¹æ®å†…å®¹è‡ªé€‚åº”åœ°ç¡®å®šç‰‡æ®µè¾¹ç•Œï¼Œè€Œéå›ºå®šçª—å£å¤§å°ï¼Œä»è€Œäº§ç”Ÿæ›´é›†ä¸­å’Œæœ‰æ„ä¹‰çš„è®°å¿†å•å…ƒã€‚è¿™ä¸ä»…å‡å°‘äº†è®°å¿†æ„å»ºçš„é¢‘ç‡ï¼Œè¿˜åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†æ›´ç²¾ç¡®å’Œé«˜æ•ˆçš„æ£€ç´¢ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç¡çœ æ—¶æ›´æ–°çš„é•¿æœŸè®°å¿†ç»´æŠ¤æœºåˆ¶
æ–°çš„è®°å¿†æ¡ç›®æœ€åˆå­˜å‚¨æ—¶å¸¦æœ‰æ—¶é—´æˆ³ï¼Œä»¥æ”¯æŒå®æ—¶å“åº”çš„å³æ—¶ï¼ˆâ€œè½¯â€ï¼‰æ›´æ–°ã€‚éšåï¼Œåœ¨æŒ‡å®šçš„ç¦»çº¿æ—¶æ®µï¼ˆå³â€œç¡çœ â€ï¼‰ï¼Œç³»ç»Ÿå¯¹è¿™äº›æ¡ç›®è¿›è¡Œé‡ç»„ã€å»é‡å’ŒæŠ½è±¡ï¼Œè§£å†³ä¸ä¸€è‡´æ€§å¹¶åŠ å¼ºè·¨çŸ¥è¯†è¿æ¥ã€‚è¿™ç§æœºåˆ¶å°†æ˜‚è´µçš„è®°å¿†ç»´æŠ¤ä¸å®æ—¶æ¨ç†è§£è€¦ï¼Œå®ç°äº†æ— å»¶è¿Ÿçš„é«˜ä¿çœŸæ›´æ–°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨LongMemEvalä¸Šï¼Œä»¥GPTå’ŒQwenä¸ºéª¨å¹²çš„LightMemåœ¨é—®ç­”å‡†ç¡®æ€§ä¸Šæ¯”æœ€å¼ºåŸºçº¿é«˜å‡º2.70% - 9.65%ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼šå‡å°‘äº†32Ã— - 117Ã—çš„æ ‡è®°ä½¿ç”¨é‡ï¼Œ17Ã— - 177Ã—çš„APIè°ƒç”¨é‡ï¼Œä»¥åŠ1.67Ã— - 12.45Ã—çš„è¿è¡Œæ—¶é—´ã€‚ç¦»çº¿æ›´æ–°åè¿™äº›ä¼˜åŠ¿ä¾ç„¶ä¿æŒã€‚æ­¤å¤–ï¼Œæ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œç¦»çº¿â€œç¡çœ æ—¶â€çš„æ•´åˆå®ç°äº†æ›´å¯é çš„é•¿æœŸçŸ¥è¯†æ›´æ–°ï¼Œå‡è½»äº†é•¿äº¤äº’ä¸­çš„ä¿¡æ¯ä¸¢å¤±å’Œä¸ä¸€è‡´é—®é¢˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ¶æ„è®¾è®¡æ€è·¯**ï¼šå€Ÿé‰´äººç±»è®°å¿†çš„åˆ†å±‚æ¶æ„æ¥è®¾è®¡å¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†ç³»ç»Ÿï¼Œé€šè¿‡å¤šé˜¶æ®µå¤„ç†å®ç°æ€§èƒ½å’Œèµ„æºä½¿ç”¨çš„å¹³è¡¡ï¼Œè¿™ç§è·¨é¢†åŸŸçš„çµæ„Ÿå€Ÿé‰´ä¸ºæ¨¡å‹æ¶æ„è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚
2. **ä¿¡æ¯å¤„ç†ç­–ç•¥**ï¼šå¯¹åŸå§‹ä¿¡æ¯è¿›è¡Œç³»ç»Ÿçš„è¿‡æ»¤ã€ç»„ç»‡å’Œæ•´åˆï¼Œå‡å°‘å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ï¼Œæé«˜ä¿¡æ¯å¤„ç†çš„æ•ˆç‡å’Œè´¨é‡ï¼Œè¿™ä¸€ç­–ç•¥å¯åº”ç”¨äºå…¶ä»–éœ€è¦å¤„ç†å¤§é‡ä¿¡æ¯çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚
3. **æ›´æ–°æœºåˆ¶**ï¼šå°†è®°å¿†ç»´æŠ¤ä¸å®æ—¶æ¨ç†è§£è€¦çš„ç¡çœ æ—¶æ›´æ–°æœºåˆ¶ï¼Œä¸ºè§£å†³æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æˆ–é•¿æœŸäº¤äº’æ—¶çš„å»¶è¿Ÿé—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œå€¼å¾—åœ¨å…¶ä»–éœ€è¦å®æ—¶å“åº”å’Œé•¿æœŸå­¦ä¹ çš„åœºæ™¯ä¸­å€Ÿé‰´ã€‚
``` 

## beyond-pass@1--self-play-with-variational-problem-synthesis-sustains-rlvr
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´ä¼ ç»Ÿï¼ŒSVSç­–ç•¥åŠ©åŠ›å¼ºåŒ–å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹æ¨ç†å‡çº§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åè®­ç»ƒçš„å…³é”®èŒƒå¼ï¼Œå°¤å…¶é€‚ç”¨äºå¤æ‚æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RLVRè®­ç»ƒè™½èƒ½æå‡Pass@1æ€§èƒ½ï¼Œä½†å´ä»¥ç­–ç•¥ç†µä¸ºä»£ä»·ï¼Œå¯¼è‡´ç”Ÿæˆå¤šæ ·æ€§é™ä½ï¼Œé™åˆ¶äº†Pass@kæ€§èƒ½ï¼ˆé€šå¸¸ä»£è¡¨LLMæ¨ç†èƒ½åŠ›çš„ä¸Šé™ï¼‰ã€‚è®­ç»ƒç†µçš„å´©æºƒä¼šä½¿ç­–ç•¥å€¾å‘äºäº§ç”ŸåŒè´¨åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œå¤±å»æ¢ç´¢æ›´é«˜çº§æ¨ç†è½¨è¿¹çš„æœºä¼šï¼Œæœ€ç»ˆå¯¼è‡´Pass@1åˆ†æ•°ä¹Ÿè¶‹äºå¹³ç¨³ã€‚æ­¤å¤–ï¼ŒRLVRè®­ç»ƒåœ¨æœ‰é™é—®é¢˜ä¸Šè¿›è¡Œï¼Œç­–ç•¥æ˜“å› é‡å¤ç”Ÿæˆè®°å¿†ä¸­çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆè€Œè·å¾—å¥–åŠ±ï¼Œç±»ä¼¼ â€œç ´è§£â€ è®­ç»ƒã€‚æ”¶é›†å¸¦æœ‰å¯éªŒè¯ç­”æ¡ˆçš„å¤§é‡é—®é¢˜é›†å¹¶éæ˜“äº‹ï¼Œé«˜è´¨é‡çš„äººå·¥æ ‡æ³¨é—®é¢˜é›†ç¨€ç¼ºä¸”å¯èƒ½ä¸ç°ä»£LLMsçš„å¼ºæ¨ç†èƒ½åŠ›ä¸åŒ¹é…ï¼Œåˆæˆæ•°æ®åˆç¼ºä¹ç²¾ç¡®çš„å‚è€ƒç­”æ¡ˆã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§ç®€å•æœ‰æ•ˆçš„é—®é¢˜å¢å¼ºæ–¹æ³•ï¼Œä»¥ç»´æŒæ•°æ®å¤šæ ·æ€§ã€åŒ¹é…æ¨¡å‹èƒ½åŠ›å¹¶ç¡®ä¿å‡†ç¡®çš„æ ‡æ³¨ç­”æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåœ¨çº¿è‡ªåšå¼ˆä¸å˜åˆ†é—®é¢˜åˆæˆï¼ˆSVSï¼‰ç­–ç•¥
è¯¥ç­–ç•¥ä¿ƒä½¿ç­–ç•¥æ¨¡å‹åŸºäºå…¶å¯¹å…·æœ‰æŒ‘æˆ˜æ€§ä¸”è¡¨ç°ä¸ä½³çš„è®­ç»ƒé›†é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆæ¥ç”Ÿæˆå˜åˆ†é—®é¢˜ã€‚ä»…å¢å¼ºå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œä»¥æœ‰æ•ˆé’ˆå¯¹ç­–ç•¥çš„æœ€å¼±èƒ½åŠ›ã€‚å˜åˆ†é—®é¢˜åº”å…·æœ‰é‡æ–°è¡¨è¿°çš„æè¿°å’Œç»“æ„ï¼Œä½†ä¿ç•™åŸå§‹è¯­ä¹‰ï¼Œä¸”ä¸åŸå§‹é—®é¢˜å…±äº«ç›¸åŒçš„æ ‡å‡†ç­”æ¡ˆï¼Œç¡®ä¿ç²¾ç¡®æ€§å¹¶æ¶ˆé™¤é¢å¤–çš„æ ‡æ³¨è®¡ç®—ã€‚åˆæˆåï¼Œç­–ç•¥æ¨¡å‹éœ€è§£å†³å…¶è‡ªç”Ÿæˆçš„å˜åˆ†é—®é¢˜ï¼Œé€šè¿‡ç­”æ¡ˆä¸åŸå§‹é—®é¢˜æ ‡å‡†ç­”æ¡ˆçš„ä¸€è‡´æ€§æ¥éªŒè¯å˜åˆ†é—®é¢˜çš„æ­£ç¡®æ€§ã€‚æœ€åï¼Œå°†åŸå§‹é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€è‡ªç”Ÿæˆçš„å˜åˆ†é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆæ”¶é›†èµ·æ¥ç”¨äºç­–ç•¥æ›´æ–°ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ é—®é¢˜è§£å†³å’Œé—®é¢˜åˆæˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSVSæ¡†æ¶çš„ç‹¬ç«‹æ€§ä¸å…¼å®¹æ€§
SVSæ¡†æ¶ä»…ä¾èµ–äºç­–ç•¥æ¨¡å‹æœ¬èº«ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨æŒ‡å¯¼æˆ–è’¸é¦ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„è‡ªæˆ‘æ”¹è¿›å®ç°æ‰€æœ‰æå‡ã€‚æ­¤å¤–ï¼ŒSVSå¢å¼ºä¸RLVRç®—æ³•æ— å…³ï¼Œå¯çµæ´»èå…¥å…¶ä»–æ–¹æ³•ï¼Œå¦‚PPOã€GSPOå’ŒReinforce++ç­‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨3Båˆ°32Bä¸åŒè§„æ¨¡çš„LLMsä¸Šè¿›è¡Œå®éªŒï¼Œå¹¶åœ¨12ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ¨ç†åŸºå‡†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒSVSåœ¨æ‰€æœ‰æ¨¡å‹è§„æ¨¡å’ŒåŸºå‡†æ°´å¹³ä¸Šå§‹ç»ˆä¼˜äºæ ‡å‡†RLVRï¼Œåœ¨æ‰€æœ‰å®éªŒä¸­å¹³å‡æ¯”åŸºçº¿æé«˜çº¦3%ã€‚å¾—ç›Šäºåœ¨çº¿æ•°æ®æ›´æ–°ç­–ç•¥ï¼ŒSVSè®­ç»ƒå°†ç­–ç•¥ç†µå§‹ç»ˆç»´æŒåœ¨ç¨³å®šèŒƒå›´å†…ï¼Œæ— æ˜æ˜¾ä¸‹é™æˆ–æ¿€å¢ï¼Œè¡¨æ˜æ›´å¯æŒç»­çš„è®­ç»ƒå’Œé•¿æœŸçš„è‡ªæˆ‘æ”¹è¿›ã€‚åœ¨ç«äº‰çº§åˆ«çš„AIME24å’ŒAIME25åŸºå‡†ä¸Šï¼ŒSVSåœ¨Pass@32ä¸Šåˆ†åˆ«å®ç°äº†18.3%å’Œ22.8%çš„æ˜¾è‘—æå‡ï¼Œè€Œæ ‡å‡†RLVRå‡ ä¹æ²¡æœ‰æ”¹è¿›ã€‚åœ¨å››ä¸ªæƒå¨åŸºå‡†ä¸Šï¼ŒSVSå®ç°äº†å¯æ‰©å±•çš„Pass@kæå‡ï¼Œæ˜¾è‘—æ‰©å±•äº†æ¨¡å‹çš„æ¨ç†è¾¹ç•Œã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **é—®é¢˜å¢å¼ºæ€è·¯**ï¼šä»è®­ç»ƒé—®é¢˜çš„è§’åº¦åˆ†æç­–ç•¥çš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œå¹¶é€šè¿‡å¢å¼ºå’Œæ›´æ–°è®­ç»ƒé—®é¢˜æ¥ç¼“è§£è®­ç»ƒä¸­çš„ç†µå´©æºƒï¼Œä¸ºè§£å†³ç±»ä¼¼çš„æ€§èƒ½ä¸å¤šæ ·æ€§æƒè¡¡é—®é¢˜æä¾›äº†æ–°çš„æ€è€ƒæ–¹å‘ã€‚
2. **è‡ªåšå¼ˆä¸é—®é¢˜åˆæˆæ–¹æ³•**ï¼šSVSç­–ç•¥ä¸­åˆ©ç”¨ç­–ç•¥è‡ªèº«çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆç”Ÿæˆå˜åˆ†é—®é¢˜çš„æ–¹å¼ï¼Œå®ç°äº†æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›ï¼Œè¿™ç§æ— éœ€å¤–éƒ¨æŒ‡å¯¼çš„è‡ªå¢å¼ºæ¨¡å¼å¯åº”ç”¨äºå…¶ä»–éœ€è¦æå‡æ¨¡å‹èƒ½åŠ›å’Œå¤šæ ·æ€§çš„åœºæ™¯ã€‚
3. **ç®—æ³•å…¼å®¹æ€§**ï¼šSVSä¸RLVRç®—æ³•æ— å…³ä¸”å¯çµæ´»èå…¥å…¶ä»–æ–¹æ³•çš„ç‰¹æ€§ï¼Œä¸ºåœ¨ä¸åŒçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­åº”ç”¨è¯¥ç­–ç•¥æä¾›äº†ä¾¿åˆ©ï¼Œå¼€å‘è€…å¯æ ¹æ®è‡ªèº«éœ€æ±‚é€‰æ‹©åˆé€‚çš„RLVRç®—æ³•ä¸SVSç»“åˆï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚ 
``` 

## agent-data-protocol--unifying-datasets-for-diverse--effective-fine-tuning-of-llm-agents
### Abstract
Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ADPï¼šå¼€å¯å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è®­ç»ƒæ ‡å‡†åŒ–æ–°æ—¶ä»£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„è®­ç»ƒå¾—ç›Šäºä¸°å¯Œä¸”æ˜“è·å–çš„äº’è”ç½‘è§„æ¨¡æ•°æ®ï¼Œä½†åè®­ç»ƒé˜¶æ®µé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ™ºèƒ½ä½“åº”ç”¨é¢†åŸŸã€‚é«˜è´¨é‡çš„ç‰¹å®šä»»åŠ¡æ•°æ®éœ€ç²¾å¿ƒç­–åˆ’ï¼Œè€Œè®¸å¤šç°å®ä¸–ç•Œä»»åŠ¡æä¸ºå¤æ‚ï¼Œæ™ºèƒ½ä½“æ¨¡å‹éœ€é‡‡å–é¡ºåºè¡ŒåŠ¨å¹¶ä¸ä¸–ç•Œè¿­ä»£äº¤äº’ï¼Œæ„å»ºæ­¤ç±»åœºæ™¯çš„æ•°æ®é›†éœ€è®°å½•å’Œæ„å»ºæ™ºèƒ½ä½“è¡Œä¸ºè½¨è¿¹ï¼Œéš¾åº¦è¿œè¶…æ”¶é›†é™æ€è¾“å…¥ - è¾“å‡ºå¯¹ã€‚å°½ç®¡å·²æœ‰å¤šç§åˆ›å»ºæ™ºèƒ½ä½“æ•°æ®é›†çš„æ–¹æ³•ï¼Œæ¶µç›–æ‰‹åŠ¨ç­–åˆ’ã€åˆæˆæ•°æ®ç”Ÿæˆã€è®°å½•æ™ºèƒ½ä½“æ‰§è¡Œç­‰ï¼Œäº§ç”Ÿäº†åŒ…æ‹¬ç½‘é¡µå¯¼èˆªã€è½¯ä»¶å¼€å‘ç­‰å¤šç§ä»»åŠ¡çš„æ•°æ®é›†ï¼Œä½†å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å­¦æœ¯ç ”ç©¶ä¸­ä»è¾ƒä¸ºç½•è§ã€‚åŸå› å¹¶éç¼ºä¹æ•°æ®ï¼Œè€Œæ˜¯ç°æœ‰æ•°æ®é›†æ ¼å¼å’Œè¡¨ç¤ºä¸ä¸€è‡´ï¼Œç¢ç‰‡åŒ–ä¸¥é‡ï¼Œéš¾ä»¥æœ‰æ•ˆç»„åˆã€å…±äº«å’Œåˆ©ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ™ºèƒ½ä½“æ•°æ®åè®®ï¼ˆADPï¼‰
ADPæ˜¯ä¸€ç§è½»é‡çº§è¡¨ç¤ºè¯­è¨€ï¼Œä½œä¸ºä¸åŒæ ¼å¼çš„æ™ºèƒ½ä½“æ•°æ®é›†ä¸ç»Ÿä¸€çš„ä¸‹æ¸¸æ™ºèƒ½ä½“è®­ç»ƒç®¡é“ä¹‹é—´çš„â€œä¸­é—´è¯­è¨€â€ã€‚å®ƒä»¥Pydanticæ¨¡å¼å®ç°ï¼Œè¡¨è¾¾ä¸å¸¸è§æ™ºèƒ½ä½“ç”¨ä¾‹ï¼ˆå¦‚é€šä¿¡ã€æµè§ˆã€ç¼–ç å’Œå„ç§å·¥å…·è°ƒç”¨ï¼‰ç›¸å¯¹åº”çš„è¡ŒåŠ¨å’Œè§‚å¯Ÿï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„è‡ªåŠ¨éªŒè¯ä¿æŒé«˜æ•°æ®è´¨é‡ã€‚ADPå°†æ•°æ®ç»Ÿä¸€ä¸ºè½¨è¿¹å¯¹è±¡ï¼ŒåŒ…æ‹¬è¡ŒåŠ¨ï¼ˆAPIè¡ŒåŠ¨ã€ä»£ç è¡ŒåŠ¨ã€æ¶ˆæ¯è¡ŒåŠ¨ï¼‰å’Œè§‚å¯Ÿï¼ˆæ–‡æœ¬è§‚å¯Ÿã€ç½‘é¡µè§‚å¯Ÿï¼‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œèƒ½æ•è·å¤šç§ä»»åŠ¡ï¼Œä¸”æ— éœ€é’ˆå¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œå·¥ç¨‹å¤„ç†å³å¯è½»æ¾è§£æå’Œè®­ç»ƒã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®ç°æ•°æ®é›†è½¬æ¢ä¸å‘å¸ƒ
å®ç°äº†å°†13ä¸ªç°æœ‰æ•°æ®é›†è½¬æ¢ä¸ºADPçš„è½¬æ¢å™¨ï¼Œä»¥åŠä»ADPåˆ°3ç§ä¸åŒæ™ºèƒ½ä½“æ¶æ„çš„è½¬æ¢å™¨ï¼Œå±•ç¤ºäº†å…¶é€šç”¨æ€§ã€‚åŸºäºæ­¤ï¼Œåˆ›å»ºå¹¶å‘å¸ƒäº†æœ€å¤§çš„å…¬å¼€å¯ç”¨æ™ºèƒ½ä½“è®­ç»ƒæ•°æ®é›†ADP Dataset V1ï¼ŒåŒ…å«130ä¸‡ä¸ªè®­ç»ƒè½¨è¿¹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½¿ç”¨ADPè®­ç»ƒæ™ºèƒ½ä½“åœ¨å¤šä¸ªé¢†åŸŸå¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬ç¼–ç ï¼ˆSWE - Bench Verifiedï¼‰ã€ç½‘é¡µæµè§ˆï¼ˆWebArenaï¼‰ã€ç ”ç©¶ï¼ˆGAIAï¼‰å’Œæ™ºèƒ½ä½“å·¥å…·ä½¿ç”¨ï¼ˆAgentBenchï¼‰ç­‰ã€‚ç»“æœå¹³å‡æ¯”åŸºç¡€æ¨¡å‹æé«˜20%ï¼Œä¸ç±»ä¼¼è§„æ¨¡æ¨¡å‹çš„å…¶ä»–æœ€å…ˆè¿›ç»“æœç›¸æ¯”å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜ã€‚åŒæ—¶ï¼Œè·¨ä»»åŠ¡è½¬ç§»æœ‰æ˜¾è‘—ç›Šå¤„ï¼Œåœ¨ADPæ•°æ®ä¸Šè®­ç»ƒæ¯”åœ¨å•ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒæœ‰æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒADPè¿˜èƒ½å®ç°ç³»ç»Ÿçš„è·¨æ•°æ®é›†åˆ†æï¼Œæ­ç¤ºå…¬å¼€å¯ç”¨æ•°æ®çš„è¶‹åŠ¿å’Œæ”¹è¿›æ–¹å‘ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ADPä¸ºæ™ºèƒ½ä½“æ¨¡å‹å¾®è°ƒæä¾›äº†æ‰€éœ€çš„æ ‡å‡†åŒ–ï¼Œä½¿å¤§è§„æ¨¡ç›‘ç£æ™ºèƒ½ä½“è®­ç»ƒå…·æœ‰å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé™ä½äº†æ ‡å‡†åŒ–ã€å¯æ‰©å±•å’Œå¯é‡å¤çš„æ™ºèƒ½ä½“è®­ç»ƒçš„é—¨æ§›ã€‚å…¶æå‡ºçš„ç»Ÿä¸€æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œä¸ºè§£å†³æ•°æ®é›†ç¢ç‰‡åŒ–é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå¯¹äºæ•´åˆä¸åŒæ¥æºå’Œæ ¼å¼çš„æ•°æ®å…·æœ‰é‡è¦å€Ÿé‰´æ„ä¹‰ã€‚åŒæ—¶ï¼Œå…¬å¼€æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†çš„åšæ³•ï¼Œæœ‰åŠ©äºç¤¾åŒºé‡‡ç”¨å¹¶é¼“åŠ±æ–°æ•°æ®é›†çš„è´¡çŒ®ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“æ¨¡å‹è®­ç»ƒé¢†åŸŸçš„å‘å±•ã€‚
``` 

## beyond-magic-words--sharpness-aware-prompt-evolving-for-robust-large-language-models-with-tare
### Abstract
The performance of Large Language Models (LLMs) hinges on carefully engineered prompts. However, prevailing prompt optimization methods, ranging from heuristic edits and reinforcement learning to evolutionary search, primarily target point-wise accuracy. They seldom enforce paraphrase invariance or searching stability, and therefore cannot remedy this brittleness in practice. Automated prompt search remains brittle: small, semantically preserving paraphrases often cause large performance swings. We identify this brittleness as the textual sharpness of the prompt landscape. In this work, we provide the first formal treatment of textual sharpness in the discrete, semantic space of prompts, together with an operational robustness criterion over a semantic neighborhood; the design is black-box or API-only, requiring no gradients to update the model's parameters. Then we introduce TARE (Textual Sharpness-Aware Evolving), a derivative-free framework that alternates between an inner, sampling-based adversarial search that stresses a prompt with hard paraphrases and an outer, robust selection that prefers candidates whose neighborhoods remain strong. We further propose ATARE, which learns anisotropic weights to shape the semantic neighborhood and adapts its radius over time to balance exploration and fidelity. Diverse tasks evaluate our methods, whose design for minimizing textual sharpness gap leads to prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining computationally practical.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´é­”æ³•è¯å±€é™ï¼šTAREåŠ©åŠ›å¤§è¯­è¨€æ¨¡å‹ç¨³å¥å‘å±•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç²¾å¿ƒè®¾è®¡çš„æç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚å¯å‘å¼ç¼–è¾‘ã€å¼ºåŒ–å­¦ä¹ å’Œè¿›åŒ–æœç´¢ç­‰ï¼Œä¸»è¦å…³æ³¨é€ç‚¹å‡†ç¡®æ€§ï¼Œå¾ˆå°‘å¼ºåˆ¶é‡Šä¹‰ä¸å˜æ€§æˆ–æœç´¢ç¨³å®šæ€§ï¼Œå› æ­¤åœ¨å®è·µä¸­æ— æ³•è§£å†³æç¤ºçš„è„†æ€§é—®é¢˜ã€‚è‡ªåŠ¨æç¤ºæœç´¢ä»ç„¶å¾ˆè„†å¼±ï¼šå¾®å°çš„ã€è¯­ä¹‰ä¿æŒçš„é‡Šä¹‰å¾€å¾€ä¼šå¯¼è‡´æ€§èƒ½å¤§å¹…æ³¢åŠ¨ã€‚è®ºæ–‡å°†è¿™ç§è„†æ€§è¯†åˆ«ä¸ºæç¤ºç©ºé—´çš„æ–‡æœ¬å°–é”åº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
é¦–æ¬¡åœ¨æç¤ºçš„ç¦»æ•£è¯­ä¹‰ç©ºé—´ä¸­å¯¹æ–‡æœ¬å°–é”åº¦è¿›è¡Œäº†æ­£å¼å¤„ç†ï¼Œå¹¶åœ¨è¯­ä¹‰é‚»åŸŸä¸Šæå‡ºäº†ä¸€ä¸ªæ“ä½œé²æ£’æ€§æ ‡å‡†ã€‚è¯¥è®¾è®¡æ˜¯é»‘ç›’æˆ–ä»…APIçš„ï¼Œæ— éœ€æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2
å¼•å…¥TAREï¼ˆTextual Sharpness - Aware Evolvingï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— å¯¼æ•°æ¡†æ¶ï¼Œåœ¨åŸºäºé‡‡æ ·çš„å†…éƒ¨å¯¹æŠ—æ€§æœç´¢ï¼ˆç”¨ç¡¬é‡Šä¹‰å¼ºè°ƒæç¤ºï¼‰å’Œå¤–éƒ¨ç¨³å¥é€‰æ‹©ï¼ˆåå¥½é‚»åŸŸä»ç„¶å¼ºå¤§çš„å€™é€‰æç¤ºï¼‰ä¹‹é—´äº¤æ›¿ã€‚è¿›ä¸€æ­¥æå‡ºATAREï¼Œå…¶å­¦ä¹ å„å‘å¼‚æ€§æƒé‡æ¥å¡‘é€ è¯­ä¹‰é‚»åŸŸï¼Œå¹¶éšæ—¶é—´è°ƒæ•´å…¶åŠå¾„ä»¥å¹³è¡¡æ¢ç´¢å’Œä¿çœŸåº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¤šæ ·åŒ–çš„ä»»åŠ¡å¯¹æå‡ºçš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œæ—¨åœ¨æœ€å°åŒ–æ–‡æœ¬å°–é”åº¦å·®è·çš„è®¾è®¡æ‰€äº§ç”Ÿçš„æç¤ºåœ¨é‡Šä¹‰ä¸‹ä¿æŒå‡†ç¡®æ€§ï¼Œä¼˜äºä»…å…³æ³¨å‡†ç¡®æ€§çš„æç¤ºæœç´¢ï¼ŒåŒæ—¶åœ¨è®¡ç®—ä¸Šå…·æœ‰å®ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡å¯¹æç¤ºè„†æ€§é—®é¢˜çš„æ·±å…¥åˆ†æä»¥åŠæå‡ºçš„TAREå’ŒATAREæ–¹æ³•ä¸ºå¤§è¯­è¨€æ¨¡å‹æç¤ºä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯¹äºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹è¯­ä¹‰ä¿æŒçš„æ–‡æœ¬å˜åŒ–æ—¶çš„ç¨³å¥æ€§å…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›å¯¹æç¤ºç¨³å®šæ€§æœ‰è¾ƒé«˜è¦æ±‚çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
```

## memgen--weaving-generative-latent-memory-for-self-evolving-agents
### Abstract
Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions. Existing paradigms remain constrained: parametric memory forcibly adjusts model parameters, and retrieval-based memory externalizes experience into structured databases, yet neither captures the fluid interweaving of reasoning and memory that underlies human cognition. To address this gap, we propose MemGen, a dynamic generative memory framework that equips agents with a human-esque cognitive faculty. It consists of a \textit{memory trigger}, which monitors the agent's reasoning state to decide explicit memory invocation, and a \textit{memory weaver}, which takes the agent's current state as stimulus to construct a latent token sequence as machine-native memory to enrich its reasoning. In this way, MemGen enables agents to recall and augment latent memory throughout reasoning, producing a tightly interwoven cycle of memory and cognition. Extensive experiments across eight benchmarks show that MemGen surpasses leading external memory systems such as ExpeL and AWM by up to $38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain generalization ability. More importantly, we find that without explicit supervision, MemGen spontaneously evolves distinct human-like memory faculties, including planning memory, procedural memory, and working memory, suggesting an emergent trajectory toward more naturalistic forms of machine cognition.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | MemGenï¼šèµ‹äºˆæ™ºèƒ½ä½“ç±»äººè®¤çŸ¥çš„ç”Ÿæˆå¼æ½œåœ¨è®°å¿†æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“é¢†åŸŸï¼Œæ™ºèƒ½ä½“çš„è®°å¿†å¡‘é€ ç€å®ƒä»¬ä¸ç¯å¢ƒäº¤äº’å¹¶é€æ­¥å®Œå–„è‡ªèº«çš„æ–¹å¼ï¼Œå¦‚åŒäººç±»å¤§è„‘ä¸€èˆ¬ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è®°å¿†èŒƒå¼å­˜åœ¨å±€é™æ€§ï¼šå‚æ•°åŒ–è®°å¿†å¼ºè¡Œè°ƒæ•´æ¨¡å‹å‚æ•°ï¼ŒåŸºäºæ£€ç´¢çš„è®°å¿†å°†ç»éªŒå¤–åŒ–åˆ°ç»“æ„åŒ–æ•°æ®åº“ä¸­ï¼Œä½†è¿™ä¸¤ç§æ–¹å¼éƒ½æœªèƒ½æ•æ‰åˆ°äººç±»è®¤çŸ¥ä¸­æ¨ç†ä¸è®°å¿†ç›¸äº’äº¤ç»‡çš„æµåŠ¨æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†MemGenã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé…å¤‡è®°å¿†è§¦å‘å™¨ï¼ˆmemory triggerï¼‰
è¯¥ç»„ä»¶è´Ÿè´£ç›‘æµ‹æ™ºèƒ½ä½“çš„æ¨ç†çŠ¶æ€ï¼Œä»¥æ­¤æ¥å†³å®šä½•æ—¶æ˜ç¡®è°ƒç”¨è®°å¿†ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªèº«æ¨ç†çŠ¶æ€çš„æƒ…å†µé€‚æ—¶å”¤èµ·è®°å¿†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥è®°å¿†ç¼–ç»‡å™¨ï¼ˆmemory weaverï¼‰
å®ƒå°†æ™ºèƒ½ä½“çš„å½“å‰çŠ¶æ€ä½œä¸ºåˆºæ¿€ï¼Œæ„å»ºä¸€ä¸ªæ½œåœ¨æ ‡è®°åºåˆ—ä½œä¸ºæœºå™¨åŸç”Ÿè®°å¿†ï¼Œè¿›è€Œä¸°å¯Œæ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMemGenä½¿æ™ºèƒ½ä½“åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿå›å¿†å¹¶å¢å¼ºæ½œåœ¨è®°å¿†ï¼Œå½¢æˆè®°å¿†ä¸è®¤çŸ¥ç´§å¯†äº¤ç»‡çš„å¾ªç¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMemGençš„æ€§èƒ½ä¼˜äºé¢†å…ˆçš„å¤–éƒ¨è®°å¿†ç³»ç»Ÿï¼Œå¦‚ExpeLå’ŒAWMï¼Œæå‡å¹…åº¦é«˜è¾¾38.22%ï¼Œè¶…è¿‡GRPOçš„å¹…åº¦é«˜è¾¾13.44%ï¼Œå¹¶ä¸”å±•ç°å‡ºå¼ºå¤§çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨æ²¡æœ‰æ˜ç¡®ç›‘ç£çš„æƒ…å†µä¸‹ï¼ŒMemGenè‡ªå‘åœ°è¿›åŒ–å‡ºäº†ä¸åŒçš„ç±»äººè®°å¿†èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§„åˆ’è®°å¿†ã€ç¨‹åºè®°å¿†å’Œå·¥ä½œè®°å¿†ï¼Œè¿™æš—ç¤ºç€æœç€æ›´è‡ªç„¶ä¸»ä¹‰çš„æœºå™¨è®¤çŸ¥å½¢å¼å‘å±•çš„æ–°å…´è½¨è¿¹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MemGenä¸ºæ™ºèƒ½ä½“è®°å¿†çš„æ„å»ºæä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶åŠ¨æ€ç”Ÿæˆå¼è®°å¿†æ¡†æ¶æ¨¡æ‹Ÿäººç±»è®¤çŸ¥ä¸­æ¨ç†ä¸è®°å¿†çš„äº¤ç»‡æ¨¡å¼ï¼Œå¯¹äºæå‡æ™ºèƒ½ä½“çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ•ˆæœæ˜¾è‘—ã€‚åœ¨æ„å»ºæ™ºèƒ½ä½“ç›¸å…³ç³»ç»Ÿæ—¶ï¼Œå¯å€Ÿé‰´å…¶è®°å¿†è§¦å‘å™¨å’Œè®°å¿†ç¼–ç»‡å™¨çš„è®¾è®¡ç†å¿µï¼Œä»¥å¢å¼ºæ™ºèƒ½ä½“åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è®°å¿†è°ƒç”¨å’Œå¢å¼ºèƒ½åŠ›ï¼Œæ¨åŠ¨æœºå™¨è®¤çŸ¥æœç€æ›´æ¥è¿‘äººç±»è‡ªç„¶è®¤çŸ¥çš„æ–¹å‘å‘å±•ã€‚
``` 

## plan-and-budget--effective-and-efficient-test-time-scaling-on-large-language-model-reasoning
### Abstract
Large Language Models (LLMs) have achieved remarkable success in complex reasoning tasks, but their inference remains computationally inefficient. We observe a common failure mode in many prevalent LLMs, overthinking, where models generate verbose and tangential reasoning traces even for simple queries. Recent works have tried to mitigate this by enforcing fixed token budgets, however, this can lead to underthinking, especially on harder problems. Through empirical analysis, we identify that this inefficiency often stems from unclear problem-solving strategies. To formalize this, we develop a theoretical model, BBAM (Bayesian Budget Allocation Model), which models reasoning as a sequence of sub-questions with varying uncertainty, and introduce the $E^3$ metric to capture the trade-off between correctness and computation efficiency. Building on theoretical results from BBAM, we propose Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex queries into sub-questions and allocates token budgets based on estimated complexity using adaptive scheduling. Plan-and-Budget improves reasoning efficiency across a range of tasks and models, achieving up to +70% accuracy gains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close performance gaps without retraining. Our code is available at https://github.com/junhongmit/P-and-B.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®¡åˆ’ä¸é¢„ç®—ï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„é«˜æ•ˆæµ‹è¯•æ—¶é—´ç¼©æ”¾

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹åœ¨è®¡ç®—ä¸Šæ•ˆç‡è¾ƒä½ã€‚åœ¨è®¸å¤šæµè¡Œçš„LLMsä¸­å­˜åœ¨ä¸€ç§å¸¸è§çš„å¤±è´¥æ¨¡å¼â€”â€”è¿‡åº¦æ€è€ƒï¼Œå³æ¨¡å‹å³ä½¿å¯¹äºç®€å•æŸ¥è¯¢ä¹Ÿä¼šç”Ÿæˆå†—é•¿ä¸”ç¦»é¢˜çš„æ¨ç†ç—•è¿¹ã€‚è¿‘æœŸä¸€äº›å·¥ä½œé€šè¿‡å¼•å…¥å›ºå®šçš„ä»¤ç‰Œçº¦æŸæ¥ç¼“è§£è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œä½†è¿™åˆå¯èƒ½å¯¼è‡´æ€è€ƒä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ›´éš¾çš„é—®é¢˜æ—¶ã€‚æ­¤å¤–ï¼Œç›®å‰ç¼ºä¹åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­ç³»ç»Ÿåœ°è§£å†³è¿‡åº¦æ€è€ƒå’Œæ€è€ƒä¸è¶³è¿™ä¸¤ä¸ªé—®é¢˜çš„ç ”ç©¶ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ»ç”»LLMsçš„å†…éƒ¨æ¨ç†å’Œæ¨æ–­æœºåˆ¶ï¼Œå¹¶å¼•å¯¼å…¶æ ¹æ®ä»»åŠ¡å¤æ‚æ€§è‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè´å¶æ–¯é¢„ç®—åˆ†é…æ¨¡å‹ï¼ˆBBAMï¼‰
å°†æ¨ç†å»ºæ¨¡ä¸ºä¸€ç³»åˆ—å…·æœ‰ä¸åŒä¸ç¡®å®šæ€§çš„å­é—®é¢˜ï¼ŒæŠŠæ¨ç†è¿‡ç¨‹æ¦‚å¿µåŒ–ä¸ºç”±ä¸åŒç¨‹åº¦ä¸ç¡®å®šæ€§ç‰¹å¾çš„å­é—®é¢˜åºåˆ—ï¼Œå¹¶ä¸ºä¸ç¡®å®šæ€§è¾ƒé«˜çš„å­é—®é¢˜åˆ†é…æ›´å¤šçš„è®¡ç®—é¢„ç®—ï¼Œä»è€Œå®ç°æ›´æ ¡å‡†å’Œé«˜æ•ˆçš„æ¨æ–­ã€‚åŒæ—¶ï¼Œé€šè¿‡å¯¹ä¸ç¡®å®šæ€§çš„åˆ†æï¼ˆä»¥æ¨¡å‹åœ¨æ¯ä¸€æ­¥çš„è¾¹é™…ä¸‹ä¸€ä¸ªä»¤ç‰Œåˆ†å¸ƒçš„ç†µæ¥é‡åŒ–ï¼‰ï¼Œå‘ç°é«˜ç†µå¸¸ä¸ä¸å¿…è¦çš„æ·±åº¦æ¨ç†ï¼ˆè¿‡åº¦æ€è€ƒï¼‰ç›¸å…³ï¼Œè€Œæ—©æœŸæ­¥éª¤ä¸­è§‚å¯Ÿåˆ°çš„ä½ç†µå¾€å¾€å¯¼è‡´æ¨ç†è¿‡æ—©æˆªæ–­ï¼ˆæ€è€ƒä¸è¶³ï¼‰ï¼Œä¸ç¡®å®šæ€§å¯ä½œä¸ºåŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦çš„æœ‰ä»·å€¼ä¿¡å·ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºPlan-and-Budgetæ¡†æ¶
è¯¥æ¡†æ¶ä¸æ¨¡å‹æ— å…³ä¸”åœ¨æµ‹è¯•æ—¶ä½¿ç”¨ï¼Œç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆã€‚åœ¨è®¡åˆ’é˜¶æ®µï¼Œæ¨¡å‹å°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºä¸€ç³»åˆ—å­é—®é¢˜ï¼Œä¸ºç»“æ„åŒ–æ¨ç†æä¾›è½¯æ”¯æ¶ï¼›åœ¨é¢„ç®—é˜¶æ®µï¼Œéµå¾ªBBAMåŸåˆ™ï¼Œåº”ç”¨åŸºäºç®€åŒ–è¡°å‡çš„è°ƒåº¦ç­–ç•¥ï¼Œæ ¹æ®æ¯ä¸ªå­é—®é¢˜çš„ä¸ç¡®å®šæ€§æ¨¡å¼åŠ¨æ€åˆ†é…ä»¤ç‰Œé¢„ç®—ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†EÂ³æŒ‡æ ‡ï¼ˆæ•ˆç‡æ„ŸçŸ¥æœ‰æ•ˆæ€§è¯„ä¼°åˆ†æ•°ï¼‰ï¼Œç”¨äºæ•æ‰æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œè¯¥æŒ‡æ ‡èƒ½æ›´å…¨é¢åœ°è¡¡é‡æ¨æ–­æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªæœ€å…ˆè¿›çš„LLMsï¼ˆDS - Qwen - 32Bã€QwQ - 32Bã€DS - LLaMA - 70Bå’ŒOpenAI o4 - miniï¼‰ä¸Šï¼Œé’ˆå¯¹æ•°å­¦æ¨ç†ã€æŒ‡ä»¤è·Ÿéšå’Œä»£ç†è§„åˆ’ä¸‰ä¸ªä»£è¡¨æ€§ä»»åŠ¡é¢†åŸŸè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚è¯¥æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒï¼Œä»…ä¾é æç¤ºå’Œè½»é‡çº§è§„åˆ’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPlan - and - Budgetåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡æŒç»­æ”¹è¿›æ‰€æœ‰LLMsï¼Œä¸å¼ºåŸºçº¿ç›¸æ¯”ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®æ€§æå‡é«˜è¾¾70%ï¼Œä»¤ç‰Œä½¿ç”¨é‡å‡å°‘é«˜è¾¾39%ï¼Œç»¼åˆæ•ˆç‡æå‡ï¼ˆä»¥EÂ³è¡¡é‡ï¼‰é«˜è¾¾187.5%ã€‚ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä»£ç†è§„åˆ’ä»»åŠ¡é¢†åŸŸï¼Œè¾ƒå°çš„DS - Qwen - 32Bæ¨¡å‹ä½¿ç”¨Plan - and - Budgetåï¼ŒEÂ³ä»0.16æå‡åˆ°0.46ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ç¼©å°äº†ä¸è¾ƒå¤§çš„DS - LLaMA - 70Bæ¨¡å‹ï¼ˆEÂ³ = 0.50ï¼‰çš„å·®è·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ä¸ç¡®å®šæ€§çš„åº”ç”¨**ï¼šé€šè¿‡é‡åŒ–ä¸ç¡®å®šæ€§æ¥æŒ‡å¯¼è®¡ç®—èµ„æºçš„åˆ†é…ï¼Œä¸ºä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥å€Ÿé‰´è¿™ç§æ€è·¯ï¼Œæ ¹æ®ä»»åŠ¡çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚
2. **ç»“æ„åŒ–æ¨ç†ä¸é¢„ç®—åˆ†é…**ï¼šå°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå­é—®é¢˜å¹¶è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼ŒåŒæ—¶æ ¹æ®å­é—®é¢˜çš„ä¸ç¡®å®šæ€§åˆ†é…ä»¤ç‰Œé¢„ç®—ï¼Œè¿™ç§æ–¹æ³•æœ‰åŠ©äºæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å¯¹äºå…¶ä»–éœ€è¦å¤„ç†å¤æ‚ä»»åŠ¡çš„æ¨¡å‹æˆ–ç³»ç»Ÿï¼Œä¹Ÿå¯ä»¥è€ƒè™‘é‡‡ç”¨ç±»ä¼¼çš„ç»“æ„åŒ–å’Œé¢„ç®—åˆ†é…ç­–ç•¥ã€‚
3. **æ¨¡å‹æ— å…³çš„æ–¹æ³•**ï¼šPlan - and - Budgetæ¡†æ¶ä¸æ¨¡å‹æ— å…³ï¼Œä»…ä¾é æç¤ºå’Œè½»é‡çº§è§„åˆ’å°±èƒ½æå‡æ¨¡å‹æ€§èƒ½ï¼Œè¿™ä¸ºåœ¨ä¸æ”¹å˜æ¨¡å‹ç»“æ„å’Œå‚æ•°çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹æ•ˆç‡æä¾›äº†ä¸€ç§å¯è¡Œçš„é€”å¾„ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚
``` 

## generalizing-test-time-compute-optimal-scaling-as-an-optimizable-graph
### Abstract
Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢æµ‹è¯•æ—¶è®¡ç®—æœ€ä¼˜ç¼©æ”¾çš„å…¨æ–°å›¾ä¼˜åŒ–æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ†é…é¢å¤–è®¡ç®—èµ„æºæ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ€§èƒ½ï¼Œå¸¸è§æ–¹å¼æœ‰å¹¶è¡Œã€é¡ºåºæˆ–æ··åˆç¼©æ”¾ã€‚ç„¶è€Œï¼Œå…ˆå‰ç ”ç©¶å¸¸å‡å®šå›ºå®šçš„åä½œæ¶æ„ï¼ˆå¦‚æ‹“æ‰‘ç»“æ„ï¼‰ä¸”ä»…ä½¿ç”¨å•ä¸€æ¨¡å‹ï¼Œå¿½è§†äº†ä¸åŒä»»åŠ¡ä¸‹æœ€ä¼˜æ¶æ„å’Œæ¨¡å‹ç»„åˆå­˜åœ¨å·®å¼‚ã€‚ä¾‹å¦‚ï¼Œä¸åŒä»»åŠ¡å¯¹æ¶æ„æ¨¡å¼æœ‰ä¸åŒåå¥½ï¼ŒMATHä»»åŠ¡æ›´é’çæ··åˆç»“æ„ï¼ŒMMLUä»»åŠ¡åˆ™åœ¨çº¯å¹¶è¡Œç»“æ„ä¸‹è¡¨ç°æ›´å¥½ï¼›ä¸åŒä»»åŠ¡å¯¹æ¨¡å‹ç»„åˆçš„éœ€æ±‚ä¹Ÿä¸åŒï¼ŒMATHä»1B - 3Bæ¨¡å‹çš„æ··åˆä¸­å—ç›Šï¼ŒMMLUåˆ™æ›´å€¾å‘äºå•ä¸ª8Bæ¨¡å‹ã€‚å› æ­¤ï¼Œåœ¨å›ºå®šé¢„ç®—ä¸‹å¯»æ‰¾è®¡ç®—æœ€ä¼˜çš„æ¨¡å‹ç»„åˆå’Œæ¶æ„æˆä¸ºäºŸå¾…ç ”ç©¶çš„æ–°é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé—®é¢˜å½¢å¼åŒ–
å°†åœ¨TTSä¸­å¯»æ‰¾è®¡ç®—æœ€ä¼˜çš„æ¨¡å‹ç»„åˆå’Œæ¶æ„é—®é¢˜å½¢å¼åŒ–ä¸ºå¤šLLMåä½œå›¾ã€‚å›¾ä¸­èŠ‚ç‚¹å¯¹è§’è‰²å’ŒLLMæ¨¡å‹åˆ†é…è¿›è¡Œç¼–ç ï¼Œè¾¹æ•æ‰ä¿¡æ¯æµåŠ¨ã€‚è¿™ç§å›¾è§†å›¾ä¸ºåŠ¨æ€ä¼˜åŒ–æä¾›äº†ç³»ç»ŸåŸºç¡€ï¼Œä½†é¢ä¸´ç»„åˆæœç´¢ç©ºé—´å·¨å¤§å’Œéœ€æ ¹æ®ä»»åŠ¡å®šåˆ¶è®¾è®¡ä¸¤å¤§æŒ‘æˆ˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºAgent - REINFORCEæ¡†æ¶
é€šè¿‡å…ˆå¯¼å®éªŒå¾—å‡ºå…³äºTTSåä½œå›¾çš„ä¸‰ä¸ªç»éªŒæ€§è§è§£ï¼šæœ‰æ•ˆåä½œå¯¹ç‰¹å®šæ¨¡å‹ç»„åˆæœ‰æ˜æ˜¾åå¥½ï¼›å®½åº¦å’Œæ·±åº¦å­˜åœ¨ä»»åŠ¡ç›¸å…³çš„æœ€ä¼˜å€¼ï¼›å›¾çš„å®½åº¦å’Œæ·±åº¦ç›¸äº’ä¾èµ–ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæå‡ºAgent - REINFORCEæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±LLM - agentå¢å¼ºçš„æ¡†æ¶ï¼Œå°†é‡‡æ · - æ¢¯åº¦ - æ›´æ–°æ˜ å°„ä¸ºé‡‡æ · - åé¦ˆ - æ›´æ–°ï¼Œå…¶ä¸­åé¦ˆä½œä¸ºæ–‡æœ¬æ¢¯åº¦æ¥æ›´æ–°æ¦‚ç‡å›¾ï¼Œä»¥é«˜æ•ˆæœç´¢æœ€ä¼˜å¤šLLMåä½œå›¾ã€‚è¯¥æ¡†æ¶åŒ…å«Agentã€Archiveå’ŒEnvironmentä¸‰ä¸ªç»„ä»¶ï¼ŒAgentæ ¹æ®ç»éªŒè§è§£åˆå§‹åŒ–æœ‰å‰æ™¯çš„æ¨¡å‹æ—å’Œå¤§å°å¹¶æ›´æ–°åˆ†å¸ƒï¼ŒArchiveè®°å½•ç»“æœï¼ŒEnvironmentè¯„ä¼°å¹¶è¿”å›åé¦ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒAgent - REINFORCEåœ¨æ ·æœ¬æ•ˆç‡å’Œæœç´¢æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿå’ŒåŸºäºLLMçš„åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‡†ç¡®æ€§å’Œæ¨ç†å»¶è¿Ÿçš„è”åˆç›®æ ‡ä¸‹æœ‰æ•ˆè¯†åˆ«æœ€ä¼˜å›¾ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **é—®é¢˜ç ”ç©¶è§†è§’**ï¼šå…³æ³¨åˆ°ä¸åŒä»»åŠ¡ä¸‹TTSæ¶æ„å’Œæ¨¡å‹ç»„åˆçš„é€‚åº”æ€§é—®é¢˜ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–ç ”ç©¶æä¾›äº†æ–°çš„æ€è€ƒæ–¹å‘ï¼Œæç¤ºç ”ç©¶è€…åœ¨è®¾è®¡æ¨¡å‹å’Œæ¶æ„æ—¶éœ€è€ƒè™‘ä»»åŠ¡ç‰¹å¼‚æ€§ã€‚
2. **æ–¹æ³•æ¡†æ¶**ï¼šæå‡ºçš„Agent - REINFORCEæ¡†æ¶å°†LLMä¸æ¦‚ç‡å›¾ä¼˜åŒ–ç›¸ç»“åˆï¼Œä¸ºè§£å†³å¤æ‚çš„æœç´¢é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œåœ¨å…¶ä»–æ¶‰åŠå¤§è§„æ¨¡æœç´¢å’Œä¼˜åŒ–çš„åœºæ™¯ä¸­å¯èƒ½å…·æœ‰å€Ÿé‰´ä»·å€¼ã€‚
3. **ç»éªŒè§è§£**ï¼šé€šè¿‡å…ˆå¯¼å®éªŒå¾—å‡ºçš„å…³äºTTSåä½œå›¾çš„ç»éªŒæ€§è§è§£ï¼Œå¯¹ç†è§£å¤§è¯­è¨€æ¨¡å‹åä½œæœºåˆ¶ä»¥åŠæŒ‡å¯¼æ¨¡å‹æ¶æ„å’Œç»„åˆçš„è®¾è®¡å…·æœ‰ä¸€å®šçš„å‚è€ƒæ„ä¹‰ã€‚ 
``` 

## on-predictability-of-reinforcement-learning-dynamics-for-large-language-models
### Abstract
Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \textgreater 96\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ åŠ¨åŠ›å­¦ï¼Œå¼€å¯é«˜æ•ˆè®­ç»ƒæ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—çš„å¿«é€Ÿè¿›å±•ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ç›Šäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é©±åŠ¨çš„è®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºRLè®­ç»ƒè¿‡ç¨‹ä¸­æ½œåœ¨çš„å‚æ•°åŠ¨åŠ›å­¦ç†è§£ç”šå°‘ã€‚è™½ç„¶å·²æœ‰ä¸€äº›å…³äºRLè®­ç»ƒåLLMsçš„è§£é‡Šæ€§ç ”ç©¶ï¼Œå¦‚ç¥ç»å…ƒå½’å› ã€ç”µè·¯åˆ†æç­‰ï¼Œä½†è¿™äº›ç ”ç©¶ä¸»è¦èšç„¦äºè®­ç»ƒçš„ç»ˆç‚¹ï¼Œå¯¹RLè¿‡ç¨‹æœ¬èº«çš„æ¢ç´¢ä¸è¶³ã€‚ç†è§£RLè¿‡ç¨‹ä¸­çš„å‚æ•°åŠ¨åŠ›å­¦ï¼Œä¸ä»…æœ‰åŠ©äºä¼˜åŒ–RLèŒƒå¼ï¼Œè¿˜èƒ½ä¸ºæ¨ç†èƒ½åŠ›çš„æ¶Œç°æä¾›è§è§£ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ­ç¤ºRLè¿‡ç¨‹çš„é»‘ç®±ï¼Œæ¢ç©¶RLå¼•å¯¼çš„å‚æ•°æ›´æ–°æ˜¯å¦éµå¾ªä¸€è‡´æ¨¡å¼ï¼Œä»¥åŠè¿™äº›æ¨¡å¼å¦‚ä½•äº§ç”Ÿæ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°Rank - 1 Dominanceå±æ€§
é€šè¿‡å¯¹å‚æ•°æ›´æ–°çŸ©é˜µÎ”ğ‘¾è¿›è¡Œé€æ­¥åˆ†æï¼Œè¿ç”¨æ­£äº¤å­ç©ºé—´æŠ•å½±ç­‰æ•°å­¦å·¥å…·ï¼Œå¯¹Î”ğ‘¾è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰åå‘ç°ï¼Œå…¶é¡¶éƒ¨å¥‡å¼‚å­ç©ºé—´ï¼ˆå³Rank - 1å­ç©ºé—´ï¼‰å‡ ä¹å®Œå…¨å†³å®šäº†RLå¸¦æ¥çš„æ¨ç†èƒ½åŠ›æå‡ã€‚å°†Î”ğ‘¾çš„Rank - 1åˆ†é‡æ·»åŠ åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œå°±è¶³ä»¥æ¢å¤å‡ ä¹æ‰€æœ‰RLè®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡ï¼Œä¸”è¿™ä¸€å±æ€§ä¸ä»…åœ¨è®­ç»ƒæ”¶æ•›æ—¶æˆç«‹ï¼Œåœ¨RLè®­ç»ƒçš„ä»»ä½•ä¸­é—´æ­¥éª¤éƒ½æˆç«‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå‘ç°Rank - 1 Linear Dynamicså±æ€§
è¿ç”¨åæœ€å°äºŒä¹˜æ³•ï¼ˆPLSï¼‰è·Ÿè¸ªRank - 1å­ç©ºé—´åœ¨è®­ç»ƒæ­¥éª¤ä¸­çš„ç»´åº¦è½¨è¿¹ï¼Œè§‚å¯Ÿåˆ°å…¶å‘ˆç°å‡ ä¹ä¸¥æ ¼çš„çº¿æ€§ä¸Šå‡è¶‹åŠ¿ï¼Œçº¿æ€§é€Ÿç‡æŒ‡æ ‡ğ‘…Â²è¶…è¿‡0.96ï¼Œä½¿å¾—å¯ä»¥ä»è®­ç»ƒæ—©æœŸçš„çŸ­çª—å£å‡†ç¡®é¢„æµ‹ç›®æ ‡æ­¥éª¤çš„Rank - 1å­ç©ºé—´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºAlphaRLåŠ é€Ÿæ¡†æ¶
åŸºäºä¸Šè¿°ä¸¤ä¸ªå‘ç°ï¼Œæå‡ºAlphaRLï¼Œè¿™æ˜¯ä¸€ä¸ªæ’ä»¶å¼åŠ é€Ÿæ¡†æ¶ã€‚å¯¹äºä»»ä½•åº”ç”¨äºLLMsçš„RLç®—æ³•ï¼ŒAlphaRLåªéœ€ä¸€ä¸ªæ—©æœŸè®­ç»ƒçª—å£æ¥è®¡ç®—Î”ğ‘¾çš„åˆå§‹Rank - 1å­ç©ºé—´åŠå…¶çº¿æ€§å¢é•¿ç‡ï¼Œç„¶åç›´æ¥é¢„æµ‹è¾¾åˆ°ç›®æ ‡æ¨ç†æ€§èƒ½çš„æœ€ç»ˆå‚æ•°æ›´æ–°ï¼Œè€Œæ— éœ€è¿è¡Œå®Œæ•´çš„è®­ç»ƒè®¡åˆ’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨8ä¸ªä¸åŒè§„æ¨¡ï¼ˆå‚æ•°ä»7Båˆ°32Bï¼‰çš„æ¨¡å‹ï¼ˆå¦‚Qwen2.5å’ŒQwen3ï¼‰ä¸Šï¼Œä½¿ç”¨7ç§ä¸åŒçš„å…ˆè¿›è®­ç»ƒç®—æ³•ï¼ˆå¦‚RLOOã€GRPOå’ŒDAPOï¼‰è¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚å¯¹äºRank - 1 Dominanceå±æ€§ï¼ŒRank - 1å­ç©ºé—´å¹³å‡å¯æ¢å¤99.17%çš„æ¨ç†èƒ½åŠ›ï¼›å¯¹äºRank - 1 Linear Dynamicså±æ€§ï¼ŒRank - 1å­ç©ºé—´æ¼”åŒ–çš„çº¿æ€§åº¦å¹³å‡ğ‘…Â²ä¸º0.914ï¼ŒåŸºäºæ—©æœŸçŠ¶æ€å¯¹åæœŸçŠ¶æ€çš„é¢„æµ‹å¹³å‡è¯¯å·®å°äº5%ã€‚AlphaRLåœ¨ä¿æŒ>96%æœ€ç»ˆæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾2.5å€çš„åŠ é€Ÿã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç ”ç©¶æ€è·¯**ï¼šæœ¬æ–‡é€šè¿‡å¯¹å‚æ•°æ›´æ–°çŸ©é˜µçš„æ·±å…¥åˆ†æï¼Œå‘ç°äº†RLè®­ç»ƒè¿‡ç¨‹ä¸­å‚æ•°åŠ¨åŠ›å­¦çš„ä¸¤ä¸ªé‡è¦å±æ€§ï¼Œä¸ºç ”ç©¶LLMsçš„è®­ç»ƒè¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§ä»æ•°å­¦è§’åº¦å‰–æè®­ç»ƒè¿‡ç¨‹çš„æ€è·¯ã€‚
2. **åŠ é€Ÿæ¡†æ¶**ï¼šAlphaRLçš„æå‡ºä¸ºRLè®­ç»ƒæä¾›äº†ä¸€ç§æ— éœ€é¢å¤–æ¨¡å—ã€è¶…å‚æ•°è°ƒæ•´æˆ–äººå·¥å¹²é¢„çš„åŠ é€Ÿæ–¹æ¡ˆï¼Œä¸”ä¸ç°æœ‰åŠ é€ŸèŒƒå¼å…¼å®¹ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œç›¸å…³ç ”ç©¶å¯è€ƒè™‘å€Ÿé‰´å…¶æ€è·¯æ¥ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ã€‚
3. **å®éªŒéªŒè¯**ï¼šé€šè¿‡åœ¨å¤šç§æ¨¡å‹å’Œç®—æ³•ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒæ¥éªŒè¯å‘ç°çš„å±æ€§å’Œæå‡ºçš„æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œè¿™ç§å…¨é¢çš„å®éªŒéªŒè¯æ–¹å¼å€¼å¾—å€Ÿé‰´ï¼Œä»¥ç¡®ä¿ç ”ç©¶æˆæœçš„å¯é æ€§å’Œæ™®é€‚æ€§ã€‚
```

## in-the-flow-agentic-system-optimization-for-effective-planning-and-tool-use
### Abstract
Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | AgentFlowï¼šå¼€å¯å¯è®­ç»ƒçš„æ™ºèƒ½ä½“ç³»ç»Ÿä¼˜åŒ–æ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ çš„æ¨åŠ¨ä¸‹ï¼Œæ¨ç†èƒ½åŠ›å–å¾—æ˜¾è‘—è¿›å±•ã€‚å…¶ä¸­ï¼Œå·¥å…·å¢å¼ºæ–¹æ³•é€šè¿‡å°†æ¨ç†ä¸å·¥å…·è°ƒç”¨äº¤ç»‡åœ¨å®Œæ•´ä¸Šä¸‹æ–‡ä¸‹ï¼Œè®­ç»ƒå•ä¸€ã€æ•´ä½“çš„ç­–ç•¥ï¼Œä»¥å®ç°çŸ¥è¯†æ£€ç´¢å’Œç²¾ç¡®è®¡ç®—ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨é¢å¯¹é•¿è§†é‡ä»»åŠ¡ã€å¤šæ ·åŒ–å·¥å…·æ—¶ï¼Œè®­ç»ƒç¨³å®šæ€§å·®ï¼Œä¸”åœ¨æ–°åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›å¼±ã€‚æ™ºèƒ½ä½“ç³»ç»Ÿè™½é€šè¿‡å°†å·¥ä½œåˆ†è§£åˆ°ä¸“é—¨æ¨¡å—æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ä»æ— éœ€è®­ç»ƒæˆ–ä¾èµ–ä¸å¤šè½®äº¤äº’å®æ—¶åŠ¨æ€è§£è€¦çš„ç¦»çº¿è®­ç»ƒï¼Œéš¾ä»¥åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°ç¨³å¥åè°ƒã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨å·¥å…·é›†æˆçš„æ™ºèƒ½ä½“ç³»ç»Ÿä¸­å­¦ä¹ é•¿è§†é‡æ¨ç†å¹¶å¤„ç†ç¨€ç–å¥–åŠ±ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAgentFlowæ¡†æ¶
AgentFlowæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„ã€åœ¨æµç¨‹ä¸­çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”±è§„åˆ’å™¨ã€æ‰§è¡Œå™¨ã€éªŒè¯å™¨å’Œç”Ÿæˆå™¨å››ä¸ªä¸“é—¨æ¨¡å—ç»„æˆã€‚è¿™äº›æ¨¡å—é€šè¿‡å…±äº«çš„åŠ¨æ€è®°å¿†å’Œå·¥å…·é›†ï¼Œåœ¨å¤šè½®äº¤äº’ä¸­è¿­ä»£åä½œã€‚ä¸å…ˆå‰çš„æ™ºèƒ½ä½“ç³»ç»Ÿä¸åŒï¼ŒAgentFlowåœ¨å®æ—¶å¤šè½®å¾ªç¯ä¸­ç›´æ¥å¯¹è§„åˆ’å™¨è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½åŠ¨æ€é€‚åº”å·¥å…·è°ƒç”¨ã€éªŒè¯å™¨ä¿¡å·å’Œè®°å¿†æ›´æ–°æ‰€å¡‘é€ çš„è½¨è¿¹ã€‚åŠ¨æ€è®°å¿†ä½œä¸ºæ¨ç†è¿‡ç¨‹çš„ç¡®å®šæ€§ã€ç»“æ„åŒ–è®°å½•ï¼Œå®ç°äº†é€æ˜çš„çŠ¶æ€è·Ÿè¸ªã€å¯æ§çš„è¡Œä¸ºå’Œæœ‰ç•Œçš„ä¸Šä¸‹æ–‡å¢é•¿ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºFlow - GRPOç®—æ³•
ä¸ºåœ¨æ™ºèƒ½ä½“ç³»ç»Ÿä¸­å¯¹è§„åˆ’å™¨è¿›è¡Œç­–ç•¥è®­ç»ƒï¼Œå…‹æœç¨€ç–ã€è½¨è¿¹çº§å¥–åŠ±å›ºæœ‰çš„é•¿è§†é‡ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œæå‡ºFlow - based Group Refined Policy Optimizationï¼ˆFlow - GRPOï¼‰ç®—æ³•ã€‚è¯¥ç®—æ³•åœ¨æµç¨‹å±•å¼€ä¸­è¿è¡Œï¼Œæ•è·å®æ—¶ç³»ç»Ÿå¼•å‘çš„çŠ¶æ€ã€åŠ¨ä½œå’Œå·¥å…·äº‹ä»¶çš„å®Œæ•´è½¨è¿¹ã€‚é€šè¿‡ä¸ºæ•´ä¸ªè½¨è¿¹åˆ†é…å•ä¸€ã€å¯éªŒè¯çš„æœ€ç»ˆç»“æœå¥–åŠ±ï¼Œå¹¶å°†å…¶å¹¿æ’­åˆ°æ¯ä¸€è½®ï¼Œå°†å¤šè½®å¼ºåŒ–å­¦ä¹ æŒ‘æˆ˜è½¬åŒ–ä¸ºä¸€ç³»åˆ—å•è½®æ›´æ–°ã€‚åŒæ—¶ï¼Œç»“åˆç»„å½’ä¸€åŒ–ä¼˜åŠ¿æ¥ç¨³å®šè®­ç»ƒï¼Œå®ç°ç¨³å¥çš„ä¿¡ç”¨åˆ†é…ï¼Œä½¿è§„åˆ’å™¨èƒ½ä»ç¨€ç–åé¦ˆä¸­å­¦ä¹ æœ‰æ•ˆçš„é•¿è§†é‡ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åä¸ªæ¶µç›–ä¸åŒæ¨ç†é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥7Bè§„æ¨¡ä¸ºéª¨å¹²çš„AgentFlowè¡¨ç°å‡ºè‰²ã€‚åœ¨çŸ¥è¯†å¯†é›†å‹æœç´¢ä»»åŠ¡ä¸Šï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†14.9%ï¼›åœ¨æ›´å¹¿æ³›çš„æ™ºèƒ½ä½“ä»»åŠ¡ä¸Šï¼Œæé«˜äº†14.0%ï¼›åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œæé«˜äº†14.5%ï¼›åœ¨ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œæé«˜äº†4.1%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ7Béª¨å¹²ç³»ç»Ÿç”šè‡³åœ¨æ‰€æœ‰é¢†åŸŸè¶…è¶Šäº†çº¦200Bå‚æ•°çš„GPT - 4oã€‚è¿›ä¸€æ­¥åˆ†æè¯å®äº†åœ¨æµç¨‹ä¼˜åŒ–çš„ç›Šå¤„ï¼ŒåŒ…æ‹¬æ”¹è¿›çš„è§„åˆ’ã€å¢å¼ºçš„å·¥å…·è°ƒç”¨å¯é æ€§ï¼Œä»¥åŠä¸æ¨¡å‹è§„æ¨¡å’Œæ¨ç†è½®æ•°çš„æ­£å‘æ‰©å±•å…³ç³»ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AgentFlowçš„å¯è®­ç»ƒæ¡†æ¶å’ŒFlow - GRPOç®—æ³•ä¸ºè§£å†³é•¿è§†é‡æ¨ç†å’Œç¨€ç–å¥–åŠ±é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶é€šè¿‡æ¨¡å—åä½œå’ŒåŠ¨æ€è®°å¿†å®ç°çš„å¯æ§è¡Œä¸ºå’Œé€æ˜è·Ÿè¸ªï¼Œä»¥åŠå°†å¤šè½®ä¼˜åŒ–è½¬åŒ–ä¸ºå•è½®æ›´æ–°çš„ä¿¡ç”¨åˆ†é…ç­–ç•¥ï¼Œå¯¹äºå¼€å‘æ›´é«˜æ•ˆã€ç¨³å¥çš„æ™ºèƒ½ä½“ç³»ç»Ÿå…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯å‚è€ƒå…¶æ¶æ„è®¾è®¡å’Œè®­ç»ƒæ–¹æ³•ï¼Œæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°å’Œé€‚åº”æ€§ã€‚
``` 

## let-s-think-in-two-steps--mitigating-agreement-bias-in-mllms-with-self-grounded-verification
### Abstract
Verifiers -- functions assigning rewards to agent behavior -- have been key for AI progress in domains like math and board games. However, extending these gains to domains without clear-cut success criteria (e.g.,computer use) remains a challenge: while humans can recognize suitable outcomes, translating this intuition into scalable rules is non-trivial. Multimodal Large Language Models(MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers of agent trajectories across web navigation, computer use, and robotic manipulation, and identify a critical limitation: agreement bias, a strong tendency for MLLMs to favor information in their context window, often generating chains of thought to rationalize flawed behavior. This bias is pervasive across models, resilient to test-time scaling, and can impact several methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs despite MLLMs showing strong, human-aligned priors on desired behavior. To address this, we propose Self-Grounded Verification (SGV), a lightweight method that enables more effective use of MLLMs' knowledge and reasoning by harnessing their own sampling mechanisms via unconditional and conditional generation. SGV operates in two steps: first, the MLLM is elicited to retrieve broad priors about task completion, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in accuracy and failure detection rates, and can perform real-time supervision of heterogeneous agents, boosting task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting a new state of the art on the benchmark, surpassing the previous best by 48%.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´å¤šæ¨¡æ€å¤§æ¨¡å‹éªŒè¯åå·®ï¼ŒSGVå¼€å¯æ™ºèƒ½éªŒè¯æ–°ç¯‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ•°å­¦å’Œæ£‹ç›˜æ¸¸æˆç­‰é¢†åŸŸï¼Œä¸ºæ™ºèƒ½ä½“è¡Œä¸ºåˆ†é…å¥–åŠ±çš„éªŒè¯å™¨æ˜¯æ¨åŠ¨äººå·¥æ™ºèƒ½è¿›æ­¥çš„å…³é”®å› ç´ ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æˆæœæ‰©å±•åˆ°æ²¡æœ‰æ˜ç¡®æˆåŠŸæ ‡å‡†çš„é¢†åŸŸï¼ˆå¦‚è®¡ç®—æœºä½¿ç”¨ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å°½ç®¡äººç±»èƒ½å¤Ÿè¯†åˆ«åˆé€‚çš„ç»“æœï¼Œä½†å°†è¿™ç§ç›´è§‰è½¬åŒ–ä¸ºå¯æ‰©å±•çš„è§„åˆ™å¹¶éæ˜“äº‹ã€‚å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‡­å€Ÿå…¶ä¸–ç•ŒçŸ¥è¯†ã€ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ä»¥åŠæ¨ç†èƒ½åŠ›ï¼Œæˆä¸ºäº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ä½†åœ¨è¯„ä¼°MLLMsä½œä¸ºæ™ºèƒ½ä½“è½¨è¿¹éªŒè¯å™¨æ—¶ï¼Œå‘ç°äº†ä¸€ä¸ªå…³é”®é™åˆ¶â€”â€”ä¸€è‡´æ€§åå·®ï¼Œå³MLLMså€¾å‘äºåçˆ±å…¶ä¸Šä¸‹æ–‡çª—å£ä¸­çš„ä¿¡æ¯ï¼Œå¸¸ä¸ºæœ‰ç¼ºé™·çš„è¡Œä¸ºç”Ÿæˆæ¨ç†é“¾æ¥åˆç†åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°ä¸€è‡´æ€§åå·®é—®é¢˜
é€šè¿‡åœ¨ç½‘é¡µå¯¼èˆªã€è®¡ç®—æœºä½¿ç”¨å’Œæœºå™¨äººæ“ä½œç­‰ä»»åŠ¡ä¸­è¯„ä¼°MLLMsä½œä¸ºæ™ºèƒ½ä½“è½¨è¿¹éªŒè¯å™¨ï¼Œè¯†åˆ«å‡ºMLLMså­˜åœ¨ä¸€è‡´æ€§åå·®è¿™ä¸€å…³é”®é™åˆ¶ï¼Œä¸”è¯¥åå·®åœ¨å„ç§æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹æµ‹è¯•æ—¶çš„ç¼©æ”¾å…·æœ‰æŠ—æ€§ï¼Œè¿˜ä¼šå½±å“ä¾èµ–MLLMsä½œä¸ºè¯„ä¼°å™¨çš„æ–¹æ³•ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSelf - Grounded Verificationï¼ˆSGVï¼‰æ–¹æ³•
è¿™æ˜¯ä¸€ç§è½»é‡çº§æ–¹æ³•ï¼Œé€šè¿‡æ— æ¡ä»¶å’Œæ¡ä»¶ç”Ÿæˆåˆ©ç”¨MLLMsè‡ªèº«çš„é‡‡æ ·æœºåˆ¶ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨MLLMsçš„çŸ¥è¯†å’Œæ¨ç†ã€‚SGVåˆ†ä¸¤æ­¥æ“ä½œï¼šé¦–å…ˆï¼Œä¿ƒä½¿MLLMæ£€ç´¢ä¸ä»»åŠ¡å®Œæˆç›¸å…³çš„å¹¿æ³›å…ˆéªŒçŸ¥è¯†ï¼Œä¸”ç‹¬ç«‹äºæ­£åœ¨è¯„ä¼°çš„æ•°æ®ï¼›ç„¶åï¼Œä»¥è‡ªæˆ‘ç”Ÿæˆçš„å…ˆéªŒä¸ºæ¡ä»¶ï¼Œå¯¹å€™é€‰è½¨è¿¹è¿›è¡Œæ¨ç†å’Œè¯„ä¼°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¥è‡ªVisualWebArenaå’ŒOSWorldçš„1200å¤šä¸ªæ™ºèƒ½ä½“è½¨è¿¹ä¸Šï¼ŒSGVæé«˜äº†å¤šä¸ªMLLMså’ŒLRMsçš„éªŒè¯æ€§èƒ½ï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾17ä¸ªç™¾åˆ†ç‚¹ï¼ŒçœŸé˜´æ€§è¯†åˆ«ç‡æå‡é«˜è¾¾20ä¸ªç™¾åˆ†ç‚¹ï¼Œä¼˜äºåˆ©ç”¨æ—¢å®šæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯å’Œè®¿é—®ç‰¹æƒä¿¡æ¯æŒ‡ä»¤çš„åŸºçº¿ã€‚ä¸SGVé…åˆï¼ŒReActæ™ºèƒ½ä½“åœ¨VisualWebArenaä¸Šå®ç°äº†5ä¸ªç™¾åˆ†ç‚¹çš„æå‡ï¼ˆçº¦10%çš„ç›¸å¯¹æå‡ï¼‰ï¼Œä»¥è¶…å‡ºä¹‹å‰æœ€ä½³ç»“æœ16ä¸ªç™¾åˆ†ç‚¹ï¼ˆçº¦48%çš„ç›¸å¯¹æå‡ï¼‰çš„æˆç»©å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œä¸”ä»¤ç‰Œä½¿ç”¨é‡å‡ ä¹æ²¡æœ‰å¢åŠ ã€‚åœ¨OSWorldä¸­ï¼ŒGUIä¸“å®¶UI - TARS - 1.5æå‡äº†3ä¸ªç™¾åˆ†ç‚¹ï¼ˆçº¦15%çš„ç›¸å¯¹æå‡ï¼‰ï¼Œåœ¨robomimicçš„å·¥å…·æ‚¬æŒ‚ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£ç­–ç•¥æ¯”ç¥è°•ç›‘ç£é«˜å‡º8ä¸ªç™¾åˆ†ç‚¹ï¼ˆçº¦33%çš„ç›¸å¯¹æå‡ï¼‰ã€‚æ­¤å¤–ï¼ŒSGVè¿˜ä½¿MLLMéªŒè¯å™¨èƒ½å¤Ÿæ£€æµ‹è‡ªåŠ¨è„šæœ¬è¯„ä¼°ä¸­çš„ç¼ºé™·ï¼Œå¹¶é¼“åŠ±æ™ºèƒ½ä½“å›æº¯å’Œé¿å…è´ªå©ªç­–ç•¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **é—®é¢˜å‘ç°ä¸åˆ†æ**ï¼šåœ¨ä½¿ç”¨MLLMsä½œä¸ºéªŒè¯å™¨æ—¶ï¼Œæ•é”åœ°å‘ç°ä¸€è‡´æ€§åå·®é—®é¢˜ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æ”¹è¿›æ–¹å‘ï¼Œæé†’ç ”ç©¶è€…åœ¨åº”ç”¨MLLMsæ—¶è¦å…³æ³¨å…¶æ½œåœ¨çš„åå·®é—®é¢˜ã€‚
2. **æ–¹æ³•åˆ›æ–°**ï¼šSGVæ–¹æ³•ä¸ºè§£å†³MLLMséªŒè¯è¿‡ç¨‹ä¸­çš„é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œé€šè¿‡åˆ†æ­¥éª¤åˆ©ç”¨MLLMsçš„èƒ½åŠ›ï¼Œåœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æå‡éªŒè¯å‡†ç¡®æ€§ï¼Œè¿™ç§åˆ©ç”¨æ¨¡å‹è‡ªèº«æœºåˆ¶è¿›è¡Œæ”¹è¿›çš„æ–¹å¼å€¼å¾—å€Ÿé‰´ã€‚
3. **å¤šé¢†åŸŸåº”ç”¨éªŒè¯**ï¼šé€šè¿‡åœ¨ç½‘é¡µå¯¼èˆªã€è®¡ç®—æœºä½¿ç”¨å’Œæœºå™¨äººæ“ä½œç­‰å¤šä¸ªé¢†åŸŸè¿›è¡Œå®éªŒéªŒè¯ï¼Œå±•ç¤ºäº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œä¸ºå…¶ä»–è·¨é¢†åŸŸç ”ç©¶æä¾›äº†å®éªŒè®¾è®¡å’Œè¯„ä¼°çš„å‚è€ƒã€‚
``` 

## vision-zero--scalable-vlm-self-improvement-via-strategic-gamified-self-play
### Abstract
Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in "Who Is the Spy"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Vision - Zeroï¼šå¼€å¯VLMè‡ªæå‡çš„å…¨æ–°ç¯‡ç« 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰ï¼Œå¼ºåŒ–å­¦ä¹ è™½èƒ½æœ‰æ•ˆæå‡è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äººå·¥æ„å»ºå’ŒéªŒè¯çš„åŠ³åŠ¨å¯†é›†å‹æ•°æ®é›†ï¼Œè¿™å¯¼è‡´è®­ç»ƒæˆæœ¬æé«˜ï¼Œæå¤§åœ°é™åˆ¶äº†VLMsçš„å®é™…éƒ¨ç½²ã€‚ä¸ºçªç ´è¿™ä¸€å›°å¢ƒï¼Œè®ºæ–‡æå‡ºäº†Vision - Zeroæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°VLMçš„è‡ªæˆ‘æå‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šStrategic Self - Play Frameworkï¼ˆç­–ç•¥æ€§è‡ªæˆ‘åšå¼ˆæ¡†æ¶ï¼‰
Vision - Zeroåœ¨ â€œè°æ˜¯é—´è°â€ é£æ ¼çš„æ¸¸æˆä¸­è®­ç»ƒVLMsï¼Œæ¨¡å‹åœ¨å¤šä¸ªè§’è‰²ä¸­è¿›è¡Œç­–ç•¥æ¨ç†å’Œè¡ŒåŠ¨ã€‚é€šè¿‡äº¤äº’å¼æ¸¸æˆç©æ³•ï¼Œæ¨¡å‹å¯è‡ªä¸»ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šGameplay from Arbitrary Imagesï¼ˆä»»æ„å›¾åƒçš„æ¸¸æˆç©æ³•ï¼‰
ä¸ç°æœ‰æ¸¸æˆåŒ–æ¡†æ¶ä¸åŒï¼ŒVision - Zeroèƒ½ä»ä»»æ„å›¾åƒç”Ÿæˆæ¸¸æˆï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œå¯¹ä¸åŒä»»åŠ¡è¡¨ç°å‡ºå¾ˆå¼ºçš„æ³›åŒ–æ€§ã€‚è®ºæ–‡ä½¿ç”¨åŸºäºCLEVRçš„åˆæˆåœºæ™¯ã€å›¾è¡¨å’ŒçœŸå®ä¸–ç•Œå›¾åƒè¿™ä¸‰ç§ä¸åŒç±»å‹çš„å›¾åƒæ•°æ®é›†æ¥è¯æ˜å…¶å¤šåŠŸèƒ½æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šSustainable Performance Gainï¼ˆå¯æŒç»­çš„æ€§èƒ½æå‡ï¼‰
å¼•å…¥è¿­ä»£è‡ªæˆ‘åšå¼ˆç­–ç•¥ä¼˜åŒ–ï¼ˆIterative - SPOï¼‰è¿™ä¸€æ–°é¢–çš„è®­ç»ƒç®—æ³•ï¼Œåœ¨è‡ªæˆ‘åšå¼ˆå’Œå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¹‹é—´äº¤æ›¿ï¼Œç¼“è§£äº†ä»…è‡ªæˆ‘åšå¼ˆè®­ç»ƒä¸­å¸¸è§çš„æ€§èƒ½å¹³å°æœŸé—®é¢˜ï¼Œå®ç°äº†é•¿æœŸçš„æŒç»­æ”¹è¿›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å°½ç®¡ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®ï¼ŒVision - Zeroåœ¨æ¨ç†ã€å›¾è¡¨é—®ç­”å’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ç†è§£ä»»åŠ¡ä¸Šä»å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–åŸºäºæ³¨é‡Šçš„æ–¹æ³•ã€‚åœ¨ä¸å…¶ä»–æœ€å…ˆè¿›çš„åè®­ç»ƒæ–¹æ³•çš„æ€§èƒ½æ¯”è¾ƒä¸­ï¼ŒVision - Zeroåœ¨æ¨ç†ã€å›¾è¡¨/OCRå’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ç­‰æ–¹é¢åŒæ—¶æå‡äº†æ€§èƒ½ï¼Œä¼˜äºåœ¨æ˜‚è´µçš„äººå·¥æ ‡è®°æ•°æ®é›†ä¸Šè®­ç»ƒçš„åŸºçº¿æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ•°æ®ç”Ÿæˆæ–¹å¼**ï¼šé€šè¿‡æ¸¸æˆåŒ–çš„è‡ªæˆ‘åšå¼ˆè‡ªä¸»ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä¸ºé™ä½è®­ç»ƒæˆæœ¬æä¾›äº†æ–°æ€è·¯ã€‚
2. **æ¨¡å‹è®­ç»ƒç®—æ³•**ï¼šIterative - SPOç®—æ³•åœ¨è‡ªæˆ‘åšå¼ˆå’ŒRLVRä¹‹é—´äº¤æ›¿ï¼Œæœ‰æ•ˆç¼“è§£æ€§èƒ½å¹³å°æœŸé—®é¢˜ï¼Œè¿™ç§è®­ç»ƒæ–¹å¼å¯å¯å‘å…¶ä»–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–ç­–ç•¥ã€‚
3. **æ³›åŒ–èƒ½åŠ›æå‡**ï¼šä»ä»»æ„å›¾åƒç”Ÿæˆæ¸¸æˆçš„èƒ½åŠ›ï¼Œæœ‰åŠ©äºæå‡æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„åº”ç”¨æ‹“å±•äº†æ–¹å‘ã€‚
``` 

## demystifying-and-enhancing-the-efficiency-of-large-language-model-based-search-agents
### Abstract
Large Language Model (LLM)-based search agents have shown remarkable capabilities in solving complex tasks by dynamically decomposing problems and addressing them through interleaved reasoning and retrieval. However, this interleaved paradigm introduces substantial efficiency bottlenecks. First, we observe that both highly accurate and overly approximate retrieval methods degrade system efficiency: exact search incurs significant retrieval overhead, while coarse retrieval requires additional reasoning steps during generation. Second, we identify inefficiencies in system design, including improper scheduling and frequent retrieval stalls, which lead to cascading latency -- where even minor delays in retrieval amplify end-to-end inference time. To address these challenges, we introduce SearchAgent-X, a high-efficiency inference framework for LLM-based search agents. SearchAgent-X leverages high-recall approximate retrieval and incorporates two key techniques: priority-aware scheduling and non-stall retrieval. Extensive experiments demonstrate that SearchAgent-X consistently outperforms state-of-the-art systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without compromising generation quality. SearchAgent-X is available at https://github.com/tiannuo-yang/SearchAgent-X.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜å¹¶æå‡å¤§è¯­è¨€æ¨¡å‹æœç´¢ä»£ç†æ•ˆç‡ï¼ŒSearchAgent - Xå¼ºåŠ¿ç™»åœº

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šå¸¸é‡‡ç”¨é¡ºåºçš„å…ˆæ£€ç´¢åç”Ÿæˆæ–¹æ³•ï¼Œé™åˆ¶äº†ä¸çŸ¥è¯†åº“çš„åŠ¨æ€äº¤äº’ã€‚è€ŒRAG 2.0ï¼ˆæœç´¢ä»£ç†ï¼‰åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€ä¸”è‡ªé€‚åº”åœ°äº¤é”™æ¨ç†æ­¥éª¤ä¸æ£€ç´¢è°ƒç”¨ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›å¤çš„è´¨é‡å’Œæ·±åº¦ã€‚ç„¶è€Œï¼Œè¿™ç§äº¤é”™èŒƒå¼å¸¦æ¥äº†æ˜¾è‘—çš„æ•ˆç‡ç“¶é¢ˆã€‚ä¸€æ–¹é¢ï¼Œè¿‡é«˜ç²¾åº¦å’Œè¿‡åº¦è¿‘ä¼¼çš„æ£€ç´¢æ–¹æ³•éƒ½ä¼šé™ä½ç³»ç»Ÿæ•ˆç‡ï¼Œç²¾ç¡®æœç´¢ä¼šäº§ç”Ÿæ˜¾è‘—çš„æ£€ç´¢å¼€é”€ï¼Œè€Œç²—ç•¥æ£€ç´¢åˆ™åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éœ€è¦é¢å¤–çš„æ¨ç†æ­¥éª¤ã€‚å¦ä¸€æ–¹é¢ï¼Œç³»ç»Ÿè®¾è®¡å­˜åœ¨ä½æ•ˆç‡é—®é¢˜ï¼ŒåŒ…æ‹¬ä¸å½“çš„è°ƒåº¦å’Œé¢‘ç¹çš„æ£€ç´¢åœé¡¿ï¼Œä¼šå¯¼è‡´çº§è”å»¶è¿Ÿï¼Œå³ä½¿æ£€ç´¢ä¸­çš„å¾®å°å»¶è¿Ÿä¹Ÿä¼šæ”¾å¤§ç«¯åˆ°ç«¯çš„æ¨ç†æ—¶é—´ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œæœç´¢ä»£ç†æé«˜çš„ç”Ÿæˆè´¨é‡å¾€å¾€ä»¥æ•ˆç‡ä¸ºä»£ä»·ï¼Œè€Œåœ¨æ¨ç† - æœç´¢åœºæ™¯ä¸­ï¼Œä½å»¶è¿Ÿå“åº”å¯¹äºç¡®ä¿æ— ç¼çš„ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ï¼Œåœ¨åŸºäºLLMçš„æœç´¢ä»£ç†çš„åè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ä¹Ÿå¾ˆå…³é”®ï¼Œç°æœ‰ä¼˜åŒ–æŠ€æœ¯æ— æ³•ä¸“é—¨è§£å†³å¤šæ­¥æ¨ç†å’ŒåŠ¨æ€æ£€ç´¢ç´§å¯†äº¤é”™å¸¦æ¥çš„ç‹¬ç‰¹è®¡ç®—æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œéœ€è¦å¯¹åŸºäºLLMçš„æœç´¢ä»£ç†çš„æ•ˆç‡å› ç´ è¿›è¡Œåˆ†æå¹¶æå‡ºæ”¹è¿›æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡‡ç”¨é«˜å¬å›ç‡è¿‘ä¼¼æ£€ç´¢
SearchAgent - Xé€‰æ‹©åŸºäºé«˜å¬å›ç‡è¿‘ä¼¼æ£€ç´¢æ–¹æ³•æ„å»ºï¼Œå› ä¸ºè¿‡é«˜æˆ–è¿‡ä½çš„æ£€ç´¢åŠªåŠ›éƒ½ä¼šå¯¼è‡´æ•ˆç‡ä¸‹é™ï¼Œé«˜å¬å›ç‡è¿‘ä¼¼æ£€ç´¢èƒ½åœ¨æ²¡æœ‰ä¸å¿…è¦æ£€ç´¢æˆæœ¬çš„æƒ…å†µä¸‹æœ‰æ•ˆæ”¯æŒæ¨ç†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¼˜å…ˆçº§æ„ŸçŸ¥è°ƒåº¦
é€šè¿‡å®æ—¶çŠ¶æ€å¯¹è¯·æ±‚è¿›è¡Œä¼˜å…ˆçº§æ„ŸçŸ¥è°ƒåº¦ï¼Œä»¥æé«˜KV - ç¼“å­˜çš„åˆ©ç”¨ç‡ï¼Œè§£å†³ä¸å½“è°ƒåº¦é—®é¢˜ï¼Œé¿å…æ ‡å‡†ç­–ç•¥ï¼ˆå¦‚FCFSï¼‰æ— æ³•å¯¹æœ€èƒ½ä»KV - ç¼“å­˜é‡ç”¨ä¸­å—ç›Šçš„è¯·æ±‚è¿›è¡Œä¼˜å…ˆçº§æ’åºçš„æƒ…å†µã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ— åœé¡¿æ£€ç´¢
æå‡ºæ— åœé¡¿æ£€ç´¢æœºåˆ¶ï¼Œé€šè¿‡è‡ªé€‚åº”æœºåˆ¶å…è®¸ç”Ÿæˆç»§ç»­è¿›è¡Œè€Œæ— éœ€ä¸å¿…è¦çš„ç­‰å¾…ï¼ŒåŒæ—¶ç¡®ä¿è¶³å¤Ÿçš„æ£€ç´¢è´¨é‡ï¼Œå…‹æœé¢‘ç¹çš„æ£€ç´¢åœé¡¿é—®é¢˜ï¼Œé¿å…å¼‚æ­¥æ£€ç´¢å’Œä»¤ç‰Œç”Ÿæˆä¹‹é—´çš„æ—¶é—´é”™ä½å¯¼è‡´è¯·æ±‚ç­‰å¾…å’Œä¸å¿…è¦çš„é‡æ–°è®¡ç®—ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å„ç§æ“ä½œè®¾ç½®ä¸‹ï¼ŒSearchAgent - Xå§‹ç»ˆä¸”æ˜¾è‘—ä¼˜äºvLLMå’ŒåŸºäºHNSWçš„æ£€ç´¢ç­‰æœ€å…ˆè¿›çš„åŸºçº¿ç³»ç»Ÿã€‚åœ¨ç¦»çº¿å’Œåœ¨çº¿æ¨ç†åœºæ™¯ä¸­ï¼Œé€šè¿‡æé«˜LLM KV - ç¼“å­˜åˆ©ç”¨ç‡ï¼ˆä»0.07æé«˜åˆ°0.65ï¼‰ï¼ŒSearchAgent - Xåœ¨ä¸å½±å“ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ç³»ç»Ÿæ€§èƒ½çš„å¤§å¹…æå‡ï¼Œååé‡æœ€é«˜æé«˜3.4å€ï¼Œå»¶è¿Ÿé™ä½5å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ•ˆç‡åˆ†æè§†è§’**ï¼šè®ºæ–‡å¯¹æ£€ç´¢å‡†ç¡®æ€§å’Œæ£€ç´¢å»¶è¿Ÿè¿™ä¸¤ä¸ªå…³é”®å› ç´ å¦‚ä½•å½±å“åŸºäºLLMçš„æœç´¢ä»£ç†æ•ˆç‡è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†éå•è°ƒå…³ç³»å’Œçº§è”å»¶è¿Ÿé—®é¢˜ï¼Œè¿™ç§å¯¹æ•ˆç‡å› ç´ çš„ç³»ç»Ÿåˆ†ææ–¹æ³•ä¸ºå…¶ä»–ç›¸å…³ç ”ç©¶æä¾›äº†å€Ÿé‰´ï¼Œæœ‰åŠ©äºå‘ç°å’Œè§£å†³ç±»ä¼¼çš„æ•ˆç‡ç“¶é¢ˆã€‚
2. **ä¼˜åŒ–æœºåˆ¶**ï¼šæå‡ºçš„ä¼˜å…ˆçº§æ„ŸçŸ¥è°ƒåº¦å’Œæ— åœé¡¿æ£€ç´¢ç­‰ä¼˜åŒ–æœºåˆ¶ï¼Œé’ˆå¯¹ç³»ç»Ÿè®¾è®¡ä¸­çš„ä½æ•ˆç‡é—®é¢˜ï¼Œä»è°ƒåº¦å’Œæ£€ç´¢è¿‡ç¨‹çš„æ”¹è¿›å…¥æ‰‹ï¼Œä¸ºæé«˜æœç´¢ä»£ç†ç³»ç»Ÿçš„æ•ˆç‡æä¾›äº†å…·ä½“çš„è§£å†³æ€è·¯ï¼Œåœ¨å…¶ä»–æ¶‰åŠå¤šæ­¥éª¤äº¤äº’å’ŒåŠ¨æ€èµ„æºè°ƒç”¨çš„ç³»ç»Ÿä¸­ä¹Ÿå¯èƒ½å…·æœ‰åº”ç”¨ä»·å€¼ã€‚
3. **å¹³è¡¡ç­–ç•¥**ï¼šåœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å¯»æ±‚å¹³è¡¡ï¼Œé‡‡ç”¨é«˜å¬å›ç‡è¿‘ä¼¼æ£€ç´¢çš„æ–¹å¼ï¼Œä¸ºåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†æä¾›äº†ä¸€ç§å¯è¡Œçš„ç­–ç•¥ï¼Œå¯¹äºèµ„æºæ•æ„Ÿå‹çš„åº”ç”¨åœºæ™¯æœ‰å‚è€ƒæ„ä¹‰ã€‚
``` 

## evotest--evolutionary-test-time-learning-for-self-improving-agentic-systems
### Abstract
A fundamental limitation of current AI agents is their inability to learn complex skills on the fly at test time, often behaving like "clever but clueless interns" in novel environments. This severely limits their practical utility. To systematically measure and drive progress on this challenge, we first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a new evaluation setup where an agent must play the same game for several consecutive episodes, attempting to improve its performance from one episode to the next. On J-TTL, we find that existing adaptation methods like reflection, memory, or reinforcement learning struggle. To address the challenges posed by our benchmark, we present EvoTest, an evolutionary test-time learning framework that improves an agent without any fine-tuning or gradients-by evolving the entire agentic system after every episode. EvoTest has two roles: the Actor Agent, which plays the game, and the Evolver Agent, which analyzes the episode transcript to propose a revised configuration for the next run. This configuration rewrites the prompt, updates memory by logging effective state-action choices, tunes hyperparameters, and learns the tool-use routines. On our J-TTL benchmark, EvoTest consistently increases performance, outperforming not only reflection and memory-only baselines but also more complex online fine-tuning methods. Notably, our method is the only one capable of winning two games (Detective and Library), while all baselines fail to win any.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | EvoTestï¼šå¼€å¯æ™ºèƒ½ä½“è‡ªæˆ‘è¿›åŒ–æ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰AIæ™ºèƒ½ä½“å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§å±€é™ï¼Œå³åœ¨æµ‹è¯•æ—¶æ— æ³•å³æ—¶å­¦ä¹ å¤æ‚æŠ€èƒ½ï¼Œåœ¨å…¨æ–°ç¯å¢ƒä¸­å¸¸è¡¨ç°å¾—åƒâ€œèªæ˜å´æ‡µæ‡‚çš„å®ä¹ ç”Ÿâ€ï¼Œåªèƒ½æ‰§è¡ŒæŒ‡ä»¤å´éš¾ä»¥ä»ç»éªŒä¸­æ”¹è¿›è‡ªèº«æµç¨‹ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å®ƒä»¬åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„å¯é æ€§ä¸å®é™…æ•ˆç”¨ã€‚ç”±äºç¼ºä¹ä¸“é—¨ç”¨äºè¡¡é‡æ™ºèƒ½ä½“å¿«é€Ÿã€ä¼šè¯å†…æ”¹è¿›èƒ½åŠ›çš„æ ‡å‡†åŒ–æµ‹è¯•å¹³å°ï¼Œè¯¥é¢†åŸŸåœ¨è¿™æ–¹é¢çš„è¿›å±•å—åˆ°é˜»ç¢ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¡¡é‡å’Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨è¿™ä¸€æŒ‘æˆ˜ä¸Šå–å¾—è¿›å±•ï¼Œè®ºæ–‡ä½œè€…åšå‡ºäº†ç›¸å…³ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºJerichoæµ‹è¯•æ—¶å­¦ä¹ ï¼ˆJ - TTLï¼‰åŸºå‡†
è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒä»»åŠ¡æ˜¯è®©æ™ºèƒ½ä½“è¿ç»­å¤šæ¬¡å°è¯•ç©åŒä¸€ä¸ªå¤æ‚çš„åŸºäºæ–‡æœ¬çš„å†’é™©æ¸¸æˆï¼Œæ™ºèƒ½ä½“éœ€åœ¨æ¯ä¸ªå›åˆä¸­é€šè¿‡æ ‡å‡†å¾ªç¯ä¸ç¯å¢ƒäº¤äº’ï¼Œç›®æ ‡æ˜¯ä»…åˆ©ç”¨å•ä¸ªä¼šè¯å†…æ”¶é›†çš„ç»éªŒæé«˜æœ€ç»ˆå¾—åˆ†ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†ç°æœ‰é€‚åº”èŒƒå¼çš„ä¸è¶³ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºEvoTestè¿›åŒ–æµ‹è¯•æ—¶å­¦ä¹ æ¡†æ¶
è¯¥æ¡†æ¶æ—¨åœ¨å®ç°æ— éœ€å¾®è°ƒçš„å¿«é€Ÿã€æ•´ä½“é€‚åº”ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªä¸åŒè§’è‰²å°†è¡ŒåŠ¨ä¸é€‚åº”è§£è€¦ï¼šæ‰§è¡Œå®Œæ•´å›åˆçš„Actoræ™ºèƒ½ä½“å’Œåœ¨ç‹¬ç«‹å›åˆä¹‹é—´æ”¹è¿›ç³»ç»Ÿçš„Evolveræ™ºèƒ½ä½“ã€‚æ¯ä¸ªå›åˆåï¼ŒEvolveræ™ºèƒ½ä½“åˆ†æå®Œæ•´è®°å½•å¹¶ä¸ºæ•´ä¸ªæ™ºèƒ½ä½“ç³»ç»Ÿæå‡ºä¿®è®¢é…ç½®ï¼ŒåŒ…æ‹¬é‡å†™æŒ‡å¯¼æç¤ºä»¥ç¼–ç æ–°ç­–ç•¥ã€æ›´æ–°ç»“æ„åŒ–éƒ¨ç½²æ—¶è®°å¿†ã€è°ƒæ•´å†³ç­–è¶…å‚æ•°ä»¥åŠå®Œå–„å·¥å…·ä½¿ç”¨ä¾‹ç¨‹ï¼Œä»è€Œå°†ä¸€ä¸ªå›åˆçš„å™è¿°è½¬åŒ–ä¸ºä¸‹ä¸€æ¬¡å°è¯•çš„å¤šæ–¹é¢æ”¹è¿›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨J - TTLåŸºå‡†ä¸Šï¼ŒEvoTestå±•ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç›¸è¾ƒäºæœ€å¼ºçš„æç¤ºè¿›åŒ–åŸºçº¿æå‡äº†38%ï¼Œç›¸è¾ƒäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ æå‡äº†57%ï¼Œåœ¨æ¯ä¸ªæ¸¸æˆä¸­å‡è¶…è¶Šäº†æ‰€æœ‰åŸºäºå¼ºåå°„ã€åŸºäºè®°å¿†å’ŒåŸºäºæ¢¯åº¦çš„åŸºçº¿æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ³•æ˜¯å”¯ä¸€èƒ½å¤Ÿèµ¢å¾—ä¸¤ä¸ªæ¸¸æˆï¼ˆDetectiveå’ŒLibraryï¼‰çš„æ–¹æ³•ï¼Œè€Œæ‰€æœ‰åŸºçº¿æ–¹æ³•å‡æœªèƒ½èµ¢å¾—ä»»ä½•æ¸¸æˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„J - TTLåŸºå‡†ä¸ºè¡¡é‡æ™ºèƒ½ä½“æµ‹è¯•æ—¶å­¦ä¹ èƒ½åŠ›æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹å¼ï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤è¿›ä¸€æ­¥æ¢ç´¢æ™ºèƒ½ä½“åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„å­¦ä¹ èƒ½åŠ›ã€‚EvoTestæ¡†æ¶é€šè¿‡å¯¹æ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œæ•´ä½“è¿›åŒ–å®ç°æµ‹è¯•æ—¶å­¦ä¹ çš„æ€è·¯å…·æœ‰åˆ›æ–°æ€§ï¼Œä¸ºå¼€å‘æ›´è‡ªä¸»ã€é€‚åº”æ€§æ›´å¼ºçš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ï¼Œå…¶è§£è€¦æ‰§è¡Œä¸é€‚åº”çš„è®¾è®¡ä»¥åŠå¯¹æ™ºèƒ½ä½“é…ç½®å¤šæ–¹é¢çš„æ”¹è¿›ç­–ç•¥ï¼Œéƒ½å¯ä¸ºç›¸å…³ç ”ç©¶æä¾›å€Ÿé‰´ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ€è€ƒå¦‚ä½•è®©æ™ºèƒ½ä½“æ›´å¥½åœ°ä»è‡ªèº«ç»éªŒä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚
``` 

## orthalign--orthogonal-subspace-decomposition-for-non-interfering-multi-objective-alignment
### Abstract
Large language model (LLM) alignment faces a critical dilemma when addressing multiple human preferences: improvements in one dimension frequently come at the expense of others, creating unavoidable trade-offs between competing objectives like helpfulness and harmlessness. While prior work mainly focuses on constraint-based optimization algorithms and data selection strategies to mitigate conflicts, these approaches overlook the fundamental issue of resolving conflicts directly at the parameter level. In this paper, we present OrthAlign, an innovative approach that pioneers a new paradigm by leveraging orthogonal subspace decomposition to fundamentally resolve gradient-level conflicts in multi-objective preference alignment. OrthAlign strategically decomposes parameter update spaces into orthogonal subspaces, ensuring that optimization toward different preferences occurs in mathematically non-interfering directions. Building upon this, we provide theoretical guarantees demonstrating that when parameter increments satisfy both orthogonal subspace constraints and spectral norm bounds, the resulting updates exhibit linear Lipschitz growth rather than exponential instability, ensuring stable convergence across all preference dimensions. Extensive experiments show that: I. OrthAlign achieves maximum single-preference improvements ranging from 34.61% to 50.89% after multiple-objective alignment across helpful, harmless, and truthful dimensions. II. With an average overall reward improvement of 13.96%.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | OrthAlignï¼šå¼€å¯å¤šç›®æ ‡å¯¹é½æ— å¹²æ‰°æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯¹é½å¤šç§äººç±»åå¥½æ—¶é¢ä¸´å…³é”®å›°å¢ƒï¼Œæå‡æŸä¸€ç»´åº¦æ€§èƒ½å¸¸ä»¥ç‰ºç‰²å…¶ä»–ç»´åº¦ä¸ºä»£ä»·ï¼Œåœ¨è¯¸å¦‚æœ‰ç”¨æ€§å’Œæ— å®³æ€§ç­‰ç›¸äº’ç«äº‰çš„ç›®æ ‡é—´äº§ç”Ÿä¸å¯é¿å…çš„æƒè¡¡ã€‚å…ˆå‰å·¥ä½œä¸»è¦èšç„¦äºåŸºäºçº¦æŸçš„ä¼˜åŒ–ç®—æ³•å’Œæ•°æ®é€‰æ‹©ç­–ç•¥æ¥ç¼“è§£å†²çªï¼Œä½†è¿™äº›æ–¹æ³•å¿½è§†äº†åœ¨å‚æ•°å±‚é¢ç›´æ¥è§£å†³å†²çªè¿™ä¸€æ ¹æœ¬é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå…¨æ–°èŒƒå¼OrthAlign
åˆ©ç”¨æ­£äº¤å­ç©ºé—´åˆ†è§£ä»æ ¹æœ¬ä¸Šè§£å†³å¤šç›®æ ‡åå¥½å¯¹é½ä¸­çš„æ¢¯åº¦çº§å†²çªã€‚é€šè¿‡å°†å‚æ•°æ›´æ–°ç©ºé—´æˆ˜ç•¥æ€§åœ°åˆ†è§£ä¸ºæ­£äº¤å­ç©ºé—´ï¼Œç¡®ä¿é’ˆå¯¹ä¸åŒåå¥½çš„ä¼˜åŒ–åœ¨æ•°å­¦ä¸Šäº’ä¸å¹²æ‰°çš„æ–¹å‘ä¸Šè¿›è¡Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæä¾›ç†è®ºä¿éšœ
åŸºäºç¨³å®šæ€§ç†è®ºï¼Œå½“æ¨¡å‹å‚æ•°å¢é‡åŒæ—¶æ»¡è¶³ä¸¤ä¸ªçº¦æŸæ¡ä»¶ï¼Œå³é™åˆ¶æ›´æ–°åœ¨ä¸»å­ç©ºé—´çš„æ­£äº¤è¡¥ç©ºé—´ä»¥é¿å…ä¸å…³é”®å’Œå…ˆéªŒæ–¹å‘å¹²æ‰°ï¼Œä»¥åŠè£å‰ªæ›´æ–°çš„è°±èŒƒæ•°ä»¥æ§åˆ¶æ”¾å¤§ç‡æ—¶ï¼Œå¯ä¿è¯æ¯å±‚ä»¥åŠæ•´ä¸ªæ¨¡å‹çš„Lipschitzä¸Šç•Œå‘ˆçº¿æ€§å¢é•¿ï¼Œç¡®ä¿ç´¯ç§¯æ›´æ–°è¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨4ä¸ªåŸºå‡†æµ‹è¯•ã€è¶…è¿‡7ç§åŸºçº¿æ–¹æ³•ä¸ŠéªŒè¯äº†OrthAlignçš„æœ‰æ•ˆæ€§ã€‚ä¸è¡¨ç°æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ä¸¤ç›®æ ‡å¯¹é½ä¸Šå¹³å‡æå‡20.23%ï¼Œåœ¨ä¸‰ç›®æ ‡å¯¹é½ä¸Šå¹³å‡æå‡13.96%ã€‚æ­¤å¤–ï¼ŒOrthAlignè¿˜å¯ä½œä¸ºç°æœ‰å¯¹é½æŠ€æœ¯çš„æ€§èƒ½å¢å¼ºå™¨ï¼Œå¹³å‡å°†æ— å®³æ€§æå‡25.06%ï¼Œæœ‰ç”¨æ€§æå‡4.86%ï¼Œå¯ä½œä¸ºå³æ’å³ç”¨æ¨¡å—æ— ç¼é›†æˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„å­ç©ºé—´ç§©é€‰æ‹©ç†è®ºå’Œæ¡†æ¶ä¸ºå¤šç›®æ ‡åå¥½å¯¹é½ï¼ˆMPAï¼‰é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå¯¹äºè§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šç›®æ ‡ä¼˜åŒ–ä¸­é¢ä¸´çš„å†²çªé—®é¢˜å…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ï¼Œæœ‰æœ›æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚åŒæ—¶ï¼Œå…¶åŸºäºæ¢¯åº¦ç¨³å®šæ€§ç†è®ºçš„å‚æ•°æ›´æ–°æ–¹å¼ä»¥åŠå¯¹å‚æ•°å¢é‡çº¦æŸæ¡ä»¶çš„è®¾å®šç­‰ï¼Œä¸ºåç»­ç›¸å…³ç ”ç©¶åœ¨ç†è®ºå’Œå®è·µå±‚é¢éƒ½æä¾›äº†å‚è€ƒã€‚
``` 

## navigate-the-unknown--enhancing-llm-reasoning-with-intrinsic-motivation-guided-exploration
### Abstract
Reinforcement Learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimization (GRPO) face critical limitations due to their reliance on sparse outcome-based rewards and inadequate mechanisms for incentivizing exploration. These limitations result in inefficient guidance for reasoning. Specifically, sparse rewards fail to deliver sufficient feedback, particularly for challenging problems. Furthermore, such rewards induce systematic biases that prioritize exploitation of familiar trajectories over novel solution discovery. These shortcomings critically hinder performance in complex reasoning tasks, which inherently demand iterative refinement across intermediate steps. To address these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd foR LLM Reasoning (i-MENTOR), a method designed to deliver dense rewards and amplify exploration in the RL-based paradigm. i-MENTOR introduces three innovations: trajectory-aware exploration rewards that mitigate bias in token-level strategies while maintaining computational efficiency; error-conditioned reward allocation to ensure efficient exploration on challenging samples while intrinsically stabilizing training; and advantage-preserving integration that maintains advantage distribution integrity while incorporating exploratory guidance. Experiments across 4 public datasets demonstrate i-MENTOR's effectiveness, achieving a 22.23\% improvement on AIME 2024.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | i - MENTORï¼šå¼€å¯å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¢ç´¢æ–°å¾ç¨‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¯¸å¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç»„æ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ä¸»æµRLæ–¹æ³•å­˜åœ¨ä¸¥é‡å±€é™æ€§ã€‚ä¸€æ–¹é¢ï¼Œå®ƒä»¬ä¾èµ–åŸºäºç¨€ç–ç»“æœçš„å¥–åŠ±ï¼Œæ— æ³•ä¸ºæ¨ç†æä¾›å……åˆ†åé¦ˆï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æ—¶ã€‚å¦ä¸€æ–¹é¢ï¼Œè¿™äº›å¥–åŠ±æœºåˆ¶ç¼ºä¹æœ‰æ•ˆçš„æ¢ç´¢æ¿€åŠ±ï¼Œä¼šå¯¼è‡´ç³»ç»Ÿæ€§åå·®ï¼Œä½¿å¾—æ¨¡å‹æ›´å€¾å‘äºåˆ©ç”¨ç†Ÿæ‚‰çš„æ¨ç†è½¨è¿¹ï¼Œè€Œéæ¢ç´¢æ–°çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸¥é‡é˜»ç¢äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå› ä¸ºå¤æ‚æ¨ç†ä»»åŠ¡éœ€è¦åœ¨ä¸­é—´æ­¥éª¤è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿæ¢ç´¢æ–¹æ³•åº”ç”¨äºLLMæ¨ç†ä»»åŠ¡æ—¶ï¼Œé¢ä¸´åŠ¨æ€æƒ…èŠ‚é•¿åº¦å¯¼è‡´çš„è®¡ç®—è¿‡è½½ã€å¤§åŠ¨ä½œç©ºé—´å¸¦æ¥çš„è®¡ç®—æˆæœ¬é—®é¢˜ä»¥åŠæ¢ç´¢å¥–åŠ±ä¸ç»“æœå¥–åŠ±çš„å†²çªç­‰æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè½¨è¿¹æ„ŸçŸ¥æ¢ç´¢å¥–åŠ±
åœ¨åºåˆ—çº§åˆ«æ“ä½œï¼Œä»¥æ¶ˆé™¤ç”±åŠ¨æ€æƒ…èŠ‚é•¿åº¦å¼•èµ·çš„è®¡ç®—è¿‡è½½å’Œåºåˆ—é•¿åº¦åå·®ã€‚é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªè½»é‡çº§çš„è½¨è¿¹æ„ŸçŸ¥ç½‘ç»œï¼Œè¯¥ç»„ä»¶èƒ½å¤Ÿé«˜æ•ˆåœ°æ•æ‰æ¨ç†åºåˆ—çš„ç‹¬ç‰¹æ€§ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—çš„å¯è¿½æº¯æ€§ï¼Œå‡è½»äº†æ ‡è®°çº§ç­–ç•¥ä¸­çš„åå·®ï¼Œä¸”ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé”™è¯¯æ¡ä»¶å¥–åŠ±åˆ†é…
æœ‰é€‰æ‹©åœ°ä»…å¯¹é”™è¯¯çš„æ¨ç†è½¨è¿¹åº”ç”¨æ¢ç´¢æ¿€åŠ±ï¼Œåœ¨ä¿æŒç¨³å®šè®­ç»ƒè¿‡ç¨‹çš„åŒæ—¶æé«˜æ¢ç´¢æ•ˆç‡ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬è¿›è¡Œé«˜æ•ˆæ¢ç´¢ï¼Œä»æœ¬è´¨ä¸Šç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¼˜åŠ¿ä¿æŒé›†æˆ
åœ¨ä¼˜åŠ¿è®¡ç®—ä¹‹åå¼•å…¥æ¢ç´¢æ¿€åŠ±ï¼Œè§£å†³äº†æ¢ç´¢å¥–åŠ±ä¸åŸºäºç»“æœçš„å¥–åŠ±ä¹‹é—´çš„å›ºæœ‰å†²çªã€‚é€šè¿‡ä¿æŒç»“æœé©±åŠ¨çš„ä¼˜åŠ¿åˆ†å¸ƒçš„ç»Ÿè®¡å®Œæ•´æ€§ï¼Œå®ƒæ•´åˆäº†æ¢ç´¢æŒ‡å¯¼ï¼Œå®ç°äº†è¿™äº›ç›®æ ‡ä¹‹é—´çš„æœ‰æ•ˆåè°ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†å’Œä¸¤ä¸ªåŸºç¡€LLMsï¼ˆå³Qwen2.5 - 3Bå’Œdeepseek - 7bï¼‰ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œi - MENTORåœ¨å„ç§RLæ–¹æ³•å’ŒåŸºç¡€æ¨¡å‹ä¸­å§‹ç»ˆæœ‰æ•ˆï¼Œæ˜¾è‘—å¢å¼ºäº†LLMsçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨AIME 2024æ•°æ®é›†ä¸Šå®ç°äº†22.23%çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜æ­ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„æ¨¡å¼ï¼ši - MENTORåœ¨å›°éš¾æ•°æ®é›†ä¸Šçš„æ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼Œå…¶ä¸ºæ¯ä¸ªå“åº”åºåˆ—æä¾›çš„å¯†é›†æ¢ç´¢å¥–åŠ±æœ‰åŠ©äºæ¨¡å‹å…‹æœå…·æœ‰æŒ‘æˆ˜æ€§æ ·æœ¬ä¸­çš„å­¦ä¹ éšœç¢ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æå‡ºçš„i - MENTORæ–¹æ³•åŠå…¶åˆ›æ–°ç‚¹ä¸ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚è½¨è¿¹æ„ŸçŸ¥æ¢ç´¢å¥–åŠ±ã€é”™è¯¯æ¡ä»¶å¥–åŠ±åˆ†é…ä»¥åŠä¼˜åŠ¿ä¿æŒé›†æˆç­‰æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸRLæ–¹æ³•åœ¨LLMæ¨ç†ä»»åŠ¡ä¸­çš„å¥–åŠ±ç¨€ç–å’Œæ¢ç´¢ä¸è¶³ç­‰é—®é¢˜ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯å€Ÿé‰´è¿™äº›æœºåˆ¶æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶ä¹Ÿä¸ºè¿›ä¸€æ­¥ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ çš„ç»“åˆæä¾›äº†æœ‰ç›Šå‚è€ƒã€‚
``` 

## learning-what-reinforcement-learning-can-t--interleaved-online-fine-tuning-for-hardest-questions
### Abstract
Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReLIFTï¼šçªç ´å¼ºåŒ–å­¦ä¹ å±€é™ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ¶Œç°å‡ºæ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚è§„åˆ’å’Œè‡ªæˆ‘åæ€ç­‰å¤æ‚è¡Œä¸ºã€‚ç„¶è€Œï¼Œå½“å‰å½¢å¼çš„RLä¸è¶³ä»¥è¯±å¯¼è¶…è¶ŠåŸºç¡€æ¨¡å‹å›ºæœ‰å±€é™æ€§çš„èƒ½åŠ›ï¼Œå®ƒä¸»è¦æ˜¯åŸºäºæ¨¡å‹çš„ç°æœ‰çŸ¥è¯†è¿›è¡Œä¼˜åŒ–ï¼Œè€Œéä¿ƒè¿›æ–°çŸ¥è¯†çš„è·å–ã€‚ç ”ç©¶æ˜¾ç¤ºï¼ŒRLä¸»è¦å¼ºåŒ–ç°æœ‰è¡Œä¸ºï¼Œæ— æ³•èµ‹äºˆLLMæ–°çš„æ¨ç†æŠ€èƒ½ï¼Œè¿˜ä¼šæŠ‘åˆ¶æ¢ç´¢ï¼Œä½¿æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šé™·å…¥åœæ»ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯å€ŸåŠ©é«˜è´¨é‡æ¼”ç¤ºæ•°æ®çº³å…¥æ–°çŸ¥è¯†å’Œæ¨ç†æ¨¡å¼ï¼Œä½†å¯¹å¤§é‡é«˜è´¨é‡æ¼”ç¤ºæ•°æ®ä¾èµ–åº¦é«˜ï¼Œä¸”è®­ç»ƒçš„æ¨¡å‹åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆç»“åˆRLå’ŒSFTä»¥æå‡LLMæ¨ç†å’ŒOODæ³›åŒ–èƒ½åŠ›ã€å‡å°‘å¯¹æ˜‚è´µæ¼”ç¤ºæ•°æ®çš„ä¾èµ–å¹¶å®ç°è¶…è¶Šå½“å‰è®¤çŸ¥çº¦æŸçš„èƒ½åŠ›ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç³»ç»Ÿåˆ†æRLå’ŒSFTè®­ç»ƒåŠ¨æ€
é€šè¿‡æ£€æŸ¥æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ä¸åŒéš¾åº¦é—®é¢˜çš„å‡†ç¡®æ€§å˜åŒ–ï¼Œå°†é—®é¢˜åˆ’åˆ†ä¸ºå››ä¸ªéš¾åº¦çº§åˆ«ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å„ä¸ªæ£€æŸ¥ç‚¹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°RLå¯¹è¾ƒä½éš¾åº¦é—®é¢˜æ›´æœ‰æ•ˆï¼ŒSFTå¯¹æœ€å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜æ›´æœ‰ç›Šï¼›å¯¹äºç®€å•é—®é¢˜ï¼ŒSFTå¯èƒ½é™ä½æ¨¡å‹ç°æœ‰æ€§èƒ½ä¸”å¢åŠ å“åº”é•¿åº¦ï¼Œå¯¹äºæŒ‘æˆ˜æ€§é—®é¢˜ï¼ŒRLçš„æ”¹è¿›ç›¸æ¯”SFTä¸æ˜æ˜¾ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºReLIFTæ¡†æ¶
å³å¼ºåŒ–å­¦ä¹ ä¸åœ¨çº¿å¾®è°ƒäº¤é”™çš„æ¡†æ¶ã€‚åœ¨RLè®­ç»ƒæœŸé—´ï¼Œæ ¹æ®rolloutä¸­è§‚å¯Ÿåˆ°çš„å‡†ç¡®æ€§æ”¶é›†æå…·æŒ‘æˆ˜æ€§çš„ç¤ºä¾‹ï¼Œå½“è¯†åˆ«åˆ°æ­¤ç±»æŒ‘æˆ˜æ€§é—®é¢˜æ—¶ï¼Œè·å–é«˜è´¨é‡çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è§£å†³æ–¹æ¡ˆå¹¶è¿‡æ»¤æ‰é”™è¯¯ç­”æ¡ˆï¼Œå°†è¿™äº›ç¤ºä¾‹æ·»åŠ åˆ°SFTç¼“å†²åŒºï¼Œå½“ç¼“å†²åŒºä¸­æŒ‘æˆ˜æ€§é—®é¢˜æ•°é‡è¶³å¤Ÿæ—¶ï¼Œå¯¹è¿™äº›é—®é¢˜æ‰§è¡Œä¸€æ­¥SFTï¼Œä»¥åŠ¨æ€è¯†åˆ«æŒ‘æˆ˜æ€§ç¤ºä¾‹è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œè§£å†³æ¨¡å‹å‡ºç°çš„å¼±ç‚¹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†å’Œä¸€ä¸ªOODåŸºå‡†ä¸Šè¿›è¡Œç»¼åˆå®éªŒï¼Œä½¿ç”¨Qwen2.5 - Math - 7Bæ—¶ï¼ŒReLIFTæ–¹æ³•è¾¾åˆ°äº†52.6%çš„æ–°çš„æœ€å…ˆè¿›å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„RLVRåŸºçº¿ã€‚ä¸çº¯SFTã€çº¯RLä»¥åŠRLå’ŒSFTçš„ç»„åˆæ–¹æ³•ï¼ˆå¦‚å¸¦SFTæŸå¤±çš„RLã€SFTåæ¥RLå’ŒLUFFYç­‰ï¼‰ç›¸æ¯”ï¼ŒReLIFTè¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œæ‰€éœ€çš„è¯¦ç»†æ¼”ç¤ºæ•°æ®å’ŒGPUè®­ç»ƒæ—¶é—´æå°‘ï¼Œè¿˜èƒ½ç”Ÿæˆæ›´ç®€æ´çš„è§£å†³æ–¹æ¡ˆï¼Œæé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚å°†ReLIFTæ‰©å±•åˆ°æ›´å°æ›´å¼±çš„åŸºç¡€æ¨¡å‹æ—¶ï¼Œä¹Ÿå§‹ç»ˆè§‚å¯Ÿåˆ°ä¼˜è¶Šçš„ç»“æœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡å¯¹RLå’ŒSFTè®­ç»ƒåŠ¨æ€çš„åˆ†æä¸ºç†è§£å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æä¾›äº†æ–°è§†è§’ï¼Œå…¶æå‡ºçš„ReLIFTæ¡†æ¶ä¸ºæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„æœ‰æ•ˆé€”å¾„ï¼Œåœ¨èµ„æºåˆ©ç”¨æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¯¹äºåœ¨æœ‰é™æ•°æ®å’Œè®¡ç®—èµ„æºä¸‹æå‡æ¨¡å‹æ€§èƒ½å…·æœ‰å€Ÿé‰´æ„ä¹‰ï¼ŒåŒæ—¶ä¹Ÿä¸ºåç»­ç ”ç©¶å¦‚ä½•æ›´å¥½åœ°ç»“åˆä¸åŒè®­ç»ƒæ–¹æ³•ä»¥çªç ´æ¨¡å‹èƒ½åŠ›é™åˆ¶æä¾›äº†æ€è·¯ã€‚
``` 

## reinforced-preference-optimization-for-recommendation
### Abstract
Recent breakthroughs in large language models (LLMs) have fundamentally shifted recommender systems from discriminative to generative paradigms, where user behavior modeling is achieved by generating target items conditioned on historical interactions. Yet current generative recommenders still suffer from two core limitations: the lack of high-quality negative modeling and the reliance on implicit rewards. Reinforcement learning with verifiable rewards (RLVR) offers a natural solution by enabling on-policy sampling of harder negatives and grounding optimization in explicit reward signals. However, applying RLVR to generative recommenders remains non-trivial. Its unique generation space often leads to invalid or repetitive items that undermine sampling efficiency, and ranking supervision is sparse since most items receive identical zero rewards. To address these challenges, we propose Reinforced Preference Optimization for Recommendation (ReRe), a reinforcement-based paradigm tailored to LLM-based recommenders, an important direction in generative recommendation. ReRe incorporates constrained beam search to improve sampling efficiency and diversify hard negatives, while augmenting rule-based accuracy rewards with auxiliary ranking rewards for finer-grained supervision. Extensive experiments on three real-world datasets demonstrate that ReRe consistently outperforms both traditional and LLM-based recommenders in ranking performance. Further analysis shows that ReRe not only enhances performance across both base and SFT-initialized models but also generalizes robustly across different backbone families and scales. Beyond empirical gains, we systematically investigate the design space of RLVR in recommendation across generation, sampling strategy, reward modeling, and optimization algorithm, offering insights for future research.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReReï¼šå¼ºåŒ–åå¥½ä¼˜åŒ–åŠ©åŠ›æ¨èç³»ç»Ÿæ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çªç ´ä½¿æ¨èç³»ç»Ÿä»åˆ¤åˆ«èŒƒå¼å‘ç”ŸæˆèŒƒå¼è½¬å˜ï¼ŒåŸºäºLLMsçš„ç”Ÿæˆå¼æ¨èå™¨é€šè¿‡æ ¹æ®å†å²äº¤äº’ç”Ÿæˆç›®æ ‡é¡¹ç›®æ¥å¯¹ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡ã€‚ç„¶è€Œï¼Œå½“å‰ç”Ÿæˆå¼æ¨èå™¨å­˜åœ¨ä¸¤ä¸ªæ ¸å¿ƒå±€é™ï¼šä¸€æ˜¯ç¼ºä¹é«˜è´¨é‡çš„è´Ÿæ ·æœ¬å»ºæ¨¡ï¼ŒäºŒæ˜¯ä¾èµ–éšå¼å¥–åŠ±ã€‚è™½ç„¶å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æä¾›äº†ä¸€ç§è‡ªç„¶çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å°†å…¶åº”ç”¨äºç”Ÿæˆå¼æ¨èå™¨å¹¶éæ˜“äº‹ï¼Œå…¶ç‹¬ç‰¹çš„ç”Ÿæˆç©ºé—´å¸¸å¯¼è‡´æ— æ•ˆæˆ–é‡å¤çš„é¡¹ç›®ï¼Œé™ä½é‡‡æ ·æ•ˆç‡ï¼Œä¸”æ’åç›‘ç£ç¨€ç–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡‡ç”¨çº¦æŸè§£ç 
åœ¨ç”Ÿæˆæ–¹é¢ï¼ŒReReåœ¨æ¯ä¸€æ­¥é€šè¿‡å±è”½æ— æ•ˆæ ‡è®°é‡‡ç”¨çº¦æŸè§£ç ï¼Œç¡®ä¿åªç”Ÿæˆæœ‰æ•ˆé¡¹ç›®ï¼Œä»æ ¹æœ¬ä¸Šä½¿è¾“å‡ºç©ºé—´ä¸å¼€æ”¾å¼è¯­è¨€ä»»åŠ¡çš„è¾“å‡ºç©ºé—´åŒºåˆ†å¼€æ¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¿ç”¨æŸæœç´¢é‡‡æ ·
åœ¨é‡‡æ ·æ–¹é¢ï¼ŒReReè¿ç”¨æŸæœç´¢ï¼ˆbeam searchï¼‰åœ¨å•æ¬¡éå†ä¸­é«˜æ•ˆç”Ÿæˆå¤šæ ·çš„å€™é€‰é¡¹ç›®ï¼Œä¿è¯é‡‡æ ·æ•ˆç‡çš„åŒæ—¶ç¡®ä¿æ¥è§¦åˆ°æœ‰ä¿¡æ¯çš„è´Ÿæ ·æœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¹è¿›å¥–åŠ±å»ºæ¨¡
åœ¨å¥–åŠ±å»ºæ¨¡æ–¹é¢ï¼ŒReReç”¨æ’åå¥–åŠ±å¢å¼ºåŸºäºè§„åˆ™çš„å‡†ç¡®æ€§å¥–åŠ±ï¼Œæ ¹æ®ç”Ÿæˆæ¦‚ç‡å¯¹éš¾è´Ÿæ ·æœ¬åˆ†é…é¢å¤–æƒ©ç½šï¼Œæå‡è´Ÿæ ·æœ¬è´¨é‡å’Œæ’åä¿¡å·çš„ä¿çœŸåº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReReåœ¨æ’åæ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºä¼ ç»Ÿå’ŒåŸºäºLLMçš„æ¨èå™¨ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼ŒReReä¸ä»…åœ¨åŸºç¡€æ¨¡å‹å’ŒSFTåˆå§‹åŒ–æ¨¡å‹ä¸Šéƒ½èƒ½æå‡æ€§èƒ½ï¼Œè¿˜èƒ½åœ¨ä¸åŒçš„éª¨å¹²æ¨¡å‹æ—å’Œè§„æ¨¡ä¸Šç¨³å¥åœ°æ³›åŒ–ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡å¯¹RLVRåœ¨æ¨èç³»ç»Ÿä¸­çš„è®¾è®¡ç©ºé—´è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œæ¶µç›–ç”ŸæˆèŒƒå¼ã€é‡‡æ ·ç­–ç•¥ã€å¥–åŠ±å»ºæ¨¡å’Œä¼˜åŒ–ç®—æ³•ç­‰æ–¹é¢ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†è§è§£å’Œå‚è€ƒã€‚å…¶æå‡ºçš„ReReæ–¹æ³•ä¸ºè§£å†³ç”Ÿæˆå¼æ¨èå™¨çš„ç°æœ‰å±€é™æä¾›äº†æœ‰æ•ˆæ€è·¯ï¼Œåœ¨å®é™…åº”ç”¨ä¸­æœ‰æœ›æå‡æ¨èç³»ç»Ÿçš„æ€§èƒ½å’Œæ•ˆæœï¼Œå¯¹äºæ¨èç³»ç»Ÿé¢†åŸŸçš„ç ”ç©¶å’Œå®è·µå…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ã€‚
``` 

## rlbff--binary-flexible-feedback-to-bridge-between-human-feedback-&-verifiable-rewards
### Abstract
Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models: https://huggingface.co/collections/nvidia/reward-models-10-2025
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RLBFFï¼šèåˆäººç±»åé¦ˆä¸å¯éªŒè¯å¥–åŠ±çš„å…¨æ–°å¼ºåŒ–å­¦ä¹ èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åè®­ç»ƒé˜¶æ®µï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å’ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ˜¯ä¸¤ç§ä¸»è¦çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œå„è‡ªå…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒRLHFç”±äºä¾èµ–é€šå¸¸ç¼ºä¹æ˜ç¡®æ ‡å‡†çš„äººç±»åˆ¤æ–­ï¼Œåœ¨å¯è§£é‡Šæ€§å’Œå¥–åŠ±æ“çºµæ–¹é¢å­˜åœ¨å›°éš¾ï¼›RLVRåˆ™å› ä¸“æ³¨äºåŸºäºæ­£ç¡®æ€§çš„éªŒè¯å™¨ï¼Œåº”ç”¨èŒƒå›´å—åˆ°é™åˆ¶ã€‚ä¸ºäº†ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œè®ºæ–‡æå‡ºäº†åŸºäºäºŒå…ƒçµæ´»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLBFFï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç‹¬ç‰¹çš„åé¦ˆå½¢å¼
RLBFFä»è‡ªç„¶è¯­è¨€åé¦ˆä¸­æå–å¯ä»¥ç”¨äºŒå…ƒæ–¹å¼å›ç­”çš„åŸåˆ™ï¼ˆä¾‹å¦‚ä¿¡æ¯å‡†ç¡®æ€§ï¼šâ€œæ˜¯â€ï¼Œæˆ–ä»£ç å¯è¯»æ€§ï¼šâ€œå¦â€ï¼‰ï¼Œå¹¶å°†å¥–åŠ±æ¨¡å‹è®­ç»ƒä½œä¸ºä¸€ä¸ªè•´å«ä»»åŠ¡ï¼ˆå³å“åº”æ˜¯å¦æ»¡è¶³ä»»æ„åŸåˆ™ï¼‰ã€‚è¿™ç§æ–¹å¼ç»“åˆäº†äººç±»é©±åŠ¨åå¥½çš„å¤šæ ·æ€§å’ŒåŸºäºè§„åˆ™éªŒè¯çš„ç²¾ç¡®æ€§ï¼Œä½¿å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°å“åº”è´¨é‡ä¸­é™¤æ­£ç¡®æ€§ä¹‹å¤–çš„ç»†å¾®å·®åˆ«ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¿å…å¸¸è§é—®é¢˜çš„è®¾è®¡é€‰æ‹©
 - **åŸºäºåŸåˆ™çš„è€ƒé‡**ï¼šåœ¨ä¸åŒåœºæ™¯ä¸­ï¼Œäººç±»å¯¹å“åº”çš„å–œå¥½åŸå› å„å¼‚ï¼Œæ˜ç¡®è€ƒè™‘åˆ¤æ–­èƒŒåçš„åŸåˆ™å¯ä½¿è®­ç»ƒæ›´æœ‰æ•ˆï¼Œä¼˜åŒ–ç›®æ ‡æ›´æ¸…æ™°ã€‚
 - **å•å“åº”é€‰æ‹©**ï¼šç›¸è¾ƒäºRLHFå¸¸ç”¨çš„å“åº”å¯¹å½¢å¼ï¼ŒRLBFFé‡‡ç”¨å•å“åº”å½¢å¼ï¼Œå› ä¸ºåœ¨å¤§å¤šæ•°åœ¨çº¿æ–‡æœ¬åé¦ˆåœºæ™¯ä¸­ï¼Œå•å“åº”æ›´è‡ªç„¶ï¼Œä¸”èƒ½é¿å…å“åº”å¯¹æ˜“äº§ç”Ÿçš„ä½ç½®åå·®é—®é¢˜ã€‚
 - **äºŒå…ƒå½¢å¼**ï¼šä¸Likertè¯„åˆ†ç›¸æ¯”ï¼ŒäºŒå…ƒå½¢å¼åœ¨åŸåˆ™å±‚é¢æ›´æ˜“äºæ ¡å‡†ï¼Œå¯å‡å°‘æ³¨é‡Šå·®å¼‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½¿ç”¨è½¬æ¢åçš„å¼€æºæ•°æ®é›†HelpSteer3 - Feedbackè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œåœ¨RM - Benchä¸Šè¾¾åˆ°86.2%çš„æˆç»©ï¼Œåœ¨JudgeBenchä¸Šè¾¾åˆ°81.4%çš„æˆç»©ï¼ˆæˆªè‡³2025å¹´9æœˆ24æ—¥åœ¨æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸€ï¼‰ï¼Œæ€§èƒ½ä¼˜äºBradley - Terryæ¨¡å‹ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¨ç†æ—¶æŒ‡å®šæ„Ÿå…´è¶£çš„åŸåˆ™æ¥è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹çš„å…³æ³¨ç‚¹ã€‚æœ€åï¼Œè®ºæ–‡æä¾›äº†ä¸€ä¸ªå®Œå…¨å¼€æºçš„æ–¹æ³•ï¼ˆåŒ…æ‹¬æ•°æ®ï¼‰ï¼Œä½¿ç”¨RLBFFå’Œå¥–åŠ±æ¨¡å‹å¯¹é½Qwen3 - 32Bï¼Œåœ¨MT - Benchã€WildBenchå’ŒArena Hard v2ç­‰é€šç”¨å¯¹é½åŸºå‡†ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡o3 - miniå’ŒDeepSeek R1çš„æ€§èƒ½ï¼Œä¸”æ¨ç†æˆæœ¬ä¸åˆ°5%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - **æ–¹æ³•èåˆæ€è·¯**ï¼šRLBFFèåˆRLHFå’ŒRLVRä¼˜åŠ¿çš„æ€è·¯ä¸ºè§£å†³ä¸åŒå¼ºåŒ–å­¦ä¹ èŒƒå¼çš„å±€é™æ€§æä¾›äº†æ–°æ–¹å‘ï¼Œåœ¨å…¶ä»–ç›¸å…³ç ”ç©¶ä¸­å¯å€Ÿé‰´è¿™ç§èåˆä¸åŒæ–¹æ³•ä¼˜åŠ¿çš„ç­–ç•¥ã€‚
 - **æ•°æ®è½¬æ¢ä¸åˆ©ç”¨**ï¼šå°†ç°æœ‰å¼€æºæ•°æ®é›†è½¬æ¢ä¸ºé€‚åˆæ–°æ–¹æ³•çš„æ•°æ®å½¢å¼ï¼Œä¸ºåœ¨ç¼ºä¹ç›´æ¥å¯ç”¨æ•°æ®æ—¶å¼€å±•ç ”ç©¶æä¾›äº†ä¸€ç§å¯è¡Œé€”å¾„ã€‚
 - **æ¨¡å‹å®šåˆ¶åŒ–**ï¼šå…è®¸ç”¨æˆ·åœ¨æ¨ç†æ—¶è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹å…³æ³¨ç‚¹çš„è®¾è®¡ï¼Œä¸ºæ¨¡å‹æ›´å¥½åœ°æ»¡è¶³å¤šæ ·åŒ–éœ€æ±‚æä¾›äº†å‚è€ƒï¼Œåœ¨å¼€å‘å…·æœ‰ä¸ªæ€§åŒ–éœ€æ±‚çš„æ¨¡å‹æ—¶å¯è€ƒè™‘ç±»ä¼¼è®¾è®¡ã€‚
``` 

## medagent-pro--towards-evidence-based-multi-modal-medical-diagnosis-via-reasoning-agentic-workflow
### Abstract
In modern medicine, clinical diagnosis relies on the comprehensive analysis of primarily textual and visual data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in large Vision-Language Models (VLMs) and agent-based methods hold great potential for medical diagnosis, thanks to the ability to effectively integrate multi-modal patient data. However, they often provide direct answers and draw empirical-driven conclusions without quantitative analysis, which reduces their reliability and clinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigm that follows the diagnosis principle in modern medicine, to decouple the process into sequential components for step-by-step, evidence-based reasoning. Our MedAgent-Pro workflow presents a hierarchical diagnostic structure to mirror this principle, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, an RAG-based agent is designed to retrieve medical guidelines to ensure alignment with clinical standards. For patient-level reasoning, we propose to integrate professional tools such as visual models to enable quantitative assessments. Meanwhile, we propose to verify the reliability of each step to achieve evidence-based diagnosis, enforcing rigorous logical reasoning and a well-founded conclusion. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro to mainstream VLMs, agentic systems and state-of-the-art expert models. Ablation studies and human evaluation by clinical experts further validate its robustness and clinical relevance. Code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | MedAgent - Proï¼šå¼€å¯å¾ªè¯å¤šæ¨¡æ€åŒ»å­¦è¯Šæ–­æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°ä»£åŒ»å­¦ä¸­ï¼Œä¸´åºŠè¯Šæ–­æ˜¯ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œå®ƒéœ€è¦ç»¼åˆåˆ†ææ–‡æœ¬å’Œè§†è§‰ç­‰å¤šæ¨¡æ€æ‚£è€…æ•°æ®ï¼Œå¹¶å€ŸåŠ©åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ç¡®ä¿æ¨ç†çš„ç³»ç»Ÿæ€§å’Œä¸¥è°¨æ€§ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’ŒåŸºäºä»£ç†çš„æ–¹æ³•åœ¨åŒ»å­¦è¯Šæ–­æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€æ‚£è€…æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œåœ¨æ²¡æœ‰å®šé‡åˆ†æçš„æƒ…å†µä¸‹å¾—å‡ºç»éªŒé©±åŠ¨çš„ç»“è®ºï¼Œè¿™é™ä½äº†å®ƒä»¬çš„å¯é æ€§å’Œä¸´åºŠå¯ç”¨æ€§ã€‚ä¼ ç»Ÿçš„VQAæ¨¡å‹ä¹Ÿå¸¸ä»“ä¿ƒç”Ÿæˆè¯Šæ–­ç»“è®ºï¼Œä¾èµ–ç»éªŒæ€§å†…éƒ¨çŸ¥è¯†è€Œç¼ºä¹ç»†ç²’åº¦åˆ†æã€‚å½“å‰æ–¹æ³•åœ¨æ»¡è¶³ä¸´åºŠæ ‡å‡†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼ŒVLMsç¼ºä¹è¶³å¤ŸåŒ»å­¦çŸ¥è¯†å’Œæ·±åº¦åˆ†æèƒ½åŠ›ï¼Œä¸€äº›å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹è§†è§‰æ„ŸçŸ¥èƒ½åŠ›æœ‰é™ï¼Œè€Œç°æœ‰çš„åŒ»ç–—ä»£ç†ç³»ç»Ÿåªæ˜¯ç®€å•åœ°å°†å·¥å…·ç²˜åˆåœ¨ä¸€èµ·ï¼Œç¼ºä¹ä¸´åºŠå¯¼å‘çš„å·¥ä½œæµç¨‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥å®ç°æ›´å¯é çš„åŒ»å­¦è¯Šæ–­ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ–°çš„ä»£ç†æ¨ç†èŒƒå¼MedAgent - Pro
éµå¾ªç°ä»£åŒ»å­¦çš„è¯Šæ–­åŸåˆ™ï¼Œå°†è¯Šæ–­è¿‡ç¨‹è§£è€¦ä¸ºé¡ºåºç»„ä»¶ï¼Œè¿›è¡Œé€æ­¥çš„å¾ªè¯æ¨ç†ã€‚å…¶å·¥ä½œæµç¨‹å‘ˆç°åˆ†å±‚è¯Šæ–­ç»“æ„ï¼ŒåŒ…æ‹¬ç–¾ç—…çº§åˆ«çš„æ ‡å‡†åŒ–è®¡åˆ’ç”Ÿæˆå’Œæ‚£è€…çº§åˆ«çš„ä¸ªæ€§åŒ–é€æ­¥æ¨ç†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡åŸºäºRAGçš„ä»£ç†ç”¨äºç–¾ç—…çº§åˆ«è§„åˆ’
ä¸ºæ”¯æŒç–¾ç—…çº§åˆ«è§„åˆ’ï¼Œè®¾è®¡äº†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ä»£ç†ï¼Œç”¨äºæ£€ç´¢åŒ»å­¦æŒ‡å—ï¼Œç¡®ä¿ä¸ä¸´åºŠæ ‡å‡†ä¸€è‡´ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•´åˆä¸“ä¸šå·¥å…·ç”¨äºæ‚£è€…çº§åˆ«æ¨ç†
åœ¨æ‚£è€…çº§åˆ«æ¨ç†æ–¹é¢ï¼Œæè®®æ•´åˆè§†è§‰æ¨¡å‹ç­‰ä¸“ä¸šå·¥å…·ï¼Œå®ç°å®šé‡è¯„ä¼°ã€‚åŒæ—¶ï¼Œæè®®éªŒè¯æ¯ä¸ªæ­¥éª¤çš„å¯é æ€§ï¼Œä»¥å®ç°å¾ªè¯è¯Šæ–­ï¼Œå¼ºåŒ–ä¸¥æ ¼çš„é€»è¾‘æ¨ç†å’Œæœ‰æ ¹æ®çš„ç»“è®ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¹¿æ³›çš„è§£å‰–åŒºåŸŸã€æˆåƒæ¨¡æ€å’Œç–¾ç—…ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedAgent - Proä¼˜äºä¸»æµçš„VLMsã€ä»£ç†ç³»ç»Ÿå’Œæœ€å…ˆè¿›çš„ä¸“å®¶æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶å’Œä¸´åºŠä¸“å®¶çš„äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†å®ƒçš„é²æ£’æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **éµå¾ªä¸´åºŠåŸåˆ™çš„è®¾è®¡æ€è·¯**ï¼šMedAgent - Proéµå¾ªç°ä»£åŒ»å­¦è¯Šæ–­åŸåˆ™è¿›è¡Œè®¾è®¡ï¼Œè¿™ç§å°†AIæ–¹æ³•ä¸ä¸´åºŠå®é™…ç´§å¯†ç»“åˆçš„æ€è·¯ï¼Œä¸ºå…¶ä»–åŒ»ç–—AIç ”ç©¶æä¾›äº†å€Ÿé‰´ï¼Œå¼ºè°ƒäº†æ–¹æ³•çš„ä¸´åºŠé€‚ç”¨æ€§ã€‚
2. **åˆ†å±‚è¯Šæ–­ç»“æ„**ï¼šå…¶åˆ†å±‚è¯Šæ–­ç»“æ„ï¼Œå³ç–¾ç—…çº§åˆ«å’Œæ‚£è€…çº§åˆ«çš„åˆ†æ­¥æ¨ç†ï¼Œä¸ºå¤„ç†å¤æ‚çš„åŒ»å­¦è¯Šæ–­ä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå¯å¯å‘å…¶ä»–å¤šæ¨¡æ€åŒ»å­¦åˆ†æä»»åŠ¡é‡‡ç”¨ç±»ä¼¼çš„ç»“æ„åŒ–å¤„ç†æ–¹å¼ã€‚
3. **å·¥å…·æ•´åˆä¸éªŒè¯æœºåˆ¶**ï¼šæ•´åˆä¸“ä¸šå·¥å…·è¿›è¡Œå®šé‡è¯„ä¼°ä»¥åŠéªŒè¯æ¯ä¸ªæ­¥éª¤å¯é æ€§çš„åšæ³•ï¼Œå¯¹äºæé«˜AIè¯Šæ–­çš„å‡†ç¡®æ€§å’Œå¯é æ€§å…·æœ‰é‡è¦æ„ä¹‰ï¼Œåœ¨å…¶ä»–éœ€è¦ç²¾å‡†åˆ¤æ–­çš„AIåº”ç”¨åœºæ™¯ä¸­ä¹Ÿå¯è€ƒè™‘ç±»ä¼¼æœºåˆ¶ã€‚
``` 

## guidedsampling--steering-llms-towards-diverse-candidate-solutions-at-inference-time
### Abstract
Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -> 3.03), yielding a diverse set of candidates than traditional RS.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | GuidedSamplingï¼šå¼•é¢†å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶ç”Ÿæˆå¤šæ ·å€™é€‰è§£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¸å¤šé¢†åŸŸå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†æ— é™æ‰©å¤§æ¨¡å‹è§„æ¨¡å› è®­ç»ƒæ•°æ®éœ€æ±‚ç­‰é—®é¢˜å˜å¾—æ„ˆå‘ä¸å¯è¡Œã€‚å› æ­¤ï¼Œç ”ç©¶é‡ç‚¹è½¬å‘åœ¨æ¨ç†é˜¶æ®µæ›´å¥½åœ°åˆ©ç”¨è®¡ç®—èµ„æºä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚é‡å¤é‡‡æ ·ï¼ˆRSï¼‰ä½œä¸ºä¸€ç§å¸¸ç”¨çš„æ¨ç†æ—¶é—´ç®—æ³•ï¼Œè™½èƒ½æå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå´å¸¸éš¾ä»¥ç”Ÿæˆå¤šæ ·çš„å€™é€‰è§£ï¼Œå¾€å¾€ä¾èµ–ç›¸åŒçš„åº•å±‚æ–¹æ³•è§£å†³é—®é¢˜ï¼Œäº§ç”Ÿå†—ä½™æ ·æœ¬ã€‚è¿™æ˜¯ç”±äºLLMsä¼ ç»Ÿä¸Šè¢«è®­ç»ƒä¸ºæ¯ä¸ªè¾“å…¥ç”Ÿæˆå•ä¸€æ­£ç¡®å“åº”ï¼Œå¯¼è‡´RSç¼ºä¹å¯¹è§£ç©ºé—´çš„å……åˆ†æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºGuidedSamplingç®—æ³•
è¯¥ç®—æ³•å°†æ¨ç†è¿‡ç¨‹ä¸­çš„æ¢ç´¢å’Œç”Ÿæˆé˜¶æ®µè§£è€¦ã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œè¯†åˆ«å¯ç”¨äºè§£å†³é—®é¢˜çš„å¤šä¸ªæ¦‚å¿µï¼›åœ¨ç”Ÿæˆé˜¶æ®µï¼Œåº”ç”¨ç‰¹å®šæ¦‚å¿µæä¾›æœ€ç»ˆå€™é€‰è§£ã€‚é€šè¿‡è¿™ç§è§£è€¦ï¼Œå¢å¼ºäº†æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆå€™é€‰è§£çš„å¤šæ ·æ€§ï¼Œå¹¶å®ç°å¯¹æ¢ç´¢çš„æ˜¾å¼æ§åˆ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç”¨äºæ”¹è¿›LLMåè®­ç»ƒ
åœ¨éšæœºæŠ½å–çš„10kä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†OpenMathInstruct - 2å®ä¾‹ä¸Šï¼Œä½¿ç”¨GuidedSamplingç”Ÿæˆå¤šæ ·çš„è§£è½¨è¿¹ï¼Œå¹¶åŸºäºæ­¤å¯¹LLMsè¿›è¡Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§å¾®è°ƒæ–¹å¼ä¼˜äºåŸºäºä¼ ç»ŸRSã€æ€ç»´æ ‘ï¼ˆToTï¼‰å’Œå…¶ä»–è‡ªæ ¡æ­£æ–¹æ³•ç”Ÿæˆçš„è½¨è¿¹è®­ç»ƒçš„æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸RSç›¸æ¯”ï¼ŒGuidedSamplingä½¿åŸºç¡€æ¨¡å‹åœ¨pass@50ä¸Šå¹³å‡æå‡çº¦21.6%ã€‚
- åŸºäºGuidedSamplingè½¨è¿¹è®­ç»ƒçš„æ¨¡å‹åœ¨pass@5ä¸Šå¹³å‡æå‡çº¦9.7%ï¼Œä¼˜äºåŸºäºä¼ ç»ŸRSè®­ç»ƒçš„æ¨¡å‹ã€‚
- æå–åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„å€™é€‰è§£ä¸­çš„æ¦‚å¿µè¿›è¡Œåˆ†æå‘ç°ï¼Œä¸RSç›¸æ¯”ï¼ŒGuidedSamplingç”Ÿæˆçš„å€™é€‰è§£å¤šæ ·æ€§å¢åŠ äº†17.63%ã€‚
- åŸºäºGuidedSamplingè½¨è¿¹å¾®è°ƒçš„LLMsåœ¨MATHåŸºå‡†æµ‹è¯•ä¸Špass@5æå‡3.43%ï¼Œåœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•GPQA - Diamondã€HumanEvalå’ŒOlympiadBenchä¸Šä¹Ÿæœ‰æå‡ï¼Œæ³›åŒ–èƒ½åŠ›å¾—åˆ°æ”¹å–„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- è§£è€¦æ¢ç´¢å’Œç”Ÿæˆé˜¶æ®µçš„æ€è·¯ä¸ºæ”¹è¿›æ¨ç†ç®—æ³•æä¾›äº†æ–°æ–¹å‘ï¼Œæœ‰åŠ©äºåœ¨æ¨ç†æ—¶ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„è§£ï¼Œè§£å†³å¤æ‚ä»»åŠ¡æ—¶å¯å°è¯•åº”ç”¨æ­¤æ€è·¯ã€‚
- GuidedSamplingç”¨äºLLMåè®­ç»ƒçš„æ–¹æ³•ï¼Œä¸ºæå‡æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œåœ¨æ¨¡å‹å¾®è°ƒé˜¶æ®µå¯è€ƒè™‘å€Ÿé‰´è¿™ç§ç”Ÿæˆå¤šæ ·è§£è½¨è¿¹çš„æ–¹å¼ã€‚
``` 

