# Paper List from BIB File: tmp8c4fwrj2.bib
- [25/05] **Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability**  
[[Paper](http://arxiv.org/pdf/2505.13258v1)] [[Code/Page]()] [[TLDR/Notes](#effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability)]

- [25/05] **Hybrid Latent Reasoning via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.18454v1)] [[Code/Page]()] [[TLDR/Notes](#hybrid-latent-reasoning-via-reinforcement-learning)]

- [25/05] **EvolveSearch: An Iterative Self-Evolving Search Agent**  
[[Paper](http://arxiv.org/pdf/2505.22501v1)] [[Code/Page]()] [[TLDR/Notes](#evolvesearch--an-iterative-self-evolving-search-agent)]

- [25/04] **Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration**  
[[Paper](http://arxiv.org/pdf/2504.04915v1)] [[Code/Page](https://github.com/ritaranx/Collab-RAG/.)] [[TLDR/Notes](#collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration)]

- [25/03] **Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.09516v3)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning)]

- [25/01] **Search-o1: Agentic Search-Enhanced Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2501.05366v1)] [[Code/Page](https://github.com/sunnynexus/Search-o1}.)] [[TLDR/Notes](#search-o1--agentic-search-enhanced-large-reasoning-models)]

- [25/03] **Agent models: Internalizing Chain-of-Action Generation into Reasoning models**  
[[Paper](http://arxiv.org/pdf/2503.06580v1)] [[Code/Page](https://github.com/ADaM-BJTU/AutoCoA)] [[TLDR/Notes](#agent-models--internalizing-chain-of-action-generation-into-reasoning-models)]

- [25/05] **Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging**  
[[Paper](http://arxiv.org/pdf/2505.09316v1)] [[Code/Page]()] [[TLDR/Notes](#scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging)]

- [25/06] **ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering**  
[[Paper](http://arxiv.org/pdf/2506.00232v1)] [[Code/Page]()] [[TLDR/Notes](#composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering)]

- [24/11] **Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models**  
[[Paper](http://arxiv.org/pdf/2411.19443v1)] [[Code/Page](https://github.com/ictnlp/Auto-RAG}.)] [[TLDR/Notes](#auto-rag--autonomous-retrieval-augmented-generation-for-large-language-models)]

- [25/05] **LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization**  
[[Paper](http://arxiv.org/pdf/2505.17447v1)] [[Code/Page]()] [[TLDR/Notes](#lets--learning-to-think-and-search-via-process-and-outcome-reward-hybridization)]

- [25/03] **R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.05592v2)] [[Code/Page]()] [[TLDR/Notes](#r1-searcher--incentivizing-the-search-capability-in-llms-via-reinforcement-learning)]

- [25/03] **ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2503.19470v2)] [[Code/Page]()] [[TLDR/Notes](#research--learning-to-reason-with-search-for-llms-via-reinforcement-learning)]

- [25/06] **Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source Search in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.08352v1)] [[Code/Page](https://github.com/wentao0429/Reasoning-search.)] [[TLDR/Notes](#reinforcement-fine-tuning-for-reasoning-towards-multi-step-multi-source-search-in-large-language-models)]

- [25/05] **ZeroSearch: Incentivize the Search Capability of LLMs without Searching**  
[[Paper](http://arxiv.org/pdf/2505.04588v2)] [[Code/Page]()] [[TLDR/Notes](#zerosearch--incentivize-the-search-capability-of-llms-without-searching)]

- [25/06] **Constructing and Evaluating Declarative RAG Pipelines in PyTerrier**  
[[Paper](http://arxiv.org/pdf/2506.10802v1)] [[Code/Page]()] [[TLDR/Notes](#constructing-and-evaluating-declarative-rag-pipelines-in-pyterrier)]

- [25/05] **Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization**  
[[Paper](http://arxiv.org/pdf/2505.15444v1)] [[Code/Page]()] [[TLDR/Notes](#single-llm--multiple-roles--a-unified-retrieval-augmented-generation-framework-using-role-specific-token-optimization)]

- [25/06] **R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.04185v1)] [[Code/Page](https://github.com/QingFei1/R-Search.)] [[TLDR/Notes](#r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning)]

- [23/05] **Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy**  
[[Paper](http://arxiv.org/pdf/2305.15294v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-retrieval-augmented-large-language-models-with-iterative-retrieval-generation-synergy)]

- [25/05] **Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs**  
[[Paper](http://arxiv.org/pdf/2505.11277v3)] [[Code/Page]()] [[TLDR/Notes](#search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms)]

- [24/10] **SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback**  
[[Paper](http://arxiv.org/pdf/2410.18141v2)] [[Code/Page]()] [[TLDR/Notes](#smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback)]

- [25/05] **Neuro-Symbolic Query Compiler**  
[[Paper](http://arxiv.org/pdf/2505.11932v1)] [[Code/Page]()] [[TLDR/Notes](#neuro-symbolic-query-compiler)]

- [25/05] **Interleaved Reasoning for Large Language Models via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.19640v1)] [[Code/Page]()] [[TLDR/Notes](#interleaved-reasoning-for-large-language-models-via-reinforcement-learning)]

- [25/01] **Chain-of-Retrieval Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2501.14342v2)] [[Code/Page]()] [[TLDR/Notes](#chain-of-retrieval-augmented-generation)]

- [25/03] **OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query Processing**  
[[Paper](http://arxiv.org/pdf/2503.02603v2)] [[Code/Page]()] [[TLDR/Notes](#okralong--a-flexible-retrieval-augmented-framework-for-long-text-query-processing)]

- [24/12] **RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation**  
[[Paper](http://arxiv.org/pdf/2412.11919v1)] [[Code/Page](https://github.com/sunnynexus/RetroLLM}.)] [[TLDR/Notes](#retrollm--empowering-large-language-models-to-retrieve-fine-grained-evidence-within-generation)]

- [25/05] **StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.15107v2)] [[Code/Page](https://github.com/Zillwang/StepSearch.)] [[TLDR/Notes](#stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization)]

- [25/04] **ToolRL: Reward is All Tool Learning Needs**  
[[Paper](http://arxiv.org/pdf/2504.13958v1)] [[Code/Page]()] [[TLDR/Notes](#toolrl--reward-is-all-tool-learning-needs)]

- [25/04] **DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments**  
[[Paper](http://arxiv.org/pdf/2504.03160v4)] [[Code/Page](https://github.com/GAIR-NLP/DeepResearcher.)] [[TLDR/Notes](#deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments)]

- [25/05] **An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents**  
[[Paper](http://arxiv.org/pdf/2505.15117v1)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents)]

- [25/05] **R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.17005v1)] [[Code/Page](https://github.com/RUCAIBox/R1-Searcher-plus.)] [[TLDR/Notes](#r1-searcher++--incentivizing-the-dynamic-knowledge-acquisition-of-llms-via-reinforcement-learning)]

- [25/05] **Removal of Hallucination on Hallucination: Debate-Augmented RAG**  
[[Paper](http://arxiv.org/pdf/2505.18581v1)] [[Code/Page](https://github.com/Huenao/Debate-Augmented-RAG.)] [[TLDR/Notes](#removal-of-hallucination-on-hallucination--debate-augmented-rag)]

- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.22019v2)] [[Code/Page](https://github.com/Alibaba-NLP/VRAG.)] [[TLDR/Notes](#vrag-rl--empower-vision-perception-based-rag-for-visually-rich-information-understanding-via-iterative-reasoning-with-reinforcement-learning)]

- [25/01] **AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation using Tree-based Search**  
[[Paper](http://arxiv.org/pdf/2501.10053v2)] [[Code/Page]()] [[TLDR/Notes](#airrag--activating-intrinsic-reasoning-for-retrieval-augmented-generation-using-tree-based-search)]

- [24/11] **AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning**  
[[Paper](http://arxiv.org/pdf/2411.16495v3)] [[Code/Page]()] [[TLDR/Notes](#atomr--atomic-operator-empowered-large-language-models-for-heterogeneous-knowledge-reasoning)]

- [25/05] **SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis**  
[[Paper](http://arxiv.org/pdf/2505.16834v2)] [[Code/Page](https://github.com/RUCAIBox/SimpleDeepSearcher.)] [[TLDR/Notes](#simpledeepsearcher--deep-information-seeking-via-web-powered-reasoning-trajectory-synthesis)]

- [25/05] **ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision**  
[[Paper](http://arxiv.org/pdf/2505.21250v1)] [[Code/Page](https://leeds1219.github.io/ReSCORE.)] [[TLDR/Notes](#rescore--label-free-iterative-retriever-training-for-multi-hop-question-answering-with-relevance-consistency-supervision)]

- [24/07] **Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach**  
[[Paper](http://arxiv.org/pdf/2407.13101v2)] [[Code/Page]()] [[TLDR/Notes](#retrieve--summarize--plan--advancing-multi-hop-question-answering-with-an-iterative-approach)]

- [25/02] **RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation**  
[[Paper](http://arxiv.org/pdf/2502.13957v2)] [[Code/Page](https://rag-gym.github.io.)] [[TLDR/Notes](#rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation)]

- [24/05] **FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research**  
[[Paper](http://arxiv.org/pdf/2405.13576v2)] [[Code/Page](https://github.com/RUC-NLPIR/FlashRAG.)] [[TLDR/Notes](#flashrag--a-modular-toolkit-for-efficient-retrieval-augmented-generation-research)]



# TLDR/Notes
## effective-and-transparent-rag--adaptive-reward-reinforcement-learning-for-decision-traceability
### Abstract
Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£å†³RAGä¸¤å¤§ç—›ç‚¹ï¼ARENAç”¨å¼ºåŒ–å­¦ä¹ è®©ç”Ÿæˆæ›´æœ‰æ•ˆã€æ›´é€æ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Retrieval - Augmented Generationï¼ˆRAGï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡è¡¨ç°ä¸Šæ•ˆæœæ˜¾è‘—ï¼Œä½†ä»å­˜åœ¨ä¸¤å¤§æœªè§£å†³çš„æŒ‘æˆ˜ï¼š
1. **æœ‰æ•ˆæ€§ä¸è¶³**ï¼šç°æœ‰ç ”ç©¶å¤šèšç„¦äºæ‰“é€ æ›´å¼ºçš„RAGæ£€ç´¢å™¨ï¼Œå´å¿½ç•¥äº†å¦‚ä½•æå‡ç”Ÿæˆå™¨ï¼ˆLLMï¼‰åˆ©ç”¨æ£€ç´¢ä¿¡æ¯è¿›è¡Œæ¨ç†å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚å®é™…æµ‹è¯•å‘ç°ï¼Œåœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸åŒæ£€ç´¢ä¸Šä¸‹æ–‡ä¸‹7Bè§„æ¨¡çš„æ¨¡å‹ï¼Œå›ç­”å‡†ç¡®ç‡æ¯”OpenAI - o1ã€DeepSeek - R1ç­‰æ¨ç†æ¨¡å‹ä½15 - 35%ï¼Œè¿™è¡¨æ˜ç”Ÿæˆå™¨çš„æ¨ç†èƒ½åŠ›æ˜¯å½“å‰RAG pipelineçš„å…³é”®ç“¶é¢ˆã€‚
2. **é€æ˜åº¦ç¼ºå¤±**ï¼šå¤šæ•°RAGæ–¹æ³•æœªå…³æ³¨å“ªäº›æ£€ç´¢å†…å®¹çœŸæ­£å¯¹æ¨ç†è¿‡ç¨‹æœ‰è´¡çŒ®ï¼Œå¯¼è‡´å¯è§£é‡Šæ€§å’Œå¯è§æ€§ä¸è¶³ã€‚å¾ˆå¤šç”Ÿæˆå™¨è¾“å‡ºæ— ç»“æ„ç­”æ¡ˆï¼Œéšè—äº†å†³ç­–è¿‡ç¨‹ï¼Œé™ä½äº†å¯ä¿¡åº¦ï¼Œåœ¨å¤„ç†å¤šæ–‡æ¡£å¤šè·³æ¨ç†æ—¶ï¼Œå°æ¨¡å‹å› æ¨ç†èƒ½åŠ›æœ‰é™æ›´éš¾åº”å¯¹å™ªå£°æˆ–å†—ä½™ä¸Šä¸‹æ–‡ã€‚
åŒæ—¶ï¼Œç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œå¥–åŠ±è®¾è®¡é€šç”¨ã€è¾“å‡ºæ ¼å¼æœªè€ƒè™‘å¤šè·³QAç»“æ„ï¼Œä¸”KLæ­£åˆ™åŒ–ä¸ç¨³å®šæ˜“å¯¼è‡´è®­ç»ƒå‘æ•£ï¼Œé™åˆ¶äº†åœ¨æ£€ç´¢ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºARENAæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºARENAæ¡†æ¶ï¼Œå®ç°é€æ˜ä¸”æœ‰æ•ˆçš„RAGç”Ÿæˆ
ARENAï¼ˆAdaptive - Rewarded Evidence Navigation Agentï¼‰æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„é€æ˜RAGç”Ÿæˆå™¨æ¡†æ¶ã€‚å®ƒå¼•å…¥ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒ…å«é€‰å®šçš„å‚è€ƒèµ„æ–™ã€æ˜ç¡®çš„æ¨ç†è½¨è¿¹å’Œæœ€ç»ˆç­”æ¡ˆï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–ç”Ÿæˆï¼Œæ¨¡å‹èƒ½è¯†åˆ«å…³é”®è¯æ®ã€è¿›è¡Œç»“æ„åŒ–æ¨ç†å¹¶ç”Ÿæˆå¸¦æœ‰å¯è§£é‡Šå†³ç­–è½¨è¿¹çš„ç­”æ¡ˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è‡ªé€‚åº”ä»»åŠ¡ç‰¹å®šå¥–åŠ±ä¸ç¨³å®šä¼˜åŒ–ç­–ç•¥
ä¸ºå¤šè·³é—®ç­”ä»»åŠ¡å®šåˆ¶äº†ä¸€å¥—è‡ªé€‚åº”çš„ã€ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±æœºåˆ¶ï¼Œä»æ ¼å¼ã€å‡†ç¡®æ€§ã€ç›¸å…³æ€§å’Œé¢å¤–å¥–åŠ±ç­‰å¤šä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹è¾“å‡ºï¼Œæä¾›å¯è§£é‡Šä¸”ç»†ç²’åº¦çš„è®­ç»ƒä¿¡å·ã€‚åŒæ—¶æ”¹è¿›KLå…¬å¼å®ç°KLç¨³å®šåŒ–ï¼Œè§£å†³äº†ç°æœ‰RLæ–¹æ³•ä¸­KLæ­£åˆ™åŒ–ä¸ç¨³å®šå¯¼è‡´è®­ç»ƒå‘æ•£çš„é—®é¢˜ï¼Œè®©è®­ç»ƒæ›´ç¨³å®šé€‚ç”¨äºæ£€ç´¢ç±»ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å°†ARENAåº”ç”¨äºQwen2.5 - 7B - Instructå’ŒLlama3.1 - 8B - Instructç­‰å¼€æºæ¨¡å‹ï¼Œåœ¨å¤šä¸ªRAGåŸºçº¿çš„å¤§é‡å®éªŒè¡¨æ˜ï¼š
1. åœ¨æ‰€æœ‰å¤šè·³QAæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹å‡†ç¡®ç‡æå‡äº†10 - 30%ï¼Œæ€§èƒ½å¯ä¸OpenAI - o1ã€DeepSeek - R1ç­‰å•†ä¸šå¼€å‘çš„SOTAå¤§è¯­è¨€æ¨¡å‹åª²ç¾ã€‚
2. è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼ŒARENAåœ¨æ–°æ•°æ®é›†ä¸Šæ— éœ€é¢å¤–è®­ç»ƒå°±èƒ½å¾ˆå¥½åœ°é€‚é…ï¼Œå…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§ï¼Œåœ¨æ•°æ®é›†å’Œæ¨¡å‹ backbone ä¸Šæ³›åŒ–æ€§è‰¯å¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å…³æ³¨ç”Ÿæˆå™¨ä¼˜åŒ–**ï¼šè®ºæ–‡æŒ‡å‡ºå½“å‰RAGç³»ç»Ÿä¸­ç”Ÿæˆå™¨æ¨ç†èƒ½åŠ›ä¸è¶³æ˜¯å…³é”®é—®é¢˜ï¼Œæé†’ç ”ç©¶è€…ä»¬é™¤äº†æ£€ç´¢å™¨ï¼Œè¦é‡è§†ç”Ÿæˆå™¨çš„ä¼˜åŒ–ï¼Œä¸ºRAGç ”ç©¶æä¾›äº†æ–°çš„å…³æ³¨æ–¹å‘ã€‚
2. **å¼ºåŒ–å­¦ä¹ åœ¨RAGçš„åº”ç”¨æ¨¡å¼**ï¼šå±•ç¤ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆç»“æ„åŒ–ç”Ÿæˆã€è‡ªé€‚åº”å¥–åŠ±è®¾è®¡å’Œç¨³å®šè®­ç»ƒæ¥æå‡RAGæ¨ç†çš„é€æ˜æ€§å’Œæœ‰æ•ˆæ€§çš„æ¨¡å¼ï¼Œä¸ºåç»­åˆ©ç”¨RLä¼˜åŒ–RAGç”Ÿæˆå™¨æä¾›äº†å‚è€ƒæ¡†æ¶ã€‚
3. **å¼€æºèµ„æºè´¡çŒ®**ï¼šå…¬å¼€äº†æ¨¡å‹å’Œä»£ç ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šå¯¹RAGç”Ÿæˆå™¨ä¼˜åŒ–è¿›è¡Œç ”ç©¶ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸå‘å±•ã€‚
```

## hybrid-latent-reasoning-via-reinforcement-learning
### Abstract
Recent advances in large language models (LLMs) have introduced latent
reasoning as a promising alternative to autoregressive reasoning. By performing
internal computation with hidden states from previous steps, latent reasoning
benefit from more informative features rather than sampling a discrete
chain-of-thought (CoT) path. Yet latent reasoning approaches are often
incompatible with LLMs, as their continuous paradigm conflicts with the
discrete nature of autoregressive generation. Moreover, these methods rely on
CoT traces for training and thus fail to exploit the inherent reasoning
patterns of LLMs. In this work, we explore latent reasoning by leveraging the
intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we
introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid
latent reasoning approach that (1) integrates prior hidden states into sampled
tokens with a learnable gating mechanism, and (2) initializes training with
predominantly token embeddings while progressively incorporating more hidden
features. This design maintains LLMs' generative capabilities and incentivizes
hybrid reasoning using both discrete and continuous representations. In
addition, the hybrid HRPO introduces stochasticity into latent reasoning via
token sampling, thereby enabling RL-based optimization without requiring CoT
trajectories. Extensive evaluations across diverse benchmarks show that HRPO
outperforms prior methods in both knowledge- and reasoning-intensive tasks.
Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing
behaviors like cross-lingual patterns and shorter completion lengths,
highlighting the potential of our RL-based approach and offer insights for
future work in latent reasoning.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„æ··åˆæ½œåœ¨æ¨ç†ï¼šè§£é”å¤§æ¨¡å‹å†…åœ¨æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼Œæ½œåœ¨æ¨ç†ä½œä¸º autoregressiveï¼ˆè‡ªå›å½’ï¼‰æ¨ç†çš„æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºæ½œåŠ›ã€‚ä¼ ç»Ÿè‡ªå›å½’æ¨ç†ä¾èµ–ç¦»æ•£çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è§£ç ä¸é‡‡æ ·ï¼Œè€Œæ½œåœ¨æ¨ç†å€ŸåŠ©å‰åºæ­¥éª¤çš„è¿ç»­éšè—çŠ¶æ€å®ç°å†…éƒ¨æ¨ç†ï¼Œèƒ½åˆ©ç”¨æ›´ä¸°å¯Œä¿¡æ¯ã€‚ä½†ç°æœ‰æ½œåœ¨æ¨ç†æ–¹æ³•å­˜åœ¨è¯¸å¤šé—®é¢˜ï¼šä¸€æ˜¯ä¸LLMså…¼å®¹æ€§å·®ï¼Œè¿ç»­èŒƒå¼å’Œè‡ªå›å½’ç”Ÿæˆçš„ç¦»æ•£æ€§å†²çªï¼Œå°†éšè—çŠ¶æ€è¾“å…¥ä¸‹ä¸€æ­¥è§£ç ä¼šé™ä½ç”Ÿæˆè´¨é‡ï¼ˆå¦‚é‡å¤ã€ä¸è¿è´¯ï¼‰ï¼›äºŒæ˜¯ä¾èµ–CoTè½¨è¿¹è®­ç»ƒï¼Œæ—¢å¿½è§†LLMså›ºæœ‰æ¨ç†èƒ½åŠ›ï¼Œåˆå¸¦æ¥é«˜æ˜‚è®­ç»ƒæˆæœ¬ï¼ˆå¦‚å¤šé˜¶æ®µè®­ç»ƒã€ä»å¤´è®­å¤šå—æ¨¡å‹ï¼‰ï¼Œé™åˆ¶äº†é€‚ç”¨èŒƒå›´ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½æ— ç¼æ•´åˆè¿ç»­è¡¨ç¤ºã€ä¾æ‰˜é¢„è®­ç»ƒLLMsæ³›åŒ–æ€§ã€å‡å°‘CoTä¾èµ–çš„æ½œåœ¨æ¨ç†æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºHRPOæ¡†æ¶ï¼Œé¦–æ¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ··åˆæ¨ç†æ–¹æ¡ˆ  
Hybrid Reasoning Policy Optimizationï¼ˆHRPOï¼‰æ˜¯é¦–ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ··åˆæ½œåœ¨æ¨ç†ä¼˜åŒ–æ¡†æ¶ï¼ŒæŠŠç­–ç•¥å­¦ä¹ å’Œæ½œåœ¨æ¨ç†ç»Ÿä¸€èµ·æ¥ï¼Œæ— éœ€ä¾èµ–CoTè½¨è¿¹å°±èƒ½åˆ©ç”¨LLMså†…åœ¨æ¨ç†æ¨¡å¼ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹è‡ªä¸»å‘å±•æ½œåœ¨æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ½œåœ¨æ¨ç†æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–°è·¯å¾„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡é—¨æ§æœºåˆ¶ï¼Œå¹³è¡¡ç”Ÿæˆèƒ½åŠ›ä¸è¿ç»­æ¨ç†  
ä¸ºåœ¨ä¿ç•™LLMsç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå¼•å¯¼æ¨¡å‹åœ¨è¿ç»­ç©ºé—´æ¨ç†ï¼ŒHRPOå¼•å…¥**å¯å­¦ä¹ é—¨æ§æœºåˆ¶**ã€‚è®­ç»ƒåˆæœŸï¼Œè¾“å…¥ä¸»è¦æ¥è‡ªé‡‡æ ·tokençš„åµŒå…¥ï¼Œä¿è¯ç”Ÿæˆè´¨é‡ï¼›éšè®­ç»ƒæ¨è¿›ï¼Œé—¨æ§å­¦ä¹ èå…¥å‰åºéšè—çŠ¶æ€ä¸­æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼ŒåŠ©åŠ›å†…éƒ¨æ¨ç†ã€‚è¿™ç§æ¸è¿›å¼èåˆï¼Œè®©ç¦»æ•£tokenå’Œè¿ç»­éšè—è¡¨ç¤ºååŒä½œç”¨ï¼Œå®ç°æ··åˆæ¨ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå€ŸåŠ©é‡‡æ ·éšæœºæ€§ï¼Œå®ç°æ— CoTçš„RLä¼˜åŒ–  
HRPOé€šè¿‡tokené‡‡æ ·ç»™æ½œåœ¨æ¨ç†å¼•å…¥éšæœºæ€§ï¼Œä½¿å¾—rolloutï¼ˆè½¨è¿¹å±•å¼€ï¼‰èƒ½åƒæ ‡å‡†RLæ–¹æ³•ä¸€æ ·æ‰§è¡Œã€‚æ··åˆè¾“å‡ºï¼ˆtoken + æ½œåœ¨è¡¨ç¤ºï¼‰å­˜å…¥rollout bufferç”¨äºç­–ç•¥æ›´æ–°ï¼ŒåŸºäºç®€å•çš„â€œç»“æœå¯¼å‘å‹å¥–åŠ±â€è®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼Œå®ç°ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œè‡ªé€‚åº”æ•´åˆtokençº§å’Œæ½œåœ¨è¡¨ç¤ºä¿¡æ¯ï¼Œè§£é”ç°æœ‰LLMsçš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHRPOè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬å…¶ä»–æ½œåœ¨æ¨ç†åŸºçº¿ï¼‰ï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹æŒç»­æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç»HRPOè®­ç»ƒçš„LLMsä¿ç•™äº†å¯è§£é‡Šæ€§ï¼Œè¿˜å±•ç°å‡ºè·¨è¯­è¨€æ¨¡å¼ã€æ›´çŸ­ç”Ÿæˆé•¿åº¦ç­‰æœ‰è¶£è¡Œä¸ºï¼ŒéªŒè¯äº†åŸºäºRLçš„æ··åˆæ½œåœ¨æ¨ç†æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œä¹Ÿä¸ºåç»­æ½œåœ¨æ¨ç†ç ”ç©¶æä¾›äº†æ–°è§†è§’ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•åˆ›æ–°è§’åº¦ï¼šå°†å¼ºåŒ–å­¦ä¹ ä¸æ½œåœ¨æ¨ç†ç»“åˆï¼Œè·³å‡ºâ€œä¾èµ–CoTæ ‡æ³¨å’Œå¤šé˜¶æ®µè®­ç»ƒâ€çš„ä¼ ç»Ÿæ€è·¯ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡å¼€è¾Ÿæ–°èŒƒå¼ï¼Œè¯æ˜äº†RLåœ¨è§£é”LLMså†…åœ¨æ¨ç†æ¨¡å¼ä¸Šçš„ä»·å€¼ã€‚  
2. å·¥ç¨‹è®¾è®¡è§’åº¦ï¼šé—¨æ§æœºåˆ¶çš„â€œæ¸è¿›å¼èåˆâ€æ€è·¯ï¼Œä¸ºå¹³è¡¡æ¨¡å‹æ—¢æœ‰èƒ½åŠ›ï¼ˆå¦‚ç”Ÿæˆï¼‰ä¸æ–°èƒ½åŠ›ï¼ˆå¦‚è¿ç»­ç©ºé—´æ¨ç†ï¼‰æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼Œåœ¨éœ€å…¼é¡¾â€œå­˜é‡èƒ½åŠ›â€å’Œâ€œå¢é‡èƒ½åŠ›â€çš„æ¨¡å‹ä¼˜åŒ–åœºæ™¯ä¸­å…·å€Ÿé‰´æ€§ã€‚  
3. å®éªŒä¸åˆ†æè§’åº¦ï¼šä¸ä»…éªŒè¯æ€§èƒ½ï¼Œè¿˜å…³æ³¨æ¨¡å‹è¡Œä¸ºï¼ˆå¯è§£é‡Šæ€§ã€è·¨è¯­è¨€ç­‰ï¼‰ï¼Œè¿™ç§ä»â€œæ•ˆæœâ€åˆ°â€œç‰¹æ€§â€çš„å…¨é¢åˆ†æï¼Œèƒ½å¸®åŠ©ç ”ç©¶è€…æ›´æ·±å…¥ç†è§£æ–¹æ³•å¯¹æ¨¡å‹çš„å¡‘é€ ï¼Œä¸ºåç»­ä¼˜åŒ–æŒ‡æ˜æ–¹å‘ã€‚  
```

## evolvesearch--an-iterative-self-evolving-search-agent
### Abstract
The rapid advancement of large language models (LLMs) has transformed the
landscape of agentic information seeking capabilities through the integration
of tools such as search engines and web browsers. However, current mainstream
approaches for enabling LLM web search proficiency face significant challenges:
supervised fine-tuning struggles with data production in open-search domains,
while RL converges quickly, limiting their data utilization efficiency. To
address these issues, we propose EvolveSearch, a novel iterative self-evolution
framework that combines SFT and RL to enhance agentic web search capabilities
without any external human-annotated reasoning data. Extensive experiments on
seven multi-hop question-answering (MHQA) benchmarks demonstrate that
EvolveSearch consistently improves performance across iterations, ultimately
achieving an average improvement of 4.7\% over the current state-of-the-art
across seven benchmarks, opening the door to self-evolution agentic
capabilities in open web search domains.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | EvolveSearchï¼šå¼€å¯æ™ºèƒ½ä½“ç½‘ç»œæœç´¢è‡ªè¿›åŒ–æ–°ç¯‡ç« 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é£é€Ÿå‘å±•ï¼Œå€ŸåŠ©æœç´¢å¼•æ“ã€ç½‘é¡µæµè§ˆå™¨ç­‰å·¥å…·é©æ–°äº†æ™ºèƒ½ä½“ä¿¡æ¯è·å–èƒ½åŠ›ï¼Œä½†å½“å‰ä¸»æµè®©LLMå…·å¤‡ç½‘ç»œæœç´¢èƒ½åŠ›çš„æ–¹æ³•å­˜åœ¨æ˜æ˜¾æŒ‘æˆ˜ï¼šæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¼€æ”¾æœç´¢é¢†åŸŸé¢ä¸´æ•°æ®ç”Ÿæˆéš¾é¢˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ”¶æ•›è¿‡å¿«å¯¼è‡´æ•°æ®åˆ©ç”¨æ•ˆç‡å—é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºEvolveSearchæ¡†æ¶ï¼Œæ—¨åœ¨æ— éœ€å¤–éƒ¨äººå·¥æ ‡æ³¨æ¨ç†æ•°æ®çš„æƒ…å†µä¸‹å¢å¼ºæ™ºèƒ½ä½“ç½‘ç»œæœç´¢èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–åˆ›è¿­ä»£å¼è‡ªè¿›åŒ–æ¡†æ¶
EvolveSearchæ˜¯é¦–ä¸ªå°†å¼ºåŒ–å­¦ä¹ ä¸æœ‰ç›‘ç£å¾®è°ƒè¿­ä»£ç»“åˆï¼Œç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹ç½‘ç»œæœç´¢åœºæ™¯èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡äº¤æ›¿è¿›è¡ŒRLæ¢ç´¢é˜¶æ®µä¸SFTä¼˜åŒ–é˜¶æ®µï¼Œè®©æ¨¡å‹ä»è‡ªèº«ç»éªŒä¸­å­¦ä¹ é²æ£’æœ‰æ•ˆçš„æœç´¢è¡Œä¸ºï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚
- RLæ¢ç´¢é˜¶æ®µï¼šæ¨¡å‹ä¸ç½‘ç»œæœç´¢ç¯å¢ƒäº¤äº’ï¼Œåˆ©ç”¨å·¥å…·èƒ½åŠ›å¹¶æ¥æ”¶æ··åˆå¥–åŠ±ä¿¡å·ï¼Œä»é«˜å¥–åŠ±çš„rolloutsï¼ˆæ‰§è¡Œè½¨è¿¹ï¼‰ä¸­å­¦ä¹ ã€‚
- SFTä¼˜åŒ–é˜¶æ®µï¼šä¾æ®ä¸‰ä¸ªæ ‡å‡†ç­›é€‰RLé˜¶æ®µä¸­è¡¨ç°æœ€ä½³çš„rolloutsï¼Œç”¨äºSFTä¼˜åŒ–æ¨¡å‹ï¼Œä¸ºä¸‹ä¸€ä¸ªRLå¾ªç¯æä¾›æ›´å¼ºçš„åˆå§‹åŒ–ï¼ˆå†·å¯åŠ¨ç­–ç•¥ï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— äººå·¥æ ‡æ³¨æ¨ç†æ•°æ®ä¾èµ–
EvolveSearchä¸ä¾èµ–å¤–éƒ¨äººå·¥æ ‡æ³¨çš„æ¨ç†æ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å‹äº§ç”Ÿçš„é«˜è´¨é‡rolloutsï¼Œé€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„ç›‘ç£ä¿¡å·å®ç°æŒç»­è‡ªæˆ‘æ”¹è¿›ã€‚åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œå°†RLå¾—åˆ°çš„ä¼˜è´¨rolloutsèå…¥æ•°æ®æ± ï¼Œç»ç­›é€‰åç”¨äºåç»­SFTï¼Œè®©æ¨¡å‹ä¸æ–­è¿›åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ä¸ƒä¸ªå¤šè·³é—®ç­”ï¼ˆMHQAï¼‰åŸºå‡†æµ‹è¯•ä¸Šå¼€å±•å¤§é‡å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒEvolveSearchåœ¨è¿­ä»£è¿‡ç¨‹ä¸­æ€§èƒ½æŒç»­æå‡ï¼Œæœ€ç»ˆåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾ƒå½“å‰æœ€å…ˆè¿›æ–¹æ³•å¹³å‡æå‡4.7%ï¼Œæœ‰åŠ›è¯æ˜äº†åœ¨è‡ªè¿›åŒ–æ¡†æ¶ä¸­ç»“åˆæœ‰ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œä¹Ÿå±•ç°äº†ä»é«˜å¥–åŠ±rolloutsä¸­è¿­ä»£å­¦ä¹ ã€ä¸ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®å°±èƒ½å®ç°æ€§èƒ½å¤§å¹…æå‡çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ¡†æ¶è®¾è®¡æ€è·¯ï¼šå…¶è¿­ä»£ç»“åˆRLä¸SFTçš„æ€è·¯ä¸ºè§£å†³å¼€æ”¾é¢†åŸŸä»»åŠ¡ä¸­æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹æ€§èƒ½æå‡éš¾é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘åç»­åœ¨æ™ºèƒ½ä½“èƒ½åŠ›å¢å¼ºã€å·¥å…·ä½¿ç”¨ä¼˜åŒ–ç­‰æ–¹å‘çš„ç ”ç©¶ã€‚
- æ•°æ®åˆ©ç”¨æ–¹å¼ï¼šä¸ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®ï¼Œè½¬è€Œä»æ¨¡å‹è‡ªèº«äº¤äº’ç»éªŒä¸­æŒ–æ˜ä¼˜è´¨æ•°æ®ç”¨äºè¿­ä»£ä¼˜åŒ–ï¼Œè¿™ç§è‡ªç›‘ç£å¼çš„æ”¹è¿›è·¯å¾„ä¸ºæ•°æ®è·å–å›°éš¾åœºæ™¯ä¸‹çš„æ¨¡å‹è®­ç»ƒæä¾›äº†å‚è€ƒã€‚
- å¤šä»»åŠ¡éªŒè¯æ¨¡å¼ï¼šåœ¨å¤šä¸ªMHQAæ•°æ®é›†ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ä¸é€šç”¨æ€§ï¼Œè¿™ç§å…¨é¢çš„å®éªŒéªŒè¯æ–¹å¼å€¼å¾—ç§‘ç ”å·¥ä½œä¸­å€Ÿé‰´ï¼Œä»¥å……åˆ†è¯´æ˜æ–¹æ³•çš„æ™®é€‚ä»·å€¼ã€‚
```

## collab-rag--boosting-retrieval-augmented-generation-for-complex-question-answering-via-white-box-and-black-box-llm-collaboration
### Abstract
Retrieval-Augmented Generation (RAG) systems often struggle to handle
multi-hop question-answering tasks accurately due to irrelevant context
retrieval and limited complex reasoning capabilities. We introduce Collab-RAG,
a collaborative training framework that leverages mutual enhancement between a
white-box small language model (SLM) and a blackbox large language model (LLM)
for RAG. Specifically, the SLM decomposes complex queries into simpler
sub-questions, thus enhancing the accuracy of the retrieval and facilitating
more effective reasoning by the black-box LLM. Concurrently, the black-box LLM
provides feedback signals to improve the SLM's decomposition capability. We
observe that Collab-RAG relies solely on supervision from an affordable
black-box LLM without additional distillation from frontier LLMs, yet
demonstrates strong generalization across multiple black-box LLMs. Experimental
evaluations across five multi-hop QA datasets demonstrate that Collab-RAG
substantially outperforms existing black-box-only and SLM fine-tuning baselines
by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a
frozen 32B LLM in question decomposition, highlighting the efficiency of
Collab-RAG in improving reasoning and retrieval for complex questions. The code
of Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Collab-RAGï¼šç™½ç›’å°æ¨¡å‹ä¸é»‘ç›’å¤§æ¨¡å‹åä½œï¼Œçªç ´å¤æ‚é—®ç­”åœºæ™¯ä¸‹çš„RAGç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½åœ¨ä¼—å¤šè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å¹»è§‰ã€éš¾é€‚é…é¢†åŸŸçŸ¥è¯†ç­‰é—®é¢˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯é€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œç„¶è€Œåœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­ï¼ŒRAGå¸¸å› æ£€ç´¢åˆ°æ— å…³ä¸Šä¸‹æ–‡ã€å¤æ‚æ¨ç†èƒ½åŠ›å—é™è€Œè¡¨ç°ä¸ä½³ã€‚ç°æœ‰æå‡æ£€ç´¢è´¨é‡çš„æ–¹æ³•å¤šèšç„¦å•æ­¥æ£€ç´¢ä¼˜åŒ–ï¼Œéš¾åº”å¯¹éœ€è¿­ä»£è¯æ®æ”¶é›†çš„å¤æ‚é—®ç­”ï¼›æ— è®­ç»ƒçš„LLMæŸ¥è¯¢åˆ†è§£èƒ½åŠ›æœ‰é™ï¼›å°æ¨¡å‹å¾®è°ƒæ–¹æ³•å¯¹é»‘ç›’LLMå‚æ•°æ›´æ–°åˆä½æ•ˆæ˜‚è´µã€‚å› æ­¤ï¼Œå……åˆ†é‡Šæ”¾é»‘ç›’LLMåœ¨å¤æ‚é—®ç­”çš„èƒ½åŠ›ä»å…·æŒ‘æˆ˜ï¼Œè¿™å‚¬ç”Ÿäº†Collab - RAGçš„ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCollab - RAGæ¡†æ¶ï¼Œå®ç°ç™½ç›’å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸é»‘ç›’å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠ¨æ€åä½œ  
SLMä½œä¸ºåˆ†è§£å™¨ï¼Œå°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºæ›´ç®€å•çš„å­é—®é¢˜ï¼Œæå‡æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡çš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºé»‘ç›’LLMæ›´æœ‰æ•ˆæ¨ç†é“ºè·¯ï¼›é»‘ç›’LLMä½œä¸ºé˜…è¯»å™¨ï¼Œä¸ºæ¯ä¸ªå­é—®é¢˜ç”Ÿæˆä¸­é—´ç­”æ¡ˆå¹¶åˆæˆæœ€ç»ˆå“åº”ï¼Œå€ŸåŠ©å­é—®é¢˜æ£€ç´¢æ¥é€æ­¥è§£ç­”å¤æ‚é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºé»‘ç›’LLMåé¦ˆçš„è‡ªæ”¹è¿›è®­ç»ƒç­–ç•¥  
ç›´æ¥ç”¨SLMåšé—®é¢˜åˆ†è§£å› æ¨ç†èƒ½åŠ›æœ‰é™ä¸”é«˜è´¨é‡æ ‡æ³¨æˆæœ¬é«˜è€Œæ•ˆæœä¸ä½³ã€‚Collab - RAGä»…ä¾èµ–é»‘ç›’LLMï¼ˆå¦‚GPT - 4o - miniï¼‰çš„åé¦ˆæ¥ä¼˜åŒ–ï¼Œå°†é»‘ç›’LLMå»ºæ¨¡ä¸ºèƒ½ç”Ÿæˆå“åº”çš„ç¯å¢ƒï¼ŒSLMä¸ä¹‹å¤šè½®äº¤äº’è¿­ä»£ä¼˜åŒ–åˆ†è§£ç­–ç•¥ã€‚è®¾è®¡è¿­ä»£åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œä¾æ®é»‘ç›’LLMåé¦ˆï¼ˆé€šè¿‡åŸºäºè§„åˆ™çš„è¯„ä¼°æ–¹æ³•ï¼Œä»å­é—®é¢˜æ ¼å¼å’Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§åˆ¤æ–­åˆ†è§£æ˜¯å¦æœ‰æ•ˆï¼‰æå‡SLMåˆ†è§£èƒ½åŠ›ï¼Œæ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨æˆ–å‰æ²¿LLMè’¸é¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒCollab - RAGå¹³å‡æ¯”ç°æœ‰ä»…é»‘ç›’LLMå’ŒSLMå¾®è°ƒåŸºçº¿æ–¹æ³•æ€§èƒ½é«˜å‡º1.8% - 14.2%ã€‚åœ¨é—®é¢˜åˆ†è§£ä»»åŠ¡ä¸Šï¼Œå¾®è°ƒåçš„3Bå‚æ•°SLMè¡¨ç°è¶…è¿‡å†»ç»“çš„32Bå‚æ•°LLMï¼Œæœ‰åŠ›è¯æ˜äº†Collab - RAGåœ¨æå‡å¤æ‚é—®é¢˜æ¨ç†å’Œæ£€ç´¢æ–¹é¢çš„é«˜æ•ˆæ€§ï¼Œä¸”ä»…ç”¨GPT - 4o - miniç›‘ç£è®­ç»ƒå´èƒ½åœ¨å¤šä¸ªé»‘ç›’LLMä¸Šæœ‰å¼ºæ³›åŒ–æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹åä½œæ€è·¯ï¼šåœ¨RAGç­‰éœ€å¤šç»„ä»¶é…åˆçš„ä»»åŠ¡ä¸­ï¼Œæ¢ç´¢ä¸åŒâ€œèƒ½åŠ›äº’è¡¥â€æ¨¡å‹ï¼ˆå¦‚ç™½ç›’å°æ¨¡å‹ä¸é»‘ç›’å¤§æ¨¡å‹ï¼‰çš„åä½œæ¨¡å¼ï¼Œå‘æŒ¥å„è‡ªä¼˜åŠ¿ï¼ˆå°æ¨¡å‹å¯è®­ç»ƒä¼˜åŒ–ã€å¤§æ¨¡å‹å¼ºæ¨ç†ç­‰ï¼‰ã€‚
2. è®­ç»ƒä¼˜åŒ–æ–¹å¼ï¼šåˆ©ç”¨ç°æœ‰é»‘ç›’å¤§æ¨¡å‹åé¦ˆæ¥ä¼˜åŒ–å°æ¨¡å‹ï¼Œé¿å…é«˜æˆæœ¬æ ‡æ³¨ä¸å‰æ²¿å¤§æ¨¡å‹è’¸é¦ï¼Œä¸ºèµ„æºæœ‰é™æƒ…å†µä¸‹æå‡æ¨¡å‹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚æŸ¥è¯¢åˆ†è§£ï¼‰æä¾›æ€è·¯ã€‚
3. å¤æ‚ä»»åŠ¡å¤„ç†ï¼šé’ˆå¯¹å¤æ‚å¤šè·³é—®ç­”è¿™ç±»éœ€åˆ†æ­¥æ¨ç†ã€å¤šè¯æ®æ•´åˆçš„ä»»åŠ¡ï¼Œé€šè¿‡â€œåˆ†è§£ - å­ä»»åŠ¡å¤„ç† - åˆæˆâ€çš„ pipeline è®¾è®¡ï¼Œä¸ºçªç ´ä»»åŠ¡éš¾ç‚¹æä¾›äº†å¯å‚è€ƒçš„æ¶æ„èŒƒå¼ã€‚
```

## search-r1--training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning
### Abstract
Efficiently acquiring external knowledge and up-to-date information is
essential for effective reasoning and text generation in large language models
(LLMs). Prompting advanced LLMs with reasoning capabilities to use search
engines during inference is often suboptimal, as the LLM might not fully
possess the capability on how to interact optimally with the search engine.
This paper introduces Search-R1, an extension of reinforcement learning (RL)
for reasoning frameworks where the LLM learns to autonomously generate
(multiple) search queries during step-by-step reasoning with real-time
retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn
search interactions, leveraging retrieved token masking for stable RL training
and a simple outcome-based reward function. Experiments on seven
question-answering datasets show that Search-R1 improves performance by 41%
(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same
setting. This paper further provides empirical insights into RL optimization
methods, LLM choices, and response length dynamics in retrieval-augmented
reasoning. The code and model checkpoints are available at
https://github.com/PeterGriffinJin/Search-R1.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Search-R1ï¼šç”¨å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹å­¦ä¼šæ¨ç†ä¸æœç´¢å¼•æ“åä½œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆæ–¹é¢è¡¨ç°å“è¶Šï¼Œä½†é¢å¯¹å¤æ‚æ¨ç†ä»»åŠ¡å’Œè·å–å¤–éƒ¨å®æ—¶ä¿¡æ¯æ—¶ä»å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰å°†LLMsä¸æœç´¢å¼•æ“ç»“åˆçš„æ–¹å¼ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”ŸæˆRAGã€æŠŠæœç´¢å¼•æ“å½“å·¥å…·ï¼‰å­˜åœ¨å±€é™ï¼šRAGè™½èƒ½åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œä½†LLMsåœ¨è®­ç»ƒä¸­æœªè¢«ä¼˜åŒ–ä»¥é«˜æ•ˆä¸æœç´¢å¼•æ“äº¤äº’ï¼›å·¥å…·ç±»æ–¹æ³•é‡Œï¼ŒåŸºäºæç¤ºçš„æ–¹å¼æ³›åŒ–æ€§å·®ï¼ŒåŸºäºè®­ç»ƒçš„æ–¹å¼åˆå› ä¾èµ–å¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨è½¨è¿¹å’Œæœç´¢æ“ä½œä¸å¯å¾®åˆ†ï¼Œéš¾ä»¥æœ‰æ•ˆæ‰©å±•ã€‚åŒæ—¶ï¼ŒæŠŠå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºâ€œæœç´¢ + æ¨ç†â€åœºæ™¯ä¹Ÿé¢ä¸´æ¡†æ¶ç¨³å®šæ€§ã€å¤šè½®äº¤é”™æ¨ç†ä¸æœç´¢ã€å¥–åŠ±è®¾è®¡ä¸‰å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLMsåœ¨æ¨ç†æ—¶è‡ªä¸»ä¸”é«˜æ•ˆåœ°åˆ©ç”¨æœç´¢å¼•æ“ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå«æœç´¢å¼•æ“çš„RLç¯å¢ƒä¸ç¨³å®šä¼˜åŒ–æœºåˆ¶  
Search - R1å°†æœç´¢å¼•æ“å»ºæ¨¡ä¸ºç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼Œè®©è½¨è¿¹åºåˆ—èƒ½äº¤é”™ç”ŸæˆLLM tokenså’Œæ‰§è¡Œæœç´¢å¼•æ“æ£€ç´¢ã€‚å®ƒå…¼å®¹PPOã€GRPOç­‰å¤šç§RLç®—æ³•ï¼Œè¿˜é€šè¿‡â€œæ£€ç´¢tokenæ©ç â€æŠ€æœ¯ä¿éšœRLè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè®©æ¨¡å‹åœ¨æ•´åˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ—¶ä¹Ÿèƒ½ç¨³å®šä¼˜åŒ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šè½®æ£€ç´¢ä¸æ¨ç†çš„äº¤äº’æµç¨‹  
æ¨¡å‹å¯åœ¨<search>å’Œ</search>æ ‡è®°è§¦å‘ä¸‹è°ƒç”¨æœç´¢ï¼Œæ£€ç´¢å†…å®¹è¢«åŒ…è£¹åœ¨<information>å’Œ</information>é—´ï¼ŒLLMæ¨ç†æ­¥éª¤ç”¨<think>å’Œ<think>å°è£…ï¼Œæœ€ç»ˆç­”æ¡ˆä¹Ÿä»¥ç‰¹å®šæ ¼å¼è¾“å‡ºï¼Œä»¥æ­¤å®ç°ç»“æ„åŒ–ã€è¿­ä»£å¼çš„å†³ç­–è¿‡ç¨‹ï¼Œè®©å¤§æ¨¡å‹èƒ½ä¾æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼Œå®Œæˆå¤šè½®æ¨ç†ä¸æœç´¢çš„äº¤é”™åä½œã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç®€æ´çš„åŸºäºç»“æœçš„å¥–åŠ±å‡½æ•°è®¾è®¡  
æ‘’å¼ƒå¤æ‚çš„è¿‡ç¨‹å‹å¥–åŠ±ï¼Œé‡‡ç”¨ç®€å•çš„ç»“æœå¯¼å‘å¥–åŠ±å‡½æ•°ã€‚å®éªŒè¯æ˜è¿™ç§æç®€è®¾è®¡åœ¨â€œæœç´¢ + æ¨ç†â€åœºæ™¯ä¸­èƒ½æœ‰æ•ˆå¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼ŒåŠ©åŠ›Search - R1æˆä¸ºDeepSeek - R1 Zeroçš„æ‰©å±•ï¼Œä¸ºæ£€ç´¢é©±åŠ¨å†³ç­–å¼•å…¥æœç´¢å¢å¼ºçš„RLè®­ç»ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šå¼€å±•å®éªŒï¼ŒåŒä¸€è®¾ç½®ä¸‹ï¼ˆç›¸åŒæ£€ç´¢æ¨¡å‹ã€è®­ç»ƒæ•°æ®ã€é¢„è®­ç»ƒLLMï¼‰ï¼ŒSearch - R1è®©Qwen2.5 - 7Bå’ŒQwen2.5 - 3Båˆ†åˆ«æ¯”å„ç±»RAGåŸºçº¿æ¨¡å‹æ€§èƒ½æå‡41%å’Œ20%ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜åœ¨RLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©ã€æ£€ç´¢å¢å¼ºæ¨ç†ä¸­çš„å“åº”é•¿åº¦åŠ¨æ€ç­‰æ–¹é¢ç»™å‡ºäº†å®è¯æ€§è§è§£ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æŠ€æœ¯æ€è·¯å±‚é¢ï¼šä¸ºè§£å†³å¤§æ¨¡å‹â€œæœç´¢ + æ¨ç†â€éš¾é¢˜æä¾›äº†RLæ¡†æ¶æ–°æ€è·¯ï¼ŒæŠŠæœç´¢å¼•æ“çº³å…¥ç¯å¢ƒã€è®¾è®¡å¤šè½®äº¤äº’æµç¨‹ç­‰åšæ³•ï¼Œä¸ºåç»­ä¼˜åŒ–å¤§æ¨¡å‹å¤–éƒ¨çŸ¥è¯†åˆ©ç”¨æ–¹å¼æä¾›äº†å‚è€ƒèŒƒå¼ã€‚  
2. å®éªŒä¸åˆ†æå±‚é¢ï¼šç³»ç»Ÿçš„å®éªŒä¸ä»…éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿˜å¯¹RLæ–¹æ³•é€‰æ‹©ã€ä¸åŒLLMé€‚é…ã€å“åº”é•¿åº¦ç­‰ç»´åº¦å±•å¼€ç ”ç©¶ï¼Œè¿™äº›å®è¯æ´å¯Ÿèƒ½è¾…åŠ©ç ”ç©¶è€…åœ¨ç±»ä¼¼â€œæ£€ç´¢ + æ¨ç†â€ä»»åŠ¡ä¸­åšæ›´ä¼˜å†³ç­–ã€‚  
3. å·¥ç¨‹è½åœ°å±‚é¢ï¼šä»£ç å’Œæ¨¡å‹ checkpoint å¼€æºï¼ˆhttps://github.com/PeterGriffinJin/Search - R1ï¼‰ï¼Œä¾¿äºç¤¾åŒºåŸºäºè¯¥å·¥ä½œè¿›ä¸€æ­¥æ¢ç´¢å¤§æ¨¡å‹ä¸æœç´¢å¼•æ“åä½œçš„æ›´å¤šå¯èƒ½ã€‚  
```

## search-o1--agentic-search-enhanced-large-reasoning-models
### Abstract
Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive
long stepwise reasoning capabilities through large-scale reinforcement
learning. However, their extended reasoning processes often suffer from
knowledge insufficiency, leading to frequent uncertainties and potential
errors. To address this limitation, we introduce \textbf{Search-o1}, a
framework that enhances LRMs with an agentic retrieval-augmented generation
(RAG) mechanism and a Reason-in-Documents module for refining retrieved
documents. Search-o1 integrates an agentic search workflow into the reasoning
process, enabling dynamic retrieval of external knowledge when LRMs encounter
uncertain knowledge points. Additionally, due to the verbose nature of
retrieved documents, we design a separate Reason-in-Documents module to deeply
analyze the retrieved information before injecting it into the reasoning chain,
minimizing noise and preserving coherent reasoning flow. Extensive experiments
on complex reasoning tasks in science, mathematics, and coding, as well as six
open-domain QA benchmarks, demonstrate the strong performance of Search-o1.
This approach enhances the trustworthiness and applicability of LRMs in complex
reasoning tasks, paving the way for more reliable and versatile intelligent
systems. The code is available at
\url{https://github.com/sunnynexus/Search-o1}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Search-o1ï¼šè®©å¤§æ¨ç†æ¨¡å‹æ‹¥æœ‰æ™ºèƒ½æœç´¢å¢å¼ºèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œä»¥OpenAI - o1ã€Qwen - QwQã€DeepSeek - R1ä¸ºä»£è¡¨çš„å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºå¼ºå¤§çš„é•¿æ­¥éª¤æ¨ç†èƒ½åŠ›ï¼Œèƒ½åœ¨æ•°å­¦ã€ç¼–ç ç­‰å¤æ‚ä»»åŠ¡ä¸­æ¨¡æ‹Ÿäººç±»è§£å†³é—®é¢˜çš„æ€è·¯ã€‚ä½†è¿™ç±»æ¨¡å‹åœ¨å»¶é•¿æ¨ç†é“¾æ—¶ï¼Œå¸¸å› **çŸ¥è¯†ä¸è¶³**å¼•å‘ä¸ç¡®å®šæ€§ä¸é”™è¯¯ã€‚æ¯”å¦‚åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­ï¼Œæ¨¡å‹ä¼šé¢‘ç¹å‡ºç°â€œperhapsâ€â€œalternativelyâ€è¿™ç±»ä½“ç°ä¸ç¡®å®šçš„è¯æ±‡ï¼ˆå¦‚å›¾1å·¦æ‰€ç¤ºï¼‰ï¼›ä¼ ç»Ÿé¢å‘é—®é¢˜çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä¹Ÿéš¾ä»¥æœ‰æ•ˆå¡«è¡¥çŸ¥è¯†ç¼ºå£ï¼ˆå¦‚å›¾1å³å¯¹æ¯”å®éªŒï¼‰ã€‚è€Œä¸”äººå·¥éªŒè¯æ¨ç†è¿‡ç¨‹æˆæœ¬é«˜ï¼Œè‡ªåŠ¨è¡¥å……o1ç±»æ¨ç†æ‰€éœ€çŸ¥è¯†æˆäº†éš¾é¢˜ï¼Œé™åˆ¶äº†LRMså®ç°æ›´å¯é æ¨ç†çš„å‘å±•ã€‚äºæ˜¯ï¼Œè®ºæ–‡å›¢é˜Ÿå¸Œæœ›é€šè¿‡è®©æ¨¡å‹è‡ªä¸»æ£€ç´¢æ¥å¢å¼ºLRMsçš„o1å¼æ¨ç†èƒ½åŠ›ï¼Œæå‡ºSearch - o1æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–ä¸ªé›†æˆæ™ºèƒ½æœç´¢å·¥ä½œæµçš„å¤§æ¨ç†æ¨¡å‹å¢å¼ºæ¡†æ¶
Search - o1æ˜¯é¦–ä¸ªæŠŠ**æ™ºèƒ½æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆagentic RAGï¼‰æœºåˆ¶**èå…¥LRMsçš„o1å¼æ¨ç†è¿‡ç¨‹çš„æ¡†æ¶ï¼Œå®ç°çŸ¥è¯†è‡ªä¸»è¡¥å……ã€‚ä¸åŒäºä¼ ç»ŸRAGåªåœ¨é—®é¢˜å±‚é¢å•æ¬¡æ£€ç´¢ï¼ŒSearch - o1è®©æ¨¡å‹åœ¨é‡åˆ°çŸ¥è¯†çŸ­ç¼ºæ—¶ï¼Œä¸»åŠ¨ç”Ÿæˆæœç´¢æŸ¥è¯¢æ¥è§¦å‘æ£€ç´¢ï¼Œä¸”å•è½®æ¨ç†ä¸­èƒ½å¤šæ¬¡è§¦å‘è¿­ä»£æ£€ç´¢ï¼Œæ»¡è¶³å„æ¨ç†æ­¥éª¤çš„çŸ¥è¯†éœ€æ±‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–‡æ¡£å†…æ¨ç†ï¼ˆReason - in - Documentsï¼‰æ¨¡å—è§£å†³çŸ¥è¯†æ•´åˆéš¾é¢˜
è€ƒè™‘åˆ°æ£€ç´¢æ–‡æ¡£å­˜åœ¨å†—ä½™ä¿¡æ¯ã€LRMsé•¿æ–‡æ¡£ç†è§£èƒ½åŠ›æœ‰é™è¿™ä¸¤ä¸ªæ•´åˆçŸ¥è¯†çš„éš¾ç‚¹ï¼Œè®¾è®¡ç‹¬ç«‹äºä¸»æ¨ç†é“¾çš„Reason - in - Documentsæ¨¡å—ã€‚å®ƒä¼šç»“åˆå½“å‰æœç´¢æŸ¥è¯¢å’Œä¹‹å‰æ¨ç†æ­¥éª¤ï¼Œæ·±åº¦åˆ†ææ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæç‚¼å‡ºèƒ½æ— ç¼èå…¥æ¨ç†é“¾çš„ä¿¡æ¯ï¼Œå‡å°‘å™ªå£°è¿˜èƒ½ä¿æŒæ¨ç†è¿è´¯æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ç§‘å­¦ã€æ•°å­¦ã€ç¼–ç ç­‰5ä¸ªå¤æ‚æ¨ç†é¢†åŸŸï¼Œä»¥åŠ6ä¸ªå¼€æ”¾åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ä¸­éªŒè¯Search - o1ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSearch - o1åœ¨æ¨ç†é¢†åŸŸè¡¨ç°å“è¶Šï¼ŒåŒæ—¶åœ¨é€šç”¨çŸ¥è¯†æ–¹é¢ä¹Ÿæœ‰å¤§å¹…æå‡ï¼›è¿›ä¸€æ­¥å®šé‡åˆ†æä¹Ÿè¯å®äº†å®ƒçš„æ•ˆç‡ä¸å¯æ‰©å±•æ€§ï¼Œä¸ºLRMså®ç°å¯ä¿¡æ¨ç†æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§£å†³çŸ¥è¯†ä¸è¶³æ–°æ€è·¯ï¼šå½“æ¨¡å‹ç±»æ–¹æ³•åœ¨çŸ¥è¯†è¦†ç›–ä¸Šé¢ä¸´ç“¶é¢ˆæ—¶ï¼Œå¯å‚è€ƒSearch - o1å¼•å…¥æ™ºèƒ½æ£€ç´¢æœºåˆ¶ï¼Œè®©æ¨¡å‹åŠ¨æ€è·å–å¤–éƒ¨çŸ¥è¯†ï¼Œä¸ºé•¿æ¨ç†é“¾è¡¥å…¨ä¿¡æ¯ã€‚
2. æ¨¡å—è§£è€¦ä¸åä½œï¼šæŠŠçŸ¥è¯†æ£€ç´¢ã€çŸ¥è¯†ç²¾ç‚¼å’Œä¸»æ¨ç†è¿‡ç¨‹åšåˆç†è§£è€¦ï¼ˆå¦‚å•ç‹¬çš„Reason - in - Documentsæ¨¡å—ï¼‰ï¼Œå†è®©å„éƒ¨åˆ†åä½œï¼Œèƒ½æœ‰æ•ˆå¤„ç†å¤–éƒ¨çŸ¥è¯†æ•´åˆæ—¶çš„å†—ä½™ã€ç†è§£å—é™ç­‰é—®é¢˜ï¼Œè¿™ç§æ¨¡å—åŒ–è®¾è®¡æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€å¤–éƒ¨çŸ¥è¯†æ³¨å…¥çš„AIç³»ç»Ÿã€‚
3. å¤šé¢†åŸŸéªŒè¯èŒƒå¼ï¼šè®ºæ–‡åœ¨å¤æ‚æ¨ç†å¤šé¢†åŸŸå’Œå¼€æ”¾åŸŸQAåŸºå‡†æµ‹è¯•çš„å®éªŒéªŒè¯æ–¹å¼ï¼Œä¸ºè¯„ä¼°æ¨¡å‹åœ¨â€œæ¨ç† + çŸ¥è¯†è¡¥å……â€ç±»æ”¹è¿›çš„æœ‰æ•ˆæ€§æä¾›äº†å…¨é¢éªŒè¯çš„èŒƒä¾‹ï¼Œåç»­ç±»ä¼¼å¢å¼ºå‹æ¨¡å‹ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å¤šåœºæ™¯æµ‹è¯•æ€è·¯ã€‚
```

## agent-models--internalizing-chain-of-action-generation-into-reasoning-models
### Abstract
Traditional agentic workflows rely on external prompts to manage interactions
with tools and the environment, which limits the autonomy of reasoning models.
We position \emph{Large Agent Models (LAMs)} that internalize the generation of
\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when
and how to use external tools. Our proposed AutoCoA framework combines
supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the
model to seamlessly switch between reasoning and action while efficiently
managing environment interactions. Main components include step-level action
triggering, trajectory-level CoA optimization, and an internal world model to
reduce real-environment interaction costs. Evaluations on open-domain QA tasks
demonstrate that AutoCoA-trained agent models significantly outperform
ReAct-based workflows in task completion, especially in tasks that require
long-term reasoning and multi-step actions. Code and dataset are available at
https://github.com/ADaM-BJTU/AutoCoA
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»æ¨ç†åˆ°è¡ŒåŠ¨ï¼šLarge Agent Models å¦‚ä½•é‡å¡‘æ™ºèƒ½ä½“èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½è¿ˆå‘é€šç”¨æ™ºèƒ½çš„è¿›ç¨‹ä¸­ï¼ŒOpenAI å‹¾å‹’å‡ºä» Chatbotï¼ˆå¦‚ GPT - 3.5ï¼‰ã€Reasonerï¼ˆå¦‚ o1ï¼‰åˆ° Agentï¼ˆå¦‚ Operatorã€Deep Researchï¼‰çš„æ¼”è¿›è·¯å¾„ã€‚ä¼ ç»Ÿæ™ºèƒ½ä½“å·¥ä½œæµï¼ˆå¦‚ ReActï¼‰ä¾èµ–å¤–éƒ¨æç¤ºæ¥ç®¡ç†å·¥å…·ä¸ç¯å¢ƒäº¤äº’ï¼Œé™åˆ¶äº†æ¨ç†æ¨¡å‹çš„è‡ªä¸»æ€§â€”â€”æ€ç»´ä¸è¡ŒåŠ¨çš„åˆ‡æ¢å¸¸ç”±é¢„è®¾æµç¨‹è§¦å‘ï¼Œå±äºâ€œè¢«åŠ¨â€è¡Œä¸ºã€‚è€Œæ–°ä¸€ä»£æ™ºèƒ½ä½“éœ€å…·å¤‡ä¸»åŠ¨å†³ç­–ä½•æ—¶ã€å¦‚ä½•ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ï¼Œå®ç°æ€ç»´ï¼ˆChain - of - Thought, CoTï¼‰ä¸è¡ŒåŠ¨ï¼ˆChain - of - Action, CoAï¼‰çš„æ·±åº¦èåˆã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè®ºæ–‡æå‡º**Large Agent Models (LAMs)** è¿™ä¸€æ¦‚å¿µï¼Œæ—¨åœ¨è®©æ¨ç†æ¨¡å‹å†…åŒ–ä¸ºèƒ½è‡ªä¸»ç”Ÿæˆ Chain - of - Action çš„æ™ºèƒ½ä½“æ¨¡å‹ï¼Œçªç ´ä¼ ç»Ÿå·¥ä½œæµçš„å±€é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰ Large Agent Modelsï¼ˆLAMsï¼‰èŒƒå¼  
LAMs æ˜¯åœ¨æ¨ç†æ¨¡å‹åŸºç¡€ä¸Šï¼Œé€šè¿‡é¢å‘ä»»åŠ¡çš„å·¥å…·å¢å¼ºè®­ç»ƒå¾—åˆ°çš„ç”Ÿæˆå¼æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆäº¤é”™çš„â€œæ€ç»´é“¾ï¼ˆCoTï¼‰â€ä¸â€œè¡ŒåŠ¨é“¾ï¼ˆCoAï¼‰â€åºåˆ—ï¼šCoT æ”¯æ’‘å†…éƒ¨æ¨ç†è§„åˆ’ï¼ŒCoA åˆ™é€šè¿‡è°ƒç”¨å·¥å…·ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ã€‚ä¸ä¾èµ–å¤–éƒ¨æç¤ºçš„ä¼ ç»Ÿæ™ºèƒ½ä½“å·¥ä½œæµä¸åŒï¼ŒLAMs æŠŠå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ˆå³ CoA ç”Ÿæˆï¼‰å†…åŒ–ä¸ºæ¨¡å‹å›ºæœ‰è¡Œä¸ºï¼Œå®ç°â€œä¸»åŠ¨â€å†³å®šä½•æ—¶ã€å¦‚ä½•è¡ŒåŠ¨ï¼Œæ„å»ºâ€œäºº - æ¨¡å‹ - ç¯å¢ƒâ€ä¸‰å…ƒäº¤äº’ç»“æ„ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡º AutoCoA è®­ç»ƒæ¡†æ¶  
AutoCoA ç»“åˆ**æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ä¸**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼Œè®©æ¨¡å‹åœ¨æ¨ç†ä¸è¡ŒåŠ¨é—´æ— ç¼åˆ‡æ¢ï¼ŒåŒæ—¶é«˜æ•ˆç®¡ç†ç¯å¢ƒäº¤äº’ã€‚å…¶æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š  
- æ­¥éª¤çº§è¡ŒåŠ¨è§¦å‘ï¼šæ¨¡å‹èƒ½è‡ªä¸»å†³å®šä½•æ—¶ä»â€œæ€è€ƒï¼ˆâŸ¨thinkâŸ©ï¼‰â€åˆ‡æ¢åˆ°â€œè¡ŒåŠ¨ï¼ˆâŸ¨actionâŸ©ï¼‰â€ï¼›  
- è½¨è¿¹çº§ CoA ä¼˜åŒ–ï¼šä»æ•´ä¸ªæ€ç»´ - è¡ŒåŠ¨è½¨è¿¹å±‚é¢ä¼˜åŒ–è¡ŒåŠ¨é“¾ï¼Œç¡®ä¿å¤šæ­¥è¡ŒåŠ¨çš„è¿è´¯æ€§ä¸ä»»åŠ¡å¯¼å‘æ€§ï¼›  
- å†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼šå‡å°‘ä¸çœŸå®ç¯å¢ƒç›´æ¥äº¤äº’çš„æˆæœ¬ï¼Œé€šè¿‡å†…éƒ¨æ¨¡æ‹Ÿæˆ–é¢„å­¦ä¹ æ¥é¢„ä¼°è¡ŒåŠ¨åé¦ˆï¼Œæå‡æ•ˆç‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå½¢å¼åŒ– Agent Model æ¨ç†è¿‡ç¨‹  
å°† Agent Model çš„æ¨ç†å»ºæ¨¡ä¸º**éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰**ï¼ŒçŠ¶æ€ç”±åˆå§‹ç¯å¢ƒçŠ¶æ€ \( s_0 \)ã€ä»»åŠ¡ä¸Šä¸‹æ–‡ \( t_c \) å’Œå·²ç”Ÿæˆåºåˆ— \( x_{1:n} \) ç»„æˆã€‚æ¨¡å‹çš„ç­–ç•¥ \( \pi_\theta \) å†³å®šæ¯ä¸€æ­¥ç”Ÿæˆçš„ token ç±»å‹ï¼ˆæ˜¯è§¦å‘æ€è€ƒçš„ âŸ¨thinkâŸ© è¿˜æ˜¯è§¦å‘è¡ŒåŠ¨çš„ âŸ¨actionâŸ©ï¼‰ï¼Œè¡ŒåŠ¨æ—¶è¿˜éœ€æŒ‡å®šå·¥å…·ç±»å‹ï¼ˆå¦‚æœç´¢ï¼‰ä¸å‚æ•°ï¼ˆå¦‚æœç´¢æŸ¥è¯¢ï¼‰ï¼Œç¯å¢ƒåé¦ˆå†å¼•å¯¼åç»­æ¨ç†ä¸è¡ŒåŠ¨ï¼Œç›´è‡³ä»»åŠ¡å®Œæˆã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨**å¼€æ”¾åŸŸé—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡**ä¸Šè¯„ä¼° AutoCoA è®­ç»ƒçš„æ™ºèƒ½ä½“æ¨¡å‹ï¼š  
- å¯¹æ¯”åŸºäº ReAct çš„ä¼ ç»Ÿå·¥ä½œæµï¼ŒLAMs åœ¨ä»»åŠ¡å®Œæˆåº¦ä¸Šæ˜¾è‘—æ›´ä¼˜ï¼›  
- å°¤å…¶åœ¨éœ€è¦é•¿æœŸæ¨ç†ã€å¤šæ­¥è¡ŒåŠ¨çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œå†…åŒ– CoA ç”Ÿæˆçš„æ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„è¿è´¯æ€§ä¸æ‰§è¡Œèƒ½åŠ›ï¼ŒéªŒè¯äº†è®©æ¨¡å‹ä¸»åŠ¨ç®¡ç†â€œæ€ç»´ - è¡ŒåŠ¨â€å¾ªç¯çš„ä»·å€¼ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼å‡çº§è§†è§’ï¼šä»â€œå¤–éƒ¨æç¤ºé©±åŠ¨â€åˆ°â€œæ¨¡å‹å†…éƒ¨èƒ½åŠ›å†…åŒ–â€æ˜¯æ™ºèƒ½ä½“å‘å±•çš„å…³é”®è¶‹åŠ¿ï¼Œç±»ä¼¼æ¨ç†æ¨¡å‹ä» CoT Prompting åˆ°å†…ç½® CoT ç”Ÿæˆçš„æ¼”è¿›ï¼Œä¸ºæ„å»ºæ›´è‡ªä¸»çš„æ™ºèƒ½ä½“æä¾›äº†æ–¹å‘ï¼›  
2. è®­ç»ƒæ¡†æ¶å¤ç”¨ï¼šAutoCoA ç»“åˆ SFT + RL çš„æ€è·¯å¯è¿ç§»åˆ°å¤šå·¥å…·ã€å¤šä»»åŠ¡åœºæ™¯ï¼Œä¸ºåç»­å¼€å‘æ›´å¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›æŠ€æœ¯å‚è€ƒï¼›  
3. ç¯å¢ƒäº¤äº’ä¼˜åŒ–ï¼šå†…éƒ¨ä¸–ç•Œæ¨¡å‹çš„è®¾è®¡æ€è·¯ï¼Œå¯¹é™ä½çœŸå®ç¯å¢ƒäº¤äº’æˆæœ¬ï¼ˆå¦‚å‡å°‘ API è°ƒç”¨ã€ç‰©ç†ä¸–ç•Œè¯•é”™ï¼‰å…·æœ‰å¯å‘ï¼Œå¯ç”¨äºèµ„æºå—é™æˆ–é«˜æˆæœ¬äº¤äº’åœºæ™¯çš„æ™ºèƒ½ä½“å¼€å‘ã€‚  

æ€»ä¹‹ï¼Œè¿™ç¯‡è®ºæ–‡ä¸ºæ™ºèƒ½ä½“ä»â€œè¢«åŠ¨å“åº”æµç¨‹â€è¿ˆå‘â€œä¸»åŠ¨è§„åˆ’è¡ŒåŠ¨â€æä¾›äº†ç†è®ºä¸æŠ€æœ¯è“å›¾ï¼Œæ˜¯ç†è§£ AGI è·¯å¾„ä¸Šâ€œAgent é˜¶æ®µâ€æŠ€æœ¯çªç ´çš„é‡è¦å‚è€ƒï½ 
```

## scent-of-knowledge--optimizing-search-enhanced-reasoning-with-information-foraging
### Abstract
Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨â€œä¿¡æ¯è§…é£Ÿâ€ä¼˜åŒ–æœç´¢å¢å¼ºæ¨ç†ï¼šInForageæ¡†æ¶å¦‚ä½•è®©LLMæ›´æ™ºèƒ½ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œçŸ¥è¯†æˆªæ–­â€é—®é¢˜ä¸€ç›´æ˜¯è¡Œä¸šç—›ç‚¹â€”â€”æ¨¡å‹æ— æ³•è·å–è®­ç»ƒåæ›´æ–°çš„çŸ¥è¯†ï¼Œä¹Ÿéš¾ä»¥åº”å¯¹æ¨¡ç³Šã€å¤šæ­¥éª¤æˆ–åŠ¨æ€å˜åŒ–çš„å¤æ‚ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„â€œæ£€ç´¢å¢å¼ºç”Ÿæˆâ€æ–¹æ³•é‡‡ç”¨**é™æ€çš„é¢„æ¨ç†æ£€ç´¢ç­–ç•¥**ï¼ŒæŠŠæ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸€è‚¡è„‘å¡è¿›promptå°±å®Œäº‹ï¼Œé¢å¯¹éœ€è¦è¿­ä»£æ¨ç†ã€é€æ­¥æŒ–æ˜è¯æ®çš„å¤æ‚åœºæ™¯ï¼ˆæ¯”å¦‚â€œæˆ‘è¦å»NeurIPSå‚ä¼šï¼Œéœ€è¦ç­¾è¯å—ï¼Ÿâ€è¿™ç±»è¦å…ˆæŸ¥ä¼šè®®åœ°ç‚¹ã€å†æŸ¥ç­¾è¯æ”¿ç­–çš„é—®é¢˜ï¼‰ï¼Œå°±æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚  

å¥½åœ¨â€œæµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯â€çš„å‘å±•è®©LLMèƒ½åŠ¨æ€è°ƒç”¨å¤–éƒ¨å·¥å…·äº†ï¼Œè¿™æ¨åŠ¨å¤§å®¶æŠŠæ€è·¯ä»â€œé™æ€é¢„æ¨ç†æ£€ç´¢â€è½¬å‘â€œè‡ªé€‚åº”çš„æ¨ç†æ—¶æ£€ç´¢â€ã€‚ä½†åŠ¨æ€æ£€ç´¢ä¹Ÿæœ‰ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯å¤æ‚ä»»åŠ¡çš„ä¿¡æ¯éœ€æ±‚æ˜¯éšå«ä¸”åŠ¨æ€å˜åŒ–çš„ï¼Œå•æ¬¡æ£€ç´¢åªèƒ½æ‹¿åˆ°å±€éƒ¨ä¿¡æ¯â€œè¡¥ä¸â€ï¼Œå¾—é è¿­ä»£ç§¯ç´¯ï¼›äºŒæ˜¯ä¿¡æ¯è¡¥ä¸çš„ä»·å€¼ä¸å–å†³äºè‡ªèº«ï¼Œè€Œè¦çœ‹å®ƒå¯¹æœ€ç»ˆæ¨ç†çš„è´¡çŒ®ã€‚  

äººç±»é¢å¯¹è¿™ç±»ä¿¡æ¯æœç´¢ä»»åŠ¡æ—¶ï¼Œå´èƒ½é«˜æ•ˆç”¨å‡ æ¬¡è¿­ä»£æœç´¢è§£å†³é—®é¢˜â€”â€”è¿™èƒŒåæ˜¯â€œä¿¡æ¯è§…é£Ÿç†è®ºï¼ˆIFTï¼‰â€ï¼šäººç±»ä¼šæƒè¡¡ä¿¡æ¯è¡¥ä¸çš„ä»·å€¼å’Œè·å–æˆæœ¬ï¼Œç”¨â€œä¿¡æ¯æ°”å‘³ï¼ˆscentï¼‰â€æŒ‡å¼•æœç´¢æ–¹å‘ã€‚å—æ­¤å¯å‘ï¼Œè®ºæ–‡æå‡ºInForageæ¡†æ¶ï¼Œæƒ³è®©LLMä¹Ÿå­¦ä¼šè¿™ç§â€œåŠ¨æ€ä¿¡æ¯è§…é£Ÿâ€çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºä¿¡æ¯è§…é£Ÿç†è®ºçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶  
InForageæŠŠâ€œæ£€ç´¢å¢å¼ºæ¨ç†â€å½¢å¼åŒ–ä¸º**åŠ¨æ€ä¿¡æ¯æœç´¢è¿‡ç¨‹**ï¼Œç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–ã€‚å’Œä¼ ç»Ÿåªçœ‹æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®çš„æ–¹æ³•ä¸åŒï¼Œå®ƒå…³æ³¨**ä¸­é—´æ£€ç´¢æ­¥éª¤çš„è´¨é‡**ï¼šå‚è€ƒIFTé‡Œâ€œä¿¡æ¯æ°”å‘³â€çš„æ¦‚å¿µï¼Œè®©æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è‡ªé€‚åº”æœç´¢è¡Œä¸ºè¿­ä»£æ”¶é›†ã€æ•´åˆä¿¡æ¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰é‡å¥–åŠ±æœºåˆ¶å¼•å¯¼æ¨ç†è¡Œä¸º  
ä¸ºäº†æ¿€åŠ±æ¨¡å‹å­¦ä¹ â€œé«˜æ•ˆä¿¡æ¯è§…é£Ÿâ€ï¼ŒInForageè®¾è®¡äº†ä¸‰ç±»å¥–åŠ±ï¼š  
- ç»“æœå¥–åŠ±ï¼ˆOutcome Rewardï¼‰ï¼šå¥–åŠ±æœ€ç»ˆç­”æ¡ˆæ­£ç¡®çš„æ¨ç†è½¨è¿¹ï¼›  
- ä¿¡æ¯å¢ç›Šå¥–åŠ±ï¼ˆInformation Gain Rewardï¼‰ï¼šå¥–åŠ±é‚£äº›èƒ½æŒ–æ˜å‡ºæœ‰ä»·å€¼è¯æ®çš„ä¸­é—´æ£€ç´¢æ­¥éª¤ï¼›  
- æ•ˆç‡æƒ©ç½šï¼ˆEfficiency Penaltyï¼‰ï¼šé¿å…æ— æ„ä¹‰çš„å†—é•¿æ¨ç†ï¼Œé¼“åŠ±â€œé«˜æ•ˆä¿¡æ¯è§…é£Ÿâ€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºäººç±»å¼•å¯¼çš„ç»†ç²’åº¦æ•°æ®é›†  
ç°æœ‰QAæ•°æ®é›†å¤§å¤šåªæœ‰â€œé—®é¢˜-ç­”æ¡ˆâ€å¯¹ï¼Œç¼ºå°‘ä¸­é—´æ¨ç†/æ£€ç´¢æ­¥éª¤çš„è®°å½•ï¼Œä¹Ÿå¾ˆéš¾æ”¯æ’‘å¤æ‚å¤šæ­¥æ¨ç†çš„è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ª**äººç±»å¼•å¯¼çš„æ•°æ®é›†**ï¼šæ ‡æ³¨è€…ä»â€œç§å­é—®é¢˜â€å‡ºå‘ï¼Œæ¨¡æ‹ŸçœŸå®ç½‘é¡µæµè§ˆæ—¶çš„è¿­ä»£æœç´¢ã€é€‰æ–‡æ¡£ã€æç‚¼å­é—®é¢˜çš„è¿‡ç¨‹ï¼Œè¿˜è¦æ±‚æ¯ä¸ªæ¡ˆä¾‹è‡³å°‘åŒ…å«4æ¬¡ä¿¡æ¯è·³è·ƒæˆ–äº¤å‰æ¡ä»¶ï¼Œä¿è¯ä»»åŠ¡å¤æ‚åº¦ã€‚æ•°æ®é›†è®°å½•äº†æœç´¢å’Œæ¨ç†çš„æ¯ä¸€æ­¥ï¼Œèƒ½ä¸ºâ€œæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§ã€ä¸­é—´æ£€ç´¢è´¨é‡ã€æ•´ä½“æ¨ç†æ•ˆç‡â€æä¾›ç›‘ç£ä¿¡å·ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ä¸‰ç±»ä»»åŠ¡ä¸Šåšäº†è¯„ä¼°ï¼šé€šç”¨é—®ç­”ã€å¤šè·³æ¨ç†ä»»åŠ¡ï¼Œä»¥åŠå›¢é˜Ÿæ–°å¼€å‘çš„**å®æ—¶ç½‘ç»œQAæ•°æ®é›†**ã€‚ç»“æœæ˜¾ç¤ºï¼ŒInForageåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½**æŒç»­è¶…è¶ŠåŸºçº¿æ–¹æ³•**ï¼Œè¯æ˜äº†ä»â€œä¸°å¯Œç›‘ç£çš„æœç´¢å¢å¼ºæ¨ç†æ•°æ®â€ä¸­å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œä¹ŸéªŒè¯äº†InForageæ„å»ºâ€œé²æ£’ã€è‡ªé€‚åº”ã€é«˜æ•ˆæ¨ç†æ™ºèƒ½ä½“â€çš„æ½œåŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºè·¨ç•Œèåˆ**ï¼šæŠŠè®¤çŸ¥ç§‘å­¦é‡Œçš„â€œä¿¡æ¯è§…é£Ÿç†è®ºâ€å¼•å…¥LLMçš„æ£€ç´¢å¢å¼ºæ¨ç†ï¼Œä¸ºåŠ¨æ€æ£€ç´¢æä¾›äº†æ–°é¢–çš„ç†è®ºè§†è§’ï¼›  
2. **å¼ºåŒ–å­¦ä¹ ç»†ç²’åº¦ç›‘ç£**ï¼šè·³å‡ºâ€œåªçœ‹æœ€ç»ˆç»“æœâ€çš„æ€ç»´ï¼Œè®¾è®¡å¯¹ä¸­é—´æ­¥éª¤çš„å¥–åŠ±æœºåˆ¶ï¼Œæ›´è´´åˆå¤æ‚ä»»åŠ¡é‡Œâ€œè¿­ä»£ç§¯ç´¯è¯æ®â€çš„é€»è¾‘ï¼›  
3. **æ•°æ®é›†æ„å»ºæ€è·¯**ï¼šé’ˆå¯¹â€œå¤æ‚å¤šæ­¥æ¨ç†ç¼ºæ•°æ®â€çš„ç—›ç‚¹ï¼Œäººå·¥æ¨¡æ‹ŸçœŸå®ä¿¡æ¯æœç´¢è½¨è¿¹ï¼Œè®°å½•ç»†ç²’åº¦æ­¥éª¤â€”â€”è¿™ç§æ„å»ºâ€œå¸¦ä¸­é—´è¿‡ç¨‹çš„å¤æ‚ä»»åŠ¡æ•°æ®é›†â€çš„æ€è·¯ï¼Œå¯¹åç»­ç ”ç©¶å¾ˆæœ‰å‚è€ƒä»·å€¼ï¼›  
4. **åŠ¨æ€æ£€ç´¢çš„è½åœ°æ½œåŠ›**ï¼šInForageå±•ç¤ºäº†LLMåœ¨â€œæ¨¡ç³Šã€å¤šæ­¥éª¤ã€ä¿¡æ¯éœ€æ±‚åŠ¨æ€å˜åŒ–â€åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›æå‡ï¼Œä¸ºæ£€ç´¢å¢å¼ºä»â€œé™æ€â€è½¬å‘â€œåŠ¨æ€è‡ªé€‚åº”â€æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚  
```

## composerag--a-modular-and-composable-rag-for-corpus-grounded-multi-hop-question-answering
### Abstract
Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet
many suffer from monolithic designs that tightly couple core functions like
query reformulation, retrieval, reasoning, and verification. This limits their
interpretability, systematic evaluation, and targeted improvement, especially
for complex multi-hop question answering. We introduce ComposeRAG, a novel
modular abstraction that decomposes RAG pipelines into atomic, composable
modules. Each module, such as Question Decomposition, Query Rewriting,
Retrieval Decision, and Answer Verification, acts as a parameterized
transformation on structured inputs/outputs, allowing independent
implementation, upgrade, and analysis. To enhance robustness against errors in
multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that
iteratively revisits and refines earlier steps upon verification failure.
Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently
outperforms strong baselines in both accuracy and grounding fidelity.
Specifically, it achieves up to a 15% accuracy improvement over
fine-tuning-based methods and up to a 5% gain over reasoning-specialized
pipelines under identical retrieval conditions. Crucially, ComposeRAG
significantly enhances grounding: its verification-first design reduces
ungrounded answers by over 10% in low-quality retrieval settings, and by
approximately 3% even with strong corpora. Comprehensive ablation studies
validate the modular architecture, demonstrating distinct and additive
contributions from each component. These findings underscore ComposeRAG's
capacity to deliver flexible, transparent, scalable, and high-performing
multi-hop reasoning with improved grounding and interpretability.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ComposeRAGï¼šæ¨¡å—åŒ–å¯ç»„åˆçš„å¤šè·³é—®ç­”RAGæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç±»NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¾èµ–é™æ€é¢„è®­ç»ƒçŸ¥è¯†æ˜“äº§ç”Ÿé”™è¯¯ï¼ˆå¹»è§‰ï¼‰ä¸”éš¾è·å–æœ€æ–°æˆ–ç‰¹å®šé¢†åŸŸä¿¡æ¯ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è™½èƒ½æ•´åˆå¤–éƒ¨çŸ¥è¯†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œç„¶è€Œä¼ ç»ŸRAGåœ¨å¤æ‚å¤šè·³é—®ç­”æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¤šè·³é—®ç­”éœ€å¤šæ–‡æ¡£é—´åˆ†è§£ä¸é€æ­¥æ¨ç†ï¼Œç°æœ‰æ–¹æ³•å¸¸å› æ•´ä½“å¼æˆ–ä¸é€æ˜æ¶æ„ï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§ã€é€‚åº”æ€§ä¸ç³»ç»Ÿæ”¹è¿›ç©ºé—´ï¼Œå¦‚æ—©æœŸæ–¹æ¡ˆæ˜“å‡ºé”™ä¼ æ’­ï¼Œåç»­ç³»ç»Ÿåˆæœ‰ä¾èµ–å¾®è°ƒæˆ–éªŒè¯æœºåˆ¶ä¸è¶³ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´çµæ´»ã€é€æ˜ä¸”èƒ½åº”å¯¹å¤šè·³é—®ç­”çš„RAGæ¶æ„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¨¡å—åŒ–æ¶æ„è®¾è®¡
å°†RAG pipelineåˆ†è§£ä¸ºå¦‚é—®é¢˜åˆ†è§£ã€æŸ¥è¯¢é‡å†™ã€æ£€ç´¢å†³ç­–ã€ç­”æ¡ˆéªŒè¯ç­‰åŸå­åŒ–ã€å¯ç»„åˆæ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—å¯¹ç»“æ„åŒ–è¾“å…¥/è¾“å‡ºåšå‚æ•°åŒ–è½¬æ¢ï¼Œæ”¯æŒç‹¬ç«‹å®ç°ã€å‡çº§ä¸åˆ†æï¼Œä¸ºå¤šè·³é—®ç­”æä¾›çµæ´»ä¸”å¯è§£é‡Šçš„æ¨ç†ç»„ä»¶ï¼Œå®ç°â€œå³æ’å³ç”¨â€ä¸é€æ˜åŒ–åˆ†ææ”¹è¿›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¸¦è‡ªåæ€çš„ç¼–æ’ç­–ç•¥
å¼•å…¥è‡ªåæ€æœºåˆ¶å¢å¼ºå¤šæ­¥æ¨ç†é²æ£’æ€§ï¼Œå½“éªŒè¯å¤±è´¥æ—¶è¿­ä»£å›é¡¾å’Œä¼˜åŒ–æ—©æœŸæ­¥éª¤ã€‚é€šè¿‡è‡ªåæ€çš„é—®é¢˜åˆ†è§£ç­‰æ¨ç†æ­¥éª¤åè°ƒï¼Œä»æ—©æœŸé”™è¯¯ä¸­æ¢å¤ï¼Œæå‡æ•´ä½“é²æ£’æ€§ï¼Œä¸”åæ€æ­¥éª¤å¢åŠ èƒ½å¸¦æ¥æ€§èƒ½æå‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡å—ä»·å€¼éªŒè¯ä¸å¯å‡çº§æ€§
ä¸€æ–¹é¢é‡åŒ–æ ¸å¿ƒæ¨¡å—ï¼ˆå¦‚é—®é¢˜åˆ†è§£ã€æ®µè½é‡æ’ã€ç­”æ¡ˆéªŒè¯ï¼‰å„è‡ªç‹¬ç‰¹ä¸”å¯å åŠ çš„æ€§èƒ½è´¡çŒ®ï¼›å¦ä¸€æ–¹é¢éªŒè¯ç‹¬ç«‹æ¨¡å—å¯å‡çº§æ€§ï¼Œå•ç‹¬å¢å¼ºå•ä¸ªç»„ä»¶ï¼ˆå¦‚ç‰¹å®šä»»åŠ¡ç”¨æ›´å¼ºå¤§LLMï¼‰èƒ½å¸¦æ¥å¯è¡¡é‡æ”¹è¿›ï¼Œä½“ç°ç³»ç»Ÿæ‰©å±•æ€§ä¸é€‚åº”æ€§ï¼Œè¿˜æœ‰åƒæ£€ç´¢å†³ç­–è¿™ç±»æ³¨é‡æ•ˆç‡ç»„ä»¶å¯¹æ€§èƒ½å’Œèµ„æºåˆ©ç”¨æœ‰ç§¯æå½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨HotpotQAã€2WikiMultiHopQAã€MuSiQueã€Bamboogleå››ä¸ªå¤šè·³QAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒComposeRAGåœ¨å‡†ç¡®ç‡å’Œ grounding ä¿çœŸåº¦ä¸ŠæŒç»­è¶…è¶Šå¼ºåŸºçº¿ã€‚ç›¸æ¯”åŸºäºå¾®è°ƒæ–¹æ³•å‡†ç¡®ç‡æœ€å¤šæå‡15%ï¼Œç›¸åŒæ£€ç´¢æ¡ä»¶ä¸‹æ¯”æ¨ç†ä¸“ç”¨ pipeline æœ€å¤šé«˜5%ï¼›éªŒè¯ä¼˜å…ˆè®¾è®¡åœ¨ä½è´¨é‡æ£€ç´¢åœºæ™¯å‡å°‘è¶…10%æ— æ ¹æ®ç­”æ¡ˆï¼Œé«˜è´¨é‡è¯­æ–™ä¸‹ä¹Ÿçº¦é™3%ï¼›æ¶ˆèå®éªŒè¯å®æ¨¡å—åŒ–æ¶æ„ä»·å€¼ï¼Œå„ç»„ä»¶è´¡çŒ®ç‹¬ç‰¹ä¸”å¯å åŠ ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æ¶æ„è®¾è®¡ä¸Šï¼Œæ¨¡å—åŒ–æ€è·¯ä¸ºå¤æ‚ç³»ç»Ÿæ‹†è§£æä¾›èŒƒä¾‹ï¼Œä¾¿äºåˆ†æã€å‡çº§ä¸é€‚é…æ–°ä»»åŠ¡é¢†åŸŸï¼Œå¯åº”ç”¨äºéœ€å¤šç»„ä»¶åä½œçš„AIç³»ç»Ÿï¼›è‡ªåæ€æœºåˆ¶ä¸ºå¤„ç†å¤šæ­¥æ¨ç†ä¸­é”™è¯¯ä¼ æ’­é—®é¢˜æä¾›æ–°æ€è·¯ï¼Œåœ¨éœ€è¿­ä»£ä¼˜åŒ–çš„ä»»åŠ¡æµç¨‹é‡Œå€¼å¾—å‚è€ƒï¼›å¯¹æ¨¡å—ä»·å€¼é‡åŒ–ä¸å¯å‡çº§æ€§éªŒè¯ï¼Œå¼•å¯¼åç»­å·¥ä½œå…³æ³¨ç»„ä»¶çº§ä¼˜åŒ–ä¸ç³»ç»Ÿæ‰©å±•æ€§ï¼Œä¸ºæ„å»ºæ›´çµæ´»é«˜æ•ˆAIç³»ç»Ÿæä¾›äº†ä»è®¾è®¡åˆ°éªŒè¯çš„å®Œæ•´æ€è·¯å‚è€ƒã€‚
```

## auto-rag--autonomous-retrieval-augmented-generation-for-large-language-models
### Abstract
Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | Auto-RAGï¼šè®©å¤§æ¨¡å‹è‡ªä¸»è¿­ä»£æ£€ç´¢ï¼Œé‡Šæ”¾æ¨ç†å†³ç­–æ½œèƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é¢†åŸŸï¼Œå°½ç®¡RAGèƒ½æå‡è¾“å‡ºè´¨é‡ã€å‡å°‘å¹»è§‰ï¼Œä½†ä»å­˜åœ¨æ£€ç´¢å†…å®¹å™ªå£°å¤§ã€å•æ¬¡æ£€ç´¢éš¾æ»¡è¶³å¤æ‚æŸ¥è¯¢éœ€æ±‚ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›ï¼Œè¿­ä»£æ£€ç´¢åº”è¿è€Œç”Ÿï¼Œå®ƒèƒ½åŠ¨æ€æ›´æ–°æ£€ç´¢ç»“æœä»¥åŒ¹é…ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¿¡æ¯éœ€æ±‚ã€‚ç„¶è€Œç°æœ‰è¿­ä»£æ£€ç´¢æ–¹æ³•å¤šä¾èµ–å°‘æ ·æœ¬æç¤ºæˆ–äººå·¥è§„åˆ™ï¼Œæ—¢å¢åŠ æ¨ç†å¼€é”€ï¼Œåˆå¿½è§†äº†LLMå¼ºå¤§çš„æ¨ç†å†³ç­–èƒ½åŠ›ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨LLMæ¥å†³å®šâ€œä½•æ—¶æ£€ç´¢ã€æ£€ç´¢ä»€ä¹ˆâ€ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºAuto - RAGï¼Œæ—¨åœ¨ä¾æ‰˜LLMçš„å†³ç­–èƒ½åŠ›å®ç°è‡ªä¸»è¿­ä»£æ£€ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»¥LLMå†³ç­–èƒ½åŠ›ä¸ºæ ¸å¿ƒçš„è‡ªä¸»è¿­ä»£æ£€ç´¢æ¶æ„
Auto - RAGå°†LLMä¸æ£€ç´¢å™¨çš„äº¤äº’å»ºæ¨¡ä¸ºå¤šè½®å¯¹è¯å½¢å¼çš„è¿­ä»£æ£€ç´¢è¿‡ç¨‹ã€‚åœ¨è¿­ä»£ä¸­ï¼ŒLLMåŸºäºå½“å‰çŠ¶æ€è¿›è¡Œæ¨ç†ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æ£€ç´¢ä»¥åŠè¦è·å–ä½•ç§ä¿¡æ¯ï¼ŒæŒç»­ä¸æ£€ç´¢å™¨å¯¹è¯ã€è§„åˆ’æ£€ç´¢å’Œä¼˜åŒ–æŸ¥è¯¢ï¼Œç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå……åˆ†å‘æŒ¥LLMæ¨ç†å†³ç­–èƒ½åŠ›æ¥é©±åŠ¨è¿­ä»£æ£€ç´¢ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªä¸»åˆæˆæ¨ç†å‹å†³ç­–æŒ‡ä»¤çš„æ–¹æ³•
ä¸ºè®©LLMåœ¨è¿­ä»£æ£€ç´¢ä¸­å…·å¤‡è‡ªä¸»å†³ç­–èƒ½åŠ›ï¼Œæœ¬æ–‡ç ”å‘äº†åœ¨è¿­ä»£æ£€ç´¢ä¸­è‡ªä¸»åˆæˆåŸºäºæ¨ç†çš„å†³ç­–æŒ‡ä»¤çš„æ–¹æ³•ï¼Œå¹¶å¯¹å¦‚Llama - 3 - 8B - Instructç­‰æœ€æ–°å¼€æºLLMè¿›è¡Œå¾®è°ƒï¼Œä½¿LLMèƒ½åœ¨è¿­ä»£æ£€ç´¢é‡Œè‡ªä¸»å†³å®šæ£€ç´¢æ—¶æœºä¸å†…å®¹ï¼Œæ‘†è„±å¯¹äººå·¥è§„åˆ™å’Œå¤§é‡å°‘æ ·æœ¬æç¤ºçš„ä¾èµ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¶µç›–å¼€æ”¾åŸŸé—®ç­”å’Œå¤šè·³é—®ç­”çš„å…­ä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAuto - RAGå³ä¾¿åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä»å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ†æè¡¨æ˜Auto - RAGèƒ½ä¾æ®é—®é¢˜å¤æ‚åº¦å’Œæ£€ç´¢çŸ¥è¯†çš„ç›¸å…³æ€§åŠ¨æ€è°ƒæ•´è¿­ä»£æ¬¡æ•°ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼›è¿˜èƒ½ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾è¿­ä»£æ£€ç´¢è¿‡ç¨‹ï¼Œæå‡å¯è§£é‡Šæ€§ä¸ç”¨æˆ·ç›´è§‚ä½“éªŒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¶æ„è®¾è®¡å±‚é¢ï¼šå°†è¿­ä»£æ£€ç´¢å»ºæ¨¡ä¸ºå¤šè½®å¯¹è¯äº¤äº’ï¼Œä¸ºåˆ©ç”¨LLMæ¨ç†èƒ½åŠ›ä¼˜åŒ–RAGç³»ç»Ÿæä¾›äº†æ–°çš„æ¶æ„æ€è·¯ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§äº¤äº’æ¨¡å¼æ¥å¢å¼ºç³»ç»Ÿå¯¹å¤æ‚ä»»åŠ¡çš„å¤„ç†èƒ½åŠ›ã€‚
2. æ–¹æ³•åˆ›æ–°å±‚é¢ï¼šè‡ªä¸»åˆæˆæ¨ç†å‹å†³ç­–æŒ‡ä»¤å¹¶å¾®è°ƒLLMçš„æ–¹æ³•ï¼Œä¸ºé‡Šæ”¾LLMåœ¨è¿­ä»£ä»»åŠ¡ä¸­çš„è‡ªä¸»å†³ç­–æ½œèƒ½æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œå…¶ä»–éœ€è®©æ¨¡å‹è‡ªä¸»å†³ç­–çš„è¿­ä»£ç±»ä»»åŠ¡å¯å‚è€ƒè¯¥æ–¹æ³•æ¥è®¾è®¡æŒ‡ä»¤ä¸å¾®è°ƒç­–ç•¥ã€‚
3. æ•ˆæœæå‡ä¸ä½“éªŒä¼˜åŒ–å±‚é¢ï¼šAuto - RAGä¾æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´è¿­ä»£æ¬¡æ•°ä»¥åŠç”¨è‡ªç„¶è¯­è¨€è§£é‡Šè¿‡ç¨‹çš„ç‰¹æ€§ï¼Œä¸ºæå‡AIç³»ç»Ÿçš„æ•ˆç‡ä¸ç”¨æˆ·ä½“éªŒæä¾›äº†å€Ÿé‰´ï¼Œåœ¨æ„å»ºæ™ºèƒ½ç³»ç»Ÿæ—¶å¯è€ƒè™‘è®©ç³»ç»Ÿå…·å¤‡è‡ªé€‚åº”è°ƒæ•´å’Œè¿‡ç¨‹å¯è§£é‡Šæ€§çš„èƒ½åŠ›ã€‚
```

## lets--learning-to-think-and-search-via-process-and-outcome-reward-hybridization
### Abstract
Large language models (LLMs) have demonstrated impressive capabilities in
reasoning with the emergence of reasoning models like OpenAI-o1 and
DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into
the realm of retrieval-augmented generation (RAG) via outcome-supervised
reinforcement learning (RL) approaches, while the correctness of intermediate
think-and-search steps is usually neglected. To address this issue, we design a
process-level reward module to mitigate the unawareness of intermediate
reasoning steps in outcome-level supervision without additional annotation.
Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel
framework that hybridizes stepwise process reward and outcome-based reward to
current RL methods for RAG. Extensive experiments demonstrate the
generalization and inference efficiency of LeTS across various RAG benchmarks.
In addition, these results reveal the potential of process- and outcome-level
reward hybridization in boosting LLMs' reasoning ability via RL under other
scenarios. The code will be released soon.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | LeTSï¼šé€šè¿‡è¿‡ç¨‹ä¸ç»“æœå¥–åŠ±æ··åˆï¼Œè®©å¤§æ¨¡å‹å­¦ä¼šâ€œæ€è€ƒ-æœç´¢â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†ä»…ä¾èµ–æ¨¡å‹å†…éƒ¨å‚æ•°çŸ¥è¯†å­˜åœ¨ç”Ÿæˆå¹»è§‰ã€ä¿¡æ¯è¿‡æ—¶ç­‰é—®é¢˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æˆä¸ºç¼“è§£è¿™äº›é—®é¢˜çš„æœ‰æ•ˆèŒƒå¼ï¼Œç„¶è€Œä¼ ç»ŸRAGåœ¨å¤æ‚æŸ¥è¯¢ä¸‹çš„å¤šè·³æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚åç»­åŸºäºç»“æœç›‘ç£çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•è™½å°è¯•ä¼˜åŒ–RAGçš„â€œæ€è€ƒ-æœç´¢â€è¿‡ç¨‹ï¼Œå´å¿½ç•¥äº†ä¸­é—´æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´**å†—ä½™æœç´¢**ï¼ˆé‡å¤è·å–ç›¸ä¼¼/æ— ç”¨ä¿¡æ¯ï¼‰ä¸**æ— å…³æœç´¢**ï¼ˆä¸­é—´æ­¥éª¤å¼•å…¥æ— å…³å†…å®¹è¯¯å¯¼æ¨¡å‹ï¼‰ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æ€§èƒ½æå‡ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸é¢å¤–æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå¯¹ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡Œæœ‰æ•ˆç›‘ç£ï¼Œæˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å—  
ä¸ºå¼¥è¡¥ç»“æœçº§ç›‘ç£å¯¹ä¸­é—´æ­¥éª¤çš„â€œå¿½è§†â€ï¼Œè®ºæ–‡æå‡ºä¸¤ä¸ªåŸºäºè§„åˆ™çš„è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å—ï¼š  
- **çŸ¥è¯†å†—ä½™å¥–åŠ±**ï¼šæƒ©ç½šé‚£äº›é‡å¤æ£€ç´¢å·²è¢«æ›´ä¼˜è½¨è¿¹ï¼ˆrolloutsï¼‰è¦†ç›–ä¿¡æ¯çš„æ­¥éª¤ï¼Œå‡å°‘å†—ä½™æœç´¢ï¼›  
- **çŸ¥è¯†åŒ¹é…å¥–åŠ±**ï¼šé€šè¿‡å¯¹æ¯”é«˜è¡¨ç°è½¨è¿¹ï¼Œè¯†åˆ«å¹¶å¥–åŠ±å¼±è½¨è¿¹ä¸­æ­£ç¡®çš„åŠ¨ä½œï¼Œå¼•å¯¼æ¨¡å‹èšç„¦æœ‰æ•ˆæœç´¢ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºLeTSæ¡†æ¶ï¼Œèåˆè¿‡ç¨‹ä¸ç»“æœå¥–åŠ±  
åŸºäºè¿‡ç¨‹çº§å¥–åŠ±æ¨¡å—ï¼Œè®ºæ–‡æå‡ºâ€œLearning to Think-and-Searchï¼ˆLeTSï¼‰â€æ¡†æ¶ã€‚å®ƒé€šè¿‡**ä¼˜åŠ¿é‡ç¼©æ”¾ï¼ˆadvantage rescalingï¼‰**æœºåˆ¶ï¼Œå°†â€œé€æ­¥è¿‡ç¨‹å¥–åŠ±â€ä¸â€œç»“æœå¯¼å‘å¥–åŠ±â€æ··åˆåˆ°ç°æœ‰RAGå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸­ã€‚è¯¥æ¡†æ¶èƒ½æ›´ç²¾ç»†åœ°å¯¹è½¨è¿¹åˆ†ç±»ï¼ˆåŒºåˆ†â€œè¡¨ç°ä¼˜â€å’Œâ€œè¡¨ç°å·®â€è½¨è¿¹ï¼‰ï¼Œå¹¶åœ¨ç­–ç•¥æ›´æ–°æ—¶å®ç°æ›´ç²¾å‡†çš„â€œåŠŸåŠ³åˆ†é…â€ï¼Œä»è€Œä¼˜åŒ–å¤§æ¨¡å‹çš„â€œæ€è€ƒ-æœç´¢â€è¡Œä¸ºã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªRAGåŸºå‡†æµ‹è¯•ä¸­éªŒè¯LeTSçš„æ•ˆæœï¼š  
- **æ€§èƒ½æå‡**ï¼šåœ¨å„ç±»RAGä»»åŠ¡ä¸Šå®ç°å¹³å‡2.61%çš„æ€§èƒ½å¢ç›Šï¼›  
- **æ•ˆç‡ä¼˜åŒ–**ï¼šç”Ÿæˆtokenæ•°å’Œæœç´¢æ¬¡æ•°åˆ†åˆ«å‡å°‘11.15%ã€30.85%ï¼Œæ¨ç†æ•ˆç‡æ˜¾è‘—æå‡ï¼›  
- **æ³›åŒ–æ€§ä¸é²æ£’æ€§**ï¼šåœ¨ä¸åŒåœºæ™¯ä¸‹ï¼ˆåŸºç¡€æ¨¡å‹ä¸æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼‰å‡å±•ç°å¼ºæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æ­ç¤ºäº†â€œè¿‡ç¨‹+ç»“æœâ€å¥–åŠ±æ··åˆåœ¨å…¶ä»–åœºæ™¯ä¸‹æå‡LLMsæ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ä¸­é—´æ­¥éª¤ç›‘ç£æ€è·¯**ï¼šä¼ ç»ŸRLåœ¨RAGä¸­åªçœ‹â€œç»“æœâ€ï¼ŒLeTSåˆ™å…³æ³¨â€œè¿‡ç¨‹â€ï¼Œä¸ºè§£å†³å¤šæ­¥éª¤ä»»åŠ¡ä¸­ä¸­é—´ç¯èŠ‚å¤±æ§é—®é¢˜æä¾›äº†æ–°è§†è§’â€”â€”é€šè¿‡è½»é‡è§„åˆ™è®¾è®¡è¿‡ç¨‹å¥–åŠ±ï¼Œæ— éœ€é¢å¤–æ ‡æ³¨å³å¯çº¦æŸä¸­é—´è¡Œä¸ºï¼›  
2. **å¥–åŠ±æ··åˆèŒƒå¼**ï¼šå°†ç»†ç²’åº¦è¿‡ç¨‹å¥–åŠ±ä¸ç²—ç²’åº¦ç»“æœå¥–åŠ±ç»“åˆï¼Œè¯æ˜äº†å¤šç»´åº¦å¥–åŠ±åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¯¹å¤æ‚ä»»åŠ¡ï¼ˆå¦‚å¤šè·³æ¨ç†+æ£€ç´¢ï¼‰çš„ä¼˜åŒ–ä»·å€¼ï¼Œå¯å¯å‘å…¶ä»–éœ€è¦åˆ†æ­¥å†³ç­–çš„LLMåº”ç”¨ï¼›  
3. **æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡**ï¼šå®éªŒä¸­åŒæ—¶å®ç°æ€§èƒ½ä¸æ¨ç†æ•ˆç‡æå‡ï¼Œè¯´æ˜LeTSä¸æ˜¯â€œä¸ºäº†ç²¾åº¦ç‰ºç‰²é€Ÿåº¦â€ï¼Œè€Œæ˜¯çœŸæ­£ä¼˜åŒ–äº†â€œæ€è€ƒ-æœç´¢â€çš„å†³ç­–é€»è¾‘ï¼Œè¿™å¯¹å®é™…è½åœ°çš„å·¥ä¸šåŒ–å¤§æ¨¡å‹åº”ç”¨æå…·å‚è€ƒæ„ä¹‰ã€‚  
```

## r1-searcher--incentivizing-the-search-capability-in-llms-via-reinforcement-learning
### Abstract
Existing Large Reasoning Models (LRMs) have shown the potential of
reinforcement learning (RL) to enhance the complex reasoning capabilities of
Large Language Models~(LLMs). While they achieve remarkable performance on
challenging tasks such as mathematics and coding, they often rely on their
internal knowledge to solve problems, which can be inadequate for
time-sensitive or knowledge-intensive questions, leading to inaccuracies and
hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel
two-stage outcome-based RL approach designed to enhance the search capabilities
of LLMs. This method allows LLMs to autonomously invoke external search systems
to access additional knowledge during the reasoning process. Our framework
relies exclusively on RL, without requiring process rewards or distillation for
a cold start. % effectively generalizing to out-of-domain datasets and
supporting both Base and Instruct models. Our experiments demonstrate that our
method significantly outperforms previous strong RAG methods, even when
compared to the closed-source GPT-4o-mini.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | R1-Searcherï¼šç”¨å¼ºåŒ–å­¦ä¹ æ¿€å‘å¤§æ¨¡å‹æœç´¢èƒ½åŠ›ï¼Œçªç ´çŸ¥è¯†å±€é™

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦ã€ç¼–ç ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¾èµ–å†…éƒ¨çŸ¥è¯†è§£å†³é—®é¢˜ï¼Œé¢å¯¹æ—¶æ•ˆæ€§å¼ºæˆ–çŸ¥è¯†å¯†é›†å‹é—®é¢˜æ—¶ï¼Œå®¹æ˜“å‡ºç°ä¸å‡†ç¡®ç”šè‡³å¹»è§‰ï¼ˆhallucinationsï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶èšç„¦äºç”¨å¤–éƒ¨ä¿¡æ¯å¢å¼ºLLMsï¼ˆå³æ£€ç´¢å¢å¼ºç”ŸæˆRAGï¼‰ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¾èµ–é—­æºå¤§æ¨¡å‹ã€è’¸é¦å¯¼è‡´æ³›åŒ–å·®ã€æ¨ç†å¼€é”€å¤§ç­‰ä¸è¶³ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºR1 - Searcherï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€åŠ±LLMsè‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿï¼Œæå‡æœç´¢èƒ½åŠ›ä¸æ¨ç†æ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶
è®¾è®¡ä¸¤é˜¶æ®µRLæ–¹æ³•é€æ­¥æå‡LLMsæœç´¢èƒ½åŠ›ã€‚ç¬¬ä¸€é˜¶æ®µç”¨â€œæ£€ç´¢å¥–åŠ± + æ ¼å¼å¥–åŠ±â€ï¼Œè®©æ¨¡å‹å…ˆå­¦ä¼šæ­£ç¡®è°ƒç”¨å¤–éƒ¨æ£€ç´¢ç³»ç»Ÿçš„æ ¼å¼ï¼Œä¸è€ƒè™‘ç­”æ¡ˆæ­£ç¡®æ€§ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥â€œç­”æ¡ˆå¥–åŠ±â€ï¼Œæ¿€åŠ±æ¨¡å‹åˆ©ç”¨æ£€ç´¢åˆ°çš„çŸ¥è¯†å‡†ç¡®è§£é¢˜ï¼Œå…¨ç¨‹ä»…ä¾èµ–åŸºäºç»“æœçš„RLï¼Œæ— éœ€è’¸é¦æˆ–å†·å¯åŠ¨æ—¶çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€‚é…è®­ç»ƒçš„æ”¹è¿›RLè®­ç»ƒæ–¹æ³•
ä¸ºæ”¯æŒè®­ç»ƒä¸­LLMsä¸å¤–éƒ¨æ£€ç´¢ç¯å¢ƒçš„äº¤äº’æ¢ç´¢ï¼ŒåŸºäºReinforce++æå‡ºæ”¹è¿›æ–¹æ³•ï¼Œç»“åˆRAG - åŸºäºçš„rolloutå’Œæ£€ç´¢æ©ç çš„æŸå¤±è®¡ç®—ï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒæ—¶èƒ½è‡ªä¸»æ¢ç´¢å¦‚ä½•ç”¨æ£€ç´¢è§£å†³é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé’ˆå¯¹æ€§çš„æ•°æ®é€‰æ‹©ç­–ç•¥
ä»HotpotQAå’Œ2WikiMultiHopQAç­‰å¤šè·³é—®ç­”æ•°æ®é›†é€‰è®­ç»ƒæ•°æ®ï¼ŒæŒ‰æ¨¡å‹æ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€rolloutsæ•°é‡ï¼Œå°†æ•°æ®åˆ†ä¸ºæ˜“ã€ä¸­ã€éš¾ä¸‰ä¸ªéš¾åº¦ç­‰çº§ï¼Œåˆ†é˜¶æ®µæ„å»ºè®­ç»ƒé›†ï¼Œè§£å†³æ£€ç´¢ç¯å¢ƒæŸ¥è¯¢èŒƒå›´å¤–ç­‰è®­ç»ƒæ•ˆç‡é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨HotpotQAã€2WikiMultiHopQAã€Bamboogleã€Musiqueç­‰å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒR1 - Searcherè¡¨ç°ä¼˜å¼‚ï¼šç”¨Qwen - 2.5 - 7B - Baseæ—¶ï¼Œåœ¨HotpotQAä¸Šæ¯”å¼ºåŸºçº¿ï¼ˆå«GPT - 4o - miniçš„ReARTeRï¼‰æå‡è¾¾48.22%ï¼Œåœ¨2Wikiä¸Šæå‡21.72%ï¼›åœ¨è®­ç»ƒæœªè§è¿‡çš„Bamboogleåœ¨çº¿æœç´¢åœºæ™¯ä¸­ï¼Œæ¯”32Bå‚æ•°çš„Search - o1æå‡11.4%ï¼›æ•´ä½“åœ¨å››ä¸ªæ•°æ®é›†ä¸ŠæŒç»­è¶…è¶Šç°æœ‰RAGæ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡é—­æºçš„GPT - 4o - miniã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¼ºåŒ–å­¦ä¹ é©±åŠ¨æœç´¢èƒ½åŠ›ï¼šçº¯RLæ–¹å¼è®­ç»ƒï¼Œæ‘†è„±è’¸é¦å’ŒSFTå†·å¯åŠ¨ä¾èµ–ï¼Œä¸ºå¤§æ¨¡å‹èƒ½åŠ›å¢å¼ºæä¾›æ–°èŒƒå¼ï¼Œä¸”å¯¹Baseå’ŒInstructæ¨¡å‹éƒ½æœ‰æ•ˆã€‚
2. ä¸¤é˜¶æ®µè®­ç»ƒæ€è·¯ï¼šåˆ†é˜¶æ®µèšç„¦â€œå­¦ä¼šè°ƒç”¨â€å’Œâ€œç”¨å¥½æ£€ç´¢è§£é¢˜â€ï¼Œè¿™ç§åˆ†æ­¥å¼ºåŒ–çš„æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€åˆ†æ­¥éª¤å­¦ä¹ çš„å¤§æ¨¡å‹èƒ½åŠ›è®­ç»ƒåœºæ™¯ã€‚
3. æ•°æ®ä¸è®­ç»ƒé€‚é…ï¼šæŒ‰ä»»åŠ¡éš¾åº¦åˆ†å±‚é€‰æ•°æ®ã€æ”¹è¿›RLè®­ç»ƒé€‚é…æ£€ç´¢ç¯å¢ƒï¼Œä¸ºå¤„ç†å¤–éƒ¨å·¥å…·äº¤äº’ç±»è®­ç»ƒçš„æ•ˆç‡ä¸æ•ˆæœé—®é¢˜æä¾›äº†å®è·µå‚è€ƒã€‚
```

## research--learning-to-reason-with-search-for-llms-via-reinforcement-learning
### Abstract
Large Language Models (LLMs) have shown remarkable capabilities in reasoning,
exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating
reasoning with external search processes remains challenging, especially for
complex multi-hop questions requiring multiple retrieval steps. We propose
ReSearch, a novel framework that trains LLMs to Reason with Search via
reinforcement learning without using any supervised data on reasoning steps.
Our approach treats search operations as integral components of the reasoning
chain, where when and how to perform searches is guided by text-based thinking,
and search results subsequently influence further reasoning. We train ReSearch
on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct
extensive experiments. Despite being trained on only one dataset, our models
demonstrate strong generalizability across various benchmarks. Analysis reveals
that ReSearch naturally elicits advanced reasoning capabilities such as
reflection and self-correction during the reinforcement learning process.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReSearchï¼šç”¨å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹å­¦ä¼šâ€œè¾¹æœç´¢è¾¹æ¨ç†â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼ŒåƒOpenAI - o1ã€DeepSeek - R1ç­‰æ¨¡å‹çš„æˆåŠŸå°±æ˜¯ä¾‹è¯ã€‚ä¸è¿‡ï¼Œå°†æ¨ç†ä¸å¤–éƒ¨æœç´¢æµç¨‹ç»“åˆä»é¢‡å…·æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é¢å¯¹å¤æ‚å¤šè·³é—®é¢˜ï¼ˆéœ€å¤šè½®æ£€ç´¢çš„é—®é¢˜ï¼‰æ—¶ã€‚ç°æœ‰å¤šæ­¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•å¤šä¾èµ–äººå·¥è®¾è®¡æç¤ºæˆ–å¯å‘å¼è§„åˆ™ï¼Œä¸ä»…è€—æ—¶è€—åŠ›ï¼Œå¯¹å¤æ‚é—®é¢˜ä¹Ÿç¼ºä¹æ‰©å±•æ€§ï¼›è€Œä¸”ç»™å¤šæ­¥RAGæ¡†æ¶æ ‡æ³¨æ¨ç†æ­¥éª¤æˆæœ¬é«˜ã€ä¸ç°å®ã€‚åŒæ—¶ï¼Œå½“ä¸‹åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å·¥ä½œï¼Œå¤§å¤šèšç„¦å†…éƒ¨æ¨ç†ï¼Œé²œå°‘æ¢ç´¢å¦‚ä½•æŠŠæ¨ç†ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æœ‰æ•ˆç»“åˆã€‚äºæ˜¯ï¼Œæœ¬æ–‡æå‡ºReSearchæ¡†æ¶ï¼Œæ—¨åœ¨ç”¨å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹å­¦ä¼šâ€œè¾¹æœç´¢è¾¹æ¨ç†â€ï¼Œæ— éœ€æ¨ç†æ­¥éª¤çš„æœ‰ç›‘ç£æ•°æ®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReSearchæ¡†æ¶ï¼Œå°†æœç´¢ä½œä¸ºæ¨ç†é“¾çš„ä¸€éƒ¨åˆ†
ReSearchæŠŠæœç´¢æ“ä½œè§†ä¸ºé“¾å¼æ¨ç†è¿‡ç¨‹çš„æœ‰æœºç»„æˆï¼Œæ¨ç†é“¾ä¸ä»…åŒ…å«åŸºäºæ–‡æœ¬çš„æ€è€ƒï¼ˆç”¨`[]`åŒ…è£¹ï¼‰ï¼Œè¿˜æœ‰æœç´¢æŸ¥è¯¢ï¼ˆç”¨`<search>` `</search>`åŒ…è£¹ï¼‰å’Œæ£€ç´¢ç»“æœï¼ˆç”¨`<result>` `</result>`åŒ…è£¹ï¼‰ã€‚ä½•æ—¶ã€å¦‚ä½•æ‰§è¡Œæœç´¢ç”±ä¹‹å‰çš„æ–‡æœ¬æ€è€ƒå¼•å¯¼ï¼Œæœç´¢ç»“æœåˆä¼šå½±å“åç»­æ–‡æœ¬æ€è€ƒã€‚å¹¶ä¸”ï¼Œè¯¥æ¡†æ¶ä¸æä¾›æ¨ç†æ­¥éª¤çš„æœ‰ç›‘ç£æ•°æ®è®©æ¨¡å‹æ¨¡ä»¿ï¼Œè€Œæ˜¯å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOç®—æ³•ï¼‰æ¿€åŠ±æ¨¡å‹â€œè¾¹æœç´¢è¾¹æ¨ç†â€ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ ä»å¤´è®­ç»ƒæ¨¡å‹
åœ¨Qwen2.5 - 7Bï¼ˆ- Instructï¼‰å’ŒQwen2.5 - 32Bï¼ˆ- Instructï¼‰æ¨¡å‹ä¸Šä»å¤´è®­ç»ƒReSearchã€‚åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ€è·¯ï¼Œé‡‡æ ·å¤šä¸ªâ€œè¾¹æœç´¢è¾¹æ¨ç†â€çš„é“¾ï¼ˆå³rolloutsï¼‰ï¼Œä¼˜åŒ–å¤§æ¨¡å‹ç­–ç•¥ï¼Œè®©ç”Ÿæˆé«˜å¥–åŠ±rolloutsçš„æ¦‚ç‡æœ€å¤§åŒ–ï¼Œä»¥æ­¤è®©æ¨¡å‹å­¦ä¼šåœ¨æ¨ç†ä¸­åˆç†è¿ç”¨æœç´¢ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRewardå»ºæ¨¡ä¸GRPOç®—æ³•ç»“åˆ
é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä½œä¸ºå­¦ä¹ ç®—æ³•ï¼Œå®ƒä»ä¸€ç»„rolloutsä¸­ä¼°è®¡åŸºçº¿ï¼Œè€Œéåƒè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰é‚£æ ·è®­ç»ƒå•ç‹¬çš„ critic æ¨¡å‹ã€‚é€šè¿‡è®¾è®¡å¥–åŠ±å»ºæ¨¡æ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒä¸­é€æ­¥æŒæ¡â€œè¾¹æœç´¢è¾¹æ¨ç†â€çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆéœ€å¤šæ­¥æ¨ç†å’Œå¤šæ¬¡ä¿¡æ¯æ£€ç´¢ï¼‰ä¸Šå¼€å±•å¤§é‡å®éªŒã€‚è®­ç»ƒåçš„ReSearchæ¨¡å‹ç›¸æ¯”åŸºçº¿æ¨¡å‹ï¼Œç»å¯¹æ€§èƒ½æå‡8.9% - 22.4%ä¸ç­‰ã€‚è€Œä¸”ä»…åœ¨ä¸€ä¸ªç‰¹å®šè®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œå´èƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œåˆ†æè®­ç»ƒè¿‡ç¨‹å‘ç°ï¼ŒReSearchåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½è‡ªç„¶æ¿€å‘åå°„ã€è‡ªæˆ‘ä¿®æ­£ç­‰é«˜çº§æ¨ç†èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šå°†å¤–éƒ¨æœç´¢æ·±åº¦èå…¥æ¨ç†é“¾ï¼Œä¸ºè§£å†³å¤§æ¨¡å‹â€œæ¨ç† + å¤–éƒ¨å·¥å…·â€ç»“åˆéš¾é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œä¸å†ä¾èµ–äººå·¥è®¾è®¡çš„ç¹çæç¤ºæˆ–å¯å‘å¼è§„åˆ™ï¼Œè€Œæ˜¯è®©æ¨¡å‹è‡ªä¸»å­¦ä¹ ä½•æ—¶ã€å¦‚ä½•ç”¨æœç´¢è¾…åŠ©æ¨ç†ã€‚
2. å¼ºåŒ–å­¦ä¹ åº”ç”¨ï¼šè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨æ— éœ€æ¨ç†æ­¥éª¤æœ‰ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½æœ‰æ•ˆè®­ç»ƒå¤§æ¨¡å‹è·å¾—â€œè¾¹æœç´¢è¾¹æ¨ç†â€èƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡å¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œåç»­å·¥ä½œå¯å€Ÿé‰´è¿™ç§â€œæ— ç›‘ç£æ•°æ®ä¾èµ–çš„RLè®­ç»ƒâ€æ€è·¯æ‹“å±•æ¨¡å‹èƒ½åŠ›ã€‚
3. æ³›åŒ–æ€§éªŒè¯ï¼šä»…ç”¨å•ä¸€è®­ç»ƒé›†è®­ç»ƒå´èƒ½åœ¨å¤šåŸºå‡†æµ‹è¯•è¡¨ç°è‰¯å¥½ï¼Œè¯´æ˜è¯¥æ¡†æ¶åœ¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›åŸ¹å…»ä¸Šæœ‰ä¼˜åŠ¿ï¼Œä¸ºæ‰“é€ æ›´é€šç”¨çš„å¤§æ¨¡å‹æ¨ç† + å·¥å…·ä½¿ç”¨èƒ½åŠ›æä¾›äº†å‚è€ƒï¼Œåç»­å¯å›´ç»•å¦‚ä½•è¿›ä¸€æ­¥æå‡æ³›åŒ–æ€§ã€é€‚é…æ›´å¤šå·¥å…·å±•å¼€ç ”ç©¶ã€‚
```

## reinforcement-fine-tuning-for-reasoning-towards-multi-step-multi-source-search-in-large-language-models
### Abstract
Large language models (LLMs) can face factual limitations when responding to
time-sensitive queries about recent events that arise after their knowledge
thresholds in the training corpus. Existing search-augmented approaches fall
into two categories, each with distinct limitations: multi-agent search
frameworks incur substantial computational overhead by separating search
planning and response synthesis across multiple LLMs, while single-LLM
tool-calling methods restrict themselves to sequential planned, single-query
searches from sole search sources. We present Reasoning-Search (R-Search), a
single-LLM search framework that unifies multi-step planning, multi-source
search execution, and answer synthesis within one coherent inference process.
Innovatively, it structure the output into four explicitly defined components,
including reasoning steps that guide the search process (<think>), a
natural-language directed acyclic graph that represents the search plans with
respect to diverse sources (<search>), retrieved results from executing the
search plans (<result>), and synthesized final answers (<answer>). To enable
effective generation of these structured outputs, we propose a specialized
Reinforcement Fine-Tuning (ReFT) method based on GRPO, together with a
multi-component reward function that optimizes LLM's answer correctness,
structural validity of the generated DAG, and adherence to the defined output
format. Experimental evaluation on FinSearchBench-24, SearchExpertBench-25, and
seven Q and A benchmarks demonstrates that R-Search outperforms
state-of-the-art methods, while achieving substantial efficiency gains through
70% reduction in context token usage and approximately 50% decrease in
execution latency. Code is available at
https://github.com/wentao0429/Reasoning-search.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§æ¨¡å‹æœç´¢å¢å¼ºæ–°èŒƒå¼ï¼šR-Search å¦‚ä½•ç”¨å•æ¨¡å‹ç»Ÿä¸€å¤šæ­¥å¤šæºæœç´¢ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨è¯¸å¤šä»»åŠ¡å±•ç°å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œä½†é¢å¯¹è®­ç»ƒè¯­æ–™çŸ¥è¯†é˜ˆå€¼ä¹‹åå‡ºç°çš„**æ—¶æ•ˆæ€§æŸ¥è¯¢ï¼ˆå¦‚è¿‘æœŸäº‹ä»¶ã€åŠ¨æ€äº‹å®ï¼‰**æ—¶ï¼Œä¼šå› è®­ç»ƒè¯­æ–™é™æ€æ€§å¯¼è‡´äº‹å®æ€§å±€é™ã€‚ä¸ºå¼¥è¡¥çŸ¥è¯†ç¼ºå£ï¼Œç°æœ‰æœç´¢å¢å¼ºæ–¹æ³•åˆ†ä¸¤ç±»å´å„æœ‰ç¼ºé™·ï¼š  
- **å¤šæ™ºèƒ½ä½“æœç´¢æ¡†æ¶**ï¼ˆå¦‚ MindSearchã€FinSearch ç­‰ï¼‰ï¼šå°†â€œæœç´¢è§„åˆ’â†’æ‰§è¡Œâ†’ç»“æœåˆæˆâ€æ‹†åˆ†åˆ°å¤šä¸ª LLM å®Œæˆï¼Œè™½èƒ½åšå…¨å±€è§„åˆ’ï¼Œä½†å¤šé˜¶æ®µè°ƒç”¨å¸¦æ¥**å·¨é‡è®¡ç®—å¼€é”€**ï¼ˆtoken æ¶ˆè€—éšæœç´¢å¤æ‚åº¦å€å¢ï¼‰ï¼Œä¸”â€œè§„åˆ’ - æ‰§è¡Œâ€åˆ†ç¦»æ˜“å¯¼è‡´åˆæˆé˜¶æ®µä¸åŸå§‹è§„åˆ’é€»è¾‘è„±èŠ‚ï¼Œå‡ºç°â€œæœ‰è§„åˆ’æ— æ¨ç†â€é—®é¢˜ã€‚  
- **å• LLM å·¥å…·è°ƒç”¨æ–¹æ³•**ï¼ˆå¦‚ Search-R1ã€ZeroSearch ç­‰ï¼‰ï¼šè™½æ•ˆç‡è¾ƒé«˜ï¼Œä½†ä¾èµ–ç¦»çº¿çŸ¥è¯†åº“/æ¨¡æ‹Ÿç¯å¢ƒï¼Œéš¾è·å–çœŸå®å®æ—¶ä¿¡æ¯ï¼›ä¸”å¤šä¸º**å•æŸ¥è¯¢ã€å•æºé¡ºåºæœç´¢**ï¼Œæ— æ³•åˆ©ç”¨å¤šæŸ¥è¯¢ä¾èµ–ä¸å¤šæºäº’è¡¥æ€§ï¼Œåº”å¯¹å¤æ‚æœç´¢éœ€æ±‚æ—¶æˆ˜ç•¥æ·±åº¦ä¸è¶³ã€‚  

å› æ­¤ï¼Œè®ºæ–‡æå‡º **Reasoning-Searchï¼ˆR-Searchï¼‰**ï¼Œç›®æ ‡æ˜¯ç”¨**å•ä¸ª LLM** ç»Ÿä¸€â€œå¤šæ­¥è§„åˆ’â†’å¤šæºæœç´¢æ‰§è¡Œâ†’ç­”æ¡ˆåˆæˆâ€å…¨æµç¨‹ï¼ŒåŒæ—¶è§£å†³æ•ˆç‡ä¸èƒ½åŠ›ç“¶é¢ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå• LLM æ¡†æ¶ä¸‹çš„ç»“æ„åŒ–è¾“å‡ºè®¾è®¡  
R-Search æŠŠæ¨ç†è¿‡ç¨‹çš„è¾“å‡ºæ‹†åˆ†ä¸º**å››ä¸ªæ˜¾å¼ç»„ä»¶**ï¼Œè®©å• LLM ä¸€æ¬¡æ¨ç†å®Œæˆå¤šä»»åŠ¡ï¼š  
- `<reason>`ï¼šæŒ‡å¯¼æœç´¢è¿‡ç¨‹çš„æ¨ç†æ­¥éª¤ï¼ˆè§£é‡Šâ€œä¸ºä»€ä¹ˆè¿™ä¹ˆæœâ€ï¼‰ï¼›  
- `<search>`ï¼šè‡ªç„¶è¯­è¨€æœ‰å‘æ— ç¯å›¾ï¼ˆNL-DAGï¼‰ï¼Œè¡¨ç¤ºè·¨å¤šæºçš„æœç´¢è®¡åˆ’ï¼ˆå®šä¹‰â€œæœä»€ä¹ˆã€æ€ä¹ˆæœã€æºä¹‹é—´å’‹é…åˆâ€ï¼‰ï¼›  
- `<result>`ï¼šæ‰§è¡Œæœç´¢è®¡åˆ’åè·å–çš„ç»“æœï¼›  
- `<answer>`ï¼šæœ€ç»ˆåˆæˆçš„ç­”æ¡ˆã€‚  
è¿™ç§è®¾è®¡è®©â€œæ¨ç† - è§„åˆ’ - æ‰§è¡Œ - åˆæˆâ€åœ¨**åŒä¸€ LLM æ¨ç†æµç¨‹**å®Œæˆï¼Œæ—¢é¿å…å¤šæ™ºèƒ½ä½“çš„ç¢ç‰‡åŒ–æ¶æ„ï¼Œåˆçªç ´å•å·¥å…·è°ƒç”¨çš„å•æº/å•æŸ¥è¯¢é™åˆ¶ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäº GRPO çš„å¼ºåŒ–å¾®è°ƒï¼ˆReFTï¼‰ä¸å¤šç»´åº¦å¥–åŠ±å‡½æ•°  
ä¸ºè®© LLM é«˜æ•ˆç”Ÿæˆä¸Šè¿°ç»“æ„åŒ–è¾“å‡ºï¼Œè®ºæ–‡æå‡º**ä¸“é—¨é¢å‘æœç´¢ä»»åŠ¡çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•**ï¼š  
- åŸºäº GRPOï¼ˆä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼‰è®¾è®¡è®­ç»ƒèŒƒå¼ï¼Œè®© LLM å­¦ä¼šç”Ÿæˆåˆè§„ä¸”æœ‰æ•ˆçš„æœç´¢é€»è¾‘ï¼›  
- æ„å»º**å¤šç»„ä»¶å¥–åŠ±å‡½æ•°**ï¼ŒåŒæ—¶ä¼˜åŒ–ä¸‰ä¸ªç»´åº¦ï¼š  
  - ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆæœ€ç»ˆå›ç­”æ˜¯å¦å‡†ç¡®ï¼‰ï¼›  
  - DAG ç»“æ„æœ‰æ•ˆæ€§ï¼ˆNL-DAG æ˜¯å¦èƒ½è¢«å¯é è§£æã€æ‰§è¡Œï¼‰ï¼›  
  - è¾“å‡ºæ ¼å¼åˆè§„æ€§ï¼ˆæ˜¯å¦ä¸¥æ ¼éµå¾ª `<reason>/<search>/<result>/<answer>` ç»“æ„ï¼‰ã€‚  
  ä¸‰è€…ç»“åˆç¡®ä¿ LLM æ—¢è¾“å‡ºå‡†ç¡®ç­”æ¡ˆï¼Œåˆç”Ÿæˆå¯æ‰§è¡Œã€æ˜“è§£æçš„æœç´¢è®¡åˆ’ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ **FinSearchBench-24ã€SearchExpertBench-25** ç­‰é‡‘è/å¤æ‚æœç´¢åŸºå‡†ï¼Œä»¥åŠ 7 ä¸ªé€šç”¨ QA åŸºå‡†ä¸Šæµ‹è¯•ï¼ŒéªŒè¯ R-Search ä¼˜åŠ¿ï¼š  
- **æ•ˆæœé¢†å…ˆ**ï¼šè¶…è¶Šç°æœ‰ SOTA æ–¹æ³•ï¼›  
- **æ•ˆç‡æš´å¢**ï¼šä¸Šä¸‹æ–‡ token æ¶ˆè€—å‡å°‘ 70%ï¼Œæ‰§è¡Œå»¶è¿Ÿé™ä½çº¦ 50%ï¼›  
- å¤šæº/å¤šæ­¥æœç´¢èƒ½åŠ›éªŒè¯ï¼šåœ¨éœ€è·¨æºã€å¤šè½®æŸ¥è¯¢çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œç»“æ„åŒ–è¾“å‡ºä¸å¼ºåŒ–å¾®è°ƒè®© R-Search æ¯”å•å·¥å…·è°ƒç”¨æ›´å…·ç­–ç•¥æ·±åº¦ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ¶æ„è®¾è®¡æ€è·¯**ï¼šç”¨â€œå•æ¨¡å‹ + ç»“æ„åŒ–è¾“å‡ºâ€ç»Ÿä¸€å¤šé˜¶æ®µä»»åŠ¡ï¼Œå‡å°‘å¤šæ™ºèƒ½ä½“ç¢ç‰‡åŒ–å¼€é”€ï¼Œä¸ºå¤æ‚ä»»åŠ¡çš„ç«¯åˆ°ç«¯æ¨ç†æä¾›èŒƒå¼å‚è€ƒï¼›  
2. **å¼ºåŒ–å­¦ä¹ åœ¨ LLM å·¥å…·å¢å¼ºçš„è½åœ°**ï¼šé€šè¿‡å¤šç»´åº¦å¥–åŠ±å‡½æ•°ï¼ŒæŠŠâ€œç­”æ¡ˆè´¨é‡ã€ç»“æ„æœ‰æ•ˆæ€§ã€æ ¼å¼åˆè§„æ€§â€çº³å…¥è®­ç»ƒç›®æ ‡ï¼Œè§£å†³ç»“æ„åŒ–è¾“å‡ºçš„å¯æ§ç”Ÿæˆéš¾é¢˜ï¼›  
3. **å¤šæºæœç´¢èƒ½åŠ›çš„é‡Šæ”¾**ï¼šNL-DAG è®¾è®¡è®©å• LLM èƒ½åè°ƒå¤šæºï¼ˆå¦‚å­¦æœ¯åº“ã€å®æ—¶æ–°é—»ã€å‚ç›´æœç´¢å¼•æ“ï¼‰ï¼ŒæŒ–æ˜ä¸åŒä¿¡æ¯åº“çš„äº’è¡¥ä»·å€¼ï¼Œå¯å‘æœªæ¥å¤šæºèåˆç±»ä»»åŠ¡çš„è§£æ³•ã€‚  

ä»£ç å·²å¼€æºï¼ˆhttps://github.com/wentao0429/Reasoning-searchï¼‰ï¼Œå¯¹å¤§æ¨¡å‹å·¥å…·å¢å¼ºã€æœç´¢è§„åˆ’æ–¹å‘æ„Ÿå…´è¶£çš„åŒå­¦å¯æ·±å…¥ç ”ç©¶ï½
```

## zerosearch--incentivize-the-search-capability-of-llms-without-searching
### Abstract
Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a novel RL
framework that incentivizes the capabilities of LLMs to use a real search
engine with simulated searches during training. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both useful and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ZeroSearchï¼šæ— éœ€çœŸå®æœç´¢ï¼Œæ¿€å‘å¤§æ¨¡å‹æœç´¢èƒ½åŠ›æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨è¯¸å¤šä¸‹æ¸¸ä»»åŠ¡è¡¨ç°å“è¶Šï¼Œä½†çŸ¥è¯†é™æ€ä¸”æ˜“ç”Ÿæˆå¹»è§‰æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Œéœ€å¤–æ¥ä¿¡æ¯å¢å¼ºå¯é æ€§ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯å¸¸ç”¨æ–¹æ¡ˆï¼Œä¸è¿‡æ—©æœŸåŸºäºæç¤ºå·¥ç¨‹ã€åç»­ç›‘ç£å¾®è°ƒæˆ–æ¨ç†æ—¶ç¼©æ”¾æŠ€æœ¯ç­‰å­˜åœ¨å·¥ç¨‹å¤æ‚ã€è®¡ç®—å¼€é”€å¤§ç­‰é—®é¢˜ã€‚è¿‘å¹´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæå‡å¤§æ¨¡å‹æœç´¢èƒ½åŠ›ï¼Œå´é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯æœç´¢å¼•æ“è¿”å›æ–‡æ¡£è´¨é‡ä¸å¯æ§ï¼Œç»™è®­ç»ƒå¼•å…¥å™ªå£°ä¸ä¸ç¨³å®šï¼›äºŒæ˜¯RLè®­ç»ƒéœ€é¢‘ç¹è°ƒç”¨æœç´¢APIï¼Œæˆæœ¬é«˜æ˜‚é™åˆ¶æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºZeroSearchæ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒæ—¶ç”¨æ¨¡æ‹Ÿæœç´¢æ¥æ¿€å‘å¤§æ¨¡å‹ä½¿ç”¨çœŸå®æœç´¢å¼•æ“çš„èƒ½åŠ›ï¼Œè§£å†³ä¸Šè¿°ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè½»é‡ç›‘ç£å¾®è°ƒæ‰“é€ æ£€ç´¢æ¨¡å—  
é€šè¿‡è½»é‡çº§ç›‘ç£å¾®è°ƒï¼ŒæŠŠå¤§è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºèƒ½å“åº”æŸ¥è¯¢ã€ç”Ÿæˆâ€œæœ‰ç”¨â€å’Œâ€œå«å™ªå£°â€æ–‡æ¡£çš„æ£€ç´¢æ¨¡å—ã€‚åˆ©ç”¨promptè®¾è®¡åŒºåˆ†ä¸åŒè´¨é‡æ–‡æ¡£ï¼Œè®©æ¨¡æ‹Ÿç”¨çš„å¤§æ¨¡å‹å­¦ä¼šç”Ÿæˆä¸åŒè´¨é‡æ–‡æ¡£ï¼Œæ›¿ä»£çœŸå®æœç´¢å¼•æ“è¿”å›å†…å®¹ï¼Œæ—¢è§„é¿APIæˆæœ¬ï¼Œåˆèƒ½æ§åˆ¶æ–‡æ¡£è´¨é‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯¾ç¨‹å¼rolloutç­–ç•¥æ¸è¿›è®­ç»ƒ  
RLè®­ç»ƒé˜¶æ®µé‡‡ç”¨åŸºäºè¯¾ç¨‹çš„rolloutç­–ç•¥ï¼Œè®©ç”Ÿæˆæ–‡æ¡£çš„è´¨é‡é€æ­¥é™ä½ï¼Œæ¨¡æ‹Ÿè¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„æ£€ç´¢åœºæ™¯ï¼Œå¾ªåºæ¸è¿›æ¿€å‘æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚å…ˆè®©ç­–ç•¥æ¨¡å‹å­¦ä¹ åŸºç¡€è¾“å‡ºæ ¼å¼ä¸ä»»åŠ¡è¦æ±‚ï¼Œå†é€‚åº”æ›´å¤æ‚å¸¦å™ªå£°çš„æ£€ç´¢åœºæ™¯ï¼Œå®ç°èƒ½åŠ›é˜¶æ¢¯å¼æå‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒéªŒè¯ZeroSearchæœ‰æ•ˆæ€§ï¼šç”¨3Bå‚æ•°å¤§æ¨¡å‹ä½œæ£€ç´¢æ¨¡å—æ—¶ï¼Œèƒ½æœ‰æ•ˆæ¿€å‘ç­–ç•¥æ¨¡å‹ä½¿ç”¨çœŸå®æœç´¢å¼•æ“çš„èƒ½åŠ›ï¼›7Bå‚æ•°æ¨¡å‹ä½œæ¨¡æ‹Ÿæ£€ç´¢æ¨¡å—ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥æ¨¡å‹æ€§èƒ½ä¸çœŸå®æœç´¢å¼•æ“è®­ç»ƒçš„ç›¸å½“ï¼›14Bå‚æ•°æ¨¡å‹ä½œæ¨¡æ‹Ÿæ—¶ï¼Œæ€§èƒ½ç”šè‡³è¶…è¶ŠçœŸå®æœç´¢å¼•æ“ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä¸Šæ³›åŒ–æ€§å¥½ï¼Œè¿˜èƒ½å…¼å®¹REINFORCEã€PPOã€GRPOç­‰å¤šç§RLç®—æ³•ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šå·§å¦™åˆ©ç”¨å¤§æ¨¡å‹é¢„è®­ç»ƒç§¯ç´¯çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œç”¨æ¨¡å‹æ¨¡æ‹Ÿæœç´¢å¼•æ“ï¼Œç»•å¼€çœŸå®æœç´¢APIè°ƒç”¨æˆæœ¬ä¸è´¨é‡ä¸å¯æ§é—®é¢˜ï¼Œä¸ºRLç»“åˆæœç´¢åœºæ™¯è®­ç»ƒå¼€è¾Ÿæ–°è·¯å¾„ã€‚  
2. è®­ç»ƒç­–ç•¥ï¼šè¯¾ç¨‹å¼rolloutæœºåˆ¶ä¸ºå¤„ç†â€œéš¾åº¦é€’å¢ä»»åŠ¡â€ç±»è®­ç»ƒæä¾›å‚è€ƒï¼Œå¯è¿ç§»åˆ°éœ€é€æ­¥æå‡æ¨¡å‹èƒ½åŠ›çš„åœºæ™¯ï¼›è½»é‡ç›‘ç£å¾®è°ƒå¿«é€Ÿæ„å»ºåŠŸèƒ½æ¨¡å—çš„æ€è·¯ï¼Œåœ¨æ‰“é€ ç‰¹å®šèƒ½åŠ›å­æ¨¡å—æ—¶å€¼å¾—å€Ÿé‰´ã€‚  
3. å…¼å®¹æ€§ï¼šå¯¹ä¸åŒç±»å‹ï¼ˆåŸºç¡€/æŒ‡ä»¤å¾®è°ƒï¼‰ã€ä¸åŒå‚æ•°è§„æ¨¡å¤§æ¨¡å‹ä»¥åŠå¤šç§RLç®—æ³•çš„è‰¯å¥½å…¼å®¹æ€§ï¼Œè®©è¯¥æ¡†æ¶åœ¨å·¥ä¸šç•Œæˆ–ç ”ç©¶ä¸­æ›´æ˜“è½åœ°æ‹“å±•ï¼Œå‡å°‘é€‚é…æˆæœ¬ã€‚  
```

## constructing-and-evaluating-declarative-rag-pipelines-in-pyterrier
### Abstract
Search engines often follow a pipeline architecture, where complex but
effective reranking components are used to refine the results of an initial
retrieval. Retrieval augmented generation (RAG) is an exciting application of
the pipeline architecture, where the final component generates a coherent
answer for the users from the retrieved documents. In this demo paper, we
describe how such RAG pipelines can be formulated in the declarative PyTerrier
architecture, and the advantages of doing so. Our PyTerrier-RAG extension for
PyTerrier provides easy access to standard RAG datasets and evaluation
measures, state-of-the-art LLM readers, and using PyTerrier's unique operator
notation, easy-to-build pipelines. We demonstrate the succinctness of indexing
and RAG pipelines on standard datasets (including Natural Questions) and how to
build on the larger PyTerrier ecosystem with state-of-the-art sparse,
learned-sparse, and dense retrievers, and other neural rankers.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨PyTerrieræ„å»ºä¸è¯„ä¼°å£°æ˜å¼RAGæµæ°´çº¿ï¼šç®€åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆå®éªŒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œå¤šé˜¶æ®µæ¶æ„ï¼ˆå¦‚æ’åºå­¦ä¹ ã€ç¥ç»é‡æ’å™¨ã€å¯†é›†æ£€ç´¢ç­‰ï¼‰æ„ˆå‘é‡è¦ï¼›è€Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä½œä¸ºçƒ­é—¨å¤šé˜¶æ®µæ¶æ„ï¼Œèƒ½ç»“åˆæ£€ç´¢æ–‡æ¡£è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ›´å¯é å›ç­”ï¼Œä½†ä¼ ç»ŸRAGå®éªŒå­˜åœ¨è¯¸å¤šä¸ä¾¿ï¼šç°æœ‰æ¡†æ¶è¦ä¹ˆéœ€ç”¨æˆ·è‡ªè¡Œå®ç°æµæ°´çº¿ï¼ˆå¦‚DSPyï¼‰ã€è¦ä¹ˆä»…æ”¯æŒå•ä¸€é¡ºåºå¼RAGï¼ˆå¦‚BERGENï¼‰ã€è¦ä¹ˆä¾èµ–é…ç½®æ–‡ä»¶ä¸”ç»„ä»¶æœ‰é™ï¼ˆå¦‚FlashRAGï¼‰ã€‚åŒæ—¶ï¼ŒPyTerrierä½œä¸ºæ”¯æŒå¤šé˜¶æ®µä¿¡æ¯æ£€ç´¢å®éªŒçš„å£°æ˜å¼å¹³å°ï¼Œå°šæœªé’ˆå¯¹RAGåœºæ™¯åšä¸“é¡¹æ‹“å±•ã€‚å› æ­¤ï¼Œæœ¬æ–‡åŠ¨æœºæ˜¯åŸºäºPyTerrieræ‰“é€ PyTerrier - RAGæ‰©å±•ï¼Œç®€åŒ–RAGæµæ°´çº¿æ„å»ºã€æ•°æ®é›†ä¸è¯„ä¼°æ¥å…¥ï¼Œè®©ç ”ç©¶è€…æ›´é«˜æ•ˆå¼€å±•RAGå®éªŒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºPyTerrierå£°æ˜å¼æ¶æ„å°è£…RAGæµæ°´çº¿  
PyTerrieræœ¬èº«æ”¯æŒå°†ç´¢å¼•ã€æ£€ç´¢ç­‰æ“ä½œä»¥å£°æ˜å¼è¡¨è¾¾å¼ç»„åˆæˆæµæ°´çº¿ï¼ŒPyTerrier - RAGåœ¨æ­¤åŸºç¡€ä¸Šï¼Œä¸ºRAGåœºæ™¯å®šåˆ¶æ•°æ®æ¨¡å‹ä¸ç®—å­ï¼Œè®©RAGæµæ°´çº¿ï¼ˆä»æ–‡æ¡£æ£€ç´¢åˆ°LLMç”Ÿæˆå›ç­”ï¼‰èƒ½åƒæ­ç§¯æœ¨ä¸€æ ·ç”¨ç›´è§‚ç®—å­å¿«é€Ÿæ„å»ºï¼Œæ— éœ€å…³æ³¨æŸ¥è¯¢å±‚é¢ç»†èŠ‚ï¼Œä»£ç å¯è¯»æ€§å¼ºã€‚ä¾‹å¦‚å€ŸåŠ©PyTerrierç‰¹æœ‰çš„ç®—å­ç¬¦å·ï¼Œèƒ½æŠŠæ£€ç´¢å™¨ã€é‡æ’å™¨ã€LLMé˜…è¯»å™¨ç­‰ç»„ä»¶æµç•…ä¸²è”æˆRAGæµç¨‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸°å¯Œç”Ÿæ€ä¸å·¥å…·é“¾æ•´åˆ  
ä¸€æ–¹é¢ï¼ŒPyTerrier - RAGæä¾›æ ‡å‡†RAGæ•°æ®é›†ï¼ˆå¦‚Natural Questionsï¼‰çš„ä¾¿æ·è®¿é—®æ–¹å¼ï¼Œè¿˜åŒ…å«é¢„æ„å»ºç´¢å¼•ï¼Œçœå»ç ”ç©¶è€…æ‰‹åŠ¨å¤„ç†æ•°æ®çš„ç¹çï¼›å¦ä¸€æ–¹é¢ï¼Œæ•´åˆå‰æ²¿LLMé˜…è¯»å™¨ã€å¤šç§æ£€ç´¢å™¨ï¼ˆç¨€ç–ã€ä¹ å¾—ç¨€ç–ã€å¯†é›†æ£€ç´¢ç­‰ï¼‰ä¸ç¥ç»æ’åºå™¨ï¼Œè®©å®éªŒèƒ½è½»æ¾å¯¹æ¥å­¦ç•Œæœ€æ–°æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œå†…ç½®QAä»»åŠ¡è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚EM%ã€F1ï¼‰ï¼Œä¸€ç«™å¼æ»¡è¶³RAGä»æ•°æ®å‡†å¤‡ã€æµæ°´çº¿æ­å»ºåˆ°ç»“æœè¯„ä¼°çš„å…¨æµç¨‹éœ€æ±‚ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¯æŒå¤šå…ƒRAGæ¶æ„  
ä¸åŒäºéƒ¨åˆ†ä»…æ”¯æŒâ€œæ£€ç´¢å™¨ + é˜…è¯»å™¨â€é¡ºåºå¼RAGçš„æ¡†æ¶ï¼ˆå¦‚BERGENï¼‰ï¼ŒPyTerrier - RAGè¿˜èƒ½æ”¯æŒè¿­ä»£å¼/è‡ªé€‚åº”RAGï¼ˆå¦‚IR - CoTè¿™ç±»éœ€å¤šè½®æ£€ç´¢æ¨ç†çš„åœºæ™¯ï¼‰ã€‚é€šè¿‡æ‰©å±•PyTerrieræ•°æ®æ¨¡å‹é€‚é…å¤šè½®äº¤äº’é€»è¾‘ï¼Œè®©å¤æ‚RAGå®éªŒä¹Ÿèƒ½åœ¨å£°æ˜å¼èŒƒå¼ä¸‹é«˜æ•ˆå®ç°ï¼ŒåŠ©åŠ›ç ”ç©¶è€…æ¢ç´¢å¤šæ­¥æ¨ç†ç­‰æ›´å…·æŒ‘æˆ˜æ€§çš„RAGä»»åŠ¡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨æ ‡å‡†æ•°æ®é›†ï¼ˆå¦‚Natural Questionsï¼‰ä¸Šå±•ç¤ºäº†PyTerrier - RAGçš„ç®€æ´æ€§ï¼šä»ç´¢å¼•æ„å»ºåˆ°RAGæµæ°´çº¿æ­å»ºï¼Œä»£ç è¡¨è¾¾æä¸ºç´§å‡‘ã€‚åŒæ—¶ï¼Œå€ŸåŠ©PyTerrierç”Ÿæ€ä¸­å„ç±»å…ˆè¿›æ£€ç´¢ã€æ’åºç»„ä»¶ï¼ŒéªŒè¯äº†åœ¨ä¸åŒRAGæ¶æ„ä¸‹ï¼ˆåŒ…æ‹¬è¿­ä»£å¼ï¼‰èƒ½é«˜æ•ˆæ•´åˆæŠ€æœ¯æ¨¡å—ï¼Œå®Œæˆä»æ–‡æ¡£æ£€ç´¢åˆ°ç­”æ¡ˆç”Ÿæˆçš„å…¨æµç¨‹å®éªŒï¼Œå……åˆ†ä½“ç°å…¶åœ¨RAGç ”ç©¶ä¸­â€œä½ä»£ç è´Ÿæ‹…ã€é«˜ç»„ä»¶å…¼å®¹æ€§â€çš„ä¼˜åŠ¿ï¼Œè¯æ˜è¯¥æ‰©å±•èƒ½æœ‰æ•ˆé™ä½RAGå®éªŒé—¨æ§›ã€æå‡ç ”ç©¶æ•ˆç‡ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å£°æ˜å¼æ¶æ„æ€ç»´ï¼šåœ¨æ„å»ºå¤æ‚å¤šé˜¶æ®µç³»ç»Ÿï¼ˆå¦‚RAGï¼‰æ—¶ï¼Œé‡‡ç”¨å£°æ˜å¼èŒƒå¼èƒ½è®©ä»£ç æ›´æ˜“è¯»ã€æ˜“ç»´æŠ¤ï¼Œèšç„¦é€»è¾‘æœ¬èº«è€Œéæ‰§è¡Œç»†èŠ‚ï¼Œè¿™ä¸€æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–AIæµæ°´çº¿ç±»é¡¹ç›®ï¼ˆå¦‚å¤šæ¨¡æ€æ£€ç´¢ã€æ¨èç³»ç»Ÿç­‰ï¼‰ã€‚  
2. ç”Ÿæ€åŒ–å·¥å…·é“¾æ‰“é€ ï¼šé€šè¿‡æ•´åˆæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ã€å‰æ²¿æ¨¡å‹ç­‰èµ„æºï¼Œå½¢æˆâ€œä¸€ç«™å¼â€ç ”ç©¶å·¥å…·ï¼Œä¸ºé¢†åŸŸå†…å·¥å…·å¼€å‘æä¾›å‚è€ƒâ€”â€”è®©ç ”ç©¶è€…å‡å°‘é‡å¤é€ è½®å­ï¼ŒæŠŠç²¾åŠ›æ”¾åœ¨ç®—æ³•åˆ›æ–°ä¸Šã€‚  
3. å¤šå…ƒæ¶æ„æ”¯æŒï¼šå¯¹RAGè¿™ç±»ä»åœ¨å¿«é€Ÿæ¼”è¿›çš„æŠ€æœ¯ï¼Œå·¥å…·éœ€å‰ç»æ€§æ”¯æŒä¸åŒæµæ´¾ï¼ˆé¡ºåºå¼ã€è¿­ä»£å¼ç­‰ï¼‰æ¶æ„ï¼ŒPyTerrier - RAGçš„è®¾è®¡æ€è·¯å¯ç¤ºæˆ‘ä»¬ï¼Œå¹³å°ç±»é¡¹ç›®è¦é¢„ç•™æ‹“å±•æ€§ï¼Œé€‚é…å­¦æœ¯ç ”ç©¶çš„å¤šæ ·æ€§éœ€æ±‚ã€‚  
```

## single-llm--multiple-roles--a-unified-retrieval-augmented-generation-framework-using-role-specific-token-optimization
### Abstract
Existing studies have optimized retrieval-augmented generation (RAG) across
various sub-tasks, such as query understanding and retrieval refinement, but
integrating these optimizations into a unified framework remains challenging.
To tackle this problem, this work proposes RoleRAG, a unified RAG framework
that achieves efficient multi-task processing through role-specific token
optimization. RoleRAG comprises six modules, each handling a specific sub-task
within the RAG process. Additionally, we introduce a query graph to represent
the decomposition of the query, which can be dynamically resolved according to
the decomposing state. All modules are driven by the same underlying LLM,
distinguished by task-specific role tokens that are individually optimized.
This design allows RoleRAG to dynamically activate different modules within a
single LLM instance, thereby streamlining deployment and reducing resource
consumption. Experimental results on five open-domain question-answering
datasets demonstrate the effectiveness, generalizability, and flexibility of
our framework.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å•LLMç©è½¬å¤šè§’è‰²ï¼šRoleRAGç”¨è§’è‰²ä¸“å±Tokenæ‰“é€ ç»Ÿä¸€RAGæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨å‡†ç¡®æ€§ã€å¯é æ€§å’Œæ—¶æ•ˆæ€§ä¸Šä»å­˜æŒ‘æˆ˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ºè¿™äº›é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£æ³•ï¼Œå®ƒèƒ½è®©LLMç»“åˆå¤–éƒ¨çŸ¥è¯†ç”Ÿæˆæ›´ä¼˜å“åº”ã€‚ç„¶è€Œç°æœ‰RAGä¼˜åŒ–ç ”ç©¶å­˜åœ¨ä¸¤å¤§å›°å¢ƒï¼šä¸€æ˜¯å¤šæ•°ç ”ç©¶èšç„¦å•ä¸ªå­ä»»åŠ¡ä¼˜åŒ–ï¼ˆå¦‚æŸ¥è¯¢ç†è§£ã€æ£€ç´¢ç²¾ä¿®ç­‰ï¼‰ï¼Œå´éš¾æ•´åˆåˆ°ç»Ÿä¸€æ¡†æ¶ï¼›äºŒæ˜¯å°‘æ•°å°è¯•åœ¨å•LLMå†…æ•´åˆå¤šç»„ä»¶çš„å·¥ä½œï¼Œé¢ä¸´é¢å¤–æ•°æ®æ”¶é›†ã€é‡è®­æˆæœ¬é«˜ï¼Œä»¥åŠè‡ªåæ€æœºåˆ¶å¤æ‚åº¦ä¸Šå‡å¯¼è‡´æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæ‰“é€ èƒ½é«˜æ•ˆæ•´åˆå¤šä»»åŠ¡ã€ä½èµ„æºæ¶ˆè€—çš„ç»Ÿä¸€RAGæ¡†æ¶æˆä¸ºå…³é”®è¯‰æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRoleRAGç»Ÿä¸€æ¡†æ¶ï¼ŒåŸºäºè§’è‰²ä¸“å±Tokenä¼˜åŒ–å®ç°å¤šä»»åŠ¡é«˜æ•ˆå¤„ç†  
RoleRAGåŒ…å«å…­ä¸ªæ¨¡å—ï¼ˆæŸ¥è¯¢å›¾æ„å»ºå™¨ã€æ£€ç´¢åˆ¤æ–­å™¨ã€å­ç­”æ¡ˆç”Ÿæˆå™¨ã€æ€»ç»“å™¨ã€æ–°æŸ¥è¯¢ç”Ÿæˆå™¨ã€ç­”æ¡ˆæ¨ç†å™¨ï¼‰ï¼Œå„æ¨¡å—å¯¹åº”RAGæµç¨‹ä¸­ç‰¹å®šå­ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥ä»»åŠ¡ä¸“å±çš„â€œè§’è‰²Tokenâ€å¹¶ä¼˜åŒ–å…¶åµŒå…¥ï¼Œè®©åŒä¸€åŸºç¡€LLMèƒ½åœ¨ä¸åŒæ¨¡å—é—´åŠ¨æ€åˆ‡æ¢åŠŸèƒ½ã€‚è®­ç»ƒæ—¶ä»…ä¼˜åŒ–è§’è‰²Tokenï¼Œæ¨ç†æ—¶å•LLMå®ä¾‹é ä¸åŒè§’è‰²Tokenæ¿€æ´»å¯¹åº”æ¨¡å—ï¼Œç®€åŒ–éƒ¨ç½²ä¸”é™ä½èµ„æºæ¶ˆè€—ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡æŸ¥è¯¢å›¾åŠ¨æ€åˆ†è§£ä¸ä¼˜åŒ–æœºåˆ¶  
å¼•å…¥æŸ¥è¯¢å›¾æ¥è¡¨ç¤ºæŸ¥è¯¢çš„åˆ†è§£ç»“æ„ï¼Œèƒ½ä¾æ®åˆ†è§£çŠ¶æ€åŠ¨æ€è§£æã€‚æ¯”å¦‚å°†å¤æ‚åŸå§‹æŸ¥è¯¢æ‹†åˆ†ä¸ºå¤šä¸ªå­æŸ¥è¯¢å¹¶æ„å»ºæœ‰å‘æ— ç¯å›¾ï¼Œè¿‡ç¨‹ä¸­è¿˜ä¼šåŠ¨æ€å‰”é™¤å†—ä½™å­æŸ¥è¯¢ã€æŒ‰éœ€ç”Ÿæˆæ–°å­æŸ¥è¯¢ï¼Œæå‡æ£€ç´¢æ•ˆç‡ä¸ç›¸å…³æ€§ï¼Œæ›´å¥½å¤„ç†å¤æ‚æŸ¥è¯¢åœºæ™¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå‘å¸ƒè¦†ç›–RAGå…¨æµç¨‹çš„æ•°æ®é›†  
ä¸ºè®­ç»ƒä¸åŒRAGæ¨¡å—ï¼Œå›¢é˜Ÿå‘å¸ƒäº†é¦–ä¸ªæ¶µç›–RAGç³»ç»Ÿå®Œæ•´ pipeline çš„æ•°æ®é›†ï¼Œä¸ºç›¸å…³æ¨¡å—è®­ç»ƒæä¾›å…¨é¢æ•°æ®æ”¯æ’‘ï¼ŒåŠ©åŠ›æ•´ä¸ªæ¡†æ¶æ€§èƒ½æå‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šå¼€å±•å®éªŒï¼ŒRoleRAGåœ¨ç²¾ç¡®åŒ¹é…åˆ†æ•°ï¼ˆexact match scoreï¼‰æŒ‡æ ‡ä¸Šï¼Œè¾ƒç°æœ‰SOTA RAGæ–¹æ³•å®ç°äº†16% - 64%çš„æ€§èƒ½æå‡ã€‚é¢å¤–å®éªŒä¹ŸéªŒè¯äº†æ¡†æ¶åœ¨æ³›åŒ–æ€§ä¸é²æ£’æ€§ä¸Šçš„ä¼˜åŠ¿ï¼Œå……åˆ†å±•ç°å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹ç¨³å®šé«˜æ•ˆå·¥ä½œçš„èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šä»»åŠ¡ç»Ÿä¸€æ¡†æ¶æ€è·¯ï¼šå°†RAGå„å­ä»»åŠ¡æ•´åˆåˆ°å•LLMé©±åŠ¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡è§’è‰²Tokenè§£è€¦ä¸åŒåŠŸèƒ½æ¨¡å—ï¼Œä¸ºå¤æ‚ä»»åŠ¡æµç¨‹çš„å·¥ç¨‹åŒ–è½åœ°æä¾›äº†â€œå•æ¨¡å‹å¤šèƒ½åŠ›â€çš„é«˜æ•ˆèŒƒå¼ï¼Œå‡å°‘å¤šæ¨¡å‹åä½œçš„èµ„æºå¼€é”€ã€‚  
2. åŠ¨æ€æŸ¥è¯¢å¤„ç†æœºåˆ¶ï¼šæŸ¥è¯¢å›¾çš„åŠ¨æ€æ„å»ºä¸ä¼˜åŒ–ï¼Œä¸ºå¤„ç†å¤æ‚ã€å¤šæ­¥éª¤æ¨ç†ç±»æŸ¥è¯¢æä¾›äº†ç»“æ„åŒ–åˆ†è§£æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°éœ€åˆ†æ­¥æ‹†è§£é—®é¢˜çš„æ™ºèƒ½é—®ç­”ã€çŸ¥è¯†æ¨ç†ç­‰åœºæ™¯ã€‚  
3. è½»é‡åŒ–è®­ç»ƒç­–ç•¥ï¼šä»…ä¼˜åŒ–è§’è‰²Tokenè€Œéå…¨æ¨¡å‹å‚æ•°ï¼Œåœ¨ä¿è¯æ¨¡å—åŠŸèƒ½å®ç°çš„åŒæ—¶å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ï¼Œè¿™ç§â€œè½¯æç¤º + å±€éƒ¨è°ƒä¼˜â€çš„æ–¹å¼å¯¹èµ„æºæœ‰é™çš„ç ”å‘åœºæ™¯å¾ˆæœ‰å‚è€ƒä»·å€¼ã€‚  
4. å…¨æµç¨‹æ•°æ®é›†å»ºè®¾ï¼šæ‰“é€ è¦†ç›–RAGå…¨æµç¨‹çš„æ•°æ®é›†ï¼Œå¡«è¡¥é¢†åŸŸæ•°æ®ç©ºç™½ï¼Œä¸ºåç»­ç ”ç©¶ä¸­æ¨¡å—è®­ç»ƒã€æ€§èƒ½å¯¹æ¯”ç­‰æä¾›äº†åŸºç¡€èµ„æºï¼Œå¯å‘ç ”ç©¶è€…é‡è§†å‚ç›´é¢†åŸŸå…¨é“¾è·¯æ•°æ®çš„æ„å»ºã€‚  
```

## r-search--empowering-llm-reasoning-with-search-via-multi-reward-reinforcement-learning
### Abstract
Large language models (LLMs) have notably progressed in multi-step and
long-chain reasoning. However, extending their reasoning capabilities to
encompass deep interactions with search remains a non-trivial challenge, as
models often fail to identify optimal reasoning-search interaction
trajectories, resulting in suboptimal responses. We propose R-Search, a novel
reinforcement learning framework for Reasoning-Search integration, designed to
enable LLMs to autonomously execute multi-step reasoning with deep search
interaction, and learn optimal reasoning search interaction trajectories via
multi-reward signals, improving response quality in complex logic- and
knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when
to retrieve or reason, while globally integrating key evidence to enhance deep
knowledge interaction between reasoning and search. During RL training,
R-Search provides multi-stage, multi-type rewards to jointly optimize the
reasoning-search trajectory. Experiments on seven datasets show that R-Search
outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%
(out-of-domain). The code and data are available at
https://github.com/QingFei1/R-Search.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | R-Searchï¼šç”¨å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ èµ‹èƒ½LLMæ¨ç†ä¸æœç´¢æ·±åº¦äº¤äº’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥éª¤å’Œé•¿é“¾æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¦è®©å…¶æ¨ç†èƒ½åŠ›ä¸æœç´¢æ·±åº¦äº¤äº’ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹å¸¸éš¾ä»¥æ‰¾åˆ°æœ€ä¼˜çš„â€œæ¨ç† - æœç´¢â€äº¤äº’è½¨è¿¹ï¼Œå¯¼è‡´è¾“å‡ºè´¨é‡æ¬ ä½³ã€‚åœ¨å¤æ‚çš„é€»è¾‘å¯†é›†å‹å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚å¤šè·³é—®ç­”ï¼‰ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯æ¨¡å‹å†…éƒ¨å†³å®šçš„æ£€ç´¢æ—¶æœºæœªå¿…å¥‘åˆå®é™…éœ€æ±‚ï¼›äºŒæ˜¯æ¨ç†ä¸æœç´¢çš„æ¨¡å—åŒ–è®¾è®¡é™åˆ¶äº†å¤–éƒ¨çŸ¥è¯†ä¸æ¨ç†é“¾çš„æ·±åº¦äº¤äº’ï¼Œæ˜“è®©æ¨¡å‹åŸºäºå±€éƒ¨ä¿¡æ¯åšå†³ç­–ï¼Œæœ€ç»ˆå½±å“è¾“å‡ºè´¨é‡ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLMåŠ¨æ€æ•´åˆå¤–éƒ¨çŸ¥è¯†ã€å­¦ä¹ æœ€ä¼˜äº¤äº’è½¨è¿¹æˆä¸ºå…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šR - Searchæ¡†æ¶è®¾è®¡â€”â€”å¼ºåŒ–å­¦ä¹ é©±åŠ¨æ¨ç†ä¸æœç´¢æ·±åº¦æ•´åˆ  
æå‡ºR - Searchè¿™ä¸€åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°é¢–æ¡†æ¶ï¼Œè®©LLMèƒ½åŠ¨æ€äº¤é”™æ‰§è¡Œå¤šæ­¥éª¤æ¨ç†ä¸æœç´¢ã€‚LLMå¯åœ¨ä»»æ„tokençº§æ¨ç†æ­¥éª¤è§¦å‘æ£€ç´¢ï¼Œå°†æ£€ç´¢å†…å®¹æ— ç¼èå…¥æ¨ç†è¿‡ç¨‹ï¼Œå®ç°æ¨ç†ä¸å¤–éƒ¨çŸ¥è¯†çš„æ·±åº¦è€¦åˆï¼›äº¤äº’åï¼ŒLLMè¿˜èƒ½é€šè¿‡æ¨ç†æŠŠæ£€ç´¢æ–‡æ¡£æç‚¼ä¸ºè¯æ®ï¼Œä»å…¨å±€è§†è§’é‡æ–°è¯„ä¼°å’Œæ„å»ºå…³é”®çŸ¥è¯†ï¼Œèšç„¦è§£å†³ä»»åŠ¡çš„æ ¸å¿ƒäº‹å®ã€‚è¯¥æ¡†æ¶èƒ½å¼•å¯¼LLMä¿éšœä¸­é—´æ¨ç†çš„åˆç†æ€§ä¸æ£€ç´¢çŸ¥è¯†çš„å®Œæ•´æ€§ï¼Œè”åˆä¼˜åŒ–RAGä¸­å¤æ‚çš„â€œæ¨ç† - æœç´¢â€è½¨è¿¹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé˜¶æ®µå¤šç±»å‹å¥–åŠ±æœºåˆ¶â€”â€”å¼•å¯¼å­¦ä¹ æœ€ä¼˜äº¤äº’åºåˆ—  
è®¾è®¡äº†åŒ…å«ç­”æ¡ˆè´¨é‡ã€è¯æ®è´¨é‡ã€æ ¼å¼æ­£ç¡®æ€§ç­‰çš„å¤šé˜¶æ®µã€å¤šç±»å‹å¥–åŠ±æœºåˆ¶ã€‚è¿™äº›äº’è¡¥çš„å¥–åŠ±ä¿¡å·æ¨åŠ¨æ¨¡å‹å­¦ä¹ æœ€ä¼˜â€œæ¨ç† - æœç´¢â€äº¤äº’åºåˆ—ï¼Œå…¶ä¸­è¯æ®å¥–åŠ±èƒ½ä¿ƒä½¿æ¨¡å‹å…³æ³¨å…³é”®ä¸­é—´æ¨ç†æ­¥éª¤çš„äº‹å®è´¨é‡ï¼ŒåŠ©åŠ›æ„å»ºæ›´ç¨³å¥çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘èµ°æ·å¾„æˆ–è‡†æµ‹è¡Œä¸ºçš„é£é™©ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šR - Search - as - a - Toolï¼ˆRSToolï¼‰â€”â€”æ¨¡å—åŒ–ä¸å®ç”¨æ‹“å±•  
æå‡ºRSToolï¼Œå°†æ¨ç†ä¸­é«˜è´¨é‡è¯æ®æ¨¡å—åŒ–å°è£…ä¸ºå¯è¿ç§»ç»„ä»¶ï¼ŒæŠŠå¤æ‚ä¸”è€—æ—¶çš„â€œæ¨ç† - æœç´¢â€äº¤äº’å¸è½½åˆ°æœ¬åœ°éƒ¨ç½²ï¼Œå…·å¤‡å¾ˆå¼ºçš„å®é™…å¯æ‰©å±•æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨7ä¸ªæ¶µç›–å¤šè·³å’Œå•è·³é—®ç­”ä»»åŠ¡çš„æ•°æ®é›†ä¸Šå¼€å±•å®éªŒï¼Œç»“æœæ˜¾ç¤ºR - Searchåœ¨åŸŸå†…ï¼ˆin - domainï¼‰æ¯”å…ˆè¿›RAGåŸºçº¿æœ€å¤šé¢†å…ˆ32.2%ï¼Œåœ¨åŸŸå¤–ï¼ˆout - of - domainï¼‰æœ€å¤šé¢†å…ˆ25.1%ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒå’Œè®­ç»ƒåŠ¨æ€åˆ†æç­‰è¿›ä¸€æ­¥éªŒè¯äº†è¯æ®æ•´åˆä¸å¤šå¥–åŠ±å»ºæ¨¡çš„æœ‰æ•ˆæ€§ï¼Œè¿˜æ­ç¤ºäº†ä¸åŒRLç®—æ³•ä¸‹çš„æ€§èƒ½è¶‹åŠ¿ä¸æ£€ç´¢è¡Œä¸ºç­‰æ´å¯Ÿã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥RAGåœºæ™¯æ¥ä¼˜åŒ–â€œæ¨ç† - æœç´¢â€è½¨è¿¹ï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡ä¸­æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†æ·±åº¦äº¤äº’é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯ç¤ºåœ¨éœ€åŠ¨æ€äº¤äº’çš„ä»»åŠ¡ä¸­å¯æ¢ç´¢RLé©±åŠ¨çš„æ¡†æ¶è®¾è®¡ã€‚  
2. å¥–åŠ±æœºåˆ¶æ„å»ºï¼šå¤šç»´åº¦ã€å¤šé˜¶æ®µçš„å¥–åŠ±è®¾è®¡æ€è·¯ï¼Œèƒ½ä¸ºå¼ºåŒ–å­¦ä¹ ä¸­å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ æä¾›æ›´å…¨é¢çš„ä¿¡å·å‚è€ƒï¼Œå¯å€Ÿé‰´åˆ°éœ€å¤šç›®æ ‡ä¼˜åŒ–çš„ä»»åŠ¡åœºæ™¯ã€‚  
3. å·¥å…·åŒ–è½åœ°ï¼šRSToolçš„æ¨¡å—åŒ–æ€è·¯ï¼Œä¸ºå¤æ‚AIèƒ½åŠ›å‘å®ç”¨å·¥å…·è½¬åŒ–ã€é™ä½éƒ¨ç½²æˆæœ¬æä¾›äº†å‚è€ƒï¼Œåˆ©äºæ¨åŠ¨æŠ€æœ¯åœ¨å®é™…åœºæ™¯çš„è½åœ°åº”ç”¨ã€‚  
```

## enhancing-retrieval-augmented-large-language-models-with-iterative-retrieval-generation-synergy
### Abstract
Large language models are powerful text processors and reasoners, but are
still subject to limitations including outdated knowledge and hallucinations,
which necessitates connecting them to the world. Retrieval-augmented large
language models have raised extensive attention for grounding model generation
on external knowledge. However, retrievers struggle to capture relevance,
especially for queries with complex information needs. Recent work has proposed
to improve relevance modeling by having large language models actively involved
in retrieval, i.e., to improve retrieval with generation. In this paper, we
show that strong performance can be achieved by a method we call Iter-RetGen,
which synergizes retrieval and generation in an iterative manner. A model
output shows what might be needed to finish a task, and thus provides an
informative context for retrieving more relevant knowledge which in turn helps
generate a better output in the next iteration. Compared with recent work which
interleaves retrieval with generation when producing an output, Iter-RetGen
processes all retrieved knowledge as a whole and largely preserves the
flexibility in generation without structural constraints. We evaluate
Iter-RetGen on multi-hop question answering, fact verification, and commonsense
reasoning, and show that it can flexibly leverage parametric knowledge and
non-parametric knowledge, and is superior to or competitive with
state-of-the-art retrieval-augmented baselines while causing fewer overheads of
retrieval and generation. We can further improve performance via
generation-augmented retrieval adaptation.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¿­ä»£æ£€ç´¢-ç”ŸæˆååŒï¼Œå¢å¼ºæ£€ç´¢å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½å¼ºå¤§ä½†å­˜åœ¨çŸ¥è¯†è¿‡æ—¶ã€æ˜“äº§ç”Ÿå¹»è§‰ç­‰å±€é™ï¼Œæ£€ç´¢å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ä¸ºå°†æ¨¡å‹ç”Ÿæˆé”šå®šå¤–éƒ¨çŸ¥è¯†æä¾›äº†æ€è·¯ã€‚ç„¶è€Œä¼ ç»Ÿæ£€ç´¢å¢å¼ºæ–¹æ³•å­˜åœ¨ä¸è¶³ï¼šä¸€æ¬¡æ€§æ£€ç´¢éš¾ä»¥æ»¡è¶³å¤æ‚ä¿¡æ¯éœ€æ±‚ä»»åŠ¡ï¼›è¿‘æœŸå°†æ£€ç´¢ä¸ç”Ÿæˆäº¤é”™çš„æ–¹æ³•ï¼Œå­˜åœ¨æ— æ³•æ•´ä½“å¤„ç†æ£€ç´¢çŸ¥è¯†ã€å¢åŠ æ£€ç´¢å’Œç”Ÿæˆå¼€é”€ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡è¿­ä»£å¼çš„æ£€ç´¢ - ç”ŸæˆååŒï¼ˆIter - RetGenï¼‰æ¥å¢å¼ºæ£€ç´¢å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºIter - RetGenæ–¹æ³•ï¼Œå®ç°è¿­ä»£å¼æ£€ç´¢ - ç”ŸæˆååŒ
Iter - RetGenè¿­ä»£æ‰§è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆå’Œç”Ÿæˆå¢å¼ºæ£€ç´¢ã€‚é¦–å…ˆåŸºäºä»»åŠ¡è¾“å…¥ï¼ˆåˆå§‹ç”¨ä»»åŠ¡è¾“å…¥ä½œæŸ¥è¯¢ï¼‰è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆå¾—åˆ°è¾“å‡ºï¼Œè¯¥è¾“å‡ºå±•ç¤ºäº†å®Œæˆä»»åŠ¡æ‰€éœ€ä¿¡æ¯ï¼Œå¯ä½œä¸ºç”Ÿæˆå¢å¼ºæ£€ç´¢çš„ä¸Šä¸‹æ–‡æ¥è·å–æ›´ç›¸å…³çŸ¥è¯†ï¼Œæ–°æ£€ç´¢çŸ¥è¯†åˆèƒ½åŠ©åŠ›ä¸‹ä¸€è½®æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæ•´ä½“å¤„ç†æ£€ç´¢åˆ°çš„çŸ¥è¯†ä¸”ä¿ç•™ç”Ÿæˆçµæ´»æ€§ï¼Œæ— ç»“æ„çº¦æŸã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç”Ÿæˆå¢å¼ºçš„æ£€ç´¢é€‚é…
åˆ©ç”¨æ¨¡å‹ç”Ÿæˆæ¥é€‚é…æ£€ç´¢ï¼Œä»èƒ½è®¿é—®æ¨¡å‹ç”Ÿæˆçš„é‡æ’åºå™¨ä¸­æå–çŸ¥è¯†åˆ°ä»…èƒ½è®¿é—®ä»»åŠ¡è¾“å…¥çš„å¯†é›†æ£€ç´¢å™¨ï¼Œåœ¨ç”¨æˆ·è¾“å…¥æ˜“æ”¶é›†ä½†ç›¸å…³çŸ¥è¯†æˆ–ç†æƒ³è¾“å‡ºæ— æ ‡æ³¨åœºæ™¯ä¸‹æœ‰ç›Šï¼Œå¯è¿›ä¸€æ­¥æå‡æ€§èƒ½å¹¶å‡å°‘è¿­ä»£æ¬¡æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šè·³é—®ç­”ã€äº‹å®éªŒè¯å’Œå¸¸è¯†æ¨ç†ä¸‰é¡¹ä»»åŠ¡ä¸Šè¯„ä¼°Iter - RetGenã€‚åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹è®©å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¨ç†é“¾å’Œæœ€ç»ˆç­”æ¡ˆï¼Œç»“æœæ˜¾ç¤ºï¼šåœ¨å…­ä¸ªæ•°æ®é›†ä¸­å››ä¸ªæ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒä¹‹å‰æœ€å…ˆè¿›çš„æ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œæœ€å¤šå®ç°8.6%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œå¦å¤–ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å…·ç«äº‰åŠ›ï¼›ç”Ÿæˆé€šå¸¸ä»æ›´å¤šè¿­ä»£ä¸­å—ç›Šï¼Œä¸¤æ¬¡è¿­ä»£æ€§èƒ½æå‡æœ€æ˜æ˜¾ï¼Œå¯é€šè¿‡é€‰æ‹©åˆé€‚è¿­ä»£æ¬¡æ•°å®šåˆ¶æ€§èƒ½ - æˆæœ¬æƒè¡¡ï¼›ç»ç”Ÿæˆå¢å¼ºçš„æ£€ç´¢é€‚é…åèƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚åŒæ—¶å‘ç°ç²¾ç¡®åŒ¹é…ç­‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯èƒ½ä½ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œç”¨å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ›´å¯é ï¼›Iter - RetGenåœ¨é—®ç­”ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºSelf - Askï¼Œæ— è®ºä¸Šä¸‹æ–‡éå‚æ•°çŸ¥è¯†æ˜¯å¦æåŠç­”æ¡ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ–¹æ³•è®¾è®¡ä¸Šï¼ŒIter - RetGençš„è¿­ä»£ååŒæ€è·¯ä¸ºå¤„ç†å¤æ‚ä¿¡æ¯éœ€æ±‚ä»»åŠ¡æä¾›äº†æ›´ä¼˜èŒƒå¼ï¼Œé¿å…ä¼ ç»Ÿäº¤é”™æ–¹æ³•çš„å±€é™ï¼Œç®€åŒ–æµç¨‹ä¸”å‡å°‘æ£€ç´¢ç”Ÿæˆå¼€é”€ï¼›è¯„ä¼°è§†è§’ä¸Šï¼Œæé†’ç ”ç©¶è€…å…³æ³¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯¹å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„ä¸è¶³ï¼Œè€ƒè™‘ç”¨å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ›´å¯é ï¼›æŠ€æœ¯æ‹“å±•ä¸Šï¼Œç”Ÿæˆå¢å¼ºçš„æ£€ç´¢é€‚é…ä¸ºåœ¨æ•°æ®æ ‡æ³¨ä¸è¶³åœºæ™¯ä¸‹ä¼˜åŒ–æ£€ç´¢æä¾›äº†æ–°é€”å¾„ï¼Œå¯å€Ÿé‰´è¯¥æ€è·¯æ‹“å±•æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†äº¤äº’çš„æ–¹å¼ï¼Œæå‡æ¨¡å‹åˆ©ç”¨å‚æ•°çŸ¥è¯†å’Œéå‚æ•°çŸ¥è¯†çš„èƒ½åŠ›ã€‚
```

## search-and-refine-during-think--autonomous-retrieval-augmented-reasoning-of-llms
### Abstract
Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤§æ¨¡å‹è¾¹â€œæ€è€ƒ-æœç´¢-ç²¾ç‚¼â€è¾¹æ¨ç†ï¼šAutoRefine é©æ–°æ£€ç´¢å¢å¼ºèŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§èƒ½åŠ›ï¼Œä½†å—é™äºè®­ç»ƒè¯­æ–™çš„çŸ¥è¯†å‚¨å¤‡ï¼Œåœ¨éœ€è¦å®æ—¶æˆ–ç²¾å‡†çŸ¥è¯†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡è®©LLMè°ƒç”¨å¤–éƒ¨å·¥å…·æŸ¥è¯¢çŸ¥è¯†åº“æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤å¤§æ ¸å¿ƒç¼ºé™·ï¼š  
1. **ç¼ºä¹å¯¹æ£€ç´¢æ–‡æ¡£çš„ç²¾ç‚¼**ï¼šä¼ ç»Ÿâ€œæ€è€ƒæ—¶æœç´¢ï¼ˆsearch-during-thinkï¼‰â€èŒƒå¼ç›´æ¥åŸºäºæ£€ç´¢åˆ°çš„ï¼ˆå¯èƒ½åŒ…å«å™ªå£°ã€æ— å…³ä¿¡æ¯çš„ï¼‰æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼Œæ²¡æœ‰å…ˆæç‚¼å…³é”®ä¿¡æ¯ï¼Œé™åˆ¶äº†æ¨¡å‹è¯†åˆ«ç¼ºå¤±çŸ¥è¯†ã€åŸºäºä¸å®Œæ•´è¯æ®æ¨ç†æˆ–è¿­ä»£ä¼˜åŒ–æ£€ç´¢çš„èƒ½åŠ›ï¼›  
2. **æ£€ç´¢ä¸“å±å¥–åŠ±æœªå……åˆ†æ¢ç´¢**ï¼šå¤šæ•°æ£€ç´¢å¢å¼ºæ¨ç†æ–¹æ³•ä»…ä¾èµ–â€œç»“æœå¯¼å‘â€å¥–åŠ±ï¼ˆå¦‚æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼‰ï¼Œå¯¹â€œå¦‚ä½•æå‡æ£€ç´¢è¿‡ç¨‹æœ¬èº«â€ç¼ºä¹ç›´æ¥æŒ‡å¯¼ï¼Œå¯¼è‡´æ¨¡å‹éš¾å­¦å¦‚ä½•è·å–æ›´ç›¸å…³ã€æ›´æœ‰ä¿¡æ¯é‡çš„æ–‡æ¡£ã€‚  

ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡º **AutoRefine** â€”â€” ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæ¡†æ¶ï¼Œé‡å¡‘æ£€ç´¢å¢å¼ºæ¨ç†èŒƒå¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šâ€œæ€è€ƒæ—¶æœç´¢+ç²¾ç‚¼ï¼ˆsearch-and-refine-during-thinkï¼‰â€æ–°èŒƒå¼  
ä¼ ç»ŸRAGæ˜¯â€œæ€è€ƒâ†’æœç´¢â†’ç›´æ¥ç”Ÿæˆç­”æ¡ˆâ€ï¼ŒAutoRefineåˆ™åœ¨â€œæœç´¢â€ä¸â€œç”Ÿæˆç­”æ¡ˆâ€ä¹‹é—´æ’å…¥**æ˜¾å¼çš„çŸ¥è¯†ç²¾ç‚¼æ­¥éª¤**ã€‚å…·ä½“é€šè¿‡ `<search>...</search>[documents]<refine>...</refine>` è¿™æ ·çš„æ¨¡æ¿ï¼Œå¼ºåˆ¶æ¨¡å‹å…ˆå¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œâ€œè¿‡æ»¤å™ªå£°ã€æç‚¼å…³é”®ã€ç»„ç»‡è¯æ®â€çš„è¿­ä»£æ“ä½œï¼Œå†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚è¿™ä¸€æ­¥è®©æ¨¡å‹åœ¨å›ç­”å‰å…ˆâ€œæ¶ˆåŒ–â€æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œé¿å…è¢«æ— å…³å†…å®¹å¹²æ‰°ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèåˆâ€œç»“æœå¥–åŠ±+æ£€ç´¢ä¸“å±å¥–åŠ±â€çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ  
AutoRefineé‡‡ç”¨ **Group Relative Policy Optimizationï¼ˆGRPOï¼‰** ç®—æ³•ï¼ŒåŒæ—¶ä¼˜åŒ–ä¸¤ç§å¥–åŠ±ï¼š  
- **ç»“æœå¯¼å‘å¥–åŠ±**ï¼šè¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼›  
- **æ£€ç´¢ä¸“å±å¥–åŠ±**ï¼šåŸºäº `<refine>` å—ä¸­æç‚¼å†…å®¹çš„è´¨é‡è®¡ç®—ï¼Œç›´æ¥æŒ‡å¯¼â€œå¦‚ä½•æ›´å¥½æ£€ç´¢ä¸åˆ©ç”¨æ–‡æ¡£â€ã€‚  
è®­ç»ƒæ—¶ï¼Œæ¨¡å‹ä¼šç”Ÿæˆå¤šæ¡åŒ…å«â€œæ€è€ƒã€æœç´¢ã€ç²¾ç‚¼ã€å›ç­”â€çš„æ¨ç†è½¨è¿¹ï¼Œå†ç”¨GRPOå¯¹è¿™äº›è½¨è¿¹åšä¼˜åŒ–ï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨æ¨ç†ä¸­æ›´æ™ºèƒ½åœ°è°ƒç”¨æ£€ç´¢å·¥å…·ã€æ›´é«˜æ•ˆåœ°å¤„ç†ä¿¡æ¯ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å•è·³ï¼ˆsingle-hopï¼‰å’Œå¤šè·³ï¼ˆmulti-hopï¼‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­éªŒè¯AutoRefineï¼š  
- **æ€§èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•**ï¼šåœ¨å¤æ‚çš„å¤šè·³æ¨ç†åœºæ™¯ä¸‹æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œè¯æ˜â€œæœç´¢+ç²¾ç‚¼â€èŒƒå¼å¯¹é•¿é“¾æ¡ã€å¤šæ­¥éª¤æ¨ç†çš„æœ‰æ•ˆæ€§ï¼›  
- **æ£€ç´¢è´¨é‡æ›´é«˜é¢‘**ï¼šåˆ†ææ˜¾ç¤ºAutoRefineæ›´é¢‘ç¹å‘èµ·é«˜è´¨é‡æœç´¢ï¼Œä¸”èƒ½æœ‰æ•ˆæ•´åˆã€åˆæˆè¯æ®ï¼Œå‡å°‘å™ªå£°å¹²æ‰°ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **èŒƒå¼åˆ›æ–°**ï¼šâ€œæœç´¢åç²¾ç‚¼å†å›ç­”â€çš„æµç¨‹è®¾è®¡ï¼Œä¸ºè§£å†³â€œæ£€ç´¢ä¿¡æ¯å™ªå£°/æ— å…³â€é—®é¢˜æä¾›äº†æ–°è§†è§’ï¼Œå¯å¯å‘åç»­RAGç±»å·¥ä½œä¼˜åŒ–æ¨ç†æµç¨‹ï¼›  
2. **å¥–åŠ±è®¾è®¡**ï¼šå°†â€œè¿‡ç¨‹æ€§å¥–åŠ±ï¼ˆæ£€ç´¢è´¨é‡ï¼‰â€ä¸â€œç»“æœæ€§å¥–åŠ±ï¼ˆç­”æ¡ˆæ­£ç¡®ï¼‰â€ç»“åˆï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¼•å¯¼æ¨¡å‹â€œè¿‡ç¨‹ä¼˜åŒ–â€ä¸Šçš„æ½œåŠ›ï¼Œä¸ºRLä¸LLMç»“åˆçš„å¥–åŠ±æœºåˆ¶è®¾è®¡æä¾›å‚è€ƒï¼›  
3. **å¤šè·³æ¨ç†é€‚é…**ï¼šåœ¨å¤šè·³ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œè¯´æ˜è¯¥æ–¹æ³•å¯¹éœ€è¦å¤šæ¬¡æ£€ç´¢ã€ä¿¡æ¯æ‹¼æ¥çš„å¤æ‚ä»»åŠ¡å‹å¥½ï¼Œå¯è¿ç§»åˆ°çŸ¥è¯†å¯†é›†å‹çš„é•¿æ–‡æœ¬æ¨ç†ã€å¤šæ­¥éª¤å†³ç­–ç­‰åœºæ™¯ã€‚  

æ€»ä¹‹ï¼ŒAutoRefineé€šè¿‡â€œæµç¨‹èŒƒå¼+å¥–åŠ±æœºåˆ¶â€çš„åŒé‡åˆ›æ–°ï¼Œè®©å¤§æ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºæ¨ç†ä¸­æ›´è‡ªä¸»ã€æ›´æ™ºèƒ½ï¼Œä¸ºçªç ´â€œçŸ¥è¯†å‚¨å¤‡é™åˆ¶+æ£€ç´¢ä¿¡æ¯å™ªå£°â€ä¸¤å¤§ç—›ç‚¹æä¾›äº†ä¸€å¥—ç®€æ´è€Œæœ‰åŠ›çš„æ–¹æ¡ˆ~
```

## smartrag--jointly-learn-rag-related-tasks-from-the-environment-feedback
### Abstract
RAG systems consist of multiple modules to work together. However, these
modules are usually separately trained. We argue that a system like RAG that
incorporates multiple modules should be jointly optimized to achieve optimal
performance. To demonstrate this, we design a specific pipeline called
\textbf{SmartRAG} that includes a policy network and a retriever. The policy
network can serve as 1) a decision maker that decides when to retrieve, 2) a
query rewriter to generate a query most suited to the retriever, and 3) an
answer generator that produces the final response with/without the
observations. We then propose to jointly optimize the whole system using a
reinforcement learning algorithm, with the reward designed to encourage the
system to achieve the best performance with minimal retrieval cost. When
jointly optimized, all the modules can be aware of how other modules are
working and thus find the best way to work together as a complete system.
Empirical results demonstrate that the jointly optimized SmartRAG can achieve
better performance than separately optimized counterparts.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SmartRAGï¼šç”¨ç¯å¢ƒåé¦ˆç«¯åˆ°ç«¯ä¼˜åŒ–RAGç³»ç»Ÿï¼Œçªç ´æ¨¡å—åˆ†ç¦»è®­ç»ƒç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¸å¤šé¢†åŸŸå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å¤„ç†æ¨¡å‹å‚æ•°ä¹‹å¤–çš„çŸ¥è¯†ç±»é—®é¢˜ä»é¢‡å…·æŒ‘æˆ˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä»å¤–éƒ¨å·¥å…·æ£€ç´¢ä¿¡æ¯æœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨è¿™ç±»åœºæ™¯çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRAGç³»ç»Ÿçš„å¤šä¸ªæ¨¡å—ï¼ˆå¦‚æ£€ç´¢å™¨ã€å†³ç­–å™¨ã€æŸ¥è¯¢é‡å†™å™¨ç­‰ï¼‰å¾€å¾€æ˜¯**åˆ†ç¦»è®­ç»ƒ**çš„ã€‚è¿™å¸¦æ¥ä¸¤å¤§é—®é¢˜ï¼šä¸€æ˜¯ä¸­é—´æ¨¡å—çš„â€œé»„é‡‘ç­”æ¡ˆâ€ï¼ˆæœ€ä¼˜è¾“å‡ºï¼‰é€šå¸¸éš¾ä»¥è·å–ï¼Œç”šè‡³ä¾èµ–ç‰¹å®šæ¨¡å‹æˆ–æ£€ç´¢å™¨ï¼›äºŒæ˜¯åˆ†ç¦»ä¼˜åŒ–ä¼šå¯¼è‡´å„æ¨¡å—ç¼ºä¹å¯¹æ•´ä½“ç³»ç»Ÿçš„ååŒæ„ŸçŸ¥ï¼Œéš¾ä»¥è¾¾åˆ°å…¨å±€æœ€ä¼˜æ€§èƒ½ã€‚å› æ­¤ï¼Œè®ºæ–‡æå‡ºè¦å¯¹RAGè¿™ç±»å¤šæ¨¡å—ç³»ç»Ÿè¿›è¡Œ**ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–**ï¼Œè®©å„æ¨¡å—åœ¨åä½œä¸­ç›¸äº’é€‚é…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡SmartRAGç³»ç»Ÿæ¶æ„ï¼Œè®©Policy Networkèº«å…¼ä¸‰èŒ  
SmartRAGçš„æ ¸å¿ƒç”±**ç­–ç•¥ç½‘ç»œï¼ˆPolicy Networkï¼‰**å’Œ**æ£€ç´¢å™¨ï¼ˆRetrieverï¼‰**æ„æˆã€‚å…¶ä¸­ï¼Œç­–ç•¥ç½‘ç»œæ‰¿æ‹…ä¸‰ä¸ªå…³é”®è§’è‰²ï¼š  
- å†³ç­–å™¨ï¼ˆDecision Makerï¼‰ï¼šæ ¹æ®è¾“å…¥é—®é¢˜ä¸å·²æœ‰è§‚æµ‹ï¼Œå†³å®šæ˜¯å¦éœ€è¦å‘èµ·æ£€ç´¢ï¼›  
- æŸ¥è¯¢é‡å†™å™¨ï¼ˆQuery Rewriterï¼‰ï¼šè‹¥å†³å®šæ£€ç´¢ï¼Œç”Ÿæˆæ›´é€‚é…æ£€ç´¢å™¨çš„æŸ¥è¯¢è¯­å¥ï¼›  
- ç­”æ¡ˆç”Ÿæˆå™¨ï¼ˆAnswer Generatorï¼‰ï¼šè‹¥è®¤ä¸ºå½“å‰ä¿¡æ¯è¶³å¤Ÿï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚  
è¿™ç§â€œä¸€ä¸“å¤šèƒ½â€çš„è®¾è®¡è®©å•ä¸ªç­–ç•¥ç½‘ç»œä¸²è”èµ·RAGçš„æ ¸å¿ƒæµç¨‹ï¼Œä¸ºåç»­ç«¯åˆ°ç«¯ä¼˜åŒ–æ‰“ä¸‹åŸºç¡€ã€‚  

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–  
è®ºæ–‡é‡‡ç”¨**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰**ç®—æ³•å¯¹SmartRAGè¿›è¡Œè”åˆè®­ç»ƒï¼Œç”¨**ç¯å¢ƒåé¦ˆ**æ›¿ä»£ä¼ ç»Ÿâ€œé»„é‡‘ç­”æ¡ˆâ€ä½œä¸ºç›‘ç£ä¿¡å·ã€‚è®¾è®¡å¥–åŠ±å‡½æ•°æ—¶ï¼Œå…¼é¡¾ä¸¤å¤§ç›®æ ‡ï¼šä¸€æ˜¯â€œæ­£ç¡®å›ç­”é—®é¢˜â€ï¼ŒäºŒæ˜¯â€œæœ€å°åŒ–æ£€ç´¢æˆæœ¬â€ï¼ˆå‡å°‘ä¸å¿…è¦çš„æ£€ç´¢æ¬¡æ•°ï¼‰ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œç­–ç•¥ç½‘ç»œèƒ½åœ¨ä¸ç¯å¢ƒï¼ˆå¦‚å¤–éƒ¨çŸ¥è¯†åº“ã€æ£€ç´¢å·¥å…·ï¼‰çš„äº¤äº’ä¸­ï¼Œå­¦ä¹ åˆ°å„æ¨¡å—é—´çš„æœ€ä¼˜åä½œæ–¹å¼â€”â€”ä½•æ—¶æ£€ç´¢ã€æ£€ç´¢ä»€ä¹ˆã€å¦‚ä½•å›ç­”ï¼Œè®©æ•´ä¸ªç³»ç»Ÿå½¢æˆæœ‰æœºæ•´ä½“ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†SmartRAGçš„æœ‰æ•ˆæ€§ï¼Œæ ¸å¿ƒç»“è®ºæ˜¯ï¼š**è”åˆä¼˜åŒ–çš„SmartRAGæ˜¾è‘—ä¼˜äºæ¨¡å—åˆ†ç¦»è®­ç»ƒçš„åŸºçº¿ç³»ç»Ÿ**ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åˆ†æï¼Œè®ºæ–‡è¿˜å±•ç¤ºäº†SmartRAGå¦‚ä½•å­¦ä¹ â€œä½•æ—¶æ£€ç´¢ã€æ£€ç´¢ä»€ä¹ˆã€å¦‚ä½•å›ç­”â€è¿™ä¸‰é¡¹å…³é”®èƒ½åŠ›ï¼Œè¯æ˜äº†æ¨¡å—é—´ååŒæ„ŸçŸ¥å¯¹ç³»ç»Ÿæ€§èƒ½çš„æå‡ä½œç”¨ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ¶æ„è®¾è®¡æ€è·¯**ï¼šå°†RAGå¤šæ¨¡å—åŠŸèƒ½æ”¶æ•›åˆ°â€œç­–ç•¥ç½‘ç»œ+æ£€ç´¢å™¨â€çš„ç®€æ´æ¶æ„ï¼Œç”¨å•ä¸€ç½‘ç»œæ‰¿è½½å†³ç­–ã€é‡å†™ã€ç”Ÿæˆï¼Œä¸ºå¤æ‚ç³»ç»Ÿçš„æ¨¡å—åŒ–æ•´åˆæä¾›äº†å‚è€ƒï¼›  
2. **ä¼˜åŒ–èŒƒå¼åˆ›æ–°**ï¼šç”¨å¼ºåŒ–å­¦ä¹ åšç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼Œæ‘†è„±å¯¹â€œé»„é‡‘ç­”æ¡ˆâ€çš„å¼ºä¾èµ–ï¼Œæ›´è´´åˆçœŸå®åœºæ™¯ä¸­æ¨¡å—è¾“å‡ºéš¾æ ‡æ³¨çš„ç—›ç‚¹ï¼›  
3. **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šå¹³è¡¡â€œä»»åŠ¡æ•ˆæœï¼ˆå›ç­”æ­£ç¡®ç‡ï¼‰â€ä¸â€œèµ„æºæˆæœ¬ï¼ˆæ£€ç´¢æ¬¡æ•°ï¼‰â€ï¼Œè¿™ç§å¤šç›®æ ‡æƒè¡¡çš„æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€èµ„æºçº¦æŸçš„AIç³»ç»Ÿè®¾è®¡ä¸­ã€‚  

æ€»ä¹‹ï¼ŒSmartRAGä¸ºRAGç³»ç»Ÿçš„å·¥ç¨‹åŒ–è½åœ°æä¾›äº†â€œååŒä¼˜åŒ–â€çš„æ–°èŒƒå¼ï¼Œè®©å¤šæ¨¡å—ç³»ç»Ÿä»â€œå„è‡ªä¸ºæˆ˜â€èµ°å‘â€œå…¨å±€æœ€ä¼˜â€ï¼Œå€¼å¾—ä»äº‹æ£€ç´¢å¢å¼ºã€å¤§æ¨¡å‹åº”ç”¨çš„ç ”ç©¶è€…ä¸å·¥ç¨‹å¸ˆæ·±å…¥å‚è€ƒ~
```

## neuro-symbolic-query-compiler
### Abstract
Precise recognition of search intent in Retrieval-Augmented Generation (RAG)
systems remains a challenging goal, especially under resource constraints and
for complex queries with nested structures and dependencies. This paper
presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar
rules and compiler design, to bridge this gap. It theoretically designs a
minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize
complex queries. Unlike previous methods, this grammar maintains completeness
while minimizing redundancy. Based on this, QCompiler includes a Query
Expression Translator, a Lexical Syntax Parser, and a Recursive Descent
Processor to compile queries into Abstract Syntax Trees (ASTs) for execution.
The atomicity of the sub-queries in the leaf nodes ensures more precise
document retrieval and response generation, significantly improving the RAG
system's ability to address complex queries.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç¥ç»ç¬¦å·æŸ¥è¯¢ç¼–è¯‘å™¨ï¼šç²¾å‡†ç ´è§£RAGå¤æ‚æŸ¥è¯¢éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ç²¾å‡†è¯†åˆ«æœç´¢æ„å›¾ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶é¢å¯¹èµ„æºçº¦æŸä»¥åŠå¸¦æœ‰åµŒå¥—ç»“æ„å’Œä¾èµ–å…³ç³»çš„å¤æ‚æŸ¥è¯¢æ—¶ã€‚ä¼ ç»ŸRAGç³»ç»Ÿå¤„ç†å¤æ‚æŸ¥è¯¢æ—¶ï¼Œä¸€æ¬¡æ€§æ£€ç´¢åˆ°æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„æ¦‚ç‡å¤§å¹…é™ä½ï¼Œæ€§èƒ½å—é™ã€‚åŒæ—¶ï¼Œäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰è™½æ‹Ÿåˆèƒ½åŠ›å¼ºï¼Œä½†åœ¨éœ€æ˜¾å¼ç¬¦å·æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼›ç°æœ‰å¤„ç†å¤æ‚æŸ¥è¯¢çš„æ–¹æ³•è¦ä¹ˆå­˜åœ¨å†—ä½™ã€èµ„æºæµªè´¹é—®é¢˜ï¼Œè¦ä¹ˆè¿‡åº¦ä¾èµ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ï¼Œåœ¨æ€§èƒ½ä¸æ•ˆç‡é—´éš¾ä»¥å¹³è¡¡ã€‚æ­¤å¤–ï¼Œå¦‚ä½•å¤åˆ»äººç±»å¤§è„‘ä¸­ç¥ç»è®¡ç®—ä¸ç¬¦å·æ¨ç†çš„ååŒæœºåˆ¶æ¥å¤„ç†å¤æ‚æŸ¥è¯¢ï¼Œä¹Ÿæ˜¯å¾…è§£éš¾é¢˜ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºQCompileræ¡†æ¶æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡æç®€ä¸”å……åˆ†çš„BNFè¯­æ³•G[q]  
ä»ç†è®ºå±‚é¢è®¾è®¡äº†æœ€å°å´è¶³å¤Ÿç”¨çš„Backus - Naur Formï¼ˆBNFï¼‰è¯­æ³•G[q]æ¥å½¢å¼åŒ–å¤æ‚æŸ¥è¯¢ã€‚è¯¥è¯­æ³•ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œåœ¨æœ€å°åŒ–å†—ä½™çš„åŒæ—¶ä¿æŒå®Œæ•´æ€§ï¼Œä¸ºç²¾å‡†è¯†åˆ«æœç´¢æ„å›¾å¥ å®šåŸºç¡€ï¼Œèƒ½å°†å¤æ‚æŸ¥è¯¢çº³å…¥è§„èŒƒçš„è¯­æ³•æ¡†æ¶å†…å¤„ç†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºç¥ç»ç¬¦å·æ¡†æ¶QCompiler  
QCompilerå—è¯­è¨€å­¦è¯­æ³•è§„åˆ™å’Œç¼–è¯‘å™¨è®¾è®¡å¯å‘ï¼Œèåˆç¥ç»è®¡ç®—ä¸ç¬¦å·æ¨ç†ã€‚å®ƒåŒ…å«æŸ¥è¯¢è¡¨è¾¾å¼ç¿»è¯‘å™¨ã€è¯æ³•è¯­æ³•è§£æå™¨å’Œé€’å½’ä¸‹é™å¤„ç†å™¨ï¼Œèƒ½å°†æŸ¥è¯¢ç¼–è¯‘æˆæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰æ‰§è¡Œã€‚å…¶ä¸­ï¼ŒæŸ¥è¯¢è¡¨è¾¾å¼ç¿»è¯‘å™¨å¯å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æˆåŸºäºBNFçš„è¡¨è¾¾å¼ï¼›è¯æ³•è¯­æ³•è§£æå™¨è´Ÿè´£è§£ææˆASTï¼›é€’å½’ä¸‹é™å¤„ç†å™¨å€ŸåŠ©è¯­æ³•è§„åˆ™åœ¨èŠ‚ç‚¹é€’å½’æ¨ç†ã€‚ASTèƒ½æ•æ‰åŸå§‹å¤æ‚æŸ¥è¯¢çš„éšå¼æœç´¢æ„å›¾ã€åµŒå¥—ç»“æ„å’Œä¾èµ–å…³ç³»ï¼Œå¶èŠ‚ç‚¹å­æŸ¥è¯¢çš„åŸå­æ€§ä¿éšœäº†æ–‡æ¡£æ£€ç´¢å’Œå“åº”ç”Ÿæˆæ›´ç²¾å‡†ï¼Œæå‡RAGå¤„ç†å¤æ‚æŸ¥è¯¢èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å››ä¸ªå¤šè·³åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†QCompilerçš„æ•ˆæœï¼Œå±•ç°å‡ºå¯¹å¤æ‚æŸ¥è¯¢å‡ºè‰²çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å°†å¤æ‚æŸ¥è¯¢ç¼–è¯‘ä¸ºASTç»“æ„ï¼Œèƒ½ä»¥æ›´å°‘ä½†æ›´ç²¾å‡†çš„æ–‡æ¡£æ£€ç´¢ï¼Œæå‡å“åº”å‡†ç¡®æ€§ï¼Œåœ¨å¤„ç†åµŒå¥—ç»“æ„ä¸ä¾èµ–å…³ç³»çš„å¤æ‚æŸ¥è¯¢åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†è¿™ç§æ ‘ç»“æ„è¡¨ç¤ºå¤æ‚æŸ¥è¯¢çš„å‡†ç¡®æ€§ï¼Œä»¥åŠå¯¹RAGç³»ç»Ÿæ•ˆç‡å’Œç²¾åº¦çš„æå‡ä½œç”¨ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¯­æ³•è®¾è®¡æ€è·¯ï¼šåœ¨å¤„ç†éœ€å½¢å¼åŒ–è¡¨ç¤ºçš„ä»»åŠ¡æ—¶ï¼Œå¯å€Ÿé‰´è¿™ç§â€œæç®€ä¸”å……åˆ†â€çš„è¯­æ³•è®¾è®¡ç†å¿µï¼Œå¹³è¡¡è¡¨è¾¾èƒ½åŠ›ä¸å†—ä½™åº¦ï¼Œä¸ºåç»­å¤„ç†å»ºç«‹é«˜æ•ˆè§„èŒƒçš„åŸºç¡€ã€‚  
2. ç¥ç» - ç¬¦å·èåˆï¼šå°†ç¥ç»ç½‘ç»œçš„æ„ŸçŸ¥å¤„ç†èƒ½åŠ›ä¸ç¬¦å·ç³»ç»Ÿçš„è§„åˆ™æ¨ç†èƒ½åŠ›ç»“åˆï¼Œä¸ºè§£å†³éœ€å¤æ‚æ¨ç†ã€ç»“æ„è§£æçš„AIä»»åŠ¡æä¾›äº†èŒƒå¼å‚è€ƒï¼Œæ¯”å¦‚åœ¨æ™ºèƒ½é—®ç­”ã€å¤æ‚æŒ‡ä»¤ç†è§£ç­‰åœºæ™¯å¯å°è¯•ç±»ä¼¼èåˆæ€è·¯ã€‚  
3. å·¥ç¨‹å®ç”¨æ€§ï¼šQCompilerå…è®¸ç”Ÿäº§ç¯å¢ƒå¼€å‘è€…éªŒè¯å­æŸ¥è¯¢èŠ‚ç‚¹æ­£ç¡®æ€§å¹¶å¹²é¢„æ¨ç†è¿‡ç¨‹ï¼Œè¿™ç§å¯è§£é‡Šã€å¯å¹²é¢„çš„è®¾è®¡æ€è·¯ï¼Œå¯¹æ‰“é€ å¯é çš„å·¥ä¸šçº§AIç³»ç»Ÿå¾ˆæœ‰å€Ÿé‰´æ„ä¹‰ï¼Œèƒ½æå‡ç³»ç»Ÿåœ¨å®é™…éƒ¨ç½²ä¸­çš„ç¨³å®šæ€§ä¸å¯ç»´æŠ¤æ€§ã€‚  
```

## interleaved-reasoning-for-large-language-models-via-reinforcement-learning
### Abstract
Long chain-of-thought (CoT) significantly enhances large language models'
(LLM) reasoning capabilities. However, the extensive reasoning traces lead to
inefficiencies and an increased time-to-first-token (TTFT). We propose a novel
training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs
to interleave thinking and answering for multi-hop questions. We observe that
models inherently possess the ability to perform interleaved reasoning, which
can be further enhanced through RL. We introduce a simple yet effective
rule-based reward to incentivize correct intermediate steps, which guides the
policy model toward correct reasoning paths by leveraging intermediate signals
generated during interleaved reasoning. Extensive experiments conducted across
five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)
demonstrate consistent improvements over traditional think-answer reasoning,
without requiring external tools. Specifically, our approach reduces TTFT by
over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,
our method, trained solely on question answering and logical reasoning
datasets, exhibits strong generalization ability to complex reasoning datasets
such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to
reveal several valuable insights into conditional reward modeling.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ é©±åŠ¨å¤§æ¨¡å‹çš„äº¤é”™æ¨ç†ï¼šæ•ˆç‡ä¸èƒ½åŠ›åŒæå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å€ŸåŠ©é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½æ˜¾è‘—å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†ä¼ ç»Ÿâ€œæ€è€ƒ - å›ç­”â€èŒƒå¼å­˜åœ¨ä¸¤å¤§å…³é”®ç¼ºé™·ï¼šä¸€æ˜¯ç”Ÿæˆç­”æ¡ˆå‰è¦å®Œæˆå®Œæ•´æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´é¦– token ç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰å¤§å¹…å¢åŠ ï¼Œåœ¨å®æ—¶äº¤äº’åœºæ™¯ï¼ˆå¦‚å¯¹è¯åŠ©æ‰‹ï¼‰ä¸­å½±å“ç”¨æˆ·ä½“éªŒï¼›äºŒæ˜¯å»¶è¿Ÿåˆ°æ¨ç†ç»“æŸæ‰ç”Ÿæˆç­”æ¡ˆï¼Œæ˜“è®©é”™è¯¯ä¸­é—´æ­¥éª¤ä¼ æ’­ï¼Œå¼•å‘æœ€ç»ˆç­”æ¡ˆä¸å‡†ç¡®ä¸æ¨ç†ä½æ•ˆï¼ˆå¦‚è¿‡åº¦/ä¸è¶³æ€è€ƒï¼‰ã€‚åŒæ—¶ï¼Œç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ¨ç†å‹ LLM æ—¶ï¼Œå¸¸æŠŠä¸­é—´æ¨ç†è½¨è¿¹å½“é™„å±å“ï¼Œæœªå……åˆ†åˆ©ç”¨å…¶ä¸­é—´ä¿¡å·è¾…åŠ©è®­ç»ƒä¸äº¤äº’ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©æ¨¡å‹åœ¨æ¨ç†ä¸­äº¤é”™â€œæ€è€ƒâ€ä¸â€œå›ç­”â€ã€åˆ©ç”¨ä¸­é—´ä¿¡å·ä¼˜åŒ–è®­ç»ƒå’Œäº¤äº’ï¼Œæˆä¸ºå¾…è§£éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºäº¤é”™æ¨ç† RL è®­ç»ƒèŒƒå¼  
æå‡ºâ€œäº¤é”™æ¨ç†â€è¿™ä¸€å…¨æ–° RL è®­ç»ƒèŒƒå¼ï¼Œè®© LLM åœ¨å¤šè·³é—®é¢˜æ¨ç†ä¸­äº¤æ›¿è¿›è¡Œæ€è€ƒä¸å›ç­”ï¼Œæ— éœ€å¤–éƒ¨å·¥å…·ã€‚è¯¥èŒƒå¼ä½¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆæœ‰ä¿¡æ¯é‡çš„ä¸­é—´ç­”æ¡ˆï¼Œæ—¢ç»™ç”¨æˆ·åŠæ—¶åé¦ˆï¼ˆé™ä½ TTFTï¼‰ï¼Œåˆä¸ºè‡ªèº«åç»­æ¨ç†æä¾›å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼å‘æ­£ç¡®æœ€ç»ˆç­”æ¡ˆæ¨è¿›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶  
å¼•å…¥ç®€å•æœ‰æ•ˆçš„åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œæ¿€åŠ±æ¨¡å‹ç”Ÿæˆæ­£ç¡®ä¸­é—´æ­¥éª¤ã€‚åˆ©ç”¨äº¤é”™æ¨ç†ä¸­äº§ç”Ÿçš„ä¸­é—´ä¿¡å·ï¼Œä¸ºç­–ç•¥æ¨¡å‹æŒ‡æ˜æ­£ç¡®æ¨ç†è·¯å¾„ï¼Œåœ¨è®­ç»ƒæ—¶æä¾›å¯†é›†ä¸”ä¸€è‡´çš„åé¦ˆï¼Œè§£å†³ä¼ ç»Ÿè®­ç»ƒä¸­ä¸­é—´æ­¥éª¤éš¾ credit assignmentï¼ˆ credit assignment æŒ‡è®­ç»ƒä¸­å¦‚ä½•æŠŠæœ€ç»ˆç»“æœçš„å¥–æƒ©åˆ†é…åˆ°å„ä¸­é—´æ­¥éª¤ï¼‰çš„é—®é¢˜ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ•ˆç‡ä¸å‡†ç¡®ç‡æå‡ï¼šåœ¨ 5 ä¸ªä¸åŒæ•°æ®é›†ä¸Šï¼Œç”¨ PPOã€GRPOã€REINFORCE++ ä¸‰ç§ RL ç®—æ³•å®éªŒï¼Œç›¸æ¯”ä¼ ç»Ÿâ€œæ€è€ƒ - å›ç­”â€æ¨ç†ï¼Œå¹³å‡ TTFT é™ä½è¶… 80%ï¼›Pass@1 å‡†ç¡®ç‡æœ€å¤šæå‡ 19.3%ã€‚  
2. æ³›åŒ–èƒ½åŠ›éªŒè¯ï¼šä»…åœ¨é—®ç­”å’Œé€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ¨¡å‹å¯¹ MATHã€GPQAã€MMLU ç­‰å¤æ‚æ¨ç†æ•°æ®é›†å±•ç°å¼ºæ³›åŒ–èƒ½åŠ›ã€‚  
3. å¥–åŠ±å»ºæ¨¡æ´å¯Ÿï¼šæ·±å…¥åˆ†æ conditional reward modelingï¼Œå¾—åˆ°å…³äºå¥–åŠ±å»ºæ¨¡ã€ç¨³å®š RL è®­ç»ƒå’Œæ¨¡å‹æ¨ç†åŠ¨æ€çš„æœ‰ä»·å€¼è§è§£ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è®­ç»ƒèŒƒå¼åˆ›æ–°ï¼šâ€œäº¤é”™æ¨ç†â€èŒƒå¼ä¸ºä¼˜åŒ–å¤§æ¨¡å‹æ¨ç†æ—¶çš„äº¤äº’æ•ˆç‡ä¸è®­ç»ƒæœ‰æ•ˆæ€§æä¾›æ–°æ€è·¯ï¼Œæ‰“ç ´â€œå…ˆå®Œæ•´æ€è€ƒå†å›ç­”â€çš„å›ºå®šé¡ºåºï¼Œå¯å‘åç»­æ¢ç´¢æ›´çµæ´»çš„æ¨ç†äº¤äº’æ¨¡å¼ã€‚  
2. å¥–åŠ±æœºåˆ¶è®¾è®¡ï¼šåŸºäºè§„åˆ™çš„ç®€å•å¥–åŠ±åœ¨é¿å…å¤æ‚ reward model è®­ç»ƒçš„åŒæ—¶ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸­é—´ä¿¡å·å¼•å¯¼è®­ç»ƒï¼Œè¯æ˜æ— éœ€å¤æ‚æ¨¡å‹ä¹Ÿèƒ½æŒ–æ˜ä¸­é—´æ­¥éª¤ä»·å€¼ï¼Œä¸ºè½»é‡åŒ–å¥–åŠ±è®¾è®¡æä¾›å‚è€ƒã€‚  
3. æ³›åŒ–æ€§æ¢ç´¢ï¼šä»…ç”¨åŸºç¡€æ¨ç†ç±»æ•°æ®é›†è®­ç»ƒå´èƒ½æ³›åŒ–åˆ°å¤æ‚ä»»åŠ¡ï¼Œè¯´æ˜è¯¥æ–¹æ³•æŠ“å‡†äº†æ¨ç†èƒ½åŠ›çš„å…±æ€§æœ¬è´¨ï¼Œä¸ºå¤§æ¨¡å‹è·¨ä»»åŠ¡æ¨ç†èƒ½åŠ›åŸ¹å…»æä¾›å®è·µèŒƒä¾‹ã€‚  
```

## chain-of-retrieval-augmented-generation
### Abstract
This paper introduces an approach for training o1-like RAG models that
retrieve and reason over relevant information step by step before generating
the final answer. Conventional RAG methods usually perform a single retrieval
step before the generation process, which limits their effectiveness in
addressing complex queries due to imperfect retrieval results. In contrast, our
proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the
model to dynamically reformulate the query based on the evolving state. To
train CoRAG effectively, we utilize rejection sampling to automatically
generate intermediate retrieval chains, thereby augmenting existing RAG
datasets that only provide the correct final answer. At test time, we propose
various decoding strategies to scale the model's test-time compute by
controlling the length and number of sampled retrieval chains. Experimental
results across multiple benchmarks validate the efficacy of CoRAG, particularly
in multi-hop question answering tasks, where we observe more than 10 points
improvement in EM score compared to strong baselines. On the KILT benchmark,
CoRAG establishes a new state-of-the-art performance across a diverse range of
knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to
understand the scaling behavior of CoRAG, laying the groundwork for future
research aimed at developing factual and grounded foundation models.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´å•æ­¥æ£€ç´¢é™åˆ¶ï¼šCoRAGå¼€å¯å¤šæ­¥æ£€ç´¢å¢å¼ºç”Ÿæˆæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ä¼ä¸šåº”ç”¨ç­‰åœºæ™¯ä¸­ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒèƒ½è®©å¤§æ¨¡å‹ç»“åˆä¸“æœ‰æ•°æ®æºç”Ÿæˆå¯é ä¸”åŸºäºäº‹å®çš„å“åº”ã€‚ä½†ä¼ ç»ŸRAGå­˜åœ¨å±€é™ï¼šå¸¸è§„RAGåœ¨ç”Ÿæˆå‰ä»…æ‰§è¡Œ**å•æ­¥æ£€ç´¢**ï¼Œé¢å¯¹å¤æ‚æŸ¥è¯¢æ—¶ï¼Œå› æ£€ç´¢ç»“æœå¯èƒ½ä¸å®Œå–„ï¼Œéš¾ä»¥æœ‰æ•ˆåº”å¯¹ï¼›è€Œä¸”æ£€ç´¢æ¨¡å‹ä¸ºæ•ˆç‡é‡‡ç”¨çš„å›ºå®šå¤§å°å‘é‡è¡¨ç¤ºç­‰æ¶æ„ï¼Œé™åˆ¶äº†å¤„ç†å¤æ‚æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œåœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¹Ÿéš¾ç¡®å®šåˆå§‹è¯¥æ£€ç´¢ä»€ä¹ˆä¿¡æ¯ã€‚ä¸ºæ‰“ç ´æ£€ç´¢è´¨é‡ç“¶é¢ˆï¼Œè®©æ¨¡å‹èƒ½åƒäººç±»è§£å†³å¤æ‚é—®é¢˜é‚£æ ·è¿­ä»£æ£€ç´¢ä¿¡æ¯ï¼Œæœ¬æ–‡æå‡ºCoRAGæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCoRAGå¤šæ­¥åŠ¨æ€æ£€ç´¢ä¸æŸ¥è¯¢é‡æ„èŒƒå¼  
ä¼ ç»ŸRAGå•æ­¥æ£€ç´¢åç”Ÿæˆï¼ŒCoRAGåˆ™å…è®¸æ¨¡å‹**åŸºäºçŠ¶æ€æ¼”åŒ–åŠ¨æ€é‡æ„æŸ¥è¯¢**ï¼Œåœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå‰ï¼Œé€æ­¥æ£€ç´¢å’Œæ¨ç†ç›¸å…³ä¿¡æ¯ã€‚è¿™ç§æ–¹å¼æ¨¡æ‹Ÿäººç±»è§£å†³å¤æ‚é—®é¢˜æ—¶è¿­ä»£æ‰¾ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œèƒ½åœ¨æ£€ç´¢å™¨æ²¡è¿”å›æœ‰ç”¨ä¿¡æ¯æ—¶ï¼Œå°è¯•ä¸åŒæŸ¥è¯¢é‡å†™ç­–ç•¥ï¼Œæ¢ç´¢æŸ¥è¯¢çš„å¤šæ–¹é¢ä¿¡æ¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‹’ç»é‡‡æ ·å¢å¼ºæ•°æ®é›†ä¸æ˜¾å¼è®­ç»ƒ  
ç°æœ‰RAGæ•°æ®é›†å¸¸åªç»™æœ€ç»ˆæ­£ç¡®ç­”æ¡ˆï¼Œä¸ºæœ‰æ•ˆè®­ç»ƒCoRAGï¼Œæœ¬æ–‡ç”¨**æ‹’ç»é‡‡æ ·**è‡ªåŠ¨ç”Ÿæˆä¸­é—´æ£€ç´¢é“¾ï¼Œæ‰©å……æ•°æ®é›†ã€‚ä¹‹åç”¨å¼€æºè¯­è¨€æ¨¡å‹åœ¨è¿™äº›å¢å¼ºæ•°æ®é›†ä¸Šï¼Œä»¥æ ‡å‡†ä¸‹ä¸€ä¸ªtokené¢„æµ‹ç›®æ ‡å¾®è°ƒï¼Œè®©æ¨¡å‹æ˜¾å¼å­¦ä¹ é€æ­¥æ£€ç´¢çš„èƒ½åŠ›ï¼Œè€Œéä»…ä¾èµ–æ¨¡å‹ä¸Šä¸‹æ–‡å­¦ä¹ æˆ–ä¸“æœ‰æ¨¡å‹è’¸é¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ ·æµ‹è¯•æ—¶è§£ç ç­–ç•¥è°ƒæ§è®¡ç®—èµ„æº  
æµ‹è¯•é˜¶æ®µï¼Œæå‡ºè´ªå¿ƒè§£ç ã€best - of - Né‡‡æ ·ã€æ ‘æœç´¢ç­‰**å¤šç§è§£ç ç­–ç•¥**ï¼Œé€šè¿‡æ§åˆ¶é‡‡æ ·æ£€ç´¢é“¾çš„é•¿åº¦å’Œæ•°é‡ï¼Œè°ƒèŠ‚æ¨¡å‹æµ‹è¯•æ—¶çš„è®¡ç®—å¼€é”€ï¼ˆå¦‚tokenæ¶ˆè€—ã€æ£€ç´¢å™¨è°ƒç”¨é¢‘ç‡ç­‰ï¼‰ï¼Œå®ç°æµ‹è¯•æ—¶è®¡ç®—èµ„æºçš„çµæ´»ç¼©æ”¾ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
- å¤šè·³é—®ç­”ä»»åŠ¡ï¼šåœ¨å¤šè·³æ¨ç†ç±»QAä»»åŠ¡ä¸­ï¼Œç›¸æ¯”å¼ºåŸºçº¿ï¼ŒCoRAGåœ¨EMåˆ†æ•°ä¸Šæå‡è¶…10ä¸ªç‚¹ï¼ŒéªŒè¯äº†å…¶å¤„ç†éœ€å¤šæ­¥æ£€ç´¢æ¨ç†å¤æ‚ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚  
- KILTåŸºå‡†æµ‹è¯•ï¼šåœ¨æ¶µç›–å¤šæ ·çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„KILTåŸºå‡†ä¸Šï¼ŒCoRAGåœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡çš„éšè—æµ‹è¯•é›†ä¸Šå–å¾—å…¨æ–°SOTAæˆç»©ã€‚  
- ç¼©æ”¾è¡Œä¸ºåˆ†æï¼šä¸åŒè§£ç ç­–ç•¥ä¸‹ï¼Œæ€»tokenæ¶ˆè€—å’Œæ¨¡å‹æ€§èƒ½è¿‘ä¼¼å‘ˆå¯¹æ•°çº¿æ€§å…³ç³»ï¼ˆç³»æ•°å› æ•°æ®é›†è€Œå¼‚ï¼‰ï¼›è¿˜å‘ç°å¯¹ä¸åŒä»»åŠ¡ç±»å‹CoRAGç¼©æ”¾è¡Œä¸ºä¸åŒï¼Œå¦‚NQè¿™ç±»ç°æœ‰SOTAæ£€ç´¢å™¨å·²é«˜å¬å›çš„æ•°æ®é›†ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾æ”¶ç›Šæœ‰é™ï¼Œä¸ºä¾æŸ¥è¯¢å¤æ‚åº¦å’Œæ£€ç´¢å™¨è´¨é‡åŠ¨æ€åˆ†é…æµ‹è¯•è®¡ç®—èµ„æºæä¾›ä¾æ®ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æŠ€æœ¯èŒƒå¼å±‚é¢ï¼šCoRAGæå‡ºçš„å¤šæ­¥åŠ¨æ€æ£€ç´¢ + æŸ¥è¯¢é‡æ„æ€è·¯ï¼Œä¸ºRAGçªç ´å•æ­¥é™åˆ¶ã€å¤„ç†å¤æ‚ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ï¼Œåç»­ç ”ç©¶å¯åœ¨æ­¤åŸºç¡€ä¸Šæ¢ç´¢æ›´çµæ´»çš„æ£€ç´¢ - ç”Ÿæˆäº¤äº’é€»è¾‘ã€‚  
- æ•°æ®å¢å¼ºå±‚é¢ï¼šç”¨æ‹’ç»é‡‡æ ·è‡ªåŠ¨ç”Ÿæˆä¸­é—´æ£€ç´¢é“¾æ¥å¢å¼ºæ•°æ®é›†ï¼Œä¸ºè§£å†³RAGæ•°æ®é›†ç¼ºå°‘ä¸­é—´è¿‡ç¨‹æ ‡æ³¨çš„é—®é¢˜æä¾›äº†å¯è¡Œæ–¹æ³•ï¼Œå¯å€Ÿé‰´åˆ°éœ€ä¸­é—´æ­¥éª¤æ•°æ®çš„æ¨¡å‹è®­ç»ƒåœºæ™¯ã€‚  
- æµ‹è¯•ä¼˜åŒ–å±‚é¢ï¼šé€šè¿‡å¤šæ ·è§£ç ç­–ç•¥è°ƒæ§æµ‹è¯•æ—¶è®¡ç®—ï¼Œè¿™ç§æ ¹æ®ä»»åŠ¡å’Œæ£€ç´¢å™¨æƒ…å†µçµæ´»åˆ†é…èµ„æºçš„æ€è·¯ï¼Œå¯¹å¤§æ¨¡å‹å®é™…éƒ¨ç½²æ—¶å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬å¾ˆæœ‰å¯å‘ï¼Œå¯ç”¨äºä¼˜åŒ–æ¨¡å‹æ¨ç†é˜¶æ®µçš„èµ„æºåˆ©ç”¨ã€‚  
- ç ”ç©¶æ–¹å‘å±‚é¢ï¼šCoRAGä¸ºå¼€å‘äº‹å®æ€§ã€æœ‰ä¾æ®çš„åŸºç¡€æ¨¡å‹ï¼ˆfoundation modelsï¼‰å¥ å®šåŸºç¡€ï¼Œåç»­åœ¨å‡å°‘æ¨¡å‹å¹»è§‰ã€æå‡å†…å®¹å¯é æ€§æ–¹é¢ï¼Œå¯å»¶ç»­å…¶å¤šæ­¥æ£€ç´¢æ¨ç†çš„æ€è·¯æ·±å…¥æ¢ç´¢ã€‚  
```

## okralong--a-flexible-retrieval-augmented-framework-for-long-text-query-processing
### Abstract
Large Language Models (LLMs) encounter challenges in efficiently processing
long-text queries, as seen in applications like enterprise document analysis
and financial report comprehension. While conventional solutions employ
long-context processing or Retrieval-Augmented Generation (RAG), they suffer
from prohibitive input expenses or incomplete information. Recent advancements
adopt context compression and dynamic retrieval loops, but still sacrifice
critical details or incur iterative costs. To address these limitations, we
propose OkraLong, a novel framework that flexibly optimizes the entire
processing workflow. Unlike prior static or coarse-grained adaptive strategies,
OkraLong adopts fine-grained orchestration through three synergistic
components: analyzer, organizer and executor. The analyzer characterizes the
task states, which guide the organizer in dynamically scheduling the workflow.
The executor carries out the execution and generates the final answer.
Experimental results demonstrate that OkraLong not only enhances answer
accuracy but also achieves cost-effectiveness across a variety of datasets.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | OkraLongï¼šçµæ´»åº”å¯¹é•¿æ–‡æœ¬æŸ¥è¯¢çš„æ£€ç´¢å¢å¼ºæ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ä¼ä¸šæ–‡æ¡£åˆ†æã€è´¢åŠ¡æŠ¥å‘Šç†è§£ç­‰å®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†é•¿æ–‡æœ¬æŸ¥è¯¢æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„é•¿ä¸Šä¸‹æ–‡ï¼ˆLCï¼‰å¤„ç†æ–¹æ¡ˆè™½èƒ½åˆ©ç”¨æ¨¡å‹å…¨å±€è¯­å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†è¾“å…¥æˆæœ¬è¿‡é«˜ï¼›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ¡ˆè™½é€šè¿‡æ£€ç´¢å‡å°‘è¾“å…¥é•¿åº¦ï¼Œå´å­˜åœ¨ä¿¡æ¯é—æ¼é£é™©ã€‚è¿‘å¹´çš„ä¸Šä¸‹æ–‡å‹ç¼©å’ŒåŠ¨æ€RAGç­‰æ”¹è¿›æ–¹æ³•ï¼Œè¦ä¹ˆä¸¢å¤±å…³é”®ç»†èŠ‚ï¼Œè¦ä¹ˆå¸¦æ¥è¿­ä»£æˆæœ¬ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºOkraLongæ¡†æ¶ï¼Œæ—¨åœ¨çµæ´»ä¼˜åŒ–é•¿æ–‡æœ¬æŸ¥è¯¢å¤„ç†å…¨æµç¨‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»†ç²’åº¦ç¼–æ’çš„ä¸‰ç»„ä»¶æ¶æ„  
OkraLongç”±åˆ†æå™¨ï¼ˆAnalyzerï¼‰ã€ç¼–æ’å™¨ï¼ˆOrganizerï¼‰å’Œæ‰§è¡Œå™¨ï¼ˆExecutorï¼‰ä¸‰ä¸ªååŒç»„ä»¶æ„æˆã€‚åˆ†æå™¨åŸºäºæŸ¥è¯¢è¯­ä¹‰å’Œåˆæ­¥æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œç²¾å‡†åˆ»ç”»ä»»åŠ¡çŠ¶æ€ï¼›ç¼–æ’å™¨ä¾æ®åˆ†æç»“æœåŠ¨æ€ç”Ÿæˆä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ï¼›æ‰§è¡Œå™¨åˆ™è´Ÿè´£æ‰§è¡Œå¤šæ ·çš„å¤„ç†æµç¨‹ä¸ç­–ç•¥ï¼Œå®ç°å¯¹å…¨å¤„ç†æµç¨‹çš„ç»†ç²’åº¦ä¼˜åŒ–ï¼ŒåŒºåˆ«äºä»¥å¾€é™æ€æˆ–ç²—ç²’åº¦çš„è‡ªé€‚åº”ç­–ç•¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä»»åŠ¡é€‚é…ä¸æˆæœ¬æ„ŸçŸ¥çš„çµæ´»ä¼˜åŒ–  
ä¸€æ–¹é¢ï¼ŒOkraLongèƒ½æ ¹æ®ä¸åŒä»»åŠ¡éœ€æ±‚å®šåˆ¶ç­–ç•¥ï¼Œæ¯”å¦‚æ¯”è¾ƒç±»æŸ¥è¯¢éœ€å•ç‹¬å®ä½“æ£€ç´¢ã€æ¡¥æ¥ç±»æŸ¥è¯¢è§¦å‘åˆ†æ­¥æ¨ç†é“¾ï¼Œæå‡å›ç­”å‡†ç¡®æ€§ï¼›å¦ä¸€æ–¹é¢ï¼Œæˆæœ¬æ„ŸçŸ¥è°ƒåº¦å¯åœ¨ä¸åŒåœºæ™¯åŠ¨æ€åˆ†é…tokené¢„ç®—ä¸ä¿¡æ¯èµ„æºï¼Œå¦‚è¯­ä¹‰å¯†é›†æŸ¥è¯¢åˆ†é…å¤šèšåˆä¸Šä¸‹æ–‡ã€ç‰¹å®šä¿¡æ¯æŸ¥è¯¢ç”¨é¶å‘ä¸Šä¸‹æ–‡åˆ‡ç‰‡ï¼Œå‡å°‘å†—ä½™å¼€é”€ã€‚åŒæ—¶è¿˜å¼€å‘åˆ›æ–°æ‰§è¡Œç®—å­æ”¯æ’‘å®šåˆ¶ç­–ç•¥ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡ä½¿ç”¨æ¶µç›–å¤šé¢†åŸŸã€ä»ç®€å•äº‹å®æŸ¥è¯¢åˆ°å¤æ‚å¤šæ­¥æ¨ç†ä»»åŠ¡çš„é•¿æ–‡æœ¬ç†è§£æ•°æ®é›†è¯„ä¼°OkraLongã€‚ç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ç°æœ‰å…ˆè¿›æ–¹æ³•ï¼ŒOkraLongä¸ä»…æå‡äº†å›ç­”å‡†ç¡®ç‡ï¼Œè¿˜åœ¨æˆæœ¬æ•ˆç›Šæ–¹é¢è¡¨ç°æ›´ä¼˜ï¼ŒéªŒè¯äº†æ¡†æ¶åœ¨ accuracy å’Œ cost-effectiveness ä¸Šçš„åŒé‡ä¼˜åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æŠ€æœ¯è®¾è®¡è§’åº¦ï¼Œå…¶ç»†ç²’åº¦æ‹†åˆ†ç³»ç»Ÿç»„ä»¶ï¼ˆåˆ†æ - ç¼–æ’ - æ‰§è¡Œï¼‰çš„æ€è·¯ï¼Œä¸ºå¤æ‚NLPä»»åŠ¡æµç¨‹ä¼˜åŒ–æä¾›äº†æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„å‚è€ƒèŒƒå¼ï¼›åœ¨å®é™…è½åœ°å±‚é¢ï¼Œæˆæœ¬æ„ŸçŸ¥ä¸ä»»åŠ¡é€‚é…çš„ç»“åˆæ€è·¯ï¼Œå¯¹äºä¼ä¸šçº§å¤§æ¨¡å‹åº”ç”¨ï¼ˆå¦‚æ–‡æ¡£æ£€ç´¢ã€æŠ¥å‘Šåˆ†æï¼‰å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬æå…·å€Ÿé‰´ä»·å€¼ï¼Œèƒ½æŒ‡å¯¼å¼€å‘è€…åœ¨ä¸åŒä¸šåŠ¡åœºæ™¯ä¸‹æ›´æ™ºèƒ½åœ°è°ƒåº¦èµ„æºã€è®¾è®¡æµç¨‹ã€‚
```

## retrollm--empowering-large-language-models-to-retrieve-fine-grained-evidence-within-generation
### Abstract
Large language models (LLMs) exhibit remarkable generative capabilities but
often suffer from hallucinations. Retrieval-augmented generation (RAG) offers
an effective solution by incorporating external knowledge, but existing methods
still face several limitations: additional deployment costs of separate
retrievers, redundant input tokens from retrieved text chunks, and the lack of
joint optimization of retrieval and generation. To address these issues, we
propose \textbf{RetroLLM}, a unified framework that integrates retrieval and
generation into a single, cohesive process, enabling LLMs to directly generate
fine-grained evidence from the corpus with constrained decoding. Moreover, to
mitigate false pruning in the process of constrained evidence generation, we
introduce (1) hierarchical FM-Index constraints, which generate
corpus-constrained clues to identify a subset of relevant documents before
evidence generation, reducing irrelevant decoding space; and (2) a
forward-looking constrained decoding strategy, which considers the relevance of
future sequences to improve evidence accuracy. Extensive experiments on five
open-domain QA datasets demonstrate RetroLLM's superior performance across both
in-domain and out-of-domain tasks. The code is available at
\url{https://github.com/sunnynexus/RetroLLM}.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RetroLLMï¼šè®©å¤§æ¨¡å‹åœ¨ç”Ÿæˆä¸­ç›´æ¥è·å–ç»†ç²’åº¦è¯æ®ï¼Œé©æ–°æ£€ç´¢å¢å¼ºèŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç”Ÿæˆèƒ½åŠ›å¼ºå¤§ï¼Œä½†æ˜“å‡ºç°â€œå¹»è§‰â€é—®é¢˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†ç¼“è§£è¯¥é—®é¢˜ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•å­˜åœ¨è¯¸å¤šå±€é™ï¼šéœ€é¢å¤–éƒ¨ç½²ç‹¬ç«‹æ£€ç´¢å™¨å¢åŠ æˆæœ¬ã€æ£€ç´¢æ–‡æœ¬å—å¸¦æ¥å†—ä½™è¾“å…¥tokenã€æ£€ç´¢ä¸ç”Ÿæˆç¼ºä¹è”åˆä¼˜åŒ–ç­‰ã€‚æ­¤å¤–ï¼Œç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰è™½å°è¯•æ•´åˆä½†ä»éœ€æ˜ å°„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ— ç¼è¡”æ¥æ£€ç´¢ä¸ç”Ÿæˆã€‚åŒæ—¶ï¼Œç›´æ¥åŸºäºFM - Indexçš„çº¦æŸç”Ÿæˆè¯æ®æ˜“å‡ºç°â€œé”™è¯¯å‰ªæâ€ï¼Œå³æ—©æœŸè§£ç æ­¥éª¤çš„é”™è¯¯å¯¼è‡´æ­£ç¡®è¯æ®åºåˆ—è¢«å‰ªæã€‚è¿™äº›ç—›ç‚¹æ¨åŠ¨äº†RetroLLMçš„æå‡ºï¼Œæ—¨åœ¨æ‰“é€ ç»Ÿä¸€æ¡†æ¶ï¼Œè®©æ£€ç´¢ä¸ç”Ÿæˆåœ¨è‡ªå›å½’è§£ç ä¸­æ— ç¼èåˆï¼ŒåŒæ—¶è§£å†³é”™è¯¯å‰ªæé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRetroLLMç»Ÿä¸€æ¡†æ¶
RetroLLMå°†æ£€ç´¢ä¸ç”Ÿæˆæ•´åˆåˆ°å•ä¸€è‡ªå›å½’è§£ç è¿‡ç¨‹ï¼Œè®©å¤§æ¨¡å‹èƒ½åœ¨çº¦æŸè§£ç ä¸‹ä»è¯­æ–™åº“ç›´æ¥ç”Ÿæˆç»†ç²’åº¦è¯æ®ä¸æœ€ç»ˆç­”æ¡ˆã€‚æ— éœ€ç‹¬ç«‹åµŒå…¥æ¨¡å‹ï¼Œæ¨¡å‹å¯è‡ªä¸»å†³å®šæ£€ç´¢å¤šå°‘è¯æ®åŠä½•æ—¶ç”Ÿæˆæœ€ç»ˆå“åº”ï¼Œå®ç°RAGä»»åŠ¡çš„è”åˆä¼˜åŒ–ï¼Œæå‡ç³»ç»Ÿçµæ´»æ€§ä¸æ•´ä½“æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†å±‚FM - Indexçº¦æŸåº”å¯¹é”™è¯¯å‰ªæ
æ„å»ºåˆ†å±‚FM - Indexï¼Œå…ˆç”Ÿæˆè¯­æ–™åº“çº¦æŸçº¿ç´¢ä»¥è¯†åˆ«å€™é€‰æ–‡æ¡£å­é›†ï¼Œå†åœ¨è¯¥å­é›†çš„FM - Indexçº¦æŸä¸‹ç”Ÿæˆè¯æ®ï¼Œå¤§å¹…å‡å°‘æ—©æœŸè§£ç æ­¥éª¤ä¸­æ— å…³è§£ç ç©ºé—´ï¼Œç¼“è§£é”™è¯¯å‰ªæé—®é¢˜ï¼Œè®©è¯æ®ç”Ÿæˆæ›´èšç„¦äºç›¸å…³æ–‡æ¡£èŒƒå›´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå‰ç»æ€§çº¦æŸè§£ç ç­–ç•¥æå‡è¯æ®å‡†ç¡®æ€§
åœ¨è¯æ®ç”Ÿæˆæ—¶å¼•å…¥å‰ç»æ€§çº¦æŸè§£ç ï¼Œåˆ©ç”¨æ–‡æ¡£FM - Indexä¾æ®çº¿ç´¢è¯†åˆ«å€™é€‰æ–‡æ¡£å†…çš„æœªæ¥çª—å£ï¼Œå†é€šè¿‡ç›¸å…³æ€§æ¨¡å‹å¯¹è¿™äº›çª—å£æ‰“åˆ†ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ç›¸å…³çš„è¯æ®ï¼Œè€ƒè™‘æœªæ¥åºåˆ—ç›¸å…³æ€§ä»¥æå‡è¯æ®å‡†ç¡®æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªå¼€æ”¾åŸŸQAæ•°æ®é›†ä¸Šå¼€å±•å¤§é‡å®éªŒï¼Œæ¶µç›–åŸŸå†…ä¸åŸŸå¤–ä»»åŠ¡ã€ä¸åŒåŸºç¡€å¤§æ¨¡å‹åŠä¸åŒå‚æ•°è§„æ¨¡ã€‚ç»“æœè¡¨æ˜ï¼ŒRetroLLMç›¸æ¯”ä¼ ç»ŸRAGå’Œå¤æ‚RAGç­–ç•¥è¡¨ç°æ›´ä¼˜ï¼ŒéªŒè¯äº†å…¶åœ¨æ•´åˆæ£€ç´¢ç”Ÿæˆä¸æå‡è¯æ®ç”Ÿæˆè´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¶æ„è®¾è®¡å±‚é¢ï¼šRetroLLMçš„ç»Ÿä¸€æ¡†æ¶æ€è·¯ä¸ºæ‰“ç ´æ£€ç´¢ä¸ç”Ÿæˆçš„åˆ†ç¦»å£å’æä¾›èŒƒä¾‹ï¼Œå¯ç¤ºåç»­å·¥ä½œæ€è€ƒå¦‚ä½•æ›´æ·±åº¦æ•´åˆä¸åŒæ¨¡å—ä»¥é™ä½éƒ¨ç½²æˆæœ¬ä¸æå‡ååŒæ€§ã€‚
2. æŠ€æœ¯ä¼˜åŒ–å±‚é¢ï¼šåˆ†å±‚FM - Indexä¸å‰ç»æ€§çº¦æŸè§£ç é’ˆå¯¹çº¦æŸç”Ÿæˆä¸­çš„é”™è¯¯å‰ªæé—®é¢˜æå‡ºåˆ›æ–°è§£æ³•ï¼Œä¸ºå¤„ç†å¤§è§„æ¨¡è¯­æ–™ä¸‹çš„ç”Ÿæˆçº¦æŸä¸ç›¸å…³æ€§ä¿éšœæä¾›äº†æŠ€æœ¯å‚è€ƒï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€çº¦æŸç”Ÿæˆä¸”é¢ä¸´ç±»ä¼¼å‰ªæé—®é¢˜çš„åœºæ™¯ã€‚
3. å®éªŒéªŒè¯å±‚é¢ï¼šåœ¨å¤šæ•°æ®é›†ã€å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„å…¨é¢å®éªŒè®¾è®¡ï¼Œä¸ºè¯„ä¼°æ¨¡å‹æ³›åŒ–æ€§ä¸é²æ£’æ€§æä¾›äº†ç¤ºèŒƒï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å¤šç»´åº¦çš„å®éªŒéªŒè¯æ–¹å¼ã€‚
```

## stepsearch--igniting-llms-search-ability-via-step-wise-proximal-policy-optimization
### Abstract
Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our code
will be released on https://github.com/Zillwang/StepSearch.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | StepSearchï¼šç”¨åˆ†æ­¥è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç‚¹ç‡ƒå¤§æ¨¡å‹æœç´¢èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­éœ€è¦è¿­ä»£è·å–é«˜ä»·å€¼å¤–éƒ¨çŸ¥è¯†ï¼Œä½†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒLLMsè¿›è¡Œæœç´¢å¼æ–‡æ¡£æ£€ç´¢çš„å·¥ä½œï¼Œä»…ä¾èµ–å…¨å±€ä¿¡å·å¸¦æ¥çš„ç¨€ç–å¥–åŠ±ï¼Œåœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æ­¤å‰æ–¹æ³•å­˜åœ¨å¯¹ä¸­é—´æŸ¥è¯¢å’Œå¤šæ­¥æ£€ç´¢ç¼ºä¹ç»†ç²’åº¦ç›‘ç£ã€å¤šæ•°å¤šè·³é—®ç­”æ¡†æ¶åœ¨æŸ¥è¯¢è½¨è¿¹å»ºæ¨¡ä¸Šå­˜åœ¨å…³é”®ç¼ºå£ç­‰é—®é¢˜ï¼Œä¸ºå¡«è¡¥è¿™äº›ç ”ç©¶ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºStepSearchæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºé€šç”¨å¤šè·³æœç´¢æ•°æ®  
åŸºäºMuSiQueæ•°æ®é›†å¼€å‘æ–°é¢–çš„æ•°æ® pipelineï¼Œç”Ÿæˆ6ä¸‡æ¡ç»è¿‡ç­›é€‰çš„å­é—®é¢˜æœç´¢å…³é”®è¯ï¼Œè¿™äº›å…³é”®è¯å¯åœ¨ä¸åŒæ£€ç´¢æ•°æ®é›†ä¸Šæ³›åŒ–ã€‚å…·ä½“æµç¨‹ä¸ºåˆ©ç”¨GPT - 4oä¸°å¯Œåˆ†è§£åçš„MuSiQueé—®é¢˜ï¼Œå¾—åˆ°è¿è´¯çš„å­é—®é¢˜ - ç­”æ¡ˆå¯¹å¹¶ç”Ÿæˆæ¯ä¸€æ­¥çš„æœç´¢æŸ¥è¯¢ï¼›å°†å¢å¼ºåçš„æ­¥éª¤é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæœç´¢æŸ¥è¯¢é›†ï¼›å‘å¤šä¸ªæ¥æºå‘å‡ºæŸ¥è¯¢å¹¶ç­›é€‰å‡ºæœ‰æ•ˆç»“æœã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºStepSearchåˆ†æ­¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶  
åœ¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åŸºç¡€ä¸Šå¢åŠ åŸºäºtokençº§åˆ«çš„å¥–åŠ±ï¼ˆä¿¡æ¯å¢ç›Šå’Œå†—ä½™æƒ©ç½šï¼‰ï¼Œç”¨äºæŸ¥è¯¢æ„å»ºå’Œæ–‡æ¡£æ£€ç´¢ã€‚å°†æ¯ä¸€è½®äº¤äº’åˆ’åˆ†ä¸ºæ€è€ƒâ†’æœç´¢â†’å›ç­”é˜¶æ®µï¼Œä¸ºæ¯ä¸ªtokenåˆ†é…ä¿¡æ¯å¢ç›Šä¿¡å·å’Œå†—ä½™æƒ©ç½šï¼Œå¼•å¯¼æ¨¡å‹å°†å¤šè·³æŸ¥è¯¢åˆ†è§£ä¸ºèšç„¦çš„æœç´¢å­ä»»åŠ¡ï¼ŒåŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼Œæ›´æœ‰æ•ˆåœ°æ•´åˆå¤–éƒ¨è¯æ®ã€‚åŒæ—¶è®¾è®¡ç®€çº¦çš„æç¤ºæ¨¡æ¿ï¼Œåœ¨è®­ç»ƒæ—¶è§£è€¦å‚æ•°æ›´æ–°ä¸æ£€ç´¢äº§ç‰©ï¼Œèšç„¦æ¨¡å‹å†…éƒ¨æ¨ç†å’Œæœç´¢ç­–ç•¥å‚æ•°å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ ‡å‡†å¤šè·³QAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒStepSearchæ˜¾è‘—è¶…è¶Šå…¨å±€å¥–åŠ±åŸºçº¿ã€‚ä½¿ç”¨ä»…19kçš„è®­ç»ƒæ•°æ®ï¼Œå¯¹äº3Bå’Œ7Bè§„æ¨¡çš„æ¨¡å‹ï¼Œç›¸æ¯”å„ç§åŸºäºRLçš„æœç´¢åŸºçº¿åˆ†åˆ«å®ç°äº†11.2%å’Œ4.2%çš„ç»å¯¹æ€§èƒ½æå‡ï¼›åœ¨ä¸åŒå¤šè·³QAåŸºå‡†ä¸Šï¼Œç›¸æ¯”æ ‡å‡†RLåŸºçº¿åˆ†åˆ«æœ‰5.7%ã€9.1%ã€10.0%å’Œ15.2%çš„ç»å¯¹æå‡ï¼Œè¯æ˜äº†ç»†ç²’åº¦ã€åˆ†æ­¥ç›‘ç£åœ¨ä¼˜åŒ–æ·±åº¦æœç´¢LLMsæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºæ–¹é¢ï¼šé€šè¿‡æ•°æ® pipeline ç”ŸæˆåŒ…å«å­é—®é¢˜çº§æœç´¢è½¨è¿¹çš„ç»†ç²’åº¦é—®ç­”æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ›´ä¸°å¯Œçš„å¤šè·³æ¨ç†ç›¸å…³æ•°æ®æ”¯æ’‘ï¼Œè¿™ç§ä»å·²æœ‰å¼€æºæ•°æ®é›†æ‹“å±•æ„å»ºä¸“ç”¨æ•°æ®çš„æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚
2. å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹é¢ï¼šå¼•å…¥åˆ†æ­¥çš„ã€tokençº§åˆ«çš„å¥–åŠ±æœºåˆ¶ï¼Œå…³æ³¨ä¸­é—´æ­¥éª¤çš„ä¿¡æ¯å¢ç›Šä¸å†—ä½™é—®é¢˜ï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡ä¸­å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç¨€ç–ã€ç›‘ç£ä¸ç»†çš„é—®é¢˜æä¾›äº†æ–°æ–¹å‘ï¼Œå¯ç”¨äºå…¶ä»–éœ€è¦å¤šæ­¥è¿­ä»£ã€ç»†ç²’åº¦ç›‘ç£çš„LLMsåº”ç”¨åœºæ™¯ä¼˜åŒ–ã€‚
3. è®­ç»ƒç­–ç•¥æ–¹é¢ï¼šåœ¨è®­ç»ƒæ—¶å¯¹ç‰¹å®šæ¨¡å—ï¼ˆå¦‚æ£€ç´¢äº§ç‰©ç›¸å…³éƒ¨åˆ†ï¼‰è¿›è¡Œæ¢¯åº¦å±è”½ï¼Œèšç„¦å…³é”®å­¦ä¹ ç›®æ ‡ï¼Œè¿™ç§è§£è€¦å­¦ä¹ ç›®æ ‡çš„è®­ç»ƒæŠ€å·§åœ¨èåˆå¤–éƒ¨å·¥å…·æˆ–ä¿¡æ¯çš„LLMsè®­ç»ƒä¸­å…·æœ‰å‚è€ƒä»·å€¼ã€‚
```

## toolrl--reward-is-all-tool-learning-needs
### Abstract
Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolRLï¼šå·¥å…·å­¦ä¹ ï¼Œå¥–åŠ±è®¾è®¡æ˜¯å…³é”®

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è·å¾—å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä½†SFTåœ¨é™Œç”Ÿæˆ–å¤æ‚å·¥å…·ä½¿ç”¨åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½åœ¨æ¨ç†å’Œæ³›åŒ–æ–¹é¢å±•ç°æ½œåŠ›ï¼Œç„¶è€Œå·¥å…·ä½¿ç”¨çš„å¥–åŠ±è®¾è®¡å­˜åœ¨æŒ‘æˆ˜ï¼šå¤šå·¥å…·è°ƒç”¨å‚æ•°å¤šæ ·ï¼Œç²—ç²’åº¦å¥–åŠ±ï¼ˆå¦‚ç­”æ¡ˆåŒ¹é…ï¼‰æ— æ³•æä¾›æœ‰æ•ˆå­¦ä¹ æ‰€éœ€çš„ç»†ç²’åº¦åé¦ˆã€‚å› æ­¤ï¼Œæœ¬æ–‡èšç„¦RLèŒƒå¼ä¸‹å·¥å…·é€‰æ‹©ä¸åº”ç”¨ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡ï¼Œæ¢ç´¢å¦‚ä½•é€šè¿‡åˆç†å¥–åŠ±è®¾è®¡æå‡LLMså·¥å…·ä½¿ç”¨å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡ç³»ç»Ÿç ”ç©¶RLèŒƒå¼ä¸‹å·¥å…·é€‰æ‹©ä¸åº”ç”¨çš„å¥–åŠ±è®¾è®¡  
å…¨é¢æ¢ç´¢å¥–åŠ±ç­–ç•¥çš„ç±»å‹ã€è§„æ¨¡ã€ç²’åº¦å’Œæ—¶é—´åŠ¨æ€ç­‰ç»´åº¦ï¼Œåˆ†æä¸åŒå¥–åŠ±ç­–ç•¥å¯¹å·¥å…·å­¦ä¹ çš„å½±å“ï¼Œä¸ºå·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡æä¾›ç³»ç»Ÿè®¤çŸ¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºé’ˆå¯¹æ€§å¥–åŠ±è®¾è®¡æ¡†æ¶å¹¶ç»“åˆGRPOè®­ç»ƒLLMs  
åŸºäºå¯¹å¥–åŠ±ç­–ç•¥çš„æ¢ç´¢æ´å¯Ÿï¼Œè®¾è®¡é€‚ç”¨äºå·¥å…·ä½¿ç”¨ä»»åŠ¡çš„åŸåˆ™æ€§å¥–åŠ±æ–¹æ¡ˆï¼Œå¹¶åˆ©ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œå…¼é¡¾å¥–åŠ±è®¾è®¡ä¸è®­ç»ƒç®—æ³•ï¼Œæå‡å·¥å…·ä½¿ç”¨æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼šç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æ€§èƒ½æå‡17%ï¼Œè¾ƒSFTæ¨¡å‹æå‡15%ï¼›ä¸”è®­ç»ƒé²æ£’ã€å¯æ‰©å±•ä¸”ç¨³å®šï¼Œå¥–åŠ±æ›²çº¿åœ¨è®­ç»ƒä¸­å¿«é€Ÿä¸Šå‡ï¼Œå±•ç°è‰¯å¥½å­¦ä¹ æ•ˆæœï¼›åŒæ—¶æ¨¡å‹å¯¹æœªè§è¿‡çš„åœºæ™¯å’Œä»»åŠ¡ç›®æ ‡æ³›åŒ–èƒ½åŠ›å¼ºï¼Œè¿˜å‡ºç°ä¸»åŠ¨æ€§ã€å…ƒè®¤çŸ¥æ¨ç†ç­‰æ¶Œç°è¡Œä¸ºã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¥–åŠ±è®¾è®¡ç»´åº¦çš„å…¨é¢æ¢ç´¢ä¸ºåç»­LLM - agentè®­ç»ƒæä¾›å‚è€ƒï¼Œå¦‚è®¤è¯†åˆ°è¿‡é•¿æ¨ç†è½¨è¿¹å¥–åŠ±æœªå¿…å¥½ã€åŠ¨æ€å¥–åŠ±è§„æ¨¡åŠ©åŠ›è¡Œä¸ºè¿‡æ¸¡ã€ç»†ç²’åº¦å¥–åŠ±åˆ†è§£æå‡å­¦ä¹ ç¨³å®šæ€§ç­‰ç»“è®ºï¼Œå¯æŒ‡å¯¼åç»­å¥–åŠ±æœºåˆ¶è®¾è®¡ã€‚  
2. é¦–æ¬¡å°†RLåº”ç”¨äºé€šç”¨TIRä»»åŠ¡å¹¶ç»™å‡ºå¥–åŠ±è®¾è®¡å®è¯è·¯çº¿ï¼Œä¸ºæ‰“é€ æ›´å¼ºå¤§è‡ªä¸»çš„LLMæ™ºèƒ½ä½“é“ºè·¯ï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤æ¡†æ¶æ‹“å±•å·¥å…·å­¦ä¹ å’ŒRLç»“åˆçš„æ–¹å‘ã€‚  
3. ä»£ç å¼€æºä¾¿äºå¤ç°ä¸è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸå¿«é€Ÿå‘å±•ï¼Œä¸ºç§‘ç ”ç¤¾åŒºæä¾›äº†å¯å¤ç”¨çš„èµ„æºä¸ç ”ç©¶åŸºç¡€ã€‚  
```

## deepresearcher--scaling-deep-research-via-reinforcement-learning-in-real-world-environments
### Abstract
Large Language Models (LLMs) equipped with web search capabilities have
demonstrated impressive potential for deep research tasks. However, current
approaches predominantly rely on either manually engineered prompts (prompt
engineering-based) with brittle performance or reinforcement learning within
controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that
fail to capture the complexities of real-world interaction. In this paper, we
introduce DeepResearcher, the first comprehensive framework for end-to-end
training of LLM-based deep research agents through scaling reinforcement
learning (RL) in real-world environments with authentic web search
interactions. Unlike RAG-based approaches that assume all necessary information
exists within a fixed corpus, our method trains agents to navigate the noisy,
unstructured, and dynamic nature of the open web. We implement a specialized
multi-agent architecture where browsing agents extract relevant information
from various webpage structures and overcoming significant technical
challenges. Extensive experiments on open-domain research tasks demonstrate
that DeepResearcher achieves substantial improvements of up to 28.9 points over
prompt engineering-based baselines and up to 7.2 points over RAG-based RL
agents. Our qualitative analysis reveals emergent cognitive behaviors from
end-to-end RL training, including the ability to formulate plans,
cross-validate information from multiple sources, engage in self-reflection to
redirect research, and maintain honesty when unable to find definitive answers.
Our results highlight that end-to-end training in real-world web environments
is not merely an implementation detail but a fundamental requirement for
developing robust research capabilities aligned with real-world applications.
We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | DeepResearcherï¼šåœ¨çœŸå®ç¯å¢ƒä¸­ç”¨å¼ºåŒ–å­¦ä¹ èµ‹èƒ½æ·±åº¦ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆç½‘ç»œæœç´¢èƒ½åŠ›åï¼Œåœ¨æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼šä¸€ç±»æ˜¯ä¾èµ–äººå·¥è®¾è®¡æç¤ºè¯çš„æ–¹å¼ï¼Œæ€§èƒ½è„†å¼±ä¸ç¨³å®šï¼›å¦ä¸€ç±»æ˜¯åœ¨å—æ§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç¯å¢ƒå†…åšå¼ºåŒ–å­¦ä¹ ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œäº¤äº’çš„å¤æ‚æ€§ã€‚ä¸ºäº†çªç ´è¿™äº›å±€é™ï¼Œè®ºæ–‡æå‡ºè¦æ‰“é€ èƒ½åœ¨çœŸå®ç½‘ç»œç¯å¢ƒä¸‹ï¼Œé€šè¿‡è§„æ¨¡åŒ–å¼ºåŒ–å­¦ä¹ æ¥ç«¯åˆ°ç«¯è®­ç»ƒåŸºäºLLMçš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œä¹Ÿå°±æ˜¯DeepResearcherã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçœŸå®ç¯å¢ƒä¸‹çš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒ  
ä¸åŒäºRAGç±»æ–¹æ³•å‡è®¾æ‰€éœ€ä¿¡æ¯éƒ½åœ¨å›ºå®šè¯­æ–™åº“ï¼ŒDeepResearcherèšç„¦å¼€æ”¾ç½‘ç»œâ€œå˜ˆæ‚ã€æ— ç»“æ„ã€åŠ¨æ€â€çš„ç‰¹æ€§ï¼Œè®©æ™ºèƒ½ä½“å­¦ä¹ åœ¨çœŸå®ç½‘é¡µäº¤äº’é‡Œå¯¼èˆªï¼Œå®ç°å¯¹ç ”ç©¶æ™ºèƒ½ä½“çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œä¸å†å—é™äºå—æ§ç¯å¢ƒçš„å‡è®¾ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸“ç”¨å¤šæ™ºèƒ½ä½“æ¶æ„  
æ­å»ºäº†ç‰¹æ®Šçš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œå…¶ä¸­æµè§ˆæ™ºèƒ½ä½“è´Ÿè´£ä»å„ç±»ç½‘é¡µç»“æ„é‡Œæå–ç›¸å…³ä¿¡æ¯ï¼Œå…‹æœäº†ç½‘é¡µç»“æ„å¤šæ ·å¸¦æ¥çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œè®©æ™ºèƒ½ä½“åœ¨çœŸå®ç½‘é¡µåœºæ™¯ä¸‹èƒ½æœ‰æ•ˆè·å–ä¿¡æ¯ç”¨äºç ”ç©¶ä»»åŠ¡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡çš„å¤§é‡å®éªŒä¸­ï¼ŒDeepResearcherå¯¹æ¯”åŸºäºæç¤ºè¯å·¥ç¨‹çš„åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡æœ€é«˜å¯è¾¾28.9ä¸ªç™¾åˆ†ç‚¹ï¼›å¯¹æ¯”åŸºäºRAGçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œä¹Ÿèƒ½æå‡æœ€é«˜7.2ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†æè¿˜å‘ç°äº†ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¸¦æ¥çš„â€œæ¶Œç°è®¤çŸ¥è¡Œä¸ºâ€ï¼Œæ¯”å¦‚åˆ¶å®šè®¡åˆ’ã€å¤šæºä¿¡æ¯äº¤å‰éªŒè¯ã€è‡ªæˆ‘åæ€è°ƒæ•´ç ”ç©¶æ–¹å‘ã€æ‰¾ä¸åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ç­‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç ”ç©¶ä»·å€¼æ¥çœ‹ï¼Œè®ºæ–‡è¯æ˜äº†çœŸå®ç½‘ç»œç¯å¢ƒä¸‹çš„ç«¯åˆ°ç«¯è®­ç»ƒä¸åªæ˜¯â€œå®ç°ç»†èŠ‚â€ï¼Œæ›´æ˜¯æ‰“é€ è´´åˆçœŸå®åº”ç”¨çš„é²æ£’ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ï¼Œè¿™ä¸ºåç»­ç ”ç©¶æ™ºèƒ½ä½“çš„å¼€å‘æŒ‡æ˜äº†æ–¹å‘â€”â€”è¦é‡è§†çœŸå®åœºæ™¯äº¤äº’è®­ç»ƒã€‚ä»å·¥ç¨‹å®è·µè§’åº¦ï¼Œå…¶å¼€æºçš„DeepResearcheræ¡†æ¶ï¼ˆhttps://github.com/GAIR-NLP/DeepResearcherï¼‰èƒ½ä¸ºæƒ³æ¢ç´¢â€œå¼ºåŒ–å­¦ä¹ +çœŸå®ç½‘é¡µäº¤äº’â€åšæ·±åº¦ç ”ç©¶çš„å›¢é˜Ÿæˆ–ä¸ªäººæä¾›å‚è€ƒï¼Œå¤šæ™ºèƒ½ä½“å¤„ç†ç½‘é¡µä¿¡æ¯çš„æ€è·¯ä¹Ÿå€¼å¾—åœ¨å¤æ‚ä¿¡æ¯æŠ½å–ç±»ä»»åŠ¡ä¸­å€Ÿé‰´ã€‚è€Œä¸”å¯¹â€œæ¶Œç°è¡Œä¸ºâ€çš„åˆ†æï¼Œä¹Ÿå¯å‘ç ”ç©¶è€…å…³æ³¨è®­ç»ƒè¿‡ç¨‹ä¸­æ™ºèƒ½ä½“èƒ½åŠ›çš„è‡ªç„¶æˆé•¿ä¸å±•ç°~
```

## an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents
### Abstract
Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ é©±åŠ¨æ¨ç†-æœç´¢äº¤ç»‡å‹å¤§æ¨¡å‹æ™ºèƒ½ä½“çš„å®è¯ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†è¯¸å¤šä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨éœ€ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ã€å€ŸåŠ©å·¥å…·çš„åœºæ™¯ï¼ˆå¦‚æœç´¢ä»»åŠ¡ï¼‰ä¸­å­˜åœ¨å±€é™ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºè®­ç»ƒLLMæˆä¸ºèƒ½äº¤ç»‡æ¨ç†ä¸æœç´¢çš„æ™ºèƒ½ä½“æä¾›äº†æ½œåŠ›ï¼Œç„¶è€Œè¿™ç±»æ™ºèƒ½ä½“çš„æœ€ä¼˜è®¾è®¡ä»ä¸æ˜æ™°ï¼Œå¦‚å¥–åŠ±è®¾è®¡ã€åŸºç¡€LLMé€‰æ‹©ä¸ç‰¹æ€§ã€æœç´¢å¼•æ“åœ¨RLè¿‡ç¨‹ä¸­è§’è‰²ç­‰å…³é”®å› ç´ å¾…æ·±å…¥æ¢ç©¶ã€‚æ­¤å‰æ–¹æ³•åœ¨è®­ç»ƒæœç´¢æ™ºèƒ½ä½“æ—¶ï¼ŒåŸºäºæç¤ºæˆ–æœ‰ç›‘ç£å¾®è°ƒå­˜åœ¨éš¾æ‰©å±•ç­‰é—®é¢˜ï¼Œè€ŒRLè™½æœ‰å‰æ™¯ä½†ç›¸å…³å…³é”®é—®é¢˜ç ”ç©¶ä¸è¶³ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡å…¨é¢å®è¯ç ”ç©¶å¡«è¡¥è¿™äº›ç©ºç™½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç³»ç»Ÿæ¢ç©¶å¥–åŠ±è®¾è®¡å¯¹æœç´¢æ™ºèƒ½ä½“è®­ç»ƒçš„å½±å“ 
åŒºåˆ†æ ¼å¼å¥–åŠ±ï¼ˆåæ˜ å¯¹æ™ºèƒ½ä½“åŠ¨ä½œæ ¼å¼çš„éµå¾ªï¼‰ä¸ä¸­é—´æ£€ç´¢å¥–åŠ±ï¼ˆè¿­ä»£æ¿€åŠ±ä¸ç»“æœç›¸å…³çš„æ£€ç´¢ï¼‰ï¼Œå®è¯åˆ†æä¸åŒå¥–åŠ±åœ¨è®­ç»ƒä¸­ä½œç”¨ï¼Œæ˜ç¡®æ ¼å¼å¥–åŠ±å¯¹æ€§èƒ½æå‡æ›´æœ‰æ•ˆï¼Œä¸­é—´æ£€ç´¢å¥–åŠ±æ•ˆç”¨æœ‰é™ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ·±å…¥åˆ†æåŸºç¡€LLMç‰¹æ€§çš„å½±å“ 
å¯¹æ¯”é€šç”¨å‹ä¸æ¨ç†ä¸“ç”¨å‹LLMï¼Œä»¥åŠä¸åŒè§„æ¨¡æ¨¡å‹åœ¨RLè®­ç»ƒä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºæ¨¡å‹è§„æ¨¡ã€åˆå§‹åŒ–ç±»å‹ï¼ˆé€šç”¨æˆ–æ¨ç†ä¸“ç”¨ï¼‰å¯¹RLè®­ç»ƒç»“æœçš„æ˜¾è‘—å½±å“ï¼Œä¸ºé€‰æ‹©åˆé€‚åŸºç¡€æ¨¡å‹æä¾›ä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå‰–ææœç´¢å¼•æ“é€‰æ‹©çš„ä½œç”¨ 
ç ”ç©¶è®­ç»ƒæ—¶ä¸åŒè´¨é‡æœç´¢å¼•æ“ï¼ˆä»éšæœºå™ªå£°åˆ°å¼ºå¯†é›†æ£€ç´¢å™¨ï¼‰å¦‚ä½•å¡‘é€ RLè®­ç»ƒåŠ¨æ€ï¼Œä»¥åŠæ¨ç†æ—¶æ£€ç´¢ç³»ç»Ÿå˜åŒ–å¯¹æ™ºèƒ½ä½“é²æ£’æ€§å½±å“ï¼Œæ˜ç¡®æœç´¢å¼•æ“é€‰æ‹©åœ¨è®­ç»ƒåŠ¨æ€ä¸æ¨ç†é²æ£’æ€§ä¸Šçš„å…³é”®ä½œç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. å¥–åŠ±è®¾è®¡æ–¹é¢ï¼šåŠ å…¥æ ¼å¼å¥–åŠ±èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå°¤å…¶ä»åŸºç¡€LLMå¼€å§‹è®­ç»ƒæ—¶ï¼›ä¸­é—´æ£€ç´¢å¥–åŠ±æœªå¸¦æ¥æŒç»­æ€§èƒ½æå‡ï¼Œå®ç”¨ä»·å€¼æœ‰é™ã€‚
2. åŸºç¡€LLMå±‚é¢ï¼šé€šç”¨å‹LLMåœ¨RLåœºæ™¯ä¸­è¡¨ç°ä¼˜äºæ¨ç†ä¸“ç”¨å‹ï¼Œå¯èƒ½å› åè€…è®­ç»ƒåˆæœŸæŒ‡ä»¤éµå¾ªèƒ½åŠ›è¾ƒå¼±ï¼›æ‰©å¤§æ¨¡å‹è§„æ¨¡é€šå¸¸æå‡æœ€ç»ˆæ€§èƒ½ï¼Œä½†æ”¶ç›Šé€’å‡ã€‚
3. æœç´¢å¼•æ“é€‰æ‹©ç»´åº¦ï¼šè®­ç»ƒæ—¶æœç´¢å¼•æ“è´¨é‡å¼ºçƒˆå½±å“RLåŠ¨æ€ï¼Œç”¨æ— ä¿¡æ¯å¼•æ“ï¼ˆå¦‚éšæœºå™ªå£°ï¼‰ä¼šè®©æ™ºèƒ½ä½“å®Œå…¨å›é¿æ£€ç´¢ï¼Œå¼±å¼•æ“ï¼ˆå¦‚BM25ï¼‰å¯¼è‡´é¢‘ç¹ä½†ä½æ•ˆæœç´¢è°ƒç”¨ï¼Œå¼ºå¼•æ“ï¼ˆå¦‚å¯†é›†æ£€ç´¢å™¨ï¼‰å­¦ä¹ æ›´ç¨³å®šï¼›æ¨ç†æ—¶æœç´¢æ™ºèƒ½ä½“å¯¹ä¸åŒæ£€ç´¢ç³»ç»Ÿæ™®éé²æ£’ï¼Œæ›´å¼ºæœç´¢å¼•æ“æŒç»­å¸¦æ¥æ›´å¥½ä¸‹æ¸¸æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¥–åŠ±è®¾è®¡ï¼šåœ¨æ„å»ºRLé©±åŠ¨çš„LLMæœç´¢æ™ºèƒ½ä½“æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘æ ¼å¼å¥–åŠ±æ¥æå‡æ€§èƒ½ï¼Œå¯è°¨æ…è¯„ä¼°ä¸­é—´æ£€ç´¢å¥–åŠ±æŠ•å…¥ã€‚
2. æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®åº”ç”¨åœºæ™¯æƒè¡¡é€šç”¨å‹ä¸æ¨ç†ä¸“ç”¨å‹LLMï¼Œè‹¥è¿½æ±‚RLè®­ç»ƒæ•ˆæœåˆæœŸé€šç”¨å‹æ›´å…·ä¼˜åŠ¿ï¼ŒåŒæ—¶åˆç†è€ƒè™‘æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½æ”¶ç›Šå¹³è¡¡ã€‚
3. æœç´¢å¼•æ“é›†æˆï¼šè®­ç»ƒé˜¶æ®µé€‰æ‹©é«˜è´¨é‡æœç´¢å¼•æ“ä¿éšœå­¦ä¹ ç¨³å®šæ€§ï¼Œæ¨ç†é˜¶æ®µå¯åŸºäºåœºæ™¯çµæ´»æ›´æ¢æ£€ç´¢ç³»ç»Ÿï¼Œä¸”åˆ©ç”¨å¼ºæœç´¢å¼•æ“æå‡ä¸‹æ¸¸è¡¨ç°ï¼Œä¸ºå®é™…éƒ¨ç½²æœç´¢æ™ºèƒ½ä½“æä¾›äº†æ¸…æ™°çš„é€‰å‹ä¸ä¼˜åŒ–æ–¹å‘ã€‚ 
```

## r1-searcher++--incentivizing-the-dynamic-knowledge-acquisition-of-llms-via-reinforcement-learning
### Abstract
Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | R1-Searcher++ï¼šç”¨å¼ºåŒ–å­¦ä¹ é©±åŠ¨å¤§æ¨¡å‹åŠ¨æ€è·å–çŸ¥è¯†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½å…·å¤‡å¼ºå¤§èƒ½åŠ›ï¼Œä½†ä¾èµ–é™æ€å†…éƒ¨çŸ¥è¯†æ˜“äº§ç”Ÿå¹»è§‰ï¼Œåœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­è¡¨ç°å—é™ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ³¨å…¥å¤–éƒ¨ä¿¡æ¯ç¼“è§£æ­¤é—®é¢˜ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•å­˜åœ¨æˆæœ¬é«˜ã€æ³›åŒ–å·®æˆ–å¿½è§†æ¨¡å‹å†…éƒ¨çŸ¥è¯†ç­‰ç¼ºé™·ã€‚åŒæ—¶ï¼ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„è’¸é¦æ˜“è®©æ¨¡å‹è®°å¿†å›ºå®šè·¯å¾„ï¼Œé™åˆ¶æ³›åŒ–ï¼›æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼ˆå¦‚MCTSï¼‰æ¨ç†å¼€é”€å¤§ï¼›åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè™½èƒ½è®©æ¨¡å‹è‡ªä¸»æ¢ç´¢å¤–éƒ¨æ£€ç´¢ï¼Œä½†æ˜“è¿‡åº¦ä¾èµ–å¤–éƒ¨å¼•æ“è€Œå¿½ç•¥å†…éƒ¨çŸ¥è¯†ã€‚äººç±»è§£å†³é—®é¢˜æ—¶ä¼šå…ˆè°ƒç”¨å†…éƒ¨çŸ¥è¯†ã€ç¼ºä¿¡æ¯å†æœä¸”ä¼šè®°å¿†æ–°çŸ¥è¯†ï¼Œå¤§æ¨¡å‹ç»å¤§è§„æ¨¡é¢„è®­ç»ƒå·²æœ‰ä¸°å¯Œå†…éƒ¨çŸ¥è¯†ï¼Œå› æ­¤éœ€è®©æ¨¡å‹åŠ¨æ€åˆ‡æ¢å†…å¤–çŸ¥è¯†æºå¹¶è®°å¿†æœ‰ç”¨ä¿¡æ¯ï¼ŒR1 - Searcher++åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥
é‡‡ç”¨SFT Cold - startå’ŒRL for Dynamic Knowledge Acquisitionä¸¤é˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µç”¨æ‹’ç»é‡‡æ ·æ”¶é›†ç¬¦åˆæ ¼å¼è¦æ±‚æ•°æ®ï¼Œé€šè¿‡SFTè®©æ¨¡å‹å®Œæˆåˆæ­¥æ ¼å¼å­¦ä¹ ï¼›ç¬¬äºŒé˜¶æ®µåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ å¼•å¯¼æ¨¡å‹åŠ¨æ€è·å–çŸ¥è¯†ï¼Œä¾æ®ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±æœºåˆ¶ï¼Œè®©æ¨¡å‹åœ¨æœ‰ä¿¡å¿ƒæ—¶ä¾èµ–å†…éƒ¨çŸ¥è¯†ã€ä¸ç¡®å®šæ—¶è°ƒç”¨å¤–éƒ¨æœç´¢æœºåˆ¶ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¿€åŠ±å†…éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è®°å¿†æœºåˆ¶
è®¾è®¡å¥–åŠ±æœºåˆ¶é¼“åŠ±æ¨¡å‹åˆ©ç”¨å†…éƒ¨çŸ¥è¯†ï¼ŒåŒæ—¶å¼•å…¥è®°å¿†æœºåˆ¶ï¼Œå°†æ£€ç´¢åˆ°çš„å†…å®¹è½¬æ¢å¹¶è®°å¿†ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒä¸­ä¿ç•™é‡åˆ°çš„çŸ¥è¯†ï¼ŒæŒç»­ä¸°å¯Œå†…éƒ¨çŸ¥è¯†ï¼Œé€šè¿‡è‡ªä¸»æ¢ç´¢å’ŒåŠæ—¶è®°å¿†å¹³è¡¡å†…éƒ¨æ¨ç†ä¸å¤–éƒ¨æ£€ç´¢ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åŸºäºQwen - 2.5 - 7B - Instructå¼€å±•å¤§é‡å®éªŒï¼ŒR1 - Searcher++ç›¸æ¯”å¼ºåŸºçº¿æœ€å¤šæå‡4.3%ï¼›ä¸åŸºäºåŸå§‹RLçš„æ–¹æ³•ç›¸æ¯”ï¼Œæ£€ç´¢æ¬¡æ•°å‡å°‘42.9%ï¼Œè¯æ˜å…¶åœ¨æ€§èƒ½å’Œæ£€ç´¢æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸”ä¼˜äºç°æœ‰RAGæ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡çœ‹ï¼Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä¸ºæ¨¡å‹åˆ†é˜¶æ®µå­¦ä¹ ï¼ˆå…ˆæ ¼å¼ååŠ¨æ€çŸ¥è¯†è·å–ï¼‰æä¾›äº†æ€è·¯ï¼Œå¯å€Ÿé‰´äºéœ€åˆ†æ­¥éª¤æå‡èƒ½åŠ›çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼›å¥–åŠ±ä¸è®°å¿†æœºåˆ¶ç»“åˆï¼Œä¸ºå¹³è¡¡æ¨¡å‹å†…å¤–çŸ¥è¯†åˆ©ç”¨ã€æŒç»­è¿›åŒ–æä¾›äº†èŒƒå¼ï¼Œåœ¨éœ€ç»“åˆå†…å¤–èµ„æºä¸”è¦çŸ¥è¯†ç§¯ç´¯çš„åœºæ™¯ï¼ˆå¦‚æ™ºèƒ½é—®ç­”ç³»ç»ŸæŒç»­ä¼˜åŒ–ï¼‰æœ‰å‚è€ƒä»·å€¼ï¼›ä»å®éªŒéªŒè¯è§’åº¦ï¼ŒåŸºäºç‰¹å®šæ¨¡å‹ï¼ˆQwen - 2.5 - 7B - Instructï¼‰çš„è¯¦ç»†å¯¹æ¯”å®éªŒï¼Œå±•ç¤ºäº†æ–°æ–¹æ³•åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šçš„æå‡ï¼Œä¸ºåç»­ç±»ä¼¼æ–¹æ³•çš„å®éªŒè®¾è®¡æä¾›äº†å¯¹ç…§èŒƒä¾‹ï¼Œè®©ç ”ç©¶è€…æ˜ç™½å¦‚ä½•é€šè¿‡å¯¹æ¯”å‡¸æ˜¾æ–¹æ³•ä¼˜åŠ¿ã€‚
```

## removal-of-hallucination-on-hallucination--debate-augmented-rag
### Abstract
Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¶ˆé™¤â€œå¹»è§‰ä¸Šçš„å¹»è§‰â€ï¼šè¾©è®ºå¢å¼ºçš„RAGæ¡†æ¶DRAG

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½å…·å¤‡å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£ä¸æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨â€œå¹»è§‰â€é—®é¢˜ï¼Œå³ç”Ÿæˆå†…å®¹åç¦»äº‹å®æ­£ç¡®æ€§ï¼Œåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å¯é æ€§å—é™ã€‚ä¸ºç¼“è§£å¹»è§‰ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†æ£€ç´¢æ¥æå‡è¾“å‡ºçš„äº‹å®å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼ŒRAGä¹Ÿå¸¦æ¥æ–°æŒ‘æˆ˜ï¼šæœ‰åå·®æˆ–é”™è¯¯çš„æ£€ç´¢ç»“æœä¼šè¯¯å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å¹»è§‰é—®é¢˜åŠ å‰§ï¼Œè¿™ç§ç°è±¡è¢«ä½œè€…ä»¬ç§°ä¸ºâ€œå¹»è§‰ä¸Šçš„å¹»è§‰ï¼ˆHallucination on Hallucinationï¼‰â€ã€‚ç°æœ‰æ–¹æ³•æœªç³»ç»Ÿä¼˜åŒ–RAGå…¨æµç¨‹ï¼Œä¸”ä¾èµ–å•æ™ºèƒ½ä½“å†³ç­–æ˜“å—å›ºæœ‰åå·®å½±å“ã€‚è€Œå¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMADï¼‰åœ¨å¢å¼ºLLMé²æ£’æ€§ã€äº‹å®å‡†ç¡®æ€§ç­‰æ–¹é¢å±•ç°æ½œåŠ›ï¼Œå› æ­¤æœ¬æ–‡å—æ­¤å¯å‘ï¼Œæå‡ºè¾©è®ºå¢å¼ºçš„RAGæ¡†æ¶ï¼ˆDRAGï¼‰æ¥åº”å¯¹RAGé¢ä¸´çš„æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºâ€œå¹»è§‰ä¸Šçš„å¹»è§‰â€æ–°è§†è§’  
é¦–æ¬¡æ˜ç¡®æŒ‡å‡ºRAGä¸­å­˜åœ¨çš„å…³é”®é—®é¢˜â€”â€”å½“æ£€ç´¢æä¾›ä¸å®Œæ•´ã€æœ‰åå·®æˆ–è¯¯å¯¼æ€§ä¿¡æ¯æ—¶ï¼ŒLLMå›ºæœ‰çš„å¹»è§‰ä¼šè¢«è¿›ä¸€æ­¥æ”¾å¤§ï¼Œå³â€œå¹»è§‰ä¸Šçš„å¹»è§‰â€ï¼Œä¸ºåç»­è§£å†³RAGå…¨æµç¨‹é—®é¢˜é”šå®šäº†æ–°è§†è§’ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºDRAGè®­ç»ƒæ— å…³æ¡†æ¶ï¼Œèåˆå¤šæ™ºèƒ½ä½“è¾©è®ºåˆ°æ£€ç´¢ä¸ç”Ÿæˆé˜¶æ®µ  
åœ¨**æ£€ç´¢é˜¶æ®µ**ï¼ŒDRAGå¼•å…¥æ”¯æŒè€…ï¼ˆproponentsï¼‰ã€åå¯¹è€…ï¼ˆopponentsï¼‰å’Œè£åˆ¤ï¼ˆjudgesï¼‰è¿›è¡Œç»“æ„åŒ–è¾©è®ºï¼šæ”¯æŒè€…å€¡å¯¼æ£€ç´¢ç­–ç•¥ï¼Œåå¯¹è€…è´¨ç–‘æŸ¥è¯¢å……åˆ†æ€§ï¼Œè£åˆ¤è¯„ä¼°å®Œæ•´æ€§ï¼Œä»¥æ­¤ç»´æŠ¤åŠ¨æ€æŸ¥è¯¢æ± ï¼Œç¡®ä¿çŸ¥è¯†è¦†ç›–æ›´å…¨é¢ã€å‡å°‘äº‹å®åå·®ï¼Œæå‡æ£€ç´¢è´¨é‡ä¸å¯é æ€§ã€‚  
åœ¨**ç”Ÿæˆé˜¶æ®µ**ï¼ŒDRAGåœ¨æ™ºèƒ½ä½“é—´å»ºç«‹éå¯¹ç§°ä¿¡æ¯è§’è‰²ï¼Œå‡å°‘LLMå¯¹æ£€ç´¢å†…å®¹çš„è¿‡åº¦ä¾èµ–ï¼ŒåŒæ—¶æ¨åŠ¨ç»“æ„åŒ–å¯¹æŠ—è¾©è®ºï¼Œå¢å¼ºæ¨ç†é²æ£’æ€§ã€ç¼“è§£äº‹å®ä¸ä¸€è‡´é—®é¢˜ã€‚æ•´ä¸ªDRAGæ— éœ€é¢å¤–è®­ç»ƒï¼Œå€ŸåŠ©å¤šæ™ºèƒ½ä½“è¾©è®ºæœºåˆ¶ä¼˜åŒ–RAGå…¨æµç¨‹ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå¯¹DRAGè¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼šDRAGæå‡äº†æ£€ç´¢å¯é æ€§ï¼Œå‡å°‘äº†RAGå¼•å‘çš„å¹»è§‰é—®é¢˜ï¼ŒåŒæ—¶æ˜¾è‘—å¢å¼ºäº†æ•´ä½“äº‹å®å‡†ç¡®æ€§ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ä¼˜åŒ–RAGæµç¨‹ã€è§£å†³â€œå¹»è§‰ä¸Šçš„å¹»è§‰â€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰å±‚é¢ï¼šæå‡ºâ€œå¹»è§‰ä¸Šçš„å¹»è§‰â€ç²¾å‡†åˆ»ç”»RAGæ–°é—®é¢˜ï¼Œä¸ºé¢†åŸŸå†…åç»­ç ”ç©¶é”šå®šæ–°çš„é—®é¢˜è§†è§’ä¸æ–¹å‘ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨RAGå…¨æµç¨‹ä¸­æ£€ç´¢ä¸ç”Ÿæˆçš„è”åŠ¨æ€§é—®é¢˜ã€‚  
2. æ–¹æ³•åˆ›æ–°å±‚é¢ï¼šå°†å¤šæ™ºèƒ½ä½“è¾©è®ºæœºåˆ¶åˆ›æ–°æ€§èå…¥RAGçš„æ£€ç´¢å’Œç”ŸæˆåŒé˜¶æ®µï¼Œä¸”æ— éœ€è®­ç»ƒï¼Œä¸ºæå‡RAGæ€§èƒ½æä¾›äº†è½»é‡ä¸”æœ‰æ•ˆçš„æ¡†æ¶æ€è·¯ï¼Œåç»­åœ¨æ„å»ºå¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿä¼˜åŒ–ç”Ÿæˆæˆ–æ£€ç´¢ä»»åŠ¡æ—¶ï¼Œå¯å€Ÿé‰´è¿™ç§è§’è‰²åˆ†å·¥ã€è¾©è®ºäº¤äº’çš„æ¨¡å¼ã€‚  
3. å®è·µä»·å€¼å±‚é¢ï¼šå¼€æºä»£ç ä¸ºè¡Œä¸šæä¾›äº†å¯å¤ç°ã€å¯æ‹“å±•çš„åŸºç¡€ï¼Œä¾¿äºç ”ç©¶è€…æˆ–å¼€å‘è€…åœ¨æ­¤åŸºç¡€ä¸Šæ¢ç´¢å¤šæ™ºèƒ½ä½“ä¸RAGç»“åˆçš„æ›´å¤šå¯èƒ½æ€§ï¼Œæ¨åŠ¨æŠ€æœ¯è½åœ°ä¸è¿­ä»£ã€‚  
```

## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | s3ï¼šå°æ•°æ®é‡ä¸‹ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœç´¢æ™ºèƒ½ä½“çš„æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å€ŸåŠ©å¤–éƒ¨çŸ¥è¯†æå‡æ¨ç†èƒ½åŠ›ï¼Œè€Œè¿‘æœŸç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®©LLMæ‰®æ¼”æœç´¢æ™ºèƒ½ä½“çš„è¿›å±•ï¼Œå¯é€šè¿‡å¤šè½®äº¤äº’ä¼˜åŒ–ä¿¡æ¯è·å–ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šè¦ä¹ˆç”¨ä»…å…³æ³¨æœç´¢çš„æŒ‡æ ‡ï¼ˆå¦‚NDCGï¼‰ä¼˜åŒ–æ£€ç´¢ï¼Œå¿½ç•¥ä¸‹æ¸¸æ•ˆç”¨ï¼›è¦ä¹ˆå¾®è°ƒæ•´ä¸ªLLMæ¥è”åˆæ¨ç†ä¸æ£€ç´¢ï¼ŒæŠŠæ£€ç´¢å’Œç”Ÿæˆè€¦åˆï¼Œé™åˆ¶äº†å®é™…æœç´¢æ•ˆç”¨ä¸”éš¾é€‚é…å†»ç»“æˆ–é—­æºæ¨¡å‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§è§£è€¦æœç´¢ä¸ç”Ÿæˆã€è½»é‡ä¸”æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œåœ¨å°æ•°æ®é‡ä¸‹æå‡ä¸‹æ¸¸æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºs3æ¡†æ¶ï¼Œè§£è€¦æœç´¢å™¨ä¸ç”Ÿæˆå™¨  
s3æ˜¯è½»é‡ã€æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œå°†æœç´¢å™¨ï¼ˆSearcherï¼‰ä»ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ä¸­è§£è€¦ã€‚è®­ç»ƒæ—¶å›ºå®šç”Ÿæˆå™¨ï¼ˆå¯å¤ç”¨ä»»æ„å†»ç»“çš„LLMï¼‰ï¼Œä»…ä¸“æ³¨äºè®­ç»ƒæœç´¢å™¨ï¼Œè®©æœç´¢ä¼˜åŒ–èšç„¦åœ¨å¯¹ä¸‹æ¸¸ç”Ÿæˆè´¨é‡çš„æå‡ä¸Šï¼Œé¿å…äº†ç”Ÿæˆä¸æ£€ç´¢çš„è€¦åˆé—®é¢˜ï¼Œä¹Ÿèƒ½é€‚é…é—­æºæˆ–å†»ç»“çš„LLMã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰â€œGain Beyond RAGï¼ˆGBRï¼‰â€å¥–åŠ±ä¿¡å·  
GBRä½œä¸ºå…¨æ–°çš„å¥–åŠ±æŒ‡æ ‡ï¼Œè¡¡é‡äº†s3æ£€ç´¢åˆ°çš„æ–‡æ¡£è®©ç”Ÿæˆå™¨è¡¨ç°ï¼Œç›¸æ¯”â€œæœ´ç´ RAGï¼ˆnaÃ¯ve RAGï¼‰æ£€ç´¢â€åœ¨ç”Ÿæˆå‡†ç¡®ç‡ä¸Šçš„æå‡å¹…åº¦ã€‚é€šè¿‡è¯¥å¥–åŠ±è®­ç»ƒæœç´¢å™¨ï¼Œèƒ½ç›´æ¥é’ˆå¯¹ä¸‹æ¸¸ç”Ÿæˆæ•ˆç”¨ä¼˜åŒ–æ£€ç´¢ç»„ä»¶ï¼Œæ‘†è„±ä»…å…³æ³¨æœç´¢æŒ‡æ ‡æˆ–æ˜“è¿‡æ‹Ÿåˆçš„ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ç±»æŒ‡æ ‡çš„å±€é™ï¼Œè®©æ£€ç´¢ä¼˜åŒ–æ›´è´´åˆå®é™…ç”Ÿæˆéœ€æ±‚ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨6ä¸ªé€šç”¨QAåŸºå‡†å’Œ5ä¸ªåŒ»ç–—QAåŸºå‡†æµ‹è¯•ä¸­ï¼Œs3å±•ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼šä»…ç”¨2.4kè®­ç»ƒæ ·æœ¬ï¼Œå°±è¶…è¶Šäº†ä½¿ç”¨è¶…70å€æ•°æ®ï¼ˆå¦‚70kç”šè‡³æ›´å¤šï¼‰è®­ç»ƒçš„åŸºçº¿æ–¹æ³•ï¼ˆåƒDeepRetrievalã€Search - R1ç­‰ï¼‰ã€‚åœ¨é€šç”¨å’ŒåŒ»ç–—é¢†åŸŸçš„å¹³å‡å¾—åˆ†ä¸Šï¼Œs3å¯¹æ¯”å…¶ä»–ç»å…¸RAGã€Active RAGã€RL - Zeroé˜¶æ®µçš„æ–¹æ³•ï¼Œéƒ½å®ç°äº†æ›´ä¼˜çš„ä¸‹æ¸¸ç”Ÿæˆè¡¨ç°ï¼ŒéªŒè¯äº†å°æ•°æ®é‡ä¸‹é«˜æ•ˆè®­ç»ƒæœç´¢æ™ºèƒ½ä½“çš„èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å—åŒ–è®¾è®¡æ€è·¯ï¼šå°†å¤æ‚çš„â€œæ£€ç´¢ + ç”Ÿæˆâ€ä»»åŠ¡è§£è€¦ä¸ºæœç´¢å™¨å’Œç”Ÿæˆå™¨ç‹¬ç«‹ä¼˜åŒ–ï¼Œä¸ºæ„å»ºæ›´çµæ´»çš„RAGç³»ç»Ÿæä¾›äº†æ¨¡å—åŒ–æ€è·¯ï¼Œåç»­å¯é’ˆå¯¹ä¸åŒæ¨¡å—åˆ†åˆ«è¿­ä»£å‡çº§ã€‚  
2. å¥–åŠ±ä¿¡å·è®¾è®¡ï¼šGBRå¥–åŠ±å°†ä¸‹æ¸¸ç”Ÿæˆæ•ˆç”¨ä½œä¸ºæ£€ç´¢ä¼˜åŒ–çš„æ ¸å¿ƒä¾æ®ï¼Œè·³å‡ºäº†ä¼ ç»Ÿä»…çœ‹æœç´¢ç¯èŠ‚æŒ‡æ ‡çš„æ€ç»´å®šå¼ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨æ£€ç´¢å¢å¼ºåœºæ™¯ä¸‹çš„å¥–åŠ±å‡½æ•°è®¾è®¡æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘å…¶ä»–éœ€è¦è·¨ç¯èŠ‚ä¼˜åŒ–ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡ã€‚  
3. å°æ•°æ®é«˜æ•ˆè®­ç»ƒï¼šè¯æ˜äº†åœ¨å°‘é‡ä¼˜è´¨æ ·æœ¬ä¸‹ä¹Ÿèƒ½è®­ç»ƒå‡ºé«˜æ€§èƒ½æœç´¢æ™ºèƒ½ä½“ï¼Œé™ä½äº†å¤§è§„æ¨¡æ•°æ®æ ‡æ³¨ä¸è®­ç»ƒæˆæœ¬ï¼Œå¯¹èµ„æºæœ‰é™ä½†éœ€æ„å»ºRAGç³»ç»Ÿçš„åœºæ™¯ï¼ˆå¦‚å‚ç›´é¢†åŸŸå°å›¢é˜Ÿï¼‰æœ‰å¾ˆå¼ºçš„å€Ÿé‰´æ„ä¹‰ã€‚

## vrag-rl--empower-vision-perception-based-rag-for-visually-rich-information-understanding-via-iterative-reasoning-with-reinforcement-learning
### Abstract
Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
https://github.com/Alibaba-NLP/VRAG.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | VRAG-RLï¼šç”¨å¼ºåŒ–å­¦ä¹ èµ‹èƒ½è§†è§‰æ„ŸçŸ¥RAGï¼Œçªç ´å¯Œè§†è§‰ä¿¡æ¯ç†è§£ç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ä¿¡æ¯ç†è§£é¢†åŸŸï¼Œä¼ ç»ŸåŸºäºæ–‡æœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•éš¾ä»¥å¤„ç†è§†è§‰ç›¸å…³ä¿¡æ¯ï¼›è€Œå½“å‰åŸºäºè§†è§‰çš„RAGæ–¹æ³•åˆå—é™äºå›ºå®šæµç¨‹ï¼Œä¸”å¸¸å› æœªèƒ½å……åˆ†æ¿€æ´»æ¨¡å‹åŸºç¡€èƒ½åŠ›ï¼Œå¯¼è‡´æœ‰æ•ˆæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚åŒæ—¶ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«è¯æ˜å¯¹æ¨¡å‹æ¨ç†æœ‰å¸®åŠ©ï¼Œä½†åœ¨å¤šæ¨¡æ€RAGåœºæ™¯ä¸‹è¿˜å­˜åœ¨ä¸¤å¤§å…³é”®å±€é™ï¼šä¸€æ˜¯ç°æœ‰å¤šæ¨¡æ€RAGå¾€å¾€ä»…æŠŠå›¾åƒå¡è¿›ä¸Šä¸‹æ–‡ï¼Œæ¨ç†tokenåˆ†é…ä¸è¶³ä¸”å¿½è§†è§†è§‰ä¸“å±æ„ŸçŸ¥ï¼›äºŒæ˜¯æ¨¡å‹ä¸æœç´¢å¼•æ“äº¤äº’æ—¶ï¼Œå› è¡¨è¿°ä¸æ¸…éš¾ä»¥ç²¾å‡†æ£€ç´¢ä¿¡æ¯ï¼Œæ€§èƒ½æ¬ ä½³ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè®ºæ–‡æå‡ºVRAG - RLæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯Œè§†è§‰ä¿¡æ¯ä¸‹çš„å¤æ‚æ¨ç†éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡è§†è§‰æ„ŸçŸ¥åŠ¨ä½œç©ºé—´  
ä¸ºå¯Œè§†è§‰è¾“å…¥é‡èº«å®šåˆ¶åŠ¨ä½œç©ºé—´ï¼ŒåŒ…å«è£å‰ªã€ç¼©æ”¾ç­‰åŠ¨ä½œã€‚å€ŸåŠ©è§†è§‰æ„ŸçŸ¥tokenï¼Œè®©è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½ä»ç²—åˆ°ç»†åœ°æ”¶é›†ä¿¡æ¯ã€‚æ¯”å¦‚å¤„ç†æ–‡æ¡£é‡Œçš„å›¾åƒæˆ–å›¾è¡¨æ—¶ï¼Œæ¨¡å‹å¯é€šè¿‡æ„ŸçŸ¥tokenæ›´å…³æ³¨ä¿¡æ¯å¯†é›†åŒºåŸŸï¼Œåœ¨æœ‰é™ä¸Šä¸‹æ–‡é•¿åº¦å†…æ¿€æ´»æ¨ç†èƒ½åŠ›ï¼Œé¿å…é—æ¼ç»†èŠ‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå…¨é¢å¥–åŠ±ç»“æ„  
ä¸ºå¼¥åˆç”¨æˆ·åŸå§‹æŸ¥è¯¢ä¸æ£€ç´¢å™¨é—´çš„å·®è·ï¼Œé‡‡ç”¨æ•´åˆæŸ¥è¯¢é‡å†™ã€æ£€ç´¢æ€§èƒ½ä¸åŸºäºæ¨¡å‹å¥–åŠ±çš„ç®€å•æœ‰æ•ˆå¥–åŠ±ã€‚å°†æ£€ç´¢è¿‡ç¨‹æœ‰æ•ˆæ€§çº³å…¥å¥–åŠ±ç»“æ„ï¼Œåœ¨æ¨¡å‹ä¸æœç´¢å¼•æ“äº¤äº’æ—¶ï¼ŒåŠæ—¶æ£€ç´¢ç›¸å…³å›¾åƒèƒ½åŠ©åŠ›æ¨¡å‹æœ‰æ•ˆç­”é¢˜ï¼Œåä¹‹åˆ™æ·»å™ªå¹²æ‰°æ¨ç†ï¼Œä»¥æ­¤ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶æä¾›å…¨é¢æŒ‡å¯¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ‰“é€ å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è¿­ä»£æ¨ç†æ¡†æ¶  
å—â€œæ€è€ƒå†å›ç­”â€å’ŒReActèŒƒå¼å¯å‘ï¼ŒæŠŠVLMsä¸æœç´¢å¼•æ“äº¤äº’åŠè§†è§‰æ„ŸçŸ¥åŠ¨ä½œç©ºé—´å»ºæ¨¡ä¸ºè¿­ä»£æ¨ç†å’Œå·¥å…·è°ƒç”¨è¿‡ç¨‹ã€‚æ”¯æŒè‡ªåŠ¨é‡‡æ ·å¹¶æ•´åˆGRPOç®—æ³•ï¼Œç²¾å¿ƒè®¾è®¡é‡‡æ ·ç­–ç•¥ï¼ˆå«æ¯æ¬¡äº¤äº’åå¤„ç†ï¼‰ï¼Œç”¨åŸºäºæ¨¡å‹çš„å¥–åŠ±å’Œæ£€ç´¢å¥–åŠ±å¼•å¯¼æ¨¡å‹è®­ç»ƒï¼Œä¿éšœå¤šè½®é‡‡æ ·ä¸è®­ç»ƒçš„ç¨³å®šæ€§ã€‚è¿˜é‡æ–°æ ‡æ³¨å¯Œè§†è§‰æ–‡æ¡£æ•°æ®é›†ã€æ„å»ºæ•°æ® pipeline æ¥ä¸ºRLå’ŒSFTé«˜æ•ˆæ‰©å®¹æ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šæ ·ä¸”å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVRAG - RLå±•ç°å‡ºå¼ºå¤§æ€§èƒ½ã€‚åŸºäºQwen2.5 - VL - 7Bæ¨¡å‹æ—¶ï¼Œç›¸è¾ƒç°æœ‰æ–¹æ³•æå‡è¶…20%ï¼›åŸºäºQwen2.5 - VL - 3Bæ¨¡å‹æ—¶ï¼Œæå‡è¶…30%ï¼Œæœ‰åŠ›è¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡æ€æ„ŸçŸ¥åŠ¨ä½œç©ºé—´è®¾è®¡ï¼šåœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹ï¼Œé’ˆå¯¹ç‰¹å®šæ¨¡æ€ï¼ˆå¦‚è§†è§‰ï¼‰è®¾è®¡ä¸“å±åŠ¨ä½œç©ºé—´ï¼Œå¼•å¯¼æ¨¡å‹èšç„¦å…³é”®ä¿¡æ¯åŒºåŸŸï¼Œä¸ºæå‡å¤šæ¨¡æ€ç†è§£ç²¾ç»†åº¦æä¾›æ€è·¯ã€‚
2. å¥–åŠ±æœºåˆ¶èåˆå®è·µï¼šå°†è¿‡ç¨‹æŒ‡æ ‡ï¼ˆå¦‚æ£€ç´¢æ€§èƒ½ï¼‰ä¸ç»“æœæŒ‡æ ‡ï¼ˆæ¨¡å‹è¾“å‡ºç»“æœï¼‰èåˆè¿›å¥–åŠ±ï¼Œè®©æ¨¡å‹è®­ç»ƒæ›´è´´åˆçœŸå®åº”ç”¨åœºæ™¯ï¼Œè¿™ç§å¥–åŠ±è®¾è®¡æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€å¤šç¯èŠ‚åä½œçš„ä»»åŠ¡ã€‚
3. å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€RAGç»“åˆï¼šå€ŸåŠ©å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹åœ¨RAGä»»åŠ¡çš„æ¨ç†ä¸äº¤äº’èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤æ‚ä¿¡æ¯å¤„ç†åœºæ™¯çš„ä¼˜åŒ–æä¾›äº†æ–°èŒƒå¼å‚è€ƒã€‚
```

## airrag--activating-intrinsic-reasoning-for-retrieval-augmented-generation-using-tree-based-search
### Abstract
Leveraging the autonomous decision-making capabilities of large language
models (LLMs) has demonstrated superior performance in reasoning tasks.
However, despite the success of iterative or recursive retrieval-augmented
generation (RAG) techniques, these methods are often constrained to a single
solution space when confronted with complex problems. In this paper, we propose
a novel thinking pattern in RAG that integrates system analysis with efficient
reasoning actions, significantly activating intrinsic reasoning capabilities
and expanding the solution space of specific tasks via Monte Carlo Tree Search
(MCTS), which we refer to as AirRAG. Specifically, our approach designs five
fundamental reasoning actions, which are expanded to a broad tree-based
reasoning space using MCTS. The approach also incorporates self-consistency
verification to explore potential reasoning paths and inference scaling law.
Additionally, computationally optimal strategies are employed to allocate more
inference resources to key actions, thereby enhancing overall performance.
Experimental results demonstrate the effectiveness of AirRAG, showing
significant performance gains on complex question-answering datasets.
Furthermore, AirRAG is flexible and lightweight, making it easy to integrate
with other advanced technologies.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | AirRAGï¼šåŸºäºæ ‘æœç´¢æ¿€æ´»æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å†…åœ¨æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆäº‹å®æ€§é”™è¯¯å†…å®¹é—®é¢˜ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œå°¤å…¶åœ¨ç‰¹å®šé¢†åŸŸæˆ–çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚ä½†éšç€ä»»åŠ¡å¤æ‚åº¦æå‡ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼šå•ä¸€æŸ¥è¯¢éš¾ä»¥è·å–è¶³å¤ŸçŸ¥è¯†ã€éš¾ä»¥ç†è§£é—®é¢˜å†…åœ¨å¤æ‚æ¨ç†é€»è¾‘ï¼›æ­¤å‰é’ˆå¯¹å¤æ‚æŸ¥è¯¢åœºæ™¯çš„ç ”ç©¶å¤šä¾èµ–äººå·¥è§„åˆ™ä¸æç¤ºå·¥ç¨‹ï¼Œçµæ´»æ€§ä¸è¶³ï¼Œä¸”é€’å½’æ£€ç´¢ç­‰æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸‹æ˜“å—é™äºå•ä¸€è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œæ— æ³•å……åˆ†æ¿€æ´»LLMå†³ç­–èƒ½åŠ›ï¼Œé“¾å¼æ¨ç†è¿‡ç¨‹ä¹Ÿæ˜“é™·å…¥ç‹­çª„è§£ç©ºé—´ï¼Œå°æ¨¡å‹è‡ªæ¢ç´¢æ—¶æ›´éš¾å¼•å¯¼ã€‚å› æ­¤ï¼Œéœ€è®¾è®¡åˆç†æ¨ç†åŠ¨ä½œä»¥å…¨é¢æ¢ç´¢å¹¿é˜”ä¸”æ·±å…¥çš„è§£ç©ºé—´ï¼Œæå‡RAGæ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡äº”ç§åŸºç¡€æ¨ç†åŠ¨ä½œ  
æå‡ºç³»ç»Ÿåˆ†æã€ç›´æ¥å›ç­”ã€æ£€ç´¢å›ç­”ã€æŸ¥è¯¢è½¬æ¢ã€æ€»ç»“å›ç­”è¿™äº”ç§åŸºç¡€æ¨ç†åŠ¨ä½œï¼Œèƒ½æœ‰æ•ˆåº”å¯¹RAGåœºæ™¯ä¸‹å¤šç§é—®é¢˜ç±»å‹ï¼Œæ¶µç›–æ¸è¿›å¼æˆ–å¹¶è¡ŒæŸ¥è¯¢ç­‰åœºæ™¯ï¼Œä¸”åœ¨ç›¸å¯¹è¾ƒå°è¯­è¨€æ¨¡å‹ä¸Šä¹Ÿèƒ½é«˜æ•ˆæ‰§è¡Œï¼Œä¿éšœæ¨ç†è¿‡ç¨‹å¯æ§æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥MCTSä¸è‡ªä¸€è‡´æ€§æ‰©å±•è§£ç©ºé—´  
å€ŸåŠ©è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å°†äº”ç§åŸºç¡€æ¨ç†åŠ¨ä½œæ‰©å±•ä¸ºåŸºäºæ ‘çš„å¹¿é˜”æ¨ç†ç©ºé—´ï¼Œç»“åˆè‡ªä¸€è‡´æ€§éªŒè¯å®ç°å¯æ§æ¨ç†è·¯å¾„ç”Ÿæˆä¸é«˜æ•ˆæ¨ç†ç¼©æ”¾ï¼›åŒæ—¶ç»“åˆæŠ•ç¥¨æœºåˆ¶ä¸è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ä»å¤šæ¡æ¨ç†è·¯å¾„é€‰ç­”æ¡ˆï¼Œé€šè¿‡å…¨é¢çš„æ¨ç†ç¼©æ”¾ä¸å¯æ’æ‹”æ¶æ„æå‡æ³›åŒ–æ€§ä¸æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜AirRAGåœ¨å¤æ‚é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸æ¯”ç°æœ‰è¿­ä»£æˆ–é€’å½’RAGæ–¹æ³•æ€§èƒ½æ˜¾è‘—æå‡ï¼›ä¸”éšç€æ¨ç†è®¡ç®—é‡å¢åŠ ï¼ˆå¦‚è¾“å‡ºåºåˆ—æ•°ã€tokenæ€»é‡å˜åŒ–ï¼‰ï¼Œèƒ½å€ŸåŠ©ç”Ÿæˆå¤šæ ·æ€§ä¸è‡ªä¸€è‡´æ€§æ¢ç´¢æ½œåœ¨è§£ç©ºé—´ï¼Œæ•´ä½“æ€§èƒ½å¾—åˆ°æ˜æ˜¾å¢å¼ºï¼ˆå¦‚å›¾1å±•ç¤ºä¸åŒè¾“å‡ºåºåˆ—æ•°ä¸‹å¤šæ•°æ®é›†å¹³å‡æ€§èƒ½å¯¹æ¯”ï¼Œä½“ç°æ¨ç†è¿‡ç¨‹ä¸­tokenæ¶ˆè€—ä¸æ€§èƒ½æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨ç†åŠ¨ä½œè®¾è®¡æ€è·¯ï¼šä¸ºç‰¹å®šä»»åŠ¡åœºæ™¯å®šåˆ¶åŸºç¡€æ¨ç†åŠ¨ä½œï¼Œè¦†ç›–å¤šç±»é—®é¢˜ï¼Œå¯å¯å‘åç»­é’ˆå¯¹ä¸åŒé¢†åŸŸä»»åŠ¡è®¾è®¡é’ˆå¯¹æ€§æ¨ç†åŸè¯­ï¼Œæå‡ä»»åŠ¡å¤„ç†å…¨é¢æ€§ã€‚  
2. æ ‘æœç´¢ä¸è‡ªä¸€è‡´æ€§ç»“åˆï¼šå°†MCTSå¼•å…¥RAGæ‰©å±•è§£ç©ºé—´ï¼Œæ­é…è‡ªä¸€è‡´æ€§éªŒè¯ä¼˜åŒ–æ¨ç†è·¯å¾„ï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡ä¸‹è§£ç©ºé—´å—é™é—®é¢˜æä¾›äº†ç»“åˆç»å…¸æœç´¢ç®—æ³•ä¸å¤§æ¨¡å‹ç‰¹æ€§çš„æ–°æ€è·¯ã€‚  
3. çµæ´»è½»é‡æ¶æ„ï¼šAirRAGæ¶æ„çµæ´»ï¼Œæ˜“äºæ•´åˆå…¶ä»–å…ˆè¿›æŠ€æœ¯ä½œä¸ºé¢å¤–åŠ¨ä½œåˆ†æ”¯ï¼Œè¿™ç§æ¨¡å—åŒ–ã€å¯æ‰©å±•è®¾è®¡ç†å¿µä¾¿äºåç»­æŠ€æœ¯è¿­ä»£ä¸å¤šæ–¹æ³•èåˆï¼Œé™ä½ä¸å…¶ä»–æŠ€æœ¯ç»“åˆé—¨æ§›ã€‚  
```

## atomr--atomic-operator-empowered-large-language-models-for-heterogeneous-knowledge-reasoning
### Abstract
Despite the outstanding capabilities of large language models (LLMs),
knowledge-intensive reasoning still remains a challenging task due to LLMs'
limitations in compositional reasoning and the hallucination problem. A
prevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented
generation (RAG), which first formulates a reasoning plan by decomposing
complex questions into simpler sub-questions, and then applies iterative RAG at
each sub-question. However, prior works exhibit two crucial problems:
inadequate reasoning planning and poor incorporation of heterogeneous
knowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct
accurate heterogeneous knowledge reasoning at the atomic level. Inspired by how
knowledge graph query languages model compositional reasoning through combining
predefined operations, we propose three atomic knowledge operators, a unified
set of operators for LLMs to retrieve and manipulate knowledge from
heterogeneous sources. First, in the reasoning planning stage, AtomR decomposes
a complex question into a reasoning tree where each leaf node corresponds to an
atomic knowledge operator, achieving question decomposition that is highly
fine-grained and orthogonal. Subsequently, in the reasoning execution stage,
AtomR executes each atomic knowledge operator, which flexibly selects,
retrieves, and operates atomic level knowledge from heterogeneous sources. We
also introduce BlendQA, a challenging benchmark specially tailored for
heterogeneous knowledge reasoning. Experiments on three single-source and two
multi-source datasets show that AtomR outperforms state-of-the-art baselines by
a large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on
BlendQA. We release our code and datasets.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | AtomRï¼šåŸå­ç®—å­èµ‹èƒ½å¤§æ¨¡å‹ï¼Œçªç ´å¼‚æ„çŸ¥è¯†æ¨ç†å›°å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½èƒ½åŠ›å‡ºä¼—ï¼Œä½†åœ¨çŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ä¸­ï¼Œå—é™äºç»„åˆæ¨ç†èƒ½åŠ›ä¸è¶³ä¸å¹»è§‰é—®é¢˜ï¼Œè¡¨ç°ä»æœ‰æ¬ ç¼ºã€‚ç°æœ‰ç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•ï¼Œè™½å…ˆåˆ†è§£å¤æ‚é—®é¢˜ä¸ºå­é—®é¢˜å†è¿­ä»£æ£€ç´¢ï¼Œä½†å­˜åœ¨ä¸¤å¤§å…³é”®ç¼ºé™·ï¼šä¸€æ˜¯æ¨ç†è§„åˆ’ä¸å¤Ÿç²¾ç»†ï¼Œå­é—®é¢˜åˆ†è§£ç¼ºä¹åŸå­æ€§ä¸è¯­ä¹‰åŠŸèƒ½çº¦æŸï¼Œæ˜“ç”Ÿæˆç²—ç²’åº¦æˆ–åŠŸèƒ½ä¸å‡†ç¡®çš„å­é—®é¢˜ï¼›äºŒæ˜¯å¯¹å¼‚æ„çŸ¥è¯†æºï¼ˆå¦‚ç½‘é¡µã€æ–‡æœ¬è¯­æ–™ã€çŸ¥è¯†å›¾è°±ï¼‰çš„æ•´åˆèƒ½åŠ›å¼±ï¼Œå¤šæ•°ä»…å›ºå®šå•æºæ£€ç´¢ï¼Œå°‘æ•°æ”¯æŒå¤šæºä¹Ÿéš¾åœ¨æ­¥éª¤çº§è·¨æºæ£€ç´¢ä¸æœ‰æ•ˆçŸ¥è¯†æ“ä½œï¼›æ­¤å¤–ï¼Œå¼‚æ„çŸ¥è¯†æºä¸Šçš„ä¼˜è´¨åŸºå‡†æ•°æ®é›†ä¹Ÿå¾ˆåŒ®ä¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºAtomRæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåŸå­çŸ¥è¯†ç®—å­  
å—çŸ¥è¯†å›¾è°±æŸ¥è¯¢è¯­è¨€é€šè¿‡ç»„åˆé¢„å®šä¹‰æ“ä½œå»ºæ¨¡ç»„åˆæ¨ç†çš„å¯å‘ï¼Œæç‚¼å‡º**Searchã€Relateã€Filter**è¿™ä¸‰ç§åŸå­çŸ¥è¯†ç®—å­ã€‚å®ƒä»¬å…·å¤‡ä¸å¯åˆ†å‰²æ€§ä¸æ­£äº¤æ€§ï¼ŒåŠŸèƒ½æ— é‡å ï¼Œèƒ½è¦†ç›–çŸ¥è¯†å¯†é›†å‹æ¨ç†çš„å¤æ‚æµç¨‹ï¼Œä¸”ç»Ÿä¸€åº”ç”¨äºæ–‡æœ¬è¯­æ–™ï¼ˆTextï¼‰ã€åœ¨çº¿ç½‘é¡µï¼ˆWebï¼‰ã€ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸‰ç±»å¼‚æ„çŸ¥è¯†æºï¼Œè®©å¤§æ¨¡å‹èƒ½åŠ¨æ€æ“ä½œå¼‚æ„æºçš„åŸå­çº§çŸ¥è¯†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸå­æ¨ç†è§„åˆ’ä¸æ‰§è¡ŒåŒé˜¶æ®µæµç¨‹  
- **åŸå­æ¨ç†è§„åˆ’ï¼ˆAtomic Reasoning Planningï¼‰**ï¼šå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸º**åŸå­æ¨ç†æ ‘ï¼ˆARTï¼‰**ï¼Œæ ‘çš„æ¯ä¸ªå¶èŠ‚ç‚¹å¯¹åº”ä¸€ç§åŸå­çŸ¥è¯†ç®—å­ã€‚è¿™è¿«ä½¿å¤§æ¨¡å‹æŠŠé—®é¢˜åˆ†è§£åˆ°åŸå­ç²’åº¦ï¼Œå®ç°ç»†ç²’åº¦ä¸”åŠŸèƒ½æ­£äº¤çš„æ¨ç†è§„åˆ’ï¼Œè§£å†³æ¨ç†è§„åˆ’ä¸è¶³é—®é¢˜ã€‚  
- **åŸå­æ¨ç†æ‰§è¡Œï¼ˆAtomic Reasoning Executionï¼‰**ï¼šè‡ªåº•å‘ä¸Šé€’å½’æ‰§è¡Œæ¨ç†æ ‘ã€‚å¶èŠ‚ç‚¹æ‰§è¡Œæ—¶ï¼ŒåŠ¨æ€ä»å¤šå¼‚æ„æºé€‰ã€æ£€ç´¢å’Œæ“ä½œçŸ¥è¯†ï¼Œè§£å†³å¼‚æ„çŸ¥è¯†æ•´åˆéš¾é¢˜ï¼›éå¶èŠ‚ç‚¹åˆ™é€šè¿‡â€œå­ç­”æ¡ˆæ¨ç†ï¼ˆChild Answer Reasoningï¼‰â€æˆ–â€œç›´æ¥RAGæ¨ç†ï¼ˆDirect RAG Reasoningï¼‰â€ç”Ÿæˆè¾“å‡ºï¼Œå‰è€…æ— éœ€é¢å¤–æ£€ç´¢ï¼Œåè€…ä»…åœ¨å‰è€…å¤±è´¥æ—¶è§¦å‘ï¼Œå…¼é¡¾é²æ£’æ€§ä¸æˆæœ¬æ•ˆç‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºBlendQAåŸºå‡†æ•°æ®é›†  
é’ˆå¯¹å¼‚æ„çŸ¥è¯†æºä¼˜è´¨æ•°æ®é›†ç¼ºå¤±é—®é¢˜ï¼Œæ„å»ºäº†è·¨Textã€Webã€KGä¸‰æºçš„**BlendQA**åŸºå‡†ï¼Œä¸ºå¼‚æ„çŸ¥è¯†æ¨ç†ç ”ç©¶æä¾›æ›´å…·æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸‰ä¸ªå•æºæ•°æ®é›†å’Œä¸¤ä¸ªå¤šæºæ•°æ®é›†ï¼ˆå«è‡ªç ”BlendQAï¼‰ä¸Šå¼€å±•å®éªŒï¼ŒAtomRå¤§å¹…è¶…è¶Šç°æœ‰SOTAåŸºçº¿ã€‚å…¶ä¸­ï¼Œåœ¨2WikiMultihopæ•°æ®é›†ä¸ŠF1åˆ†æ•°æå‡9.4%ï¼Œåœ¨BlendQAæ•°æ®é›†ä¸ŠF1åˆ†æ•°æå‡9.5%ï¼Œå……åˆ†éªŒè¯äº†æ¡†æ¶åœ¨å¼‚æ„çŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **åŸå­åŒ–æ€è·¯**ï¼šå°†å¤æ‚æ¨ç†æ‹†è§£ä¸ºåŸå­çº§æ“ä½œï¼Œä¸ºå¤§æ¨¡å‹å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æä¾›äº†æ›´ç²¾ç»†çš„â€œæ“ä½œå•å…ƒâ€å‚è€ƒï¼Œå¯å¯å‘åç»­æ¨ç†æ¡†æ¶è®¾è®¡ä¸­å¯¹ä»»åŠ¡åˆ†è§£ç²’åº¦çš„æ€è€ƒã€‚  
2. **å¼‚æ„çŸ¥è¯†æ•´åˆ**ï¼šç»Ÿä¸€ç®—å­é€‚é…å¤šç±»å¼‚æ„çŸ¥è¯†æºï¼Œä¸ºè·¨æºçŸ¥è¯†æ£€ç´¢ä¸æ“ä½œæä¾›äº†é€šç”¨èŒƒå¼ï¼Œæœ‰åŠ©äºæ¨åŠ¨å¤šæºçŸ¥è¯†èåˆç±»ä»»åŠ¡çš„ç ”ç©¶ã€‚  
3. **åŸºå‡†æ•°æ®é›†æ„å»º**ï¼šé’ˆå¯¹é¢†åŸŸç—›ç‚¹æ„å»ºBlendQAï¼Œä¸ºå­¦ç•Œæä¾›äº†æ›´è´´åˆçœŸå®å¼‚æ„çŸ¥è¯†æ¨ç†åœºæ™¯çš„æµ‹è¯•åŸºå‡†ï¼Œå½°æ˜¾äº†å¡«è¡¥æ•°æ®ç©ºç™½å¯¹é¢†åŸŸå‘å±•çš„æ¨åŠ¨ä»·å€¼ã€‚  
4. **åˆ†å±‚æ‰§è¡Œç­–ç•¥**ï¼šæ¨ç†æ‰§è¡Œé˜¶æ®µçš„åˆ†å±‚ï¼ˆå¶èŠ‚ç‚¹ä¸éå¶èŠ‚ç‚¹ä¸åŒæ‰§è¡Œé€»è¾‘ï¼‰è®¾è®¡ï¼Œå¹³è¡¡äº†é²æ£’æ€§ä¸æˆæœ¬ï¼Œè¿™ç§â€œæŒ‰éœ€æ£€ç´¢â€æ€è·¯åœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„å¤§æ¨¡å‹åº”ç”¨ä¸­å€¼å¾—å€Ÿé‰´ã€‚  
```

## simpledeepsearcher--deep-information-seeking-via-web-powered-reasoning-trajectory-synthesis
### Abstract
Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | SimpleDeepSearcherï¼šç”¨ç½‘é¡µé©±åŠ¨çš„æ¨ç†è½¨è¿¹åˆæˆå®ç°æ·±åº¦ä¿¡æ¯æ£€ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æœºåˆ¶æå¤§å¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›ï¼Œåœ¨å¤æ‚æ·±åº¦æœç´¢åœºæ™¯ï¼ˆéœ€å¤šæ­¥æ¨ç†ä¸è¿­ä»£ä¿¡æ¯æ£€ç´¢ï¼‰ä¸­ä¹Ÿæœ‰è¿›å±•ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å…³é”®å±€é™ï¼šä¸€æ–¹é¢ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•å¤šåœ¨äººå·¥ç¯å¢ƒï¼ˆé™æ€æ–‡æ¡£åº“ï¼‰è¿è¡Œï¼Œä¸çœŸå®ç½‘ç»œåŠ¨æ€å­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œä¸”å’Œå®æ—¶æœç´¢APIäº¤äº’æ—¶è®¡ç®—æˆæœ¬æé«˜ï¼›å¦ä¸€æ–¹é¢ï¼Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è™½è®­ç»ƒæµç¨‹ç®€æ´ï¼Œä½†æ·±åº¦æœç´¢åœºæ™¯ä¸‹ç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®â€”â€”è¾“å…¥ä¾§ç°æœ‰QAæ•°æ®é›†ç¼ºå°‘ç½‘é¡µä¸Šé—®é¢˜çš„å¤šæ ·æ€§ä¸å¤æ‚æ€§ï¼Œè¾“å‡ºä¾§ä¼ ç»Ÿç­”æ¡ˆæ ‡æ³¨é—æ¼äº†æœç´¢é›†æˆæ¨ç†ç­–ç•¥æ‰€éœ€çš„å…³é”®æ¨ç†ç—•è¿¹ï¼ˆå¦‚æœç´¢æ“ä½œã€è¯æ®åˆæˆã€é«˜æ•ˆå†³ç­–è·¯å¾„ç­‰ï¼‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¸Œæœ›é€šè¿‡ç­–ç•¥æ€§çš„æ•°æ®å·¥ç¨‹è€Œéå¤æ‚è®­ç»ƒèŒƒå¼ï¼Œæ­å»ºè½»é‡ä¸”æœ‰æ•ˆçš„æ¡†æ¶SimpleDeepSearcheræ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºçœŸå®ç½‘é¡µæœç´¢ç¯å¢ƒçš„æ•°æ®åˆæˆæ¡†æ¶  
æ­å»ºäº†ä»¥çœŸå®ç½‘ç»œæœç´¢ç¯å¢ƒä¸ºåŸºç¡€çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·æœç´¢è¡Œä¸ºæ¥ç”Ÿæˆå¤šè½®æ¨ç†è½¨è¿¹ã€‚é€šè¿‡â€œæ¨ç† - æœç´¢ - æ€»ç»“ - ç”Ÿæˆâ€çš„è¿­ä»£å¾ªç¯ï¼Œåˆ©ç”¨å•†ä¸šæœç´¢APIç›´æ¥å¤„ç†åŸå§‹HTMLå†…å®¹ï¼Œæ•æ‰ä»ç»“æ„åŒ–æ•°æ®ç‰‡æ®µåˆ°éç»“æ„åŒ–å™äº‹è¯è¯­ç­‰å¤šæ ·çš„ç½‘ç»œä¿¡æ¯ç‰¹å¾ï¼Œè®©æ¨¡å‹èƒ½æ¥è§¦åˆ°çœŸå®çš„æœç´¢äº§ç‰©ä¸å™ªå£°æ¨¡å¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šå‡†åˆ™çš„æ•°æ®ç²¾é€‰ç­–ç•¥  
è®¾è®¡äº†å¤šå‡†åˆ™çš„æ•°æ®ç²¾é€‰ç­–ç•¥ï¼Œä»è¾“å…¥é—®é¢˜é€‰æ‹©å’Œè¾“å‡ºå“åº”è¿‡æ»¤ä¸¤æ–¹é¢ååŒä¼˜åŒ–ã€‚è¾“å…¥ä¾§ï¼Œç”¨é¢†åŸŸå¼‚è´¨æ€§ã€å…³é”®è¯å¤šæ ·æ€§ã€çŸ¥è¯†å•å…ƒå¤æ‚åº¦æ¥è¿‡æ»¤æŸ¥è¯¢ï¼Œæ„å»ºä¿¡æ¯ä¸°å¯Œçš„è®­ç»ƒåŸºç¡€ä¸”ç¡®ä¿è´´åˆçœŸå®ç½‘é¡µæœç´¢åœºæ™¯ï¼›è¾“å‡ºä¾§ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹åˆæˆçš„å“åº”æ–½åŠ å››ç»´è´¨é‡è¿‡æ»¤ï¼ˆæ ¼å¼æ ‡å‡†åŒ–ã€æ¨ç†è·¯å¾„æ§åˆ¶ã€é—®é¢˜éš¾åº¦ã€æœç´¢æœ‰æ•ˆæ€§ï¼‰ï¼Œä¿éšœå“åº”è´¨é‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªä¸åŒé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ç”¨871ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æ ·æœ¬è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒSimpleDeepSearcherå°±å±•ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ï¼šç›¸æ¯”åŸºäºæç¤ºçš„æ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†48.3%ï¼›ç›¸æ¯”åŸºäºå¼ºåŒ–å­¦ä¹ çš„RAGæ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†24.9%ã€‚è¿™è¡¨æ˜è¯¥æ¡†æ¶åœ¨æ€§èƒ½å’Œæ•ˆç‡é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ï¼Œèƒ½ä»¥ç®€å•å´æœ‰åŠ›çš„æ–¹å¼å¢å¼ºæ·±åº¦æœç´¢èƒ½åŠ›ï¼Œä¸”æ¡†æ¶æ‰©å±•æ€§å¼ºï¼Œè¿˜å¯ä¸å…¶ä»–ç±»å‹è®­ç»ƒæ•°æ®ç»“åˆï¼Œä¹Ÿé€‚ç”¨äºåŸºäºRLçš„è®­ç»ƒã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®å·¥ç¨‹æ€è·¯ï¼šå½“å¤æ‚è®­ç»ƒèŒƒå¼ï¼ˆå¦‚RLï¼‰é¢ä¸´æˆæœ¬ã€ç¯å¢ƒåŒ¹é…ç­‰é—®é¢˜æ—¶ï¼Œå¯ä»æ•°æ®å±‚é¢å…¥æ‰‹ï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®åœºæ™¯åˆæˆæ•°æ®ã€å¤šç»´åº¦ç²¾é€‰æ•°æ®æ¥ä¸ºæ¨¡å‹è®­ç»ƒæä¾›ä¼˜è´¨â€œå…»æ–™â€ï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºç“¶é¢ˆæä¾›äº†æ€è·¯ã€‚  
2. æ¨¡å—åŒ–è®¾è®¡ä¼˜åŠ¿ï¼šæ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡èƒ½è®©æ¨¡å‹æ¥è§¦çœŸå®æœç´¢ artifact å’Œå™ªå£°ï¼Œè¿˜èƒ½ä»¥æå°çš„SFTæ•°æ®é›†å®ç°ä¼˜å¼‚æ€§èƒ½ï¼Œæ‘†è„±å¯¹èµ„æºå¯†é›†å‹RLè®­ç»ƒçš„ä¾èµ–ï¼Œè¿™ç§è§£è€¦æ•°æ®åˆæˆä¸æ¨¡å‹çº¦æŸçš„æ¶æ„å€¼å¾—åœ¨ç±»ä¼¼éœ€å’Œå¤–éƒ¨ç¯å¢ƒäº¤äº’çš„ä»»åŠ¡ç³»ç»Ÿè®¾è®¡ä¸­å‚è€ƒã€‚  
3. å°æ ·æœ¬SFTä»·å€¼éªŒè¯ï¼šéªŒè¯äº†åœ¨æ·±åº¦æœç´¢ä»»åŠ¡ä¸­ï¼Œé«˜è´¨é‡å°æ ·æœ¬SFTä¹Ÿèƒ½è¶…è¶Šå¼ºåŸºçº¿ï¼ˆå°¤å…¶æ˜¯RLåŸºçº¿ï¼‰ï¼Œä¸ºåç»­åœ¨èµ„æºæœ‰é™åœºæ™¯ä¸‹æ¢ç´¢é«˜æ•ˆè®­ç»ƒæ–¹å¼æä¾›äº†å®è·µä¾æ®ã€‚  
```

## rescore--label-free-iterative-retriever-training-for-multi-hop-question-answering-with-relevance-consistency-supervision
### Abstract
Multi-hop question answering (MHQA) involves reasoning across multiple
documents to answer complex questions. Dense retrievers typically outperform
sparse methods like BM25 by leveraging semantic embeddings; however, they
require labeled query-document pairs for fine-tuning. This poses a significant
challenge in MHQA due to the high variability of queries (reformulated)
questions throughout the reasoning steps. To overcome this limitation, we
introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a
novel method for training dense retrievers for MHQA without labeled documents.
ReSCORE leverages large language models to capture each documents relevance to
the question and consistency with the correct answer and use them to train a
retriever within an iterative question-answering framework. Experiments on
three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with
significant improvements in retrieval, and in turn, the state-of-the-art MHQA
performance. Our implementation is available at:
https://leeds1219.github.io/ReSCORE.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReSCOREï¼šæ— æ ‡æ³¨æ–‡æ¡£ä¸‹å¤šè·³é—®ç­”çš„è¿­ä»£æ£€ç´¢å™¨è®­ç»ƒæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šè·³é—®ç­”ï¼ˆMHQAï¼‰éœ€è¦è·¨å¤šä¸ªæ–‡æ¡£æ¨ç†æ¥å›ç­”å¤æ‚é—®é¢˜ï¼Œå½“å‰ä¸»æµçš„å¯†é›†æ£€ç´¢å™¨è™½æ¯”BM25ç­‰ç¨€ç–æ–¹æ³•æ›´ä¼˜ï¼Œä½†éœ€æ ‡æ³¨çš„æŸ¥è¯¢ - æ–‡æ¡£å¯¹æ¥å¾®è°ƒã€‚è€ŒMHQAä¸­æŸ¥è¯¢ï¼ˆè¿­ä»£ä¸­çš„æ”¹å†™é—®é¢˜ï¼‰å˜å¼‚æ€§å¤§ï¼Œæ ‡æ³¨æˆæœ¬é«˜ã€éš¾åº¦å¤§ã€‚åŒæ—¶ï¼Œç°æœ‰å¤šè·³é—®ç­”ç³»ç»Ÿå¸¸ä¾èµ–ç¨€ç–æ£€ç´¢å™¨ï¼Œå¯†é›†æ£€ç´¢å™¨å› é¢†åŸŸé€‚é…éœ€æ ‡æ³¨æ•°æ®å—é™ï¼›ä¸”ç°æœ‰é’ˆå¯¹æ£€ç´¢å™¨è®­ç»ƒçš„æ–¹æ³•å¤šèšç„¦å•è·³ã€å¿½è§†è¿­ä»£æ¨ç†ä¸å¤šè·³åœºæ™¯ï¼Œè¿­ä»£RAGå·¥ä½œä¹Ÿæœªé‡è§†æ£€ç´¢å™¨è®­ç»ƒã€‚å› æ­¤ï¼Œéœ€ä¸€ç§æ— éœ€æ ‡æ³¨æ–‡æ¡£å°±èƒ½è®­ç»ƒå¤šè·³é—®ç­”å¯†é›†æ£€ç´¢å™¨çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReSCOREæ–¹æ³•
ReSCOREæ˜¯ä¸€ç§æ— éœ€æ ‡æ³¨æ–‡æ¡£è®­ç»ƒMHQAå¯†é›†æ£€ç´¢å™¨çš„æ–°æ–¹æ³•ã€‚å…¶æ ¸å¿ƒç›´è§‰æ˜¯æ–‡æ¡£å¯¹å›ç­”é—®é¢˜çš„é‡è¦æ€§ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»™å®šæ–‡æ¡£æ—¶ç”Ÿæˆé—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡æˆæ­£æ¯”ï¼Œä»¥æ­¤è”åˆå»ºæ¨¡æ–‡æ¡£ä¸ç­”æ¡ˆçš„ä¸€è‡´æ€§å’Œä¸é—®é¢˜çš„ç›¸å…³æ€§ï¼Œå¹¶å°†è¯¥æ¦‚ç‡ä½œä¸ºä¼ªæ ‡æ³¨ï¼ˆpseudo - GTï¼‰åœ¨è¿­ä»£RAGæ¡†æ¶ä¸­è®­ç»ƒæ£€ç´¢å™¨ã€‚é€šè¿‡LLMæ•æ‰æ–‡æ¡£å¯¹é—®é¢˜çš„ç›¸å…³æ€§å’Œä¸æ­£ç¡®ç­”æ¡ˆçš„ä¸€è‡´æ€§ï¼Œä¸ºæ— æ ‡æ³¨è®­ç»ƒæä¾›ç›‘ç£ä¿¡å·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºIQATRç³»ç»Ÿ
æå‡ºIterative Question Answerer with Trained Retrieverï¼ˆIQATRï¼‰å¤šè·³é—®ç­”ç³»ç»Ÿï¼Œå…¶æ£€ç´¢å™¨ç”±ReSCOREè®­ç»ƒè€Œæ¥ã€‚è¯¥ç³»ç»Ÿåœ¨è¿­ä»£RAGæ¡†æ¶ä¸‹è¿ä½œï¼Œé€šè¿‡è®­ç»ƒåçš„æ£€ç´¢å™¨è¿­ä»£æ£€ç´¢æ–‡æ¡£å¹¶ç”Ÿæˆç­”æ¡ˆï¼Œç›´è‡³å¾—åˆ°æœ€ç»ˆç­”æ¡ˆï¼Œå°†ReSCOREè®­ç»ƒçš„æ£€ç´¢å™¨èå…¥å®é™…å¤šè·³é—®ç­”ç³»ç»Ÿï¼ŒéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ·±å…¥åˆ†æä¼ªæ ‡æ³¨ä¸æŸ¥è¯¢æ”¹å†™
å¯¹ä¸åŒä¼ª - GTæ ‡ç­¾å’ŒæŸ¥è¯¢æ”¹å†™æ–¹æ³•çš„æ•ˆæœè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ¢ç©¶ä¸åŒå› ç´ å¯¹æ£€ç´¢å™¨è®­ç»ƒå’Œå¤šè·³é—®ç­”æ€§èƒ½çš„å½±å“ï¼Œä¸ºåç»­ç›¸å…³ç ”ç©¶æä¾›äº†åˆ†ææ€è·¯ä¸å‚è€ƒä¾æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸‰ä¸ªæµè¡Œçš„MHQAåŸºå‡†æ•°æ®é›†MuSiQueã€2WikiMHQAå’ŒHotpotQAä¸Šè¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒReSCOREç»“åˆä¸€è‡´æ€§å’Œç›¸å…³æ€§ä¸ºæ— æ ‡æ³¨æ–‡æ¡£è®­ç»ƒMHQAå¯†é›†æ£€ç´¢å™¨æä¾›äº†æœ‰æ•ˆç›‘ç£ã€‚ç»ReSCOREè®­ç»ƒçš„æ£€ç´¢å™¨ä¸ä»…æå‡äº†æ£€ç´¢è´¨é‡ï¼Œå½“é›†æˆåˆ°IQATRè¿­ä»£RAGæ¡†æ¶ä¸­æ—¶ï¼Œè¿˜åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†å½“å‰æœ€ä¼˜ï¼ˆSOTAï¼‰æ€§èƒ½ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨æ£€ç´¢å’Œé—®ç­”æ•´ä½“æµç¨‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ— ç›‘ç£/å¼±ç›‘ç£è®­ç»ƒæ€è·¯ï¼šåœ¨æ•°æ®æ ‡æ³¨æ˜‚è´µæˆ–å›°éš¾çš„ä»»åŠ¡ä¸­ï¼Œå¯å€Ÿé‰´ReSCOREåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆä¼ªæ ‡æ³¨æ¥è®­ç»ƒæ¨¡å‹çš„æ€è·¯ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚
2. è¿­ä»£æ¡†æ¶ä¸‹çš„ä»»åŠ¡é€‚é…ï¼šé’ˆå¯¹å¤šæ­¥éª¤ã€è¿­ä»£æ€§çš„ä»»åŠ¡ï¼ˆå¦‚å¤šè·³æ¨ç†ã€å¤šè½®å¯¹è¯ç­‰ï¼‰ï¼Œå¯å‚è€ƒReSCOREåœ¨è¿­ä»£RAGæ¡†æ¶å†…è®­ç»ƒæ£€ç´¢å™¨ä»¥é€‚é…ä»»åŠ¡ç‰¹æ€§çš„æ–¹å¼ï¼Œè®©æ¨¡å‹æ›´å¥½åœ°å¤„ç†ä»»åŠ¡ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚
3. å¤šå› ç´ åˆ†ææ–¹æ³•ï¼šæ–‡ä¸­å¯¹ä¼ªæ ‡æ³¨å’ŒæŸ¥è¯¢æ”¹å†™ç­‰å› ç´ çš„æ·±å…¥åˆ†æï¼Œä¸ºåç»­ç ”ç©¶ä¸­æ¢ç©¶ä¸åŒç»„ä»¶å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“æä¾›äº†èŒƒä¾‹ï¼Œæœ‰åŠ©äºæ›´å…¨é¢åœ°ç†è§£æ¨¡å‹å·¥ä½œæœºåˆ¶ä¸ä¼˜åŒ–æ–¹å‘ã€‚
```

## retrieve--summarize--plan--advancing-multi-hop-question-answering-with-an-iterative-approach
### Abstract
Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¿­ä»£å¼RAGæ–°èŒƒå¼ReSPï¼šè§£å†³å¤šè·³é—®ç­”éš¾é¢˜çš„é«˜æ•ˆè·¯å¾„

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šè·³é—®ç­”æ˜¯å¼€æ”¾åŸŸé—®ç­”ä¸­æå…·æŒ‘æˆ˜æ€§ä¸”æœ‰é‡è¦äº§ä¸šä»·å€¼çš„å­ä»»åŠ¡ï¼Œéœ€ç³»ç»Ÿæ•´åˆä¿¡æ¯å®Œæˆå¤šæ­¥æ¨ç†ã€‚åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸»æµè§£æ³•ï¼Œä½†ä¼ ç»Ÿå•è½®RAGå¤„ç†å¤šè·³é—®ç­”æ—¶ï¼Œå•æ¬¡æ£€ç´¢éš¾é›†é½æ‰€æœ‰å…³é”®ä¿¡æ¯ã€‚è¿­ä»£å¼RAGè™½é€šè¿‡å¤šè½®æ£€ç´¢+å­é—®é¢˜è§„åˆ’æå‡äº†æ€§èƒ½ï¼Œå´é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯å¤šè½®æ£€ç´¢å¯¼è‡´ä¸Šä¸‹æ–‡è¿‡è½½ï¼Œå¼•å…¥æ›´å¤šå™ªå£°ä¸”æ˜“è®©æ¨¡å‹é—æ¼å…³é”®ä¿¡æ¯ï¼›äºŒæ˜¯ç¼ºä¹æ£€ç´¢è½¨è¿¹è®°å½•ï¼Œå¼•å‘â€œè¿‡åº¦è§„åˆ’â€ï¼ˆä¿¡æ¯å¤Ÿäº†è¿˜ä¸åœè¿­ä»£ï¼‰å’Œâ€œé‡å¤è§„åˆ’â€ï¼ˆé‡å¤ç”Ÿæˆå·²æ£€ç´¢å­é—®é¢˜ï¼‰é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆå¤„ç†å¤šè½®æ£€ç´¢ä¿¡æ¯ã€ä¼˜åŒ–è¿­ä»£è¿‡ç¨‹ï¼Œæˆä¸ºè¿­ä»£å¼RAGçªç ´çš„å…³é”®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè¿­ä»£å¼RAGæ–¹æ³•ReSPï¼Œå¼•å…¥åŒåŠŸèƒ½ summarizer  
ReSPåœ¨è¿­ä»£RAGæ¡†æ¶ä¸­é›†æˆåŸºäºLLMçš„ summarizerï¼Œè¯¥ summarizer æ‰¿æ‹…åŒé‡ä»»åŠ¡ï¼šä¸€æ–¹é¢ä¸ºé¡¶å±‚ç›®æ ‡é—®é¢˜æ±‡æ€»ä½è¯ä¿¡æ¯ï¼Œå½¢æˆâ€œå…¨å±€è¯æ®è®°å¿†ï¼ˆglobal evidence memoryï¼‰â€ï¼›å¦ä¸€æ–¹é¢é’ˆå¯¹å½“å‰å­é—®é¢˜ï¼Œä»æ£€ç´¢æ–‡æ¡£ç”Ÿæˆå›åº”ï¼Œæ„å»ºâ€œå±€éƒ¨è·¯å¾„è®°å¿†ï¼ˆlocal pathway memoryï¼‰â€ã€‚é€šè¿‡åŒæ—¶å‹ç¼©é¢å‘é¡¶å±‚é—®é¢˜å’Œå½“å‰å­é—®é¢˜çš„ä¿¡æ¯ï¼Œæ—¢ç¼“è§£å¤šè½®æ£€ç´¢çš„ä¸Šä¸‹æ–‡è¿‡è½½ï¼Œåˆä¸ºåç»­è¿­ä»£æä¾›æ¸…æ™°çš„ä¿¡æ¯è½¨è¿¹å‚è€ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¼˜åŒ–è¿­ä»£é€»è¾‘ï¼Œé¿å…é‡å¤ä¸è¿‡åº¦è§„åˆ’  
æ¯æ¬¡è¿­ä»£å¼€å§‹æ—¶ï¼Œæ¨¡å‹ä¼šç»“åˆç§¯ç´¯çš„å…¨å±€è¯æ®è®°å¿†å’Œå±€éƒ¨è·¯å¾„è®°å¿†æ¥è¯„ä¼°ä¿¡æ¯æ˜¯å¦å……è¶³ã€‚è‹¥å……è¶³åˆ™ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼›è‹¥ä¸è¶³åˆ™ç”Ÿæˆæ–°å­é—®é¢˜ï¼Œä¸”ç¡®ä¿ä¸é‡å¤ç”Ÿæˆå·²æ£€ç´¢è¿‡çš„å­é—®é¢˜ã€‚è¿™ç§è®¾è®¡è®©è¿­ä»£è¿‡ç¨‹æ›´â€œèªæ˜â€ï¼Œæ—¢è§£å†³äº†å› æ— è½¨è¿¹è®°å½•å¯¼è‡´çš„é‡å¤è§„åˆ’ï¼Œä¹Ÿé¿å…äº†ä¿¡æ¯è¶³å¤Ÿå´ä¸åœè¿­ä»£çš„è¿‡åº¦è§„åˆ’é—®é¢˜ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šè·³é—®ç­”åŸºå‡†æ•°æ®é›†HotpotQAå’Œ2WikiMultihopQAä¸Šï¼ŒReSPå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼š  
- HotpotQA ä¸Šï¼ŒF1 åˆ†æ•°è¾ƒå½“å‰æœ€ä¼˜æ–¹æ³•æå‡ 4.1ï¼›  
- 2WikiMultihopQA ä¸Šï¼ŒF1 åˆ†æ•°è¾ƒå½“å‰æœ€ä¼˜æå‡ 4.4ï¼›  
æ­¤å¤–ï¼Œå¯¹æ¯”ç ”ç©¶éªŒè¯äº† ReSP å¯¹ä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–å…·å¤‡å‡ºè‰²çš„é²æ£’æ€§ï¼Œåœ¨ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦ä¸‹è¡¨ç°ç¨³å®šï¼Œè¿œè¶…å…¶ä»–RAGæ–¹æ³•ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŠŸèƒ½åˆ†è§£å¼çš„ä¿¡æ¯å‹ç¼©æ€è·¯ï¼šå°† summarizer åŠŸèƒ½æ‹†åˆ†ä¸ºå…¨å±€å’Œå±€éƒ¨ç»´åº¦ï¼Œä¸ºå¤„ç†å¤šè½®æ£€ç´¢ä¿¡æ¯æä¾›äº†åˆ†å±‚å‹ç¼©çš„æ–°èŒƒå¼ï¼Œå¯å¯å‘åç»­é•¿æ–‡æœ¬ã€å¤šè½®äº¤äº’åœºæ™¯ä¸‹çš„ä¿¡æ¯å¤„ç†æ–¹æ¡ˆï¼›  
2. è¿­ä»£è¿‡ç¨‹çš„è½¨è¿¹åŒ–ç®¡ç†ï¼šé€šè¿‡è®°å½•â€œå…¨å±€+å±€éƒ¨â€è®°å¿†æ¥å¼•å¯¼è¿­ä»£å†³ç­–ï¼Œæœ‰æ•ˆè§£å†³é‡å¤ä¸è¿‡åº¦è§„åˆ’ï¼Œè¿™ç§â€œè®°å¿†+çº¦æŸâ€çš„è¿­ä»£æ§åˆ¶é€»è¾‘ï¼Œå¯¹éœ€å¤šæ­¥æ¨ç†çš„ä»»åŠ¡ï¼ˆå¦‚å¤æ‚é—®ç­”ã€ä»»åŠ¡è§„åˆ’ï¼‰æœ‰å‚è€ƒä»·å€¼ï¼›  
3. äº§ä¸šåœºæ™¯è½åœ°æ½œåŠ›ï¼šå¤šè·³é—®ç­”åœ¨æ™ºèƒ½åŠ©æ‰‹ã€ç”Ÿæˆå¼æœç´¢å¼•æ“ç­‰é¢†åŸŸéœ€æ±‚å¼ºçƒˆï¼ŒReSP æå‡æ€§èƒ½ä¸é²æ£’æ€§çš„åŒæ—¶ï¼Œä¸ºå·¥ä¸šçº§å¤šè½®æ¨ç†åº”ç”¨æä¾›äº†æ›´å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚  
```

## rag-gym--systematic-optimization-of-language-agents-for-retrieval-augmented-generation
### Abstract
Retrieval-augmented generation (RAG) has shown great promise for
knowledge-intensive tasks and recently advanced with agentic RAG, where
language agents engage in multi-round interactions with external knowledge
sources for adaptive information retrieval. However, existing agentic RAG
methods often depend on ad-hoc prompt engineering and lack a unified
optimization framework. We introduce RAG-Gym, a comprehensive platform that
systematically explores three optimization dimensions: (1) prompt engineering,
(2) actor tuning, and (3) critic training. For prompt engineering, we propose
Re$^2$Search, a novel agent incorporating reasoning reflection that
significantly outperforms standard prompts. In actor tuning, we evaluate three
popular post-training algorithms with fine-grained process supervision and
identify direct preference optimization as the most effective. We further
demonstrate that a trained critic can enhance inference by selecting
higher-quality intermediate reasoning steps. Together, these findings lead to
the optimized Re$^2$Search++ agent, which surpasses most recent methods like
Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we
examine the impact of different reward sources and analyze scaling properties
in training and inference, offering practical insights for agentic RAG
optimization. The project homepage is available at https://rag-gym.github.io.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | RAG-Gymï¼šä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆæ‰“é€ è¯­è¨€æ™ºèƒ½ä½“çš„ç³»ç»Ÿä¼˜åŒ–å¹³å°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹çŸ¥è¯†å¯†é›†å‹é—®é¢˜æ—¶ï¼Œè‹¥ç¼ºä¹è¶³å¤Ÿæˆ–æœ€æ–°çš„é¢†åŸŸçŸ¥è¯†ï¼Œå®¹æ˜“ç»™å‡ºä¸å‡†ç¡®å›ç­”ç”šè‡³äº§ç”Ÿå¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ç»“åˆä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ç³»ç»Ÿçš„ç›¸å…³ä¿¡æ¯æ¥æ”¹å–„è¿™ä¸€æƒ…å†µï¼Œè€Œæ™ºèƒ½ä½“åŒ–çš„RAGï¼ˆagentic RAGï¼‰è®©è¯­è¨€æ™ºèƒ½ä½“ä¸å¤–éƒ¨çŸ¥è¯†æºå¤šè½®äº¤äº’ä»¥å®ç°è‡ªé€‚åº”ä¿¡æ¯æ£€ç´¢ï¼Œè¿›ä¸€æ­¥æå‡äº†æ•ˆæœã€‚ä½†ç°æœ‰agentic RAGæ–¹æ³•å­˜åœ¨ä¾èµ–ä¸´æ—¶promptå·¥ç¨‹ã€ç¼ºä¹ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶çš„é—®é¢˜ï¼›åŒæ—¶ï¼Œè™½æœ‰LLMåè®­ç»ƒç®—æ³•ï¼Œå´éš¾ç›´æ¥é€‚é…agentic RAGåŠ¨æ€è°ƒæ•´tokenç”Ÿæˆç­–ç•¥çš„éœ€æ±‚ï¼Œä¸”å¯¹ä¸­é—´æ­¥éª¤çš„ç»†ç²’åº¦ç›‘ç£ä¸è¶³ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ„å»ºç³»ç»Ÿæ¡†æ¶æ¥ä¼˜åŒ–agentic RAGã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRAG-Gymç»¼åˆå¹³å°  
RAG-Gymä»promptå·¥ç¨‹ã€actorè°ƒä¼˜ã€criticè®­ç»ƒä¸‰ä¸ªç»´åº¦ç³»ç»Ÿæ¢ç´¢agentic RAGçš„ä¼˜åŒ–ã€‚å°†çŸ¥è¯†å¯†é›†å‹é—®ç­”å»ºæ¨¡ä¸ºé«˜å±‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œæ˜ç¡®ä¸­é—´åŠ¨ä½œï¼Œä¸ºè¯­è¨€æ™ºèƒ½ä½“ä¼˜åŒ–æä¾›æ¨¡å—åŒ–æ–¹æ³•ï¼Œæ¶µç›–äº†æ™ºèƒ½ä½“ä¸IRç³»ç»Ÿäº¤äº’çš„çŠ¶æ€ã€åŠ¨ä½œã€ç¯å¢ƒå’Œå¥–åŠ±ç­‰è¦ç´ å®šä¹‰ï¼Œæ”¯æ’‘ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£ä¸ä¼˜åŒ–æ–¹æ³•çš„ç³»ç»Ÿè¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šPromptå·¥ç¨‹å±‚é¢æå‡ºReÂ²Searchæ™ºèƒ½ä½“  
è®¾è®¡äº†èå…¥æ¨ç†åæ€ï¼ˆreasoning reflectionï¼‰çš„ReÂ²Searchæ™ºèƒ½ä½“ï¼Œç›¸æ¯”æ ‡å‡†promptåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—è¶…è¶Šï¼Œè®©æ™ºèƒ½ä½“åœ¨ä¸IRç³»ç»Ÿäº¤äº’è¿‡ç¨‹ä¸­èƒ½æ›´æ™ºèƒ½åœ°ç”ŸæˆæŸ¥è¯¢ç­‰åŠ¨ä½œæ¥è¾…åŠ©å›ç­”é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šActorè°ƒä¼˜ä¸Criticè®­ç»ƒçš„æ¢ç´¢  
åœ¨actorè°ƒä¼˜ä¸­ï¼Œç”¨ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£è¯„ä¼°ä¸‰ç§æµè¡Œåè®­ç»ƒç®—æ³•ï¼Œå‘ç°ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰æ•ˆæœæœ€ä½³ï¼›åŒæ—¶è¯æ˜è®­ç»ƒcriticæ¥é€‰æ‹©æ›´é«˜è´¨é‡ä¸­é—´æ¨ç†æ­¥éª¤èƒ½æå‡æ¨ç†æ•ˆæœã€‚æ•´åˆè¿™äº›æˆæœå¾—åˆ°ä¼˜åŒ–åçš„ReÂ²Search++æ™ºèƒ½ä½“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä¼˜åŒ–åçš„ReÂ²Search++æ™ºèƒ½ä½“åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹³å‡F1ç›¸å¯¹æå‡3.2% - 11.6%ï¼Œåœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šç”šè‡³æœ‰8.5% - 24.7%çš„æå‡ï¼›æ­¤å¤–è¿˜åˆ†æäº†ä¸åŒå¥–åŠ±æºå½±å“ä»¥åŠè®­ç»ƒå’Œæ¨ç†æ—¶çš„ç¼©æ”¾ç‰¹æ€§ç­‰ï¼ŒéªŒè¯äº†å„ä¼˜åŒ–ç»´åº¦å’Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šç»´åº¦ç³»ç»Ÿä¼˜åŒ–æ€è·¯ï¼šä»promptã€actorã€criticä¸‰ä¸ªç»´åº¦ç³»ç»Ÿä¼˜åŒ–agentic RAGï¼Œä¸ºå¤æ‚æ™ºèƒ½ä½“ç³»ç»Ÿä¼˜åŒ–æä¾›äº†â€œåˆ†æ¨¡å—+ååŒâ€çš„èŒƒä¾‹ï¼Œå¯å¯å‘å…¶ä»–éœ€å¤šç¯èŠ‚åä½œçš„AIç³»ç»Ÿä¼˜åŒ–ã€‚  
2. ç»†ç²’åº¦ç›‘ç£ä»·å€¼ï¼šå¼ºè°ƒå¯¹agentic RAGä¸­é—´æ­¥éª¤çš„ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£ï¼Œè¯æ˜å…¶å¯¹æ€§èƒ½æå‡çš„é‡è¦æ€§ï¼Œåœ¨åç»­ç±»ä¼¼éœ€åŠ¨æ€äº¤äº’ã€å¤šæ­¥æ¨ç†çš„ä»»åŠ¡å‹æ™ºèƒ½ä½“ç ”å‘ä¸­ï¼Œå¯é‡è§†ä¸­é—´è¿‡ç¨‹çš„ç›‘ç£è®¾è®¡ã€‚  
3. ç®—æ³•é€‰å‹ä¸èåˆï¼šå¯¹ä¸åŒåè®­ç»ƒç®—æ³•åœ¨agentic RAGåœºæ™¯ä¸‹çš„æµ‹è¯•ä¸ä¼˜é€‰ï¼ˆå¦‚å‘ç°ç›´æ¥åå¥½ä¼˜åŒ–æ›´ä¼˜ï¼‰ï¼Œä»¥åŠcriticè®­ç»ƒè¾…åŠ©æ¨ç†çš„æ€è·¯ï¼Œä¸ºæ¨¡å‹è®­ç»ƒç­–ç•¥é€‰æ‹©å’Œæ¨¡å—é…åˆæä¾›äº†å®è·µå‚è€ƒã€‚ 
4. å®è·µæŒ‡å¯¼æ„ä¹‰ï¼šå¯¹å¥–åŠ±æºã€è®­ç»ƒä¸æ¨ç†ç¼©æ”¾ç‰¹æ€§çš„åˆ†æï¼Œç»™åç»­agentic RAGä¼˜åŒ–æä¾›äº†å¯è½åœ°çš„å®ç”¨æ´è§ï¼Œä¾¿äºç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆåœ¨å®é™…é¡¹ç›®ä¸­æƒè¡¡å†³ç­–ã€‚
```

## flashrag--a-modular-toolkit-for-efficient-retrieval-augmented-generation-research
### Abstract
With the advent of large language models (LLMs) and multimodal large language
models (MLLMs), the potential of retrieval-augmented generation (RAG) has
attracted considerable research attention. Various novel algorithms and models
have been introduced to enhance different aspects of RAG systems. However, the
absence of a standardized framework for implementation, coupled with the
inherently complex RAG process, makes it challenging and time-consuming for
researchers to compare and evaluate these approaches in a consistent
environment. Existing RAG toolkits, such as LangChain and LlamaIndex, while
available, are often heavy and inflexibly, failing to meet the customization
needs of researchers. In response to this challenge, we develop \ours{}, an
efficient and modular open-source toolkit designed to assist researchers in
reproducing and comparing existing RAG methods and developing their own
algorithms within a unified framework. Our toolkit has implemented 16 advanced
RAG methods and gathered and organized 38 benchmark datasets. It has various
features, including a customizable modular framework, multimodal RAG
capabilities, a rich collection of pre-implemented RAG works, comprehensive
datasets, efficient auxiliary pre-processing scripts, and extensive and
standard evaluation metrics. Our toolkit and resources are available at
https://github.com/RUC-NLPIR/FlashRAG.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | FlashRAGï¼šé«˜æ•ˆåŠ©åŠ›æ£€ç´¢å¢å¼ºç”Ÿæˆç ”ç©¶çš„æ¨¡å—åŒ–å·¥å…·åŒ…

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ—¶ä»£ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å‡­å€Ÿç¼“è§£æ¨¡å‹å¹»è§‰ç­‰ä¼˜åŠ¿æˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œä¼—å¤šæ–°ç®—æ³•å’Œæ¨¡å‹ä¸æ–­æ¶Œç°ä»¥ä¼˜åŒ–RAGç³»ç»Ÿä¸åŒç¯èŠ‚ã€‚ä½†å½“å‰å­˜åœ¨è¯¸å¤šé˜»ç¢ç ”ç©¶æ¨è¿›çš„é—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹æ ‡å‡†åŒ–å®ç°æ¡†æ¶ï¼ŒRAGæµç¨‹æœ¬èº«å¤æ‚ï¼Œå¯¼è‡´ç ”ç©¶è€…åœ¨ç»Ÿä¸€ç¯å¢ƒä¸‹å¯¹æ¯”è¯„ä¼°å„ç±»æ–¹æ³•å›°éš¾ä¸”è€—æ—¶ï¼›äºŒæ˜¯ç°æœ‰å·¥å…·åŒ…ï¼ˆå¦‚LangChainã€LlamaIndexï¼‰å¾€å¾€åšé‡ã€çµæ´»æ€§ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³ç ”ç©¶è€…å®šåˆ¶åŒ–éœ€æ±‚ï¼›ä¸‰æ˜¯å¾ˆå¤šæ–¹æ³•ä¸å¼€æºæˆ–éœ€ç‰¹æ®Šé…ç½®ï¼Œæ•°æ®é›†å’Œæ£€ç´¢è¯­æ–™é›¶æ•£ä¸”é¢„å¤„ç†ç¹çï¼ŒRAGç³»ç»Ÿæ¶‰åŠçš„ç´¢å¼•ã€æ£€ç´¢ã€ç”Ÿæˆç­‰ç¯èŠ‚æŠ€æœ¯å®ç°é—¨æ§›é«˜ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ä¸ªé¢å‘ç ”ç©¶ã€ç»Ÿä¸€ä¸”çµæ´»çš„RAGå·¥å…·åŒ…æ¥ç®€åŒ–æ–¹æ³•å¼€å‘ä¸å¯¹æ¯”ç ”ç©¶ï¼ŒFlashRAGåº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢çµæ´»çš„æ¨¡å—åŒ–RAGæ¡†æ¶  
FlashRAGåœ¨ç»„ä»¶å’Œ pipeline å±‚é¢å®ç°é«˜åº¦æ¨¡å—åŒ–ï¼ŒåŒ…å«5ä¸ªæ ¸å¿ƒæ¨¡å—ä¸16ä¸ªä¸åŒRAGå­ç»„ä»¶ï¼Œå„ç»„ä»¶å¯ç‹¬ç«‹é›†æˆæˆ–ç»„åˆæˆæµç¨‹ã€‚åŒæ—¶æä¾›9ç§æ ‡å‡†åŒ–RAGæµç¨‹å’Œè¾…åŠ©è„šæœ¬ï¼ˆå¦‚ç»´åŸºç™¾ç§‘ä¸‹è½½ã€åˆ†å—æ„å»ºè¯­æ–™åº“ã€æ„å»ºæ£€ç´¢ç´¢å¼•ã€å‡†å¤‡æ£€ç´¢ç»“æœç­‰ä»»åŠ¡ï¼‰ï¼Œæ‰“é€ é«˜æ•ˆä¸”æ˜“ç”¨çš„ç«¯åˆ°ç«¯RAGæ¡†æ¶ï¼Œè®©ç ”ç©¶è€…èƒ½æŒ‰éœ€å®šåˆ¶å·¥ä½œæµã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢„å®ç°å‰æ²¿RAGç®—æ³•å¹¶ç»Ÿä¸€è¯„ä¼°  
ç›®å‰FlashRAGå®ç°äº†16ç§å…ˆè¿›RAGç®—æ³•ï¼ˆå¦‚Self - RAGã€FLAREç­‰ï¼‰ï¼Œè¦†ç›–é¡ºåºã€æ¡ä»¶ã€åˆ†æ”¯ã€å¾ªç¯ç­‰RAGç±»åˆ«ã€‚è¿™äº›æ–¹æ³•åœ¨ç»Ÿä¸€æ¡†æ¶å†…è¯„ä¼°ä¸”æœ‰åŸºå‡†æŠ¥å‘Šï¼Œæ”¯æŒé€æ˜åŒ–çš„è¯„ä¼°ä¸å¯¹æ¯”ï¼Œåç»­è¿˜ä¼šæŒç»­çº³å…¥æ›´å¤šæ–¹æ³•ï¼Œä¸ºç ”ç©¶æä¾›ä¸°å¯Œä¸”è§„èŒƒçš„ç®—æ³•å‚è€ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¯æŒå¤šæ¨¡æ€RAGåœºæ™¯  
é›†æˆä¸»æµå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Qwenã€InternVLã€LLaVAï¼‰å’Œå¤šç§åŸºäºCLIPçš„æ£€ç´¢å™¨ï¼Œä¸ºç ”ç©¶è€…åœ¨çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€åœºæ™¯ä¸‹éƒ½æä¾›å¹¿æ³›æŠ€æœ¯æ”¯æŒï¼Œè¿˜é…å¤‡å¤šä¸ªå¸¸ç”¨MRAGåŸºå‡†æ•°æ®é›†ç”¨äºç³»ç»Ÿè¯„ä¼°ï¼Œæ‹“å±•äº†RAGç ”ç©¶çš„æ¨¡æ€è¾¹ç•Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šä¸°å¯Œè§„èŒƒçš„åŸºå‡†æ•°æ®é›†  
æ”¶é›†38ä¸ªå¸¸ç”¨æ•°æ®é›†å¹¶æ ‡å‡†åŒ–æ ¼å¼ï¼Œéƒ¨åˆ†æ•°æ®é›†ï¼ˆå¦‚WikiAspã€NarrativeQAï¼‰é’ˆå¯¹RAGåœºæ™¯åšç‰¹å®šè°ƒæ•´ä»¥ä¿è¯ä¸€è‡´æ€§ï¼Œä¸”è¿™äº›æ•°æ®é›†åœ¨HuggingFaceå¹³å°æ˜“è·å–ï¼Œæå‡RAGç ”ç©¶ä¸­æ•°æ®é›†çš„ä¸€è‡´æ€§ä¸å®ç”¨æ€§ï¼Œå‡å°‘ç ”ç©¶è€…æ•°æ®å‡†å¤‡æˆæœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šå¯è§†åŒ–ç½‘é¡µç•Œé¢è¾…åŠ©å®éªŒ  
å…·å¤‡ç›´è§‚çš„ç½‘é¡µç•Œé¢ï¼Œå¯å¯è§†åŒ–å®Œæ•´RAG pipelineã€‚ç”¨æˆ·èƒ½åœ¨æ£€ç´¢åˆ°ç­”æ¡ˆç”Ÿæˆå„æ­¥éª¤æŸ¥çœ‹ä¸­é—´ç»“æœï¼Œè¿˜èƒ½ä¸€é”®è°ƒå‚å’Œè‡ªåŠ¨åŸºå‡†è¯„ä¼°ï¼Œæ”¯æŒè¯­æ–™åŠ è½½ã€ç»„ä»¶å®æ—¶å¯è§†åŒ–ä¸ pipeline å…¨é¢è¯„ä¼°ï¼Œè®©RAGå®éªŒæ›´é€æ˜é«˜æ•ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­é€šè¿‡ä¸LangChainã€LlamaIndexã€Haystackç­‰ç°æœ‰å·¥å…·åŒ…ä»è‡ªåŠ¨è¯„ä¼°ã€å¤šæ¨¡æ€æ”¯æŒã€è¯­æ–™è¾…åŠ©ã€æä¾›æ•°æ®é›†æ•°é‡ã€æ”¯æŒæ–¹æ³•æ•°é‡ç­‰ç»´åº¦å¯¹æ¯”ï¼ˆå¦‚è¡¨1æ‰€ç¤ºï¼‰ï¼ŒFlashRAGåœ¨è‡ªåŠ¨è¯„ä¼°ï¼ˆæ”¯æŒï¼‰ã€å¤šæ¨¡æ€ï¼ˆæ”¯æŒï¼‰ã€è¯­æ–™è¾…åŠ©ï¼ˆæ”¯æŒï¼‰ã€æä¾›æ•°æ®é›†æ•°é‡ï¼ˆ38ä¸ªï¼‰ã€æ”¯æŒæ–¹æ³•æ•°é‡ï¼ˆ16ç§ï¼‰ç­‰æ–¹é¢å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œè¯æ˜å…¶åœ¨åŠŸèƒ½ä¸°å¯Œåº¦ã€å¯¹ç ”ç©¶æ”¯æŒçš„å…¨é¢æ€§ä¸Šè¿œè¶…åŒç±»å·¥å…·åŒ…ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å—åŒ–è®¾è®¡æ€è·¯ï¼šå°†å¤æ‚ç³»ç»Ÿæ‹†åˆ†ä¸ºå¯ç‹¬ç«‹ç»„åˆçš„æ¨¡å—ï¼Œæ—¢æ–¹ä¾¿ç ”ç©¶è€…ç†è§£å„ç¯èŠ‚åŠŸèƒ½ï¼Œåˆèƒ½å¿«é€Ÿå®šåˆ¶ç¬¦åˆéœ€æ±‚çš„å·¥ä½œæµï¼Œè¿™ç§è§£è€¦æ€æƒ³åœ¨å·¥å…·ç±»é¡¹ç›®å¼€å‘ä¸­å€¼å¾—å€Ÿé‰´ã€‚  
2. ç”Ÿæ€åŒ–èµ„æºæ•´åˆï¼šä¸ä»…å®ç°ç®—æ³•ï¼Œè¿˜æ•´åˆå¤§é‡æ•°æ®é›†å¹¶æ ‡å‡†åŒ–ï¼ŒåŒæ—¶æä¾›è¾…åŠ©è„šæœ¬ï¼Œä»ç ”ç©¶å…¨æµç¨‹é™ä½é—¨æ§›ï¼Œä¸ºé¢†åŸŸå·¥å…·æ„å»ºæä¾›äº†â€œç®—æ³• + æ•°æ® + å·¥å…·é“¾â€çš„å®Œæ•´ç”Ÿæ€å‚è€ƒã€‚  
3. å¤šæ¨¡æ€ä¸å¯è§†åŒ–æ–¹å‘ï¼šç´§è·Ÿå¤šæ¨¡æ€è¶‹åŠ¿æ”¯æŒMRAGï¼Œä¸”é€šè¿‡å¯è§†åŒ–ç•Œé¢é™ä½æŠ€æœ¯é—¨æ§›ã€æå‡å®éªŒé€æ˜åº¦ï¼Œåœ¨å·¥å…·æ˜“ç”¨æ€§å’Œå‰æ²¿æ–¹å‘é€‚é…æ€§ä¸Šç»™å‡ºäº†ä¼˜ç§€èŒƒä¾‹ï¼Œå¯¹æ‰“é€ ç”¨æˆ·å‹å¥½å‹ç ”ç©¶å·¥å…·å¾ˆæœ‰å¯å‘ã€‚  
4. ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼šä¸ºä¸åŒRAGç®—æ³•æ­å»ºç»Ÿä¸€è¯„ä¼°ç¯å¢ƒå¹¶ç”ŸæˆåŸºå‡†æŠ¥å‘Šï¼Œè®©æ–¹æ³•é—´å¯¹æ¯”å…¬å¹³é€æ˜ï¼Œè¿™ç§æ¨åŠ¨é¢†åŸŸç ”ç©¶æ ‡å‡†åŒ–çš„åšæ³•ï¼Œæœ‰åŠ©äºåŠ é€Ÿé¢†åŸŸå†…çŸ¥è¯†ç§¯ç´¯ä¸æ–¹æ³•è¿­ä»£ã€‚ 
```

