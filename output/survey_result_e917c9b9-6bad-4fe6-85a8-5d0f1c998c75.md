# Paper List of Terms(Vision Language Model+Game)
- [25/06] **An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models**  
[[Paper](http://arxiv.org/pdf/2506.09172v2)] [[Code/Page]()] [[TLDR/Notes](#an-open-source-software-toolkit-&-benchmark-suite-for-the-evaluation-and-adaptation-of-multimodal-action-models)]

- [25/06] **PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation**  
[[Paper](http://arxiv.org/pdf/2506.01091v1)] [[Code/Page]()] [[TLDR/Notes](#promptvfx--text-driven-fields-for-open-world-3d-gaussian-animation)]

- [25/05] **Vision Language Models are Biased**  
[[Paper](http://arxiv.org/pdf/2505.23941v1)] [[Code/Page]()] [[TLDR/Notes](#vision-language-models-are-biased)]

- [25/05] **VideoGameBench: Can Vision-Language Models complete popular video games?**  
[[Paper](http://arxiv.org/pdf/2505.18134v2)] [[Code/Page]()] [[TLDR/Notes](#videogamebench--can-vision-language-models-complete-popular-video-games-)]

- [25/05] **VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance**  
[[Paper](http://arxiv.org/pdf/2505.15952v1)] [[Code/Page](https://asgaardlab.github.io/videogameqa-bench/)] [[TLDR/Notes](#videogameqa-bench--evaluating-vision-language-models-for-video-game-quality-assurance)]

- [25/05] **Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.13886v1)] [[Code/Page](https://github.com/tongjingqi/Code2Logic.)] [[TLDR/Notes](#code2logic--game-code-driven-data-synthesis-for-enhancing-vlms-general-reasoning)]

- [25/05] **G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.13426v1)] [[Code/Page](https://github.com/chenllliang/G1)] [[TLDR/Notes](#g1--bootstrapping-perception-and-reasoning-abilities-of-vision-language-model-via-reinforcement-learning)]

- [25/05] **DSADF: Thinking Fast and Slow for Decision Making**  
[[Paper](http://arxiv.org/pdf/2505.08189v1)] [[Code/Page]()] [[TLDR/Notes](#dsadf--thinking-fast-and-slow-for-decision-making)]

- [25/05] **Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.03792v2)] [[Code/Page](https://github.com/langfengQ/CoSo.)] [[TLDR/Notes](#towards-efficient-online-tuning-of-vlm-agents-via-counterfactual-soft-reinforcement-learning)]

- [25/04] **Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference**  
[[Paper](http://arxiv.org/pdf/2504.09620v1)] [[Code/Page]()] [[TLDR/Notes](#metropolis-hastings-captioning-game--knowledge-fusion-of-vision-language-models-via-decentralized-bayesian-inference)]



# TLDR/Notes
## an-open-source-software-toolkit-&-benchmark-suite-for-the-evaluation-and-adaptation-of-multimodal-action-models
### Abstract
Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MultiNetï¼šå¤šæ¨¡æ€åŠ¨ä½œæ¨¡å‹è¯„ä¼°ä¸é€‚é…çš„å¼€æºå·¥å…·é›†ä¸åŸºå‡†å¥—ä»¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸï¼Œå¤šæ¨¡æ€åŠ¨ä½œæ¨¡å‹ï¼ˆç»“åˆè§†è§‰ç†è§£ã€è¯­è¨€ç†è§£ä¸åŠ¨ä½œç”Ÿæˆï¼‰æ˜¯æ„å»ºé€šç”¨æ™ºèƒ½ä½“ç³»ç»Ÿçš„é‡è¦æ–¹å‘ã€‚ç„¶è€Œå½“å‰è¿™ç±»æ¨¡å‹å­˜åœ¨è¯¸å¤šå±€é™ï¼šä¸€æ–¹é¢ï¼Œç°æœ‰ Vision - Language - Actionï¼ˆVLAï¼‰æ¨¡å‹å¤šé’ˆå¯¹æœºå™¨äººæ“ä½œç­‰ç‹­çª„é¢†åŸŸè®¾è®¡ä¸è¯„ä¼°ï¼Œåœ¨å¤šæ¨¡æ€è¯­è¨€ç†è§£ä»»åŠ¡å’Œæ›´å¹¿æ³›æ³›åŒ–èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ç¼ºä¹éªŒè¯ï¼›å¦ä¸€æ–¹é¢ï¼Œç¤¾åŒºç¼ºå°‘ä¸“é—¨ä¸ºè¿™ç±»é€šç”¨æ¨¡å‹å¤§è§„æ¨¡è®­ç»ƒä¸å…¨é¢è¯„ä¼°æ‰“é€ çš„å¼€æºåŸºå‡†ï¼Œå¤šæ•°åŸºå‡†èšç„¦ç‰¹å®šé¢†åŸŸä¸”é—­æºã€‚å› æ­¤ï¼Œæ‰“é€ èƒ½æ¨åŠ¨é€šç”¨åŠ¨ä½œæ¨¡å‹å‘å±•ä¸è¯„ä¼°çš„ç”Ÿæ€ç³»ç»Ÿæˆä¸ºè¿«åˆ‡éœ€æ±‚ï¼Œè¿™ä¹Ÿæ­£æ˜¯ MultiNet è¯ç”Ÿçš„åŠ¨æœºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤§è§„æ¨¡é€šç”¨æ•°æ®é›†å‘å¸ƒ
æ•´åˆäº†è§†è§‰ - è¯­è¨€ç†è§£ã€è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ ã€æœºå™¨äººå­¦ç­‰å¤šé¢†åŸŸçš„ä¸°å¯Œæ•°æ®æºï¼Œæ¶µç›– OBELICSï¼ˆè¶…å¤§è§„æ¨¡å›¾æ–‡ç½‘é¡µæ•°æ®ï¼‰ã€DataComp - 1Bï¼ˆå›¾åƒ - æ–‡æœ¬å¯¹ï¼‰ã€å„ç±» VQA æ•°æ®é›†ï¼ˆå¦‚ A - OKVQAã€VQA - V2 ç­‰ï¼‰ï¼Œè¿˜æœ‰å¼ºåŒ–å­¦ä¹ ä¸æœºå™¨äººä»»åŠ¡ç›¸å…³æ•°æ®é›†ï¼ˆå¦‚ DM Labã€ALE Atariã€Meta - World ç­‰ï¼‰ä»¥åŠè¯­è¨€èƒ½åŠ›è®­ç»ƒè¯„ä¼°æ•°æ®é›†ï¼ˆå¦‚ Fineweb - eduã€HellaSwag ç­‰ï¼‰ï¼Œä¸ºé€šç”¨æ¨¡å‹è®­ç»ƒè¯„ä¼°æä¾›æµ·é‡å¤šå…ƒæ•°æ®æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€æºæ•°æ®å¤„ç† SDK æä¾›
æ¨å‡ºå¼€æºè½¯ä»¶å·¥å…·åŒ…ï¼Œæ–¹ä¾¿ç”¨æˆ·è·å–æ•´åˆåçš„æ•°æ®é›†ï¼Œè¿˜èƒ½å°†æ¥è‡ªä¼—å¤šæ¥æºçš„æ§åˆ¶æ•°æ®ï¼ˆå¼ºåŒ–å­¦ä¹ å’Œæœºå™¨äººå­¦æ•°æ®ï¼‰æ ‡å‡†åŒ–ä¸ºé€šç”¨æ˜“è®¿é—®æ ¼å¼ï¼Œé™ä½æ•°æ®ä½¿ç”¨é—¨æ§›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶æ­å»º
æ„å»ºæ ‡å‡†åŒ–ä¸”åˆç†çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…å«æµ‹è¯•åˆ†å‰²ä¸ç²¾å¿ƒè®¾è®¡çš„æŒ‡æ ‡ï¼Œèƒ½è®©ç¤¾åŒºä¾¿æ·è¯„ä¼°å‰æ²¿ Vision - Language Modelsï¼ˆVLMsï¼‰å’Œ VLA æ¨¡å‹åœ¨ç†Ÿæ‚‰ä¸æ–°é¢–é¢†åŸŸï¼ˆå¦‚çœŸå®æœºå™¨äººä»»åŠ¡ã€ç¨‹åºç”Ÿæˆæ¸¸æˆç¯å¢ƒï¼‰çš„æ³›åŒ–èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¼€æºé€‚é…å‰æ²¿æ¨¡å‹
å¯¹æœ€å…ˆè¿›çš„ VLMs å’Œ VLAs è¿›è¡Œå¼€æºé€‚é…ï¼Œä½¿å®ƒä»¬èƒ½åœ¨ MultiNet çš„æ•°æ®æ ¼å¼å’Œå¤šæ ·é¢†åŸŸï¼ˆç”šè‡³è®­ç»ƒæ—¶æœªè§è¿‡çš„é¢†åŸŸï¼‰æœ‰æ•ˆè¿è¡Œï¼ŒåŠ é€Ÿé€šç”¨ AI ç³»ç»Ÿæ„å»ºè¿›ç¨‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šæ·±åº¦å®éªŒä¸åˆ†æå¼€å±•
åˆ©ç”¨ MultiNet çš„åŸºå‡†ã€æ¡†æ¶ã€è¯„ä¼°å·¥å…·å’Œæ¨¡å‹é€‚é…èƒ½åŠ›ï¼Œå¯¹ä¸»æµ VLMsã€VLAs å’Œæ–°å…´é€šç”¨æ¨¡å‹æ€§èƒ½è¿›è¡Œè¯„ä¼°ä¸åˆ†æï¼Œä¸ºæ¨¡å‹æ”¹è¿›æä¾›ä¾æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªè¯¦ç»†å±•å¼€å®éªŒç»“æœæ•°å€¼å‘ˆç°ï¼Œä½†æåˆ° MultiNet çš„åŸºå‡†ã€æ¡†æ¶ã€å·¥å…·åŒ…å’Œè¯„ä¼° harness å·²è¢«ç”¨äº VLA æ³›åŒ–å±€é™æ€§çš„ä¸‹æ¸¸ç ”ç©¶ï¼Œè¯´æ˜å…¶åœ¨æ¨åŠ¨å¤šæ¨¡æ€åŠ¨ä½œæ¨¡å‹ç ”ç©¶ä¸Šå·²å…·å¤‡å®é™…åº”ç”¨ä»·å€¼ï¼Œèƒ½ä¸ºç›¸å…³ç ”ç©¶æä¾›æœ‰æ•ˆæ”¯æ’‘ä»¥æŒ–æ˜æ¨¡å‹æ€§èƒ½ä¸ä¸è¶³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®ç”Ÿæ€æ„å»ºæ€è·¯ï¼šé€šè¿‡æ•´åˆå¤šé¢†åŸŸå¤šæ¨¡æ€æ•°æ®æ‰“é€ å¤§è§„æ¨¡é€šç”¨æ•°æ®é›†ï¼Œä¸ºé€šç”¨ AI æ¨¡å‹è®­ç»ƒæä¾›å……è¶³â€œå…»åˆ†â€ï¼Œè¿™ç§è·¨é¢†åŸŸæ•°æ®èšåˆæ€è·¯å€¼å¾—åœ¨éœ€è¦å¤šå…ƒæ•°æ®æ”¯æ’‘çš„ç ”ç©¶ä¸­å€Ÿé‰´ã€‚
2. å¼€æºå·¥å…·é“¾æ‰“é€ ï¼šä»æ•°æ®è·å–ã€å¤„ç†åˆ°æ¨¡å‹é€‚é…ã€è¯„ä¼°éƒ½æä¾›å¼€æºå·¥å…·ï¼Œé™ä½ç ”ç©¶é—¨æ§›ï¼Œæ¨åŠ¨ç¤¾åŒºåä½œï¼Œè¿™ç§å¼€æºç”Ÿæ€æ„å»ºæ–¹å¼å¯¹é¢†åŸŸå‘å±•æœ‰å¾ˆå¼ºæ¨åŠ¨ä½œç”¨ï¼Œå…¶ä»–é¢†åŸŸæ‰“é€ ç ”ç©¶åŸºç¡€è®¾æ–½æ—¶å¯å‚è€ƒã€‚
3. æ ‡å‡†åŒ–è¯„ä¼°ä½“ç³»ï¼šå»ºç«‹æ ‡å‡†åŒ–è¯„ä¼°åè®®ä¸æ–¹æ³•ï¼Œè®©ä¸åŒæ¨¡å‹åœ¨ç»Ÿä¸€æ ‡å‡†ä¸‹å¯¹æ¯”ï¼Œåˆ©äºæ¸…æ™°è®¤çŸ¥æ¨¡å‹èƒ½åŠ›è¾¹ç•Œï¼Œåœ¨éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå…¬å¹³å¯¹æ¯”è¯„ä¼°çš„åœºæ™¯ä¸­ï¼Œè¿™ç§æ ‡å‡†åŒ–æ€è·¯å¾ˆå…³é”®ã€‚

## promptvfx--text-driven-fields-for-open-world-3d-gaussian-animation
### Abstract
Visual effects (VFX) are key to immersion in modern films, games, and AR/VR.
Creating 3D effects requires specialized expertise and training in 3D animation
software and can be time consuming. Generative solutions typically rely on
computationally intense methods such as diffusion models which can be slow at
4D inference. We reformulate 3D animation as a field prediction task and
introduce a text-driven framework that infers a time-varying 4D flow field
acting on 3D Gaussians. By leveraging large language models (LLMs) and
vision-language models (VLMs) for function generation, our approach interprets
arbitrary prompts (e.g., "make the vase glow orange, then explode") and
instantly updates color, opacity, and positions of 3D Gaussians in real time.
This design avoids overheads such as mesh extraction, manual or physics-based
simulations and allows both novice and expert users to animate volumetric
scenes with minimal effort on a consumer device even in a web browser.
Experimental results show that simple textual instructions suffice to generate
compelling time-varying VFX, reducing the manual effort typically required for
rigging or advanced modeling. We thus present a fast and accessible pathway to
language-driven 3D content creation that can pave the way to democratize VFX
further.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PromptVFXï¼šæ–‡æœ¬é©±åŠ¨çš„å¼€æ”¾ä¸–ç•Œ3Dé«˜æ–¯åŠ¨ç”»ç‰¹æ•ˆæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°ä»£ç”µå½±ã€æ¸¸æˆå’ŒAR/VRé¢†åŸŸï¼Œè§†è§‰ç‰¹æ•ˆï¼ˆVFXï¼‰æ˜¯æ²‰æµ¸æ„Ÿçš„å…³é”®ã€‚ä½†ä¼ ç»Ÿåˆ›å»º3Dç‰¹æ•ˆéœ€ä¸“ä¸š3DåŠ¨ç”»è½¯ä»¶çŸ¥è¯†ä¸è®­ç»ƒï¼Œè€—æ—¶è´¹åŠ›ã€‚ç”Ÿæˆå¼æ–¹æ¡ˆå¸¸ä¾èµ–å¦‚æ‰©æ•£æ¨¡å‹è¿™ç±»è®¡ç®—å¯†é›†å‹æ–¹æ³•ï¼Œåœ¨4Dæ¨ç†æ—¶é€Ÿåº¦æ…¢ã€‚åŒæ—¶ï¼Œç°æœ‰ç»“åˆå¤§æ¨¡å‹çš„æ–¹æ¡ˆä»ä¾èµ–ç‰©ç†æ¨¡æ‹Ÿæˆ–æ‰©æ•£è®­ç»ƒå¾ªç¯ï¼Œæ— æ³•å®æ—¶ç¼–è¾‘ã€‚ä¸ºçœŸæ­£æ™®åŠ3DåŠ¨ç”»ï¼Œéœ€ä¸€ä¸ªå¿«é€Ÿç›´è§‚ã€æ”¯æŒå¼€æ”¾å¼æ–‡æœ¬æŒ‡ä»¤ä¸”èƒ½å³æ—¶åé¦ˆçš„ç³»ç»Ÿï¼ŒPromptVFXåº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†3DåŠ¨ç”»é‡æ„ä¸ºåœºé¢„æµ‹ä»»åŠ¡  
PromptVFXæŠŠ3DåŠ¨ç”»é‡æ–°å®šä¹‰ä¸ºå¯¹3Dé«˜æ–¯æº…å°„ï¼ˆGaussian Splattingï¼‰çš„æ—¶å˜å˜æ¢ä»»åŠ¡ï¼Œæå‡ºæ–‡æœ¬é©±åŠ¨æ¡†æ¶ï¼Œæ¨æ–­ä½œç”¨äº3Dé«˜æ–¯çš„æ—¶å˜4Dæµåœºã€‚æ— éœ€ç”Ÿæˆæ–°å‡ ä½•æˆ–å¯åŠ¨ç‰©ç†æ¨¡æ‹Ÿå™¨ï¼Œé€šè¿‡å¯¹é«˜æ–¯æº…å°„åœºæ™¯æ–½åŠ æ—¶å˜åœºï¼Œç›´æ¥æ”¹å˜å•ä¸ªé«˜æ–¯çš„ä¸­å¿ƒä½ç½®ã€é¢œè‰²å’Œä¸é€æ˜åº¦ç­‰å‚æ•°æ¥å®ç°ç‰¹æ•ˆï¼Œè‡ªç„¶æ”¯æŒå®æ—¶æ¸²æŸ“ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— è®­ç»ƒçš„4DåŠ¨ç”»ç”Ÿæˆæµç¨‹  
é‡‡ç”¨é›¶æ ·æœ¬å·¥ä½œæµï¼Œå€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£æè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚æŠŠè‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºåŠ¨ç”»é˜¶æ®µï¼Œç”±LLMç”Ÿæˆå˜æ¢å‡½æ•°ï¼Œå†ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åé¦ˆä¼˜åŒ–ï¼Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯è®­ç»ƒï¼Œå°±èƒ½å®ç°å¦‚ä¾æ®â€œè®©èŠ±ç“¶å…ˆæ©™è‰²å‘å…‰å†çˆ†ç‚¸â€è¿™ç±»æŒ‡ä»¤çš„åŠ¨ç”»æ•ˆæœã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå®æ—¶äº¤äº’ç¼–è¾‘ä½“éªŒ  
ç›´æ¥æ›´æ–°é«˜æ–¯å‚æ•°ï¼Œçœå»äº†ç½‘æ ¼æå–ã€æ‰©æ•£å¾ªç¯æˆ–ç‰©ç†æ¨¡æ‹Ÿå™¨ç­‰å¼€é”€ã€‚åœ¨æ¶ˆè´¹çº§è®¾å¤‡ç”šè‡³ç½‘é¡µæµè§ˆå™¨ä¸Šï¼Œèƒ½åœ¨ä¸€åˆ†é’Ÿå†…å®ŒæˆåŠ¨ç”»ç”Ÿæˆï¼Œè®©ç”¨æˆ·å³æ—¶çœ‹åˆ°æ–‡æœ¬æŒ‡ä»¤å¸¦æ¥çš„æ•ˆæœï¼Œæ— è®ºæ–°æ‰‹è¿˜æ˜¯ä¸“å®¶éƒ½èƒ½è½»æ¾ä¸ºä½“ç´ åœºæ™¯åˆ¶ä½œåŠ¨ç”»ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒå±•ç¤ºäº†å¯¹ä¸åŒåœºæ™¯å’Œç”¨æˆ·æŒ‡ä»¤çš„å¤„ç†æ•ˆæœï¼Œç®€å•æ–‡æœ¬æŒ‡ä»¤å°±èƒ½ç”Ÿæˆå¼•äººå…¥èƒœçš„æ—¶å˜VFXã€‚ä¸ç°æœ‰æ–‡æœ¬é©±åŠ¨3DåŠ¨ç”»æ–¹æ³•å¯¹æ¯”ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ï¼ŒPromptVFXèƒ½åœ¨æ•°ç§’å†…ç”Ÿæˆé«˜è´¨é‡åŠ¨ç”»ï¼Œè€Œå…¶ä»–ä¾èµ–æ‰©æ•£æˆ–ç‰©ç†æ¨¡æ‹Ÿçš„æ–¹æ³•éœ€å¤š30å€æ—¶é—´ã€‚åŒæ—¶ï¼Œåœ¨æ¶µç›–è¾“å…¥è¾“å‡ºã€ç¼–è¾‘èƒ½åŠ›ã€ç³»ç»Ÿå±æ€§ç­‰å¤šç»´åº¦çš„æ–¹æ³•å¯¹æ¯”è¡¨ï¼ˆè¡¨1ï¼‰ä¸­ï¼ŒPromptVFXæ˜¯å”¯ä¸€åŒæ—¶æ»¡è¶³æ‰€æœ‰ç‰¹æ€§ä¸”æ”¯æŒç”¨æˆ·äº¤äº’ç»†åŒ–çš„æ–¹æ³•ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æŠ€æœ¯åˆ›æ–°çœ‹ï¼Œå…¶å°†3DåŠ¨ç”»ä¸åœºé¢„æµ‹ç»“åˆã€åˆ©ç”¨å¤§æ¨¡å‹å®ç°é›¶æ ·æœ¬4DåŠ¨ç”»ç”Ÿæˆçš„æ€è·¯ï¼Œä¸º3Då†…å®¹ç”Ÿæˆé¢†åŸŸæä¾›äº†æ–°èŒƒå¼ï¼Œæ‘†è„±å¯¹ä¼ ç»Ÿè€—æ—¶æ¨¡æ‹Ÿå’Œè®­ç»ƒçš„ä¾èµ–ï¼›ä»åº”ç”¨æ™®åŠè§’åº¦ï¼Œå®æ—¶äº¤äº’ã€å¼€æ”¾å¼æ–‡æœ¬æŒ‡ä»¤çš„è®¾è®¡ï¼Œé™ä½äº†3DåŠ¨ç”»åˆ›ä½œé—¨æ§›ï¼Œä¸ºVFXæ°‘ä¸»åŒ–é“ºè·¯ï¼Œè®©æ›´å¹¿æ³›å—ä¼—èƒ½å‚ä¸3Då†…å®¹åˆ›ä½œï¼›åœ¨å·¥ç¨‹å®ç°ä¸Šï¼Œç›´æ¥æ“ä½œé«˜æ–¯å‚æ•°å®ç°å®æ—¶æ¸²æŸ“çš„æ–¹å¼ï¼Œä¸ºå®æ—¶å›¾å½¢ç¼–è¾‘ç±»åº”ç”¨æä¾›äº†é«˜æ•ˆäº¤äº’çš„å‚è€ƒæ–¹å‘ã€‚ 

## vision-language-models-are-biased
### Abstract
Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åè§ï¼šVLMsåœ¨å®¢è§‚è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ç¼ºé™·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»äº’è”ç½‘ä¸­è®°å¿†äº†æµ·é‡å…ˆéªŒçŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†è™½æœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´è¾“å‡ºå­˜åœ¨é”™è¯¯æˆ–åè§ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å› é¢„è®­ç»ƒæ–¹å¼ï¼ˆåŸºäºçº¯æ–‡æœ¬æˆ–å¤šæ¨¡æ€æ•°æ®ï¼‰ï¼Œå¯èƒ½ä»æ–‡æœ¬è¯­æ–™ç»§æ‰¿å¼ºåè§ï¼Œè€Œæ­¤å‰å…³äºVLMåè§çš„ç ”ç©¶å¤šé’ˆå¯¹å«åå‘æ€§é™ˆè¿°çš„äººå·¥æ˜¯éé¢˜ï¼Œæœªæ˜ç¡®å›¾åƒå’Œæ–‡æœ¬æç¤ºå¯¹é”™è¯¯ç­”æ¡ˆçš„å½±å“ï¼Œä¹Ÿæœªæ¢ç©¶åè§å¯¹å®¢è§‚è§†è§‰ä»»åŠ¡çš„ä½œç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è¯„ä¼°LLMså¯¹å¸¸è§ä¸»é¢˜çš„çŸ¥è¯†å¦‚ä½•ç»™VLMsåœ¨ç›®æ ‡è§†è§‰é—®é¢˜ï¼ˆè®¡æ•°ã€è¯†åˆ«ç­‰ï¼‰ä¸Šçš„å‡†ç¡®æ€§å¸¦æ¥è´Ÿé¢å½±å“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºVLMBiasæ¡†æ¶  
åˆ©ç”¨å‰æ²¿å›¾åƒç¼–è¾‘å™¨ã€VLMså’Œå›¾åƒå¤„ç†åº“ï¼Œè‡ªåŠ¨åŒ–æšä¸¾å’Œç”Ÿæˆå¸¦åè§çš„ä¸»é¢˜ã€é—®é¢˜ä¸åäº‹å®å›¾åƒï¼Œæ¶µç›–åŠ¨ç‰©ã€æ ‡å¿—ã€æ——å¸œã€å›½é™…è±¡æ£‹æ£‹å­ã€æ¸¸æˆæ£‹ç›˜ã€è§†é”™è§‰ã€å›¾æ¡ˆç½‘æ ¼7ä¸ªé¢†åŸŸï¼Œè¿˜æ‰‹åŠ¨å®¡æŸ¥å›¾åƒä¿è¯è´¨é‡ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢æµ‹è¯•å¤šç±»è§†è§‰ä»»åŠ¡ä¸VLMs  
é’ˆå¯¹è®¡æ•°ã€è¯†åˆ«ç­‰å®¢è§‚è§†è§‰ä»»åŠ¡ï¼Œæµ‹è¯•5ä¸ªä¸»æµVLMsï¼ˆå¦‚Gemini - 2.5 Proã€GPT - 4.1ç­‰ï¼‰ï¼Œæ¢ç©¶åœ¨åäº‹å®å›¾åƒä¸‹æ¨¡å‹è¡¨ç°ï¼Œè¿˜ç ”ç©¶æ·»åŠ ä¸»é¢˜åç§°æ–‡æœ¬ã€å¼•å¯¼æ¨¡å‹ä»…ä¾èµ–å›¾åƒç»†èŠ‚ç­‰æ“ä½œå¯¹ç»“æœçš„å½±å“ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
1. åŸå§‹æœªä¿®æ”¹å›¾åƒä¸Šï¼Œæ‰€æœ‰5ä¸ªVLMså¯¹VLMBiasä¸»é¢˜çš„è¯†åˆ«å’Œè®¡æ•°é—®é¢˜å‡†ç¡®ç‡è¾¾100%ï¼›ä½†é¢å¯¹åäº‹å®å›¾åƒæ—¶è¡¨ç°å·®ï¼Œå¦‚ç»™2è…¿ã€4è…¿åŠ¨ç‰©åŠ é¢å¤–è…¿åï¼Œè®¡æ•°å‡†ç¡®ç‡åˆ†åˆ«ä½è‡³1.01%ã€2.50%ã€‚  
2. çŸ¥åå“ç‰Œæ ‡å¿—ã€æ——å¸œç­‰ä¿®æ”¹è§†è§‰å…ƒç´ åï¼ŒVLMsè®¡æ•°å‡†ç¡®ç‡æä½ï¼Œæ±½è½¦æ ‡å¿—åäº‹å®å›¾åƒä¸Šå‡†ç¡®ç‡ä»…0.44%ï¼›è§†é”™è§‰ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹èƒ½è¯†åˆ«åŸçŸ¥åé”™è§‰åç§°ï¼Œå´æ— æ³•å¯Ÿè§‰ä¿®æ”¹åå›¾å½¢å˜åŒ–ï¼Œå‡†ç¡®ç‡æ¥è¿‘éšæœºï¼›å›¾æ¡ˆç½‘æ ¼ä»»åŠ¡å› æ— äº’è”ç½‘å…ˆéªŒä¿¡æ¯ï¼Œæ¨¡å‹ä¹Ÿè¡¨ç°ä¸ä½³ã€‚  
3. è¯†åˆ«ç±»æ˜¯éé¢˜ä¸­VLMsåŒæ ·è¡¨ç°æŒ£æ‰ï¼›ç»™åäº‹å®å›¾åƒåŠ ä¸»é¢˜åç§°æ–‡æœ¬ï¼Œè®¡æ•°å‡†ç¡®ç‡å†é™2 - 6ä¸ªç‚¹ï¼›å¼•å¯¼æ¨¡å‹ä»…ä¾èµ–å›¾åƒç»†èŠ‚æˆ–äºŒæ¬¡æ£€æŸ¥ï¼Œè®¡æ•°å‡†ç¡®ç‡å¹³å‡ä»…æå‡2ä¸ªç‚¹å·¦å³ï¼Œå‡¸æ˜¾åè§ä¹‹å¼ºã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æå‡ºçš„VLMBiasæ¡†æ¶ä¸ºæµ‹è¯•VLMåè§æä¾›äº†è‡ªåŠ¨åŒ–æ–¹æ¡ˆï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤æ‹“å±•æ›´å¤šæµ‹è¯•åœºæ™¯ä¸ä»»åŠ¡ï¼Œæ·±å…¥åˆ†æVLMåè§æ¥æºä¸æœºåˆ¶ã€‚  
2. å¯¹å¤šé¢†åŸŸã€å¤šä»»åŠ¡ã€å¤šæ¨¡å‹çš„æµ‹è¯•æ–¹å¼ï¼Œä¸ºè¯„ä¼°AIæ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹çš„é²æ£’æ€§æä¾›äº†å…¨é¢èŒƒä¾‹ï¼Œå¯æŒ‡å¯¼å…¶ä»–æ¨¡å‹è¯„ä¼°å·¥ä½œè®¾è®¡æ›´ä¸°å¯Œçš„å®éªŒã€‚  
3. å…³äºæ–‡æœ¬æç¤ºã€å¼•å¯¼ç­–ç•¥å¯¹VLMåè§å½±å“çš„ç ”ç©¶ï¼Œä¸ºä¼˜åŒ–VLM promptingç­–ç•¥æä¾›äº†å®è¯ä¾æ®ï¼ŒåŠ©åŠ›åç»­æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­åº”å¯¹åè§çš„èƒ½åŠ›ã€‚

## videogamebench--can-vision-language-models-complete-popular-video-games-
### Abstract
Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VideoGameBenchï¼šç”¨ç»å…¸æ¸¸æˆæŒ‘æˆ˜è§†è§‰è¯­è¨€æ¨¡å‹çš„â€œäººç±»çº§â€èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç¼–ç¨‹ã€æ•°å­¦ç­‰å¯¹äººç±»æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ä¸Šå–å¾—äº†äº®çœ¼æˆç»©ï¼Œä½†åœ¨**æ„ŸçŸ¥ã€ç©ºé—´å¯¼èˆªã€è®°å¿†ç®¡ç†**è¿™ç±»äººç±»å¤©ç”Ÿå°±æ“…é•¿çš„èƒ½åŠ›ä¸Šï¼Œç ”ç©¶ä»ä¸å……åˆ†ã€‚è€Œ90å¹´ä»£çš„ç»å…¸ç”µå­æ¸¸æˆæ˜¯ä¸ºäººç±»â€œç›´è§‰å­¦ä¹ â€è®¾è®¡çš„â€”â€”åˆ©ç”¨äº†äººç±»å…ˆå¤©çš„å½’çº³åå¥½ï¼ˆæ¯”å¦‚ç©ºé—´æ„ŸçŸ¥ã€è®°å¿†ç•™å­˜ï¼‰ï¼Œå› æ­¤æˆä¸ºè¯„ä¼°VLMsè¿™ç±»èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•åºŠã€‚äºæ˜¯ï¼Œè®ºæ–‡å›¢é˜Ÿæå‡ºäº†VideoGameBenchï¼Œè¯•å›¾ç”¨çœŸå®ç”µå­æ¸¸æˆåœºæ™¯ï¼Œæ£€éªŒVLMsæ˜¯å¦èƒ½æ¥è¿‘äººç±»åœ¨è¿™ç±»â€œè‡ªç„¶ä»»åŠ¡â€ä¸Šçš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºçœŸå®å¤æ‚çš„æ¸¸æˆåŸºå‡†æµ‹è¯•é›†  
VideoGameBenchåŒ…å«10æ¬¾90å¹´ä»£æµè¡Œæ¸¸æˆï¼ˆæ¶µç›–Game BoyæŒæœºå’ŒPC DOSå¹³å°ï¼‰ï¼Œè¦æ±‚æ¨¡å‹ä»…é€šè¿‡**åŸå§‹è§†è§‰è¾“å…¥** + **é«˜å±‚çº§çš„ç›®æ ‡ä¸æ§åˆ¶è¯´æ˜**æ¥å®æ—¶äº¤äº’é€šå…³ã€‚ç›¸æ¯”è¿‡å¾€â€œç½‘æ ¼ä¸–ç•Œâ€â€œçº¯æ–‡å­—æ¸¸æˆâ€ç±»åŸºå‡†ï¼Œå®ƒçš„ç¯å¢ƒæ›´å¤æ‚ã€æ›´è´´è¿‘çœŸå®äººç±»äº¤äº’åœºæ™¯ï¼Œä¸”æ˜¯é¦–æ‰¹èšç„¦90å¹´ä»£ç”µå­æ¸¸æˆçš„AIåŸºå‡†ä¹‹ä¸€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼ºè°ƒæ³›åŒ–æ€§ä¸æ— è¾…åŠ©è¯„ä¼°  
æµ‹è¯•é›†ä¸­ä¿ç•™3æ¬¾â€œç§˜å¯†æ¸¸æˆâ€ï¼Œä¸“é—¨æ£€éªŒæ¨¡å‹å¯¹æœªè§è¿‡ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ï¼›åŒæ—¶**ä¸å…è®¸ä»»ä½•æ¸¸æˆä¸“å±è„šæ‰‹æ¶ã€è¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚è·¯å¾„æç¤ºã€è§†è§‰è¦†ç›–å±‚ï¼‰**ï¼Œåªç»™åŸå§‹ç”»é¢å’ŒåŸºç¡€æ§åˆ¶è¯´æ˜ï¼Œå½»åº•è€ƒéªŒæ¨¡å‹â€œè£¸å¥”â€è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§£å†³å®æ—¶æ€§ç—›ç‚¹çš„Liteç‰ˆæœ¬ + æ–°é¢–è¿›åº¦è¯„ä¼°æ³•  
è€ƒè™‘åˆ°å‰æ²¿æ¨¡å‹æ¨ç†å»¶è¿Ÿåœ¨å®æ—¶æ¸¸æˆä¸­æ˜¯ç¡¬ä¼¤ï¼Œå›¢é˜Ÿæ¨å‡ºVideoGameBench Liteâ€”â€”æ¸¸æˆåœ¨ç­‰å¾…æ¨¡å‹åŠ¨ä½œæ—¶æš‚åœï¼Œæ¶ˆé™¤å»¶è¿Ÿå¹²æ‰°ã€‚æ­¤å¤–ï¼Œè¿˜æå‡º**åŸºäºYouTubeé€šå…³è§†é¢‘æ—¶é—´æˆ³ + æ„ŸçŸ¥å“ˆå¸Œ**çš„è¿›åº¦æ£€æµ‹æ–¹æ³•ï¼šçˆ¬å–æ¸¸æˆé€šå…³è§†é¢‘ï¼Œç”¨æ—¶é—´æˆ³æ ‡æ³¨â€œé‡Œç¨‹ç¢‘-ç”»é¢å¸§â€å¯¹ï¼Œå†é€šè¿‡æ„ŸçŸ¥å“ˆå¸ŒåŒ¹é…æ¨¡å‹å®é™…æ¸¸æˆç”»é¢ï¼Œåˆ¤æ–­è¿›åº¦ã€‚è¿™å¥—æ–¹æ³•è®©åç»­æ‰©å±•æ–°æ¸¸æˆä»»åŠ¡æ›´ä¾¿æ·ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å‰æ²¿VLMsåœ¨VideoGameBenchä¸Šè¡¨ç°æå·®ï¼šå³ä¾¿æ˜¯è¡¨ç°æœ€å¥½çš„Gemini 2.5 Proï¼Œåœ¨æ ‡å‡†VideoGameBenchä¸Šä»…å®Œæˆ**0.48%**çš„æ¸¸æˆè¿›åº¦ï¼Œåœ¨æ¶ˆé™¤å»¶è¿Ÿçš„Liteç‰ˆæœ¬ä¸­ä¹Ÿåªå®Œæˆ**1.6%**ã€‚ä¸”å¤šæ•°æ¨¡å‹åœ¨â€œé¼ æ ‡ç§»åŠ¨ã€åŸºç¡€å¯¼èˆªâ€ç­‰ç®€å•ç»ƒä¹ æ¸¸æˆä¸­ä¹Ÿè¡¨ç°æ‹‰èƒ¯ï¼Œè¯´æ˜å½“å‰VLMsè·ç¦»äººç±»â€œè‡ªç„¶æŒæ¡æ¸¸æˆâ€çš„èƒ½åŠ›è¿˜æœ‰æå¤§å·®è·ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ä»»åŠ¡è®¾è®¡è§†è§’**ï¼šæŠŠâ€œäººç±»å¤©ç”Ÿæ“…é•¿çš„æ„ŸçŸ¥/ç©ºé—´/è®°å¿†ä»»åŠ¡â€è½¬åŒ–ä¸ºâ€œç”µå­æ¸¸æˆé€šå…³â€è¿™æ ·çš„å…·è±¡åŒ–åŸºå‡†ï¼Œä¸ºè¯„ä¼°AIä¸äººç±»å½’çº³åå¥½çš„å¯¹é½åº¦æä¾›äº†æ–°æ€è·¯ã€‚  
2. **åŸºå‡†æ„å»ºæ€è·¯**ï¼šç”¨â€œçœŸå®å¤æ‚ç¯å¢ƒ+æ³›åŒ–æµ‹è¯•+æ— è¾…åŠ©çº¦æŸâ€çš„ç»„åˆæ‹³ï¼Œé€¼è¿«æ¨¡å‹æš´éœ²èƒ½åŠ›çŸ­æ¿ï¼›Liteç‰ˆæœ¬åˆ™é€šè¿‡â€œæš‚åœæœºåˆ¶â€åˆ†ç¦»â€œå®æ—¶æ€§â€è¿™ä¸€å˜é‡ï¼Œè®©å®éªŒæ›´å…·è§£é‡ŠåŠ›ã€‚  
3. **è¯„ä¼°æ–¹æ³•è®º**ï¼šåŸºäºYouTubeé€šå…³è§†é¢‘çš„è¿›åº¦æ£€æµ‹æ³•ï¼Œä¸ºâ€œå¦‚ä½•é‡åŒ–AIåœ¨å¼€æ”¾ä»»åŠ¡ä¸­çš„è¡¨ç°â€æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯è·¯çº¿ï¼Œé™ä½äº†åç»­æ„å»ºç±»ä¼¼æ¸¸æˆ/äº¤äº’ç±»åŸºå‡†çš„é—¨æ§›ã€‚  


è¿™ç¯‡è®ºæ–‡ç”¨â€œå¤å¤æ¸¸æˆâ€è¿™ä¸€è¶£å‘³è½½ä½“ï¼Œæ’•å¼€äº†å½“å‰VLMsåœ¨â€œç±»äººæ„ŸçŸ¥ä¸äº¤äº’â€èƒ½åŠ›ä¸Šçš„é®ç¾å¸ƒï¼Œä¹Ÿä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†â€œè¡¥å…¨äººç±»çº§å½’çº³åå¥½â€çš„æ–¹å‘â€”â€”æ¯•ç«Ÿï¼Œè¿ã€Šå¡å°”è¾¾ã€‹ã€Šè¶…çº§é©¬é‡Œå¥¥ã€‹çš„å¼€å¤´éƒ½è¿‡ä¸äº†ï¼Œè°ˆâ€œç±»äººæ™ºèƒ½â€è¿˜å¤ªæ—©~

## videogameqa-bench--evaluating-vision-language-models-for-video-game-quality-assurance
### Abstract
With video games now generating the highest revenues in the entertainment
industry, optimizing game development workflows has become essential for the
sector's sustained growth. Recent advancements in Vision-Language Models (VLMs)
offer considerable potential to automate and enhance various aspects of game
development, particularly Quality Assurance (QA), which remains one of the
industry's most labor-intensive processes with limited automation options. To
accurately evaluate the performance of VLMs in video game QA tasks and
determine their effectiveness in handling real-world scenarios, there is a
clear need for standardized benchmarks, as existing benchmarks are insufficient
to address the specific requirements of this domain. To bridge this gap, we
introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array
of game QA activities, including visual unit testing, visual regression
testing, needle-in-a-haystack tasks, glitch detection, and bug report
generation for both images and videos of various games. Code and data are
available at: https://asgaardlab.github.io/videogameqa-bench/
### ğŸŒŸ è®ºæ–‡è§£è¯» | VideoGameQA-Benchï¼šä¸ºæ¸¸æˆè´¨é‡ä¿éšœè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å…¨çƒç”µå­æ¸¸æˆäº§ä¸šè§„æ¨¡æŒç»­æ‰©å¼ ï¼Œé¢„è®¡åˆ°2028å¹´å¸‚å€¼å°†è¾¾2570äº¿ç¾å…ƒï¼Œä½†æ¸¸æˆå¼€å‘ä¸­ä¿éšœè§†è§‰è´¨é‡ä¸ä¸€è‡´æ€§çš„QAæµç¨‹ä»é«˜åº¦ä¾èµ–äººå·¥ï¼Œè€—æ—¶ã€æ˜‚è´µä¸”æ˜“å‡ºé”™ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½ä¸ºæ¸¸æˆQAè‡ªåŠ¨åŒ–å¸¦æ¥å¸Œæœ›ï¼Œç„¶è€Œç°æœ‰åŸºå‡†æµ‹è¯•è¦ä¹ˆä¾§é‡å¤æ‚æ•°å­¦æˆ–æ–‡æœ¬æ¨ç†ã€å¿½è§†æ¸¸æˆQAæ ¸å¿ƒè§†è§‰ç†è§£ä»»åŠ¡ï¼Œè¦ä¹ˆä»…è¦†ç›–QAä»»åŠ¡çš„ç‹­çª„æ–¹é¢ï¼Œæ— æ³•å…¨é¢è¯„ä¼°VLMsåœ¨å¤šæ ·QAåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚å› æ­¤ï¼Œæ‰“é€ è´´åˆæ¸¸æˆQAåœºæ™¯çš„æ ‡å‡†åŒ–åŸºå‡†æˆä¸ºå…³é”®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºVideoGameQA - BenchåŸºå‡†  
é’ˆå¯¹çœŸå®æ¸¸æˆå¼€å‘åœºæ™¯ï¼Œè®¾è®¡æ¶µç›–9ç±»ä¸åŒä»»åŠ¡ã€åŒ…å«4786ä¸ªé—®é¢˜çš„VideoGameQA - Benchã€‚ä»»åŠ¡è¦†ç›–è§†è§‰å•å…ƒæµ‹è¯•ã€å›å½’æµ‹è¯•ã€UIéªŒè¯ã€è§†é¢‘â€œå¤§æµ·æé’ˆâ€æ‰¾ glitchã€ glitchæ£€æµ‹ç­‰ï¼ŒåŒ…å«2236ä¸ªåŸºäºå›¾åƒå’Œ1200ä¸ªåŸºäºè§†é¢‘çš„æ ·æœ¬ï¼Œç´ ææ¥è‡ªè¶…800æ¬¾æ¸¸æˆä¸9ä¸ªåˆæˆæ¸¸æˆåœºæ™¯ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢è¦†ç›–æ¸¸æˆQAæ´»åŠ¨ç±»å‹  
å°†æ¸¸æˆè§†è§‰QAä»»åŠ¡æŠ½è±¡ä¸ºä¸‰ç±»å¹¶å¯¹åº”è®¾è®¡ä»»åŠ¡ï¼šéªŒè¯åœºæ™¯å®Œæ•´æ€§ï¼ˆå¦‚è§†è§‰å•å…ƒã€å›å½’æµ‹è¯•ï¼‰ã€æ— å‚è€ƒç‚¹çš„glitchæ£€æµ‹ï¼ˆä¾é å¸¸è¯†ï¼‰ã€ glitchæŠ¥å‘Šç”Ÿæˆï¼›ç»†åˆ†å‡ºå›¾åƒä¸è§†é¢‘ç»´åº¦ä¸‹çš„å„ç±»å­ä»»åŠ¡ï¼Œå¦‚é’ˆå¯¹æ¸¸æˆä¸­å¸¸è§çš„clippingé—®é¢˜è®¾è®¡å‚æ•°åŒ–clippingæ£€æµ‹ä»»åŠ¡ç­‰ï¼Œå…¨æ–¹ä½è€ƒå¯ŸVLMsåœ¨æ¸¸æˆQAå„ç¯èŠ‚èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. VLMsåœ¨å¤šæ¨¡æ€åŸºå‡†è¡¨ç°ä¸é”™ä¸”èƒ½åšOCRï¼Œä½†åœ¨ç²¾ç¡®åœºæ™¯ç†è§£ï¼ˆè¯†åˆ«ç²¾ç»†ç»†èŠ‚ï¼‰ä¸è§£æå¤æ‚UIå…ƒç´ ä¸Šè¡¨ç°å·®ï¼›  
2. å‰æ²¿VLMsåœ¨å›¾åƒglitchæ£€æµ‹ï¼ˆæœ€é«˜82.8%ï¼‰ã€è§†é¢‘glitchæ£€æµ‹ï¼ˆæœ€é«˜78.1%ï¼‰æœ‰ä¸é”™è¡¨ç°ï¼Œä½†é¢å¯¹èº«ä½“å§¿æ€ã€å¤æ‚ç‰©ä½“clippingã€å¸¸è¯†æ¨ç†ç±»glitchä»åƒåŠ›ï¼›  
3. è§†è§‰å›å½’æµ‹è¯•å¯¹VLMsè€Œè¨€ä¾æ—§æ˜¯æå¯ŒæŒ‘æˆ˜çš„ä»»åŠ¡ï¼›  
4. åœ¨è§†é¢‘ä¸­å®šä½ç‰¹å®šglitchæ—¶åˆ»ï¼ˆæ£€æµ‹+ç²¾å‡†å®šä½ï¼‰ä»å…·éš¾åº¦ï¼›  
5. å‰æ²¿VLMsèƒ½ä¸ºçº¦50%çœŸå®glitchç”Ÿæˆæœ‰ç”¨bugæŠ¥å‘Šï¼Œæä¾›å‡†ç¡®æè¿°æ€§æ€»ç»“ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¡«è¡¥é¢†åŸŸç©ºç™½ï¼šé¦–æ¬¡é’ˆå¯¹æ€§æ„å»ºæ¸¸æˆQAæ–¹å‘VLMsè¯„ä¼°åŸºå‡†ï¼Œä¸ºè¯¥é¢†åŸŸåç»­æ¨¡å‹ç ”å‘ã€ä¼˜åŒ–æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°å·¥å…·ä¸å‚è€ƒä¾æ®ï¼›  
2. ä»»åŠ¡è®¾è®¡æ€è·¯ï¼šä»çœŸå®äº§ä¸šæµç¨‹ä¸­æŠ½è±¡ä»»åŠ¡ç±»å‹å¹¶è½¬åŒ–ä¸ºæ¨¡å‹å¯è¯„ä¼°çš„å½¢å¼ï¼Œè¿™ç§è´´åˆäº§ä¸šå®é™…åœºæ™¯çš„åŸºå‡†æ„å»ºæ€è·¯ï¼Œå¯ä¸ºå…¶ä»–å‚ç›´é¢†åŸŸï¼ˆå¦‚å½±è§†åˆ¶ä½œQAã€å·¥ä¸šè®¾è®¡è§†è§‰æ£€æµ‹ç­‰ï¼‰æ‰“é€ AIè¯„ä¼°åŸºå‡†æä¾›å€Ÿé‰´ï¼›  
3. å®éªŒåˆ†æä»·å€¼ï¼šå¯¹VLMsåœ¨å„æ¸¸æˆQAå­ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜åŠ£åˆ†æï¼Œèƒ½æŒ‡å¯¼åç»­æ¨¡å‹æ”¹è¿›æ–¹å‘ï¼ˆå¦‚å¼ºåŒ–ç²¾ç»†è§†è§‰ç†è§£ã€å¸¸è¯†èåˆç­‰ï¼‰ï¼Œä¹Ÿè®©äº§ä¸šç•Œæ¸…æ™°çŸ¥æ™“å½“å‰æŠ€æœ¯åœ¨æ¸¸æˆQAè‡ªåŠ¨åŒ–ä¸Šçš„èƒ½åŠ›è¾¹ç•Œä¸æ½œåŠ›ç‚¹ã€‚

## code2logic--game-code-driven-data-synthesis-for-enhancing-vlms-general-reasoning
### Abstract
Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Code2Logicï¼šç”¨æ¸¸æˆä»£ç é©±åŠ¨æ•°æ®åˆæˆï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ç­‰åŸºç¡€ä»»åŠ¡ä¸Šå–å¾—äº†ä¸é”™è¿›å±•ï¼Œä½†é¢å¯¹éœ€è¦å¤šæ­¥æ¨ç†çš„å¤æ‚è§†è§‰ä»»åŠ¡æ—¶ä»å­˜åœ¨ä¸è¶³ã€‚å…³é”®åŸå› åœ¨äº**é«˜è´¨é‡å¤šæ¨¡æ€æ¨ç†æ•°æ®ç¨€ç¼º**ï¼Œé™åˆ¶äº†æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ã€‚å½“å‰æ„å»ºè¿™ç±»æ•°æ®çš„æ–¹æ³•ï¼Œè¦ä¹ˆä¾èµ–äººå·¥æ ‡æ³¨ï¼ˆå­˜åœ¨æ•°æ®å°‘æˆ–æˆæœ¬é«˜é—®é¢˜ï¼‰ï¼Œè¦ä¹ˆç”¨ç¥ç»ç½‘ç»œè‡ªåŠ¨åˆæˆï¼ˆæ¨ç†å‡†ç¡®æ€§éš¾ä¿è¯ï¼‰ã€ç¬¦å·å¼•æ“ï¼ˆé¢†åŸŸç‰¹å®šæ€§å¼ºã€è¿ç§»æˆæœ¬é«˜ï¼‰ï¼Œç°æœ‰æ–¹æ¡ˆéš¾ä»¥åŒæ—¶æ»¡è¶³è§„æ¨¡ã€è´¨é‡ã€ä½æˆæœ¬çš„éœ€æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCode2Logicæ¡†æ¶ï¼Œåˆ©ç”¨æ¸¸æˆä»£ç ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†æ•°æ®  
æ¸¸æˆå¤©ç„¶åŒ…å«æ¸…æ™°è§„åˆ™ã€çŠ¶æ€è½¬ç§»é€»è¾‘ä¸å› æœæ¨ç†é“¾ï¼Œä¸”ä»£ç æ˜“éªŒè¯æ­£ç¡®æ€§ã€‚Code2Logicå€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®ç°ä¸‰æ­¥æ ¸å¿ƒæµç¨‹ï¼š  
- ç”¨LLMsæŠŠâ€œéå½¢å¼åŒ–çš„æ¸¸æˆé€»è¾‘â€è½¬åŒ–ä¸ºå¯æ‰§è¡Œä»£ç ï¼›  
- ç»“åˆLLMè¾…åŠ©äººç±»è®¾è®¡QAæ¨¡æ¿ï¼Œä»ä»£ç é‡Œæå–æ¨ç†æ¨¡å¼ï¼›  
- å†ç”¨LLMså¼€å‘æ•°æ®å¼•æ“ç¨‹åºï¼Œå¤ç”¨æ ¸å¿ƒæ¸¸æˆä»£ç ã€è§„æ¨¡åŒ–å®ä¾‹åŒ–æ¨¡æ¿ï¼Œäº§å‡ºæœ€ç»ˆæ•°æ®æ ·æœ¬ã€‚é€šè¿‡è¿™å¥—æµç¨‹ï¼ŒæŠŠæ¸¸æˆä»£ç é‡Œçš„éšå¼æ¨ç†è½¬åŒ–ä¸ºæ˜¾å¼çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®æ ·æœ¬ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºGameQAæ•°æ®é›†ç”¨äºVLMsè®­ç»ƒä¸è¯„ä¼°  
åŸºäºCode2Logicç”Ÿæˆäº†GameQAæ•°æ®é›†ï¼Œå®ƒæœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼š  
- **æˆæœ¬ä½ä¸”å¯æ‰©å±•**ï¼šä»£ç ä¸€æ—¦æ­å»ºï¼Œèƒ½ä»¥æä½è®¡ç®—å¼€é”€ç”Ÿæˆå¤§é‡æ ·æœ¬ï¼Œæ¯”äººå·¥æ ‡æ³¨æˆæœ¬ä½å¾ˆå¤šï¼›  
- **æŒ‘æˆ˜æ€§å¼º**ï¼šç°æœ‰é¡¶å°–æ¨¡å‹åœ¨GameQAæµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡ä¸è¶³50%ï¼›  
- **è§„æ¨¡å¤§ä¸”å¤šæ ·**ï¼šæ¶µç›–30ç§æ¸¸æˆã€158ä¸ªä»»åŠ¡ã€3ä¸‡é“é¢˜ï¼Œè¦†ç›–3Dç©ºé—´ç†è§£ã€æ¨¡å¼è¯†åˆ«åŒ¹é…ã€å¤šæ­¥æ¨ç†ã€ç­–ç•¥è§„åˆ’ç­‰è®¤çŸ¥æŠ€èƒ½ï¼Œè¿˜æŒ‰é—®é¢˜å¤æ‚åº¦å’Œå›¾åƒå¤æ‚åº¦åšäº†ç»†åˆ†ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¨¡å‹æ³›åŒ–æ€§æµ‹è¯•ä¸Šï¼Œä»…ç”¨æ¸¸æˆæ•°æ®è®­ç»ƒçš„VLMså±•ç°å‡ºè·¨é¢†åŸŸèƒ½åŠ›ï¼š  
- å¯¹è®­ç»ƒæ—¶æ²¡è§è¿‡çš„æ¸¸æˆç±»å‹ï¼ŒQwen2.5 - VL - 7Bæ€§èƒ½æå‡3.83%ï¼›  
- åœ¨7ä¸ªä¸åŒçš„è§†è§‰ - è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ•´ä½“æ€§èƒ½æå‡2.33%ã€‚  
è¿™è¯´æ˜GameQAè®­ç»ƒæ•°æ®èƒ½æœ‰æ•ˆå¢å¼ºæ¨¡å‹åœ¨å¤šåœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ï¼ŒéªŒè¯äº†Code2Logicæ–¹æ³•çš„ä»·å€¼ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ•°æ®åˆæˆæ€è·¯åˆ›æ–°**ï¼šæŠŠæ¸¸æˆä»£ç è¿™ç±»â€œå¤©ç„¶å«é€»è¾‘ç»“æ„â€çš„èµ„æºå¼•å…¥å¤šæ¨¡æ€æ¨ç†æ•°æ®æ„å»ºï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†å…¨æ–°è§†è§’â€”â€”æŒ–æ˜é¢†åŸŸå†…éšå«é€»è¾‘çš„â€œç°æˆèµ„æºâ€ï¼Œç”¨LLMsè½¬åŒ–ä¸ºæ¨¡å‹å¯ç”¨æ•°æ®ï¼›  
2. **æµç¨‹å¯å¤ç”¨æ€§**ï¼šCode2Logicçš„â€œLLMè¾…åŠ©ä»£ç ç”Ÿæˆâ†’æ¨¡æ¿è®¾è®¡â†’è§„æ¨¡åŒ–å®ä¾‹åŒ–â€æµç¨‹ï¼Œå¯è¿ç§»åˆ°å…¶ä»–æœ‰â€œè§„åˆ™/é€»è¾‘å¯ç¼–ç â€çš„é¢†åŸŸï¼ˆå¦‚æ•™è‚²é¢˜ã€æ¨¡æ‹Ÿä»¿çœŸåœºæ™¯ï¼‰ï¼Œä¸ºå‚ç›´é¢†åŸŸå¤šæ¨¡æ€æ•°æ®æ„å»ºæä¾›å‚è€ƒï¼›  
3. **æ•°æ®é›†ä»·å€¼**ï¼šGameQAä¸ä»…ç”¨äºè®­ç»ƒï¼Œè¿˜èƒ½è¯„ä¼°æ¨¡å‹å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œè¿™ç§â€œæ—¢è®­åˆæµ‹â€ä¸”è¦†ç›–å¤šè®¤çŸ¥ç»´åº¦çš„æ•°æ®é›†å»ºè®¾æ€è·¯ï¼Œå¯¹æ¨åŠ¨VLMsæ¨ç†ç ”ç©¶æœ‰å€Ÿé‰´æ„ä¹‰ã€‚  

## g1--bootstrapping-perception-and-reasoning-abilities-of-vision-language-model-via-reinforcement-learning
### Abstract
Vision-Language Models (VLMs) excel in many direct multimodal tasks but
struggle to translate this prowess into effective decision-making within
interactive, visually rich environments like games. This ``knowing-doing'' gap
significantly limits their potential as autonomous agents, as leading VLMs
often performing badly in simple games. To address this, we introduce VLM-Gym,
a curated reinforcement learning (RL) environment featuring diverse visual
games with unified interfaces and adjustable, compositional difficulty,
specifically designed for scalable multi-game parallel training. Leveraging
VLM-Gym, we train G0 models using pure RL-driven self-evolution, which
demonstrate emergent perception and reasoning patterns. To further mitigate
challenges arising from game diversity, we develop G1 models. G1 incorporates a
perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models
consistently surpass their teacher across all games and outperform leading
proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals
an intriguing finding: perception and reasoning abilities mutually bootstrap
each other throughout the RL training process. Source code including VLM-Gym
and RL training are released at https://github.com/chenllliang/G1 to foster
future research in advancing VLMs as capable interactive agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | G1ï¼šç”¨å¼ºåŒ–å­¦ä¹ èµ‹èƒ½è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¯¸å¤šç›´æ¥çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åƒæ¸¸æˆè¿™ç±»äº¤äº’å¼ã€è§†è§‰ä¿¡æ¯ä¸°å¯Œçš„ç¯å¢ƒé‡Œï¼Œå´éš¾ä»¥å°†è‡ªèº«èƒ½åŠ›è½¬åŒ–ä¸ºæœ‰æ•ˆçš„å†³ç­–ï¼Œå­˜åœ¨â€œçŸ¥ä¸è¡Œâ€çš„å·®è·ï¼Œè¿™æå¤§é™åˆ¶äº†å®ƒä»¬ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„æ½œåŠ›ï¼Œå³ä¾¿é¡¶å°–VLMsåœ¨ç®€å•æ¸¸æˆä¸­è¡¨ç°ä¹Ÿä¸ä½³ã€‚åŒæ—¶ï¼Œç°æœ‰é’ˆå¯¹VLMsåœ¨äº¤äº’å¼æ¸¸æˆä¸­å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æœ‰æ•ˆä¸”å¯æ‰©å±•æ¡†æ¶ç¼ºå¤±ï¼Œæ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›åœ¨å…¶ä¸­èƒ½å¸¦æ¥çš„æ½œåœ¨æ”¶ç›Šä¹Ÿä¸æ˜æ™°ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡å¼€å±•äº†ç›¸å…³ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºVLM - Gymå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ  
æ‰“é€ äº†åŒ…å«å¤šç§è§†è§‰æ¸¸æˆï¼ˆå¦‚2048ã€Shisen - Shoç­‰ï¼‰çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒVLM - Gymï¼Œå…·å¤‡å¯æ‰©å±•ç¯å¢ƒï¼ˆæ”¯æŒå¤šæ¸¸æˆçŠ¶æ€ã€å¤šæ¸¸æˆåŒæ—¶å¹¶è¡Œæ‰§è¡Œï¼ŒåŠ©åŠ›å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸å¤§è§„æ¨¡æ‰¹é‡è®­ç»ƒï¼‰ã€å¹¶è¡ŒåŠ¨ä½œï¼ˆæ”¯æŒå¯¹åŒä¸€è§‚æµ‹å¹¶è¡Œé‡‡æ ·å¤šä¸ªåŠ¨ä½œå¹¶è®¡ç®—å¯¹åº”å¥–åŠ±ï¼Œä¸ºé€‚é…åŸºç¡€æ¨¡å‹çš„å…ˆè¿›RLç®—æ³•æä¾›æ”¯æŒï¼‰ã€ç»„åˆå¼éš¾åº¦ï¼ˆå„æ¸¸æˆéš¾åº¦åœ¨æ„ŸçŸ¥å¤æ‚åº¦ã€æ¨ç†æ·±åº¦ç­‰å¤šç»´åº¦å¯è°ƒä¸”èƒ½ç»„åˆï¼Œä¾¿äºç ”ç©¶VLMsåœ¨RLåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼‰ç­‰å…³é”®ç‰¹æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒG0ä¸G1æ¨¡å‹  
åˆ©ç”¨VLM - Gymï¼Œé€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ é©±åŠ¨è‡ªè¿›åŒ–è®­ç»ƒå‡ºG0æ¨¡å‹ï¼ŒG0å±•ç°å‡ºäº†emergentçš„æ„ŸçŸ¥ä¸æ¨ç†æ¨¡å¼ï¼›ä¸ºåº”å¯¹æ¸¸æˆå¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¿›ä¸€æ­¥æå‡ºG1æ¨¡å‹ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒå‰èå…¥æ„ŸçŸ¥å¢å¼ºçš„å†·å¯åŠ¨ä»¥åŠæ¥è‡ªæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è’¸é¦ï¼Œæå‡æ¨¡å‹åœ¨å„æ¸¸æˆä¸­çš„è¡¨ç°ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
G0æ¨¡å‹åœ¨è®­ç»ƒä¸­è¡¨ç°å‡ºæœ‰æ•ˆçš„æ„ŸçŸ¥ä¸æ¨ç†æ¨¡å¼ï¼Œæ¸¸æˆè¡¨ç°å¤§å¹…æå‡ï¼Œå¦‚åœ¨Shisen - Shoæ¸¸æˆä¸­åˆ†æ•°ä»1.9æå‡åˆ°12.8ï¼Œä¸”è¶…è¿‡OpenAI - o1ã€Qwen2.5VL - 72Bç­‰å¼ºå¤šæ¨¡æ€æ¨¡å‹ï¼›G1æ¨¡å‹åœ¨æ‰€æœ‰æ¸¸æˆä¸­éƒ½è¶…è¶Šäº†å…¶æ•™å¸ˆæ¨¡å‹ï¼Œè¿˜æ˜¾è‘—è¶…è¿‡äº†å¦‚Claude - 3.7 - Sonnet - Thinkingè¿™æ ·çš„é¡¶å°–ä¸“æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹G0å’ŒG1æ„ŸçŸ¥ä¸æ¨ç†å‡†ç¡®ç‡çš„è§£è€¦åˆ†æï¼Œå‘ç°æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’å¼•å¯¼æå‡ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç¯å¢ƒæ„å»ºå±‚é¢ï¼šVLM - Gymçš„è®¾è®¡æ€è·¯ä¸ºæ‰“é€ é’ˆå¯¹ç‰¹å®šæ¨¡å‹ï¼ˆå¦‚VLMsï¼‰çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒç¯å¢ƒæä¾›äº†å‚è€ƒï¼Œå…¶å¯æ‰©å±•ã€æ”¯æŒå¹¶è¡ŒåŠ¨ä½œä¸ç»„åˆéš¾åº¦è°ƒèŠ‚ç­‰ç‰¹æ€§ï¼Œå¯¹äºéœ€è¦åœ¨å¤šä»»åŠ¡ã€å¤æ‚äº¤äº’åœºæ™¯ä¸‹è®­ç»ƒçš„æ¨¡å‹ç¯å¢ƒæ„å»ºå¾ˆæœ‰å¯å‘ã€‚  
2. æ¨¡å‹è®­ç»ƒå±‚é¢ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å®ç°æ¨¡å‹è‡ªè¿›åŒ–ä»¥åŠç»“åˆæ„ŸçŸ¥å¢å¼ºå†·å¯åŠ¨å’ŒçŸ¥è¯†è’¸é¦æ¥æå‡æ¨¡å‹åœ¨å¤æ‚å¤šæ ·ä»»åŠ¡ä¸­è¡¨ç°çš„æ€è·¯ï¼Œå¯ä¸ºæå‡å…¶ä»–ç±»å‹æ¨¡å‹åœ¨äº¤äº’å¼ã€å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„èƒ½åŠ›æä¾›å€Ÿé‰´ã€‚  
3. ç ”ç©¶è§†è§’å±‚é¢ï¼šå¯¹æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’ä½œç”¨çš„åˆ†æï¼Œä¸ºåç»­ç ”ç©¶æ¨¡å‹èƒ½åŠ›ä¹‹é—´çš„å…³è”ä¸ååŒæå‡æä¾›äº†æ–°çš„æ€è€ƒè§’åº¦ã€‚  
4. å¼€æºè´¡çŒ®å±‚é¢ï¼šè®ºæ–‡å¼€æºäº†VLM - Gymå’ŒRLè®­ç»ƒç­‰ä»£ç ï¼Œä¸ºè¯¥é¢†åŸŸåç»­ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œè¿™ç§å¼€æºä¿ƒè¿›ç ”ç©¶å‘å±•çš„æ¨¡å¼å€¼å¾—å­¦ä¹ ã€‚

## dsadf--thinking-fast-and-slow-for-decision-making
### Abstract
Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DSADFï¼šå¿«æ…¢æ€ç»´åŒç³»ç»Ÿèµ‹èƒ½å†³ç­–æ™ºèƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“åœ¨æ˜ç¡®å®šä¹‰çš„ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¾èµ–è¯•é”™äº¤äº’çš„ç‰¹æ€§ä½¿å…¶åœ¨åŠ¨æ€åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å°è¯•ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¾…åŠ©RLæå‡æ³›åŒ–æ€§ï¼Œå´å­˜åœ¨RLæ™ºèƒ½ä½“ä¸åŸºç¡€æ¨¡å‹åä½œä¸æµç•…ã€é™Œç”Ÿç¯å¢ƒå†³ç­–ä¸åˆç†ã€æ•ˆç‡ç“¶é¢ˆç­‰é—®é¢˜ã€‚å¦‚ä½•å……åˆ†å‘æŒ¥åŸºç¡€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸RLå¿«é€Ÿå“åº”èƒ½åŠ›å¹¶å¢å¼ºäºŒè€…äº’åŠ¨ï¼Œä»æ˜¯å¾…è§£éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å—å¡å°¼æ›¼â€œå¿«æ€è€ƒï¼ˆSystem 1ï¼‰ä¸æ…¢æ€è€ƒï¼ˆSystem 2ï¼‰â€ç†è®ºå¯å‘ï¼Œæ¢ç´¢åœ¨å¤æ‚ä¸–ç•Œä¸­å¹³è¡¡ç›´è§‰ä¸æ·±åº¦æ¨ç†ä»¥å®ç°çµæ´»å†³ç­–çš„è·¯å¾„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰  
DSADFæ•´åˆä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šSystem 1ç”±RLæ™ºèƒ½ä½“ä¸è®°å¿†ç©ºé—´æ„æˆï¼Œè´Ÿè´£å¿«é€Ÿç›´è§‚å†³ç­–ï¼›System 2ç”±VLMé©±åŠ¨ï¼Œå¼€å±•æ·±åº¦åˆ†ææ¨ç†ã€‚å€Ÿç”±åŒç³»ç»Ÿåä½œï¼Œå……åˆ†å‘æŒ¥å„è‡ªä¼˜åŠ¿å®ç°é«˜æ•ˆè‡ªé€‚åº”å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒç³»ç»Ÿåä½œæœºåˆ¶è®¾è®¡  
System 2åˆ©ç”¨åŸºç¡€æ¨¡å‹å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œä»æŒ‡ä»¤æç¤ºä¸­æå–çº¿ç´¢ï¼Œå°†é•¿å‘¨æœŸåˆå§‹ä»»åŠ¡æ‹†è§£ä¸ºå¯ç®¡ç†çš„å•æ­¥ä»»åŠ¡ï¼Œè®©RLæ™ºèƒ½ä½“èšç„¦å•æ­¥ä»»åŠ¡æ±‚è§£ä»¥æå‡å­¦ä¹ ä¸æ‰§è¡Œæ•ˆç‡ï¼›åŒæ—¶VLMä¾æ®å†å²æ­¥éª¤æŒç»­æ›´æ–°è®°å¿†æ¨¡å—ï¼Œè¯„ä¼°RLå¯¹ä¸åŒå•æ­¥ä»»åŠ¡çš„ç†Ÿç»ƒåº¦ï¼Œæµ‹è¯•æ—¶ç²¾å‡†åˆ†é…ä»»åŠ¡ï¼Œè¿˜èƒ½åŸºäºç›®æ ‡å’Œè§‚æµ‹ç”Ÿæˆç›®æ ‡å¯¼å‘æŒ‡ä»¤åŠ©åŠ›RLèƒ½åŠ›æå‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è§†é¢‘æ¸¸æˆç¯å¢ƒCrafterå’ŒHousekeepä¸­å¼€å±•å®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†DSADFçš„æœ‰æ•ˆæ€§ã€‚å®éªŒé‡ŒåŒç³»ç»ŸååŒå·¥ä½œï¼Œåœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸æœªè§è¿‡çš„ä»»åŠ¡åœºæ™¯ä¸‹ï¼Œå†³ç­–èƒ½åŠ›å‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç°å‡ºåœ¨æ•ˆç‡ä¸æ³›åŒ–æ€§ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨é¢†åŸŸç†è®ºè¿ç§»ï¼šå°†è®¤çŸ¥ç§‘å­¦ä¸­â€œå¿«æ…¢æ€è€ƒâ€ç†è®ºå¼•å…¥å¼ºåŒ–å­¦ä¹ ä¸åŸºç¡€æ¨¡å‹ç»“åˆçš„ç ”ç©¶ï¼Œä¸ºè§£å†³æ™ºèƒ½ä½“æ³›åŒ–å’Œåä½œéš¾é¢˜æä¾›æ–°è§†è§’ï¼Œå¯å‘åç»­è·¨å­¦ç§‘èåˆç±»ç ”ç©¶æ€è·¯æ‹“å±•ã€‚  
2. æ¨¡å—åŒ–åä½œèŒƒå¼ï¼šDSADFçš„åŒç³»ç»Ÿæ¨¡å—åŒ–è®¾è®¡ï¼Œæ¸…æ™°åˆ’åˆ†å¿«é€Ÿå“åº”ä¸æ·±åº¦æ¨ç†èŒèƒ½å¹¶æ„å»ºäº’åŠ¨æœºåˆ¶ï¼Œä¸ºå¤šæ™ºèƒ½ä½“æˆ–å¤šæ¨¡å‹åä½œå®Œæˆå¤æ‚ä»»åŠ¡æä¾›äº†å¯å‚è€ƒçš„æ¶æ„èŒƒå¼ï¼Œä¾¿äºåç»­ç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šä¼˜åŒ–æ¨¡å—äº¤äº’é€»è¾‘ç­‰ã€‚  
3. ä»»åŠ¡æ‹†è§£ä¸åŠ¨æ€é€‚é…ï¼šå€ŸåŠ©VLMåšä»»åŠ¡æ‹†è§£ã€è®°å¿†æ›´æ–°ä¸æŒ‡ä»¤ç”Ÿæˆï¼Œè®©RLä¸“æ³¨å•æ­¥ä¼˜åŠ¿é¢†åŸŸï¼Œè¿™ç§â€œåˆ†è§£ - é€‚é… - æå‡â€çš„æ€è·¯ï¼Œå¯¹å¤„ç†é•¿å‘¨æœŸã€ç¨€ç–å¥–åŠ±ç­‰å¼ºåŒ–å­¦ä¹ éš¾ç‚¹ä»»åŠ¡å…·æœ‰å€Ÿé‰´ä»·å€¼ï¼Œå¯æŒ‡å¯¼ç±»ä¼¼å¤æ‚ä»»åŠ¡åˆ†è§£ä¸æ™ºèƒ½ä½“èƒ½åŠ›é€‚é…æ–¹æ¡ˆè®¾è®¡ã€‚ 

## towards-efficient-online-tuning-of-vlm-agents-via-counterfactual-soft-reinforcement-learning
### Abstract
Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.


## metropolis-hastings-captioning-game--knowledge-fusion-of-vision-language-models-via-decentralized-bayesian-inference
### Abstract
We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse
knowledge of multiple vision-language models (VLMs) by learning from each
other. Although existing methods that combine multiple models suffer from
inference costs and architectural constraints, MHCG avoids these problems by
performing decentralized Bayesian inference through a process resembling a
language game. The knowledge fusion process establishes communication between
two VLM agents alternately captioning images and learning from each other. We
conduct two image-captioning experiments with two VLMs, each pre-trained on a
different dataset. The first experiment demonstrates that MHCG achieves
consistent improvement in reference-free evaluation metrics. The second
experiment investigates how MHCG contributes to sharing VLMs' category-level
vocabulary by observing the occurrence of the vocabulary in the generated
captions.


