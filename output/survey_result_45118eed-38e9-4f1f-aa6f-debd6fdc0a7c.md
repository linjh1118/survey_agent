# Paper List of Terms(reward model)
- [25/06] **Generalist Reward Models: Found Inside Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.23235v1)] [[Code/Page]()] [[TLDR/Notes](#generalist-reward-models--found-inside-large-language-models)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **Listener-Rewarded Thinking in VLMs for Image Preferences**  
[[Paper](http://arxiv.org/pdf/2506.22832v1)] [[Code/Page](https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.)] [[TLDR/Notes](#listener-rewarded-thinking-in-vlms-for-image-preferences)]

- [25/06] **Lost at the Beginning of Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.22058v1)] [[Code/Page]()] [[TLDR/Notes](#lost-at-the-beginning-of-reasoning)]

- [25/06] **TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.22008v1)] [[Code/Page]()] [[TLDR/Notes](#trofi--trajectory-ranked-offline-inverse-reinforcement-learning)]

- [25/06] **PrefPaint: Enhancing Image Inpainting through Expert Human Feedback**  
[[Paper](http://arxiv.org/pdf/2506.21834v1)] [[Code/Page]()] [[TLDR/Notes](#prefpaint--enhancing-image-inpainting-through-expert-human-feedback)]

- [25/06] **SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2506.21669v1)] [[Code/Page]()] [[TLDR/Notes](#seea-r1--tree-structured-reinforcement-fine-tuning-for-self-evolving-embodied-agents)]

- [25/06] **Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents**  
[[Paper](http://arxiv.org/pdf/2506.21252v1)] [[Code/Page]()] [[TLDR/Notes](#agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents)]

- [25/06] **Off-Policy Evaluation and Learning for the Future under Non-Stationarity**  
[[Paper](http://arxiv.org/pdf/2506.20417v1)] [[Code/Page]()] [[TLDR/Notes](#off-policy-evaluation-and-learning-for-the-future-under-non-stationarity)]

- [25/06] **Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations**  
[[Paper](http://arxiv.org/pdf/2506.20294v1)] [[Code/Page]()] [[TLDR/Notes](#ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations)]

- [25/06] **Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference Alignment**  
[[Paper](http://arxiv.org/pdf/2506.19780v2)] [[Code/Page]()] [[TLDR/Notes](#multi-preference-lambda-weighted-listwise-dpo-for-dynamic-preference-alignment)]

- [25/06] **Inference-Time Reward Hacking in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.19248v1)] [[Code/Page]()] [[TLDR/Notes](#inference-time-reward-hacking-in-large-language-models)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.18841v1)] [[Code/Page](https://huggingface.co/THU-KEG/LongWriter-Zero-32B)] [[TLDR/Notes](#longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning)]

- [25/06] **RDPO: Real Data Preference Optimization for Physics Consistency Video Generation**  
[[Paper](http://arxiv.org/pdf/2506.18655v1)] [[Code/Page](https://wwenxu.github.io/RDPO/)] [[TLDR/Notes](#rdpo--real-data-preference-optimization-for-physics-consistency-video-generation)]

- [25/06] **Shrinking the Generation-Verification Gap with Weak Verifiers**  
[[Paper](http://arxiv.org/pdf/2506.18203v1)] [[Code/Page]()] [[TLDR/Notes](#shrinking-the-generation-verification-gap-with-weak-verifiers)]

- [25/06] **Reflective Verbal Reward Design for Pluralistic Alignment**  
[[Paper](http://arxiv.org/pdf/2506.17834v1)] [[Code/Page]()] [[TLDR/Notes](#reflective-verbal-reward-design-for-pluralistic-alignment)]

- [25/06] **DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.17533v1)] [[Code/Page]()] [[TLDR/Notes](#duashepherd--integrating-stepwise-correctness-and-potential-rewards-for-mathematical-reasoning)]

- [25/06] **Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models**  
[[Paper](http://arxiv.org/pdf/2506.16853v1)] [[Code/Page](https://github.com/seminkim/RATTPO.)] [[TLDR/Notes](#reward-agnostic-prompt-optimization-for-text-to-image-diffusion-models)]

- [25/06] **ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2506.16712v1)] [[Code/Page]()] [[TLDR/Notes](#reasongrm--enhancing-generative-reward-models-through-large-reasoning-models)]

- [25/06] **Robust Reward Modeling via Causal Rubrics**  
[[Paper](http://arxiv.org/pdf/2506.16507v1)] [[Code/Page]()] [[TLDR/Notes](#robust-reward-modeling-via-causal-rubrics)]

- [25/06] **Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples**  
[[Paper](http://arxiv.org/pdf/2506.16502v1)] [[Code/Page]()] [[TLDR/Notes](#relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples)]

- [25/06] **GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks**  
[[Paper](http://arxiv.org/pdf/2506.16114v1)] [[Code/Page]()] [[TLDR/Notes](#gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks)]

- [25/06] **AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning**  
[[Paper](http://arxiv.org/pdf/2506.15651v1)] [[Code/Page](https://github.com/cxcscmu/AutoRule.)] [[TLDR/Notes](#autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning)]

- [25/06] **SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling**  
[[Paper](http://arxiv.org/pdf/2506.15498v1)] [[Code/Page]()] [[TLDR/Notes](#spare--single-pass-annotation-with-reference-guided-evaluation-for-automatic-process-supervision-and-reward-modelling)]

- [25/06] **Reward Models in Deep Reinforcement Learning: A Survey**  
[[Paper](http://arxiv.org/pdf/2506.15421v1)] [[Code/Page]()] [[TLDR/Notes](#reward-models-in-deep-reinforcement-learning--a-survey)]

- [25/06] **Adaptive Accompaniment with ReaLchords**  
[[Paper](http://arxiv.org/pdf/2506.14723v1)] [[Code/Page]()] [[TLDR/Notes](#adaptive-accompaniment-with-realchords)]

- [25/06] **SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.14648v1)] [[Code/Page]()] [[TLDR/Notes](#senior--efficient-query-selection-and-preference-guided-exploration-in-preference-based-reinforcement-learning)]

- [25/06] **TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization**  
[[Paper](http://arxiv.org/pdf/2506.14574v1)] [[Code/Page](https://github.com/dvlab-research/TGDPO.)] [[TLDR/Notes](#tgdpo--harnessing-token-level-reward-guidance-for-enhancing-direct-preference-optimization)]

- [25/06] **Adaptive Data Augmentation for Thompson Sampling**  
[[Paper](http://arxiv.org/pdf/2506.14479v1)] [[Code/Page]()] [[TLDR/Notes](#adaptive-data-augmentation-for-thompson-sampling)]

- [25/06] **GRAM: A Generative Foundation Reward Model for Reward Generalization**  
[[Paper](http://arxiv.org/pdf/2506.14175v2)] [[Code/Page]()] [[TLDR/Notes](#gram--a-generative-foundation-reward-model-for-reward-generalization)]

- [25/06] **VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training**  
[[Paper](http://arxiv.org/pdf/2506.13888v1)] [[Code/Page]()] [[TLDR/Notes](#vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.13741v1)] [[Code/Page]()] [[TLDR/Notes](#pb$^2$--preference-space-exploration-via-population-based-methods-in-preference-based-reinforcement-learning)]

- [25/06] **$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts**  
[[Paper](http://arxiv.org/pdf/2506.15733v1)] [[Code/Page]()] [[TLDR/Notes](#$\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts)]

- [25/06] **From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment**  
[[Paper](http://arxiv.org/pdf/2506.12446v2)] [[Code/Page]()] [[TLDR/Notes](#from-outcomes-to-processes--guiding-prm-learning-from-orm-for-inference-time-alignment)]

- [25/06] **Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory**  
[[Paper](http://arxiv.org/pdf/2506.12350v1)] [[Code/Page]()] [[TLDR/Notes](#theoretical-tensions-in-rlhf--reconciling-empirical-success-with-inconsistencies-in-social-choice-theory)]

- [25/06] **TreeRL: LLM Reinforcement Learning with On-Policy Tree Search**  
[[Paper](http://arxiv.org/pdf/2506.11902v1)] [[Code/Page](https://github.com/THUDM/TreeRL.)] [[TLDR/Notes](#treerl--llm-reinforcement-learning-with-on-policy-tree-search)]

- [25/06] **Personalized LLM Decoding via Contrasting Personal Preference**  
[[Paper](http://arxiv.org/pdf/2506.12109v1)] [[Code/Page]()] [[TLDR/Notes](#personalized-llm-decoding-via-contrasting-personal-preference)]

- [25/06] **Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11474v1)] [[Code/Page](https://med-prm.github.io/)] [[TLDR/Notes](#med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards)]

- [25/06] **Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11425v2)] [[Code/Page]()] [[TLDR/Notes](#agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards)]

- [25/06] **ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems**  
[[Paper](http://arxiv.org/pdf/2506.10955v1)] [[Code/Page]()] [[TLDR/Notes](#reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems)]

- [25/06] **EQA-RM: A Generative Embodied Reward Model with Test-time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.10389v1)] [[Code/Page](https://github.com/UNITES-Lab/EQA-RM.)] [[TLDR/Notes](#eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling)]

- [25/06] **Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.21560v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning)]

- [25/06] **Unsupervised Elicitation of Language Models**  
[[Paper](http://arxiv.org/pdf/2506.10139v1)] [[Code/Page]()] [[TLDR/Notes](#unsupervised-elicitation-of-language-models)]

- [25/06] **Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput**  
[[Paper](http://arxiv.org/pdf/2506.10056v1)] [[Code/Page]()] [[TLDR/Notes](#reward-models-enable-scalable-code-verification-by-trading-accuracy-for-throughput)]

- [25/06] **DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision**  
[[Paper](http://arxiv.org/pdf/2506.09814v1)] [[Code/Page]()] [[TLDR/Notes](#dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **Know What You Don't Know: Uncertainty Calibration of Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09338v1)] [[Code/Page]()] [[TLDR/Notes](#know-what-you-don-t-know--uncertainty-calibration-of-process-reward-models)]

- [25/06] **Learning to Reason Across Parallel Samples for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.09014v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-across-parallel-samples-for-llm-reasoning)]



# TLDR/Notes
## generalist-reward-models--found-inside-large-language-models
### Abstract
The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.
### 🌟 论文解读 | 大语言模型中藏着通用奖励模型？LLM对齐新范式来了

### 📌 背景痛点/本文动机
大语言模型（LLM）对齐人类价值观（如帮助性、诚实性）是AI发展的核心挑战，主流的基于人类反馈的强化学习（RLHF）严重依赖用昂贵人类偏好数据训练的奖励模型（RM）。构建优质RM需大规模高质量人类偏好数据集，存在慢、贵、难扩展等问题。后续用AI反馈替代人类反馈的方法（如RLAIF、LLM-as-a-judge）又缺乏严谨理论基础，还易继承裁判模型的风格偏差与偏见。那么，高质量奖励信号是否必须外部获取？这成为关键问题，本文正是基于此展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：发现LLM中 latent 存在通用奖励模型  
论文发现，任何经标准下一个token预测训练的LLM中，天然潜伏着强大的通用奖励模型，将其命名为“内生奖励（endogenous reward）”。无需额外训练，就能从基础模型（预训练或有监督微调模型）中直接提取高质量奖励信号。  

💡 创新点2：理论层面建立与离线逆强化学习的联系  
从理论上证明，这种内生奖励等价于通过离线逆强化学习（IRL）学到的奖励函数。具体而言，LLM的logits可直接解释为软Q函数，借助逆软Bellman算子能从中恢复出奖励函数，为提取奖励函数提供了原理性方法，突破了过往启发式做法。  

💡 创新点3：证明基于内生奖励的RL有效性  
证明用该内生奖励进行后续强化学习后，得到的策略相比基础模型有更优的误差界。RL过程能修正标准模仿学习（下一个token预测）的复合误差，把性能差距从与任务时长相关的二次依赖（O(H²)）降到更优的线性依赖（O(H)）。这是首次从理论上证明LLM强化学习的有效性。  


### 📈 实验结果
大量实验验证了理论：提取内生奖励的方法不仅优于现有LLM-as-a-judge方法，还能超越在昂贵人类标注数据上显式训练的奖励模型。  

### 💬 可借鉴之处
论文表明奖励建模阶段可被一种原理性方法替代——提取预训练阶段已捕获的知识。这为LLM对齐以及多模态模型领域，开辟了更高效、强大且可扩展的新范式，后续在模型对齐、奖励函数设计等方向，都可借鉴这种“挖掘模型内在已有能力”的思路，减少对外部昂贵数据与额外训练的依赖。

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### 🌟 论文解读 | 用知识增强树搜索推理助力大模型攻克分子结构解析难题

### 📌 背景痛点/本文动机
分子结构解析是化学实验分析里的关键任务，要从核磁、红外等光谱数据推导分子结构，专业人员都得花10 - 15分钟分析单个分子，所以用大语言模型（LLM）自动化解析很有必要。但LLM在这任务上有挑战：一是对化学分子结构空间覆盖不足，像噻吩这类特殊杂环结构，LLM常因缺乏子结构知识误判；二是没法准确评估和修正推理过程，树搜索推理需要有效评估反馈，可LLM缺领域知识，做不好 reward model 角色。于是论文要解决这两个问题，提升LLM在分子结构解析的能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出K - MSE框架  
构建知识增强的分子结构解析推理框架K - MSE，把蒙特卡洛树搜索（MCTS）作为插件实现测试时的能力扩展，能适配任意LLM。借助MCTS平衡新解探索和已有解利用，还结合Self - Refine让LLM及时优化之前的解。

💡 创新点2：外部分子子结构知识库  
为弥补LLM化学结构空间覆盖不足，构建外部分子子结构知识库。子结构是化学空间基础元素，知识库通过自动化流程整合子结构和结构描述，给LLM补充领域知识，提升特殊结构推理准确性，减少 atypical 案例错误。

💡 创新点3：专属分子 - 光谱评分器  
设计分子 - 光谱评分器当 reward model，解决LLM解评估不准问题。评分器有分子编码器和光谱编码器，评估分子结构和光谱数据匹配度给奖励分。它还作为LLM和知识库间的检索器，用输入光谱查最相关子结构，减少子结构检索误差，增强推理稳定性。

### 📈 实验结果
在MolPuzzle基准测试上，K - MSE方法效果显著，对GPT - 4o - mini和GPT - 4o都带来超20%的性能提升，证明了框架在增强LLM分子结构解析能力上的有效性。

### 💬 可借鉴之处
1. 领域知识增强思路：面对专业领域任务，LLM通用知识不足时，构建领域子结构知识库补充，这种“外部知识 + LLM”模式可复用在其他专业领域（如生物、材料）任务。  
2. 推理过程评估优化：设计领域专属评分器做 reward model，结合树搜索框架优化推理，为需要深度推理、需评估反馈的复杂任务（如数学证明、代码调试）提供了“评分器 + 树搜索”的推理增强范式。  
3. 插件化框架设计：K - MSE作为插件适配任意LLM，这种解耦式设计方便技术落地，不同场景下可快速集成到现有LLM工作流里，降低技术迁移成本。

## listener-rewarded-thinking-in-vlms-for-image-preferences
### Abstract
Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.
### 🌟 论文解读 | 用Listener增强的RL，让视觉语言模型更懂人类图像偏好

### 📌 背景痛点/本文动机
在生成式建模领域，让视觉 - 语言模型（VLMs）精准捕捉人类视觉偏好是关键难题。现有奖励模型存在泛化能力不足问题，监督微调（SFT）易导致模型记忆训练数据，还需要复杂标注流程。强化学习（RL）里的Group Relative Policy Optimization（GRPO）虽能提升泛化性，但研究发现基于RL的偏好推理模型存在“听众分歧（listener disagreement）”问题：当模型推理轨迹和独立冻结的视觉 - 语言模型（“listener”）对同一输出的评估矛盾时，推理准确率大幅下降。所以，如何解决这种推理矛盾、提升模型与人类偏好的对齐度是本文动机所在。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：首次训练思维链风格推理模型预测人类对生成模型输出的视觉偏好
以往工作较少针对生成模型输出的人类视觉偏好来训练这种思维链推理模型，本文填补了这一空白，为视觉偏好预测提供新的模型训练思路。

💡 创新点2：识别并量化“听众分歧”这一RL视觉偏好建模的主要失效模式
通过分析发现，当模型预测和独立“listener”预测差异增大时，VLM准确率持续下降，将这种现象明确为关键问题并量化，为后续解决方法提供基础。

💡 创新点3：设计面向GRPO的listener - shaped软奖励机制
引入冻结的VLM“listener”，让其独立重新处理推理模型（reasoner）的思维链（排除最终答案token），输出对正确选择的校准置信分数，并整合到RL奖励信号中。这样既惩罚无法说服独立模型的解释，又无需额外人工标注就能提供密集、数据高效的监督，让推理模型不仅答案正确，推理过程也能被“listener”认可。

### 📈 实验结果
在ImageReward测试集上，方法达到67.4%的当前最优准确率；在大规模（120万投票）的Rapidata - HSP基准测试中，大幅超越强GRPO和SFT基线；同时减少了推理矛盾情况，且在分布外（OOD）数据集上表现出色，即便用少量偏好数据训练，也能让输出更校准、OOD鲁棒性更强。

### 💬 可借鉴之处
从方法创新角度，利用独立模型构建奖励机制来对齐推理轨迹和最终决策的思路，为解决模型推理一致性问题提供了新范式；从应用角度，证明listener增强的RL是VLMs中偏好对齐的有效实用工具，为下一代文本到图像、文本到视频系统提供了可扩展的偏好对齐解决方案，在工业界大规模生成模型偏好调优场景有借鉴价值；从问题发现角度，对“听众分歧”这种失效模式的识别和量化，让后续研究者能更关注模型推理过程的一致性问题，推动领域发展。

## lost-at-the-beginning-of-reasoning
### Abstract
Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.
### 🌟 论文解读 | 推理伊始的“迷失”：大模型长链推理首步影响与应对

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务（如数学解题、多跳问答）中借助长链思维（long - CoT）推理取得显著进展，长链推理包含回溯、自我反思与自我修正等机制。然而，大模型在长链推理中的自我修正能力仍未充分探索，且存在“过度思考”（产生冗余推理）现象。同时，受“中间迷失”（lost - in - the - middle）现象启发，研究者思考长链推理中是否存在类似位置偏差，即推理步骤的位置是否影响最终推理结果。本文聚焦长链推理起始步骤，探究其对最终预测的影响，并解决相关问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：揭示长链推理首步关键影响  
通过实验表明，在长链推理中，**第一个推理步骤**对最终预测有着极大影响。初始步骤引入的错误会大幅降低后续推理质量，在DeepSeek - R1和Qwen3等开源推理模型家族中均观察到该现象。例如，当首步错误时，模型得到错误最终答案的概率显著上升（准确率下降40%），凸显当前长链推理大模型自我修正能力的局限。  
💡 创新点2：高效早期剪枝算法  
提出利用奖励模型的高效采样策略（早期剪枝算法）。通过评估第一个推理步骤的质量，识别并保留高质量首步、舍弃次优首步，仅对有前景的推理路径继续生成，在不牺牲准确率的前提下，最多降低70%的推理成本。  
💡 创新点3：LaBoR基准测试集  
构建专门用于评估模型自我修正能力的新基准LaBoR。该基准通过自动流程构建，每个实例包含问题与有缺陷的首步推理，要求模型检测并修正初始错误以得到正确答案，为大模型鲁棒推理研究提供基础。

### 📈 实验结果
- 在探究首步影响的实验中，以AIME24和AIME25数据集的问题为对象，用DS - R1 - Qwen - 7B和Qwen3 - 8B生成推理轨迹，测量各推理步骤与最终结论的语义相似度，发现首步与最终结论语义相似度更高，验证首步对最终结论的关键影响。  
- 早期剪枝算法实验中，在四个长链推理大模型和四个数学、科学推理基准测试上，方法在保持预测准确率的同时，最多降低70%推理预算。  
- LaBoR基准测试中，评估八个开源长链推理大模型，发现初始步骤含错误时性能最多下降11.4%，体现模型在从早期错误恢复方面的脆弱性。

### 💬 可借鉴之处
- 研究视角上，开辟长链推理“起始步骤影响”这一此前未充分探索的方向，为理解大模型推理机制提供新维度，后续研究可围绕推理步骤位置偏差等深入。  
- 方法层面，早期剪枝算法为提升大模型推理效率提供新思路，利用首步质量评估减少冗余推理，在实际部署大模型推理任务时可借鉴以降低成本。  
- 基准构建上，LaBoR为评估大模型自我修正能力提供专门工具，推动该领域标准化评估，后续可基于此基准开展更多模型优化与能力研究。

## trofi--trajectory-ranked-offline-inverse-reinforcement-learning
### Abstract
In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.
### 🌟 论文解读 | TROFI：无预定义奖励下的离线逆强化学习新范式

### 📌 背景痛点/本文动机
在离线强化学习（ORL）中，智能体需依赖带奖励标签的存储转移数据训练，但在游戏开发等实际场景中，奖励函数往往难以获取或定义。同时，现有逆强化学习（IRL）和模仿学习（IL）方法多假设演示轨迹最优，且适配在线场景，无法直接用于含非专家、无奖励数据的离线场景。为解决这些问题，论文提出TROFI方法，旨在无预定义奖励函数和最优专家演示下学习离线策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：弱监督奖励学习与策略训练流程  
TROFI分三步实现离线策略学习：首先通过T - REX（Trajectory - ranked Reward EXtrapolation）利用人类偏好进行弱监督学习，得到奖励模型\(\hat{r}_\theta\)；接着用该奖励模型为原始无奖励离线数据集打标签；最后基于打标后的数据集，用TD3 + BC（Twin Delayed Deep Deterministic plus Behavioral Cloning）训练参数化策略\(\pi_\theta\)。

💡 创新点2：无需最优轨迹的奖励学习  
区别于多数IRL/IL方法对最优演示的假设，TROFI借助T - REX仅需轨迹的定性排序信息来学习奖励函数。T - REX通过神经网路近似状态下的奖励，利用轨迹排名约束（如\(\tau_i \prec \tau_j\)时，轨迹\(\tau_j\)的累计奖励预测值大于\(\tau_i\)）构建损失函数训练奖励模型，无需知晓真实奖励函数。

### 📈 实验结果
在D4RL基准测试中，TROFI持续超越基线方法，性能媲美使用真实奖励训练策略的效果；同时在3D游戏环境中验证了方法有效性。此外，对奖励模型的研究表明，设计良好、易学习的奖励函数对保证值函数与实际未来折扣奖励对齐至关重要，凸显奖励函数在离线强化学习场景的关键作用。

### 💬 可借鉴之处
1. 方法层面：为无奖励函数或难获取最优演示的离线强化学习场景提供了弱监督解决方案，结合T - REX和TD3 + BC的流程可复用至类似数据驱动且奖励稀缺的任务（如游戏NPC行为学习、复杂工业场景策略优化）。  
2. 实践层面：展示了游戏开发者如何高效利用大规模玩家无奖励数据，无需手动设计奖励函数就能训练智能体，为产业界利用玩家行为数据提供新思路。  
3. 理论与分析层面：强调了离线强化学习中奖励函数设计的重要性，为后续研究中奖励函数的构建与优化提供了经验性指导，提示需关注奖励函数的可学习性与和实际任务目标的对齐性。

## prefpaint--enhancing-image-inpainting-through-expert-human-feedback
### Abstract
Inpainting, the process of filling missing or corrupted image parts, has
broad applications, including medical imaging. However, in specialized fields
like medical polyps imaging, where accuracy and reliability are critical,
inpainting models can generate inaccurate images, leading to significant errors
in medical diagnosis and treatment. To ensure reliability, medical images
should be annotated by experts like oncologists for effective model training.
We propose PrefPaint, an approach that incorporates human feedback into the
training process of Stable Diffusion Inpainting, bypassing the need for
computationally expensive reward models. In addition, we develop a web-based
interface streamlines training, fine-tuning, and inference. This interactive
interface provides a smooth and intuitive user experience, making it easier to
offer feedback and manage the fine-tuning process. User study on various
domains shows that PrefPaint outperforms existing methods, reducing visual
inconsistencies and improving image rendering, particularly in medical
contexts, where our model generates more realistic polyps images.
### 🌟 论文解读 | PrefPaint：借专家反馈之力，革新图像修复（医疗场景尤甚）

### 📌 背景痛点/本文动机
图像修复（Inpainting）在医疗成像等领域应用广泛，可填补图像缺失或损坏部分。但在医疗息肉成像这类对准确性和可靠性要求极高的专业领域，现有修复模型易生成不准确图像，给医疗诊断和治疗带来重大误差。原因在于医疗图像修复需放射科医生、肿瘤学家等专家标注数据来训练模型，可现实中缺乏足够带标注的息肉图像，且息肉形状、大小、外观多变，让模型学习易出现错误模式。同时，传统基于人类反馈的强化学习（RLHF）需训练昂贵奖励模型，耗时耗力。因此，论文希望将人类专家反馈融入训练，提升医疗等场景下图像修复的可靠性与准确性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出PrefPaint方法，融合人类反馈到Stable Diffusion Inpainting训练  
借助Direct Preference Optimization（DPO），在去噪过程每一步直接更新参数，绕开了计算成本高昂的奖励模型，把医疗专家等的知识直接纳入训练，提升图像生成在医疗诊断等场景的精准度与可靠性。

💡 创新点2：开发易用的基于Web的交互界面  
面向医疗专业人员（即便IT背景不强也能上手）打造界面，简化训练、微调与推理流程，能方便收集专家反馈，还可跟踪模型进展与性能提升，让医疗专家能轻松参与到模型优化中。

### 📈 实验结果
多领域用户研究表明，PrefPaint超越现有方法：在人物图像修复上，减少视觉不一致并提升面部特征渲染；风景图像修复中，整合性更好且减少类似边框的不自然感；医疗领域里，生成的息肉图像更逼真、更贴合医疗场景上下文。

### 💬 可借鉴之处
1. 领域适配思路：针对医疗这类专业且数据标注难的领域，引入人类专家反馈来优化模型，为垂直领域AI模型训练提供了“专业知识注入”的参考路径。  
2. 工程落地角度：打造低门槛交互界面，降低专业人员使用AI工具的门槛，启示在AI应用落地时要重视用户体验与工具易用性，让领域专家能高效参与模型迭代。  
3. 技术优化方向：用DPO绕开传统RLHF奖励模型的高成本，为 diffusion 模型这类生成式模型的高效调优提供了轻量化技术路线参考。

## seea-r1--tree-structured-reinforcement-fine-tuning-for-self-evolving-embodied-agents
### Abstract
Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.
### 🌟 论文解读 | SEEA-R1：让具身智能自主进化的树结构强化微调框架

### 📌 背景痛点/本文动机
具身智能体需在复杂、长时程环境中完成任务，既要有底层感知与运动控制，又得具备高层推理、规划和决策能力。尽管大语言模型（LLMs）和多模态大语言模型（MLLMs）提升了智能体的抽象推理与感知能力，但在开放式具身场景中仍有不足：LLMs 缺乏感知基础，MLLMs 在结构化多步规划、长期连贯性与动态环境适应上存在挑战。为解决通用具身智能问题，智能体需具备“自我进化”能力——自主生成训练信号并通过闭环学习优化推理。  

强化微调（RFT）虽为有前景的范式，但在具身场景下面临两大核心挑战：  
1. **多步推理任务缺中间反馈**：长时程、延迟稀疏奖励的具身任务中，难对中间决策进行“ credit assignment（ credit 分配）”，导致 RFT 难有效引导策略学习；  
2. **手工奖励函数泛化性差**：现有 RFT 依赖特定模拟器或任务的奖励信号，无法泛化到新环境，阻碍智能体跨任务自我提升。  


### 🚀 核心方法（介绍本文的几个创新点）
为应对上述挑战，论文提出 **SEEA-R1**（Self-Evolving Embodied Agents - R1），首个为具身智能体“自我进化”能力量身设计的 RFT 框架，核心包含两大创新组件：  

💡 创新点1：Tree-GRPO（基于树结构的分组相对策略优化）  
将蒙特卡洛树搜索（MCTS）融入 Group Relative Policy Optimization（GRPO），让智能体可探索多条可能轨迹，把稀疏的“结果奖励”转化为结构化、逐步的“过程信号”。这一步解决了多步轨迹中 credit 分配难题，助力长序列动作的推理与决策。  

💡 创新点2：MGRM（多模态生成式奖励模型）  
为摆脱对手工或环境特定奖励函数的依赖，MGRM 从多模态、多轮次的轨迹数据中训练而来，提供“与任务无关”的奖励估计，支撑智能体在多样具身场景下的泛化与自我提升。  


### 📈 实验结果
论文在 **ALFWorld 基准测试**（考验智能体将抽象目标映射为视觉锚定动作序列的能力）上验证 SEEA-R1：  
- 文本任务与多模态任务分别取得 **85.07%** 和 **36.19%** 的成功率，超越 Qwen2.5-VL、GPT-4o 等前沿模型；  
- 用 MGRM 自监督信号替代真实环境奖励时，文本与多模态任务仍达 **80.30%** 和 **23.88%**，超过所有开源基线。  
这些结果证明 SEEA-R1 作为“自进化具身智能体”的有效性与扩展性。  


### 💬 可借鉴之处
1. **框架创新性**：首次提出面向具身智能体“自我进化”的 RFT 框架，为具身智能长期研究提供新范式；  
2. **技术融合**：Tree-GRPO 结合 MCTS 与策略优化，解决多步推理 credit 分配；MGRM 用生成式模型替代手工奖励，推动泛化能力；  
3. **开源与生态**：团队计划开源完整框架与模块（含 MGRM、训练管线等），助力具身智能社区研究；  
4. **实验设计**：在 ALFWorld 上同时验证“有环境奖励”与“自监督奖励”场景，全面展现自进化潜力。  

SEEA-R1 为具身智能的“自主进化”指明了新方向，从技术创新到开源生态，都为该领域后续研究铺好了路。

## agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents
### Abstract
As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.
### 🌟 论文解读 | Agent-RewardBench：面向真实世界多模态智能体的奖励建模统一基准

### 📌 背景痛点/本文动机
随着多模态大语言模型（MLLMs）不断发展，多模态智能体在网页导航、具身智能等真实世界任务中展现出潜力。但这些智能体因缺乏外部反馈，在自我修正与泛化能力上存在不足。利用奖励模型作为外部反馈是有前景的方向，然而目前缺乏针对智能体奖励模型选择的明确指导，也缺少专门面向智能体的奖励基准。因此，构建针对智能体的奖励基准迫在眉睫。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多维度与真实场景覆盖  
提出的Agent - RewardBench基准涵盖感知、规划、安全3个评估维度与7个真实世界智能体场景（如移动、网页、自动驾驶、Minecraft等）。在感知维度评估视觉理解与 grounding 的奖励能力；规划维度聚焦奖励模型对序列决策和任务分解的评估能力；安全维度考察在攻击和不安全环境场景下的奖励能力。

💡 创新点2：步骤级奖励评估  
不同于仅关注最终结果的评估，该基准支持在任务的单个步骤层面评估智能体能力，为规划过程中的性能提供更细致的视角，能更详细地反馈模型在每一步的奖励能力表现。

💡 创新点3：难度适配与高质量数据保障  
通过三种策略保障难度与数据质量：从10种不同的多模态模型（涵盖黑盒与白盒模型）采样以保证多样性；用小模型过滤数据来控制任务难度，避免过易或过难；人工验证数据以确保数据完整性与高质量。

### 📈 实验结果
实验表明现有先进多模态模型在该基准上表现有限：黑盒模型gemini - 1.5 - pro准确率仅61.6%，GPT - 4o为61.4%，Claude - 3.5 - Sonnet为57.9%，体现基准挑战性；像GPT - 4o这样较强的模型在安全奖励建模上准确率仅39.2%，说明智能体领域安全奖励建模仍不足；开源模型如Llama - 3.2 - 11B - Vision - Instruct在感知和规划上分别仅得53.5%和50.6%，凸显智能体奖励模型专项训练的必要性。

### 💬 可借鉴之处
1. 填补领域空白：Agent - RewardBench是首个评估多步骤智能体任务中模型奖励建模能力的基准，为从模仿学习向带反馈学习的过渡提供关键评估手段。 
2. 丰富评估维度与场景：涵盖多维度、多真实场景且采用多模型真实样本与人工验证保障数据质量，为后续奖励模型评估提供了全面且高质量的参考范式。 
3. 指导下游任务：展现出与下游任务的强关联性，说明准确的奖励建模对提升实际应用中搜索性能至关重要，为后续优化智能体性能指明方向。

## off-policy-evaluation-and-learning-for-the-future-under-non-stationarity
### Abstract
We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.
### 🌟 论文解读 | 非平稳环境下面向未来的离策略评估与学习

### 📌 背景痛点/本文动机
在推荐系统、精准医疗等决策问题中，离策略评估（OPE）用于利用历史日志数据评估新策略效果，但多数实际场景处于**非平稳环境**（如电商推荐中用户偏好、奖励随时间变化）。现有方法假设环境平稳或依赖严格奖励建模假设，难以准确估计未来策略价值——因为历史数据中没有未来环境的观测，直接套用传统OPE方法会引入显著偏差。例如电商推荐需用上个月旧策略数据估计下个月新策略价值，而用户行为的周度、季节等时间模式在历史与未来存在一致性却未被充分利用，这成为关键痛点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：定义未来离策略评估（F - OPE）问题并提出OPFV estimator  
首次形式化**未来离策略评估（F - OPE）**问题：在非平稳环境下，仅用历史日志数据估计未来任意时间点的策略价值。提出**Off - Policy Estimator for the Future Value (OPFV)**，其核心是利用时间序列中稳定的结构（如季节、周度、节假日效应）——这些效应在历史和未来数据中具有一致性。通过**新型重要性加权**，OPFV能无偏估计时间序列结构刻画的效应（称为时间序列特征），同时用回归模型处理剩余效应以平衡偏差和方差。理论分析证明了OPFV在特定条件下实现低偏差估计。

💡 创新点2：扩展OPFV实现未来离策略学习（F - OPL）  
基于OPFV estimator，进一步提出**新的策略梯度方法**，仅用历史数据主动学习未来的优质策略。该方法扩展OPFV以估计未来策略价值的策略梯度，从而指导策略优化，解决了非平稳环境下“用历史数据学未来策略”的难题。

💡 创新点3：数据驱动的时间序列特征选择  
分析时间序列特征选择对OPFV偏差 - 方差权衡的影响，提出**简单的数据驱动流程**选择时间序列特征，以优化OPFV的估计精度，让方法更具实用性。

### 📈 实验结果
在合成数据与真实推荐系统非平稳数据的实验中，OPFV在**估计未来策略价值**和**优化未来策略**两方面均显著优于现有方法。实验覆盖多种非平稳场景（如分布的平滑/突变变化），验证了OPFV在不同设定下的鲁棒性与有效性。

### 💬 可借鉴之处
1. 问题定义层面：将“时间戳”作为随机变量纳入建模，统一处理平滑与突变的非平稳性，为非平稳环境下的策略评估与学习提供了更普适的框架思路。  
2. 方法设计层面：挖掘时间序列中跨历史 - 未来的稳定结构（如周期效应），并用新型重要性加权和回归结合的方式利用这些结构，为“无未来观测时估计未来价值”提供了可复用的技术范式。  
3. 落地实践层面：数据驱动的特征选择流程与基于OPFV的策略梯度方法，为工业界（如推荐系统、广告投放）在非平稳场景下快速迭代策略提供了落地路径，减少在线实验成本与伦理风险。

## ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations
### Abstract
Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.
### 🌟 论文解读 | 突破局部最优！Ctrl - Z Sampling：可控随机之字形探索的扩散采样

### 📌 背景痛点/本文动机
扩散模型在条件生成任务中表现出色，它通过迭代去噪将高斯噪声逐步转化为目标数据分布。但由于潜在空间复杂和初始化欠佳，扩散模型常收敛到局部最优——生成结果局部视觉连贯，却全局不一致或条件错位。以往方法如强化引导信号、操纵初始噪声分布等，在引导控制或逃逸局部最优的灵活性上存在不足，要么引导效果不佳，要么需大量候选状态评估，实用性受限。因此，需要一种能有效检测并逃逸局部最优的采样策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出可控随机之字形采样（Ctrl - Z Sampling）
该方法将条件扩散生成视为“爬山”过程，基于奖励模型检测潜在局部最优（依据预测分数的停滞情况）。一旦检测到，注入噪声并回退到更早、更具噪声的时间步，以此逃离当前优化平台，鼓励在潜在空间探索更优区域。

💡 创新点2：动态平衡 backward exploration 与 forward refinement
奖励模型评估候选状态，仅接受能带来更优未来预测的状态；若附近无改进，会继续回退到噪声更大的水平以增加逃逸可能性。这种可控随机之字形过程实现了前向优化与后向探索的动态交替，提升生成结果的条件对齐度与视觉质量，且模型无关，适配现有扩散框架。

### 📈 实验结果
在文本到图像基准测试中，Ctrl - Z Sampling 在多个指标上实现生成质量的显著提升，仅需约 7.6 倍函数评估（NFEs）增加。这表明其在推理时能以适度开销大幅改进生成效果，是昂贵测试时缩放方法的实用替代方案，尤其适用于本地推理场景。

### 💬 可借鉴之处
1. 从“爬山”视角分析扩散模型条件生成，为理解模型收敛到局部最优的问题提供新角度，启发后续对生成过程动力学的研究。
2. Ctrl - Z Sampling 的设计思路——通过奖励引导的随机探索、自适应噪声注入实现可控逃逸局部最优，可借鉴到其他需在复杂空间中优化、易陷局部最优的生成或优化任务中，如其他类型生成模型（变分自编码器等）的采样策略改进。
3. 模型无关性使其能无缝融入现有扩散框架，为工程实践中提升扩散模型生成质量提供即插即用的思路，无需重新训练模型，降低应用成本。

## multi-preference-lambda-weighted-listwise-dpo-for-dynamic-preference-alignment
### Abstract
While large-scale unsupervised language models (LMs) capture broad world
knowledge and reasoning capabilities, steering their behavior toward desired
objectives remains challenging due to the lack of explicit supervision.
Existing alignment techniques, such as reinforcement learning from human
feedback (RLHF), rely on training a reward model and performing reinforcement
learning to align with human preferences. However, RLHF is often
computationally intensive, unstable, and sensitive to hyperparameters.
  To address these limitations, Direct Preference Optimization (DPO) was
introduced as a lightweight and stable alternative, enabling direct alignment
of language models with pairwise preference data via classification loss.
However, DPO and its extensions generally assume a single static preference
distribution, limiting flexibility in multi-objective or dynamic alignment
settings.
  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted
Listwise DPO, which extends DPO to incorporate multiple human preference
dimensions (e.g., helpfulness, harmlessness, informativeness) and enables
dynamic interpolation through a controllable simplex-weighted formulation. Our
method supports both listwise preference feedback and flexible alignment across
varying user intents without re-training. Empirical and theoretical analysis
demonstrates that our method is as effective as traditional DPO on static
objectives while offering greater generality and adaptability for real-world
deployment.
### 🌟 论文解读 | 多偏好动态对齐新框架：Multi - Preference Lambda - weighted Listwise DPO

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽具备强大的知识和推理能力，但由于无监督训练目标未显式编码人类偏好，难以与人类价值观和意图对齐。现有强化学习从人类反馈（RLHF）方法虽能一定程度解决对齐问题，但存在计算成本高、训练不稳定、对超参数敏感等缺陷。Direct Preference Optimization（DPO）作为RLHF的轻量替代方案，能直接通过分类损失对齐模型与成对偏好数据，然而DPO及其扩展通常假设单一静态偏好分布，在多目标或动态对齐场景下灵活性受限。实际应用中对齐常涉及多个偏好维度（如帮助性、无害性、信息性等），且不同场景下各维度重要性不同，因此需要更灵活的框架来处理多偏好维度并适应不同偏好权衡。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Multi - Preference Lambda - weighted Listwise DPO框架  
该框架扩展了DPO，使其能纳入多个人类偏好维度（如帮助性、无害性、信息性等），并通过从概率单纯形中抽取的权重向量λ实现动态插值。它将DPO从成对偏好场景扩展到列表式（listwise）偏好场景，支持在动态变化的偏好配置下进行对齐，推理时用户或下游系统可通过选择不同λ向量来调节对齐行为，无需重新训练模型。  

💡 创新点2：理论推导支撑与列表式损失设计  
从KL正则化的强化学习目标出发进行详细理论推导，得到能聚合多个奖励维度的简洁列表式损失。这种方式直接在策略空间优化列表式损失，实现端到端的微调，相比通过有监督奖励建模的方式更具优势，能更好地利用列表式偏好反馈来学习模型。  

### 📈 实验结果
文中通过在多目标对齐基准测试上的实验，证明了该方法在静态目标上与传统DPO效果相当，同时在实际部署中具有更强的通用性和适应性，在多目标对齐任务中相比传统DPO能取得有竞争力或更优的结果。  

### 💬 可借鉴之处
1. 多偏好维度处理：为处理实际应用中多维度且动态变化的人类偏好提供了新思路，不再局限于单一静态偏好分布，能适应不同场景下的偏好权衡需求。  
2. 灵活对齐与推理：实现了无需重新训练模型即可在推理时调节对齐行为，为下游应用提供了更灵活的控制方式，降低了应用成本。  
3. 理论与实践结合：从理论推导出发设计损失函数，保证了方法的合理性，同时在实验中验证了有效性，这种从理论到实践的思路值得借鉴，为后续相关方法研究提供了范式参考。

## inference-time-reward-hacking-in-large-language-models
### Abstract
A common paradigm to improve the performance of large language models is
optimizing for a reward model. Reward models assign a numerical score to LLM
outputs indicating, for example, which response would likely be preferred by a
user or is most aligned with safety goals. However, reward models are never
perfect. They inevitably function as proxies for complex desiderata such as
correctness, helpfulness, and safety. By overoptimizing for a misspecified
reward, we can subvert intended alignment goals and reduce overall performance
-- a phenomenon commonly referred to as reward hacking. In this work, we
characterize reward hacking in inference-time alignment and demonstrate when
and how we can mitigate it by hedging on the proxy reward. We study this
phenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we
introduce Best-of-Poisson (BoP) that provides an efficient, near-exact
approximation of the optimal reward-KL divergence policy at inference time. We
show that the characteristic pattern of hacking as observed in practice (where
the true reward first increases before declining) is an inevitable property of
a broad class of inference-time mechanisms, including BoN and BoP. To counter
this effect, hedging offers a tactical choice to avoid placing undue confidence
in high but potentially misleading proxy reward signals. We introduce
HedgeTune, an efficient algorithm to find the optimal inference-time parameter
and avoid reward hacking. We demonstrate through experiments that hedging
mitigates reward hacking and achieves superior distortion-reward tradeoffs with
minimal computational overhead.
### 🌟 论文解读 | 大语言模型推理时的奖励黑客攻击：分析与缓解

### 📌 背景痛点/本文动机
在大语言模型（LLM）的优化中，基于奖励模型的优化是提升性能的常用范式。奖励模型会给LLM输出分配数值分数，用于指示如用户偏好、安全目标契合度等信息。但奖励模型并非完美，它只是正确性、帮助性、安全性等复杂需求的“代理”。过度优化这种“指定不当”的奖励会引发“奖励黑客攻击”现象，破坏原本的对齐目标并降低整体性能。当前推理时对齐方法中，奖励黑客攻击在实践里已有观测，但针对推理时方法的理论分析与缓解手段较少，这成为AI对齐领域的核心挑战。本文旨在分析推理时对齐中奖励黑客攻击现象，并探索利用代理奖励信号同时缓解攻击的方式。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：刻画推理时奖励黑客攻击并提出通用缓解框架  
从数学层面刻画了推理时奖励黑客攻击现象（如定理1），明确这类攻击在包括Best - of - n（BoN）、新提出的Best - of - Poisson（BoP）等广泛推理时机制中，存在真实奖励先升后降的固有特性。同时给出了通过“对冲（hedging）”代理奖励来缓解攻击的通用框架，避免对高但可能误导的代理奖励信号过度信任。

💡 创新点2：提出Best - of - Poisson（BoP）推理时对齐策略  
BoP的核心思路是依据泊松分布选择BoN的采样数n来运行。理论证明其能在推理时实现近最优的奖励 - 失真权衡，仅用单个可调参数，在奖励均匀分布时能以约10⁻³量级的KL差距近似最优代理奖励倾斜解，可在推理时覆盖整个奖励 - 失真区域，成为计算高效且KL - 奖励权衡损失可忽略的最优倾斜分布替代方案。

💡 创新点3：提出HedgeTune算法  
这是用于在BoN、Soft - Best - of - n（SBoN）、BoP中调节参数以对冲奖励黑客攻击的高效算法（如算法4）。通过该算法能找到推理时最优参数，避免奖励黑客攻击，例如为BoN和BoP找到最优采样数n，为SBoN在固定n时找到最大化真实奖励的逆温度λ等。

### 📈 实验结果
文中通过实验展示对冲手段能缓解奖励黑客攻击，在计算开销极小的情况下实现更优的失真 - 奖励权衡。如在图1相关实验中，HedgeTune能为BoN、SBoN、BoP三种推理时机制成功找到“黑客攻击阈值”，BoN和BoP下能找到最优采样数n，SBoN下能找到对应逆温度λ（当无法用λ实现阈值时返回最佳可达奖励），验证了方法在平衡奖励与参考分布KL散度方面的有效性。

### 💬 可借鉴之处
1. 对奖励模型作为“代理”存在的固有缺陷及引发的奖励黑客攻击问题，提供了理论层面的深入分析，为后续理解推理时对齐方法的风险提供了基础。
2. 提出的BoP策略为推理时对齐提供了新的高效实现方式，在平衡计算效率与奖励 - KL权衡上有参考价值，可启发后续推理时优化策略设计。
3. HedgeTune算法给出了一种在实际中调节推理时对齐方法参数以避免过度优化代理奖励的可行方案，为工程实践中部署LLM推理时优化提供了工具思路，能指导如何在利用代理奖励信号和防止奖励黑客攻击间取得平衡。

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### 🌟 论文解读 | ReasonFlux-PRM：面向大模型长思维链推理的轨迹感知型过程奖励模型

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的复杂推理场景（如数学解题）中，Process Reward Models（PRMs，过程奖励模型）是监督中间推理步骤的有力工具。不过现有PRMs存在明显局限：它们主要基于模型最终输出训练，难以对**轨迹 - 响应（trajectory - response）**这类新兴输出形式的中间推理轨迹进行鲁棒评估。像Deepseek - R1等前沿推理模型会生成“冗长、欠规整的中间思考轨迹 + 简洁最终响应”的轨迹 - 响应对，这类数据常被用于小模型蒸馏，但现有PRMs因与中间轨迹在结构、格式上不匹配，且训练时缺乏带奖励的轨迹 - 响应数据，在监督这类数据时效果不佳甚至会损害下游训练。所以，如何让PRMs既能监督最终响应，又能有效评估中间思考轨迹，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出轨迹感知的PRM——ReasonFlux - PRM  
ReasonFlux - PRM专为评估轨迹 - 响应型推理痕迹设计，融合了**步骤级（step - level）**和**轨迹级（trajectory - level）**监督。它在涵盖数学和科学推理的10k高质量轨迹 - 响应对 curated 数据集上训练，能为思考轨迹内的每个步骤提供细粒度奖励作为监督信号，让模型中间思考轨迹与最终响应更对齐，解决了现有PRMs对中间轨迹监督能力不足的问题。  

💡 创新点2：多场景适配的奖励监督  
ReasonFlux - PRM适配离线和在线多种场景：  
- 离线场景：为轨迹 - 响应对打分，筛选高质量数据，助力小模型下游有监督微调的训练数据精选；  
- 在线场景：融入GRPO等策略优化过程，为强化学习（RL）中的策略优化提供细粒度过程奖励；  
- 测试时缩放（test - time scaling）：通过奖励引导的Best - of - N策略，评估多个生成响应并选最优，提升推理性能。  


### 📈 实验结果
在AIME、MATH500、GPQA - Diamond等挑战性下游基准测试中，ReasonFlux - PRM展现出优异性能：  
- 数据选择方面：ReasonFlux - PRM - 7B比强基线（如Qwen2.5 - Math - PRM - 72B）和人工策划基线选出的数据集质量更高；  
- 性能提升方面：ReasonFlux - PRM - 7B在有监督微调中平均提升12.1%，强化学习中平均提升4.5%，测试时缩放中平均提升6.3%；  
- 资源友好型发布：还发布了ReasonFlux - PRM - 1.5B，适配资源受限场景与边缘部署。  


### 💬 可借鉴之处
1. 问题定义与分析角度：针对新兴的轨迹 - 响应蒸馏数据趋势，深入分析现有PRMs在监督中间轨迹时的问题（结构格式不匹配、训练数据缺失），这种从产业新数据形态反推技术痛点的思路，为后续研究锚定方向提供参考；  
2. 多粒度监督融合：将步骤级和轨迹级监督结合，为处理“长链条、多阶段”的推理类任务提供了细粒度奖励设计的范例，可迁移到代码生成、复杂决策等需分步评估的场景；  
3. 多场景工程落地：从离线数据筛选、在线RL优化到测试时增强，完整覆盖大模型训练 - 推理全流程的奖励监督，展示了技术方案在产业级落地中的多维度价值，为打造端到端的大模型推理增强管线提供了实践模板；  
4. 资源分层发布：同时提供7B和1.5B规模模型，兼顾高性能与资源受限场景，体现了技术普惠性，在实际业务中可根据算力、延迟等需求灵活选择，平衡效果与成本。  

## longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning
### Abstract
Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B
### 🌟 论文解读 | LongWriter-Zero：用强化学习突破超长文本生成难题

### 📌 背景痛点/本文动机
超长文本生成（如万字级报告、叙事创作等）是大语言模型（LLM）在实际场景中至关重要的能力，但现有技术面临两大核心挑战：一是模型生成长度存在上限，二是随着文本长度增加，内容质量（连贯性、一致性、结构合理性等）会显著下降。  

此前主流方案（如LongWriter）依赖**有监督微调（SFT）**，即在人工构造的“指令 - 长文本输出”配对数据上训练模型。但这种方式存在明显缺陷：  
- 构造高质量的合成SFT数据成本极高、难度大；  
- 合成数据往往缺乏连贯性与一致性，且风格单一、过度“人工化”；  
- SFT的最大似然目标无法显式优化全局层面的文本属性（如整体连贯性、格式一致性）。  

为突破这些限制，本文提出**完全从零开始、不依赖任何标注/合成数据**的强化学习（RL）方案，让LLM自主“进化”出超长高质量文本生成能力。  


### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于强化学习的无监督超长文本生成框架  
传统SFT依赖固定参考文本，而本文采用强化学习，让模型通过**奖励信号**优化长文本生成的全局目标（无需人工构造SFT数据集）。具体采用Group Relative Policy Optimization（GRPO）算法训练策略网络：从基础模型（如Qwen2.5 - 32B）出发，让模型在“写作过程中自主规划与迭代”，逐步掌握超长文本生成能力。  

💡 创新点2：多维度奖励模型设计（Reward Design）  
针对开放域文本生成的复杂性（主观性、多维度），设计**复合奖励函数**，整合多个专项奖励模型（RM），分别引导模型优化以下关键维度：  
- 长度控制（Length RM）：确保输出满足“超长”需求，同时避免无意义冗余；  
- 写作质量（Quality RM）：评估内容流畅度、逻辑性、专业性等；  
- 结构格式（Structure RM）：保障文本结构合理（如分章节、层次清晰）。  

💡 创新点3：测试时缩放（Test - time Scaling）与持续预训练（Continual Pretraining）  
- 测试时缩放：借鉴大模型在数学/代码任务中“长思维链（CoT）”的成功经验，探索在超长文本生成中引入长CoT，增强模型推理与规划能力；  
- 持续预训练：在长文本素材与推理数据上持续预训练，进一步提升RL训练后模型的性能上限。  


### 📈 实验结果
- 基准测试碾压传统SFT：基于Qwen2.5 - 32B训练的LongWriter - Zero，在WritingBench、Arena - Write等长文本写作基准测试中，**全面超越传统SFT方法**；  
- 超越千亿参数模型：在多项指标上击败DeepSeek R1、Qwen3 - 235B等百 billion + 规模的大模型，刷新SOTA；  
- 开源资源丰富：模型 checkpoint 和数据已开源（https://huggingface.co/THU - KEG/LongWriter - Zero - 32B），为社区提供了可复现、可扩展的基础。  


### 💬 可借鉴之处
1. 范式创新：证明强化学习可在“无标注/合成数据”场景下，激活LLM的超长文本生成能力，为大模型能力解锁提供了“非SFT”的新范式；  
2. 奖励工程：多维度复合奖励模型的设计思路，可迁移到其他开放域生成任务（如创意写作、多轮对话），用于刻画“主观性强、无明确ground - truth”场景下的质量评估；  
3. 训练策略：测试时缩放（长CoT）与持续预训练的组合，为提升大模型长文本推理、生成的上限提供了可复用的技术路线；  
4. 落地价值：针对真实世界“超长文本需求”（如报告撰写、法律文书、教育内容创作），提供了更优质的技术方案，推动LLM在专业领域的落地。  


LongWriter - Zero的工作不仅解决了超长文本生成的技术痛点，更展示了强化学习在大模型能力进化中的潜力——无需依赖大量人工标注，也能让模型“自主学习”复杂任务的完成能力。这为大模型研发范式、奖励机制设计等方向，都带来了极具启发性的参考。

## rdpo--real-data-preference-optimization-for-physics-consistency-video-generation
### Abstract
Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/
### 🌟 论文解读 | RDPO：从真实视频中提炼物理先验，革新视频生成的物理一致性

### 📌 背景痛点/本文动机
视频生成技术在视觉质量上取得了显著进展，但要忠实复现真实世界的物理规律仍颇具挑战。基于偏好的模型后训练虽有望提升物理一致性，却依赖昂贵的人工标注数据集或尚未成熟的奖励模型。一方面，构建能检测任意视频物理违规的奖励函数仍是开放难题；另一方面，大规模人工偏好数据集的制作成本高、耗时长，且人类判断存在主观性差异。因此，迫切需要一种高效、无需标注的偏好优化策略来针对性提升物理真实性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出无标注框架RDPO  
Real Data Preference Optimization（RDPO）是一个无需人工标注的偏好优化框架，旨在直接从真实世界视频中提炼物理先验。它绕开了传统DPO或RLHF对人类输入的依赖，以真实视频片段作为物理动态信息的“黄金标准”，探索利用真实世界视频固有物理先验的高效路径，而非对这些视频反复训练。  

💡 创新点2：自动构建偏好对的方法  
RDPO借助预训练生成器对真实视频序列进行反向采样，自动构建在物理正确性上具有统计区分度的偏好对。在反向采样过程中，通过选择性利用 latent 表示来构造视频，这些 latent 表示既源于真实数据（富含真实物理信息），又与模型固有生成分布保持对齐（避免过度改变模型视觉风格或产生分布外输出）。  

💡 创新点3：多阶段迭代训练机制  
采用多阶段迭代训练调度，引导生成器逐步更好地遵循物理定律。利用真实视频中探索到的动态信息，让生成视频在动作连贯性与物理真实性上得到显著提升。  


### 📈 实验结果
论文通过在多个基准测试与人类评估中验证，RDPO 在多个维度（如物理一致性、视频整体质量等）实现了提升，有力证明了其在改进不同基线模型物理一致性与视频质量方面的有效性；同时还对比分析了 RDPO 与依赖手动标注的传统 DPO 方法，探索了二者结合的潜在协同效益。

### 💬 可借鉴之处
1. 无标注范式的创新：为解决需大量人工标注的任务提供了思路，展示了如何从真实数据中自动挖掘监督信号，减少对人工标注的依赖。  
2. 物理先验的利用：在视频生成这类需遵循现实世界规律的任务中，提供了从真实数据提炼领域先验（如物理规律）的范例，可启发其他需结合现实知识的生成类任务（如模拟仿真、虚拟场景构建等）。  
3. 迭代训练与分布对齐：多阶段迭代训练调度以及 latent 空间对齐的思路，对平衡“引入新先验”与“保持模型原有能力/分布”具有参考价值，在模型微调、领域适配等场景中或可复用。  

## shrinking-the-generation-verification-gap-with-weak-verifiers
### Abstract
Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.
### 🌟 论文解读 | 用弱验证器缩小生成-验证差距：Weaver框架的创新之路

### 📌 背景痛点/本文动机
在部署语言模型（LM）时，验证模型响应的质量或正确性是核心挑战，这一问题在数据集整理、模型对齐和推理时决策等LM pipeline各环节都存在。借助完美验证器结合重复采样（生成多个候选响应后选最优）能大幅提升模型在数学、代码、推理等任务的能力，但完美验证器要么不可扩展（如人工验证），要么实用性有限（如Lean这类形式化证明工具）。而作为通用验证器的LM裁判和奖励模型，与“ oracle verifiers（完美准确的验证器）”仍存在显著性能差距，即“生成 - 验证差距”——模型能生成正确响应但无法被识别。同时，弱验证器（如LM裁判、奖励模型）存在分数噪声大、校准差、假阳性率高等问题，且整合弱验证器还面临 naive 聚合不足、有限标注数据下有效集成难、推理时部署验证成本高等挑战，因此本文旨在探索如何结合多个弱验证器来改进重复采样下的响应选择，缩小生成 - 验证差距。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Weaver框架聚合弱验证器
Weaver是一个无需在真实标签上进行有监督微调来聚合弱验证器的框架。首先发现，在有大量带标签训练数据时，学习加权验证器集合（利用验证器准确率差异）能比 naive 平均（假设验证器质量一致，易让低质量验证器主导致精度下降）最多高出11.2个百分点；当缺乏大量标注数据时，将弱监督（WS）适配到验证场景，解决输出不一致和低精度验证器问题，通过过滤无信息验证器、归一化验证器分数，并基于这些分数和未知真实标签构建 latent variable model 来估计验证器准确率作为集合权重。

💡 创新点2：解决弱验证器集成的多挑战
针对弱验证器集成的三大挑战逐一应对：对于 naive 聚合不足，用加权集合替代 naive 平均，利用验证器准确率差异提升性能；对于有限标注数据下有效集成难，适配弱监督技术，处理弱验证器输出格式不一致（如logits、二分类分数、Likert分数等）和低质量问题，借助数据集统计归一化输出和过滤特定验证器；对于推理时部署验证成本高，用Weaver的组合输出分数训练紧凑的400M跨编码器，在大幅降低计算成本同时保留高准确率。

### 📈 实验结果
在测试时重复采样场景（模型生成多个候选响应后选择其一）下评估，Weaver相比验证器分数无加权平均的重复采样性能提升17.1%，相比多数投票提升13.5%；对比LM的Pass@1（选第一个候选响应的性能），在推理和数学任务上，对8B模型性能提升17.9%，对70B模型提升14.5%；用Llama 3.3 70B Instruct作生成器、70B或更小的裁判和奖励模型作验证器集合时，能达到o3 - mini水平准确率（平均87.7%），该增益堪比GPT - 4o到o3 - mini的性能跃升（69.0% vs. 86.7%，后者需大量微调与后训练）；训练的400M跨编码器蒸馏模型保留了Weaver全精度的98.7%，同时将验证计算量降低达99.97%。

### 💬 可借鉴之处
1. 多弱验证器聚合思路：当面临多个有缺陷但互补的工具/模型时，可借鉴Weaver加权聚合并结合弱监督估计权重的方式，充分利用各工具优势，减少对大量标注数据依赖。
2. 性能 - 成本权衡：在追求模型性能提升同时关注推理成本，如Weaver通过蒸馏得到紧凑模型降低计算成本，这种在应用中平衡性能与资源消耗的思路值得借鉴。
3. 弱监督适配特定场景：针对自身任务场景中类似“弱验证器输出格式不一、质量参差”等问题，可参考Weaver利用数据集统计进行归一化、过滤等手段来适配弱监督技术，拓展弱监督应用边界。

## reflective-verbal-reward-design-for-pluralistic-alignment
### Abstract
AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.
### 🌟 论文解读 | 个性化价值对齐新范式：反思式语言奖励设计

### 📌 背景痛点/本文动机
在AI与人类价值对齐的领域，主流的基于人类反馈的强化学习（RLHF）方法存在局限：人类价值具有高度异质性，不同人价值观差异甚至冲突，但RLHF常将所有人反馈聚合为单一奖励模型，这会导致少数群体偏好被过度压制。此外，在复杂场景下人类偏好不是简单“提取”而是“构建”的，现有被动收集偏好、直接标注的方式难以让用户充分反思以形成明确偏好。因此，如何兼顾价值多样性与偏好构建过程，学习个性化奖励模型成为关键问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Interactive - Reflective Dialogue Alignment（IRDA）框架  
IRDA借助大语言模型（LLM）通过交互式对话学习个性化奖励函数，包含三部分：一是**反思式语言偏好引导**，引导用户清晰表达自身价值观；二是**主动学习**，策略性选择示例供人类批判，提升数据利用效率；三是**LLM驱动的语言奖励建模**，让LLM利用上下文学习能力，从稀疏用户反馈中泛化，直接作为奖励函数评估AI行为轨迹。用LLM引导的对话替代被动标注，激发用户深思熟虑的反思（System 2认知）来解决偏好构建难题。

💡 创新点2：融合多学科视角实现个性化对齐  
结合AI、人机交互（HCI）和社会科学知识，不再假设用户能直接清晰表达新场景下偏好，而是主动引导用户通过反思将潜在价值观转化为具体偏好，弥补了过往方法在偏好构建环节的不足，为个性化AI对齐提供新 pipeline。

### 📈 实验结果
开展两项涉及30名参与者的用户研究：第一项针对“尊重行为”的个人定义构建奖励模型（21人参与），第二项探索自动驾驶伦理决策（9人参与）。结果显示，参与者价值判断差异显著，IRDA相比基线方法（如非反思式语言奖励模型）能更准确捕捉个体对价值对齐行为的定义，在准确率上比非反思式语言奖励模型提升9 - 12%，且比传统监督学习方法样本效率更高。

### 💬 可借鉴之处
1. 多学科融合思路：将AI技术与HCI、社会科学中关于人类偏好构建、反思的研究结合，为AI对齐问题提供更贴合人类认知规律的解法，启示后续研究跨学科解决复杂AI伦理与对齐问题。  
2. 个性化奖励模型构建：证明了通过引导反思、交互式对话学习个性化奖励函数的可行性，为面向个人的AI助手（如个性化智能助理）提供技术参考，让AI奖励更贴合用户个体而非群体平均。  
3. 数据高效学习：主动学习策略在个性化场景下提升样本效率，为数据稀缺场景下的奖励模型学习提供借鉴，后续可探索在更多资源受限场景的应用。  
4. 偏好构建过程重视：强调人类偏好“构建”而非“提取”，提醒研究者在设计AI对齐系统时关注用户反思、价值外显的过程，未来可深入探索如何优化对话引导策略以更好助力用户偏好构建。

## duashepherd--integrating-stepwise-correctness-and-potential-rewards-for-mathematical-reasoning
### Abstract
In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.
### 🌟 论文解读 | DuaShepherd：双奖励信号驱动，提升大模型数学推理能力

### 📌 背景痛点/本文动机
大语言模型（LLMs）在诸多自然语言处理任务中取得了出色成果，但在数学推理这类需要多步骤复杂推理的领域仍存在不足。尽管思维链（CoT）提示等方法能增强推理能力，强化学习也被用于提升模型数学推理的内在能力，不过现有基于过程的奖励模型（PRMs）存在构建成本高（需大量人工标注步骤数据）或单一信号局限等问题。同时，不同来源的过程监督数据（如PRM800K和基于蒙特卡洛采样生成的数据）蕴含不同奖励信号含义，如何整合这些互补信号来打造更强大的过程奖励模型，成为待解决的关键挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：双奖励信号融合的框架设计  
提出DuaShepherd奖励建模框架，整合“正确性”和“潜在性”两种互补奖励信号。其中，基于正确性的信号聚焦于识别逐步推理中的错误；基于潜在性的信号则关注当前步骤导向正确最终答案的可能性。通过将二者结合为复合概率，充分发挥不同信号优势来增强大模型数学推理能力。  

💡 创新点2：自动化数据集构建 pipeline  
开发自动化流程来构建大规模奖励建模数据集，该数据集同时包含正确性和潜在性两种奖励信号标注。借助已有的PRM800K和Math - Shepherd等数据集，利用训练好的奖励模型自动标注采样的推理轨迹，高效生成同时具备两类奖励标签的DuaShepherd数据集，降低了人工标注成本且实现大规模数据构建。  

💡 创新点3：多任务统一架构训练  
采用统一的多头架构，在多任务设置下并行训练两种奖励模型。类似于ArmoRM的多任务训练模式，让单个基础模型共享学习两种过程奖励信号，借助并行学习两种信号带来的互补性，提升奖励模型整体性能。  


### 📈 实验结果
在MATH500和ProcessBench等基准测试中开展实证评估，结果表明：结合两种奖励信号的DuaShepherd模型，显著优于仅用单一奖励类型训练的模型；在可比资源约束下，实现了当前最优（SOTA）的性能表现，验证了双奖励信号融合以及多任务训练等方法在提升数学推理能力上的有效性。  

### 💬 可借鉴之处
1. 多维度奖励融合思路：在需复杂推理的任务场景中，可思考整合不同维度、具备互补性的奖励信号，突破单一信号的性能瓶颈，为模型训练提供更全面指导。  
2. 自动化数据构建：面对人工标注成本高的任务，参考其自动化生成带多维度标注数据集的思路，利用已有模型或方法来自动生成训练数据，提升数据构建效率与规模。  
3. 多任务共享架构：在处理存在关联的多任务或多信号学习场景时，采用共享基础模型的多任务训练模式，能借助不同任务/信号间的互补性，提升整体模型性能，该模式在其他需多维度优化的AI任务中也有借鉴价值。

## reward-agnostic-prompt-optimization-for-text-to-image-diffusion-models
### Abstract
We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.
### 🌟 论文解读 | 打破奖励模型限制：RATTPO让文本到图像扩散模型的提示优化更通用

### 📌 背景痛点/本文动机
在文本到图像（T2I）扩散模型领域，尽管模型能依据用户提示生成图像，但输出质量受提示影响大，细微提示变化就可能导致生成质量波动。为改进生成效果需进行提示工程，而不同奖励模型用于评估图像生成各方面（如审美、人类偏好、物体空间关系等）。现有自动提示工程方法多针对特定奖励模型设计，在面对新的、不同奖励模型的提示工程场景时表现欠佳，缺乏能在测试时适应多样奖励函数的通用提示优化技术，这就是本文要解决的核心问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RATTPO方法  
RATTPO（Reward - Agnostic Test - Time Prompt Optimization）是一种灵活的测试时优化方法，无需修改就能适用于各种奖励场景。它在优化用户提示时，不需要针对特定奖励的任务描述，而是通过查询大语言模型（LLMs）迭代搜索优化提示，利用优化轨迹和一种新颖的奖励感知反馈信号（称为“hint”）作为上下文来引导优化。

💡 创新点2：引入奖励感知反馈机制（“hint”）  
“hint”是一种简洁的文本策略，用于提高奖励，类似人工编写的任务描述，但在优化过程中由独立的LLM实时生成。这个机制为优化器提供奖励感知指导，同时避免了手动重写特定奖励任务描述的需求，弥补了因不采用特定奖励设计可能带来的性能下降问题。

💡 创新点3：训练与梯度无关的自动提示工程  
RATTPO是一种无需训练、无需梯度的自动提示工程方法，能轻松应用于不同奖励设置，无需针对特定奖励进行调整或训练，突破了以往一些方法依赖训练、转移能力有限的局限。

### 📈 实验结果
实验结果表明RATTPO具有很强的通用性，在评估生成美学、人类偏好、物体空间关系等不同生成方面的多样奖励设置下，都能有效增强用户提示。在搜索效率上，RATTPO超越了其他测试时搜索基线，最多能减少3.5倍的推理预算；在有足够推理预算时，其性能能与需要特定奖励微调的基于学习的基线相媲美。

### 💬 可借鉴之处
1. 通用优化思路：RATTPO提出的奖励无关的测试时提示优化思路，为处理需适应多种评估标准（奖励模型）的任务提供了参考，在其他需根据不同评价体系优化输入的场景（如文本生成等领域）可能有借鉴价值。
2. 反馈机制设计：其“hint”这种奖励感知反馈机制的设计，展示了如何利用大语言模型实时生成指导信号来辅助优化，为结合大语言模型进行无特定任务描述的优化任务提供了新的设计方向。
3. 高效搜索优势：在搜索效率上的提升证明了该方法在资源利用上的优势，对于计算资源有限或追求高效推理的应用场景，这种高效搜索的设计思路值得学习。

## reasongrm--enhancing-generative-reward-models-through-large-reasoning-models
### Abstract
Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.
```
### 🌟 论文解读 | ReasonGRM：借大模型推理能力革新生成式奖励模型

### 📌 背景痛点/本文动机
大语言模型（LLMs）在理解、生成与决策上取得长足进步，但要让模型输出贴合人类价值观，奖励模型（RM）是关键。传统标量奖励模型（SRMs）把复杂人类偏好压缩成单一标量，易信息丢失、泛化性弱；新兴生成式奖励模型（GRMs）虽更灵活，但推理能力不足，常出现推理路径不完整或过度推测，导致任务中“幻觉”或关键信息缺失。因此，如何提升GRMs的推理质量以实现可靠偏好建模，成了核心问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ReasonGRM三阶段框架  
ReasonGRM分三步打造更优生成式奖励模型：  
- 阶段一（生成推理路径）：用Zero - RL生成简洁、以结果为导向的推理路径，减少关键信息遗漏风险；  
- 阶段二（筛选优质路径）：引入全新评估指标\( R^\star \)，依据生成可能性为推理路径打分，偏好“用最少探索达正确答案”的路径，削减训练中易引发幻觉的数据；  
- 阶段三（强化模型能力）：针对高难度示例用强化学习进一步精调模型，增强其偏好区分能力。  

💡 创新点2：定义\( R^\star \)评估指标解决数据质量难题  
\( R^\star \)结合“有效性（Validity，推理导向正确结果）”与“自一致性（Self - Consistency，推理逻辑连贯无冗余）”两大关键属性，通过生成可能性来评估推理路径，能从噪声候选集中自动选优质推理路径，破解复杂任务奖励模型训练的数据质量瓶颈。  


### 📈 实验结果
在RM - Bench、RewardBench、RMB三大公开基准测试中，ReasonGRM表现亮眼：平均超越此前最优GRMs 1.8%，在部分场景下比GPT - 4o等闭源模型领先达5.6%，还比主流SRMs平均高4.5%。实验不仅验证了方法有效性，消融实验也剖析了推理质量、\( R^\star \)过滤效果、各训练阶段对最终奖励模型的影响。  


### 💬 可借鉴之处
1. 重视推理质量在奖励模型中的价值：揭示了高质量推理路径（兼顾有效性与自一致性）对偏好建模的关键作用，为后续奖励模型设计指明“推理感知”方向；  
2. 创新评估与过滤机制：\( R^\star \)展示了如何用生成可能性量化推理质量，为处理噪声数据、构建优质训练集提供了可复用思路；  
3. 多阶段训练Pipeline：从生成到筛选再到强化学习的流程，为通用LLM向专精奖励模型转化提供了工程化参考范式；  
4. 全面实验验证：跨多个权威基准的测试+消融实验，是学术研究中验证方法普适性与模块价值的典范，值得借鉴以增强研究说服力。  
```

## robust-reward-modeling-via-causal-rubrics
### Abstract
Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.
### 🌟 论文解读 | 基于因果准则打造鲁棒奖励模型：Crome框架破解奖励黑客难题

### 📌 背景痛点/本文动机
在大语言模型（LLM）对齐人类反馈的过程中，奖励模型（RM）是核心环节，但当前奖励模型普遍面临 “奖励黑客” 问题。传统RM常把训练数据里的统计相关性错当成质量的真正因果驱动因素，比如会依据回复长度、格式这类表面或虚假属性来打分，而非事实性、相关性等真实质量维度。这是因为标准训练目标难以区分因果因素和虚假关联，最终导致RM脆弱、策略对齐失效。所以，如何让RM在未知虚假属性的情况下，只依托能获取的真实因果质量属性来实现鲁棒训练，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无虚假感知的因果框架  
提出基于因果模型的奖励模型训练框架Crome，无需预先指定或干预任何虚假属性，仅通过调用 “ oracle LLM ” 识别出的因果质量准则进行干预，就能引导RM学习，从机制上绕开了对虚假属性先验知识的依赖。  

💡 创新点2：基于因果属性的靶向反事实增强  
设计两类合成训练样本增强方式：  
- **因果增强（Causal Augmentations）**：生成在特定因果属性（如事实性）上有差异的样本对，让RM对真实质量变化的维度产生敏感性，精准捕捉因果维度的影响；  
- **中性增强（Neutral Augmentations）**：利用因果增强后的数据与原始偏好对，生成在虚假特征（如风格）上变化但因果内容保留的样本，并搭配平局标签，强制RM对虚假属性保持不变性。且整个过程无需显式知晓虚假因素，仅通过因果准则干预就可缓解对大量虚假关联的敏感。  

### 📈 实验结果
在RewardBench等多个基准测试中，Crome表现远超标准基线：  
- 在RewardBench上平均准确率提升最高达5.4%，其中安全（Safety）类别提升13.18%、推理（Reasoning）类别提升7.19%；  
- 在Best - of - N推理场景下，面对RewardBench、安全专项的WildGuardTest、推理专项的GSM8k等基准，随着N增大，Crome的RM在选择最优结果时持续领先基线，证明其在应对稀有（长尾）虚假因素时也具备鲁棒性。  

### 💬 可借鉴之处
1. 因果视角的鲁棒训练思路：将因果建模引入奖励模型训练，为解决“虚假关联干扰模型学习”这类普遍问题提供了新范式，可启发后续在其他需区分因果与相关场景的模型训练工作；  
2. 无先验的增强策略：展示了如何在不依赖虚假属性先验知识的情况下，仅通过对因果属性的干预来间接减少虚假关联影响，这种“绕开未知、强化已知因果”的思路在数据增强、鲁棒训练方向有推广价值；  
3. 多场景鲁棒性验证：在聊天、安全、推理等不同任务基准及Best - of - N这类实际推理场景下验证有效性，证明方法具备跨任务、跨场景的普适性，为工业级大模型对齐流程的鲁棒性优化提供了可落地参考。

## relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples
### Abstract
Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.
### 🌟 论文解读 | RELIC：小样本示例助力低资源印度语言奖励模型泛化能力提升

### 📌 背景痛点/本文动机
奖励模型是大语言模型（LLMs）与人类偏好对齐的关键，但现有开源多语言奖励模型多基于高资源语言的偏好数据集训练，在低资源印度语言上奖励信号不可靠。收集低资源语言大规模高质量偏好数据成本极高，基于偏好的训练方法难以实施。同时研究发现开源多语言奖励模型在低资源印度语言中无法准确区分安全与不安全响应，如对低资源语言Bodo，奖励模型给不安全响应的分数反而高于安全响应，这对依赖这些低资源语言社区使用安全对齐的LLMs构成挑战，因此需要新方法提升低资源印度语言奖励模型性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RELIC框架  
RELIC是针对低资源印度语言奖励建模的新型上下文学习框架。它训练检索器时采用 pairwise ranking 目标，从辅助高资源语言中选择上下文示例，这些示例能最有效地突出偏好响应和次偏好响应之间的区别，为奖励模型提供有判别性的上下文。

💡 创新点2：检索器训练与适用性  
RELIC使用 pairwise ranking loss（基于Thurstone、Bradley - Terry 相关理论）训练检索器，优化其选择最能区分正响应和负响应的上下文示例。且不需要访问配对偏好数据集，可应用于任何包含二元质量标签的低资源数据集，适用性广，还结合高资源语言辅助示例库，推理时能提供更丰富有判别性的上下文。

### 📈 实验结果
在 PKU - SafeRLHF、WebGPT、HH - RLHF 三个偏好数据集上，用最先进的开源奖励模型进行大量实验。结果显示RELIC显著提高低资源印度语言奖励模型准确率，持续超越现有示例选择方法。如在低资源印度语言Bodo上，用LLaMA - 3.2 - 3B奖励模型时，相比零样本提示和现有最先进示例选择方法，准确率分别提升12.81%和10.13%；在Santali语言上，基于LLAMA - 3.1 - 8B的奖励模型，RELIC比零样本提示准确率提升24.16%，比现有基于相关性的示例选择方法提升21.26%。

### 💬 可借鉴之处
1. 分析角度可借鉴：对低资源语言上开源多语言奖励模型泛化能力分析，揭示响应质量区分能力的关键缺口，为后续研究指明问题方向。
2. 方法创新可借鉴：RELIC框架针对低资源场景下数据稀缺问题，利用上下文学习和检索器结合，且基于奖励模型排序目标训练检索器的思路，为其他低资源语言任务或需要利用高资源辅助数据的任务提供了方法参考。
3. 实验设计可借鉴：在多个典型偏好数据集和开源奖励模型上验证方法有效性，这种全面的实验验证方式能有力支撑方法价值，值得相关研究实验设计学习。

## gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks
### Abstract
Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.
### 🌟 论文解读 | GFlowGR：用生成流网络优化生成式推荐框架的微调

### 📌 背景痛点/本文动机
生成式推荐（GR）框架结合物品 tokenizer 和大语言模型（LLM），在诸多场景取得成功，但目前研究多聚焦于优化物品 tokenizer 或 LLM 解码策略，对 LLM 适配推荐数据的**关键微调环节**探索不足。现有微调方法（如监督微调 SFT、直接偏好优化 DPO）存在“暴露偏差”问题：SFT 只关注数据集中正样本，既没引入负样本对比，也难探索潜在正样本；DPO 等强化学习微调方法虽能利用负样本，但依赖已收集数据，无法充分反映真实用户偏好，忽略了未观测到的潜在正样本。因此，亟需一种能探索潜在正样本、缓解暴露偏差的 LLM 微调方案。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：将生成式推荐建模为多步生成任务，提出 GFlowGR 框架  
把 GR 视为多步骤生成过程，基于生成流网络（GFlowNets）构建微调框架 GFlowGR。GFlowNets 能让 LLM 按与奖励分布成比例的概率生成 token 序列，借此识别潜在正样本、扩展推荐多样性，从生成概率层面缓解暴露偏差。  

💡 创新点2：设计自适应轨迹采样器与多维度奖励模型  
- 自适应轨迹采样器：融合传统推荐系统的协同知识，生成“从易到难”的增强轨迹，用课程式训练思路持续提供高质量训练数据；  
- 多维度奖励模型：综合考量增强信号、协同分数、语义相似度等，有效区分采样到的未观测轨迹中“用户可能偏好”的物品；  
- 置信加权机制：依据增强信号与协同分数的一致性，为每个样本分配置信权重，进一步缓解暴露偏差。  


### 📈 实验结果
论文在**两个真实世界数据集**、基于**两种不同 GR 骨干模型**开展大量实验。结果验证了 GFlowGR 在缓解暴露偏差、提升推荐效果上的有效性与鲁棒性，证明该框架能在不同数据和模型底座下稳定发挥作用。

### 💬 可借鉴之处
1. 跨领域融合思路：将生成流网络（GFlowNets）与推荐系统结合，为解决推荐场景中“暴露偏差”这类特有问题提供了新范式，启发后续跨领域技术迁移；  
2. 课程式采样与多维度奖励：自适应轨迹采样的“从易到难”课程训练、多维度奖励模型的构建方式，为处理“数据稀疏+偏好难建模”场景提供了可复用的设计思路；  
3. 问题建模视角：把推荐任务拆解为多步生成任务，重新定义推荐系统与生成式模型的交互方式，为推荐系统的“生成式转型”提供了方法论参考。  

## autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning
### Abstract
Rule-based rewards offer a promising strategy for improving reinforcement
learning from human feedback (RLHF), but current approaches often rely on
manual rule engineering. We present AutoRule, a fully automated method for
extracting rules from preference feedback and formulating them into rule-based
rewards. AutoRule extraction operates in three stages: it leverages a reasoning
model to interpret user preferences, identifies candidate rules from the
reasoning chain of these interpretations, and synthesizes them into a unified
rule set. Leveraging the finalized rule set, we employ language-model verifiers
to compute the fraction of rules satisfied by each output, using this metric as
an auxiliary reward alongside the learned reward model during policy
optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\%
relative improvement in length-controlled win rate on AlpacaEval2.0, and a
6.1\% relative gain in second-turn performance on a held-out MT-Bench subset,
compared to a GRPO baseline trained with the same learned reward model but
without the rule-based auxiliary reward. Our analysis confirms that the
extracted rules exhibit good agreement with dataset preference. We find that
AutoRule demonstrates reduced reward hacking compared to a learned reward model
when run over two episodes. Finally, our case study suggests that the extracted
rules capture unique qualities valued in different datasets. The extracted
rules are provided in the appendix, and the code is open-sourced at
https://github.com/cxcscmu/AutoRule.
### 🌟 论文解读 | AutoRule：从推理思维链中提取规则奖励，革新偏好学习

### 📌 背景痛点/本文动机
强化学习从人类反馈（RLHF）是大语言模型（LLM）对齐人类价值观和遵循指令的关键技术，但现有基于学习的奖励模型存在“奖励黑客攻击”问题（模型为追求高奖励而投机取巧，未真正提升响应质量）。同时，规则驱动的奖励虽能有效缓解该问题，但传统规则依赖人工设计或大规模众包标注，成本高且难扩展。因此，如何自动从偏好数据中提取规则来构建奖励机制，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出AutoRule自动规则提取框架  
AutoRule通过大语言模型的推理能力，从偏好数据中自动提取对齐规则。流程分三步：首先用具备推理能力的LLM为偏好输出生成逐步推理依据；接着从推理过程中提取候选规则；最后聚合训练集候选规则并合成统一规则集，借助推理链的逻辑结构捕捉更精准的偏好准则。  

💡 创新点2：规则奖励与策略优化结合  
利用最终规则集，让语言模型充当“验证器”，计算每个输出满足规则的比例，将该指标作为辅助奖励，与传统学习到的奖励模型在策略优化阶段配合使用，引导模型训练。  

### 📈 实验结果
在实验验证中，用Llama 3 8B Instruct作验证器时，规则分数（单个或累计）在UltraFeedback和MT - Bench Human Judgment数据集上与偏好高度一致；基于UltraFeedback数据用GRPO结合AutoRule训练Llama - 3 - 8B模型，对比仅用学习奖励模型的GRPO基线，AlpacaEval2.0长度控制胜率相对提升28.6%，MT - Bench保留子集的第二轮表现相对提升6.1%；奖励黑客实验表明AutoRule能减轻奖励模型过度优化；消融实验证明从推理链提取规则比仅从依据提取更有效；定性分析显示不同数据集提取的规则各有侧重（如UltraFeedback侧重对话质量，MT - Bench侧重指令遵循与复杂任务鲁棒性）。  

### 💬 可借鉴之处
1. 自动规则提取思路：摆脱人工规则依赖，借助LLM推理能力从数据中自动挖掘规则，为领域适配规则构建提供高效方案。  
2. 多奖励融合优化：将规则奖励与学习奖励结合用于策略优化，为缓解奖励黑客、提升模型真实性能提供新范式。  
3. 可解释性与适配性：提取的规则可解释，且能捕捉不同数据集的独特价值，为理解模型偏好对齐逻辑和定制化优化提供参考。

## spare--single-pass-annotation-with-reference-guided-evaluation-for-automatic-process-supervision-and-reward-modelling
### Abstract
Process or step-wise supervision has played a crucial role in advancing
complex multi-step reasoning capabilities of Large Language Models (LLMs).
However, efficient, high-quality automated process annotation remains a
significant challenge. To address this, we introduce Single-Pass Annotation
with Reference-Guided Evaluation (SPARE), a novel structured framework that
enables single-pass, per-step annotation by aligning each solution step to one
or multiple steps in a reference solution, accompanied by explicit reasoning
for evaluation. We show that reference-guided step-level evaluation effectively
facilitates process supervision on four datasets spanning three domains:
mathematical reasoning, multi-hop compositional question answering, and spatial
reasoning. We demonstrate that SPARE, when compared to baselines, improves
reasoning performance when used for: (1) fine-tuning models in an offline RL
setup for inference-time greedy-decoding, and (2) training reward models for
ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE
achieves competitive performance on challenging mathematical datasets while
offering 2.6 times greater efficiency, requiring only 38% of the runtime,
compared to tree search-based automatic annotation. The codebase, along with a
trained SPARE-PRM model, is publicly released to facilitate further research
and reproducibility.
### 🌟 论文解读 | SPARE：单遍参考引导评估，革新自动过程监督与奖励建模

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂多步推理任务中仍面临挑战，过程监督（按步骤监督）对提升LLMs多步推理能力至关重要，但高效、高质量的自动过程标注一直是难题。现有方法存在不足：基于蒙特卡洛树搜索（MCTS）的自动标注方法计算低效且未充分利用参考解答的分步信息；其他利用参考推理轨迹的方法要么依赖特定领域判别模型、适用范围窄，要么依赖更强大模型生成辅助数据，限制较多。因此，需要一种更高效且能充分利用参考信息的自动过程标注框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出SPARE框架  
Single - Pass Annotation with Reference - Guided Evaluation（SPARE）是一种新颖的结构化框架，通过将候选解答的每个步骤与参考解答中的一个或多个步骤对齐，并辅以显式推理评估，实现单遍、逐步骤的标注。该框架无需额外推理轨迹，复用了标准有监督微调（SFT）已有的参考解答，聚焦于（i）每个步骤评估的显式推理；（ii）候选输出与参考之间的多步骤对齐比较，支持与响应和参考的token长度成 additive scaling 的单遍评估。

💡 创新点2：双场景应用提升推理性能  
利用SPARE标注在两个场景提升LLMs推理能力：（i）在离线强化学习（RL）设置中微调模型，用于推理时的贪心解码；（ii）训练奖励模型（RMs），对多个LLM生成的输出进行排序和聚合。

### 📈 实验结果
1. 跨领域有效性：在涵盖数学推理、多跳组合问答、空间推理三个领域的四个数据集（如数学推理的GSM8K、MATH，问答的MuSiQue - Ans，空间推理的SpaRP）上，参考引导的步骤级评估有效促进了过程监督。
2. 性能与效率优势：与基线相比，SPARE用于离线RL微调模型和训练奖励模型时均提升了推理性能；在具挑战性的数学数据集上，SPARE性能有竞争力，且比基于树搜索的自动标注效率高2.6倍，仅需其38%的运行时间。

### 💬 可借鉴之处
1. 框架设计思路：SPARE复用已有参考解答进行过程标注的思路，为充分利用现有数据、减少额外数据收集成本提供了借鉴，在需过程监督的任务中，可思考如何复用已有标注资源。
2. 多场景应用模式：将过程标注用于离线RL微调与奖励模型训练的双场景实践，为提升LLMs推理能力提供了新的技术路线参考，后续可探索在更多任务场景下类似的应用拓展。
3. 效率与性能平衡：在追求性能的同时关注效率提升，SPARE在数学数据集上展现的高效标注优势，提示在设计自动标注或评估方法时，需兼顾性能与计算资源消耗，探索更轻量化高效的技术方案。 
4. 开源资源价值：公开代码库和训练好的SPARE - PRM模型，便于后续研究复现与拓展，这种开放共享的科研实践值得学习，利于推动领域内技术快速迭代。

## reward-models-in-deep-reinforcement-learning--a-survey
### Abstract
In reinforcement learning (RL), agents continually interact with the
environment and use the feedback to refine their behavior. To guide policy
optimization, reward models are introduced as proxies of the desired
objectives, such that when the agent maximizes the accumulated reward, it also
fulfills the task designer's intentions. Recently, significant attention from
both academic and industrial researchers has focused on developing reward
models that not only align closely with the true objectives but also facilitate
policy optimization. In this survey, we provide a comprehensive review of
reward modeling techniques within the deep RL literature. We begin by outlining
the background and preliminaries in reward modeling. Next, we present an
overview of recent reward modeling approaches, categorizing them based on the
source, the mechanism, and the learning paradigm. Building on this
understanding, we discuss various applications of these reward modeling
techniques and review methods for evaluating reward models. Finally, we
conclude by highlighting promising research directions in reward modeling.
Altogether, this survey includes both established and emerging methods, filling
the vacancy of a systematic review of reward models in current literature.
### 🌟 论文解读 | 深度强化学习中奖励模型全景式综述：从基础到前沿

### 📌 背景痛点/本文动机
强化学习（RL）中，奖励是引导智能体优化行为的核心，但现实场景里奖励常缺失或难定义。现有综述多聚焦逆强化学习、基于人类反馈的强化学习等子领域，缺乏对奖励模型本身的系统性梳理。同时，以大语言模型（LLM）、视觉 - 语言模型（VLM）为代表的基础模型在奖励建模中兴起却未被充分综述。因此，本文旨在填补空白，全面回顾深度强化学习中奖励建模技术，涵盖基础、方法、应用与未来方向。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：系统性分类框架构建
从**来源**、**机制**、**学习范式**三维度构建奖励模型分类框架。来源上区分人类提供型（含手动奖励工程、人在环奖励学习，后者又细分为从演示、目标、偏好中学习）与AI生成型（依托LLM、VLM等基础模型）；机制上分为内在奖励（驱动智能体自主探索）与外在奖励（环境直接反馈）；学习范式围绕不同反馈类型展开，清晰梳理奖励模型的多样形态与构建逻辑。
💡 创新点2：全面覆盖新兴方向
聚焦基础模型赋能的奖励建模，如LLM解读人类意图自主定义奖励、VLM处理多模态场景下奖励生成等，弥补过往综述对这类新兴方法关注不足的问题，为领域注入新视角。
💡 创新点3：完整生态梳理
不仅剖析奖励模型技术本身，还延伸至应用场景（如复杂游戏、大模型对齐人类意图、大规模智能体训练）与评估方法，搭建从技术到落地再到效果验证的完整知识链条，助力读者理解奖励模型在RL生态中角色。

### 📈 实验结果
文中未侧重传统实验指标对比（因属综述），但通过对大量前沿论文（如基于LLM的奖励设计、人在环奖励学习案例）的归纳，展现各分类下方法在不同场景（如AlphaGo的决策奖励、InstructGPT的对齐奖励）的有效性：手动奖励工程在 Gym - MuJoCo  walker 任务中通过组合生存、移动等奖励引导智能体；人在环学习从人类演示/偏好中学习的方式降低人工设计难度且提升对齐度；基础模型生成奖励在大模型智能体训练（如OpenAI - o1）中展现强大推理引导能力，验证不同奖励建模路径在各自适用场景的价值。

### 💬 可借鉴之处
1. **分类思维**：三维度分类框架为领域内研究提供清晰“坐标”，后续研究可快速定位自身方法所属维度，也便于梳理领域发展脉络，类似思路可迁移至其他AI子领域技术综述与分类。
2. **新兴技术融合视角**：关注基础模型与RL奖励建模的结合，启示研究者重视跨模态、大模型等前沿技术对传统RL模块的革新，推动学科交叉。
3. **生态化梳理**：从技术到应用再到评估的全流程覆盖，提醒从业者做研究或落地时需兼顾上下游环节，如设计奖励模型要考虑应用场景特性、评估手段合理性，为项目全周期规划提供参考。 
4. **填补领域空白价值**：作为首篇系统聚焦深度RL奖励模型的综述，为学界提供了该方向知识底座，后续研究可基于此展开更细分、深入的探索，如特定来源奖励模型的优化、跨机制奖励融合等。

## adaptive-accompaniment-with-realchords
### Abstract
Jamming requires coordination, anticipation, and collaborative creativity
between musicians. Current generative models of music produce expressive output
but are not able to generate in an \emph{online} manner, meaning simultaneously
with other musicians (human or otherwise). We propose ReaLchords, an online
generative model for improvising chord accompaniment to user melody. We start
with an online model pretrained by maximum likelihood, and use reinforcement
learning to finetune the model for online use. The finetuning objective
leverages both a novel reward model that provides feedback on both harmonic and
temporal coherency between melody and chord, and a divergence term that
implements a novel type of distillation from a teacher model that can see the
future melody. Through quantitative experiments and listening tests, we
demonstrate that the resulting model adapts well to unfamiliar input and
produce fitting accompaniment. ReaLchords opens the door to live jamming, as
well as simultaneous co-creation in other modalities.
### 🌟 论文解读 | ReaLchords：开启实时音乐即兴伴奏新篇章

### 📌 背景痛点/本文动机
音乐即兴演奏（Jamming）需要乐手间的协调、预判与协作创造力，但当前音乐生成模型虽能产出富有表现力的内容，却无法“在线”（online）生成，即无法与其他乐手（人类或其他）同步创作。在音乐伴奏场景中，现有模型多为离线生成（如先拿到完整旋律再生成伴奏），不适应实时互动、动态调整的需求；且基于最大似然估计（MLE）预训练的模型存在“暴露偏差”，在面对实时场景中常见的错误、风格变化等情况时表现不佳，难以适配陌生输入并生成契合的伴奏。因此，打造一款能在线适配用户旋律、即兴生成和弦伴奏的模型成为关键需求，ReaLchords 应运而生。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 ReaLchords 在线伴奏生成模型  
ReaLchords 聚焦于为用户旋律实时即兴生成和弦伴奏，它先基于最大似然估计完成预训练，再通过强化学习（RL）对模型进行在线场景的微调。这种方式让模型能在不知道后续旋律的情况下，动态响应正在生成的音乐叙事，满足实时互动中“条件独立假设”的约束，为现场即兴演奏等场景提供技术支撑。

💡 创新点2：结合强化学习与自监督奖励模型优化  
微调过程中引入新颖的自监督奖励模型，从和声与时间连贯性等多个角度，为旋律和和弦之间的音乐协调性提供反馈。该奖励模型无需人类标注，通过自监督训练却能在人类听觉测试中展现出与人类偏好的高度对齐，有效引导模型生成更具音乐性的伴奏。

💡 创新点3：基于“未来可见”教师模型的知识蒸馏  
设计了一种从“能看到未来旋律”的教师模型向“看不到未来”的学生模型（即在线生成模型）蒸馏知识的新颖方式。教师模型可利用完整旋律信息，学生模型则模拟实时场景只能依赖历史信息，通过这种蒸馏让学生模型学会“预判”，大幅提升生成质量，人类评估结果也验证了蒸馏对模型效果的增益。

### 📈 实验结果
- 抗扰动与适配性：对比仅用 MLE 训练的在线模型，ReaLchords 在面对测试集中旋律中途的突然转调等扰动时，能更快适应。如示例中 MLE 模型预测出不合适和弦后无法调整，而 ReaLchords 虽初期也会预测不佳但能快速修正，且客观和声质量指标也佐证了这一优势。  
- 人类评估与偏好对齐：奖励模型在无人类反馈训练下，经人类听觉测试证明与人类偏好高度契合；同时结合蒸馏与 RL 微调后的模型，在生成伴奏的节奏、和声质量等维度表现更优，各组件对最终效果均有提升贡献。  
- 定量指标验证：通过领域特定指标分析（如和声、节奏相关度量），证实 RL 微调中各组件（奖励模型、蒸馏等）对生成伴奏在节奏和和声质量上的改进作用，无 RL 微调的模型在应对错误和扰动时表现拉胯。

### 💬 可借鉴之处
- 实时互动场景建模：为音乐以外的实时协同创作（如视觉艺术同步共创、互动叙事生成等）提供了“在线生成 + 强化学习适配”的思路参考，展示了如何让生成模型适应动态、不可见后续输入的场景。  
- 自监督奖励机制：证明无需人类标注的自监督方式也能训练出与人类偏好对齐的奖励模型，在其他创意领域（如绘画风格一致性评估、文学创作连贯性打分等）可借鉴该思路构建自监督反馈信号。  
- 知识蒸馏新范式：“从能看未来的教师到只能看历史的学生”的蒸馏模式，为需要“预判”能力的任务（如实时对话回复生成、实时交通调度策略学习等）提供了蒸馏方向与目标设定的创新范式，拓展了知识蒸馏的应用边界。

## senior--efficient-query-selection-and-preference-guided-exploration-in-preference-based-reinforcement-learning
### Abstract
Preference-based Reinforcement Learning (PbRL) methods provide a solution to
avoid reward engineering by learning reward models based on human preferences.
However, poor feedback- and sample- efficiency still remain the problems that
hinder the application of PbRL. In this paper, we present a novel efficient
query selection and preference-guided exploration method, called SENIOR, which
could select the meaningful and easy-to-comparison behavior segment pairs to
improve human feedback-efficiency and accelerate policy learning with the
designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We
designed a Motion-Distinction-based Selection scheme (MDS). It selects segment
pairs with apparent motion and different directions through kernel density
estimation of states, which is more task-related and easy for human preference
labeling; (2) We proposed a novel preference-guided exploration method (PGE).
It encourages the exploration towards the states with high preference and low
visits and continuously guides the agent achieving the valuable samples. The
synergy between the two mechanisms could significantly accelerate the progress
of reward and policy learning. Our experiments show that SENIOR outperforms
other five existing methods in both human feedback-efficiency and policy
convergence speed on six complex robot manipulation tasks from simulation and
four real-worlds.
### 🌟 论文解读 | SENIOR：基于偏好强化学习的高效查询选择与偏好引导探索

### 📌 背景痛点/本文动机
强化学习（RL）在众多决策任务中取得成功，但奖励函数设计随任务复杂度提升变得困难。基于偏好的强化学习（PbRL）通过学习人类偏好来避免奖励工程，然而反馈效率与样本效率低的问题阻碍了其应用。现有方法在选择易标注的有意义片段及让探索与偏好关联方面存在不足，因此本文提出SENIOR方法来提升PbRL的反馈与探索效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于运动区分的选择方案（MDS）
通过对状态进行核密度估计，选择具有明显运动且方向不同的行为片段对。这类片段更与任务相关，也便于人类进行偏好标注，能为奖励学习获取高质量标签，解决了以往查询选择中无意义数据对多、标注难的问题。

💡 创新点2：偏好引导的探索方法（PGE）
以内在奖励的形式，鼓励智能体探索高偏好且低访问的状态，持续引导智能体获取有价值样本。让探索和人类偏好关联起来，弥补了以往RL探索中较少关注偏好相关性的缺陷，加速策略学习。并且MDS与PGE协同作用，MDS为PGE提供准确奖励指导，PGE为MDS提供更有价值的查询样本，共同推动奖励和策略学习进程。

### 📈 实验结果
在6个模拟的复杂机器人操作任务和4个真实世界任务中，SENIOR在人类反馈效率和策略收敛速度上均优于其他五种现有方法，验证了其在提升PbRL性能方面的有效性。

### 💬 可借鉴之处
1. 在查询选择层面，关注任务相关的运动等信息来挑选易标注片段，为类似依赖人类反馈的学习场景提供了从任务特征角度优化数据选择的思路。
2. 在探索机制层面，将人类偏好融入探索设计，为强化学习中探索与任务目标、人类意图结合提供了创新方向，可启发后续在提升探索针对性上的研究。
3. 多任务（模拟+真实世界）的实验验证模式，为方法泛化性验证提供了参考范式，能让后续研究更注重方法在不同场景下的有效性检验。 

## tgdpo--harnessing-token-level-reward-guidance-for-enhancing-direct-preference-optimization
### Abstract
Recent advancements in reinforcement learning from human feedback have shown
that utilizing fine-grained token-level reward models can substantially enhance
the performance of Proximal Policy Optimization (PPO) in aligning large
language models. However, it is challenging to leverage such token-level reward
as guidance for Direct Preference Optimization (DPO), since DPO is formulated
as a sequence-level bandit problem. To address this challenge, this work
decomposes the sequence-level PPO into a sequence of token-level proximal
policy optimization problems and then frames the problem of token-level PPO
with token-level reward guidance, from which closed-form optimal token-level
policy and the corresponding token-level reward can be derived. Using the
obtained reward and Bradley-Terry model, this work establishes a framework of
computable loss functions with token-level reward guidance for DPO, and
proposes a practical reward guidance based on the induced DPO reward. This
formulation enables different tokens to exhibit varying degrees of deviation
from reference policy based on their respective rewards. Experiment results
demonstrate that our method achieves substantial performance improvements over
DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on
AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at
https://github.com/dvlab-research/TGDPO.
### 🌟 论文解读 | TGDPO：用Token级奖励指导增强直接偏好优化，提升大模型对齐能力

### 📌 背景痛点/本文动机
在大语言模型（LLM）的人类反馈强化学习（RLHF）领域，近端策略优化（PPO）算法常被用于让模型与人类偏好对齐，但传统序列级奖励存在稀疏性（如延迟反馈），导致训练不稳定和样本利用低效。而直接偏好优化（DPO）虽简化了流程、无需单独训练奖励模型，但它是序列级的“老虎机问题”形式，难以直接利用细粒度的Token级奖励来指导优化。因此，如何把对PPO有效的Token级奖励优势引入DPO，成为待解决的关键挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：序列级PPO的Token级分解与最优策略推导  
论文将序列级的PPO分解为一系列Token级的近端策略优化子问题，通过上界方法重新构建问题后，推导出了**闭式的最优Token级策略**，并得到与之对应的Token级奖励表示，为后续把Token级奖励融入DPO打下基础。  

💡 创新点2：面向DPO的Token级奖励指导框架（TGDPO）  
基于推导得到的Token级奖励，结合Bradley - Terry模型以及“消除配分函数”的新理论结果，提出了**TGDPO框架**——为DPO赋予Token级奖励指导能力的偏好优化算法。同时，还设计了基于诱导DPO奖励的**实用Token级奖励指导方案**，让不同Token能依据自身奖励，呈现出与参考策略不同程度的偏离，更精细地调控生成过程。  


### 📈 实验结果
论文在AlpacaEval 2、MT - Bench、Arena - Hard三大指令跟随基准测试中验证效果：与最优基线方法相比，TGDPO在MT - Bench上胜率提升高达7.5个百分点，AlpacaEval 2上提升6.2个百分点，Arena - Hard上提升4.3个百分点。此外，实验还展现出TGDPO在损失收敛时能得到更优策略（这在传统偏好优化方法中不常见）、可控制收敛速度且对Token级奖励变化鲁棒等优势。  


### 💬 可借鉴之处
1. 分解与重构思路：把序列级问题拆分为Token级子问题并推导闭式解，这种“化整为零 + 理论推导”的思路，为处理大粒度强化学习问题提供了Token级精细调控的新视角。  
2. 跨算法融合创新：成功将对PPO有效的Token级奖励能力引入DPO，证明了不同RLHF算法间“优势特性迁移”的可行性，启发后续算法融合创新。  
3. 实用奖励设计：提出的基于诱导奖励的Token级指导方案，为实际落地时如何利用细粒度反馈优化大模型，提供了可参考的工程化思路。  

## adaptive-data-augmentation-for-thompson-sampling
### Abstract
In linear contextual bandits, the objective is to select actions that
maximize cumulative rewards, modeled as a linear function with unknown
parameters. Although Thompson Sampling performs well empirically, it does not
achieve optimal regret bounds. This paper proposes a nearly minimax optimal
Thompson Sampling for linear contextual bandits by developing a novel estimator
with the adaptive augmentation and coupling of the hypothetical samples that
are designed for efficient parameter learning. The proposed estimator
accurately predicts rewards for all arms without relying on assumptions for the
context distribution. Empirical results show robust performance and significant
improvement over existing methods.
### 🌟 论文解读 | 为汤普森采样赋能：自适应数据增强破解线性情境 bandit 遗憾界难题

### 📌 背景痛点/本文动机
线性情境 bandit（LinCB）是序列决策领域的经典框架，目标是通过选择动作最大化累积奖励（奖励由含未知参数的线性函数建模）。汤普森采样（LinTS）在实际中表现不错，但理论上遗憾界未达最优，与极小极大下界存在差距。现有 LinTS 方法要么遗憾界较高（如 $\tilde{O}(d^{3/2}\sqrt{T})$），要么依赖强假设（如独立同分布上下文、特定先验等）限制了适用范围。同时，利用缺失数据技术估计未选臂奖励的方法会因逆概率加权引入随臂数增长的方差，需突破这些局限来实现更优理论保证与普适性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建假设 bandit 问题与自适应数据增强  
为高效学习参数，设计了适配的假设 bandit 问题。该假设场景采用正交基向量集，在保留原始上下文协方差结构的同时大幅减少“有效臂数”，实现对所有臂（包括未选臂）奖励的学习，不再依赖 IID 或多样性等强假设，通过这种自适应的数据增强思路来优化参数估计。  

💡 创新点2：耦合假设与原始问题的新型估计器  
将假设 bandit 与原始问题耦合，基于包含所有臂（含未选）上下文的 Gram 矩阵，得到了一种自归一化的新估计器。该估计器能精准预测所有臂的奖励，为汤普森采样提供更可靠的参数后验（或估计分布）基础，进而推动遗憾界的优化。  

💡 创新点3：近极小极大最优的汤普森采样算法  
依托上述新型估计器，提出的 LinTS 算法在无上下文分布强假设下，实现了对数因子内的极小极大最优遗憾界 $\tilde{O}(d\sqrt{T})$，填补了过往 LinTS 理论遗憾界与极小极大下界间的关键缺口，在理论保证上更优且适用场景更普适。

### 📈 实验结果
文中虽未详细展开实验数据呈现，但摘要与引言提及“实证结果显示鲁棒性能，相较现有方法有显著提升”，说明在各类基准场景下，新算法在累积奖励、遗憾控制等指标上表现更优，验证了方法的有效性与实用性。

### 💬 可借鉴之处
1. **理论与实践结合的思路**：针对经典算法（汤普森采样）的理论短板，通过统计方法（如缺失数据视角、假设问题构造）与算法设计（耦合、自适应增强）结合，实现理论保证突破，为改进既有成熟算法提供了范本。  
2. **普适性设计理念**：摆脱对上下文分布的强假设（如 IID、特定先验），让算法能在更复杂真实场景落地，这种“去假设化”的设计思路对面向实际应用的算法研发很有启发。  
3. **跨领域技术融合**：借鉴高维参数估计、最优实验设计、缺失数据处理等领域技术适配 bandit 场景，展示了跨学科方法整合在解决复杂问题时的潜力，为后续研究打开思路。

## gram--a-generative-foundation-reward-model-for-reward-generalization
### Abstract
In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
### 🌟 论文解读 | GRAM：面向奖励泛化的生成式基础奖励模型

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的对齐工作中，奖励模型扮演着关键角色，但传统奖励模型多以判别式模型形式训练，且高度依赖带标签的人类偏好数据。这带来了两方面问题：一是强化学习算法复杂度与标注数据获取难度导致奖励模型应用成本高昂；二是现有训练方式对大量无标签数据利用不足，难以得到能灵活适配多任务的通用奖励模型。因此，本文旨在探索利用无标签和有标签数据训练奖励模型的方法，打造可泛化的基础奖励模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：两阶段训练的生成式奖励模型架构  
基于大语言模型的生成式能力，构建生成式奖励模型（GRAM），分两阶段训练。第一阶段在大规模无标签的输入 - 响应数据上进行无监督预训练，学习输入与响应间的对应关系，无需偏好标注数据，可规模化获取响应比较的通用知识；第二阶段利用人类偏好数据进行有监督微调，让模型学会预测两个响应间的偏好关系。最终得到的基础奖励模型能直接用于下游任务或仅用少量任务特定数据微调。

💡 创新点2：标签平滑下的损失函数统一视角  
引入标签平滑技术到奖励模型训练中，证明此时训练目标可转化为正则化的 pairwise ranking loss（Bradley - Terry 损失）形式。这一成果在一定程度上统一了生成式模型与判别式模型的训练目标视角，为奖励模型训练提供了新认知，且标签平滑对训练生成式奖励模型十分有益，提升了模型泛化性。

### 📈 实验结果
在响应排序、基于人类反馈的强化学习（RLHF）、任务适配等多任务场景下开展大量实验。结果显示，GRAM 在几乎无需或仅需少量微调时，在各任务上泛化性出色。例如，基于 LLaMA - 3.1 - 8B - Instruct 模型训练奖励模型时，在 RewardBench 平均准确率上，相比普通判别式和生成式奖励模型分别提升 11.0 和 5.1 个百分点，显著超越多个强基线模型。

### 💬 可借鉴之处
1. 数据利用思路：打破传统奖励模型对有标签数据的强依赖，示范了无标签数据在预训练阶段为模型注入通用知识的价值，为后续奖励模型甚至其他模型训练在数据利用上开辟了“无标签 + 有标签”结合的思路。
2. 模型架构创新：两阶段的生成式奖励模型架构为打造通用基础奖励模型提供了可行范式，展示了先无监督预训练再监督微调在奖励模型领域的有效性，可启发后续多模态、其他任务导向模型的架构设计。
3. 损失函数与正则化：标签平滑结合后对损失函数的分析与转化，为理解生成式和判别式模型在奖励建模中的联系提供新角度，也提醒开发者在模型训练中重视正则化技术对模型泛化等能力的提升作用。

## vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training
### Abstract
Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large
language models but remains underexplored for Vision-Language (VL) models. The
Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing
structured feedback, yet training effective VL-RMs faces two major challenges.
First, the bootstrapping dilemma arises as high-quality training data depends
on already strong VL models, creating a cycle where self-generated supervision
reinforces existing biases. Second, modality bias and negative example
amplification occur when VL models hallucinate incorrect visual attributes,
leading to flawed preference data that further misguides training. To address
these issues, we propose an iterative training framework leveraging vision
experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection
Sampling. Our approach refines preference datasets, enhances structured
critiques, and iteratively improves reasoning. Experiments across VL-RM
benchmarks demonstrate superior performance in hallucination detection and
multimodal reasoning, advancing VL model alignment with reinforcement learning.
### 🌟 论文解读 | VL-GenRM：借视觉专家与迭代训练，突破多模态奖励模型训练困境

### 📌 背景痛点/本文动机
在大语言模型领域，基于可验证奖励的强化微调（RFT）已取得进展，但在视觉-语言（VL）模型中仍待深入探索。视觉-语言奖励模型（VL-RM）是对齐VL模型的关键，能提供结构化反馈，然而训练高效VL-RM面临两大核心挑战：一是“自举困境”，高质量训练数据依赖强VL模型生成，易强化模型固有偏差；二是“模态偏差与负例放大”，VL模型对视觉属性的错误幻想会产生有缺陷的偏好数据，误导训练。为解决这些问题，论文提出创新训练框架VL-GenRM。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：借视觉专家自动构建偏好数据集  
利用视觉专家（如擅长目标检测、深度估计等的模型）生成大规模偏好数据集，提升VL-GenRM训练时的监督质量，打破“自举困境”中依赖自生成数据强化偏差的问题，为模型提供更可靠的训练依据。  

💡 创新点2：CoT增强VL-GenRM训练  
引入思维链（CoT）推理生成技术，为VL-GenRM训练提供系统性指导。通过结构化推理过程，增加数据中有效正确描述占比，缓解自生成数据的局限性，强化奖励建模的连贯性，让模型学习更合理的评估逻辑。  

💡 创新点3：基于边际拒绝采样的迭代自举  
通过对“正例与负例奖励信号边际”筛选出的优质推理依据，进行迭代微调，持续优化VL-GenRM的推理能力。让模型在多轮训练中逐步向更优输出适配，不断提升对视觉-语言场景的评估与推理水准。  


### 📈 实验结果
论文在VL-RM基准测试与Best-of-N采样等实验中验证方法有效性，在幻觉检测（识别VL模型错误幻想视觉属性等问题）与多模态推理任务上展现出更优性能，推动了VL模型借助强化学习实现更好对齐。  

### 💬 可借鉴之处
1. 跨模态领域数据增强思路：引入领域专家（如视觉专家）辅助构建训练数据，为突破“自举循环”提供了新范式，可推广到其他需多模态协作、依赖数据质量的任务。  
2. 结构化推理融入训练：借助CoT将模糊的评估转化为可解释的推理步骤，为提升模型可解释性与训练有效性提供了参考，在复杂任务型模型训练中具借鉴价值。  
3. 迭代式训练策略：基于奖励边际的采样与迭代微调，让模型能力逐步迭代提升，这种“小步快跑、数据择优”的训练逻辑，在强化学习与多轮优化场景中值得复用。  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### 🌟 论文解读 | 告别繁琐标注：GAN - RM 让奖励建模“以假乱真”

### 📌 背景痛点/本文动机
在视觉生成模型的训练后增强中，奖励模型至关重要。然而当前奖励建模方法存在诸多难题：一是构建奖励模型需大量人工标注偏好数据，收集成本高昂，且基于特定生成模型输出域标注的数据，在应用到不同输出域模型时存在域差距；二是为全面评估生成内容质量，需人工设计多种评估指标，既增加工程成本，又难在不同维度间取得最优平衡，还难保证与人类普遍偏好契合。因此，本文受生成对抗网络（GAN）中对抗训练启发，提出 GAN - RM 框架，旨在摆脱手动偏好标注和显式质量维度设计，高效构建奖励模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无需手动偏好标注，利用少量代理数据
GAN - RM 仅需少量（几百个）无标注的代表性样本（即偏好代理数据，Preference Proxy Data）作为外部数据。通过训练奖励模型区分偏好代理数据和生成模型输出，让模型学习评估生成样本。同时采用基于排名的自举策略，将 GAN - RM 在这些样本上的置信分数作为软标签，利用额外数据再训练 GAN - RM，使其更好捕捉潜在人类偏好。
💡 创新点2：支持多轮训练，迭代对齐偏好
GAN - RM 支持多轮训练后优化。每一轮中，将被识别为接近偏好代理数据的样本用于生成器的训练后优化，反过来再训练判别器以区分这些更难的样本。这种迭代的“以假乱真”过程能逐步让生成质量与偏好代理数据中的潜在人类偏好对齐。

### 📈 实验结果
实验表明，基于 GAN - RM 的方法在性能上可与依赖大量标注数据（如 Pickapic 的 100 万标注人类偏好数据）的方法（如相关对比方法）相当甚至超越。在图像质量实验设置中，GAN - RM 仅需 500 个偏好代理数据样本。除图像质量提升实验外，在图像安全和视频质量增强场景下的实验也凸显了 GAN - RM 框架在不同场景下的泛化能力，验证了其在测试时缩放（如 Best - of - N 样本过滤）、监督微调（SFT）和直接偏好优化（DPO）等训练后方法中的有效性。

### 💬 可借鉴之处
从方法创新角度，GAN - RM 为解决奖励建模中数据获取难、依赖特定域、人工设计维度难契合人类偏好等问题提供了新思路，其利用对抗训练和少量代理数据的方式，减少了对大规模人工标注的依赖，降低工程成本；从应用拓展角度，该框架在图像、视频等多场景的有效实验，为视觉生成模型在不同领域的训练后增强提供了可复用的奖励建模范式，后续在视觉生成相关任务中，若需构建奖励模型，可借鉴其利用少量代理数据和对抗训练的思路来降低成本与难度。

## pb$^2$--preference-space-exploration-via-population-based-methods-in-preference-based-reinforcement-learning
### Abstract
Preference-based reinforcement learning (PbRL) has emerged as a promising
approach for learning behaviors from human feedback without predefined reward
functions. However, current PbRL methods face a critical challenge in
effectively exploring the preference space, often converging prematurely to
suboptimal policies that satisfy only a narrow subset of human preferences. In
this work, we identify and address this preference exploration problem through
population-based methods. We demonstrate that maintaining a diverse population
of agents enables more comprehensive exploration of the preference landscape
compared to single-agent approaches. Crucially, this diversity improves reward
model learning by generating preference queries with clearly distinguishable
behaviors, a key factor in real-world scenarios where humans must easily
differentiate between options to provide meaningful feedback. Our experiments
reveal that current methods may fail by getting stuck in local optima,
requiring excessive feedback, or degrading significantly when human evaluators
make errors on similar trajectories, a realistic scenario often overlooked by
methods relying on perfect oracle teachers. Our population-based approach
demonstrates robust performance when teachers mislabel similar trajectory
segments and shows significantly enhanced preference exploration
capabilities,particularly in environments with complex reward landscapes.
### 🌟 论文解读 | PB²：基于群体方法破解偏好强化学习中的偏好空间探索难题

### 📌 背景痛点/本文动机
强化学习（RL）在众多领域取得成功，但传统RL依赖精心设计的奖励函数，复杂任务中因涉及主观结果或人类偏好，奖励函数难以指定。基于偏好的强化学习（PbRL）通过人类对轨迹对的偏好反馈学习行为，无需手工奖励函数，但现有PbRL方法存在关键挑战：有效探索偏好空间不足，常过早收敛到仅满足狭窄人类偏好子集的次优策略。单智能体方法易生成相似轨迹对用于偏好 elicitation，限制反馈信息多样性，奖励模型学习样本单一；且人类评估相似轨迹时反馈不一致会严重降低学习性能，依赖“完美先知教师”的方法常忽略这一现实场景。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：识别偏好探索问题  
明确PbRL中偏好探索难题，论证单智能体方法易在偏好空间收敛到次优局部极小值，导致探索不足与策略不佳。  

💡 创新点2：提出群体化PbRL框架（PB²）  
采用群体方法同时训练多个不同策略，相比单策略方法更全面探索偏好 landscape。通过不同策略收集经验构建比较对，提升评估行为的多样性，且与人类偏好对齐。借助反馈循环实现：多样策略生成独特轨迹→人类对轨迹的偏好训练奖励模型→判别器维持群体多样性并鼓励符合当前偏好的行为，让偏好查询更易区分，解决现有方法隐含假设人类能可靠评估细微差异行为的不足。  

### 📈 实验结果
通过三类实验验证：（1）在DMControl locomotion任务上设置不同相似度阈值ϵ模拟人类判断不一致，系统评估方法鲁棒性；（2）定性展示PB²在复杂偏好 landscape 中摆脱局部最优，而单智能体方法陷入其中；（3）在反馈极有限的导航任务中对比分析，体现PB²的反馈效率。实验表明PB²生成更易区分的查询提升奖励学习效率，在人类反馈不一致时更鲁棒，少反馈下性能更优；面对教师对相似轨迹段误标场景，也展现稳健性能与更强偏好探索能力。  

### 💬 可借鉴之处
1. 群体化思路拓展：将群体方法引入PbRL，为解决单智能体探索局限提供新范式，启示在依赖人类反馈、需行为多样性的任务中，可考虑多智能体并行探索模式。  
2. 现实场景适配：关注人类反馈不一致的现实情况，设计鲁棒方法，为实际应用中处理噪声反馈提供参考，强调方法要适应真实世界非理想教师的场景。  
3. 偏好探索与奖励模型协同：通过行为多样性让偏好查询更易区分，优化奖励模型学习，提示在涉及人类交互的学习任务里，重视交互内容的“可区分性”以提升反馈价值。

## $\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts
### Abstract
Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.
### 🌟 论文解读 | SPECS：用“推测草稿”加速大模型推理，平衡延迟与精度

### 📌 背景痛点/本文动机
大语言模型（LLM）推理能力的提升常依赖“测试时算力扩容”，比如分配更多计算资源做更充分的探索。但算力增加往往导致用户侧延迟升高，直接影响体验。现有测试时扩容方法多聚焦算力（FLOPS）优化精度，却忽略延迟约束。此外，基于Transformer的LLM自回归采样延迟常受限于内存加载而非总算力，而推测解码虽能借小模型提候选 token 降延迟，却会增加总计算量。于是，论文试图回答：**能否设计高效测试时扩容方法，优化延迟 - 效用权衡？**

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 SPECS 算法框架  
SPECS 受推测解码启发，是一种“延迟感知型”测试时扩容方法。它用**更小更快的草稿模型**高效生成候选序列，再结合**更大的目标模型**与**专用奖励模型**评估候选。整体遵循“草稿 - 选择”流程：迭代生成响应块，每轮用草稿模型生成候选块，经打分选择后拼接，进入下一轮；若草稿全被拒，则切换目标模型生成候选。

💡 创新点2：奖励引导的软验证与延迟机制  
- 奖励引导软验证（SUBSAMPLE 子例程）：基于草稿、目标、奖励模型计算的“分数”选候选块，既优化效用 - 延迟权衡，也避免简单丢弃高奖励但可能被 naive 推测解码漏掉的轨迹。  
- 奖励感知延迟规则（CASCADE 子例程）：自适应决定下一轮用草稿还是目标模型生成候选——让大模型处理难题步骤，小模型处理简单步骤，动态平衡算力与延迟。  

💡 创新点3：理论分析保障收敛性  
从理论上分析，SPECS 在结合草稿、目标、奖励模型优化“KL 正则化奖励最大化”目标时，其软验证方法随 beam 宽度增大，能优雅收敛到最优解。


### 📈 实验结果
论文在 MATH500、AMC23、OlympiadBench 数据集测试，用 Qwen - 1.5B - Instruct（草稿模型）、Qwen - 7B - Instruct（目标模型）与 Qwen - 7B - Math - PRM（奖励模型）验证：  
- 精度层面：SPECS 匹配甚至超越 beam search 精度；  
- 延迟层面：延迟最多降低约 19.1%，在精度与延迟间实现更优权衡。  


### 💬 可借鉴之处
1. **延迟 - 精度权衡思路**：跳出“只看算力/精度”的思维定式，把延迟作为核心约束，为大模型落地低延迟场景（如个性化交互）提供新思路；  
2. **多模型协作范式**：用“小草稿模型 + 大目标模型 + 奖励模型”分层协作，既利用小模型提速，又靠大模型保精度，还借奖励模型做灵活选择，这种“分工”模式可迁移到其他需平衡资源与效果的任务；  
3. **理论 + 实验双验证**：从理论证明收敛性，再用真实数据集验证，为方法可靠性背书，也示范了学术研究中“方法 - 理论 - 实验”闭环的重要性。  


SPECS 为大模型推理的“延迟 - 精度”难题提供了一套兼具创新性与实用性的解法，无论是工业界落地低延迟 LLM 应用，还是学术界探索测试时优化新方向，都有不少可借鉴的闪光点~

## from-outcomes-to-processes--guiding-prm-learning-from-orm-for-inference-time-alignment
### Abstract
Inference-time alignment methods have gained significant attention for their
efficiency and effectiveness in aligning large language models (LLMs) with
human preferences. However, existing dominant approaches using reward-guided
search (RGS) primarily rely on outcome reward models (ORMs), which suffer from
a critical granularity mismatch: ORMs are designed to provide outcome rewards
for complete responses, while RGS methods rely on process rewards to guide the
policy, leading to inconsistent scoring and suboptimal alignment. To address
this challenge, we introduce process reward models (PRMs) into RGS and argue
that an ideal PRM should satisfy two objectives: Score Consistency, ensuring
coherent evaluation across partial and complete responses, and Preference
Consistency, aligning partial sequence assessments with human preferences.
Based on these, we propose SP-PRM, a novel dual-consistency framework
integrating score consistency-based and preference consistency-based partial
evaluation modules without relying on human annotation. Extensive experiments
on dialogue, summarization, and reasoning tasks demonstrate that SP-PRM
substantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement
in GPT-4 evaluation scores across all tasks.
### 🌟 论文解读 | 从结果到过程：用ORM引导PRM实现推理时对齐

### 📌 背景痛点/本文动机
大语言模型（LLMs）在自然语言处理任务中表现出色，但常与人类偏好不一致。训练后对齐方法（如SFT、RLHF）计算成本高且需重新训练，推理时对齐成为有前景的替代方案。奖励引导搜索（RGS）是主流推理时对齐框架，现有方法主要依赖结果奖励模型（ORMs），存在关键的粒度不匹配问题：ORMs为完整响应提供结果奖励，而RGS方法依赖过程奖励指导策略，导致评分不一致和对齐次优。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：引入过程奖励模型（PRM）并提出双一致性目标  
为解决粒度不匹配挑战，引入PRM到RGS中，并提出理想PRM应满足的两个目标：分数一致性（Score Consistency），确保对部分和完整响应的评估连贯；偏好一致性（Preference Consistency），使部分序列评估与人类偏好对齐。  

💡 创新点2：提出SP - PRM双一致性框架  
SP - PRM是从ORM诱导PRM的新颖双一致性框架，包含两个核心模块。分数一致性模块通过将完整响应分解为部分序列，基于Bradley - Terry模型实现奖励建模，解决ORM固有的粒度不匹配，捕捉长期依赖；偏好一致性模块利用强奖励模型作为参考模型计算部分序列熵，重新加权其对训练过程的贡献，使部分序列的PRM奖励与人类偏好对齐。该框架无需人工标注，从ORM获取指导，同时兼顾部分上下文的长期对齐和人类偏好一致性。

### 📈 实验结果
在对话生成、文本摘要和复杂推理三个任务上进行广泛评估，应用于1B到3B参数的模型架构。结果表明，SP - PRM显著增强现有RGS方法，在所有任务的GPT - 4评估分数中实现了3.6% - 10.3%的提升。

### 💬 可借鉴之处
1. 针对现有方法的粒度不匹配问题，提出从结果奖励模型拓展到过程奖励模型的思路，为推理时对齐方法的改进提供了新方向，启发研究者关注奖励模型在不同序列粒度下的一致性问题。  
2. 设计的双一致性框架SP - PRM，无需人工标注即可利用现有ORM资源来训练PRM，在资源利用和减少标注成本方面提供了可借鉴的范式，对于推动推理时对齐技术在实际场景的应用有参考价值。  
3. 在多个任务和不同规模模型上的有效实验，验证了方法的通用性和鲁棒性，为后续相关任务（如对话、摘要、推理等）中改进RGS方法提供了实验设计和方法应用的参考。

## theoretical-tensions-in-rlhf--reconciling-empirical-success-with-inconsistencies-in-social-choice-theory
### Abstract
Despite its empirical success, Reinforcement Learning from Human Feedback
(RLHF) has been shown to violate almost all the fundamental axioms in social
choice theory -- such as majority consistency, pairwise majority consistency,
and Condorcet consistency. This raises a foundational question: why does RLHF
perform so well in practice if it fails these seemingly essential properties?
In this paper, we resolve this paradox by showing that under mild and
empirically plausible assumptions on the preference profile, RLHF does satisfy
pairwise majority and Condorcet consistency. These assumptions are frequently
satisfied in real-world alignment tasks, offering a theoretical explanation for
RLHF's strong practical performance. Furthermore, we show that a slight
modification to the reward modeling objective can ensure pairwise majority or
Condorcet consistency even under general preference profiles, thereby improving
the alignment process. Finally, we go beyond classical axioms in economic and
social choice theory and introduce new alignment criteria -- preference
matching, preference equivalence, and group preference matching -- that better
reflect the goal of learning distributions over responses. We show that while
RLHF satisfies the first two properties, it fails to satisfy the third. We
conclude by discussing how future alignment methods may be designed to satisfy
all three.
### 🌟 论文解读 | RLHF实践成功背后：调和社会选择理论矛盾的理论探索

### 📌 背景痛点/本文动机
大语言模型（LLMs）在众多任务中表现卓越，强化学习从人类反馈（RLHF）是让模型行为与人类期望对齐的关键方法。然而，RLHF在理论层面却与社会选择理论的诸多基础公理（如多数一致性、成对多数一致性、孔多塞一致性）相违背。这就产生了一个核心疑问：RLHF在实践中表现优异，可为何在理论上违背这些看似关键的公理？本文正是为解决这一矛盾、从理论层面解释RLHF实践成功原因并探索改进方向而展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：揭示RLHF满足公理的前提条件  
在对偏好分布做温和且符合实证的假设下，证明RLHF能满足成对多数一致性与孔多塞一致性。现实中对齐任务里常见的偏好数据结构（比如每个比较最多由一个标注者标注、允许循环偏好等情况构成的偏好分布），让RLHF可以正确识别孔多塞胜者（若存在）并赋予最高分数，从理论上解释了RLHF实践效果好的原因。  

💡 创新点2：改进奖励模型目标以保障公理满足  
针对多个标注者参与同一比较的场景，对奖励模型目标做微小修改——基于多数投票聚合偏好并赋予二元偏好。这种修改让奖励模型能满足多数一致性、成对多数一致性和孔多塞一致性，其本质是让奖励建模隐式实现了社会选择理论中满足优良公理的Copeland规则，为现有实践提供理论支撑同时，也为未来对齐策略指明方向。  

💡 创新点3：提出适配LLM对齐的新公理  
突破经济与社会选择理论的经典公理框架，提出偏好匹配、偏好等价、群体偏好匹配这三个更贴合生成模型学习响应分布目标的新对齐准则。证明目标分布是定义良好、存在且唯一的，还分析出RLHF满足前两个准则但不满足第三个，为未来方法如何满足全部准则提供思路。  

### 📈 实验结果
文中未开展传统意义上的实验验证（侧重理论分析推导），通过严谨的理论推导与论证，阐明了在特定偏好分布假设下RLHF满足经典社会选择公理的情况、修改奖励模型目标后的公理满足性，以及新提出的三个分布层面公理与RLHF的契合情况等关键结论，从理论角度支撑了各创新点的合理性与价值。  

### 💬 可借鉴之处
1. 理论解释实践：为RLHF在实践中表现好却违背经典公理这一矛盾提供了理论层面的解释，让从业者理解其成功背后的偏好分布假设等关键因素，在实际构建偏好数据、设计对齐流程时更有理论依据。  
2. 改进奖励模型思路：提出的奖励模型目标修改方式，为处理多标注者场景下的偏好聚合提供了更优路径，能指导后续优化奖励建模环节以保障对齐质量与公理满足性。  
3. 新公理拓展方向：新提出的三个适配LLM对齐的公理，为该领域后续评估方法、设计新对齐算法提供了全新的衡量维度与目标导向，推动领域从经典公理框架向更贴合生成模型特性的方向发展。

## treerl--llm-reinforcement-learning-with-on-policy-tree-search
### Abstract
Reinforcement learning (RL) with tree search has demonstrated superior
performance in traditional reasoning tasks. Compared to conventional
independent chain sampling strategies with outcome supervision, tree search
enables better exploration of the reasoning space and provides dense, on-policy
process rewards during RL training but remains under-explored in On-Policy LLM
RL. We propose TreeRL, a reinforcement learning framework that directly
incorporates on-policy tree search for RL training. Our approach includes
intermediate supervision and eliminates the need for a separate reward model
training. Existing approaches typically train a separate process reward model,
which can suffer from distribution mismatch and reward hacking. We also
introduce a cost-effective tree search approach that achieves higher search
efficiency under the same generation token budget by strategically branching
from high-uncertainty intermediate steps rather than using random branching.
Experiments on challenging math and code reasoning benchmarks demonstrate that
TreeRL achieves superior performance compared to traditional ChainRL,
highlighting the potential of tree search for LLM. TreeRL is open-sourced at
https://github.com/THUDM/TreeRL.
### 🌟 论文解读 | TreeRL：基于On - Policy树搜索的大语言模型强化学习

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务中展现出卓越能力，强化学习（RL）是提升其推理能力的有效方法。当前LLM的RL方法多独立采样轨迹并基于最终答案获取奖励，而在其他领域成功的树搜索在LLM推理的强化学习中发展不足。一方面，经典蒙特卡洛树搜索（MCTS）在相同推理成本下效果和效率不如独立采样多响应；另一方面，树搜索虽能提供细粒度过程监督，但离线过程奖励模型对RL训练性能提升贡献小。因此，探索结合树搜索的On - Policy RL训练以提升LLM推理能力具有重要意义。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出TreeRL强化学习框架
TreeRL是一种将On - Policy树搜索直接纳入RL训练的强化学习框架。该框架包含中间监督，无需单独训练奖励模型，避免了现有方法中单独训练过程奖励模型可能出现的分布不匹配和奖励黑客问题。通过树搜索为RL训练提供密集的、On - Policy的过程奖励，更好地探索推理空间。

💡 创新点2：提出高效树搜索策略EPTree
不同于MCTS将答案分解为小部分让模型逐步探索，EPTree基于熵从现有树中最不确定的中间token分叉生成新响应，直到得到最终答案。这种方式在相同生成token预算下，通过从高不确定性中间步骤策略性分支而非随机分支，实现更高搜索效率，且通常只需约两次迭代就能形成生成树，能生成更多样有效的响应。

💡 创新点3：基于树搜索的过程监督强化学习
在树的每个步骤基于优势分配信用，计算给定推理步骤的过程信号为全局优势和局部优势的加权和。全局优势反映该步骤对问题整体正确率的潜力，局部优势量化该步骤与其在树中父步骤相比的改进。这些优势信号直接从On - Policy生成的树中推导，能抵抗奖励黑客且不依赖额外奖励模型。

### 📈 实验结果
在具有挑战性的数学和代码推理基准测试（基于Qwen和GLM模型）上评估TreeRL，结果表明TreeRL比传统ChainRL性能更优，展现出树搜索对LLM的潜力。EPTree在不同推理预算下持续优于独立同分布多链采样和MCTS，TreeRL结合EPTree比采用独立同分布多链采样的ChainRL表现更好，性能提升受益于EPTree良好的PassRate表现和过程监督。

### 💬 可借鉴之处
1. 树搜索与强化学习结合的思路：为提升大模型推理能力提供了新方向，不再局限于传统的独立轨迹采样和仅基于最终结果的奖励机制，利用树搜索更好探索推理空间。
2. 高效树搜索策略设计：EPTree基于熵的高不确定性中间步骤分支方式，为在有限token预算下提升搜索效率提供了参考，可应用于其他需要高效探索的生成式任务场景。
3. 过程监督信号构建：基于树中步骤优势计算过程信号，无需额外奖励模型且抗奖励黑客的方式，为强化学习中奖励信号设计提供了创新思路，可借鉴到需要细粒度监督的RL任务中。

## personalized-llm-decoding-via-contrasting-personal-preference
### Abstract
As large language models (LLMs) are progressively deployed in various
real-world applications, personalization of LLMs has become increasingly
important. While various approaches to LLM personalization such as prompt-based
and training-based methods have been actively explored, the development of
effective decoding-time algorithms remains largely overlooked, despite their
demonstrated potential. In this paper, we propose CoPe (Contrasting Personal
Preference), a novel decoding-time approach applied after performing
parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is
to leverage reward-guided decoding specifically for personalization by
maximizing each user's implicit reward signal. We evaluate CoPe across five
open-ended personalized text generation tasks. Our empirical results
demonstrate that CoPe achieves strong performance, improving personalization by
an average of 10.57% in ROUGE-L, without relying on external reward models or
additional training procedures.
### 🌟 论文解读 | 解码阶段个性化LLM新范式：CoPe让模型更懂你

### 📌 背景痛点/本文动机
随着大语言模型（LLMs）在现实应用中广泛部署，LLM的个性化变得愈发重要。目前已有基于提示（prompt - based）和基于训练（training - based）的个性化方法，但解码阶段的有效算法开发却被忽视，尽管其有很大潜力。基于提示的方法缺乏对用户数据的直接学习，效果受限；基于训练的方法虽能更好捕捉用户偏好，但存在灾难性遗忘和计算成本增加等问题。所以本文旨在从解码阶段入手，提出新方法实现LLM的有效个性化。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出CoPe（Contrasting Personal Preference）解码阶段方法  
CoPe是在对用户特定数据进行参数高效微调（PEFT）后应用的解码阶段新方法。它属于奖励引导解码的一种，但无需外部奖励模型，而是利用PEFT调优模型和原始基础模型的似然来近似隐式用户奖励信号，将这种隐式奖励与对比解码目标相联系，实现奖励引导解码用于个性化，让生成文本更好地与用户偏好对齐。

💡 创新点2：增强PEFT捕捉隐式用户奖励  
通过Direct Preference Optimization（DPO）对比正面响应（用户提供）和负面响应（不太可能来自用户，如其他用户或合成的低隐式奖励输出）之间的隐式奖励。为避免依赖其他用户数据的隐私和实际挑战，用Best - of - N采样生成低隐式奖励的合成负面响应。这种训练方法不仅提升了PEFT的有效性，也为奖励引导解码提供更准确的隐式用户奖励建模，进而提升整体个性化效果。

### 📈 实验结果
在来自Language Model Personalization（LaMP）和LongLaMP基准的五个不同个性化开放式文本生成任务中评估CoPe。结果显示，与任务微调模型相比，CoPe在ROUGE - L上平均相对提升10.57%；与缺乏对比机制的简单个性化模型相比，在各任务中ROUGE - L平均提升5.67%。而且CoPe在不同规模和类型的最先进LLMs上泛化性良好，证明其隐式奖励最大化能进一步增强与个体用户偏好的对齐。

### 💬 可借鉴之处
1. 解码阶段个性化思路：开拓了LLM个性化在解码阶段的研究方向，展示了无需额外训练流程和外部奖励模型，仅在解码时利用模型自身似然对比实现个性化的可行性，为后续解码阶段个性化方法提供了新思路。
2. PEFT增强方式：利用DPO和合成负面样本增强PEFT捕捉用户隐式奖励的方式，为参数高效微调在个性化场景下的优化提供了可参考的技术路线，在处理用户数据隐私和避免依赖外部数据方面给出了创新解法。
3. 多任务多模型验证：在多个任务和不同LLM上验证有效性，这种全面的实验设计思路以及所展现出的泛化能力，为相关方法的实用性验证提供了范例，让该方法在实际落地到不同场景时更具可信度。

## med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards
### Abstract
Large language models have shown promise in clinical decision making, but
current approaches struggle to localize and correct errors at specific steps of
the reasoning process. This limitation is critical in medicine, where
identifying and addressing reasoning errors is essential for accurate diagnosis
and effective patient care. We introduce Med-PRM, a process reward modeling
framework that leverages retrieval-augmented generation to verify each
reasoning step against established medical knowledge bases. By verifying
intermediate reasoning steps with evidence retrieved from clinical guidelines
and literature, our model can precisely assess the reasoning quality in a
fine-grained manner. Evaluations on five medical QA benchmarks and two
open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art
performance, with improving the performance of base models by up to 13.50%
using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by
integrating it in a plug-and-play fashion with strong policy models such as
Meerkat, achieving over 80\% accuracy on MedQA for the first time using
small-scale models of 8 billion parameters. Our code and data are available at:
https://med-prm.github.io/
### 🌟 论文解读 | Med - PRM：医疗推理模型的“步步为营”新范式

### 📌 背景痛点/本文动机
临床决策制定（CDM）是一个复杂多步骤过程，大语言模型（LLMs）虽在医疗应用有进展，但现有方法难在推理特定步骤定位和纠正错误，而医疗领域识别与解决推理错误对准确诊断和有效医疗至关重要。同时，过程奖励建模（PRM）应用于医疗有挑战：一是高质量步骤级监督获取成本高且费力，现有自动标注策略易低估合理但没导向正确结果的早期步骤；二是医疗推理需大量领域知识，仅靠语言模型参数难完全涵盖，训练奖励模型缺医疗上下文也不够。这些痛点推动了Med - PRM的提出。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Med - PRM框架
Med - PRM是检索增强的过程奖励建模框架，采用RAG - AS - A - JUDGE方式，基于临床问题和检索到的医疗文档对每个推理步骤做逐步评估。该评估相比训练时基于采样的自动标注方法，更贴近专家医师标注，在训练和推理阶段融入临床知识，能更精准评估中间推理步骤。
💡 创新点2：兼具通用性与高效性
Med - PRM展现出即插即用的通用性，可与Meerkat等强大策略模型结合。且训练成本高效，如针对花费约2万美元训练数据的UltraMedical模型，Med - PRM用花费不到20美元的精心策划数据集训练，仍能提升性能，体现成本效益与扩展性。

### 📈 实验结果
在五个医疗QA基准和两个开放式诊断任务评估中，Med - PRM实现了最先进性能，能将基础模型性能提升高达13.50%。与强策略模型结合后，用80亿参数小规模模型在MedQA首次实现超80%准确率，在七个医疗基准中六个实现SOTA，在MedQA（4选项）上用8B参数模型达到80.35%准确率，平均比现有PRM基线在七个医疗基准上高3.44%。

### 💬 可借鉴之处
从技术创新看，检索增强结合过程奖励建模用于垂直领域推理评估是很好思路，为领域特定的LLM优化提供方向；在工程实践上，展示了低成本数据训练高效模型辅助强模型提升性能的路径，为资源有限但需提升模型医疗推理能力的场景提供参考；从医疗AI发展角度，强调步骤级验证和医疗知识融入，让模型推理更透明可靠，为医疗AI贴近临床实际应用标准提供了实践范式，后续医疗或其他垂直领域的推理模型优化可借鉴其步骤验证、知识融合与成本控制等思路。

## agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards
### Abstract
Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too sparse for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.
### 🌟 论文解读 | Agent-RLVR：让大模型在软件工程任务中“拜师学艺”的RL框架

### 📌 背景痛点/本文动机
强化学习从可验证奖励（RLVR）在数学、竞赛编程等可验证领域提升大语言模型（LLM）推理能力表现出色，但在智能体环境（多步骤、复杂问题求解场景）中效果骤降。这类场景奖励稀疏，前沿LLM也易高失败率，传统RLVR难以有效训练。同时，智能体环境需多轮推理、与外部环境交互，训练复杂度高，为让RLVR在复杂真实场景（如软件工程）生效，催生了Agent - RLVR框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Agent - RLVR框架适配智能体场景  
借鉴人类教学法引入“agent guidance（智能体指导）”机制，利用从高层战略规划到错误与环境交互动态反馈等多样信息线索，引导智能体走向成功轨迹，像老师指导新人一样帮智能体在复杂解空间导航，还能通过环境探索促进自我提升。训练循环分三步：先让智能体无指导尝试生成初始轨迹，用单元测试验证并补充指导；再让智能体带指导重试；最后基于指导后轨迹奖励用RLVR更新策略。
💡 创新点2：构建软件工程领域专属数据集  
精心整理含817个训练环境的数据集，涵盖问题陈述、环境和指导信息，超越传统输入 - 输出对，捕捉带集成指导信号的完整编码环境，为训练软件工程智能体提供丰富资源。

### 📈 实验结果
在SWE - Bench Verified基准测试中，Agent - RLVR将Qwen - 2.5 - 72B - Instruct的pass@1性能从9.4%提升至22.4%；指导增强的RLVR数据用于测试时奖励模型训练，能进一步把pass@1推至27.8%；指导模型在pass@1（19.8%→22.4%）和pass@32（34.2%→38.4%）上都有提升，验证指导是关键组件，也体现方法在小数据集下提升智能体多步骤推理能力的高效性。

### 💬 可借鉴之处
1. 应对稀疏奖励场景时，引入类人类教学的指导机制是有效思路，为复杂多步骤推理任务中模型训练提供新范式参考。
2. 构建领域专属、含丰富环境与指导信息的数据集，能为特定领域智能体训练筑牢数据基础，这种“定制化 + 场景化”数据构建思维值得借鉴。
3. 展示了RLVR数据在奖励模型训练等方面的额外价值，启发后续探索不同模块间数据复用与协同增效，拓展技术应用边界。

## reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems
### Abstract
There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.
### 🌟 论文解读 | ReGuidance：为困难逆问题提升采样质量的简洁扩散包装器

### 📌 背景痛点/本文动机
近年来，利用预训练扩散模型作为先验来解决逆问题以及通过奖励模型引导扩散模型的研究十分活跃。像扩散后验采样（DPS）这类无训练方法虽提供了灵活的启发式算法，但当奖励信息不足时（如低信噪比的困难逆问题场景），这些技术会偏离数据流形，无法生成逼真输出。例如在大区域图像修复、高倍数超分辨率等硬逆问题中，现有先进基线方法表现不佳，既难保证采样来自合适的倾斜密度（后验分布），生成结果的真实感也很差。本文正是为解决这些问题，提出提升样本真实感与奖励表现的方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ReGuidance方法  
ReGuidance是一个简洁的包装器算法，用于提升现有基于扩散的逆问题求解器的样本质量。它以用户选择的逆问题求解器生成的候选解$\hat{x}$为输入，分两步操作：第一步，从$\hat{x}$出发反向运行预训练扩散模型针对基础密度$q$的确定性采样器，提取与之关联的潜在变量$x^*$；第二步，以$x^*$为初始化，运行Diffusion Posterior Sampling（DPS）算法来生成新的重建结果$x_{\text{DPS}}$ 。  

💡 创新点2：理论保障  
从理论层面证明，在某些多模态数据分布下，ReGuidance能同时提升奖励并使候选解更接近数据流形。这是首次为DPS提供严格的算法保障，填补了该领域理论层面的部分空白，从数学角度支撑了方法的有效性。

### 📈 实验结果
在图像修复的传统困难逆任务（如大区域图像修复、高倍数超分辨率）中展开评估：当现有先进基线方法表现不佳时，在这些基线之上应用ReGuidance，能显著提升样本质量与测量一致性。以图像修复任务为例，对不同基线应用ReGuidance后，奖励和真实感指标都有持续且显著的提升；定性层面还能得到多样且逼真、与原始测量样本有意义区别的重建结果。 

### 💬 可借鉴之处
1. 方法设计简洁模块化：ReGuidance的两步操作思路清晰，易于理解和整合到现有基于扩散模型的逆问题求解流程中，为改进现有方法提供了轻量但有效的思路。  
2. 兼顾实证与理论：既通过实验验证在硬逆问题场景下对样本质量的提升，又给出理论证明支撑方法优势，这种从实践到理论的完整研究范式值得相关领域研究借鉴，帮助后续工作在方法创新时更注重理论与实证的结合。 
3. 针对硬逆问题场景：聚焦低信噪比等硬逆问题场景展开研究，为这类长期困扰的难题提供了新的解决方向，启发研究者关注更具挑战性的逆问题场景下的方法优化。 

## eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling
### Abstract
Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.
### 🌟 论文解读 | EQA - RM：为具身问答量身定制的生成式奖励模型，实现测试时可扩展评估

### 📌 背景痛点/本文动机
奖励模型（RMs）在大模型对齐中至关重要，但在复杂的具身任务（如具身问答，EQA）中却未得到充分探索。EQA需要智能体在3D环境中通过多模态观察和动作序列来感知、交互和推理以回答问题，对智能体的空间、时间和逻辑理解进行细致评估至关重要，而通用的奖励模型方法无法满足这一需求。现有通用奖励模型多为静态输入或简单结果设计，难以捕捉具身任务中固有的时空和逻辑依赖关系，因此迫切需要专门的机制来准确评估EQA的多方面成功指标。同时，EQA领域缺乏用于严格评估和比较奖励模型的标准化基准，当前EQA任务基准侧重于粗略的成功指标，而非对奖励模型发展至关重要的细粒度轨迹质量评估。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出EQA - RM生成式多模态奖励模型
EQA - RM是专为评估EQA轨迹而设计的新型多模态奖励模型，作为生成式奖励模型（GenRM），不仅能产生标量奖励，还能为评估提供明确的推理过程。其具有增强的空间、时间和推理处理能力，以处理EQA任务中固有的独特多模态数据流。通过高效的两阶段训练过程，第一阶段用标准的Rejective Finetuning（RFT）教会模型期望的输出格式（包含文本批评和标量分数）；第二阶段采用创新的Contrastive Group Relative Policy Optimization（C - GRPO）强化学习策略，解决仅RFT可能只学格式不学内容的问题，利用基于规则的对比奖励（源于针对性的数据增强，如视频帧打乱、空间区域随机掩码、推理步骤混乱等扰动方式），让模型区分原始连贯上下文和合成扰动上下文下的策略输出，从而内化时间顺序、细粒度空间细节和连贯逻辑流的重要性，培养对具身任务强大且敏锐的评估能力。

💡 创新点2：构建EQARewardBench基准
为解决EQA领域奖励模型评估基准缺失的问题，基于OpenEQA构建了EQARewardBench。该基准包含来自HM3D和ScanNet两种家庭环境的具身情节记忆视频，从原始问答对构建更全面的问题 - 响应 - 推理轨迹三元组，有1546个测试实例，用于评估奖励模型在轨迹质量的八个不同方面（如正确性、接地性、效率等），为EQA任务上的奖励模型提供了标准化、可比较的评估平台。

### 📈 实验结果
以Qwen2 - VL - 2B - Instruct为基础进行微调的EQA - RM展现出高样本效率，仅用700个样本在EQA - RM - Bench上达到61.9%的准确率，超越了强大的专有基线（如Gemini - 2.5 - Flash、GPT - 4o、Claude - 3.5 - Haiku）和开源的最先进模型（如RoVRM和VisualPRM）。同时，EQA - RM展示了测试时可扩展性，在推理时增加评估计算量，其在EQARewardBench上的准确率从42.47%提升到61.86%，性能提升后在基准测试中超越了领先的大型商业模型。

### 💬 可借鉴之处
1. 针对特定复杂任务设计专用奖励模型：当通用模型无法满足复杂任务（如具身任务）的评估需求时，可像EQA - RM一样针对任务特性，设计具备特定能力（如空间、时间、推理处理能力）的专用模型，解决通用模型的局限性。
2. 创新的训练策略：两阶段训练（RFT + C - GRPO）以及利用数据增强的对比奖励策略，为解决模型只学形式不学内容、提升模型对任务关键要素的理解提供了思路，可借鉴于其他需要模型深入理解任务细节的训练场景。
3. 构建领域基准：对于缺乏评估基准的领域，可像构建EQARewardBench一样，基于现有数据集构建专门的基准，推动领域内模型的评估和发展，为模型性能比较和改进提供标准平台。

## reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning
### Abstract
This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.
### 🌟 论文解读 | 小模型也能打！强化学习微调让轻量级语言模型玩转指令遵循与数学推理

### 📌 背景痛点/本文动机
生成式语言模型在自然语言理解与生成领域取得了亮眼成果，但如何让小规模语言模型在指令遵循、数学推理等不同推理任务中表现出色仍是难题。同时，不同微调技术（尤其是强化学习类技术）间的性能对比也有待深入探索。为此，研究聚焦于轻量级开源模型Qwen2.5 - 0.5B Base，探索基于强化学习的微调方法在偏好对齐与特定领域适配方面的表现，以明确轻量级模型在类人对齐学习场景下的能力与局限。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多微调技术对比与RLOO reward model探索  
对比了监督微调（SFT）、直接偏好优化（DPO）、Reinforce Leave - One - Out（RLOO）三种微调技术。在RLOO中，还评估了DeBERTa、DistilBERT、Siamese DistilBERT等不同奖励模型，以此探究奖励模型对最终策略性能的影响。  
💡 创新点2：数学推理任务的数据增强与推理工具结合  
为提升模型数学推理能力，基于Countdown数据集构造含1600个样本的高质量合成数据集（借助GPT - 4o完成问题生成与答案验证）；同时采用best - of - N采样策略，结合外部验证器来提升模型预测的可靠性与正确性，探索微调与推理时工具结合的潜力。  

### 📈 实验结果
在指令遵循任务上，DPO相比SFT在全参数与LoRA配置下均能进一步提升效果；RLOO变体里，以DeBERTa为奖励模型的版本对齐分数最高。数学推理任务中，合成数据能小幅提升性能，而结合外部验证器的best - of - N采样带来显著增益，准确率超0.81，是SFT的两倍多。整体表明轻量级模型经有效微调与工具辅助可实现不错性能，奖励模型质量、采样响应多样性对RLOO很关键，外部验证器 + best - of - N采样为数学推理提准确率提供了低成本方案。

### 💬 可借鉴之处
对于想优化小模型下游任务性能的研究者与开发者，可借鉴多强化学习微调技术对比思路，明确不同技术在偏好对齐等场景的优劣；在特定领域（如数学推理）任务中，尝试合成数据增强与推理时工具（如外部验证器 + best - of - N采样）结合的方式，在计算资源受限下提升小模型表现；同时重视奖励模型选型与采样多样性等因素对RLOO类方法的影响，为轻量级语言模型适配多任务提供实践参考。

## unsupervised-elicitation-of-language-models
### Abstract
To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.
### 🌟 论文解读 | 无监督激发语言模型潜力：ICM算法突破人类监督限制

### 📌 背景痛点/本文动机
当下预训练语言模型（LMs）的后训练范式，依旧依赖人类来指定期望行为，像通过演示或者偏好反馈等方式。然而，随着任务和模型行为愈发复杂，人类监督变得越来越不可靠，语言模型可能会学习模仿演示里的错误，或者利用反馈中的缺陷。并且，对于具备超人类能力的模型，获取高质量的人类监督存在困难甚至是不可能的。所以，如何训练语言模型去完成那些人类难以可靠演示或评估的任务，成为亟待解决的问题，本文正是为解决该挑战而生。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出无监督算法ICM
引入Internal Coherence Maximization（ICM）这一无监督算法，在没有外部监督的情况下，利用语言模型自身生成的标签对预训练语言模型进行微调。其目标是在给定由带标签输入指定的任务时，让预训练模型基于自身生成的标签在该任务上表现良好，无需使用任何提供的外部标签。
💡 创新点2：设计 scoring function 衡量标签质量
用由两部分组成的评分函数衡量模型生成标签集的质量，一是“相互可预测性（Mutual Predictability）”，计算模型在以所有其他标签为条件时推断每个标签的可能性，将所有示例的对数概率求和；二是“逻辑一致性（Logical Consistency）”，通过逻辑一致性函数检查标签集里数据点的标签之间是否逻辑一致，以此衡量标签中的不一致性。最终结合这两个部分得到整体评分函数 \( U(D) = \alpha · P_θ(D) − I(D) \) ，其中 \( \alpha \) 是平衡相互可预测性和逻辑一致性强度的超参数。 
💡 创新点3：模拟退火启发的近似搜索算法
由于找到最大化评分函数的最优标签集在计算上不可行（现实数据集规模下），ICM采用受模拟退火启发的高效近似算法。从空的标记集开始，用K个随机标记的示例初始化搜索过程，然后迭代添加标签，每次添加标签时执行采样新示例、确定标签同时修复引入的不一致、基于评分函数决定是否接受新标签这三个步骤，以此增量式扩展标签集并提高分数。

### 📈 实验结果
在GSM8k - verification、TruthfulQA和Alpaca reward modeling任务上，ICM方法匹配了基于“黄金监督（golden supervision）”训练的性能，且超过了基于众包人类监督训练的性能；在语言模型能力远超人类的任务（如从写作样本识别作者性别）上，ICM比基于人类标签的训练能显著更好地激发模型能力；在前沿模型训练方面，用ICM训练无监督奖励模型，再通过强化学习训练基于Claude 3.5 Haiku的助手，结果显示无监督奖励模型在Rewardbench评估中超过基于生产级高质量人类监督训练的对应模型，且无监督助手策略在与基于人类监督奖励模型训练的策略头对头比较中，赢得60%的对比。

### 💬 可借鉴之处
本文提出的ICM算法为突破人类监督限制训练语言模型提供了新思路，证明在现实生产规模场景中无监督激发能超越人类监督，为后训练前沿模型成为通用助手提供了实用方法；其设计的衡量标签质量的评分函数思路，以及受模拟退火启发的近似搜索算法，在处理需模型自身生成标签优化任务、解决计算不可行的优化问题等场景中，都有一定的借鉴意义，为后续相关研究在方法设计和算法选择上提供了参考方向。

## reward-models-enable-scalable-code-verification-by-trading-accuracy-for-throughput
### Abstract
The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.
### 🌟 论文解读 | 奖励模型：用精度换吞吐量，解锁大规模代码验证新范式

### 📌 背景痛点/本文动机
在利用大语言模型（LLM）解决编码任务时，“生成 - 排序”是常用范式，其中排序环节依赖验证器判断程序正确性。以往共识认为有全面验证器（如完整测试套件）时应优先选用，却鲜少考量速度与精度的权衡。但随着模型能处理更复杂软件任务，全面验证器的验证成本（如运行完整测试套件耗时）急剧攀升，甚至会抵消多采样候选方案带来的性能增益。因此，论文试图探究：在精度与速度的权衡中，结果奖励模型（ORM）能否在不牺牲过多精度的前提下，大幅提升验证速度，助力构建可扩展的代码验证系统？

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：明确ORM在可扩展程序验证中的价值  
打破“有全面验证器就弃用ORM”的固有认知，通过实证分析表明：ORM可在精度与吞吐量间做权衡，实现平均9.55倍于最强验证器（如完整测试套件）的速度提升，且相比仅用代码检查工具（linter）过滤再取常见响应的方式，精度还能高出33.55%，为大规模代码验证提供了高效路径。

💡 创新点2：提出“生成 - 剪枝 - 排序”策略  
在传统“生成 - 排序”基础上，引入“剪枝”环节：先用弱验证器（如运行部分测试用例、语法检查等）过滤明显错误的候选程序，再用ORM进行排序。该策略能减少精度损失并进一步提升吞吐量，比如仅用1个测试用例过滤，相比纯ORM策略，精度提升2.85%且速度额外加快16.93%；用10个测试用例时，精度提升10.38%，吞吐量仅损失16.69%，却仍比完整测试套件快29.71%。

💡 创新点3：揭示弱验证器作用机制  
通过实验分析“生成 - 剪枝 - 排序”有效的原因：弱验证器能移除高、低排名中错误的候选方案，缓解ORM的不精确性，降低结果方差，从机制层面解释了该策略在精度 - 速度权衡上的优势。

### 📈 实验结果
- 速度与精度权衡层面：ORM在换取吞吐量（速度）时能较好保留精度，“生成 - 剪枝 - 排序”系统相比完整测试套件，速度快11.65倍，仅损失8.33%的精度。 
- 不同策略对比：“生成 - 剪枝 - 排序”中，弱验证器（如少量测试用例）辅助ORM的方式，在精度和速度提升上均优于纯ORM或仅用弱验证器的方式；增加测试用例数量提升剪枝用弱验证器精度时，吞吐量损失可控且整体仍比最强验证器高效。
- 弱验证器价值：实证显示其能通过过滤错误候选（无论排名高低），减轻ORM误差，让整个验证排序流程更稳定高效。

### 💬 可借鉴之处
- 系统设计角度：在需验证大量程序的场景（如大规模代码库CI/CD、复杂编程任务求解），可借鉴“精度 - 吞吐量”权衡思路，引入ORM和弱验证器构建分层验证架构，平衡速度与正确性要求。 
- 方法创新角度：“生成 - 剪枝 - 排序”的两阶段策略，为处理需大量候选筛选的任务（不限于代码，如数学题求解、推理任务等）提供了性能优化范式，即先用低成本弱过滤工具缩小范围，再用更高效但稍欠精度的模型做精细排序。 
- 认知突破角度：打破“全面验证器至上”的思维定式，意识到在复杂任务场景下，结合不同精度、速度的验证工具做协同，是提升系统可扩展性的关键，为后续探索更高效的AI辅助编程、自动验证等方向打开新思路。

## dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision
### Abstract
While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.
### 🌟 论文解读 | DreamCS：无配对3D奖励监督下的几何感知文本到3D生成

### 📌 背景痛点/本文动机
文本到3D生成技术在数字漫画、游戏、电影和虚拟现实等领域展现出关键作用，但现有方法生成的3D资产往往难以契合人类偏好。当前3D内容的偏好对齐技术，通常依赖难收集的偏好配对多视角2D图像来训练2D奖励模型以指导3D生成，然而这种方式因固有2D偏差易产生几何伪影。同时，收集配对的偏好标记数据成本高、耗时且困难，且2D空间的反馈仅基于图像渲染而非3D结构，会导致如视角不一致、Janus脸问题等几何缺陷，限制了实际应用。因此，需要一种能提供几何级反馈且摆脱配对数据依赖的文本到3D生成框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建3D - MeshPref数据集
为解决配对偏好标记3D数据收集难的问题，构建了首个大规模无配对3D偏好数据集3D - MeshPref，包含14000 + 样本，每个样本有文本提示、3D资产及其偏好奖励分数。从Cap3D筛选多样高质量网格，用Llama - Mesh对几何保真度、语义对齐和结构合理性评分，再经人工验证优化分数，将高奖励3D资产选为偏好样本，低奖励选为非偏好样本。

💡 创新点2：提出RewardCS奖励模型
针对无配对数据训练3D奖励模型的难题，提出首个基于无配对数据训练的3D几何感知奖励模型RewardCS。引入基于柯西 - 施瓦茨（CS）散度的分布级训练目标，将偏好和非偏好资产的嵌入视为两个分布的样本，优化它们之间的CS散度，使模型给几何和语义更优的3D资产更高奖励。还证明了无配对数据上的CS散度与传统配对偏好监督渐近等价，为无配对奖励学习范式提供理论依据。

💡 创新点3：提出DreamCS框架
现有文本到3D框架缺乏几何级反馈原生支持，为此开发DreamCS，首个集成RewardCS到现有文本到3D管线的3D奖励引导框架。通过三项创新实现与隐式和显式3D表示无缝协作：可微分网格化使隐式场到端到端梯度流成为可能；自适应网格融合在不牺牲几何细节的情况下优化拓扑以适配RewardCS；渐进式奖励引导自动平衡粗结构细化和细粒度优化。

### 📈 实验结果
在GPTEval3D基准测试中，对于MVDream、Dreamfusion、Magic3D等单阶段和两阶段文本到3D生成管线，集成DreamCS后在几何对齐和网格质量方面比之前多视角引导方法取得更优结果。且3D奖励引导与现有2D方法互补，结合后能进一步提升性能。

### 💬 可借鉴之处
1. 数据集构建思路：面对数据收集难题时，可考虑构建大规模无配对且结合大语言模型初筛与人工精修的数据集，为模型训练提供高质量数据支撑。
2. 模型训练目标创新：在处理无配对数据场景时，借鉴基于分布级的训练目标（如CS散度）思路，摆脱配对比较依赖，拓展模型在无配对数据下的学习能力。
3. 框架集成创新：针对领域内现有框架短板（如缺乏几何级反馈支持），通过多项针对性创新（可微分网格化、自适应网格融合、渐进式奖励引导等）实现新组件与现有管线的无缝集成，为类似领域内框架升级提供参考范式。

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### 🌟 论文解读 | Athena：用数据高效的过程奖励模型提升多模态推理能力

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）和多模态大语言模型（MLLMs）在自然语言处理和多模态任务中取得显著进展，但解决复杂推理任务（如数学和多步骤推理）仍具挑战。为增强推理能力，测试时缩放（TTS）等方法被探索，其中过程奖励模型（PRMs）能为中间推理步骤提供细粒度反馈，性能更优且泛化性强。然而，PRMs 发展面临两大难题：一是获取带过程标签的高质量数据成本高（需大量人工标注或计算昂贵的自动化标注）；二是传统自动化标注（如蒙特卡洛估计）易产生噪声标签。本文旨在解决这些挑战，降低计算成本并减轻标签噪声问题，提升 PRMs 性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：利用强弱完成器预测一致性生成高质量过程标签  
传统蒙特卡洛等自动化标注方法易受完成器推理能力影响，标签有噪声且计算成本高。本文发现，强完成器即便中间步骤错误仍能得到正确答案，弱完成器则可能在中间步骤正确时也失败。基于此，提出用弱、强完成器预测一致性作为筛选可靠过程标签的标准，保留两者标签一致的步骤，减少完成器带来的偏差，提升标签质量。实验表明，约 5000 条高质量标签就能比传统方法约 30 万条大规模标注数据表现更优，且大幅降低数据合成和模型训练的计算成本。

💡 创新点2：提升 PRMs 性能的两大策略  
 - ORM 初始化：PRMs 通常基于预训练基础模型微调，而结果奖励模型（ORMs）在大规模响应级数据上训练，具备弱监督下评估中间步骤正确性的能力。因此用 ORMs 初始化 PRMs，将 ORMs 作为弱监督预训练，PRMs 再在高质量细粒度步骤数据上微调，显著提升性能。  
 - 负样本上采样：过程标签数据存在标签不平衡问题，通过对含负步骤标签的数据进行上采样，解决数据分布不均问题，优化模型训练。  

💡 创新点3：构建 Athena 系列模型并多场景验证  
基于上述方法构建结果奖励模型 Athena - ORM 和过程奖励模型 Athena - PRM，再利用 Athena - PRM 通过奖励排序微调得到 Athena - 7B。并在三个场景验证：测试时缩放（TTS）中对策略模型生成的多个输出排序；直接评估推理步骤正确性；奖励排序微调（用高奖励响应微调策略模型）。

### 📈 实验结果
- 测试时缩放场景：在 7 个多模态数学和推理基准测试中，用 Athena - PRM 配合不同规模（7B 到 72B）策略模型，推理能力显著提升。如用 Qwen2.5 - VL - 7B 作为策略模型时，在 WeMath 基准上零样本基线提升 10.2 分，在 MathVista 提升 7.1 分；在文本-only 数学基准用 Mistral - 8B 时提升 8.9 分。  
- 推理步骤正确性评估场景：在 VisualProcessBench 基准上，Athena - PRM 表现强劲，超越开源的 VisualPRM - 8B 等模型，F1 分数比之前最优结果高 3.9，展现准确评估推理步骤正确性的能力。  
- 奖励排序微调场景：基于 Qwen2.5 - VL - 7B 微调得到的 Athena - 7B，在 7 个数学和推理基准上大幅提升策略模型推理能力。  

### 💬 可借鉴之处
- 数据高效标注思路：利用多完成器预测一致性筛选标签，为解决需细粒度标注且标注成本高的任务提供了新范式，在减少数据量同时提升数据质量，实现数据高效利用。  
- 模型训练策略：ORM 初始化和负样本上采样策略，为提升奖励模型性能提供了可复用方法，可启发其他奖励模型或需细粒度反馈模型的训练优化。  
- 多场景验证模式：在测试时缩放、步骤评估、模型微调等多场景验证方法有效性，这种全面验证思路有助于更充分展示方法价值，为后续研究提供验证范式参考。

## know-what-you-don-t-know--uncertainty-calibration-of-process-reward-models
### Abstract
Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.
### 🌟 论文解读 | 校准过程奖励模型，让大模型推理更“自知”

### 📌 背景痛点/本文动机
在大语言模型（LLM）的推理时扩展（inference - time scaling）算法中，过程奖励模型（PRM）扮演着关键角色，它能指导模型在推理过程中做出决策。然而，现有先进的PRM存在校准不佳的问题，常常高估成功概率，这会限制其在诸多场景的效用，比如提供可解释的不确定性估计、判断何时“承认不懂”以及自适应调整推理计算预算等。同时，传统推理时扩展方法（如best - of - N）采用固定预算分配方式，在简单任务上浪费计算资源，在复杂任务上又可能资源不足，所以需要更智能的自适应策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：PRM校准方法——分位数回归
现有标准校准技术（如温度缩放）无法有效校准PRM，为此提出基于分位数回归的校准方案。通过收集LLM生成的中间推理步骤，并用蒙特卡洛展开得到每个前缀轨迹的真实成功概率作为数据，将PRM的预测头替换为分位数回归模型并微调，使PRM输出能更准确反映LLM得到正确答案的不确定性，同时还能给出置信边界。
💡 创新点2：实例自适应缩放（IAS）框架
利用校准后的PRM，依据对部分推理轨迹产生正确最终答案的估计可能性，动态调整推理预算。针对常用的best - of - N和beam search推理时扩展方法，该框架能让大模型在推理时像人类解决问题一样，对有挑战或有潜力的推理路径投入更多计算资源，而非像传统方法那样对每个查询分配固定数量的推理轨迹。并且从理论上证明，校准后的奖励分数能估计生成至少一个正确答案所需的额外轨迹最小数量。

### 📈 实验结果
在数学推理基准测试中：（i）提出的PRM校准方法实现了小的校准误差，性能超过基线方法；（ii）验证了校准对实现有效自适应缩放至关重要；（iii）所提IAS策略在保持最终答案准确率的同时降低了推理成本，能在对问题更有信心时合理减少计算量。

### 💬 可借鉴之处
从方法层面，分位数回归用于模型校准的思路为解决类似的概率预测校准问题提供了参考，可迁移到其他需要概率预测校准的模型场景；从应用层面，实例自适应的资源分配理念在大模型推理优化、资源高效利用方面具有启发，未来在其他需要动态资源调配的AI任务（如多步骤决策、复杂问题求解）中也可能适用；代码开源（https://github.com/azizanlab/instance - adaptive - scaling）方便研究者复现和进一步拓展相关工作，推动领域发展。

## learning-to-reason-across-parallel-samples-for-llm-reasoning
### Abstract
Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.
### 🌟 论文解读 | 融合并行与顺序推理，SSA让大模型推理更高效

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务上能力不断提升，而测试时计算资源的分配（即测试时缩放）是优化模型性能的新方向。现有测试时缩放方法分并行和顺序两类：并行缩放是独立生成多条推理路径再聚合（如多数投票）；顺序缩放则迭代优化单个解（如基于提示的自我反思）。但并行方法常孤立看待样本，顺序方法计算成本或适配性受限。本文旨在提出新方法，融合二者优势，更高效利用测试时计算资源提升推理性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Sample Set Aggregator（SSA）模型架构  
设计轻量级的SSA模型，将其与生成答案的基础模型（LMans）解耦。先由LMans并行生成K个候选答案，再把这些候选答案拼接成序列输入SSA，SSA通过强化学习优化以输出最终正确答案。这种设计让SSA能基于基础模型输出的分布特性，直接优化答案合成过程，而非孤立评估单个样本。  

💡 创新点2：基于输出分布推理，解耦训练与推理  
SSA不直接训练生成答案的基础模型（LMans可视为黑盒），而是针对其采样输出进行优化。这种“推理输出分布而非调整模型内部”的思路，让方法更灵活——可适配不同基础模型（甚至是只能通过API调用的黑盒大模型），只需用其采样答案训练SSA即可。  

💡 创新点3：统一并行与顺序缩放优势  
并行缩放能快速获取多视角答案，顺序缩放可迭代优化推理；SSA通过“并行采样+单步顺序RL聚合”的方式，在一次前向传递中结合二者长处：用并行获取多样性，用SSA的顺序推理实现精准聚合，且仅需训练小模型就能带来显著性能提升。  


### 📈 实验结果
1. 性能超越强基线：在多个数学推理数据集上，SSA相比基于奖励模型重排序等测试时缩放方法表现更优，大幅缩小了模型实际性能与“理论最优（oracle - best）”精度的差距。  
2. 泛化能力突出：跨样本集大小、基础模型家族（如Qwen 2.5、Llama 3.1）、模型规模（7B/14B/32B）和任务，SSA都展现出良好泛化性。比如在一个数据集上为特定模型训练的SSA，能成功聚合不同模型家族、规模在不同任务上的输出。  
3. 轻量化优势：紧凑的SSA模型能匹配顺序缩放中经强化训练的大模型性能，证明其作为轻量顺序缩放方式的有效性。  


### 💬 可借鉴之处
1. 架构解耦思路：将“答案生成”与“答案聚合分析”解耦，为利用黑盒大模型（如调用API的商用大模型）提供了可行路径——只需获取其输出，用SSA做后处理即可，无需改动黑盒模型本身。  
2. 测试时缩放新范式：展示了“并行采样 + 针对性小模型聚合”在推理任务上的潜力，为后续优化测试时计算效率、平衡资源与性能提供了新方向。  
3. 强化学习应用启发：通过强化学习优化聚合模型（SSA）来提升最终答案精度，验证了在“输出分布层面做推理优化”的价值，可启发更多围绕模型输出后处理的研究。

