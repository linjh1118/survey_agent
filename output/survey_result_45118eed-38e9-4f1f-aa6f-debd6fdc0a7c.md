# Paper List of Terms(reward model)
- [25/06] **Generalist Reward Models: Found Inside Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.23235v1)] [[Code/Page]()] [[TLDR/Notes](#generalist-reward-models--found-inside-large-language-models)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **Listener-Rewarded Thinking in VLMs for Image Preferences**  
[[Paper](http://arxiv.org/pdf/2506.22832v1)] [[Code/Page](https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.)] [[TLDR/Notes](#listener-rewarded-thinking-in-vlms-for-image-preferences)]

- [25/06] **Lost at the Beginning of Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.22058v1)] [[Code/Page]()] [[TLDR/Notes](#lost-at-the-beginning-of-reasoning)]

- [25/06] **TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.22008v1)] [[Code/Page]()] [[TLDR/Notes](#trofi--trajectory-ranked-offline-inverse-reinforcement-learning)]

- [25/06] **PrefPaint: Enhancing Image Inpainting through Expert Human Feedback**  
[[Paper](http://arxiv.org/pdf/2506.21834v1)] [[Code/Page]()] [[TLDR/Notes](#prefpaint--enhancing-image-inpainting-through-expert-human-feedback)]

- [25/06] **SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2506.21669v1)] [[Code/Page]()] [[TLDR/Notes](#seea-r1--tree-structured-reinforcement-fine-tuning-for-self-evolving-embodied-agents)]

- [25/06] **Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents**  
[[Paper](http://arxiv.org/pdf/2506.21252v1)] [[Code/Page]()] [[TLDR/Notes](#agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents)]

- [25/06] **Off-Policy Evaluation and Learning for the Future under Non-Stationarity**  
[[Paper](http://arxiv.org/pdf/2506.20417v1)] [[Code/Page]()] [[TLDR/Notes](#off-policy-evaluation-and-learning-for-the-future-under-non-stationarity)]

- [25/06] **Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations**  
[[Paper](http://arxiv.org/pdf/2506.20294v1)] [[Code/Page]()] [[TLDR/Notes](#ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations)]

- [25/06] **Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference Alignment**  
[[Paper](http://arxiv.org/pdf/2506.19780v2)] [[Code/Page]()] [[TLDR/Notes](#multi-preference-lambda-weighted-listwise-dpo-for-dynamic-preference-alignment)]

- [25/06] **Inference-Time Reward Hacking in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.19248v1)] [[Code/Page]()] [[TLDR/Notes](#inference-time-reward-hacking-in-large-language-models)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.18841v1)] [[Code/Page](https://huggingface.co/THU-KEG/LongWriter-Zero-32B)] [[TLDR/Notes](#longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning)]

- [25/06] **RDPO: Real Data Preference Optimization for Physics Consistency Video Generation**  
[[Paper](http://arxiv.org/pdf/2506.18655v1)] [[Code/Page](https://wwenxu.github.io/RDPO/)] [[TLDR/Notes](#rdpo--real-data-preference-optimization-for-physics-consistency-video-generation)]

- [25/06] **Shrinking the Generation-Verification Gap with Weak Verifiers**  
[[Paper](http://arxiv.org/pdf/2506.18203v1)] [[Code/Page]()] [[TLDR/Notes](#shrinking-the-generation-verification-gap-with-weak-verifiers)]

- [25/06] **Reflective Verbal Reward Design for Pluralistic Alignment**  
[[Paper](http://arxiv.org/pdf/2506.17834v1)] [[Code/Page]()] [[TLDR/Notes](#reflective-verbal-reward-design-for-pluralistic-alignment)]

- [25/06] **DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.17533v1)] [[Code/Page]()] [[TLDR/Notes](#duashepherd--integrating-stepwise-correctness-and-potential-rewards-for-mathematical-reasoning)]

- [25/06] **Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models**  
[[Paper](http://arxiv.org/pdf/2506.16853v1)] [[Code/Page](https://github.com/seminkim/RATTPO.)] [[TLDR/Notes](#reward-agnostic-prompt-optimization-for-text-to-image-diffusion-models)]

- [25/06] **ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2506.16712v1)] [[Code/Page]()] [[TLDR/Notes](#reasongrm--enhancing-generative-reward-models-through-large-reasoning-models)]

- [25/06] **Robust Reward Modeling via Causal Rubrics**  
[[Paper](http://arxiv.org/pdf/2506.16507v1)] [[Code/Page]()] [[TLDR/Notes](#robust-reward-modeling-via-causal-rubrics)]

- [25/06] **Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples**  
[[Paper](http://arxiv.org/pdf/2506.16502v1)] [[Code/Page]()] [[TLDR/Notes](#relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples)]

- [25/06] **GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks**  
[[Paper](http://arxiv.org/pdf/2506.16114v1)] [[Code/Page]()] [[TLDR/Notes](#gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks)]

- [25/06] **AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning**  
[[Paper](http://arxiv.org/pdf/2506.15651v1)] [[Code/Page](https://github.com/cxcscmu/AutoRule.)] [[TLDR/Notes](#autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning)]

- [25/06] **SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling**  
[[Paper](http://arxiv.org/pdf/2506.15498v1)] [[Code/Page]()] [[TLDR/Notes](#spare--single-pass-annotation-with-reference-guided-evaluation-for-automatic-process-supervision-and-reward-modelling)]

- [25/06] **Reward Models in Deep Reinforcement Learning: A Survey**  
[[Paper](http://arxiv.org/pdf/2506.15421v1)] [[Code/Page]()] [[TLDR/Notes](#reward-models-in-deep-reinforcement-learning--a-survey)]

- [25/06] **Adaptive Accompaniment with ReaLchords**  
[[Paper](http://arxiv.org/pdf/2506.14723v1)] [[Code/Page]()] [[TLDR/Notes](#adaptive-accompaniment-with-realchords)]

- [25/06] **SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.14648v1)] [[Code/Page]()] [[TLDR/Notes](#senior--efficient-query-selection-and-preference-guided-exploration-in-preference-based-reinforcement-learning)]

- [25/06] **TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization**  
[[Paper](http://arxiv.org/pdf/2506.14574v1)] [[Code/Page](https://github.com/dvlab-research/TGDPO.)] [[TLDR/Notes](#tgdpo--harnessing-token-level-reward-guidance-for-enhancing-direct-preference-optimization)]

- [25/06] **Adaptive Data Augmentation for Thompson Sampling**  
[[Paper](http://arxiv.org/pdf/2506.14479v1)] [[Code/Page]()] [[TLDR/Notes](#adaptive-data-augmentation-for-thompson-sampling)]

- [25/06] **GRAM: A Generative Foundation Reward Model for Reward Generalization**  
[[Paper](http://arxiv.org/pdf/2506.14175v2)] [[Code/Page]()] [[TLDR/Notes](#gram--a-generative-foundation-reward-model-for-reward-generalization)]

- [25/06] **VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training**  
[[Paper](http://arxiv.org/pdf/2506.13888v1)] [[Code/Page]()] [[TLDR/Notes](#vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.13741v1)] [[Code/Page]()] [[TLDR/Notes](#pb$^2$--preference-space-exploration-via-population-based-methods-in-preference-based-reinforcement-learning)]

- [25/06] **$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts**  
[[Paper](http://arxiv.org/pdf/2506.15733v1)] [[Code/Page]()] [[TLDR/Notes](#$\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts)]

- [25/06] **From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment**  
[[Paper](http://arxiv.org/pdf/2506.12446v2)] [[Code/Page]()] [[TLDR/Notes](#from-outcomes-to-processes--guiding-prm-learning-from-orm-for-inference-time-alignment)]

- [25/06] **Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory**  
[[Paper](http://arxiv.org/pdf/2506.12350v1)] [[Code/Page]()] [[TLDR/Notes](#theoretical-tensions-in-rlhf--reconciling-empirical-success-with-inconsistencies-in-social-choice-theory)]

- [25/06] **TreeRL: LLM Reinforcement Learning with On-Policy Tree Search**  
[[Paper](http://arxiv.org/pdf/2506.11902v1)] [[Code/Page](https://github.com/THUDM/TreeRL.)] [[TLDR/Notes](#treerl--llm-reinforcement-learning-with-on-policy-tree-search)]

- [25/06] **Personalized LLM Decoding via Contrasting Personal Preference**  
[[Paper](http://arxiv.org/pdf/2506.12109v1)] [[Code/Page]()] [[TLDR/Notes](#personalized-llm-decoding-via-contrasting-personal-preference)]

- [25/06] **Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11474v1)] [[Code/Page](https://med-prm.github.io/)] [[TLDR/Notes](#med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards)]

- [25/06] **Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11425v2)] [[Code/Page]()] [[TLDR/Notes](#agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards)]

- [25/06] **ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems**  
[[Paper](http://arxiv.org/pdf/2506.10955v1)] [[Code/Page]()] [[TLDR/Notes](#reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems)]

- [25/06] **EQA-RM: A Generative Embodied Reward Model with Test-time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.10389v1)] [[Code/Page](https://github.com/UNITES-Lab/EQA-RM.)] [[TLDR/Notes](#eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling)]

- [25/06] **Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.21560v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning)]

- [25/06] **Unsupervised Elicitation of Language Models**  
[[Paper](http://arxiv.org/pdf/2506.10139v1)] [[Code/Page]()] [[TLDR/Notes](#unsupervised-elicitation-of-language-models)]

- [25/06] **Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput**  
[[Paper](http://arxiv.org/pdf/2506.10056v1)] [[Code/Page]()] [[TLDR/Notes](#reward-models-enable-scalable-code-verification-by-trading-accuracy-for-throughput)]

- [25/06] **DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision**  
[[Paper](http://arxiv.org/pdf/2506.09814v1)] [[Code/Page]()] [[TLDR/Notes](#dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **Know What You Don't Know: Uncertainty Calibration of Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09338v1)] [[Code/Page]()] [[TLDR/Notes](#know-what-you-don-t-know--uncertainty-calibration-of-process-reward-models)]

- [25/06] **Learning to Reason Across Parallel Samples for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.09014v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-across-parallel-samples-for-llm-reasoning)]



# TLDR/Notes
## generalist-reward-models--found-inside-large-language-models
### Abstract
The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹ä¸­è—ç€é€šç”¨å¥–åŠ±æ¨¡å‹ï¼ŸLLMå¯¹é½æ–°èŒƒå¼æ¥äº†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»ä»·å€¼è§‚ï¼ˆå¦‚å¸®åŠ©æ€§ã€è¯šå®æ€§ï¼‰æ˜¯AIå‘å±•çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸»æµçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸¥é‡ä¾èµ–ç”¨æ˜‚è´µäººç±»åå¥½æ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ã€‚æ„å»ºä¼˜è´¨RMéœ€å¤§è§„æ¨¡é«˜è´¨é‡äººç±»åå¥½æ•°æ®é›†ï¼Œå­˜åœ¨æ…¢ã€è´µã€éš¾æ‰©å±•ç­‰é—®é¢˜ã€‚åç»­ç”¨AIåé¦ˆæ›¿ä»£äººç±»åé¦ˆçš„æ–¹æ³•ï¼ˆå¦‚RLAIFã€LLM-as-a-judgeï¼‰åˆç¼ºä¹ä¸¥è°¨ç†è®ºåŸºç¡€ï¼Œè¿˜æ˜“ç»§æ‰¿è£åˆ¤æ¨¡å‹çš„é£æ ¼åå·®ä¸åè§ã€‚é‚£ä¹ˆï¼Œé«˜è´¨é‡å¥–åŠ±ä¿¡å·æ˜¯å¦å¿…é¡»å¤–éƒ¨è·å–ï¼Ÿè¿™æˆä¸ºå…³é”®é—®é¢˜ï¼Œæœ¬æ–‡æ­£æ˜¯åŸºäºæ­¤å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‘ç°LLMä¸­ latent å­˜åœ¨é€šç”¨å¥–åŠ±æ¨¡å‹  
è®ºæ–‡å‘ç°ï¼Œä»»ä½•ç»æ ‡å‡†ä¸‹ä¸€ä¸ªtokené¢„æµ‹è®­ç»ƒçš„LLMä¸­ï¼Œå¤©ç„¶æ½œä¼ç€å¼ºå¤§çš„é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œå°†å…¶å‘½åä¸ºâ€œå†…ç”Ÿå¥–åŠ±ï¼ˆendogenous rewardï¼‰â€ã€‚æ— éœ€é¢å¤–è®­ç»ƒï¼Œå°±èƒ½ä»åŸºç¡€æ¨¡å‹ï¼ˆé¢„è®­ç»ƒæˆ–æœ‰ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼‰ä¸­ç›´æ¥æå–é«˜è´¨é‡å¥–åŠ±ä¿¡å·ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºå±‚é¢å»ºç«‹ä¸ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ çš„è”ç³»  
ä»ç†è®ºä¸Šè¯æ˜ï¼Œè¿™ç§å†…ç”Ÿå¥–åŠ±ç­‰ä»·äºé€šè¿‡ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰å­¦åˆ°çš„å¥–åŠ±å‡½æ•°ã€‚å…·ä½“è€Œè¨€ï¼ŒLLMçš„logitså¯ç›´æ¥è§£é‡Šä¸ºè½¯Qå‡½æ•°ï¼Œå€ŸåŠ©é€†è½¯Bellmanç®—å­èƒ½ä»ä¸­æ¢å¤å‡ºå¥–åŠ±å‡½æ•°ï¼Œä¸ºæå–å¥–åŠ±å‡½æ•°æä¾›äº†åŸç†æ€§æ–¹æ³•ï¼Œçªç ´äº†è¿‡å¾€å¯å‘å¼åšæ³•ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¯æ˜åŸºäºå†…ç”Ÿå¥–åŠ±çš„RLæœ‰æ•ˆæ€§  
è¯æ˜ç”¨è¯¥å†…ç”Ÿå¥–åŠ±è¿›è¡Œåç»­å¼ºåŒ–å­¦ä¹ åï¼Œå¾—åˆ°çš„ç­–ç•¥ç›¸æ¯”åŸºç¡€æ¨¡å‹æœ‰æ›´ä¼˜çš„è¯¯å·®ç•Œã€‚RLè¿‡ç¨‹èƒ½ä¿®æ­£æ ‡å‡†æ¨¡ä»¿å­¦ä¹ ï¼ˆä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼‰çš„å¤åˆè¯¯å·®ï¼ŒæŠŠæ€§èƒ½å·®è·ä»ä¸ä»»åŠ¡æ—¶é•¿ç›¸å…³çš„äºŒæ¬¡ä¾èµ–ï¼ˆO(HÂ²)ï¼‰é™åˆ°æ›´ä¼˜çš„çº¿æ€§ä¾èµ–ï¼ˆO(H)ï¼‰ã€‚è¿™æ˜¯é¦–æ¬¡ä»ç†è®ºä¸Šè¯æ˜LLMå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒéªŒè¯äº†ç†è®ºï¼šæå–å†…ç”Ÿå¥–åŠ±çš„æ–¹æ³•ä¸ä»…ä¼˜äºç°æœ‰LLM-as-a-judgeæ–¹æ³•ï¼Œè¿˜èƒ½è¶…è¶Šåœ¨æ˜‚è´µäººç±»æ ‡æ³¨æ•°æ®ä¸Šæ˜¾å¼è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡è¡¨æ˜å¥–åŠ±å»ºæ¨¡é˜¶æ®µå¯è¢«ä¸€ç§åŸç†æ€§æ–¹æ³•æ›¿ä»£â€”â€”æå–é¢„è®­ç»ƒé˜¶æ®µå·²æ•è·çš„çŸ¥è¯†ã€‚è¿™ä¸ºLLMå¯¹é½ä»¥åŠå¤šæ¨¡æ€æ¨¡å‹é¢†åŸŸï¼Œå¼€è¾Ÿäº†æ›´é«˜æ•ˆã€å¼ºå¤§ä¸”å¯æ‰©å±•çš„æ–°èŒƒå¼ï¼Œåç»­åœ¨æ¨¡å‹å¯¹é½ã€å¥–åŠ±å‡½æ•°è®¾è®¡ç­‰æ–¹å‘ï¼Œéƒ½å¯å€Ÿé‰´è¿™ç§â€œæŒ–æ˜æ¨¡å‹å†…åœ¨å·²æœ‰èƒ½åŠ›â€çš„æ€è·¯ï¼Œå‡å°‘å¯¹å¤–éƒ¨æ˜‚è´µæ•°æ®ä¸é¢å¤–è®­ç»ƒçš„ä¾èµ–ã€‚

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨çŸ¥è¯†å¢å¼ºæ ‘æœç´¢æ¨ç†åŠ©åŠ›å¤§æ¨¡å‹æ”»å…‹åˆ†å­ç»“æ„è§£æéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åˆ†å­ç»“æ„è§£ææ˜¯åŒ–å­¦å®éªŒåˆ†æé‡Œçš„å…³é”®ä»»åŠ¡ï¼Œè¦ä»æ ¸ç£ã€çº¢å¤–ç­‰å…‰è°±æ•°æ®æ¨å¯¼åˆ†å­ç»“æ„ï¼Œä¸“ä¸šäººå‘˜éƒ½å¾—èŠ±10 - 15åˆ†é’Ÿåˆ†æå•ä¸ªåˆ†å­ï¼Œæ‰€ä»¥ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–è§£æå¾ˆæœ‰å¿…è¦ã€‚ä½†LLMåœ¨è¿™ä»»åŠ¡ä¸Šæœ‰æŒ‘æˆ˜ï¼šä¸€æ˜¯å¯¹åŒ–å­¦åˆ†å­ç»“æ„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œåƒå™»å©è¿™ç±»ç‰¹æ®Šæ‚ç¯ç»“æ„ï¼ŒLLMå¸¸å› ç¼ºä¹å­ç»“æ„çŸ¥è¯†è¯¯åˆ¤ï¼›äºŒæ˜¯æ²¡æ³•å‡†ç¡®è¯„ä¼°å’Œä¿®æ­£æ¨ç†è¿‡ç¨‹ï¼Œæ ‘æœç´¢æ¨ç†éœ€è¦æœ‰æ•ˆè¯„ä¼°åé¦ˆï¼Œå¯LLMç¼ºé¢†åŸŸçŸ¥è¯†ï¼Œåšä¸å¥½ reward model è§’è‰²ã€‚äºæ˜¯è®ºæ–‡è¦è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæå‡LLMåœ¨åˆ†å­ç»“æ„è§£æçš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºK - MSEæ¡†æ¶  
æ„å»ºçŸ¥è¯†å¢å¼ºçš„åˆ†å­ç»“æ„è§£ææ¨ç†æ¡†æ¶K - MSEï¼ŒæŠŠè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºæ’ä»¶å®ç°æµ‹è¯•æ—¶çš„èƒ½åŠ›æ‰©å±•ï¼Œèƒ½é€‚é…ä»»æ„LLMã€‚å€ŸåŠ©MCTSå¹³è¡¡æ–°è§£æ¢ç´¢å’Œå·²æœ‰è§£åˆ©ç”¨ï¼Œè¿˜ç»“åˆSelf - Refineè®©LLMåŠæ—¶ä¼˜åŒ–ä¹‹å‰çš„è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“  
ä¸ºå¼¥è¡¥LLMåŒ–å­¦ç»“æ„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œæ„å»ºå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“ã€‚å­ç»“æ„æ˜¯åŒ–å­¦ç©ºé—´åŸºç¡€å…ƒç´ ï¼ŒçŸ¥è¯†åº“é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ•´åˆå­ç»“æ„å’Œç»“æ„æè¿°ï¼Œç»™LLMè¡¥å……é¢†åŸŸçŸ¥è¯†ï¼Œæå‡ç‰¹æ®Šç»“æ„æ¨ç†å‡†ç¡®æ€§ï¼Œå‡å°‘ atypical æ¡ˆä¾‹é”™è¯¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸“å±åˆ†å­ - å…‰è°±è¯„åˆ†å™¨  
è®¾è®¡åˆ†å­ - å…‰è°±è¯„åˆ†å™¨å½“ reward modelï¼Œè§£å†³LLMè§£è¯„ä¼°ä¸å‡†é—®é¢˜ã€‚è¯„åˆ†å™¨æœ‰åˆ†å­ç¼–ç å™¨å’Œå…‰è°±ç¼–ç å™¨ï¼Œè¯„ä¼°åˆ†å­ç»“æ„å’Œå…‰è°±æ•°æ®åŒ¹é…åº¦ç»™å¥–åŠ±åˆ†ã€‚å®ƒè¿˜ä½œä¸ºLLMå’ŒçŸ¥è¯†åº“é—´çš„æ£€ç´¢å™¨ï¼Œç”¨è¾“å…¥å…‰è°±æŸ¥æœ€ç›¸å…³å­ç»“æ„ï¼Œå‡å°‘å­ç»“æ„æ£€ç´¢è¯¯å·®ï¼Œå¢å¼ºæ¨ç†ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MolPuzzleåŸºå‡†æµ‹è¯•ä¸Šï¼ŒK - MSEæ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œå¯¹GPT - 4o - miniå’ŒGPT - 4oéƒ½å¸¦æ¥è¶…20%çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¡†æ¶åœ¨å¢å¼ºLLMåˆ†å­ç»“æ„è§£æèƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸçŸ¥è¯†å¢å¼ºæ€è·¯ï¼šé¢å¯¹ä¸“ä¸šé¢†åŸŸä»»åŠ¡ï¼ŒLLMé€šç”¨çŸ¥è¯†ä¸è¶³æ—¶ï¼Œæ„å»ºé¢†åŸŸå­ç»“æ„çŸ¥è¯†åº“è¡¥å……ï¼Œè¿™ç§â€œå¤–éƒ¨çŸ¥è¯† + LLMâ€æ¨¡å¼å¯å¤ç”¨åœ¨å…¶ä»–ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©ã€ææ–™ï¼‰ä»»åŠ¡ã€‚  
2. æ¨ç†è¿‡ç¨‹è¯„ä¼°ä¼˜åŒ–ï¼šè®¾è®¡é¢†åŸŸä¸“å±è¯„åˆ†å™¨åš reward modelï¼Œç»“åˆæ ‘æœç´¢æ¡†æ¶ä¼˜åŒ–æ¨ç†ï¼Œä¸ºéœ€è¦æ·±åº¦æ¨ç†ã€éœ€è¯„ä¼°åé¦ˆçš„å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è¯æ˜ã€ä»£ç è°ƒè¯•ï¼‰æä¾›äº†â€œè¯„åˆ†å™¨ + æ ‘æœç´¢â€çš„æ¨ç†å¢å¼ºèŒƒå¼ã€‚  
3. æ’ä»¶åŒ–æ¡†æ¶è®¾è®¡ï¼šK - MSEä½œä¸ºæ’ä»¶é€‚é…ä»»æ„LLMï¼Œè¿™ç§è§£è€¦å¼è®¾è®¡æ–¹ä¾¿æŠ€æœ¯è½åœ°ï¼Œä¸åŒåœºæ™¯ä¸‹å¯å¿«é€Ÿé›†æˆåˆ°ç°æœ‰LLMå·¥ä½œæµé‡Œï¼Œé™ä½æŠ€æœ¯è¿ç§»æˆæœ¬ã€‚

## listener-rewarded-thinking-in-vlms-for-image-preferences
### Abstract
Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨Listenerå¢å¼ºçš„RLï¼Œè®©è§†è§‰è¯­è¨€æ¨¡å‹æ›´æ‡‚äººç±»å›¾åƒåå¥½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç”Ÿæˆå¼å»ºæ¨¡é¢†åŸŸï¼Œè®©è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç²¾å‡†æ•æ‰äººç±»è§†è§‰åå¥½æ˜¯å…³é”®éš¾é¢˜ã€‚ç°æœ‰å¥–åŠ±æ¨¡å‹å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜“å¯¼è‡´æ¨¡å‹è®°å¿†è®­ç»ƒæ•°æ®ï¼Œè¿˜éœ€è¦å¤æ‚æ ‡æ³¨æµç¨‹ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é‡Œçš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰è™½èƒ½æå‡æ³›åŒ–æ€§ï¼Œä½†ç ”ç©¶å‘ç°åŸºäºRLçš„åå¥½æ¨ç†æ¨¡å‹å­˜åœ¨â€œå¬ä¼—åˆ†æ­§ï¼ˆlistener disagreementï¼‰â€é—®é¢˜ï¼šå½“æ¨¡å‹æ¨ç†è½¨è¿¹å’Œç‹¬ç«‹å†»ç»“çš„è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆâ€œlistenerâ€ï¼‰å¯¹åŒä¸€è¾“å‡ºçš„è¯„ä¼°çŸ›ç›¾æ—¶ï¼Œæ¨ç†å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è§£å†³è¿™ç§æ¨ç†çŸ›ç›¾ã€æå‡æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½åº¦æ˜¯æœ¬æ–‡åŠ¨æœºæ‰€åœ¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡è®­ç»ƒæ€ç»´é“¾é£æ ¼æ¨ç†æ¨¡å‹é¢„æµ‹äººç±»å¯¹ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„è§†è§‰åå¥½
ä»¥å¾€å·¥ä½œè¾ƒå°‘é’ˆå¯¹ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„äººç±»è§†è§‰åå¥½æ¥è®­ç»ƒè¿™ç§æ€ç»´é“¾æ¨ç†æ¨¡å‹ï¼Œæœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œä¸ºè§†è§‰åå¥½é¢„æµ‹æä¾›æ–°çš„æ¨¡å‹è®­ç»ƒæ€è·¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯†åˆ«å¹¶é‡åŒ–â€œå¬ä¼—åˆ†æ­§â€è¿™ä¸€RLè§†è§‰åå¥½å»ºæ¨¡çš„ä¸»è¦å¤±æ•ˆæ¨¡å¼
é€šè¿‡åˆ†æå‘ç°ï¼Œå½“æ¨¡å‹é¢„æµ‹å’Œç‹¬ç«‹â€œlistenerâ€é¢„æµ‹å·®å¼‚å¢å¤§æ—¶ï¼ŒVLMå‡†ç¡®ç‡æŒç»­ä¸‹é™ï¼Œå°†è¿™ç§ç°è±¡æ˜ç¡®ä¸ºå…³é”®é—®é¢˜å¹¶é‡åŒ–ï¼Œä¸ºåç»­è§£å†³æ–¹æ³•æä¾›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡é¢å‘GRPOçš„listener - shapedè½¯å¥–åŠ±æœºåˆ¶
å¼•å…¥å†»ç»“çš„VLMâ€œlistenerâ€ï¼Œè®©å…¶ç‹¬ç«‹é‡æ–°å¤„ç†æ¨ç†æ¨¡å‹ï¼ˆreasonerï¼‰çš„æ€ç»´é“¾ï¼ˆæ’é™¤æœ€ç»ˆç­”æ¡ˆtokenï¼‰ï¼Œè¾“å‡ºå¯¹æ­£ç¡®é€‰æ‹©çš„æ ¡å‡†ç½®ä¿¡åˆ†æ•°ï¼Œå¹¶æ•´åˆåˆ°RLå¥–åŠ±ä¿¡å·ä¸­ã€‚è¿™æ ·æ—¢æƒ©ç½šæ— æ³•è¯´æœç‹¬ç«‹æ¨¡å‹çš„è§£é‡Šï¼Œåˆæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨å°±èƒ½æä¾›å¯†é›†ã€æ•°æ®é«˜æ•ˆçš„ç›‘ç£ï¼Œè®©æ¨ç†æ¨¡å‹ä¸ä»…ç­”æ¡ˆæ­£ç¡®ï¼Œæ¨ç†è¿‡ç¨‹ä¹Ÿèƒ½è¢«â€œlistenerâ€è®¤å¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ImageRewardæµ‹è¯•é›†ä¸Šï¼Œæ–¹æ³•è¾¾åˆ°67.4%çš„å½“å‰æœ€ä¼˜å‡†ç¡®ç‡ï¼›åœ¨å¤§è§„æ¨¡ï¼ˆ120ä¸‡æŠ•ç¥¨ï¼‰çš„Rapidata - HSPåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¤§å¹…è¶…è¶Šå¼ºGRPOå’ŒSFTåŸºçº¿ï¼›åŒæ—¶å‡å°‘äº†æ¨ç†çŸ›ç›¾æƒ…å†µï¼Œä¸”åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå³ä¾¿ç”¨å°‘é‡åå¥½æ•°æ®è®­ç»ƒï¼Œä¹Ÿèƒ½è®©è¾“å‡ºæ›´æ ¡å‡†ã€OODé²æ£’æ€§æ›´å¼ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼Œåˆ©ç”¨ç‹¬ç«‹æ¨¡å‹æ„å»ºå¥–åŠ±æœºåˆ¶æ¥å¯¹é½æ¨ç†è½¨è¿¹å’Œæœ€ç»ˆå†³ç­–çš„æ€è·¯ï¼Œä¸ºè§£å†³æ¨¡å‹æ¨ç†ä¸€è‡´æ€§é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼›ä»åº”ç”¨è§’åº¦ï¼Œè¯æ˜listenerå¢å¼ºçš„RLæ˜¯VLMsä¸­åå¥½å¯¹é½çš„æœ‰æ•ˆå®ç”¨å·¥å…·ï¼Œä¸ºä¸‹ä¸€ä»£æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„åå¥½å¯¹é½è§£å†³æ–¹æ¡ˆï¼Œåœ¨å·¥ä¸šç•Œå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹åå¥½è°ƒä¼˜åœºæ™¯æœ‰å€Ÿé‰´ä»·å€¼ï¼›ä»é—®é¢˜å‘ç°è§’åº¦ï¼Œå¯¹â€œå¬ä¼—åˆ†æ­§â€è¿™ç§å¤±æ•ˆæ¨¡å¼çš„è¯†åˆ«å’Œé‡åŒ–ï¼Œè®©åç»­ç ”ç©¶è€…èƒ½æ›´å…³æ³¨æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## lost-at-the-beginning-of-reasoning
### Abstract
Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¨ç†ä¼Šå§‹çš„â€œè¿·å¤±â€ï¼šå¤§æ¨¡å‹é•¿é“¾æ¨ç†é¦–æ­¥å½±å“ä¸åº”å¯¹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è§£é¢˜ã€å¤šè·³é—®ç­”ï¼‰ä¸­å€ŸåŠ©é•¿é“¾æ€ç»´ï¼ˆlong - CoTï¼‰æ¨ç†å–å¾—æ˜¾è‘—è¿›å±•ï¼Œé•¿é“¾æ¨ç†åŒ…å«å›æº¯ã€è‡ªæˆ‘åæ€ä¸è‡ªæˆ‘ä¿®æ­£ç­‰æœºåˆ¶ã€‚ç„¶è€Œï¼Œå¤§æ¨¡å‹åœ¨é•¿é“¾æ¨ç†ä¸­çš„è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ä»æœªå……åˆ†æ¢ç´¢ï¼Œä¸”å­˜åœ¨â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆäº§ç”Ÿå†—ä½™æ¨ç†ï¼‰ç°è±¡ã€‚åŒæ—¶ï¼Œå—â€œä¸­é—´è¿·å¤±â€ï¼ˆlost - in - the - middleï¼‰ç°è±¡å¯å‘ï¼Œç ”ç©¶è€…æ€è€ƒé•¿é“¾æ¨ç†ä¸­æ˜¯å¦å­˜åœ¨ç±»ä¼¼ä½ç½®åå·®ï¼Œå³æ¨ç†æ­¥éª¤çš„ä½ç½®æ˜¯å¦å½±å“æœ€ç»ˆæ¨ç†ç»“æœã€‚æœ¬æ–‡èšç„¦é•¿é“¾æ¨ç†èµ·å§‹æ­¥éª¤ï¼Œæ¢ç©¶å…¶å¯¹æœ€ç»ˆé¢„æµ‹çš„å½±å“ï¼Œå¹¶è§£å†³ç›¸å…³é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºé•¿é“¾æ¨ç†é¦–æ­¥å…³é”®å½±å“  
é€šè¿‡å®éªŒè¡¨æ˜ï¼Œåœ¨é•¿é“¾æ¨ç†ä¸­ï¼Œ**ç¬¬ä¸€ä¸ªæ¨ç†æ­¥éª¤**å¯¹æœ€ç»ˆé¢„æµ‹æœ‰ç€æå¤§å½±å“ã€‚åˆå§‹æ­¥éª¤å¼•å…¥çš„é”™è¯¯ä¼šå¤§å¹…é™ä½åç»­æ¨ç†è´¨é‡ï¼Œåœ¨DeepSeek - R1å’ŒQwen3ç­‰å¼€æºæ¨ç†æ¨¡å‹å®¶æ—ä¸­å‡è§‚å¯Ÿåˆ°è¯¥ç°è±¡ã€‚ä¾‹å¦‚ï¼Œå½“é¦–æ­¥é”™è¯¯æ—¶ï¼Œæ¨¡å‹å¾—åˆ°é”™è¯¯æœ€ç»ˆç­”æ¡ˆçš„æ¦‚ç‡æ˜¾è‘—ä¸Šå‡ï¼ˆå‡†ç¡®ç‡ä¸‹é™40%ï¼‰ï¼Œå‡¸æ˜¾å½“å‰é•¿é“¾æ¨ç†å¤§æ¨¡å‹è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›çš„å±€é™ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆæ—©æœŸå‰ªæç®—æ³•  
æå‡ºåˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„é«˜æ•ˆé‡‡æ ·ç­–ç•¥ï¼ˆæ—©æœŸå‰ªæç®—æ³•ï¼‰ã€‚é€šè¿‡è¯„ä¼°ç¬¬ä¸€ä¸ªæ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œè¯†åˆ«å¹¶ä¿ç•™é«˜è´¨é‡é¦–æ­¥ã€èˆå¼ƒæ¬¡ä¼˜é¦–æ­¥ï¼Œä»…å¯¹æœ‰å‰æ™¯çš„æ¨ç†è·¯å¾„ç»§ç»­ç”Ÿæˆï¼Œåœ¨ä¸ç‰ºç‰²å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œæœ€å¤šé™ä½70%çš„æ¨ç†æˆæœ¬ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šLaBoRåŸºå‡†æµ‹è¯•é›†  
æ„å»ºä¸“é—¨ç”¨äºè¯„ä¼°æ¨¡å‹è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›çš„æ–°åŸºå‡†LaBoRã€‚è¯¥åŸºå‡†é€šè¿‡è‡ªåŠ¨æµç¨‹æ„å»ºï¼Œæ¯ä¸ªå®ä¾‹åŒ…å«é—®é¢˜ä¸æœ‰ç¼ºé™·çš„é¦–æ­¥æ¨ç†ï¼Œè¦æ±‚æ¨¡å‹æ£€æµ‹å¹¶ä¿®æ­£åˆå§‹é”™è¯¯ä»¥å¾—åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œä¸ºå¤§æ¨¡å‹é²æ£’æ¨ç†ç ”ç©¶æä¾›åŸºç¡€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- åœ¨æ¢ç©¶é¦–æ­¥å½±å“çš„å®éªŒä¸­ï¼Œä»¥AIME24å’ŒAIME25æ•°æ®é›†çš„é—®é¢˜ä¸ºå¯¹è±¡ï¼Œç”¨DS - R1 - Qwen - 7Bå’ŒQwen3 - 8Bç”Ÿæˆæ¨ç†è½¨è¿¹ï¼Œæµ‹é‡å„æ¨ç†æ­¥éª¤ä¸æœ€ç»ˆç»“è®ºçš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå‘ç°é¦–æ­¥ä¸æœ€ç»ˆç»“è®ºè¯­ä¹‰ç›¸ä¼¼åº¦æ›´é«˜ï¼ŒéªŒè¯é¦–æ­¥å¯¹æœ€ç»ˆç»“è®ºçš„å…³é”®å½±å“ã€‚  
- æ—©æœŸå‰ªæç®—æ³•å®éªŒä¸­ï¼Œåœ¨å››ä¸ªé•¿é“¾æ¨ç†å¤§æ¨¡å‹å’Œå››ä¸ªæ•°å­¦ã€ç§‘å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ–¹æ³•åœ¨ä¿æŒé¢„æµ‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæœ€å¤šé™ä½70%æ¨ç†é¢„ç®—ã€‚  
- LaBoRåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯„ä¼°å…«ä¸ªå¼€æºé•¿é“¾æ¨ç†å¤§æ¨¡å‹ï¼Œå‘ç°åˆå§‹æ­¥éª¤å«é”™è¯¯æ—¶æ€§èƒ½æœ€å¤šä¸‹é™11.4%ï¼Œä½“ç°æ¨¡å‹åœ¨ä»æ—©æœŸé”™è¯¯æ¢å¤æ–¹é¢çš„è„†å¼±æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- ç ”ç©¶è§†è§’ä¸Šï¼Œå¼€è¾Ÿé•¿é“¾æ¨ç†â€œèµ·å§‹æ­¥éª¤å½±å“â€è¿™ä¸€æ­¤å‰æœªå……åˆ†æ¢ç´¢çš„æ–¹å‘ï¼Œä¸ºç†è§£å¤§æ¨¡å‹æ¨ç†æœºåˆ¶æä¾›æ–°ç»´åº¦ï¼Œåç»­ç ”ç©¶å¯å›´ç»•æ¨ç†æ­¥éª¤ä½ç½®åå·®ç­‰æ·±å…¥ã€‚  
- æ–¹æ³•å±‚é¢ï¼Œæ—©æœŸå‰ªæç®—æ³•ä¸ºæå‡å¤§æ¨¡å‹æ¨ç†æ•ˆç‡æä¾›æ–°æ€è·¯ï¼Œåˆ©ç”¨é¦–æ­¥è´¨é‡è¯„ä¼°å‡å°‘å†—ä½™æ¨ç†ï¼Œåœ¨å®é™…éƒ¨ç½²å¤§æ¨¡å‹æ¨ç†ä»»åŠ¡æ—¶å¯å€Ÿé‰´ä»¥é™ä½æˆæœ¬ã€‚  
- åŸºå‡†æ„å»ºä¸Šï¼ŒLaBoRä¸ºè¯„ä¼°å¤§æ¨¡å‹è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›æä¾›ä¸“é—¨å·¥å…·ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸæ ‡å‡†åŒ–è¯„ä¼°ï¼Œåç»­å¯åŸºäºæ­¤åŸºå‡†å¼€å±•æ›´å¤šæ¨¡å‹ä¼˜åŒ–ä¸èƒ½åŠ›ç ”ç©¶ã€‚

## trofi--trajectory-ranked-offline-inverse-reinforcement-learning
### Abstract
In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TROFIï¼šæ— é¢„å®šä¹‰å¥–åŠ±ä¸‹çš„ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆORLï¼‰ä¸­ï¼Œæ™ºèƒ½ä½“éœ€ä¾èµ–å¸¦å¥–åŠ±æ ‡ç­¾çš„å­˜å‚¨è½¬ç§»æ•°æ®è®­ç»ƒï¼Œä½†åœ¨æ¸¸æˆå¼€å‘ç­‰å®é™…åœºæ™¯ä¸­ï¼Œå¥–åŠ±å‡½æ•°å¾€å¾€éš¾ä»¥è·å–æˆ–å®šä¹‰ã€‚åŒæ—¶ï¼Œç°æœ‰é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰å’Œæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ–¹æ³•å¤šå‡è®¾æ¼”ç¤ºè½¨è¿¹æœ€ä¼˜ï¼Œä¸”é€‚é…åœ¨çº¿åœºæ™¯ï¼Œæ— æ³•ç›´æ¥ç”¨äºå«éä¸“å®¶ã€æ— å¥–åŠ±æ•°æ®çš„ç¦»çº¿åœºæ™¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºTROFIæ–¹æ³•ï¼Œæ—¨åœ¨æ— é¢„å®šä¹‰å¥–åŠ±å‡½æ•°å’Œæœ€ä¼˜ä¸“å®¶æ¼”ç¤ºä¸‹å­¦ä¹ ç¦»çº¿ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼±ç›‘ç£å¥–åŠ±å­¦ä¹ ä¸ç­–ç•¥è®­ç»ƒæµç¨‹  
TROFIåˆ†ä¸‰æ­¥å®ç°ç¦»çº¿ç­–ç•¥å­¦ä¹ ï¼šé¦–å…ˆé€šè¿‡T - REXï¼ˆTrajectory - ranked Reward EXtrapolationï¼‰åˆ©ç”¨äººç±»åå¥½è¿›è¡Œå¼±ç›‘ç£å­¦ä¹ ï¼Œå¾—åˆ°å¥–åŠ±æ¨¡å‹\(\hat{r}_\theta\)ï¼›æ¥ç€ç”¨è¯¥å¥–åŠ±æ¨¡å‹ä¸ºåŸå§‹æ— å¥–åŠ±ç¦»çº¿æ•°æ®é›†æ‰“æ ‡ç­¾ï¼›æœ€ååŸºäºæ‰“æ ‡åçš„æ•°æ®é›†ï¼Œç”¨TD3 + BCï¼ˆTwin Delayed Deep Deterministic plus Behavioral Cloningï¼‰è®­ç»ƒå‚æ•°åŒ–ç­–ç•¥\(\pi_\theta\)ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— éœ€æœ€ä¼˜è½¨è¿¹çš„å¥–åŠ±å­¦ä¹   
åŒºåˆ«äºå¤šæ•°IRL/ILæ–¹æ³•å¯¹æœ€ä¼˜æ¼”ç¤ºçš„å‡è®¾ï¼ŒTROFIå€ŸåŠ©T - REXä»…éœ€è½¨è¿¹çš„å®šæ€§æ’åºä¿¡æ¯æ¥å­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚T - REXé€šè¿‡ç¥ç»ç½‘è·¯è¿‘ä¼¼çŠ¶æ€ä¸‹çš„å¥–åŠ±ï¼Œåˆ©ç”¨è½¨è¿¹æ’åçº¦æŸï¼ˆå¦‚\(\tau_i \prec \tau_j\)æ—¶ï¼Œè½¨è¿¹\(\tau_j\)çš„ç´¯è®¡å¥–åŠ±é¢„æµ‹å€¼å¤§äº\(\tau_i\)ï¼‰æ„å»ºæŸå¤±å‡½æ•°è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€çŸ¥æ™“çœŸå®å¥–åŠ±å‡½æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTROFIæŒç»­è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½åª²ç¾ä½¿ç”¨çœŸå®å¥–åŠ±è®­ç»ƒç­–ç•¥çš„æ•ˆæœï¼›åŒæ—¶åœ¨3Dæ¸¸æˆç¯å¢ƒä¸­éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¯¹å¥–åŠ±æ¨¡å‹çš„ç ”ç©¶è¡¨æ˜ï¼Œè®¾è®¡è‰¯å¥½ã€æ˜“å­¦ä¹ çš„å¥–åŠ±å‡½æ•°å¯¹ä¿è¯å€¼å‡½æ•°ä¸å®é™…æœªæ¥æŠ˜æ‰£å¥–åŠ±å¯¹é½è‡³å…³é‡è¦ï¼Œå‡¸æ˜¾å¥–åŠ±å‡½æ•°åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœºæ™¯çš„å…³é”®ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•å±‚é¢ï¼šä¸ºæ— å¥–åŠ±å‡½æ•°æˆ–éš¾è·å–æœ€ä¼˜æ¼”ç¤ºçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœºæ™¯æä¾›äº†å¼±ç›‘ç£è§£å†³æ–¹æ¡ˆï¼Œç»“åˆT - REXå’ŒTD3 + BCçš„æµç¨‹å¯å¤ç”¨è‡³ç±»ä¼¼æ•°æ®é©±åŠ¨ä¸”å¥–åŠ±ç¨€ç¼ºçš„ä»»åŠ¡ï¼ˆå¦‚æ¸¸æˆNPCè¡Œä¸ºå­¦ä¹ ã€å¤æ‚å·¥ä¸šåœºæ™¯ç­–ç•¥ä¼˜åŒ–ï¼‰ã€‚  
2. å®è·µå±‚é¢ï¼šå±•ç¤ºäº†æ¸¸æˆå¼€å‘è€…å¦‚ä½•é«˜æ•ˆåˆ©ç”¨å¤§è§„æ¨¡ç©å®¶æ— å¥–åŠ±æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡å¥–åŠ±å‡½æ•°å°±èƒ½è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä¸ºäº§ä¸šç•Œåˆ©ç”¨ç©å®¶è¡Œä¸ºæ•°æ®æä¾›æ–°æ€è·¯ã€‚  
3. ç†è®ºä¸åˆ†æå±‚é¢ï¼šå¼ºè°ƒäº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡çš„é‡è¦æ€§ï¼Œä¸ºåç»­ç ”ç©¶ä¸­å¥–åŠ±å‡½æ•°çš„æ„å»ºä¸ä¼˜åŒ–æä¾›äº†ç»éªŒæ€§æŒ‡å¯¼ï¼Œæç¤ºéœ€å…³æ³¨å¥–åŠ±å‡½æ•°çš„å¯å­¦ä¹ æ€§ä¸å’Œå®é™…ä»»åŠ¡ç›®æ ‡çš„å¯¹é½æ€§ã€‚

## prefpaint--enhancing-image-inpainting-through-expert-human-feedback
### Abstract
Inpainting, the process of filling missing or corrupted image parts, has
broad applications, including medical imaging. However, in specialized fields
like medical polyps imaging, where accuracy and reliability are critical,
inpainting models can generate inaccurate images, leading to significant errors
in medical diagnosis and treatment. To ensure reliability, medical images
should be annotated by experts like oncologists for effective model training.
We propose PrefPaint, an approach that incorporates human feedback into the
training process of Stable Diffusion Inpainting, bypassing the need for
computationally expensive reward models. In addition, we develop a web-based
interface streamlines training, fine-tuning, and inference. This interactive
interface provides a smooth and intuitive user experience, making it easier to
offer feedback and manage the fine-tuning process. User study on various
domains shows that PrefPaint outperforms existing methods, reducing visual
inconsistencies and improving image rendering, particularly in medical
contexts, where our model generates more realistic polyps images.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PrefPaintï¼šå€Ÿä¸“å®¶åé¦ˆä¹‹åŠ›ï¼Œé©æ–°å›¾åƒä¿®å¤ï¼ˆåŒ»ç–—åœºæ™¯å°¤ç”šï¼‰

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å›¾åƒä¿®å¤ï¼ˆInpaintingï¼‰åœ¨åŒ»ç–—æˆåƒç­‰é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œå¯å¡«è¡¥å›¾åƒç¼ºå¤±æˆ–æŸåéƒ¨åˆ†ã€‚ä½†åœ¨åŒ»ç–—æ¯è‚‰æˆåƒè¿™ç±»å¯¹å‡†ç¡®æ€§å’Œå¯é æ€§è¦æ±‚æé«˜çš„ä¸“ä¸šé¢†åŸŸï¼Œç°æœ‰ä¿®å¤æ¨¡å‹æ˜“ç”Ÿæˆä¸å‡†ç¡®å›¾åƒï¼Œç»™åŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å¸¦æ¥é‡å¤§è¯¯å·®ã€‚åŸå› åœ¨äºåŒ»ç–—å›¾åƒä¿®å¤éœ€æ”¾å°„ç§‘åŒ»ç”Ÿã€è‚¿ç˜¤å­¦å®¶ç­‰ä¸“å®¶æ ‡æ³¨æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¯ç°å®ä¸­ç¼ºä¹è¶³å¤Ÿå¸¦æ ‡æ³¨çš„æ¯è‚‰å›¾åƒï¼Œä¸”æ¯è‚‰å½¢çŠ¶ã€å¤§å°ã€å¤–è§‚å¤šå˜ï¼Œè®©æ¨¡å‹å­¦ä¹ æ˜“å‡ºç°é”™è¯¯æ¨¡å¼ã€‚åŒæ—¶ï¼Œä¼ ç»ŸåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰éœ€è®­ç»ƒæ˜‚è´µå¥–åŠ±æ¨¡å‹ï¼Œè€—æ—¶è€—åŠ›ã€‚å› æ­¤ï¼Œè®ºæ–‡å¸Œæœ›å°†äººç±»ä¸“å®¶åé¦ˆèå…¥è®­ç»ƒï¼Œæå‡åŒ»ç–—ç­‰åœºæ™¯ä¸‹å›¾åƒä¿®å¤çš„å¯é æ€§ä¸å‡†ç¡®æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºPrefPaintæ–¹æ³•ï¼Œèåˆäººç±»åé¦ˆåˆ°Stable Diffusion Inpaintingè®­ç»ƒ  
å€ŸåŠ©Direct Preference Optimizationï¼ˆDPOï¼‰ï¼Œåœ¨å»å™ªè¿‡ç¨‹æ¯ä¸€æ­¥ç›´æ¥æ›´æ–°å‚æ•°ï¼Œç»•å¼€äº†è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å¥–åŠ±æ¨¡å‹ï¼ŒæŠŠåŒ»ç–—ä¸“å®¶ç­‰çš„çŸ¥è¯†ç›´æ¥çº³å…¥è®­ç»ƒï¼Œæå‡å›¾åƒç”Ÿæˆåœ¨åŒ»ç–—è¯Šæ–­ç­‰åœºæ™¯çš„ç²¾å‡†åº¦ä¸å¯é æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘æ˜“ç”¨çš„åŸºäºWebçš„äº¤äº’ç•Œé¢  
é¢å‘åŒ»ç–—ä¸“ä¸šäººå‘˜ï¼ˆå³ä¾¿ITèƒŒæ™¯ä¸å¼ºä¹Ÿèƒ½ä¸Šæ‰‹ï¼‰æ‰“é€ ç•Œé¢ï¼Œç®€åŒ–è®­ç»ƒã€å¾®è°ƒä¸æ¨ç†æµç¨‹ï¼Œèƒ½æ–¹ä¾¿æ”¶é›†ä¸“å®¶åé¦ˆï¼Œè¿˜å¯è·Ÿè¸ªæ¨¡å‹è¿›å±•ä¸æ€§èƒ½æå‡ï¼Œè®©åŒ»ç–—ä¸“å®¶èƒ½è½»æ¾å‚ä¸åˆ°æ¨¡å‹ä¼˜åŒ–ä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¤šé¢†åŸŸç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒPrefPaintè¶…è¶Šç°æœ‰æ–¹æ³•ï¼šåœ¨äººç‰©å›¾åƒä¿®å¤ä¸Šï¼Œå‡å°‘è§†è§‰ä¸ä¸€è‡´å¹¶æå‡é¢éƒ¨ç‰¹å¾æ¸²æŸ“ï¼›é£æ™¯å›¾åƒä¿®å¤ä¸­ï¼Œæ•´åˆæ€§æ›´å¥½ä¸”å‡å°‘ç±»ä¼¼è¾¹æ¡†çš„ä¸è‡ªç„¶æ„Ÿï¼›åŒ»ç–—é¢†åŸŸé‡Œï¼Œç”Ÿæˆçš„æ¯è‚‰å›¾åƒæ›´é€¼çœŸã€æ›´è´´åˆåŒ»ç–—åœºæ™¯ä¸Šä¸‹æ–‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸé€‚é…æ€è·¯ï¼šé’ˆå¯¹åŒ»ç–—è¿™ç±»ä¸“ä¸šä¸”æ•°æ®æ ‡æ³¨éš¾çš„é¢†åŸŸï¼Œå¼•å…¥äººç±»ä¸“å®¶åé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä¸ºå‚ç›´é¢†åŸŸAIæ¨¡å‹è®­ç»ƒæä¾›äº†â€œä¸“ä¸šçŸ¥è¯†æ³¨å…¥â€çš„å‚è€ƒè·¯å¾„ã€‚  
2. å·¥ç¨‹è½åœ°è§’åº¦ï¼šæ‰“é€ ä½é—¨æ§›äº¤äº’ç•Œé¢ï¼Œé™ä½ä¸“ä¸šäººå‘˜ä½¿ç”¨AIå·¥å…·çš„é—¨æ§›ï¼Œå¯ç¤ºåœ¨AIåº”ç”¨è½åœ°æ—¶è¦é‡è§†ç”¨æˆ·ä½“éªŒä¸å·¥å…·æ˜“ç”¨æ€§ï¼Œè®©é¢†åŸŸä¸“å®¶èƒ½é«˜æ•ˆå‚ä¸æ¨¡å‹è¿­ä»£ã€‚  
3. æŠ€æœ¯ä¼˜åŒ–æ–¹å‘ï¼šç”¨DPOç»•å¼€ä¼ ç»ŸRLHFå¥–åŠ±æ¨¡å‹çš„é«˜æˆæœ¬ï¼Œä¸º diffusion æ¨¡å‹è¿™ç±»ç”Ÿæˆå¼æ¨¡å‹çš„é«˜æ•ˆè°ƒä¼˜æä¾›äº†è½»é‡åŒ–æŠ€æœ¯è·¯çº¿å‚è€ƒã€‚

## seea-r1--tree-structured-reinforcement-fine-tuning-for-self-evolving-embodied-agents
### Abstract
Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SEEA-R1ï¼šè®©å…·èº«æ™ºèƒ½è‡ªä¸»è¿›åŒ–çš„æ ‘ç»“æ„å¼ºåŒ–å¾®è°ƒæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å…·èº«æ™ºèƒ½ä½“éœ€åœ¨å¤æ‚ã€é•¿æ—¶ç¨‹ç¯å¢ƒä¸­å®Œæˆä»»åŠ¡ï¼Œæ—¢è¦æœ‰åº•å±‚æ„ŸçŸ¥ä¸è¿åŠ¨æ§åˆ¶ï¼Œåˆå¾—å…·å¤‡é«˜å±‚æ¨ç†ã€è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æå‡äº†æ™ºèƒ½ä½“çš„æŠ½è±¡æ¨ç†ä¸æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†åœ¨å¼€æ”¾å¼å…·èº«åœºæ™¯ä¸­ä»æœ‰ä¸è¶³ï¼šLLMs ç¼ºä¹æ„ŸçŸ¥åŸºç¡€ï¼ŒMLLMs åœ¨ç»“æ„åŒ–å¤šæ­¥è§„åˆ’ã€é•¿æœŸè¿è´¯æ€§ä¸åŠ¨æ€ç¯å¢ƒé€‚åº”ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³é€šç”¨å…·èº«æ™ºèƒ½é—®é¢˜ï¼Œæ™ºèƒ½ä½“éœ€å…·å¤‡â€œè‡ªæˆ‘è¿›åŒ–â€èƒ½åŠ›â€”â€”è‡ªä¸»ç”Ÿæˆè®­ç»ƒä¿¡å·å¹¶é€šè¿‡é—­ç¯å­¦ä¹ ä¼˜åŒ–æ¨ç†ã€‚  

å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è™½ä¸ºæœ‰å‰æ™¯çš„èŒƒå¼ï¼Œä½†åœ¨å…·èº«åœºæ™¯ä¸‹é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼š  
1. **å¤šæ­¥æ¨ç†ä»»åŠ¡ç¼ºä¸­é—´åé¦ˆ**ï¼šé•¿æ—¶ç¨‹ã€å»¶è¿Ÿç¨€ç–å¥–åŠ±çš„å…·èº«ä»»åŠ¡ä¸­ï¼Œéš¾å¯¹ä¸­é—´å†³ç­–è¿›è¡Œâ€œ credit assignmentï¼ˆ credit åˆ†é…ï¼‰â€ï¼Œå¯¼è‡´ RFT éš¾æœ‰æ•ˆå¼•å¯¼ç­–ç•¥å­¦ä¹ ï¼›  
2. **æ‰‹å·¥å¥–åŠ±å‡½æ•°æ³›åŒ–æ€§å·®**ï¼šç°æœ‰ RFT ä¾èµ–ç‰¹å®šæ¨¡æ‹Ÿå™¨æˆ–ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ï¼Œæ— æ³•æ³›åŒ–åˆ°æ–°ç¯å¢ƒï¼Œé˜»ç¢æ™ºèƒ½ä½“è·¨ä»»åŠ¡è‡ªæˆ‘æå‡ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ä¸ºåº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡º **SEEA-R1**ï¼ˆSelf-Evolving Embodied Agents - R1ï¼‰ï¼Œé¦–ä¸ªä¸ºå…·èº«æ™ºèƒ½ä½“â€œè‡ªæˆ‘è¿›åŒ–â€èƒ½åŠ›é‡èº«è®¾è®¡çš„ RFT æ¡†æ¶ï¼Œæ ¸å¿ƒåŒ…å«ä¸¤å¤§åˆ›æ–°ç»„ä»¶ï¼š  

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šTree-GRPOï¼ˆåŸºäºæ ‘ç»“æ„çš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰  
å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰èå…¥ Group Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼Œè®©æ™ºèƒ½ä½“å¯æ¢ç´¢å¤šæ¡å¯èƒ½è½¨è¿¹ï¼ŒæŠŠç¨€ç–çš„â€œç»“æœå¥–åŠ±â€è½¬åŒ–ä¸ºç»“æ„åŒ–ã€é€æ­¥çš„â€œè¿‡ç¨‹ä¿¡å·â€ã€‚è¿™ä¸€æ­¥è§£å†³äº†å¤šæ­¥è½¨è¿¹ä¸­ credit åˆ†é…éš¾é¢˜ï¼ŒåŠ©åŠ›é•¿åºåˆ—åŠ¨ä½œçš„æ¨ç†ä¸å†³ç­–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMGRMï¼ˆå¤šæ¨¡æ€ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼‰  
ä¸ºæ‘†è„±å¯¹æ‰‹å·¥æˆ–ç¯å¢ƒç‰¹å®šå¥–åŠ±å‡½æ•°çš„ä¾èµ–ï¼ŒMGRM ä»å¤šæ¨¡æ€ã€å¤šè½®æ¬¡çš„è½¨è¿¹æ•°æ®ä¸­è®­ç»ƒè€Œæ¥ï¼Œæä¾›â€œä¸ä»»åŠ¡æ— å…³â€çš„å¥–åŠ±ä¼°è®¡ï¼Œæ”¯æ’‘æ™ºèƒ½ä½“åœ¨å¤šæ ·å…·èº«åœºæ™¯ä¸‹çš„æ³›åŒ–ä¸è‡ªæˆ‘æå‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ **ALFWorld åŸºå‡†æµ‹è¯•**ï¼ˆè€ƒéªŒæ™ºèƒ½ä½“å°†æŠ½è±¡ç›®æ ‡æ˜ å°„ä¸ºè§†è§‰é”šå®šåŠ¨ä½œåºåˆ—çš„èƒ½åŠ›ï¼‰ä¸ŠéªŒè¯ SEEA-R1ï¼š  
- æ–‡æœ¬ä»»åŠ¡ä¸å¤šæ¨¡æ€ä»»åŠ¡åˆ†åˆ«å–å¾— **85.07%** å’Œ **36.19%** çš„æˆåŠŸç‡ï¼Œè¶…è¶Š Qwen2.5-VLã€GPT-4o ç­‰å‰æ²¿æ¨¡å‹ï¼›  
- ç”¨ MGRM è‡ªç›‘ç£ä¿¡å·æ›¿ä»£çœŸå®ç¯å¢ƒå¥–åŠ±æ—¶ï¼Œæ–‡æœ¬ä¸å¤šæ¨¡æ€ä»»åŠ¡ä»è¾¾ **80.30%** å’Œ **23.88%**ï¼Œè¶…è¿‡æ‰€æœ‰å¼€æºåŸºçº¿ã€‚  
è¿™äº›ç»“æœè¯æ˜ SEEA-R1 ä½œä¸ºâ€œè‡ªè¿›åŒ–å…·èº«æ™ºèƒ½ä½“â€çš„æœ‰æ•ˆæ€§ä¸æ‰©å±•æ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ¡†æ¶åˆ›æ–°æ€§**ï¼šé¦–æ¬¡æå‡ºé¢å‘å…·èº«æ™ºèƒ½ä½“â€œè‡ªæˆ‘è¿›åŒ–â€çš„ RFT æ¡†æ¶ï¼Œä¸ºå…·èº«æ™ºèƒ½é•¿æœŸç ”ç©¶æä¾›æ–°èŒƒå¼ï¼›  
2. **æŠ€æœ¯èåˆ**ï¼šTree-GRPO ç»“åˆ MCTS ä¸ç­–ç•¥ä¼˜åŒ–ï¼Œè§£å†³å¤šæ­¥æ¨ç† credit åˆ†é…ï¼›MGRM ç”¨ç”Ÿæˆå¼æ¨¡å‹æ›¿ä»£æ‰‹å·¥å¥–åŠ±ï¼Œæ¨åŠ¨æ³›åŒ–èƒ½åŠ›ï¼›  
3. **å¼€æºä¸ç”Ÿæ€**ï¼šå›¢é˜Ÿè®¡åˆ’å¼€æºå®Œæ•´æ¡†æ¶ä¸æ¨¡å—ï¼ˆå« MGRMã€è®­ç»ƒç®¡çº¿ç­‰ï¼‰ï¼ŒåŠ©åŠ›å…·èº«æ™ºèƒ½ç¤¾åŒºç ”ç©¶ï¼›  
4. **å®éªŒè®¾è®¡**ï¼šåœ¨ ALFWorld ä¸ŠåŒæ—¶éªŒè¯â€œæœ‰ç¯å¢ƒå¥–åŠ±â€ä¸â€œè‡ªç›‘ç£å¥–åŠ±â€åœºæ™¯ï¼Œå…¨é¢å±•ç°è‡ªè¿›åŒ–æ½œåŠ›ã€‚  

SEEA-R1 ä¸ºå…·èº«æ™ºèƒ½çš„â€œè‡ªä¸»è¿›åŒ–â€æŒ‡æ˜äº†æ–°æ–¹å‘ï¼Œä»æŠ€æœ¯åˆ›æ–°åˆ°å¼€æºç”Ÿæ€ï¼Œéƒ½ä¸ºè¯¥é¢†åŸŸåç»­ç ”ç©¶é“ºå¥½äº†è·¯ã€‚

## agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents
### Abstract
As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-RewardBenchï¼šé¢å‘çœŸå®ä¸–ç•Œå¤šæ¨¡æ€æ™ºèƒ½ä½“çš„å¥–åŠ±å»ºæ¨¡ç»Ÿä¸€åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨ç½‘é¡µå¯¼èˆªã€å…·èº«æ™ºèƒ½ç­‰çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ä½†è¿™äº›æ™ºèƒ½ä½“å› ç¼ºä¹å¤–éƒ¨åé¦ˆï¼Œåœ¨è‡ªæˆ‘ä¿®æ­£ä¸æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ã€‚åˆ©ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºå¤–éƒ¨åé¦ˆæ˜¯æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œç„¶è€Œç›®å‰ç¼ºä¹é’ˆå¯¹æ™ºèƒ½ä½“å¥–åŠ±æ¨¡å‹é€‰æ‹©çš„æ˜ç¡®æŒ‡å¯¼ï¼Œä¹Ÿç¼ºå°‘ä¸“é—¨é¢å‘æ™ºèƒ½ä½“çš„å¥–åŠ±åŸºå‡†ã€‚å› æ­¤ï¼Œæ„å»ºé’ˆå¯¹æ™ºèƒ½ä½“çš„å¥–åŠ±åŸºå‡†è¿«åœ¨çœ‰ç«ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç»´åº¦ä¸çœŸå®åœºæ™¯è¦†ç›–  
æå‡ºçš„Agent - RewardBenchåŸºå‡†æ¶µç›–æ„ŸçŸ¥ã€è§„åˆ’ã€å®‰å…¨3ä¸ªè¯„ä¼°ç»´åº¦ä¸7ä¸ªçœŸå®ä¸–ç•Œæ™ºèƒ½ä½“åœºæ™¯ï¼ˆå¦‚ç§»åŠ¨ã€ç½‘é¡µã€è‡ªåŠ¨é©¾é©¶ã€Minecraftç­‰ï¼‰ã€‚åœ¨æ„ŸçŸ¥ç»´åº¦è¯„ä¼°è§†è§‰ç†è§£ä¸ grounding çš„å¥–åŠ±èƒ½åŠ›ï¼›è§„åˆ’ç»´åº¦èšç„¦å¥–åŠ±æ¨¡å‹å¯¹åºåˆ—å†³ç­–å’Œä»»åŠ¡åˆ†è§£çš„è¯„ä¼°èƒ½åŠ›ï¼›å®‰å…¨ç»´åº¦è€ƒå¯Ÿåœ¨æ”»å‡»å’Œä¸å®‰å…¨ç¯å¢ƒåœºæ™¯ä¸‹çš„å¥–åŠ±èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ­¥éª¤çº§å¥–åŠ±è¯„ä¼°  
ä¸åŒäºä»…å…³æ³¨æœ€ç»ˆç»“æœçš„è¯„ä¼°ï¼Œè¯¥åŸºå‡†æ”¯æŒåœ¨ä»»åŠ¡çš„å•ä¸ªæ­¥éª¤å±‚é¢è¯„ä¼°æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œä¸ºè§„åˆ’è¿‡ç¨‹ä¸­çš„æ€§èƒ½æä¾›æ›´ç»†è‡´çš„è§†è§’ï¼Œèƒ½æ›´è¯¦ç»†åœ°åé¦ˆæ¨¡å‹åœ¨æ¯ä¸€æ­¥çš„å¥–åŠ±èƒ½åŠ›è¡¨ç°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šéš¾åº¦é€‚é…ä¸é«˜è´¨é‡æ•°æ®ä¿éšœ  
é€šè¿‡ä¸‰ç§ç­–ç•¥ä¿éšœéš¾åº¦ä¸æ•°æ®è´¨é‡ï¼šä»10ç§ä¸åŒçš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆæ¶µç›–é»‘ç›’ä¸ç™½ç›’æ¨¡å‹ï¼‰é‡‡æ ·ä»¥ä¿è¯å¤šæ ·æ€§ï¼›ç”¨å°æ¨¡å‹è¿‡æ»¤æ•°æ®æ¥æ§åˆ¶ä»»åŠ¡éš¾åº¦ï¼Œé¿å…è¿‡æ˜“æˆ–è¿‡éš¾ï¼›äººå·¥éªŒè¯æ•°æ®ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§ä¸é«˜è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ç°æœ‰å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šè¡¨ç°æœ‰é™ï¼šé»‘ç›’æ¨¡å‹gemini - 1.5 - proå‡†ç¡®ç‡ä»…61.6%ï¼ŒGPT - 4oä¸º61.4%ï¼ŒClaude - 3.5 - Sonnetä¸º57.9%ï¼Œä½“ç°åŸºå‡†æŒ‘æˆ˜æ€§ï¼›åƒGPT - 4oè¿™æ ·è¾ƒå¼ºçš„æ¨¡å‹åœ¨å®‰å…¨å¥–åŠ±å»ºæ¨¡ä¸Šå‡†ç¡®ç‡ä»…39.2%ï¼Œè¯´æ˜æ™ºèƒ½ä½“é¢†åŸŸå®‰å…¨å¥–åŠ±å»ºæ¨¡ä»ä¸è¶³ï¼›å¼€æºæ¨¡å‹å¦‚Llama - 3.2 - 11B - Vision - Instructåœ¨æ„ŸçŸ¥å’Œè§„åˆ’ä¸Šåˆ†åˆ«ä»…å¾—53.5%å’Œ50.6%ï¼Œå‡¸æ˜¾æ™ºèƒ½ä½“å¥–åŠ±æ¨¡å‹ä¸“é¡¹è®­ç»ƒçš„å¿…è¦æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¡«è¡¥é¢†åŸŸç©ºç™½ï¼šAgent - RewardBenchæ˜¯é¦–ä¸ªè¯„ä¼°å¤šæ­¥éª¤æ™ºèƒ½ä½“ä»»åŠ¡ä¸­æ¨¡å‹å¥–åŠ±å»ºæ¨¡èƒ½åŠ›çš„åŸºå‡†ï¼Œä¸ºä»æ¨¡ä»¿å­¦ä¹ å‘å¸¦åé¦ˆå­¦ä¹ çš„è¿‡æ¸¡æä¾›å…³é”®è¯„ä¼°æ‰‹æ®µã€‚ 
2. ä¸°å¯Œè¯„ä¼°ç»´åº¦ä¸åœºæ™¯ï¼šæ¶µç›–å¤šç»´åº¦ã€å¤šçœŸå®åœºæ™¯ä¸”é‡‡ç”¨å¤šæ¨¡å‹çœŸå®æ ·æœ¬ä¸äººå·¥éªŒè¯ä¿éšœæ•°æ®è´¨é‡ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹è¯„ä¼°æä¾›äº†å…¨é¢ä¸”é«˜è´¨é‡çš„å‚è€ƒèŒƒå¼ã€‚ 
3. æŒ‡å¯¼ä¸‹æ¸¸ä»»åŠ¡ï¼šå±•ç°å‡ºä¸ä¸‹æ¸¸ä»»åŠ¡çš„å¼ºå…³è”æ€§ï¼Œè¯´æ˜å‡†ç¡®çš„å¥–åŠ±å»ºæ¨¡å¯¹æå‡å®é™…åº”ç”¨ä¸­æœç´¢æ€§èƒ½è‡³å…³é‡è¦ï¼Œä¸ºåç»­ä¼˜åŒ–æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ˜æ–¹å‘ã€‚

## off-policy-evaluation-and-learning-for-the-future-under-non-stationarity
### Abstract
We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.
### ğŸŒŸ è®ºæ–‡è§£è¯» | éå¹³ç¨³ç¯å¢ƒä¸‹é¢å‘æœªæ¥çš„ç¦»ç­–ç•¥è¯„ä¼°ä¸å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ¨èç³»ç»Ÿã€ç²¾å‡†åŒ»ç–—ç­‰å†³ç­–é—®é¢˜ä¸­ï¼Œç¦»ç­–ç•¥è¯„ä¼°ï¼ˆOPEï¼‰ç”¨äºåˆ©ç”¨å†å²æ—¥å¿—æ•°æ®è¯„ä¼°æ–°ç­–ç•¥æ•ˆæœï¼Œä½†å¤šæ•°å®é™…åœºæ™¯å¤„äº**éå¹³ç¨³ç¯å¢ƒ**ï¼ˆå¦‚ç”µå•†æ¨èä¸­ç”¨æˆ·åå¥½ã€å¥–åŠ±éšæ—¶é—´å˜åŒ–ï¼‰ã€‚ç°æœ‰æ–¹æ³•å‡è®¾ç¯å¢ƒå¹³ç¨³æˆ–ä¾èµ–ä¸¥æ ¼å¥–åŠ±å»ºæ¨¡å‡è®¾ï¼Œéš¾ä»¥å‡†ç¡®ä¼°è®¡æœªæ¥ç­–ç•¥ä»·å€¼â€”â€”å› ä¸ºå†å²æ•°æ®ä¸­æ²¡æœ‰æœªæ¥ç¯å¢ƒçš„è§‚æµ‹ï¼Œç›´æ¥å¥—ç”¨ä¼ ç»ŸOPEæ–¹æ³•ä¼šå¼•å…¥æ˜¾è‘—åå·®ã€‚ä¾‹å¦‚ç”µå•†æ¨èéœ€ç”¨ä¸Šä¸ªæœˆæ—§ç­–ç•¥æ•°æ®ä¼°è®¡ä¸‹ä¸ªæœˆæ–°ç­–ç•¥ä»·å€¼ï¼Œè€Œç”¨æˆ·è¡Œä¸ºçš„å‘¨åº¦ã€å­£èŠ‚ç­‰æ—¶é—´æ¨¡å¼åœ¨å†å²ä¸æœªæ¥å­˜åœ¨ä¸€è‡´æ€§å´æœªè¢«å……åˆ†åˆ©ç”¨ï¼Œè¿™æˆä¸ºå…³é”®ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰æœªæ¥ç¦»ç­–ç•¥è¯„ä¼°ï¼ˆF - OPEï¼‰é—®é¢˜å¹¶æå‡ºOPFV estimator  
é¦–æ¬¡å½¢å¼åŒ–**æœªæ¥ç¦»ç­–ç•¥è¯„ä¼°ï¼ˆF - OPEï¼‰**é—®é¢˜ï¼šåœ¨éå¹³ç¨³ç¯å¢ƒä¸‹ï¼Œä»…ç”¨å†å²æ—¥å¿—æ•°æ®ä¼°è®¡æœªæ¥ä»»æ„æ—¶é—´ç‚¹çš„ç­–ç•¥ä»·å€¼ã€‚æå‡º**Off - Policy Estimator for the Future Value (OPFV)**ï¼Œå…¶æ ¸å¿ƒæ˜¯åˆ©ç”¨æ—¶é—´åºåˆ—ä¸­ç¨³å®šçš„ç»“æ„ï¼ˆå¦‚å­£èŠ‚ã€å‘¨åº¦ã€èŠ‚å‡æ—¥æ•ˆåº”ï¼‰â€”â€”è¿™äº›æ•ˆåº”åœ¨å†å²å’Œæœªæ¥æ•°æ®ä¸­å…·æœ‰ä¸€è‡´æ€§ã€‚é€šè¿‡**æ–°å‹é‡è¦æ€§åŠ æƒ**ï¼ŒOPFVèƒ½æ— åä¼°è®¡æ—¶é—´åºåˆ—ç»“æ„åˆ»ç”»çš„æ•ˆåº”ï¼ˆç§°ä¸ºæ—¶é—´åºåˆ—ç‰¹å¾ï¼‰ï¼ŒåŒæ—¶ç”¨å›å½’æ¨¡å‹å¤„ç†å‰©ä½™æ•ˆåº”ä»¥å¹³è¡¡åå·®å’Œæ–¹å·®ã€‚ç†è®ºåˆ†æè¯æ˜äº†OPFVåœ¨ç‰¹å®šæ¡ä»¶ä¸‹å®ç°ä½åå·®ä¼°è®¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‰©å±•OPFVå®ç°æœªæ¥ç¦»ç­–ç•¥å­¦ä¹ ï¼ˆF - OPLï¼‰  
åŸºäºOPFV estimatorï¼Œè¿›ä¸€æ­¥æå‡º**æ–°çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•**ï¼Œä»…ç”¨å†å²æ•°æ®ä¸»åŠ¨å­¦ä¹ æœªæ¥çš„ä¼˜è´¨ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ‰©å±•OPFVä»¥ä¼°è®¡æœªæ¥ç­–ç•¥ä»·å€¼çš„ç­–ç•¥æ¢¯åº¦ï¼Œä»è€ŒæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œè§£å†³äº†éå¹³ç¨³ç¯å¢ƒä¸‹â€œç”¨å†å²æ•°æ®å­¦æœªæ¥ç­–ç•¥â€çš„éš¾é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é©±åŠ¨çš„æ—¶é—´åºåˆ—ç‰¹å¾é€‰æ‹©  
åˆ†ææ—¶é—´åºåˆ—ç‰¹å¾é€‰æ‹©å¯¹OPFVåå·® - æ–¹å·®æƒè¡¡çš„å½±å“ï¼Œæå‡º**ç®€å•çš„æ•°æ®é©±åŠ¨æµç¨‹**é€‰æ‹©æ—¶é—´åºåˆ—ç‰¹å¾ï¼Œä»¥ä¼˜åŒ–OPFVçš„ä¼°è®¡ç²¾åº¦ï¼Œè®©æ–¹æ³•æ›´å…·å®ç”¨æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åˆæˆæ•°æ®ä¸çœŸå®æ¨èç³»ç»Ÿéå¹³ç¨³æ•°æ®çš„å®éªŒä¸­ï¼ŒOPFVåœ¨**ä¼°è®¡æœªæ¥ç­–ç•¥ä»·å€¼**å’Œ**ä¼˜åŒ–æœªæ¥ç­–ç•¥**ä¸¤æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®éªŒè¦†ç›–å¤šç§éå¹³ç¨³åœºæ™¯ï¼ˆå¦‚åˆ†å¸ƒçš„å¹³æ»‘/çªå˜å˜åŒ–ï¼‰ï¼ŒéªŒè¯äº†OPFVåœ¨ä¸åŒè®¾å®šä¸‹çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰å±‚é¢ï¼šå°†â€œæ—¶é—´æˆ³â€ä½œä¸ºéšæœºå˜é‡çº³å…¥å»ºæ¨¡ï¼Œç»Ÿä¸€å¤„ç†å¹³æ»‘ä¸çªå˜çš„éå¹³ç¨³æ€§ï¼Œä¸ºéå¹³ç¨³ç¯å¢ƒä¸‹çš„ç­–ç•¥è¯„ä¼°ä¸å­¦ä¹ æä¾›äº†æ›´æ™®é€‚çš„æ¡†æ¶æ€è·¯ã€‚  
2. æ–¹æ³•è®¾è®¡å±‚é¢ï¼šæŒ–æ˜æ—¶é—´åºåˆ—ä¸­è·¨å†å² - æœªæ¥çš„ç¨³å®šç»“æ„ï¼ˆå¦‚å‘¨æœŸæ•ˆåº”ï¼‰ï¼Œå¹¶ç”¨æ–°å‹é‡è¦æ€§åŠ æƒå’Œå›å½’ç»“åˆçš„æ–¹å¼åˆ©ç”¨è¿™äº›ç»“æ„ï¼Œä¸ºâ€œæ— æœªæ¥è§‚æµ‹æ—¶ä¼°è®¡æœªæ¥ä»·å€¼â€æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯èŒƒå¼ã€‚  
3. è½åœ°å®è·µå±‚é¢ï¼šæ•°æ®é©±åŠ¨çš„ç‰¹å¾é€‰æ‹©æµç¨‹ä¸åŸºäºOPFVçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œä¸ºå·¥ä¸šç•Œï¼ˆå¦‚æ¨èç³»ç»Ÿã€å¹¿å‘ŠæŠ•æ”¾ï¼‰åœ¨éå¹³ç¨³åœºæ™¯ä¸‹å¿«é€Ÿè¿­ä»£ç­–ç•¥æä¾›äº†è½åœ°è·¯å¾„ï¼Œå‡å°‘åœ¨çº¿å®éªŒæˆæœ¬ä¸ä¼¦ç†é£é™©ã€‚

## ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations
### Abstract
Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´å±€éƒ¨æœ€ä¼˜ï¼Ctrl - Z Samplingï¼šå¯æ§éšæœºä¹‹å­—å½¢æ¢ç´¢çš„æ‰©æ•£é‡‡æ ·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ‰©æ•£æ¨¡å‹åœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ƒé€šè¿‡è¿­ä»£å»å™ªå°†é«˜æ–¯å™ªå£°é€æ­¥è½¬åŒ–ä¸ºç›®æ ‡æ•°æ®åˆ†å¸ƒã€‚ä½†ç”±äºæ½œåœ¨ç©ºé—´å¤æ‚å’Œåˆå§‹åŒ–æ¬ ä½³ï¼Œæ‰©æ•£æ¨¡å‹å¸¸æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜â€”â€”ç”Ÿæˆç»“æœå±€éƒ¨è§†è§‰è¿è´¯ï¼Œå´å…¨å±€ä¸ä¸€è‡´æˆ–æ¡ä»¶é”™ä½ã€‚ä»¥å¾€æ–¹æ³•å¦‚å¼ºåŒ–å¼•å¯¼ä¿¡å·ã€æ“çºµåˆå§‹å™ªå£°åˆ†å¸ƒç­‰ï¼Œåœ¨å¼•å¯¼æ§åˆ¶æˆ–é€ƒé€¸å±€éƒ¨æœ€ä¼˜çš„çµæ´»æ€§ä¸Šå­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆå¼•å¯¼æ•ˆæœä¸ä½³ï¼Œè¦ä¹ˆéœ€å¤§é‡å€™é€‰çŠ¶æ€è¯„ä¼°ï¼Œå®ç”¨æ€§å—é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½æœ‰æ•ˆæ£€æµ‹å¹¶é€ƒé€¸å±€éƒ¨æœ€ä¼˜çš„é‡‡æ ·ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå¯æ§éšæœºä¹‹å­—å½¢é‡‡æ ·ï¼ˆCtrl - Z Samplingï¼‰
è¯¥æ–¹æ³•å°†æ¡ä»¶æ‰©æ•£ç”Ÿæˆè§†ä¸ºâ€œçˆ¬å±±â€è¿‡ç¨‹ï¼ŒåŸºäºå¥–åŠ±æ¨¡å‹æ£€æµ‹æ½œåœ¨å±€éƒ¨æœ€ä¼˜ï¼ˆä¾æ®é¢„æµ‹åˆ†æ•°çš„åœæ»æƒ…å†µï¼‰ã€‚ä¸€æ—¦æ£€æµ‹åˆ°ï¼Œæ³¨å…¥å™ªå£°å¹¶å›é€€åˆ°æ›´æ—©ã€æ›´å…·å™ªå£°çš„æ—¶é—´æ­¥ï¼Œä»¥æ­¤é€ƒç¦»å½“å‰ä¼˜åŒ–å¹³å°ï¼Œé¼“åŠ±åœ¨æ½œåœ¨ç©ºé—´æ¢ç´¢æ›´ä¼˜åŒºåŸŸã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€å¹³è¡¡ backward exploration ä¸ forward refinement
å¥–åŠ±æ¨¡å‹è¯„ä¼°å€™é€‰çŠ¶æ€ï¼Œä»…æ¥å—èƒ½å¸¦æ¥æ›´ä¼˜æœªæ¥é¢„æµ‹çš„çŠ¶æ€ï¼›è‹¥é™„è¿‘æ— æ”¹è¿›ï¼Œä¼šç»§ç»­å›é€€åˆ°å™ªå£°æ›´å¤§çš„æ°´å¹³ä»¥å¢åŠ é€ƒé€¸å¯èƒ½æ€§ã€‚è¿™ç§å¯æ§éšæœºä¹‹å­—å½¢è¿‡ç¨‹å®ç°äº†å‰å‘ä¼˜åŒ–ä¸åå‘æ¢ç´¢çš„åŠ¨æ€äº¤æ›¿ï¼Œæå‡ç”Ÿæˆç»“æœçš„æ¡ä»¶å¯¹é½åº¦ä¸è§†è§‰è´¨é‡ï¼Œä¸”æ¨¡å‹æ— å…³ï¼Œé€‚é…ç°æœ‰æ‰©æ•£æ¡†æ¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCtrl - Z Sampling åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°ç”Ÿæˆè´¨é‡çš„æ˜¾è‘—æå‡ï¼Œä»…éœ€çº¦ 7.6 å€å‡½æ•°è¯„ä¼°ï¼ˆNFEsï¼‰å¢åŠ ã€‚è¿™è¡¨æ˜å…¶åœ¨æ¨ç†æ—¶èƒ½ä»¥é€‚åº¦å¼€é”€å¤§å¹…æ”¹è¿›ç”Ÿæˆæ•ˆæœï¼Œæ˜¯æ˜‚è´µæµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶é€‚ç”¨äºæœ¬åœ°æ¨ç†åœºæ™¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»â€œçˆ¬å±±â€è§†è§’åˆ†ææ‰©æ•£æ¨¡å‹æ¡ä»¶ç”Ÿæˆï¼Œä¸ºç†è§£æ¨¡å‹æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜æä¾›æ–°è§’åº¦ï¼Œå¯å‘åç»­å¯¹ç”Ÿæˆè¿‡ç¨‹åŠ¨åŠ›å­¦çš„ç ”ç©¶ã€‚
2. Ctrl - Z Sampling çš„è®¾è®¡æ€è·¯â€”â€”é€šè¿‡å¥–åŠ±å¼•å¯¼çš„éšæœºæ¢ç´¢ã€è‡ªé€‚åº”å™ªå£°æ³¨å…¥å®ç°å¯æ§é€ƒé€¸å±€éƒ¨æœ€ä¼˜ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–éœ€åœ¨å¤æ‚ç©ºé—´ä¸­ä¼˜åŒ–ã€æ˜“é™·å±€éƒ¨æœ€ä¼˜çš„ç”Ÿæˆæˆ–ä¼˜åŒ–ä»»åŠ¡ä¸­ï¼Œå¦‚å…¶ä»–ç±»å‹ç”Ÿæˆæ¨¡å‹ï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ç­‰ï¼‰çš„é‡‡æ ·ç­–ç•¥æ”¹è¿›ã€‚
3. æ¨¡å‹æ— å…³æ€§ä½¿å…¶èƒ½æ— ç¼èå…¥ç°æœ‰æ‰©æ•£æ¡†æ¶ï¼Œä¸ºå·¥ç¨‹å®è·µä¸­æå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡æä¾›å³æ’å³ç”¨çš„æ€è·¯ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œé™ä½åº”ç”¨æˆæœ¬ã€‚

## multi-preference-lambda-weighted-listwise-dpo-for-dynamic-preference-alignment
### Abstract
While large-scale unsupervised language models (LMs) capture broad world
knowledge and reasoning capabilities, steering their behavior toward desired
objectives remains challenging due to the lack of explicit supervision.
Existing alignment techniques, such as reinforcement learning from human
feedback (RLHF), rely on training a reward model and performing reinforcement
learning to align with human preferences. However, RLHF is often
computationally intensive, unstable, and sensitive to hyperparameters.
  To address these limitations, Direct Preference Optimization (DPO) was
introduced as a lightweight and stable alternative, enabling direct alignment
of language models with pairwise preference data via classification loss.
However, DPO and its extensions generally assume a single static preference
distribution, limiting flexibility in multi-objective or dynamic alignment
settings.
  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted
Listwise DPO, which extends DPO to incorporate multiple human preference
dimensions (e.g., helpfulness, harmlessness, informativeness) and enables
dynamic interpolation through a controllable simplex-weighted formulation. Our
method supports both listwise preference feedback and flexible alignment across
varying user intents without re-training. Empirical and theoretical analysis
demonstrates that our method is as effective as traditional DPO on static
objectives while offering greater generality and adaptability for real-world
deployment.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šåå¥½åŠ¨æ€å¯¹é½æ–°æ¡†æ¶ï¼šMulti - Preference Lambda - weighted Listwise DPO

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½å…·å¤‡å¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºæ— ç›‘ç£è®­ç»ƒç›®æ ‡æœªæ˜¾å¼ç¼–ç äººç±»åå¥½ï¼Œéš¾ä»¥ä¸äººç±»ä»·å€¼è§‚å’Œæ„å›¾å¯¹é½ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ–¹æ³•è™½èƒ½ä¸€å®šç¨‹åº¦è§£å†³å¯¹é½é—®é¢˜ï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€è®­ç»ƒä¸ç¨³å®šã€å¯¹è¶…å‚æ•°æ•æ„Ÿç­‰ç¼ºé™·ã€‚Direct Preference Optimizationï¼ˆDPOï¼‰ä½œä¸ºRLHFçš„è½»é‡æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½ç›´æ¥é€šè¿‡åˆ†ç±»æŸå¤±å¯¹é½æ¨¡å‹ä¸æˆå¯¹åå¥½æ•°æ®ï¼Œç„¶è€ŒDPOåŠå…¶æ‰©å±•é€šå¸¸å‡è®¾å•ä¸€é™æ€åå¥½åˆ†å¸ƒï¼Œåœ¨å¤šç›®æ ‡æˆ–åŠ¨æ€å¯¹é½åœºæ™¯ä¸‹çµæ´»æ€§å—é™ã€‚å®é™…åº”ç”¨ä¸­å¯¹é½å¸¸æ¶‰åŠå¤šä¸ªåå¥½ç»´åº¦ï¼ˆå¦‚å¸®åŠ©æ€§ã€æ— å®³æ€§ã€ä¿¡æ¯æ€§ç­‰ï¼‰ï¼Œä¸”ä¸åŒåœºæ™¯ä¸‹å„ç»´åº¦é‡è¦æ€§ä¸åŒï¼Œå› æ­¤éœ€è¦æ›´çµæ´»çš„æ¡†æ¶æ¥å¤„ç†å¤šåå¥½ç»´åº¦å¹¶é€‚åº”ä¸åŒåå¥½æƒè¡¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMulti - Preference Lambda - weighted Listwise DPOæ¡†æ¶  
è¯¥æ¡†æ¶æ‰©å±•äº†DPOï¼Œä½¿å…¶èƒ½çº³å…¥å¤šä¸ªäººç±»åå¥½ç»´åº¦ï¼ˆå¦‚å¸®åŠ©æ€§ã€æ— å®³æ€§ã€ä¿¡æ¯æ€§ç­‰ï¼‰ï¼Œå¹¶é€šè¿‡ä»æ¦‚ç‡å•çº¯å½¢ä¸­æŠ½å–çš„æƒé‡å‘é‡Î»å®ç°åŠ¨æ€æ’å€¼ã€‚å®ƒå°†DPOä»æˆå¯¹åå¥½åœºæ™¯æ‰©å±•åˆ°åˆ—è¡¨å¼ï¼ˆlistwiseï¼‰åå¥½åœºæ™¯ï¼Œæ”¯æŒåœ¨åŠ¨æ€å˜åŒ–çš„åå¥½é…ç½®ä¸‹è¿›è¡Œå¯¹é½ï¼Œæ¨ç†æ—¶ç”¨æˆ·æˆ–ä¸‹æ¸¸ç³»ç»Ÿå¯é€šè¿‡é€‰æ‹©ä¸åŒÎ»å‘é‡æ¥è°ƒèŠ‚å¯¹é½è¡Œä¸ºï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºæ¨å¯¼æ”¯æ’‘ä¸åˆ—è¡¨å¼æŸå¤±è®¾è®¡  
ä»KLæ­£åˆ™åŒ–çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡å‡ºå‘è¿›è¡Œè¯¦ç»†ç†è®ºæ¨å¯¼ï¼Œå¾—åˆ°èƒ½èšåˆå¤šä¸ªå¥–åŠ±ç»´åº¦çš„ç®€æ´åˆ—è¡¨å¼æŸå¤±ã€‚è¿™ç§æ–¹å¼ç›´æ¥åœ¨ç­–ç•¥ç©ºé—´ä¼˜åŒ–åˆ—è¡¨å¼æŸå¤±ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¾®è°ƒï¼Œç›¸æ¯”é€šè¿‡æœ‰ç›‘ç£å¥–åŠ±å»ºæ¨¡çš„æ–¹å¼æ›´å…·ä¼˜åŠ¿ï¼Œèƒ½æ›´å¥½åœ°åˆ©ç”¨åˆ—è¡¨å¼åå¥½åé¦ˆæ¥å­¦ä¹ æ¨¡å‹ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­é€šè¿‡åœ¨å¤šç›®æ ‡å¯¹é½åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šä¸ä¼ ç»ŸDPOæ•ˆæœç›¸å½“ï¼ŒåŒæ—¶åœ¨å®é™…éƒ¨ç½²ä¸­å…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§å’Œé€‚åº”æ€§ï¼Œåœ¨å¤šç›®æ ‡å¯¹é½ä»»åŠ¡ä¸­ç›¸æ¯”ä¼ ç»ŸDPOèƒ½å–å¾—æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜çš„ç»“æœã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šåå¥½ç»´åº¦å¤„ç†ï¼šä¸ºå¤„ç†å®é™…åº”ç”¨ä¸­å¤šç»´åº¦ä¸”åŠ¨æ€å˜åŒ–çš„äººç±»åå¥½æä¾›äº†æ–°æ€è·¯ï¼Œä¸å†å±€é™äºå•ä¸€é™æ€åå¥½åˆ†å¸ƒï¼Œèƒ½é€‚åº”ä¸åŒåœºæ™¯ä¸‹çš„åå¥½æƒè¡¡éœ€æ±‚ã€‚  
2. çµæ´»å¯¹é½ä¸æ¨ç†ï¼šå®ç°äº†æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹å³å¯åœ¨æ¨ç†æ—¶è°ƒèŠ‚å¯¹é½è¡Œä¸ºï¼Œä¸ºä¸‹æ¸¸åº”ç”¨æä¾›äº†æ›´çµæ´»çš„æ§åˆ¶æ–¹å¼ï¼Œé™ä½äº†åº”ç”¨æˆæœ¬ã€‚  
3. ç†è®ºä¸å®è·µç»“åˆï¼šä»ç†è®ºæ¨å¯¼å‡ºå‘è®¾è®¡æŸå¤±å‡½æ•°ï¼Œä¿è¯äº†æ–¹æ³•çš„åˆç†æ€§ï¼ŒåŒæ—¶åœ¨å®éªŒä¸­éªŒè¯äº†æœ‰æ•ˆæ€§ï¼Œè¿™ç§ä»ç†è®ºåˆ°å®è·µçš„æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œä¸ºåç»­ç›¸å…³æ–¹æ³•ç ”ç©¶æä¾›äº†èŒƒå¼å‚è€ƒã€‚

## inference-time-reward-hacking-in-large-language-models
### Abstract
A common paradigm to improve the performance of large language models is
optimizing for a reward model. Reward models assign a numerical score to LLM
outputs indicating, for example, which response would likely be preferred by a
user or is most aligned with safety goals. However, reward models are never
perfect. They inevitably function as proxies for complex desiderata such as
correctness, helpfulness, and safety. By overoptimizing for a misspecified
reward, we can subvert intended alignment goals and reduce overall performance
-- a phenomenon commonly referred to as reward hacking. In this work, we
characterize reward hacking in inference-time alignment and demonstrate when
and how we can mitigate it by hedging on the proxy reward. We study this
phenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we
introduce Best-of-Poisson (BoP) that provides an efficient, near-exact
approximation of the optimal reward-KL divergence policy at inference time. We
show that the characteristic pattern of hacking as observed in practice (where
the true reward first increases before declining) is an inevitable property of
a broad class of inference-time mechanisms, including BoN and BoP. To counter
this effect, hedging offers a tactical choice to avoid placing undue confidence
in high but potentially misleading proxy reward signals. We introduce
HedgeTune, an efficient algorithm to find the optimal inference-time parameter
and avoid reward hacking. We demonstrate through experiments that hedging
mitigates reward hacking and achieves superior distortion-reward tradeoffs with
minimal computational overhead.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ—¶çš„å¥–åŠ±é»‘å®¢æ”»å‡»ï¼šåˆ†æä¸ç¼“è§£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–ä¸­ï¼ŒåŸºäºå¥–åŠ±æ¨¡å‹çš„ä¼˜åŒ–æ˜¯æå‡æ€§èƒ½çš„å¸¸ç”¨èŒƒå¼ã€‚å¥–åŠ±æ¨¡å‹ä¼šç»™LLMè¾“å‡ºåˆ†é…æ•°å€¼åˆ†æ•°ï¼Œç”¨äºæŒ‡ç¤ºå¦‚ç”¨æˆ·åå¥½ã€å®‰å…¨ç›®æ ‡å¥‘åˆåº¦ç­‰ä¿¡æ¯ã€‚ä½†å¥–åŠ±æ¨¡å‹å¹¶éå®Œç¾ï¼Œå®ƒåªæ˜¯æ­£ç¡®æ€§ã€å¸®åŠ©æ€§ã€å®‰å…¨æ€§ç­‰å¤æ‚éœ€æ±‚çš„â€œä»£ç†â€ã€‚è¿‡åº¦ä¼˜åŒ–è¿™ç§â€œæŒ‡å®šä¸å½“â€çš„å¥–åŠ±ä¼šå¼•å‘â€œå¥–åŠ±é»‘å®¢æ”»å‡»â€ç°è±¡ï¼Œç ´ååŸæœ¬çš„å¯¹é½ç›®æ ‡å¹¶é™ä½æ•´ä½“æ€§èƒ½ã€‚å½“å‰æ¨ç†æ—¶å¯¹é½æ–¹æ³•ä¸­ï¼Œå¥–åŠ±é»‘å®¢æ”»å‡»åœ¨å®è·µé‡Œå·²æœ‰è§‚æµ‹ï¼Œä½†é’ˆå¯¹æ¨ç†æ—¶æ–¹æ³•çš„ç†è®ºåˆ†æä¸ç¼“è§£æ‰‹æ®µè¾ƒå°‘ï¼Œè¿™æˆä¸ºAIå¯¹é½é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨åˆ†ææ¨ç†æ—¶å¯¹é½ä¸­å¥–åŠ±é»‘å®¢æ”»å‡»ç°è±¡ï¼Œå¹¶æ¢ç´¢åˆ©ç”¨ä»£ç†å¥–åŠ±ä¿¡å·åŒæ—¶ç¼“è§£æ”»å‡»çš„æ–¹å¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ»ç”»æ¨ç†æ—¶å¥–åŠ±é»‘å®¢æ”»å‡»å¹¶æå‡ºé€šç”¨ç¼“è§£æ¡†æ¶  
ä»æ•°å­¦å±‚é¢åˆ»ç”»äº†æ¨ç†æ—¶å¥–åŠ±é»‘å®¢æ”»å‡»ç°è±¡ï¼ˆå¦‚å®šç†1ï¼‰ï¼Œæ˜ç¡®è¿™ç±»æ”»å‡»åœ¨åŒ…æ‹¬Best - of - nï¼ˆBoNï¼‰ã€æ–°æå‡ºçš„Best - of - Poissonï¼ˆBoPï¼‰ç­‰å¹¿æ³›æ¨ç†æ—¶æœºåˆ¶ä¸­ï¼Œå­˜åœ¨çœŸå®å¥–åŠ±å…ˆå‡åé™çš„å›ºæœ‰ç‰¹æ€§ã€‚åŒæ—¶ç»™å‡ºäº†é€šè¿‡â€œå¯¹å†²ï¼ˆhedgingï¼‰â€ä»£ç†å¥–åŠ±æ¥ç¼“è§£æ”»å‡»çš„é€šç”¨æ¡†æ¶ï¼Œé¿å…å¯¹é«˜ä½†å¯èƒ½è¯¯å¯¼çš„ä»£ç†å¥–åŠ±ä¿¡å·è¿‡åº¦ä¿¡ä»»ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºBest - of - Poissonï¼ˆBoPï¼‰æ¨ç†æ—¶å¯¹é½ç­–ç•¥  
BoPçš„æ ¸å¿ƒæ€è·¯æ˜¯ä¾æ®æ³Šæ¾åˆ†å¸ƒé€‰æ‹©BoNçš„é‡‡æ ·æ•°næ¥è¿è¡Œã€‚ç†è®ºè¯æ˜å…¶èƒ½åœ¨æ¨ç†æ—¶å®ç°è¿‘æœ€ä¼˜çš„å¥–åŠ± - å¤±çœŸæƒè¡¡ï¼Œä»…ç”¨å•ä¸ªå¯è°ƒå‚æ•°ï¼Œåœ¨å¥–åŠ±å‡åŒ€åˆ†å¸ƒæ—¶èƒ½ä»¥çº¦10â»Â³é‡çº§çš„KLå·®è·è¿‘ä¼¼æœ€ä¼˜ä»£ç†å¥–åŠ±å€¾æ–œè§£ï¼Œå¯åœ¨æ¨ç†æ—¶è¦†ç›–æ•´ä¸ªå¥–åŠ± - å¤±çœŸåŒºåŸŸï¼Œæˆä¸ºè®¡ç®—é«˜æ•ˆä¸”KL - å¥–åŠ±æƒè¡¡æŸå¤±å¯å¿½ç•¥çš„æœ€ä¼˜å€¾æ–œåˆ†å¸ƒæ›¿ä»£æ–¹æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºHedgeTuneç®—æ³•  
è¿™æ˜¯ç”¨äºåœ¨BoNã€Soft - Best - of - nï¼ˆSBoNï¼‰ã€BoPä¸­è°ƒèŠ‚å‚æ•°ä»¥å¯¹å†²å¥–åŠ±é»‘å®¢æ”»å‡»çš„é«˜æ•ˆç®—æ³•ï¼ˆå¦‚ç®—æ³•4ï¼‰ã€‚é€šè¿‡è¯¥ç®—æ³•èƒ½æ‰¾åˆ°æ¨ç†æ—¶æœ€ä¼˜å‚æ•°ï¼Œé¿å…å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œä¾‹å¦‚ä¸ºBoNå’ŒBoPæ‰¾åˆ°æœ€ä¼˜é‡‡æ ·æ•°nï¼Œä¸ºSBoNåœ¨å›ºå®šnæ—¶æ‰¾åˆ°æœ€å¤§åŒ–çœŸå®å¥–åŠ±çš„é€†æ¸©åº¦Î»ç­‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­é€šè¿‡å®éªŒå±•ç¤ºå¯¹å†²æ‰‹æ®µèƒ½ç¼“è§£å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œåœ¨è®¡ç®—å¼€é”€æå°çš„æƒ…å†µä¸‹å®ç°æ›´ä¼˜çš„å¤±çœŸ - å¥–åŠ±æƒè¡¡ã€‚å¦‚åœ¨å›¾1ç›¸å…³å®éªŒä¸­ï¼ŒHedgeTuneèƒ½ä¸ºBoNã€SBoNã€BoPä¸‰ç§æ¨ç†æ—¶æœºåˆ¶æˆåŠŸæ‰¾åˆ°â€œé»‘å®¢æ”»å‡»é˜ˆå€¼â€ï¼ŒBoNå’ŒBoPä¸‹èƒ½æ‰¾åˆ°æœ€ä¼˜é‡‡æ ·æ•°nï¼ŒSBoNä¸‹èƒ½æ‰¾åˆ°å¯¹åº”é€†æ¸©åº¦Î»ï¼ˆå½“æ— æ³•ç”¨Î»å®ç°é˜ˆå€¼æ—¶è¿”å›æœ€ä½³å¯è¾¾å¥–åŠ±ï¼‰ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨å¹³è¡¡å¥–åŠ±ä¸å‚è€ƒåˆ†å¸ƒKLæ•£åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹å¥–åŠ±æ¨¡å‹ä½œä¸ºâ€œä»£ç†â€å­˜åœ¨çš„å›ºæœ‰ç¼ºé™·åŠå¼•å‘çš„å¥–åŠ±é»‘å®¢æ”»å‡»é—®é¢˜ï¼Œæä¾›äº†ç†è®ºå±‚é¢çš„æ·±å…¥åˆ†æï¼Œä¸ºåç»­ç†è§£æ¨ç†æ—¶å¯¹é½æ–¹æ³•çš„é£é™©æä¾›äº†åŸºç¡€ã€‚
2. æå‡ºçš„BoPç­–ç•¥ä¸ºæ¨ç†æ—¶å¯¹é½æä¾›äº†æ–°çš„é«˜æ•ˆå®ç°æ–¹å¼ï¼Œåœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸å¥–åŠ± - KLæƒè¡¡ä¸Šæœ‰å‚è€ƒä»·å€¼ï¼Œå¯å¯å‘åç»­æ¨ç†æ—¶ä¼˜åŒ–ç­–ç•¥è®¾è®¡ã€‚
3. HedgeTuneç®—æ³•ç»™å‡ºäº†ä¸€ç§åœ¨å®é™…ä¸­è°ƒèŠ‚æ¨ç†æ—¶å¯¹é½æ–¹æ³•å‚æ•°ä»¥é¿å…è¿‡åº¦ä¼˜åŒ–ä»£ç†å¥–åŠ±çš„å¯è¡Œæ–¹æ¡ˆï¼Œä¸ºå·¥ç¨‹å®è·µä¸­éƒ¨ç½²LLMæ¨ç†æ—¶ä¼˜åŒ–æä¾›äº†å·¥å…·æ€è·¯ï¼Œèƒ½æŒ‡å¯¼å¦‚ä½•åœ¨åˆ©ç”¨ä»£ç†å¥–åŠ±ä¿¡å·å’Œé˜²æ­¢å¥–åŠ±é»‘å®¢æ”»å‡»é—´å–å¾—å¹³è¡¡ã€‚

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonFlux-PRMï¼šé¢å‘å¤§æ¨¡å‹é•¿æ€ç»´é“¾æ¨ç†çš„è½¨è¿¹æ„ŸçŸ¥å‹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ¨ç†åœºæ™¯ï¼ˆå¦‚æ•°å­¦è§£é¢˜ï¼‰ä¸­ï¼ŒProcess Reward Modelsï¼ˆPRMsï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰æ˜¯ç›‘ç£ä¸­é—´æ¨ç†æ­¥éª¤çš„æœ‰åŠ›å·¥å…·ã€‚ä¸è¿‡ç°æœ‰PRMså­˜åœ¨æ˜æ˜¾å±€é™ï¼šå®ƒä»¬ä¸»è¦åŸºäºæ¨¡å‹æœ€ç»ˆè¾“å‡ºè®­ç»ƒï¼Œéš¾ä»¥å¯¹**è½¨è¿¹ - å“åº”ï¼ˆtrajectory - responseï¼‰**è¿™ç±»æ–°å…´è¾“å‡ºå½¢å¼çš„ä¸­é—´æ¨ç†è½¨è¿¹è¿›è¡Œé²æ£’è¯„ä¼°ã€‚åƒDeepseek - R1ç­‰å‰æ²¿æ¨ç†æ¨¡å‹ä¼šç”Ÿæˆâ€œå†—é•¿ã€æ¬ è§„æ•´çš„ä¸­é—´æ€è€ƒè½¨è¿¹ + ç®€æ´æœ€ç»ˆå“åº”â€çš„è½¨è¿¹ - å“åº”å¯¹ï¼Œè¿™ç±»æ•°æ®å¸¸è¢«ç”¨äºå°æ¨¡å‹è’¸é¦ï¼Œä½†ç°æœ‰PRMså› ä¸ä¸­é—´è½¨è¿¹åœ¨ç»“æ„ã€æ ¼å¼ä¸Šä¸åŒ¹é…ï¼Œä¸”è®­ç»ƒæ—¶ç¼ºä¹å¸¦å¥–åŠ±çš„è½¨è¿¹ - å“åº”æ•°æ®ï¼Œåœ¨ç›‘ç£è¿™ç±»æ•°æ®æ—¶æ•ˆæœä¸ä½³ç”šè‡³ä¼šæŸå®³ä¸‹æ¸¸è®­ç»ƒã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®©PRMsæ—¢èƒ½ç›‘ç£æœ€ç»ˆå“åº”ï¼Œåˆèƒ½æœ‰æ•ˆè¯„ä¼°ä¸­é—´æ€è€ƒè½¨è¿¹ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè½¨è¿¹æ„ŸçŸ¥çš„PRMâ€”â€”ReasonFlux - PRM  
ReasonFlux - PRMä¸“ä¸ºè¯„ä¼°è½¨è¿¹ - å“åº”å‹æ¨ç†ç—•è¿¹è®¾è®¡ï¼Œèåˆäº†**æ­¥éª¤çº§ï¼ˆstep - levelï¼‰**å’Œ**è½¨è¿¹çº§ï¼ˆtrajectory - levelï¼‰**ç›‘ç£ã€‚å®ƒåœ¨æ¶µç›–æ•°å­¦å’Œç§‘å­¦æ¨ç†çš„10ké«˜è´¨é‡è½¨è¿¹ - å“åº”å¯¹ curated æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½ä¸ºæ€è€ƒè½¨è¿¹å†…çš„æ¯ä¸ªæ­¥éª¤æä¾›ç»†ç²’åº¦å¥–åŠ±ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®©æ¨¡å‹ä¸­é—´æ€è€ƒè½¨è¿¹ä¸æœ€ç»ˆå“åº”æ›´å¯¹é½ï¼Œè§£å†³äº†ç°æœ‰PRMså¯¹ä¸­é—´è½¨è¿¹ç›‘ç£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šåœºæ™¯é€‚é…çš„å¥–åŠ±ç›‘ç£  
ReasonFlux - PRMé€‚é…ç¦»çº¿å’Œåœ¨çº¿å¤šç§åœºæ™¯ï¼š  
- ç¦»çº¿åœºæ™¯ï¼šä¸ºè½¨è¿¹ - å“åº”å¯¹æ‰“åˆ†ï¼Œç­›é€‰é«˜è´¨é‡æ•°æ®ï¼ŒåŠ©åŠ›å°æ¨¡å‹ä¸‹æ¸¸æœ‰ç›‘ç£å¾®è°ƒçš„è®­ç»ƒæ•°æ®ç²¾é€‰ï¼›  
- åœ¨çº¿åœºæ™¯ï¼šèå…¥GRPOç­‰ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ç­–ç•¥ä¼˜åŒ–æä¾›ç»†ç²’åº¦è¿‡ç¨‹å¥–åŠ±ï¼›  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest - time scalingï¼‰ï¼šé€šè¿‡å¥–åŠ±å¼•å¯¼çš„Best - of - Nç­–ç•¥ï¼Œè¯„ä¼°å¤šä¸ªç”Ÿæˆå“åº”å¹¶é€‰æœ€ä¼˜ï¼Œæå‡æ¨ç†æ€§èƒ½ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AIMEã€MATH500ã€GPQA - Diamondç­‰æŒ‘æˆ˜æ€§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux - PRMå±•ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼š  
- æ•°æ®é€‰æ‹©æ–¹é¢ï¼šReasonFlux - PRM - 7Bæ¯”å¼ºåŸºçº¿ï¼ˆå¦‚Qwen2.5 - Math - PRM - 72Bï¼‰å’Œäººå·¥ç­–åˆ’åŸºçº¿é€‰å‡ºçš„æ•°æ®é›†è´¨é‡æ›´é«˜ï¼›  
- æ€§èƒ½æå‡æ–¹é¢ï¼šReasonFlux - PRM - 7Båœ¨æœ‰ç›‘ç£å¾®è°ƒä¸­å¹³å‡æå‡12.1%ï¼Œå¼ºåŒ–å­¦ä¹ ä¸­å¹³å‡æå‡4.5%ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ä¸­å¹³å‡æå‡6.3%ï¼›  
- èµ„æºå‹å¥½å‹å‘å¸ƒï¼šè¿˜å‘å¸ƒäº†ReasonFlux - PRM - 1.5Bï¼Œé€‚é…èµ„æºå—é™åœºæ™¯ä¸è¾¹ç¼˜éƒ¨ç½²ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰ä¸åˆ†æè§’åº¦ï¼šé’ˆå¯¹æ–°å…´çš„è½¨è¿¹ - å“åº”è’¸é¦æ•°æ®è¶‹åŠ¿ï¼Œæ·±å…¥åˆ†æç°æœ‰PRMsåœ¨ç›‘ç£ä¸­é—´è½¨è¿¹æ—¶çš„é—®é¢˜ï¼ˆç»“æ„æ ¼å¼ä¸åŒ¹é…ã€è®­ç»ƒæ•°æ®ç¼ºå¤±ï¼‰ï¼Œè¿™ç§ä»äº§ä¸šæ–°æ•°æ®å½¢æ€åæ¨æŠ€æœ¯ç—›ç‚¹çš„æ€è·¯ï¼Œä¸ºåç»­ç ”ç©¶é”šå®šæ–¹å‘æä¾›å‚è€ƒï¼›  
2. å¤šç²’åº¦ç›‘ç£èåˆï¼šå°†æ­¥éª¤çº§å’Œè½¨è¿¹çº§ç›‘ç£ç»“åˆï¼Œä¸ºå¤„ç†â€œé•¿é“¾æ¡ã€å¤šé˜¶æ®µâ€çš„æ¨ç†ç±»ä»»åŠ¡æä¾›äº†ç»†ç²’åº¦å¥–åŠ±è®¾è®¡çš„èŒƒä¾‹ï¼Œå¯è¿ç§»åˆ°ä»£ç ç”Ÿæˆã€å¤æ‚å†³ç­–ç­‰éœ€åˆ†æ­¥è¯„ä¼°çš„åœºæ™¯ï¼›  
3. å¤šåœºæ™¯å·¥ç¨‹è½åœ°ï¼šä»ç¦»çº¿æ•°æ®ç­›é€‰ã€åœ¨çº¿RLä¼˜åŒ–åˆ°æµ‹è¯•æ—¶å¢å¼ºï¼Œå®Œæ•´è¦†ç›–å¤§æ¨¡å‹è®­ç»ƒ - æ¨ç†å…¨æµç¨‹çš„å¥–åŠ±ç›‘ç£ï¼Œå±•ç¤ºäº†æŠ€æœ¯æ–¹æ¡ˆåœ¨äº§ä¸šçº§è½åœ°ä¸­çš„å¤šç»´åº¦ä»·å€¼ï¼Œä¸ºæ‰“é€ ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹æ¨ç†å¢å¼ºç®¡çº¿æä¾›äº†å®è·µæ¨¡æ¿ï¼›  
4. èµ„æºåˆ†å±‚å‘å¸ƒï¼šåŒæ—¶æä¾›7Bå’Œ1.5Bè§„æ¨¡æ¨¡å‹ï¼Œå…¼é¡¾é«˜æ€§èƒ½ä¸èµ„æºå—é™åœºæ™¯ï¼Œä½“ç°äº†æŠ€æœ¯æ™®æƒ æ€§ï¼Œåœ¨å®é™…ä¸šåŠ¡ä¸­å¯æ ¹æ®ç®—åŠ›ã€å»¶è¿Ÿç­‰éœ€æ±‚çµæ´»é€‰æ‹©ï¼Œå¹³è¡¡æ•ˆæœä¸æˆæœ¬ã€‚  

## longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning
### Abstract
Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B
### ğŸŒŸ è®ºæ–‡è§£è¯» | LongWriter-Zeroï¼šç”¨å¼ºåŒ–å­¦ä¹ çªç ´è¶…é•¿æ–‡æœ¬ç”Ÿæˆéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¶…é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚ä¸‡å­—çº§æŠ¥å‘Šã€å™äº‹åˆ›ä½œç­‰ï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…åœºæ™¯ä¸­è‡³å…³é‡è¦çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰æŠ€æœ¯é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯æ¨¡å‹ç”Ÿæˆé•¿åº¦å­˜åœ¨ä¸Šé™ï¼ŒäºŒæ˜¯éšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œå†…å®¹è´¨é‡ï¼ˆè¿è´¯æ€§ã€ä¸€è‡´æ€§ã€ç»“æ„åˆç†æ€§ç­‰ï¼‰ä¼šæ˜¾è‘—ä¸‹é™ã€‚  

æ­¤å‰ä¸»æµæ–¹æ¡ˆï¼ˆå¦‚LongWriterï¼‰ä¾èµ–**æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼Œå³åœ¨äººå·¥æ„é€ çš„â€œæŒ‡ä»¤ - é•¿æ–‡æœ¬è¾“å‡ºâ€é…å¯¹æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ä½†è¿™ç§æ–¹å¼å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š  
- æ„é€ é«˜è´¨é‡çš„åˆæˆSFTæ•°æ®æˆæœ¬æé«˜ã€éš¾åº¦å¤§ï¼›  
- åˆæˆæ•°æ®å¾€å¾€ç¼ºä¹è¿è´¯æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸”é£æ ¼å•ä¸€ã€è¿‡åº¦â€œäººå·¥åŒ–â€ï¼›  
- SFTçš„æœ€å¤§ä¼¼ç„¶ç›®æ ‡æ— æ³•æ˜¾å¼ä¼˜åŒ–å…¨å±€å±‚é¢çš„æ–‡æœ¬å±æ€§ï¼ˆå¦‚æ•´ä½“è¿è´¯æ€§ã€æ ¼å¼ä¸€è‡´æ€§ï¼‰ã€‚  

ä¸ºçªç ´è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡º**å®Œå…¨ä»é›¶å¼€å§‹ã€ä¸ä¾èµ–ä»»ä½•æ ‡æ³¨/åˆæˆæ•°æ®**çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ¡ˆï¼Œè®©LLMè‡ªä¸»â€œè¿›åŒ–â€å‡ºè¶…é•¿é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ¡†æ¶  
ä¼ ç»ŸSFTä¾èµ–å›ºå®šå‚è€ƒæ–‡æœ¬ï¼Œè€Œæœ¬æ–‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè®©æ¨¡å‹é€šè¿‡**å¥–åŠ±ä¿¡å·**ä¼˜åŒ–é•¿æ–‡æœ¬ç”Ÿæˆçš„å…¨å±€ç›®æ ‡ï¼ˆæ— éœ€äººå·¥æ„é€ SFTæ•°æ®é›†ï¼‰ã€‚å…·ä½“é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•è®­ç»ƒç­–ç•¥ç½‘ç»œï¼šä»åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Qwen2.5 - 32Bï¼‰å‡ºå‘ï¼Œè®©æ¨¡å‹åœ¨â€œå†™ä½œè¿‡ç¨‹ä¸­è‡ªä¸»è§„åˆ’ä¸è¿­ä»£â€ï¼Œé€æ­¥æŒæ¡è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç»´åº¦å¥–åŠ±æ¨¡å‹è®¾è®¡ï¼ˆReward Designï¼‰  
é’ˆå¯¹å¼€æ”¾åŸŸæ–‡æœ¬ç”Ÿæˆçš„å¤æ‚æ€§ï¼ˆä¸»è§‚æ€§ã€å¤šç»´åº¦ï¼‰ï¼Œè®¾è®¡**å¤åˆå¥–åŠ±å‡½æ•°**ï¼Œæ•´åˆå¤šä¸ªä¸“é¡¹å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œåˆ†åˆ«å¼•å¯¼æ¨¡å‹ä¼˜åŒ–ä»¥ä¸‹å…³é”®ç»´åº¦ï¼š  
- é•¿åº¦æ§åˆ¶ï¼ˆLength RMï¼‰ï¼šç¡®ä¿è¾“å‡ºæ»¡è¶³â€œè¶…é•¿â€éœ€æ±‚ï¼ŒåŒæ—¶é¿å…æ— æ„ä¹‰å†—ä½™ï¼›  
- å†™ä½œè´¨é‡ï¼ˆQuality RMï¼‰ï¼šè¯„ä¼°å†…å®¹æµç•…åº¦ã€é€»è¾‘æ€§ã€ä¸“ä¸šæ€§ç­‰ï¼›  
- ç»“æ„æ ¼å¼ï¼ˆStructure RMï¼‰ï¼šä¿éšœæ–‡æœ¬ç»“æ„åˆç†ï¼ˆå¦‚åˆ†ç« èŠ‚ã€å±‚æ¬¡æ¸…æ™°ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - time Scalingï¼‰ä¸æŒç»­é¢„è®­ç»ƒï¼ˆContinual Pretrainingï¼‰  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼šå€Ÿé‰´å¤§æ¨¡å‹åœ¨æ•°å­¦/ä»£ç ä»»åŠ¡ä¸­â€œé•¿æ€ç»´é“¾ï¼ˆCoTï¼‰â€çš„æˆåŠŸç»éªŒï¼Œæ¢ç´¢åœ¨è¶…é•¿æ–‡æœ¬ç”Ÿæˆä¸­å¼•å…¥é•¿CoTï¼Œå¢å¼ºæ¨¡å‹æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ï¼›  
- æŒç»­é¢„è®­ç»ƒï¼šåœ¨é•¿æ–‡æœ¬ç´ æä¸æ¨ç†æ•°æ®ä¸ŠæŒç»­é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡RLè®­ç»ƒåæ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- åŸºå‡†æµ‹è¯•ç¢¾å‹ä¼ ç»ŸSFTï¼šåŸºäºQwen2.5 - 32Bè®­ç»ƒçš„LongWriter - Zeroï¼Œåœ¨WritingBenchã€Arena - Writeç­‰é•¿æ–‡æœ¬å†™ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼Œ**å…¨é¢è¶…è¶Šä¼ ç»ŸSFTæ–¹æ³•**ï¼›  
- è¶…è¶Šåƒäº¿å‚æ•°æ¨¡å‹ï¼šåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡»è´¥DeepSeek R1ã€Qwen3 - 235Bç­‰ç™¾ billion + è§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œåˆ·æ–°SOTAï¼›  
- å¼€æºèµ„æºä¸°å¯Œï¼šæ¨¡å‹ checkpoint å’Œæ•°æ®å·²å¼€æºï¼ˆhttps://huggingface.co/THU - KEG/LongWriter - Zero - 32Bï¼‰ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¯å¤ç°ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°ï¼šè¯æ˜å¼ºåŒ–å­¦ä¹ å¯åœ¨â€œæ— æ ‡æ³¨/åˆæˆæ•°æ®â€åœºæ™¯ä¸‹ï¼Œæ¿€æ´»LLMçš„è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹èƒ½åŠ›è§£é”æä¾›äº†â€œéSFTâ€çš„æ–°èŒƒå¼ï¼›  
2. å¥–åŠ±å·¥ç¨‹ï¼šå¤šç»´åº¦å¤åˆå¥–åŠ±æ¨¡å‹çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å¼€æ”¾åŸŸç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚åˆ›æ„å†™ä½œã€å¤šè½®å¯¹è¯ï¼‰ï¼Œç”¨äºåˆ»ç”»â€œä¸»è§‚æ€§å¼ºã€æ— æ˜ç¡®ground - truthâ€åœºæ™¯ä¸‹çš„è´¨é‡è¯„ä¼°ï¼›  
3. è®­ç»ƒç­–ç•¥ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆé•¿CoTï¼‰ä¸æŒç»­é¢„è®­ç»ƒçš„ç»„åˆï¼Œä¸ºæå‡å¤§æ¨¡å‹é•¿æ–‡æœ¬æ¨ç†ã€ç”Ÿæˆçš„ä¸Šé™æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯è·¯çº¿ï¼›  
4. è½åœ°ä»·å€¼ï¼šé’ˆå¯¹çœŸå®ä¸–ç•Œâ€œè¶…é•¿æ–‡æœ¬éœ€æ±‚â€ï¼ˆå¦‚æŠ¥å‘Šæ’°å†™ã€æ³•å¾‹æ–‡ä¹¦ã€æ•™è‚²å†…å®¹åˆ›ä½œï¼‰ï¼Œæä¾›äº†æ›´ä¼˜è´¨çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæ¨åŠ¨LLMåœ¨ä¸“ä¸šé¢†åŸŸçš„è½åœ°ã€‚  


LongWriter - Zeroçš„å·¥ä½œä¸ä»…è§£å†³äº†è¶…é•¿æ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ç—›ç‚¹ï¼Œæ›´å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹èƒ½åŠ›è¿›åŒ–ä¸­çš„æ½œåŠ›â€”â€”æ— éœ€ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨ï¼Œä¹Ÿèƒ½è®©æ¨¡å‹â€œè‡ªä¸»å­¦ä¹ â€å¤æ‚ä»»åŠ¡çš„å®Œæˆèƒ½åŠ›ã€‚è¿™ä¸ºå¤§æ¨¡å‹ç ”å‘èŒƒå¼ã€å¥–åŠ±æœºåˆ¶è®¾è®¡ç­‰æ–¹å‘ï¼Œéƒ½å¸¦æ¥äº†æå…·å¯å‘æ€§çš„å‚è€ƒã€‚

## rdpo--real-data-preference-optimization-for-physics-consistency-video-generation
### Abstract
Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/
### ğŸŒŸ è®ºæ–‡è§£è¯» | RDPOï¼šä»çœŸå®è§†é¢‘ä¸­æç‚¼ç‰©ç†å…ˆéªŒï¼Œé©æ–°è§†é¢‘ç”Ÿæˆçš„ç‰©ç†ä¸€è‡´æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†é¢‘ç”ŸæˆæŠ€æœ¯åœ¨è§†è§‰è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¦å¿ å®å¤ç°çœŸå®ä¸–ç•Œçš„ç‰©ç†è§„å¾‹ä»é¢‡å…·æŒ‘æˆ˜ã€‚åŸºäºåå¥½çš„æ¨¡å‹åè®­ç»ƒè™½æœ‰æœ›æå‡ç‰©ç†ä¸€è‡´æ€§ï¼Œå´ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨æ•°æ®é›†æˆ–å°šæœªæˆç†Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚ä¸€æ–¹é¢ï¼Œæ„å»ºèƒ½æ£€æµ‹ä»»æ„è§†é¢‘ç‰©ç†è¿è§„çš„å¥–åŠ±å‡½æ•°ä»æ˜¯å¼€æ”¾éš¾é¢˜ï¼›å¦ä¸€æ–¹é¢ï¼Œå¤§è§„æ¨¡äººå·¥åå¥½æ•°æ®é›†çš„åˆ¶ä½œæˆæœ¬é«˜ã€è€—æ—¶é•¿ï¼Œä¸”äººç±»åˆ¤æ–­å­˜åœ¨ä¸»è§‚æ€§å·®å¼‚ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§é«˜æ•ˆã€æ— éœ€æ ‡æ³¨çš„åå¥½ä¼˜åŒ–ç­–ç•¥æ¥é’ˆå¯¹æ€§æå‡ç‰©ç†çœŸå®æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ— æ ‡æ³¨æ¡†æ¶RDPO  
Real Data Preference Optimizationï¼ˆRDPOï¼‰æ˜¯ä¸€ä¸ªæ— éœ€äººå·¥æ ‡æ³¨çš„åå¥½ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥ä»çœŸå®ä¸–ç•Œè§†é¢‘ä¸­æç‚¼ç‰©ç†å…ˆéªŒã€‚å®ƒç»•å¼€äº†ä¼ ç»ŸDPOæˆ–RLHFå¯¹äººç±»è¾“å…¥çš„ä¾èµ–ï¼Œä»¥çœŸå®è§†é¢‘ç‰‡æ®µä½œä¸ºç‰©ç†åŠ¨æ€ä¿¡æ¯çš„â€œé»„é‡‘æ ‡å‡†â€ï¼Œæ¢ç´¢åˆ©ç”¨çœŸå®ä¸–ç•Œè§†é¢‘å›ºæœ‰ç‰©ç†å…ˆéªŒçš„é«˜æ•ˆè·¯å¾„ï¼Œè€Œéå¯¹è¿™äº›è§†é¢‘åå¤è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªåŠ¨æ„å»ºåå¥½å¯¹çš„æ–¹æ³•  
RDPOå€ŸåŠ©é¢„è®­ç»ƒç”Ÿæˆå™¨å¯¹çœŸå®è§†é¢‘åºåˆ—è¿›è¡Œåå‘é‡‡æ ·ï¼Œè‡ªåŠ¨æ„å»ºåœ¨ç‰©ç†æ­£ç¡®æ€§ä¸Šå…·æœ‰ç»Ÿè®¡åŒºåˆ†åº¦çš„åå¥½å¯¹ã€‚åœ¨åå‘é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡é€‰æ‹©æ€§åˆ©ç”¨ latent è¡¨ç¤ºæ¥æ„é€ è§†é¢‘ï¼Œè¿™äº› latent è¡¨ç¤ºæ—¢æºäºçœŸå®æ•°æ®ï¼ˆå¯Œå«çœŸå®ç‰©ç†ä¿¡æ¯ï¼‰ï¼Œåˆä¸æ¨¡å‹å›ºæœ‰ç”Ÿæˆåˆ†å¸ƒä¿æŒå¯¹é½ï¼ˆé¿å…è¿‡åº¦æ”¹å˜æ¨¡å‹è§†è§‰é£æ ¼æˆ–äº§ç”Ÿåˆ†å¸ƒå¤–è¾“å‡ºï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šé˜¶æ®µè¿­ä»£è®­ç»ƒæœºåˆ¶  
é‡‡ç”¨å¤šé˜¶æ®µè¿­ä»£è®­ç»ƒè°ƒåº¦ï¼Œå¼•å¯¼ç”Ÿæˆå™¨é€æ­¥æ›´å¥½åœ°éµå¾ªç‰©ç†å®šå¾‹ã€‚åˆ©ç”¨çœŸå®è§†é¢‘ä¸­æ¢ç´¢åˆ°çš„åŠ¨æ€ä¿¡æ¯ï¼Œè®©ç”Ÿæˆè§†é¢‘åœ¨åŠ¨ä½œè¿è´¯æ€§ä¸ç‰©ç†çœŸå®æ€§ä¸Šå¾—åˆ°æ˜¾è‘—æå‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸äººç±»è¯„ä¼°ä¸­éªŒè¯ï¼ŒRDPO åœ¨å¤šä¸ªç»´åº¦ï¼ˆå¦‚ç‰©ç†ä¸€è‡´æ€§ã€è§†é¢‘æ•´ä½“è´¨é‡ç­‰ï¼‰å®ç°äº†æå‡ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶åœ¨æ”¹è¿›ä¸åŒåŸºçº¿æ¨¡å‹ç‰©ç†ä¸€è‡´æ€§ä¸è§†é¢‘è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼›åŒæ—¶è¿˜å¯¹æ¯”åˆ†æäº† RDPO ä¸ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨çš„ä¼ ç»Ÿ DPO æ–¹æ³•ï¼Œæ¢ç´¢äº†äºŒè€…ç»“åˆçš„æ½œåœ¨ååŒæ•ˆç›Šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ— æ ‡æ³¨èŒƒå¼çš„åˆ›æ–°ï¼šä¸ºè§£å†³éœ€å¤§é‡äººå·¥æ ‡æ³¨çš„ä»»åŠ¡æä¾›äº†æ€è·¯ï¼Œå±•ç¤ºäº†å¦‚ä½•ä»çœŸå®æ•°æ®ä¸­è‡ªåŠ¨æŒ–æ˜ç›‘ç£ä¿¡å·ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚  
2. ç‰©ç†å…ˆéªŒçš„åˆ©ç”¨ï¼šåœ¨è§†é¢‘ç”Ÿæˆè¿™ç±»éœ€éµå¾ªç°å®ä¸–ç•Œè§„å¾‹çš„ä»»åŠ¡ä¸­ï¼Œæä¾›äº†ä»çœŸå®æ•°æ®æç‚¼é¢†åŸŸå…ˆéªŒï¼ˆå¦‚ç‰©ç†è§„å¾‹ï¼‰çš„èŒƒä¾‹ï¼Œå¯å¯å‘å…¶ä»–éœ€ç»“åˆç°å®çŸ¥è¯†çš„ç”Ÿæˆç±»ä»»åŠ¡ï¼ˆå¦‚æ¨¡æ‹Ÿä»¿çœŸã€è™šæ‹Ÿåœºæ™¯æ„å»ºç­‰ï¼‰ã€‚  
3. è¿­ä»£è®­ç»ƒä¸åˆ†å¸ƒå¯¹é½ï¼šå¤šé˜¶æ®µè¿­ä»£è®­ç»ƒè°ƒåº¦ä»¥åŠ latent ç©ºé—´å¯¹é½çš„æ€è·¯ï¼Œå¯¹å¹³è¡¡â€œå¼•å…¥æ–°å…ˆéªŒâ€ä¸â€œä¿æŒæ¨¡å‹åŸæœ‰èƒ½åŠ›/åˆ†å¸ƒâ€å…·æœ‰å‚è€ƒä»·å€¼ï¼Œåœ¨æ¨¡å‹å¾®è°ƒã€é¢†åŸŸé€‚é…ç­‰åœºæ™¯ä¸­æˆ–å¯å¤ç”¨ã€‚  

## shrinking-the-generation-verification-gap-with-weak-verifiers
### Abstract
Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¼±éªŒè¯å™¨ç¼©å°ç”Ÿæˆ-éªŒè¯å·®è·ï¼šWeaveræ¡†æ¶çš„åˆ›æ–°ä¹‹è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨éƒ¨ç½²è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ—¶ï¼ŒéªŒè¯æ¨¡å‹å“åº”çš„è´¨é‡æˆ–æ­£ç¡®æ€§æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¿™ä¸€é—®é¢˜åœ¨æ•°æ®é›†æ•´ç†ã€æ¨¡å‹å¯¹é½å’Œæ¨ç†æ—¶å†³ç­–ç­‰LM pipelineå„ç¯èŠ‚éƒ½å­˜åœ¨ã€‚å€ŸåŠ©å®Œç¾éªŒè¯å™¨ç»“åˆé‡å¤é‡‡æ ·ï¼ˆç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”åé€‰æœ€ä¼˜ï¼‰èƒ½å¤§å¹…æå‡æ¨¡å‹åœ¨æ•°å­¦ã€ä»£ç ã€æ¨ç†ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†å®Œç¾éªŒè¯å™¨è¦ä¹ˆä¸å¯æ‰©å±•ï¼ˆå¦‚äººå·¥éªŒè¯ï¼‰ï¼Œè¦ä¹ˆå®ç”¨æ€§æœ‰é™ï¼ˆå¦‚Leanè¿™ç±»å½¢å¼åŒ–è¯æ˜å·¥å…·ï¼‰ã€‚è€Œä½œä¸ºé€šç”¨éªŒè¯å™¨çš„LMè£åˆ¤å’Œå¥–åŠ±æ¨¡å‹ï¼Œä¸â€œ oracle verifiersï¼ˆå®Œç¾å‡†ç¡®çš„éªŒè¯å™¨ï¼‰â€ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå³â€œç”Ÿæˆ - éªŒè¯å·®è·â€â€”â€”æ¨¡å‹èƒ½ç”Ÿæˆæ­£ç¡®å“åº”ä½†æ— æ³•è¢«è¯†åˆ«ã€‚åŒæ—¶ï¼Œå¼±éªŒè¯å™¨ï¼ˆå¦‚LMè£åˆ¤ã€å¥–åŠ±æ¨¡å‹ï¼‰å­˜åœ¨åˆ†æ•°å™ªå£°å¤§ã€æ ¡å‡†å·®ã€å‡é˜³æ€§ç‡é«˜ç­‰é—®é¢˜ï¼Œä¸”æ•´åˆå¼±éªŒè¯å™¨è¿˜é¢ä¸´ naive èšåˆä¸è¶³ã€æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹æœ‰æ•ˆé›†æˆéš¾ã€æ¨ç†æ—¶éƒ¨ç½²éªŒè¯æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ï¼Œå› æ­¤æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•ç»“åˆå¤šä¸ªå¼±éªŒè¯å™¨æ¥æ”¹è¿›é‡å¤é‡‡æ ·ä¸‹çš„å“åº”é€‰æ‹©ï¼Œç¼©å°ç”Ÿæˆ - éªŒè¯å·®è·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºWeaveræ¡†æ¶èšåˆå¼±éªŒè¯å™¨
Weaveræ˜¯ä¸€ä¸ªæ— éœ€åœ¨çœŸå®æ ‡ç­¾ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒæ¥èšåˆå¼±éªŒè¯å™¨çš„æ¡†æ¶ã€‚é¦–å…ˆå‘ç°ï¼Œåœ¨æœ‰å¤§é‡å¸¦æ ‡ç­¾è®­ç»ƒæ•°æ®æ—¶ï¼Œå­¦ä¹ åŠ æƒéªŒè¯å™¨é›†åˆï¼ˆåˆ©ç”¨éªŒè¯å™¨å‡†ç¡®ç‡å·®å¼‚ï¼‰èƒ½æ¯” naive å¹³å‡ï¼ˆå‡è®¾éªŒè¯å™¨è´¨é‡ä¸€è‡´ï¼Œæ˜“è®©ä½è´¨é‡éªŒè¯å™¨ä¸»å¯¼è‡´ç²¾åº¦ä¸‹é™ï¼‰æœ€å¤šé«˜å‡º11.2ä¸ªç™¾åˆ†ç‚¹ï¼›å½“ç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®æ—¶ï¼Œå°†å¼±ç›‘ç£ï¼ˆWSï¼‰é€‚é…åˆ°éªŒè¯åœºæ™¯ï¼Œè§£å†³è¾“å‡ºä¸ä¸€è‡´å’Œä½ç²¾åº¦éªŒè¯å™¨é—®é¢˜ï¼Œé€šè¿‡è¿‡æ»¤æ— ä¿¡æ¯éªŒè¯å™¨ã€å½’ä¸€åŒ–éªŒè¯å™¨åˆ†æ•°ï¼Œå¹¶åŸºäºè¿™äº›åˆ†æ•°å’ŒæœªçŸ¥çœŸå®æ ‡ç­¾æ„å»º latent variable model æ¥ä¼°è®¡éªŒè¯å™¨å‡†ç¡®ç‡ä½œä¸ºé›†åˆæƒé‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£å†³å¼±éªŒè¯å™¨é›†æˆçš„å¤šæŒ‘æˆ˜
é’ˆå¯¹å¼±éªŒè¯å™¨é›†æˆçš„ä¸‰å¤§æŒ‘æˆ˜é€ä¸€åº”å¯¹ï¼šå¯¹äº naive èšåˆä¸è¶³ï¼Œç”¨åŠ æƒé›†åˆæ›¿ä»£ naive å¹³å‡ï¼Œåˆ©ç”¨éªŒè¯å™¨å‡†ç¡®ç‡å·®å¼‚æå‡æ€§èƒ½ï¼›å¯¹äºæœ‰é™æ ‡æ³¨æ•°æ®ä¸‹æœ‰æ•ˆé›†æˆéš¾ï¼Œé€‚é…å¼±ç›‘ç£æŠ€æœ¯ï¼Œå¤„ç†å¼±éªŒè¯å™¨è¾“å‡ºæ ¼å¼ä¸ä¸€è‡´ï¼ˆå¦‚logitsã€äºŒåˆ†ç±»åˆ†æ•°ã€Likertåˆ†æ•°ç­‰ï¼‰å’Œä½è´¨é‡é—®é¢˜ï¼Œå€ŸåŠ©æ•°æ®é›†ç»Ÿè®¡å½’ä¸€åŒ–è¾“å‡ºå’Œè¿‡æ»¤ç‰¹å®šéªŒè¯å™¨ï¼›å¯¹äºæ¨ç†æ—¶éƒ¨ç½²éªŒè¯æˆæœ¬é«˜ï¼Œç”¨Weaverçš„ç»„åˆè¾“å‡ºåˆ†æ•°è®­ç»ƒç´§å‡‘çš„400Mè·¨ç¼–ç å™¨ï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—æˆæœ¬åŒæ—¶ä¿ç•™é«˜å‡†ç¡®ç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æµ‹è¯•æ—¶é‡å¤é‡‡æ ·åœºæ™¯ï¼ˆæ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”åé€‰æ‹©å…¶ä¸€ï¼‰ä¸‹è¯„ä¼°ï¼ŒWeaverç›¸æ¯”éªŒè¯å™¨åˆ†æ•°æ— åŠ æƒå¹³å‡çš„é‡å¤é‡‡æ ·æ€§èƒ½æå‡17.1%ï¼Œç›¸æ¯”å¤šæ•°æŠ•ç¥¨æå‡13.5%ï¼›å¯¹æ¯”LMçš„Pass@1ï¼ˆé€‰ç¬¬ä¸€ä¸ªå€™é€‰å“åº”çš„æ€§èƒ½ï¼‰ï¼Œåœ¨æ¨ç†å’Œæ•°å­¦ä»»åŠ¡ä¸Šï¼Œå¯¹8Bæ¨¡å‹æ€§èƒ½æå‡17.9%ï¼Œå¯¹70Bæ¨¡å‹æå‡14.5%ï¼›ç”¨Llama 3.3 70B Instructä½œç”Ÿæˆå™¨ã€70Bæˆ–æ›´å°çš„è£åˆ¤å’Œå¥–åŠ±æ¨¡å‹ä½œéªŒè¯å™¨é›†åˆæ—¶ï¼Œèƒ½è¾¾åˆ°o3 - miniæ°´å¹³å‡†ç¡®ç‡ï¼ˆå¹³å‡87.7%ï¼‰ï¼Œè¯¥å¢ç›Šå ªæ¯”GPT - 4oåˆ°o3 - miniçš„æ€§èƒ½è·ƒå‡ï¼ˆ69.0% vs. 86.7%ï¼Œåè€…éœ€å¤§é‡å¾®è°ƒä¸åè®­ç»ƒï¼‰ï¼›è®­ç»ƒçš„400Mè·¨ç¼–ç å™¨è’¸é¦æ¨¡å‹ä¿ç•™äº†Weaverå…¨ç²¾åº¦çš„98.7%ï¼ŒåŒæ—¶å°†éªŒè¯è®¡ç®—é‡é™ä½è¾¾99.97%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šå¼±éªŒè¯å™¨èšåˆæ€è·¯ï¼šå½“é¢ä¸´å¤šä¸ªæœ‰ç¼ºé™·ä½†äº’è¡¥çš„å·¥å…·/æ¨¡å‹æ—¶ï¼Œå¯å€Ÿé‰´WeaveråŠ æƒèšåˆå¹¶ç»“åˆå¼±ç›‘ç£ä¼°è®¡æƒé‡çš„æ–¹å¼ï¼Œå……åˆ†åˆ©ç”¨å„å·¥å…·ä¼˜åŠ¿ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®ä¾èµ–ã€‚
2. æ€§èƒ½ - æˆæœ¬æƒè¡¡ï¼šåœ¨è¿½æ±‚æ¨¡å‹æ€§èƒ½æå‡åŒæ—¶å…³æ³¨æ¨ç†æˆæœ¬ï¼Œå¦‚Weaveré€šè¿‡è’¸é¦å¾—åˆ°ç´§å‡‘æ¨¡å‹é™ä½è®¡ç®—æˆæœ¬ï¼Œè¿™ç§åœ¨åº”ç”¨ä¸­å¹³è¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—çš„æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚
3. å¼±ç›‘ç£é€‚é…ç‰¹å®šåœºæ™¯ï¼šé’ˆå¯¹è‡ªèº«ä»»åŠ¡åœºæ™¯ä¸­ç±»ä¼¼â€œå¼±éªŒè¯å™¨è¾“å‡ºæ ¼å¼ä¸ä¸€ã€è´¨é‡å‚å·®â€ç­‰é—®é¢˜ï¼Œå¯å‚è€ƒWeaveråˆ©ç”¨æ•°æ®é›†ç»Ÿè®¡è¿›è¡Œå½’ä¸€åŒ–ã€è¿‡æ»¤ç­‰æ‰‹æ®µæ¥é€‚é…å¼±ç›‘ç£æŠ€æœ¯ï¼Œæ‹“å±•å¼±ç›‘ç£åº”ç”¨è¾¹ç•Œã€‚

## reflective-verbal-reward-design-for-pluralistic-alignment
### Abstract
AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸ªæ€§åŒ–ä»·å€¼å¯¹é½æ–°èŒƒå¼ï¼šåæ€å¼è¯­è¨€å¥–åŠ±è®¾è®¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨AIä¸äººç±»ä»·å€¼å¯¹é½çš„é¢†åŸŸï¼Œä¸»æµçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•å­˜åœ¨å±€é™ï¼šäººç±»ä»·å€¼å…·æœ‰é«˜åº¦å¼‚è´¨æ€§ï¼Œä¸åŒäººä»·å€¼è§‚å·®å¼‚ç”šè‡³å†²çªï¼Œä½†RLHFå¸¸å°†æ‰€æœ‰äººåé¦ˆèšåˆä¸ºå•ä¸€å¥–åŠ±æ¨¡å‹ï¼Œè¿™ä¼šå¯¼è‡´å°‘æ•°ç¾¤ä½“åå¥½è¢«è¿‡åº¦å‹åˆ¶ã€‚æ­¤å¤–ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹äººç±»åå¥½ä¸æ˜¯ç®€å•â€œæå–â€è€Œæ˜¯â€œæ„å»ºâ€çš„ï¼Œç°æœ‰è¢«åŠ¨æ”¶é›†åå¥½ã€ç›´æ¥æ ‡æ³¨çš„æ–¹å¼éš¾ä»¥è®©ç”¨æˆ·å……åˆ†åæ€ä»¥å½¢æˆæ˜ç¡®åå¥½ã€‚å› æ­¤ï¼Œå¦‚ä½•å…¼é¡¾ä»·å€¼å¤šæ ·æ€§ä¸åå¥½æ„å»ºè¿‡ç¨‹ï¼Œå­¦ä¹ ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹æˆä¸ºå…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºInteractive - Reflective Dialogue Alignmentï¼ˆIRDAï¼‰æ¡†æ¶  
IRDAå€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡äº¤äº’å¼å¯¹è¯å­¦ä¹ ä¸ªæ€§åŒ–å¥–åŠ±å‡½æ•°ï¼ŒåŒ…å«ä¸‰éƒ¨åˆ†ï¼šä¸€æ˜¯**åæ€å¼è¯­è¨€åå¥½å¼•å¯¼**ï¼Œå¼•å¯¼ç”¨æˆ·æ¸…æ™°è¡¨è¾¾è‡ªèº«ä»·å€¼è§‚ï¼›äºŒæ˜¯**ä¸»åŠ¨å­¦ä¹ **ï¼Œç­–ç•¥æ€§é€‰æ‹©ç¤ºä¾‹ä¾›äººç±»æ‰¹åˆ¤ï¼Œæå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼›ä¸‰æ˜¯**LLMé©±åŠ¨çš„è¯­è¨€å¥–åŠ±å»ºæ¨¡**ï¼Œè®©LLMåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä»ç¨€ç–ç”¨æˆ·åé¦ˆä¸­æ³›åŒ–ï¼Œç›´æ¥ä½œä¸ºå¥–åŠ±å‡½æ•°è¯„ä¼°AIè¡Œä¸ºè½¨è¿¹ã€‚ç”¨LLMå¼•å¯¼çš„å¯¹è¯æ›¿ä»£è¢«åŠ¨æ ‡æ³¨ï¼Œæ¿€å‘ç”¨æˆ·æ·±æ€ç†Ÿè™‘çš„åæ€ï¼ˆSystem 2è®¤çŸ¥ï¼‰æ¥è§£å†³åå¥½æ„å»ºéš¾é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèåˆå¤šå­¦ç§‘è§†è§’å®ç°ä¸ªæ€§åŒ–å¯¹é½  
ç»“åˆAIã€äººæœºäº¤äº’ï¼ˆHCIï¼‰å’Œç¤¾ä¼šç§‘å­¦çŸ¥è¯†ï¼Œä¸å†å‡è®¾ç”¨æˆ·èƒ½ç›´æ¥æ¸…æ™°è¡¨è¾¾æ–°åœºæ™¯ä¸‹åå¥½ï¼Œè€Œæ˜¯ä¸»åŠ¨å¼•å¯¼ç”¨æˆ·é€šè¿‡åæ€å°†æ½œåœ¨ä»·å€¼è§‚è½¬åŒ–ä¸ºå…·ä½“åå¥½ï¼Œå¼¥è¡¥äº†è¿‡å¾€æ–¹æ³•åœ¨åå¥½æ„å»ºç¯èŠ‚çš„ä¸è¶³ï¼Œä¸ºä¸ªæ€§åŒ–AIå¯¹é½æä¾›æ–° pipelineã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¼€å±•ä¸¤é¡¹æ¶‰åŠ30åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ï¼šç¬¬ä¸€é¡¹é’ˆå¯¹â€œå°Šé‡è¡Œä¸ºâ€çš„ä¸ªäººå®šä¹‰æ„å»ºå¥–åŠ±æ¨¡å‹ï¼ˆ21äººå‚ä¸ï¼‰ï¼Œç¬¬äºŒé¡¹æ¢ç´¢è‡ªåŠ¨é©¾é©¶ä¼¦ç†å†³ç­–ï¼ˆ9äººå‚ä¸ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…ä»·å€¼åˆ¤æ–­å·®å¼‚æ˜¾è‘—ï¼ŒIRDAç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼ˆå¦‚éåæ€å¼è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼‰èƒ½æ›´å‡†ç¡®æ•æ‰ä¸ªä½“å¯¹ä»·å€¼å¯¹é½è¡Œä¸ºçš„å®šä¹‰ï¼Œåœ¨å‡†ç¡®ç‡ä¸Šæ¯”éåæ€å¼è¯­è¨€å¥–åŠ±æ¨¡å‹æå‡9 - 12%ï¼Œä¸”æ¯”ä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•æ ·æœ¬æ•ˆç‡æ›´é«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šå­¦ç§‘èåˆæ€è·¯ï¼šå°†AIæŠ€æœ¯ä¸HCIã€ç¤¾ä¼šç§‘å­¦ä¸­å…³äºäººç±»åå¥½æ„å»ºã€åæ€çš„ç ”ç©¶ç»“åˆï¼Œä¸ºAIå¯¹é½é—®é¢˜æä¾›æ›´è´´åˆäººç±»è®¤çŸ¥è§„å¾‹çš„è§£æ³•ï¼Œå¯ç¤ºåç»­ç ”ç©¶è·¨å­¦ç§‘è§£å†³å¤æ‚AIä¼¦ç†ä¸å¯¹é½é—®é¢˜ã€‚  
2. ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹æ„å»ºï¼šè¯æ˜äº†é€šè¿‡å¼•å¯¼åæ€ã€äº¤äº’å¼å¯¹è¯å­¦ä¹ ä¸ªæ€§åŒ–å¥–åŠ±å‡½æ•°çš„å¯è¡Œæ€§ï¼Œä¸ºé¢å‘ä¸ªäººçš„AIåŠ©æ‰‹ï¼ˆå¦‚ä¸ªæ€§åŒ–æ™ºèƒ½åŠ©ç†ï¼‰æä¾›æŠ€æœ¯å‚è€ƒï¼Œè®©AIå¥–åŠ±æ›´è´´åˆç”¨æˆ·ä¸ªä½“è€Œéç¾¤ä½“å¹³å‡ã€‚  
3. æ•°æ®é«˜æ•ˆå­¦ä¹ ï¼šä¸»åŠ¨å­¦ä¹ ç­–ç•¥åœ¨ä¸ªæ€§åŒ–åœºæ™¯ä¸‹æå‡æ ·æœ¬æ•ˆç‡ï¼Œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„å¥–åŠ±æ¨¡å‹å­¦ä¹ æä¾›å€Ÿé‰´ï¼Œåç»­å¯æ¢ç´¢åœ¨æ›´å¤šèµ„æºå—é™åœºæ™¯çš„åº”ç”¨ã€‚  
4. åå¥½æ„å»ºè¿‡ç¨‹é‡è§†ï¼šå¼ºè°ƒäººç±»åå¥½â€œæ„å»ºâ€è€Œéâ€œæå–â€ï¼Œæé†’ç ”ç©¶è€…åœ¨è®¾è®¡AIå¯¹é½ç³»ç»Ÿæ—¶å…³æ³¨ç”¨æˆ·åæ€ã€ä»·å€¼å¤–æ˜¾çš„è¿‡ç¨‹ï¼Œæœªæ¥å¯æ·±å…¥æ¢ç´¢å¦‚ä½•ä¼˜åŒ–å¯¹è¯å¼•å¯¼ç­–ç•¥ä»¥æ›´å¥½åŠ©åŠ›ç”¨æˆ·åå¥½æ„å»ºã€‚

## duashepherd--integrating-stepwise-correctness-and-potential-rewards-for-mathematical-reasoning
### Abstract
In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DuaShepherdï¼šåŒå¥–åŠ±ä¿¡å·é©±åŠ¨ï¼Œæå‡å¤§æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†å‡ºè‰²æˆæœï¼Œä½†åœ¨æ•°å­¦æ¨ç†è¿™ç±»éœ€è¦å¤šæ­¥éª¤å¤æ‚æ¨ç†çš„é¢†åŸŸä»å­˜åœ¨ä¸è¶³ã€‚å°½ç®¡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºç­‰æ–¹æ³•èƒ½å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ä¹Ÿè¢«ç”¨äºæå‡æ¨¡å‹æ•°å­¦æ¨ç†çš„å†…åœ¨èƒ½åŠ›ï¼Œä¸è¿‡ç°æœ‰åŸºäºè¿‡ç¨‹çš„å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰å­˜åœ¨æ„å»ºæˆæœ¬é«˜ï¼ˆéœ€å¤§é‡äººå·¥æ ‡æ³¨æ­¥éª¤æ•°æ®ï¼‰æˆ–å•ä¸€ä¿¡å·å±€é™ç­‰é—®é¢˜ã€‚åŒæ—¶ï¼Œä¸åŒæ¥æºçš„è¿‡ç¨‹ç›‘ç£æ•°æ®ï¼ˆå¦‚PRM800Kå’ŒåŸºäºè’™ç‰¹å¡æ´›é‡‡æ ·ç”Ÿæˆçš„æ•°æ®ï¼‰è•´å«ä¸åŒå¥–åŠ±ä¿¡å·å«ä¹‰ï¼Œå¦‚ä½•æ•´åˆè¿™äº›äº’è¡¥ä¿¡å·æ¥æ‰“é€ æ›´å¼ºå¤§çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæˆä¸ºå¾…è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒå¥–åŠ±ä¿¡å·èåˆçš„æ¡†æ¶è®¾è®¡  
æå‡ºDuaShepherdå¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ•´åˆâ€œæ­£ç¡®æ€§â€å’Œâ€œæ½œåœ¨æ€§â€ä¸¤ç§äº’è¡¥å¥–åŠ±ä¿¡å·ã€‚å…¶ä¸­ï¼ŒåŸºäºæ­£ç¡®æ€§çš„ä¿¡å·èšç„¦äºè¯†åˆ«é€æ­¥æ¨ç†ä¸­çš„é”™è¯¯ï¼›åŸºäºæ½œåœ¨æ€§çš„ä¿¡å·åˆ™å…³æ³¨å½“å‰æ­¥éª¤å¯¼å‘æ­£ç¡®æœ€ç»ˆç­”æ¡ˆçš„å¯èƒ½æ€§ã€‚é€šè¿‡å°†äºŒè€…ç»“åˆä¸ºå¤åˆæ¦‚ç‡ï¼Œå……åˆ†å‘æŒ¥ä¸åŒä¿¡å·ä¼˜åŠ¿æ¥å¢å¼ºå¤§æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªåŠ¨åŒ–æ•°æ®é›†æ„å»º pipeline  
å¼€å‘è‡ªåŠ¨åŒ–æµç¨‹æ¥æ„å»ºå¤§è§„æ¨¡å¥–åŠ±å»ºæ¨¡æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒæ—¶åŒ…å«æ­£ç¡®æ€§å’Œæ½œåœ¨æ€§ä¸¤ç§å¥–åŠ±ä¿¡å·æ ‡æ³¨ã€‚å€ŸåŠ©å·²æœ‰çš„PRM800Kå’ŒMath - Shepherdç­‰æ•°æ®é›†ï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹è‡ªåŠ¨æ ‡æ³¨é‡‡æ ·çš„æ¨ç†è½¨è¿¹ï¼Œé«˜æ•ˆç”ŸæˆåŒæ—¶å…·å¤‡ä¸¤ç±»å¥–åŠ±æ ‡ç­¾çš„DuaShepherdæ•°æ®é›†ï¼Œé™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ä¸”å®ç°å¤§è§„æ¨¡æ•°æ®æ„å»ºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šä»»åŠ¡ç»Ÿä¸€æ¶æ„è®­ç»ƒ  
é‡‡ç”¨ç»Ÿä¸€çš„å¤šå¤´æ¶æ„ï¼Œåœ¨å¤šä»»åŠ¡è®¾ç½®ä¸‹å¹¶è¡Œè®­ç»ƒä¸¤ç§å¥–åŠ±æ¨¡å‹ã€‚ç±»ä¼¼äºArmoRMçš„å¤šä»»åŠ¡è®­ç»ƒæ¨¡å¼ï¼Œè®©å•ä¸ªåŸºç¡€æ¨¡å‹å…±äº«å­¦ä¹ ä¸¤ç§è¿‡ç¨‹å¥–åŠ±ä¿¡å·ï¼Œå€ŸåŠ©å¹¶è¡Œå­¦ä¹ ä¸¤ç§ä¿¡å·å¸¦æ¥çš„äº’è¡¥æ€§ï¼Œæå‡å¥–åŠ±æ¨¡å‹æ•´ä½“æ€§èƒ½ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MATH500å’ŒProcessBenchç­‰åŸºå‡†æµ‹è¯•ä¸­å¼€å±•å®è¯è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼šç»“åˆä¸¤ç§å¥–åŠ±ä¿¡å·çš„DuaShepherdæ¨¡å‹ï¼Œæ˜¾è‘—ä¼˜äºä»…ç”¨å•ä¸€å¥–åŠ±ç±»å‹è®­ç»ƒçš„æ¨¡å‹ï¼›åœ¨å¯æ¯”èµ„æºçº¦æŸä¸‹ï¼Œå®ç°äº†å½“å‰æœ€ä¼˜ï¼ˆSOTAï¼‰çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†åŒå¥–åŠ±ä¿¡å·èåˆä»¥åŠå¤šä»»åŠ¡è®­ç»ƒç­‰æ–¹æ³•åœ¨æå‡æ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šç»´åº¦å¥–åŠ±èåˆæ€è·¯ï¼šåœ¨éœ€å¤æ‚æ¨ç†çš„ä»»åŠ¡åœºæ™¯ä¸­ï¼Œå¯æ€è€ƒæ•´åˆä¸åŒç»´åº¦ã€å…·å¤‡äº’è¡¥æ€§çš„å¥–åŠ±ä¿¡å·ï¼Œçªç ´å•ä¸€ä¿¡å·çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ›´å…¨é¢æŒ‡å¯¼ã€‚  
2. è‡ªåŠ¨åŒ–æ•°æ®æ„å»ºï¼šé¢å¯¹äººå·¥æ ‡æ³¨æˆæœ¬é«˜çš„ä»»åŠ¡ï¼Œå‚è€ƒå…¶è‡ªåŠ¨åŒ–ç”Ÿæˆå¸¦å¤šç»´åº¦æ ‡æ³¨æ•°æ®é›†çš„æ€è·¯ï¼Œåˆ©ç”¨å·²æœ‰æ¨¡å‹æˆ–æ–¹æ³•æ¥è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæå‡æ•°æ®æ„å»ºæ•ˆç‡ä¸è§„æ¨¡ã€‚  
3. å¤šä»»åŠ¡å…±äº«æ¶æ„ï¼šåœ¨å¤„ç†å­˜åœ¨å…³è”çš„å¤šä»»åŠ¡æˆ–å¤šä¿¡å·å­¦ä¹ åœºæ™¯æ—¶ï¼Œé‡‡ç”¨å…±äº«åŸºç¡€æ¨¡å‹çš„å¤šä»»åŠ¡è®­ç»ƒæ¨¡å¼ï¼Œèƒ½å€ŸåŠ©ä¸åŒä»»åŠ¡/ä¿¡å·é—´çš„äº’è¡¥æ€§ï¼Œæå‡æ•´ä½“æ¨¡å‹æ€§èƒ½ï¼Œè¯¥æ¨¡å¼åœ¨å…¶ä»–éœ€å¤šç»´åº¦ä¼˜åŒ–çš„AIä»»åŠ¡ä¸­ä¹Ÿæœ‰å€Ÿé‰´ä»·å€¼ã€‚

## reward-agnostic-prompt-optimization-for-text-to-image-diffusion-models
### Abstract
We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ‰“ç ´å¥–åŠ±æ¨¡å‹é™åˆ¶ï¼šRATTPOè®©æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºä¼˜åŒ–æ›´é€šç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹é¢†åŸŸï¼Œå°½ç®¡æ¨¡å‹èƒ½ä¾æ®ç”¨æˆ·æç¤ºç”Ÿæˆå›¾åƒï¼Œä½†è¾“å‡ºè´¨é‡å—æç¤ºå½±å“å¤§ï¼Œç»†å¾®æç¤ºå˜åŒ–å°±å¯èƒ½å¯¼è‡´ç”Ÿæˆè´¨é‡æ³¢åŠ¨ã€‚ä¸ºæ”¹è¿›ç”Ÿæˆæ•ˆæœéœ€è¿›è¡Œæç¤ºå·¥ç¨‹ï¼Œè€Œä¸åŒå¥–åŠ±æ¨¡å‹ç”¨äºè¯„ä¼°å›¾åƒç”Ÿæˆå„æ–¹é¢ï¼ˆå¦‚å®¡ç¾ã€äººç±»åå¥½ã€ç‰©ä½“ç©ºé—´å…³ç³»ç­‰ï¼‰ã€‚ç°æœ‰è‡ªåŠ¨æç¤ºå·¥ç¨‹æ–¹æ³•å¤šé’ˆå¯¹ç‰¹å®šå¥–åŠ±æ¨¡å‹è®¾è®¡ï¼Œåœ¨é¢å¯¹æ–°çš„ã€ä¸åŒå¥–åŠ±æ¨¡å‹çš„æç¤ºå·¥ç¨‹åœºæ™¯æ—¶è¡¨ç°æ¬ ä½³ï¼Œç¼ºä¹èƒ½åœ¨æµ‹è¯•æ—¶é€‚åº”å¤šæ ·å¥–åŠ±å‡½æ•°çš„é€šç”¨æç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œè¿™å°±æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRATTPOæ–¹æ³•  
RATTPOï¼ˆReward - Agnostic Test - Time Prompt Optimizationï¼‰æ˜¯ä¸€ç§çµæ´»çš„æµ‹è¯•æ—¶ä¼˜åŒ–æ–¹æ³•ï¼Œæ— éœ€ä¿®æ”¹å°±èƒ½é€‚ç”¨äºå„ç§å¥–åŠ±åœºæ™¯ã€‚å®ƒåœ¨ä¼˜åŒ–ç”¨æˆ·æç¤ºæ—¶ï¼Œä¸éœ€è¦é’ˆå¯¹ç‰¹å®šå¥–åŠ±çš„ä»»åŠ¡æè¿°ï¼Œè€Œæ˜¯é€šè¿‡æŸ¥è¯¢å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿­ä»£æœç´¢ä¼˜åŒ–æç¤ºï¼Œåˆ©ç”¨ä¼˜åŒ–è½¨è¿¹å’Œä¸€ç§æ–°é¢–çš„å¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œhintâ€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡æ¥å¼•å¯¼ä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥å¥–åŠ±æ„ŸçŸ¥åé¦ˆæœºåˆ¶ï¼ˆâ€œhintâ€ï¼‰  
â€œhintâ€æ˜¯ä¸€ç§ç®€æ´çš„æ–‡æœ¬ç­–ç•¥ï¼Œç”¨äºæé«˜å¥–åŠ±ï¼Œç±»ä¼¼äººå·¥ç¼–å†™çš„ä»»åŠ¡æè¿°ï¼Œä½†åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç”±ç‹¬ç«‹çš„LLMå®æ—¶ç”Ÿæˆã€‚è¿™ä¸ªæœºåˆ¶ä¸ºä¼˜åŒ–å™¨æä¾›å¥–åŠ±æ„ŸçŸ¥æŒ‡å¯¼ï¼ŒåŒæ—¶é¿å…äº†æ‰‹åŠ¨é‡å†™ç‰¹å®šå¥–åŠ±ä»»åŠ¡æè¿°çš„éœ€æ±‚ï¼Œå¼¥è¡¥äº†å› ä¸é‡‡ç”¨ç‰¹å®šå¥–åŠ±è®¾è®¡å¯èƒ½å¸¦æ¥çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®­ç»ƒä¸æ¢¯åº¦æ— å…³çš„è‡ªåŠ¨æç¤ºå·¥ç¨‹  
RATTPOæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€æ— éœ€æ¢¯åº¦çš„è‡ªåŠ¨æç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œèƒ½è½»æ¾åº”ç”¨äºä¸åŒå¥–åŠ±è®¾ç½®ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šå¥–åŠ±è¿›è¡Œè°ƒæ•´æˆ–è®­ç»ƒï¼Œçªç ´äº†ä»¥å¾€ä¸€äº›æ–¹æ³•ä¾èµ–è®­ç»ƒã€è½¬ç§»èƒ½åŠ›æœ‰é™çš„å±€é™ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜RATTPOå…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ï¼Œåœ¨è¯„ä¼°ç”Ÿæˆç¾å­¦ã€äººç±»åå¥½ã€ç‰©ä½“ç©ºé—´å…³ç³»ç­‰ä¸åŒç”Ÿæˆæ–¹é¢çš„å¤šæ ·å¥–åŠ±è®¾ç½®ä¸‹ï¼Œéƒ½èƒ½æœ‰æ•ˆå¢å¼ºç”¨æˆ·æç¤ºã€‚åœ¨æœç´¢æ•ˆç‡ä¸Šï¼ŒRATTPOè¶…è¶Šäº†å…¶ä»–æµ‹è¯•æ—¶æœç´¢åŸºçº¿ï¼Œæœ€å¤šèƒ½å‡å°‘3.5å€çš„æ¨ç†é¢„ç®—ï¼›åœ¨æœ‰è¶³å¤Ÿæ¨ç†é¢„ç®—æ—¶ï¼Œå…¶æ€§èƒ½èƒ½ä¸éœ€è¦ç‰¹å®šå¥–åŠ±å¾®è°ƒçš„åŸºäºå­¦ä¹ çš„åŸºçº¿ç›¸åª²ç¾ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é€šç”¨ä¼˜åŒ–æ€è·¯ï¼šRATTPOæå‡ºçš„å¥–åŠ±æ— å…³çš„æµ‹è¯•æ—¶æç¤ºä¼˜åŒ–æ€è·¯ï¼Œä¸ºå¤„ç†éœ€é€‚åº”å¤šç§è¯„ä¼°æ ‡å‡†ï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰çš„ä»»åŠ¡æä¾›äº†å‚è€ƒï¼Œåœ¨å…¶ä»–éœ€æ ¹æ®ä¸åŒè¯„ä»·ä½“ç³»ä¼˜åŒ–è¾“å…¥çš„åœºæ™¯ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸï¼‰å¯èƒ½æœ‰å€Ÿé‰´ä»·å€¼ã€‚
2. åé¦ˆæœºåˆ¶è®¾è®¡ï¼šå…¶â€œhintâ€è¿™ç§å¥–åŠ±æ„ŸçŸ¥åé¦ˆæœºåˆ¶çš„è®¾è®¡ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®æ—¶ç”ŸæˆæŒ‡å¯¼ä¿¡å·æ¥è¾…åŠ©ä¼˜åŒ–ï¼Œä¸ºç»“åˆå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ— ç‰¹å®šä»»åŠ¡æè¿°çš„ä¼˜åŒ–ä»»åŠ¡æä¾›äº†æ–°çš„è®¾è®¡æ–¹å‘ã€‚
3. é«˜æ•ˆæœç´¢ä¼˜åŠ¿ï¼šåœ¨æœç´¢æ•ˆç‡ä¸Šçš„æå‡è¯æ˜äº†è¯¥æ–¹æ³•åœ¨èµ„æºåˆ©ç”¨ä¸Šçš„ä¼˜åŠ¿ï¼Œå¯¹äºè®¡ç®—èµ„æºæœ‰é™æˆ–è¿½æ±‚é«˜æ•ˆæ¨ç†çš„åº”ç”¨åœºæ™¯ï¼Œè¿™ç§é«˜æ•ˆæœç´¢çš„è®¾è®¡æ€è·¯å€¼å¾—å­¦ä¹ ã€‚

## reasongrm--enhancing-generative-reward-models-through-large-reasoning-models
### Abstract
Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonGRMï¼šå€Ÿå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›é©æ–°ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£ã€ç”Ÿæˆä¸å†³ç­–ä¸Šå–å¾—é•¿è¶³è¿›æ­¥ï¼Œä½†è¦è®©æ¨¡å‹è¾“å‡ºè´´åˆäººç±»ä»·å€¼è§‚ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯å…³é”®ã€‚ä¼ ç»Ÿæ ‡é‡å¥–åŠ±æ¨¡å‹ï¼ˆSRMsï¼‰æŠŠå¤æ‚äººç±»åå¥½å‹ç¼©æˆå•ä¸€æ ‡é‡ï¼Œæ˜“ä¿¡æ¯ä¸¢å¤±ã€æ³›åŒ–æ€§å¼±ï¼›æ–°å…´ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰è™½æ›´çµæ´»ï¼Œä½†æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå¸¸å‡ºç°æ¨ç†è·¯å¾„ä¸å®Œæ•´æˆ–è¿‡åº¦æ¨æµ‹ï¼Œå¯¼è‡´ä»»åŠ¡ä¸­â€œå¹»è§‰â€æˆ–å…³é”®ä¿¡æ¯ç¼ºå¤±ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡GRMsçš„æ¨ç†è´¨é‡ä»¥å®ç°å¯é åå¥½å»ºæ¨¡ï¼Œæˆäº†æ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReasonGRMä¸‰é˜¶æ®µæ¡†æ¶  
ReasonGRMåˆ†ä¸‰æ­¥æ‰“é€ æ›´ä¼˜ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼š  
- é˜¶æ®µä¸€ï¼ˆç”Ÿæˆæ¨ç†è·¯å¾„ï¼‰ï¼šç”¨Zero - RLç”Ÿæˆç®€æ´ã€ä»¥ç»“æœä¸ºå¯¼å‘çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘å…³é”®ä¿¡æ¯é—æ¼é£é™©ï¼›  
- é˜¶æ®µäºŒï¼ˆç­›é€‰ä¼˜è´¨è·¯å¾„ï¼‰ï¼šå¼•å…¥å…¨æ–°è¯„ä¼°æŒ‡æ ‡\( R^\star \)ï¼Œä¾æ®ç”Ÿæˆå¯èƒ½æ€§ä¸ºæ¨ç†è·¯å¾„æ‰“åˆ†ï¼Œåå¥½â€œç”¨æœ€å°‘æ¢ç´¢è¾¾æ­£ç¡®ç­”æ¡ˆâ€çš„è·¯å¾„ï¼Œå‰Šå‡è®­ç»ƒä¸­æ˜“å¼•å‘å¹»è§‰çš„æ•°æ®ï¼›  
- é˜¶æ®µä¸‰ï¼ˆå¼ºåŒ–æ¨¡å‹èƒ½åŠ›ï¼‰ï¼šé’ˆå¯¹é«˜éš¾åº¦ç¤ºä¾‹ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ç²¾è°ƒæ¨¡å‹ï¼Œå¢å¼ºå…¶åå¥½åŒºåˆ†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰\( R^\star \)è¯„ä¼°æŒ‡æ ‡è§£å†³æ•°æ®è´¨é‡éš¾é¢˜  
\( R^\star \)ç»“åˆâ€œæœ‰æ•ˆæ€§ï¼ˆValidityï¼Œæ¨ç†å¯¼å‘æ­£ç¡®ç»“æœï¼‰â€ä¸â€œè‡ªä¸€è‡´æ€§ï¼ˆSelf - Consistencyï¼Œæ¨ç†é€»è¾‘è¿è´¯æ— å†—ä½™ï¼‰â€ä¸¤å¤§å…³é”®å±æ€§ï¼Œé€šè¿‡ç”Ÿæˆå¯èƒ½æ€§æ¥è¯„ä¼°æ¨ç†è·¯å¾„ï¼Œèƒ½ä»å™ªå£°å€™é€‰é›†ä¸­è‡ªåŠ¨é€‰ä¼˜è´¨æ¨ç†è·¯å¾„ï¼Œç ´è§£å¤æ‚ä»»åŠ¡å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æ•°æ®è´¨é‡ç“¶é¢ˆã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨RM - Benchã€RewardBenchã€RMBä¸‰å¤§å…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonGRMè¡¨ç°äº®çœ¼ï¼šå¹³å‡è¶…è¶Šæ­¤å‰æœ€ä¼˜GRMs 1.8%ï¼Œåœ¨éƒ¨åˆ†åœºæ™¯ä¸‹æ¯”GPT - 4oç­‰é—­æºæ¨¡å‹é¢†å…ˆè¾¾5.6%ï¼Œè¿˜æ¯”ä¸»æµSRMså¹³å‡é«˜4.5%ã€‚å®éªŒä¸ä»…éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œæ¶ˆèå®éªŒä¹Ÿå‰–æäº†æ¨ç†è´¨é‡ã€\( R^\star \)è¿‡æ»¤æ•ˆæœã€å„è®­ç»ƒé˜¶æ®µå¯¹æœ€ç»ˆå¥–åŠ±æ¨¡å‹çš„å½±å“ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é‡è§†æ¨ç†è´¨é‡åœ¨å¥–åŠ±æ¨¡å‹ä¸­çš„ä»·å€¼ï¼šæ­ç¤ºäº†é«˜è´¨é‡æ¨ç†è·¯å¾„ï¼ˆå…¼é¡¾æœ‰æ•ˆæ€§ä¸è‡ªä¸€è‡´æ€§ï¼‰å¯¹åå¥½å»ºæ¨¡çš„å…³é”®ä½œç”¨ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹è®¾è®¡æŒ‡æ˜â€œæ¨ç†æ„ŸçŸ¥â€æ–¹å‘ï¼›  
2. åˆ›æ–°è¯„ä¼°ä¸è¿‡æ»¤æœºåˆ¶ï¼š\( R^\star \)å±•ç¤ºäº†å¦‚ä½•ç”¨ç”Ÿæˆå¯èƒ½æ€§é‡åŒ–æ¨ç†è´¨é‡ï¼Œä¸ºå¤„ç†å™ªå£°æ•°æ®ã€æ„å»ºä¼˜è´¨è®­ç»ƒé›†æä¾›äº†å¯å¤ç”¨æ€è·¯ï¼›  
3. å¤šé˜¶æ®µè®­ç»ƒPipelineï¼šä»ç”Ÿæˆåˆ°ç­›é€‰å†åˆ°å¼ºåŒ–å­¦ä¹ çš„æµç¨‹ï¼Œä¸ºé€šç”¨LLMå‘ä¸“ç²¾å¥–åŠ±æ¨¡å‹è½¬åŒ–æä¾›äº†å·¥ç¨‹åŒ–å‚è€ƒèŒƒå¼ï¼›  
4. å…¨é¢å®éªŒéªŒè¯ï¼šè·¨å¤šä¸ªæƒå¨åŸºå‡†çš„æµ‹è¯•+æ¶ˆèå®éªŒï¼Œæ˜¯å­¦æœ¯ç ”ç©¶ä¸­éªŒè¯æ–¹æ³•æ™®é€‚æ€§ä¸æ¨¡å—ä»·å€¼çš„å…¸èŒƒï¼Œå€¼å¾—å€Ÿé‰´ä»¥å¢å¼ºç ”ç©¶è¯´æœåŠ›ã€‚  
```

## robust-reward-modeling-via-causal-rubrics
### Abstract
Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå› æœå‡†åˆ™æ‰“é€ é²æ£’å¥–åŠ±æ¨¡å‹ï¼šCromeæ¡†æ¶ç ´è§£å¥–åŠ±é»‘å®¢éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»åé¦ˆçš„è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯æ ¸å¿ƒç¯èŠ‚ï¼Œä½†å½“å‰å¥–åŠ±æ¨¡å‹æ™®éé¢ä¸´ â€œå¥–åŠ±é»‘å®¢â€ é—®é¢˜ã€‚ä¼ ç»ŸRMå¸¸æŠŠè®­ç»ƒæ•°æ®é‡Œçš„ç»Ÿè®¡ç›¸å…³æ€§é”™å½“æˆè´¨é‡çš„çœŸæ­£å› æœé©±åŠ¨å› ç´ ï¼Œæ¯”å¦‚ä¼šä¾æ®å›å¤é•¿åº¦ã€æ ¼å¼è¿™ç±»è¡¨é¢æˆ–è™šå‡å±æ€§æ¥æ‰“åˆ†ï¼Œè€Œéäº‹å®æ€§ã€ç›¸å…³æ€§ç­‰çœŸå®è´¨é‡ç»´åº¦ã€‚è¿™æ˜¯å› ä¸ºæ ‡å‡†è®­ç»ƒç›®æ ‡éš¾ä»¥åŒºåˆ†å› æœå› ç´ å’Œè™šå‡å…³è”ï¼Œæœ€ç»ˆå¯¼è‡´RMè„†å¼±ã€ç­–ç•¥å¯¹é½å¤±æ•ˆã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®©RMåœ¨æœªçŸ¥è™šå‡å±æ€§çš„æƒ…å†µä¸‹ï¼Œåªä¾æ‰˜èƒ½è·å–çš„çœŸå®å› æœè´¨é‡å±æ€§æ¥å®ç°é²æ£’è®­ç»ƒï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— è™šå‡æ„ŸçŸ¥çš„å› æœæ¡†æ¶  
æå‡ºåŸºäºå› æœæ¨¡å‹çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ¡†æ¶Cromeï¼Œæ— éœ€é¢„å…ˆæŒ‡å®šæˆ–å¹²é¢„ä»»ä½•è™šå‡å±æ€§ï¼Œä»…é€šè¿‡è°ƒç”¨ â€œ oracle LLM â€ è¯†åˆ«å‡ºçš„å› æœè´¨é‡å‡†åˆ™è¿›è¡Œå¹²é¢„ï¼Œå°±èƒ½å¼•å¯¼RMå­¦ä¹ ï¼Œä»æœºåˆ¶ä¸Šç»•å¼€äº†å¯¹è™šå‡å±æ€§å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå› æœå±æ€§çš„é¶å‘åäº‹å®å¢å¼º  
è®¾è®¡ä¸¤ç±»åˆæˆè®­ç»ƒæ ·æœ¬å¢å¼ºæ–¹å¼ï¼š  
- **å› æœå¢å¼ºï¼ˆCausal Augmentationsï¼‰**ï¼šç”Ÿæˆåœ¨ç‰¹å®šå› æœå±æ€§ï¼ˆå¦‚äº‹å®æ€§ï¼‰ä¸Šæœ‰å·®å¼‚çš„æ ·æœ¬å¯¹ï¼Œè®©RMå¯¹çœŸå®è´¨é‡å˜åŒ–çš„ç»´åº¦äº§ç”Ÿæ•æ„Ÿæ€§ï¼Œç²¾å‡†æ•æ‰å› æœç»´åº¦çš„å½±å“ï¼›  
- **ä¸­æ€§å¢å¼ºï¼ˆNeutral Augmentationsï¼‰**ï¼šåˆ©ç”¨å› æœå¢å¼ºåçš„æ•°æ®ä¸åŸå§‹åå¥½å¯¹ï¼Œç”Ÿæˆåœ¨è™šå‡ç‰¹å¾ï¼ˆå¦‚é£æ ¼ï¼‰ä¸Šå˜åŒ–ä½†å› æœå†…å®¹ä¿ç•™çš„æ ·æœ¬ï¼Œå¹¶æ­é…å¹³å±€æ ‡ç­¾ï¼Œå¼ºåˆ¶RMå¯¹è™šå‡å±æ€§ä¿æŒä¸å˜æ€§ã€‚ä¸”æ•´ä¸ªè¿‡ç¨‹æ— éœ€æ˜¾å¼çŸ¥æ™“è™šå‡å› ç´ ï¼Œä»…é€šè¿‡å› æœå‡†åˆ™å¹²é¢„å°±å¯ç¼“è§£å¯¹å¤§é‡è™šå‡å…³è”çš„æ•æ„Ÿã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨RewardBenchç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCromeè¡¨ç°è¿œè¶…æ ‡å‡†åŸºçº¿ï¼š  
- åœ¨RewardBenchä¸Šå¹³å‡å‡†ç¡®ç‡æå‡æœ€é«˜è¾¾5.4%ï¼Œå…¶ä¸­å®‰å…¨ï¼ˆSafetyï¼‰ç±»åˆ«æå‡13.18%ã€æ¨ç†ï¼ˆReasoningï¼‰ç±»åˆ«æå‡7.19%ï¼›  
- åœ¨Best - of - Næ¨ç†åœºæ™¯ä¸‹ï¼Œé¢å¯¹RewardBenchã€å®‰å…¨ä¸“é¡¹çš„WildGuardTestã€æ¨ç†ä¸“é¡¹çš„GSM8kç­‰åŸºå‡†ï¼Œéšç€Nå¢å¤§ï¼ŒCromeçš„RMåœ¨é€‰æ‹©æœ€ä¼˜ç»“æœæ—¶æŒç»­é¢†å…ˆåŸºçº¿ï¼Œè¯æ˜å…¶åœ¨åº”å¯¹ç¨€æœ‰ï¼ˆé•¿å°¾ï¼‰è™šå‡å› ç´ æ—¶ä¹Ÿå…·å¤‡é²æ£’æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å› æœè§†è§’çš„é²æ£’è®­ç»ƒæ€è·¯ï¼šå°†å› æœå»ºæ¨¡å¼•å…¥å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œä¸ºè§£å†³â€œè™šå‡å…³è”å¹²æ‰°æ¨¡å‹å­¦ä¹ â€è¿™ç±»æ™®éé—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘åç»­åœ¨å…¶ä»–éœ€åŒºåˆ†å› æœä¸ç›¸å…³åœºæ™¯çš„æ¨¡å‹è®­ç»ƒå·¥ä½œï¼›  
2. æ— å…ˆéªŒçš„å¢å¼ºç­–ç•¥ï¼šå±•ç¤ºäº†å¦‚ä½•åœ¨ä¸ä¾èµ–è™šå‡å±æ€§å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡å¯¹å› æœå±æ€§çš„å¹²é¢„æ¥é—´æ¥å‡å°‘è™šå‡å…³è”å½±å“ï¼Œè¿™ç§â€œç»•å¼€æœªçŸ¥ã€å¼ºåŒ–å·²çŸ¥å› æœâ€çš„æ€è·¯åœ¨æ•°æ®å¢å¼ºã€é²æ£’è®­ç»ƒæ–¹å‘æœ‰æ¨å¹¿ä»·å€¼ï¼›  
3. å¤šåœºæ™¯é²æ£’æ€§éªŒè¯ï¼šåœ¨èŠå¤©ã€å®‰å…¨ã€æ¨ç†ç­‰ä¸åŒä»»åŠ¡åŸºå‡†åŠBest - of - Nè¿™ç±»å®é™…æ¨ç†åœºæ™¯ä¸‹éªŒè¯æœ‰æ•ˆæ€§ï¼Œè¯æ˜æ–¹æ³•å…·å¤‡è·¨ä»»åŠ¡ã€è·¨åœºæ™¯çš„æ™®é€‚æ€§ï¼Œä¸ºå·¥ä¸šçº§å¤§æ¨¡å‹å¯¹é½æµç¨‹çš„é²æ£’æ€§ä¼˜åŒ–æä¾›äº†å¯è½åœ°å‚è€ƒã€‚

## relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples
### Abstract
Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RELICï¼šå°æ ·æœ¬ç¤ºä¾‹åŠ©åŠ›ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±æ¨¡å‹æ³›åŒ–èƒ½åŠ›æå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½çš„å…³é”®ï¼Œä½†ç°æœ‰å¼€æºå¤šè¯­è¨€å¥–åŠ±æ¨¡å‹å¤šåŸºäºé«˜èµ„æºè¯­è¨€çš„åå¥½æ•°æ®é›†è®­ç»ƒï¼Œåœ¨ä½èµ„æºå°åº¦è¯­è¨€ä¸Šå¥–åŠ±ä¿¡å·ä¸å¯é ã€‚æ”¶é›†ä½èµ„æºè¯­è¨€å¤§è§„æ¨¡é«˜è´¨é‡åå¥½æ•°æ®æˆæœ¬æé«˜ï¼ŒåŸºäºåå¥½çš„è®­ç»ƒæ–¹æ³•éš¾ä»¥å®æ–½ã€‚åŒæ—¶ç ”ç©¶å‘ç°å¼€æºå¤šè¯­è¨€å¥–åŠ±æ¨¡å‹åœ¨ä½èµ„æºå°åº¦è¯­è¨€ä¸­æ— æ³•å‡†ç¡®åŒºåˆ†å®‰å…¨ä¸ä¸å®‰å…¨å“åº”ï¼Œå¦‚å¯¹ä½èµ„æºè¯­è¨€Bodoï¼Œå¥–åŠ±æ¨¡å‹ç»™ä¸å®‰å…¨å“åº”çš„åˆ†æ•°åè€Œé«˜äºå®‰å…¨å“åº”ï¼Œè¿™å¯¹ä¾èµ–è¿™äº›ä½èµ„æºè¯­è¨€ç¤¾åŒºä½¿ç”¨å®‰å…¨å¯¹é½çš„LLMsæ„æˆæŒ‘æˆ˜ï¼Œå› æ­¤éœ€è¦æ–°æ–¹æ³•æå‡ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±æ¨¡å‹æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRELICæ¡†æ¶  
RELICæ˜¯é’ˆå¯¹ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±å»ºæ¨¡çš„æ–°å‹ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ã€‚å®ƒè®­ç»ƒæ£€ç´¢å™¨æ—¶é‡‡ç”¨ pairwise ranking ç›®æ ‡ï¼Œä»è¾…åŠ©é«˜èµ„æºè¯­è¨€ä¸­é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹èƒ½æœ€æœ‰æ•ˆåœ°çªå‡ºåå¥½å“åº”å’Œæ¬¡åå¥½å“åº”ä¹‹é—´çš„åŒºåˆ«ï¼Œä¸ºå¥–åŠ±æ¨¡å‹æä¾›æœ‰åˆ¤åˆ«æ€§çš„ä¸Šä¸‹æ–‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ£€ç´¢å™¨è®­ç»ƒä¸é€‚ç”¨æ€§  
RELICä½¿ç”¨ pairwise ranking lossï¼ˆåŸºäºThurstoneã€Bradley - Terry ç›¸å…³ç†è®ºï¼‰è®­ç»ƒæ£€ç´¢å™¨ï¼Œä¼˜åŒ–å…¶é€‰æ‹©æœ€èƒ½åŒºåˆ†æ­£å“åº”å’Œè´Ÿå“åº”çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚ä¸”ä¸éœ€è¦è®¿é—®é…å¯¹åå¥½æ•°æ®é›†ï¼Œå¯åº”ç”¨äºä»»ä½•åŒ…å«äºŒå…ƒè´¨é‡æ ‡ç­¾çš„ä½èµ„æºæ•°æ®é›†ï¼Œé€‚ç”¨æ€§å¹¿ï¼Œè¿˜ç»“åˆé«˜èµ„æºè¯­è¨€è¾…åŠ©ç¤ºä¾‹åº“ï¼Œæ¨ç†æ—¶èƒ½æä¾›æ›´ä¸°å¯Œæœ‰åˆ¤åˆ«æ€§çš„ä¸Šä¸‹æ–‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ PKU - SafeRLHFã€WebGPTã€HH - RLHF ä¸‰ä¸ªåå¥½æ•°æ®é›†ä¸Šï¼Œç”¨æœ€å…ˆè¿›çš„å¼€æºå¥–åŠ±æ¨¡å‹è¿›è¡Œå¤§é‡å®éªŒã€‚ç»“æœæ˜¾ç¤ºRELICæ˜¾è‘—æé«˜ä½èµ„æºå°åº¦è¯­è¨€å¥–åŠ±æ¨¡å‹å‡†ç¡®ç‡ï¼ŒæŒç»­è¶…è¶Šç°æœ‰ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ã€‚å¦‚åœ¨ä½èµ„æºå°åº¦è¯­è¨€Bodoä¸Šï¼Œç”¨LLaMA - 3.2 - 3Bå¥–åŠ±æ¨¡å‹æ—¶ï¼Œç›¸æ¯”é›¶æ ·æœ¬æç¤ºå’Œç°æœ‰æœ€å…ˆè¿›ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡12.81%å’Œ10.13%ï¼›åœ¨Santaliè¯­è¨€ä¸Šï¼ŒåŸºäºLLAMA - 3.1 - 8Bçš„å¥–åŠ±æ¨¡å‹ï¼ŒRELICæ¯”é›¶æ ·æœ¬æç¤ºå‡†ç¡®ç‡æå‡24.16%ï¼Œæ¯”ç°æœ‰åŸºäºç›¸å…³æ€§çš„ç¤ºä¾‹é€‰æ‹©æ–¹æ³•æå‡21.26%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†æè§’åº¦å¯å€Ÿé‰´ï¼šå¯¹ä½èµ„æºè¯­è¨€ä¸Šå¼€æºå¤šè¯­è¨€å¥–åŠ±æ¨¡å‹æ³›åŒ–èƒ½åŠ›åˆ†æï¼Œæ­ç¤ºå“åº”è´¨é‡åŒºåˆ†èƒ½åŠ›çš„å…³é”®ç¼ºå£ï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜é—®é¢˜æ–¹å‘ã€‚
2. æ–¹æ³•åˆ›æ–°å¯å€Ÿé‰´ï¼šRELICæ¡†æ¶é’ˆå¯¹ä½èµ„æºåœºæ™¯ä¸‹æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ£€ç´¢å™¨ç»“åˆï¼Œä¸”åŸºäºå¥–åŠ±æ¨¡å‹æ’åºç›®æ ‡è®­ç»ƒæ£€ç´¢å™¨çš„æ€è·¯ï¼Œä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€ä»»åŠ¡æˆ–éœ€è¦åˆ©ç”¨é«˜èµ„æºè¾…åŠ©æ•°æ®çš„ä»»åŠ¡æä¾›äº†æ–¹æ³•å‚è€ƒã€‚
3. å®éªŒè®¾è®¡å¯å€Ÿé‰´ï¼šåœ¨å¤šä¸ªå…¸å‹åå¥½æ•°æ®é›†å’Œå¼€æºå¥–åŠ±æ¨¡å‹ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢çš„å®éªŒéªŒè¯æ–¹å¼èƒ½æœ‰åŠ›æ”¯æ’‘æ–¹æ³•ä»·å€¼ï¼Œå€¼å¾—ç›¸å…³ç ”ç©¶å®éªŒè®¾è®¡å­¦ä¹ ã€‚

## gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks
### Abstract
Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GFlowGRï¼šç”¨ç”Ÿæˆæµç½‘ç»œä¼˜åŒ–ç”Ÿæˆå¼æ¨èæ¡†æ¶çš„å¾®è°ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”Ÿæˆå¼æ¨èï¼ˆGRï¼‰æ¡†æ¶ç»“åˆç‰©å“ tokenizer å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨è¯¸å¤šåœºæ™¯å–å¾—æˆåŠŸï¼Œä½†ç›®å‰ç ”ç©¶å¤šèšç„¦äºä¼˜åŒ–ç‰©å“ tokenizer æˆ– LLM è§£ç ç­–ç•¥ï¼Œå¯¹ LLM é€‚é…æ¨èæ•°æ®çš„**å…³é”®å¾®è°ƒç¯èŠ‚**æ¢ç´¢ä¸è¶³ã€‚ç°æœ‰å¾®è°ƒæ–¹æ³•ï¼ˆå¦‚ç›‘ç£å¾®è°ƒ SFTã€ç›´æ¥åå¥½ä¼˜åŒ– DPOï¼‰å­˜åœ¨â€œæš´éœ²åå·®â€é—®é¢˜ï¼šSFT åªå…³æ³¨æ•°æ®é›†ä¸­æ­£æ ·æœ¬ï¼Œæ—¢æ²¡å¼•å…¥è´Ÿæ ·æœ¬å¯¹æ¯”ï¼Œä¹Ÿéš¾æ¢ç´¢æ½œåœ¨æ­£æ ·æœ¬ï¼›DPO ç­‰å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•è™½èƒ½åˆ©ç”¨è´Ÿæ ·æœ¬ï¼Œä½†ä¾èµ–å·²æ”¶é›†æ•°æ®ï¼Œæ— æ³•å……åˆ†åæ˜ çœŸå®ç”¨æˆ·åå¥½ï¼Œå¿½ç•¥äº†æœªè§‚æµ‹åˆ°çš„æ½œåœ¨æ­£æ ·æœ¬ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½æ¢ç´¢æ½œåœ¨æ­£æ ·æœ¬ã€ç¼“è§£æš´éœ²åå·®çš„ LLM å¾®è°ƒæ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†ç”Ÿæˆå¼æ¨èå»ºæ¨¡ä¸ºå¤šæ­¥ç”Ÿæˆä»»åŠ¡ï¼Œæå‡º GFlowGR æ¡†æ¶  
æŠŠ GR è§†ä¸ºå¤šæ­¥éª¤ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŸºäºç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰æ„å»ºå¾®è°ƒæ¡†æ¶ GFlowGRã€‚GFlowNets èƒ½è®© LLM æŒ‰ä¸å¥–åŠ±åˆ†å¸ƒæˆæ¯”ä¾‹çš„æ¦‚ç‡ç”Ÿæˆ token åºåˆ—ï¼Œå€Ÿæ­¤è¯†åˆ«æ½œåœ¨æ­£æ ·æœ¬ã€æ‰©å±•æ¨èå¤šæ ·æ€§ï¼Œä»ç”Ÿæˆæ¦‚ç‡å±‚é¢ç¼“è§£æš´éœ²åå·®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è‡ªé€‚åº”è½¨è¿¹é‡‡æ ·å™¨ä¸å¤šç»´åº¦å¥–åŠ±æ¨¡å‹  
- è‡ªé€‚åº”è½¨è¿¹é‡‡æ ·å™¨ï¼šèåˆä¼ ç»Ÿæ¨èç³»ç»Ÿçš„ååŒçŸ¥è¯†ï¼Œç”Ÿæˆâ€œä»æ˜“åˆ°éš¾â€çš„å¢å¼ºè½¨è¿¹ï¼Œç”¨è¯¾ç¨‹å¼è®­ç»ƒæ€è·¯æŒç»­æä¾›é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼›  
- å¤šç»´åº¦å¥–åŠ±æ¨¡å‹ï¼šç»¼åˆè€ƒé‡å¢å¼ºä¿¡å·ã€ååŒåˆ†æ•°ã€è¯­ä¹‰ç›¸ä¼¼åº¦ç­‰ï¼Œæœ‰æ•ˆåŒºåˆ†é‡‡æ ·åˆ°çš„æœªè§‚æµ‹è½¨è¿¹ä¸­â€œç”¨æˆ·å¯èƒ½åå¥½â€çš„ç‰©å“ï¼›  
- ç½®ä¿¡åŠ æƒæœºåˆ¶ï¼šä¾æ®å¢å¼ºä¿¡å·ä¸ååŒåˆ†æ•°çš„ä¸€è‡´æ€§ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ç½®ä¿¡æƒé‡ï¼Œè¿›ä¸€æ­¥ç¼“è§£æš´éœ²åå·®ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨**ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†**ã€åŸºäº**ä¸¤ç§ä¸åŒ GR éª¨å¹²æ¨¡å‹**å¼€å±•å¤§é‡å®éªŒã€‚ç»“æœéªŒè¯äº† GFlowGR åœ¨ç¼“è§£æš´éœ²åå·®ã€æå‡æ¨èæ•ˆæœä¸Šçš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ï¼Œè¯æ˜è¯¥æ¡†æ¶èƒ½åœ¨ä¸åŒæ•°æ®å’Œæ¨¡å‹åº•åº§ä¸‹ç¨³å®šå‘æŒ¥ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨é¢†åŸŸèåˆæ€è·¯ï¼šå°†ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰ä¸æ¨èç³»ç»Ÿç»“åˆï¼Œä¸ºè§£å†³æ¨èåœºæ™¯ä¸­â€œæš´éœ²åå·®â€è¿™ç±»ç‰¹æœ‰é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å‘åç»­è·¨é¢†åŸŸæŠ€æœ¯è¿ç§»ï¼›  
2. è¯¾ç¨‹å¼é‡‡æ ·ä¸å¤šç»´åº¦å¥–åŠ±ï¼šè‡ªé€‚åº”è½¨è¿¹é‡‡æ ·çš„â€œä»æ˜“åˆ°éš¾â€è¯¾ç¨‹è®­ç»ƒã€å¤šç»´åº¦å¥–åŠ±æ¨¡å‹çš„æ„å»ºæ–¹å¼ï¼Œä¸ºå¤„ç†â€œæ•°æ®ç¨€ç–+åå¥½éš¾å»ºæ¨¡â€åœºæ™¯æä¾›äº†å¯å¤ç”¨çš„è®¾è®¡æ€è·¯ï¼›  
3. é—®é¢˜å»ºæ¨¡è§†è§’ï¼šæŠŠæ¨èä»»åŠ¡æ‹†è§£ä¸ºå¤šæ­¥ç”Ÿæˆä»»åŠ¡ï¼Œé‡æ–°å®šä¹‰æ¨èç³»ç»Ÿä¸ç”Ÿæˆå¼æ¨¡å‹çš„äº¤äº’æ–¹å¼ï¼Œä¸ºæ¨èç³»ç»Ÿçš„â€œç”Ÿæˆå¼è½¬å‹â€æä¾›äº†æ–¹æ³•è®ºå‚è€ƒã€‚  

## autorule--reasoning-chain-of-thought-extracted-rule-based-rewards-improve-preference-learning
### Abstract
Rule-based rewards offer a promising strategy for improving reinforcement
learning from human feedback (RLHF), but current approaches often rely on
manual rule engineering. We present AutoRule, a fully automated method for
extracting rules from preference feedback and formulating them into rule-based
rewards. AutoRule extraction operates in three stages: it leverages a reasoning
model to interpret user preferences, identifies candidate rules from the
reasoning chain of these interpretations, and synthesizes them into a unified
rule set. Leveraging the finalized rule set, we employ language-model verifiers
to compute the fraction of rules satisfied by each output, using this metric as
an auxiliary reward alongside the learned reward model during policy
optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\%
relative improvement in length-controlled win rate on AlpacaEval2.0, and a
6.1\% relative gain in second-turn performance on a held-out MT-Bench subset,
compared to a GRPO baseline trained with the same learned reward model but
without the rule-based auxiliary reward. Our analysis confirms that the
extracted rules exhibit good agreement with dataset preference. We find that
AutoRule demonstrates reduced reward hacking compared to a learned reward model
when run over two episodes. Finally, our case study suggests that the extracted
rules capture unique qualities valued in different datasets. The extracted
rules are provided in the appendix, and the code is open-sourced at
https://github.com/cxcscmu/AutoRule.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AutoRuleï¼šä»æ¨ç†æ€ç»´é“¾ä¸­æå–è§„åˆ™å¥–åŠ±ï¼Œé©æ–°åå¥½å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»ä»·å€¼è§‚å’Œéµå¾ªæŒ‡ä»¤çš„å…³é”®æŠ€æœ¯ï¼Œä½†ç°æœ‰åŸºäºå­¦ä¹ çš„å¥–åŠ±æ¨¡å‹å­˜åœ¨â€œå¥–åŠ±é»‘å®¢æ”»å‡»â€é—®é¢˜ï¼ˆæ¨¡å‹ä¸ºè¿½æ±‚é«˜å¥–åŠ±è€ŒæŠ•æœºå–å·§ï¼ŒæœªçœŸæ­£æå‡å“åº”è´¨é‡ï¼‰ã€‚åŒæ—¶ï¼Œè§„åˆ™é©±åŠ¨çš„å¥–åŠ±è™½èƒ½æœ‰æ•ˆç¼“è§£è¯¥é—®é¢˜ï¼Œä½†ä¼ ç»Ÿè§„åˆ™ä¾èµ–äººå·¥è®¾è®¡æˆ–å¤§è§„æ¨¡ä¼—åŒ…æ ‡æ³¨ï¼Œæˆæœ¬é«˜ä¸”éš¾æ‰©å±•ã€‚å› æ­¤ï¼Œå¦‚ä½•è‡ªåŠ¨ä»åå¥½æ•°æ®ä¸­æå–è§„åˆ™æ¥æ„å»ºå¥–åŠ±æœºåˆ¶ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAutoRuleè‡ªåŠ¨è§„åˆ™æå–æ¡†æ¶  
AutoRuleé€šè¿‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»åå¥½æ•°æ®ä¸­è‡ªåŠ¨æå–å¯¹é½è§„åˆ™ã€‚æµç¨‹åˆ†ä¸‰æ­¥ï¼šé¦–å…ˆç”¨å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMä¸ºåå¥½è¾“å‡ºç”Ÿæˆé€æ­¥æ¨ç†ä¾æ®ï¼›æ¥ç€ä»æ¨ç†è¿‡ç¨‹ä¸­æå–å€™é€‰è§„åˆ™ï¼›æœ€åèšåˆè®­ç»ƒé›†å€™é€‰è§„åˆ™å¹¶åˆæˆç»Ÿä¸€è§„åˆ™é›†ï¼Œå€ŸåŠ©æ¨ç†é“¾çš„é€»è¾‘ç»“æ„æ•æ‰æ›´ç²¾å‡†çš„åå¥½å‡†åˆ™ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§„åˆ™å¥–åŠ±ä¸ç­–ç•¥ä¼˜åŒ–ç»“åˆ  
åˆ©ç”¨æœ€ç»ˆè§„åˆ™é›†ï¼Œè®©è¯­è¨€æ¨¡å‹å……å½“â€œéªŒè¯å™¨â€ï¼Œè®¡ç®—æ¯ä¸ªè¾“å‡ºæ»¡è¶³è§„åˆ™çš„æ¯”ä¾‹ï¼Œå°†è¯¥æŒ‡æ ‡ä½œä¸ºè¾…åŠ©å¥–åŠ±ï¼Œä¸ä¼ ç»Ÿå­¦ä¹ åˆ°çš„å¥–åŠ±æ¨¡å‹åœ¨ç­–ç•¥ä¼˜åŒ–é˜¶æ®µé…åˆä½¿ç”¨ï¼Œå¼•å¯¼æ¨¡å‹è®­ç»ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒéªŒè¯ä¸­ï¼Œç”¨Llama 3 8B Instructä½œéªŒè¯å™¨æ—¶ï¼Œè§„åˆ™åˆ†æ•°ï¼ˆå•ä¸ªæˆ–ç´¯è®¡ï¼‰åœ¨UltraFeedbackå’ŒMT - Bench Human Judgmentæ•°æ®é›†ä¸Šä¸åå¥½é«˜åº¦ä¸€è‡´ï¼›åŸºäºUltraFeedbackæ•°æ®ç”¨GRPOç»“åˆAutoRuleè®­ç»ƒLlama - 3 - 8Bæ¨¡å‹ï¼Œå¯¹æ¯”ä»…ç”¨å­¦ä¹ å¥–åŠ±æ¨¡å‹çš„GRPOåŸºçº¿ï¼ŒAlpacaEval2.0é•¿åº¦æ§åˆ¶èƒœç‡ç›¸å¯¹æå‡28.6%ï¼ŒMT - Benchä¿ç•™å­é›†çš„ç¬¬äºŒè½®è¡¨ç°ç›¸å¯¹æå‡6.1%ï¼›å¥–åŠ±é»‘å®¢å®éªŒè¡¨æ˜AutoRuleèƒ½å‡è½»å¥–åŠ±æ¨¡å‹è¿‡åº¦ä¼˜åŒ–ï¼›æ¶ˆèå®éªŒè¯æ˜ä»æ¨ç†é“¾æå–è§„åˆ™æ¯”ä»…ä»ä¾æ®æå–æ›´æœ‰æ•ˆï¼›å®šæ€§åˆ†ææ˜¾ç¤ºä¸åŒæ•°æ®é›†æå–çš„è§„åˆ™å„æœ‰ä¾§é‡ï¼ˆå¦‚UltraFeedbackä¾§é‡å¯¹è¯è´¨é‡ï¼ŒMT - Benchä¾§é‡æŒ‡ä»¤éµå¾ªä¸å¤æ‚ä»»åŠ¡é²æ£’æ€§ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªåŠ¨è§„åˆ™æå–æ€è·¯ï¼šæ‘†è„±äººå·¥è§„åˆ™ä¾èµ–ï¼Œå€ŸåŠ©LLMæ¨ç†èƒ½åŠ›ä»æ•°æ®ä¸­è‡ªåŠ¨æŒ–æ˜è§„åˆ™ï¼Œä¸ºé¢†åŸŸé€‚é…è§„åˆ™æ„å»ºæä¾›é«˜æ•ˆæ–¹æ¡ˆã€‚  
2. å¤šå¥–åŠ±èåˆä¼˜åŒ–ï¼šå°†è§„åˆ™å¥–åŠ±ä¸å­¦ä¹ å¥–åŠ±ç»“åˆç”¨äºç­–ç•¥ä¼˜åŒ–ï¼Œä¸ºç¼“è§£å¥–åŠ±é»‘å®¢ã€æå‡æ¨¡å‹çœŸå®æ€§èƒ½æä¾›æ–°èŒƒå¼ã€‚  
3. å¯è§£é‡Šæ€§ä¸é€‚é…æ€§ï¼šæå–çš„è§„åˆ™å¯è§£é‡Šï¼Œä¸”èƒ½æ•æ‰ä¸åŒæ•°æ®é›†çš„ç‹¬ç‰¹ä»·å€¼ï¼Œä¸ºç†è§£æ¨¡å‹åå¥½å¯¹é½é€»è¾‘å’Œå®šåˆ¶åŒ–ä¼˜åŒ–æä¾›å‚è€ƒã€‚

## spare--single-pass-annotation-with-reference-guided-evaluation-for-automatic-process-supervision-and-reward-modelling
### Abstract
Process or step-wise supervision has played a crucial role in advancing
complex multi-step reasoning capabilities of Large Language Models (LLMs).
However, efficient, high-quality automated process annotation remains a
significant challenge. To address this, we introduce Single-Pass Annotation
with Reference-Guided Evaluation (SPARE), a novel structured framework that
enables single-pass, per-step annotation by aligning each solution step to one
or multiple steps in a reference solution, accompanied by explicit reasoning
for evaluation. We show that reference-guided step-level evaluation effectively
facilitates process supervision on four datasets spanning three domains:
mathematical reasoning, multi-hop compositional question answering, and spatial
reasoning. We demonstrate that SPARE, when compared to baselines, improves
reasoning performance when used for: (1) fine-tuning models in an offline RL
setup for inference-time greedy-decoding, and (2) training reward models for
ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE
achieves competitive performance on challenging mathematical datasets while
offering 2.6 times greater efficiency, requiring only 38% of the runtime,
compared to tree search-based automatic annotation. The codebase, along with a
trained SPARE-PRM model, is publicly released to facilitate further research
and reproducibility.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SPAREï¼šå•éå‚è€ƒå¼•å¯¼è¯„ä¼°ï¼Œé©æ–°è‡ªåŠ¨è¿‡ç¨‹ç›‘ç£ä¸å¥–åŠ±å»ºæ¨¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿‡ç¨‹ç›‘ç£ï¼ˆæŒ‰æ­¥éª¤ç›‘ç£ï¼‰å¯¹æå‡LLMså¤šæ­¥æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†é«˜æ•ˆã€é«˜è´¨é‡çš„è‡ªåŠ¨è¿‡ç¨‹æ ‡æ³¨ä¸€ç›´æ˜¯éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼šåŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•è®¡ç®—ä½æ•ˆä¸”æœªå……åˆ†åˆ©ç”¨å‚è€ƒè§£ç­”çš„åˆ†æ­¥ä¿¡æ¯ï¼›å…¶ä»–åˆ©ç”¨å‚è€ƒæ¨ç†è½¨è¿¹çš„æ–¹æ³•è¦ä¹ˆä¾èµ–ç‰¹å®šé¢†åŸŸåˆ¤åˆ«æ¨¡å‹ã€é€‚ç”¨èŒƒå›´çª„ï¼Œè¦ä¹ˆä¾èµ–æ›´å¼ºå¤§æ¨¡å‹ç”Ÿæˆè¾…åŠ©æ•°æ®ï¼Œé™åˆ¶è¾ƒå¤šã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆä¸”èƒ½å……åˆ†åˆ©ç”¨å‚è€ƒä¿¡æ¯çš„è‡ªåŠ¨è¿‡ç¨‹æ ‡æ³¨æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSPAREæ¡†æ¶  
Single - Pass Annotation with Reference - Guided Evaluationï¼ˆSPAREï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„ç»“æ„åŒ–æ¡†æ¶ï¼Œé€šè¿‡å°†å€™é€‰è§£ç­”çš„æ¯ä¸ªæ­¥éª¤ä¸å‚è€ƒè§£ç­”ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ­¥éª¤å¯¹é½ï¼Œå¹¶è¾…ä»¥æ˜¾å¼æ¨ç†è¯„ä¼°ï¼Œå®ç°å•éã€é€æ­¥éª¤çš„æ ‡æ³¨ã€‚è¯¥æ¡†æ¶æ— éœ€é¢å¤–æ¨ç†è½¨è¿¹ï¼Œå¤ç”¨äº†æ ‡å‡†æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å·²æœ‰çš„å‚è€ƒè§£ç­”ï¼Œèšç„¦äºï¼ˆiï¼‰æ¯ä¸ªæ­¥éª¤è¯„ä¼°çš„æ˜¾å¼æ¨ç†ï¼›ï¼ˆiiï¼‰å€™é€‰è¾“å‡ºä¸å‚è€ƒä¹‹é—´çš„å¤šæ­¥éª¤å¯¹é½æ¯”è¾ƒï¼Œæ”¯æŒä¸å“åº”å’Œå‚è€ƒçš„tokené•¿åº¦æˆ additive scaling çš„å•éè¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒåœºæ™¯åº”ç”¨æå‡æ¨ç†æ€§èƒ½  
åˆ©ç”¨SPAREæ ‡æ³¨åœ¨ä¸¤ä¸ªåœºæ™¯æå‡LLMsæ¨ç†èƒ½åŠ›ï¼šï¼ˆiï¼‰åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®¾ç½®ä¸­å¾®è°ƒæ¨¡å‹ï¼Œç”¨äºæ¨ç†æ—¶çš„è´ªå¿ƒè§£ç ï¼›ï¼ˆiiï¼‰è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ï¼Œå¯¹å¤šä¸ªLLMç”Ÿæˆçš„è¾“å‡ºè¿›è¡Œæ’åºå’Œèšåˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. è·¨é¢†åŸŸæœ‰æ•ˆæ€§ï¼šåœ¨æ¶µç›–æ•°å­¦æ¨ç†ã€å¤šè·³ç»„åˆé—®ç­”ã€ç©ºé—´æ¨ç†ä¸‰ä¸ªé¢†åŸŸçš„å››ä¸ªæ•°æ®é›†ï¼ˆå¦‚æ•°å­¦æ¨ç†çš„GSM8Kã€MATHï¼Œé—®ç­”çš„MuSiQue - Ansï¼Œç©ºé—´æ¨ç†çš„SpaRPï¼‰ä¸Šï¼Œå‚è€ƒå¼•å¯¼çš„æ­¥éª¤çº§è¯„ä¼°æœ‰æ•ˆä¿ƒè¿›äº†è¿‡ç¨‹ç›‘ç£ã€‚
2. æ€§èƒ½ä¸æ•ˆç‡ä¼˜åŠ¿ï¼šä¸åŸºçº¿ç›¸æ¯”ï¼ŒSPAREç”¨äºç¦»çº¿RLå¾®è°ƒæ¨¡å‹å’Œè®­ç»ƒå¥–åŠ±æ¨¡å‹æ—¶å‡æå‡äº†æ¨ç†æ€§èƒ½ï¼›åœ¨å…·æŒ‘æˆ˜æ€§çš„æ•°å­¦æ•°æ®é›†ä¸Šï¼ŒSPAREæ€§èƒ½æœ‰ç«äº‰åŠ›ï¼Œä¸”æ¯”åŸºäºæ ‘æœç´¢çš„è‡ªåŠ¨æ ‡æ³¨æ•ˆç‡é«˜2.6å€ï¼Œä»…éœ€å…¶38%çš„è¿è¡Œæ—¶é—´ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šSPAREå¤ç”¨å·²æœ‰å‚è€ƒè§£ç­”è¿›è¡Œè¿‡ç¨‹æ ‡æ³¨çš„æ€è·¯ï¼Œä¸ºå……åˆ†åˆ©ç”¨ç°æœ‰æ•°æ®ã€å‡å°‘é¢å¤–æ•°æ®æ”¶é›†æˆæœ¬æä¾›äº†å€Ÿé‰´ï¼Œåœ¨éœ€è¿‡ç¨‹ç›‘ç£çš„ä»»åŠ¡ä¸­ï¼Œå¯æ€è€ƒå¦‚ä½•å¤ç”¨å·²æœ‰æ ‡æ³¨èµ„æºã€‚
2. å¤šåœºæ™¯åº”ç”¨æ¨¡å¼ï¼šå°†è¿‡ç¨‹æ ‡æ³¨ç”¨äºç¦»çº¿RLå¾®è°ƒä¸å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„åŒåœºæ™¯å®è·µï¼Œä¸ºæå‡LLMsæ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„æŠ€æœ¯è·¯çº¿å‚è€ƒï¼Œåç»­å¯æ¢ç´¢åœ¨æ›´å¤šä»»åŠ¡åœºæ™¯ä¸‹ç±»ä¼¼çš„åº”ç”¨æ‹“å±•ã€‚
3. æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡ï¼šåœ¨è¿½æ±‚æ€§èƒ½çš„åŒæ—¶å…³æ³¨æ•ˆç‡æå‡ï¼ŒSPAREåœ¨æ•°å­¦æ•°æ®é›†ä¸Šå±•ç°çš„é«˜æ•ˆæ ‡æ³¨ä¼˜åŠ¿ï¼Œæç¤ºåœ¨è®¾è®¡è‡ªåŠ¨æ ‡æ³¨æˆ–è¯„ä¼°æ–¹æ³•æ—¶ï¼Œéœ€å…¼é¡¾æ€§èƒ½ä¸è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæ¢ç´¢æ›´è½»é‡åŒ–é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚ 
4. å¼€æºèµ„æºä»·å€¼ï¼šå…¬å¼€ä»£ç åº“å’Œè®­ç»ƒå¥½çš„SPARE - PRMæ¨¡å‹ï¼Œä¾¿äºåç»­ç ”ç©¶å¤ç°ä¸æ‹“å±•ï¼Œè¿™ç§å¼€æ”¾å…±äº«çš„ç§‘ç ”å®è·µå€¼å¾—å­¦ä¹ ï¼Œåˆ©äºæ¨åŠ¨é¢†åŸŸå†…æŠ€æœ¯å¿«é€Ÿè¿­ä»£ã€‚

## reward-models-in-deep-reinforcement-learning--a-survey
### Abstract
In reinforcement learning (RL), agents continually interact with the
environment and use the feedback to refine their behavior. To guide policy
optimization, reward models are introduced as proxies of the desired
objectives, such that when the agent maximizes the accumulated reward, it also
fulfills the task designer's intentions. Recently, significant attention from
both academic and industrial researchers has focused on developing reward
models that not only align closely with the true objectives but also facilitate
policy optimization. In this survey, we provide a comprehensive review of
reward modeling techniques within the deep RL literature. We begin by outlining
the background and preliminaries in reward modeling. Next, we present an
overview of recent reward modeling approaches, categorizing them based on the
source, the mechanism, and the learning paradigm. Building on this
understanding, we discuss various applications of these reward modeling
techniques and review methods for evaluating reward models. Finally, we
conclude by highlighting promising research directions in reward modeling.
Altogether, this survey includes both established and emerging methods, filling
the vacancy of a systematic review of reward models in current literature.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±æ¨¡å‹å…¨æ™¯å¼ç»¼è¿°ï¼šä»åŸºç¡€åˆ°å‰æ²¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œå¥–åŠ±æ˜¯å¼•å¯¼æ™ºèƒ½ä½“ä¼˜åŒ–è¡Œä¸ºçš„æ ¸å¿ƒï¼Œä½†ç°å®åœºæ™¯é‡Œå¥–åŠ±å¸¸ç¼ºå¤±æˆ–éš¾å®šä¹‰ã€‚ç°æœ‰ç»¼è¿°å¤šèšç„¦é€†å¼ºåŒ–å­¦ä¹ ã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ç­‰å­é¢†åŸŸï¼Œç¼ºä¹å¯¹å¥–åŠ±æ¨¡å‹æœ¬èº«çš„ç³»ç»Ÿæ€§æ¢³ç†ã€‚åŒæ—¶ï¼Œä»¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸ºä»£è¡¨çš„åŸºç¡€æ¨¡å‹åœ¨å¥–åŠ±å»ºæ¨¡ä¸­å…´èµ·å´æœªè¢«å……åˆ†ç»¼è¿°ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨å¡«è¡¥ç©ºç™½ï¼Œå…¨é¢å›é¡¾æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å»ºæ¨¡æŠ€æœ¯ï¼Œæ¶µç›–åŸºç¡€ã€æ–¹æ³•ã€åº”ç”¨ä¸æœªæ¥æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç³»ç»Ÿæ€§åˆ†ç±»æ¡†æ¶æ„å»º
ä»**æ¥æº**ã€**æœºåˆ¶**ã€**å­¦ä¹ èŒƒå¼**ä¸‰ç»´åº¦æ„å»ºå¥–åŠ±æ¨¡å‹åˆ†ç±»æ¡†æ¶ã€‚æ¥æºä¸ŠåŒºåˆ†äººç±»æä¾›å‹ï¼ˆå«æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹ã€äººåœ¨ç¯å¥–åŠ±å­¦ä¹ ï¼Œåè€…åˆç»†åˆ†ä¸ºä»æ¼”ç¤ºã€ç›®æ ‡ã€åå¥½ä¸­å­¦ä¹ ï¼‰ä¸AIç”Ÿæˆå‹ï¼ˆä¾æ‰˜LLMã€VLMç­‰åŸºç¡€æ¨¡å‹ï¼‰ï¼›æœºåˆ¶ä¸Šåˆ†ä¸ºå†…åœ¨å¥–åŠ±ï¼ˆé©±åŠ¨æ™ºèƒ½ä½“è‡ªä¸»æ¢ç´¢ï¼‰ä¸å¤–åœ¨å¥–åŠ±ï¼ˆç¯å¢ƒç›´æ¥åé¦ˆï¼‰ï¼›å­¦ä¹ èŒƒå¼å›´ç»•ä¸åŒåé¦ˆç±»å‹å±•å¼€ï¼Œæ¸…æ™°æ¢³ç†å¥–åŠ±æ¨¡å‹çš„å¤šæ ·å½¢æ€ä¸æ„å»ºé€»è¾‘ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢è¦†ç›–æ–°å…´æ–¹å‘
èšç„¦åŸºç¡€æ¨¡å‹èµ‹èƒ½çš„å¥–åŠ±å»ºæ¨¡ï¼Œå¦‚LLMè§£è¯»äººç±»æ„å›¾è‡ªä¸»å®šä¹‰å¥–åŠ±ã€VLMå¤„ç†å¤šæ¨¡æ€åœºæ™¯ä¸‹å¥–åŠ±ç”Ÿæˆç­‰ï¼Œå¼¥è¡¥è¿‡å¾€ç»¼è¿°å¯¹è¿™ç±»æ–°å…´æ–¹æ³•å…³æ³¨ä¸è¶³çš„é—®é¢˜ï¼Œä¸ºé¢†åŸŸæ³¨å…¥æ–°è§†è§’ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå®Œæ•´ç”Ÿæ€æ¢³ç†
ä¸ä»…å‰–æå¥–åŠ±æ¨¡å‹æŠ€æœ¯æœ¬èº«ï¼Œè¿˜å»¶ä¼¸è‡³åº”ç”¨åœºæ™¯ï¼ˆå¦‚å¤æ‚æ¸¸æˆã€å¤§æ¨¡å‹å¯¹é½äººç±»æ„å›¾ã€å¤§è§„æ¨¡æ™ºèƒ½ä½“è®­ç»ƒï¼‰ä¸è¯„ä¼°æ–¹æ³•ï¼Œæ­å»ºä»æŠ€æœ¯åˆ°è½åœ°å†åˆ°æ•ˆæœéªŒè¯çš„å®Œæ•´çŸ¥è¯†é“¾æ¡ï¼ŒåŠ©åŠ›è¯»è€…ç†è§£å¥–åŠ±æ¨¡å‹åœ¨RLç”Ÿæ€ä¸­è§’è‰²ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªä¾§é‡ä¼ ç»Ÿå®éªŒæŒ‡æ ‡å¯¹æ¯”ï¼ˆå› å±ç»¼è¿°ï¼‰ï¼Œä½†é€šè¿‡å¯¹å¤§é‡å‰æ²¿è®ºæ–‡ï¼ˆå¦‚åŸºäºLLMçš„å¥–åŠ±è®¾è®¡ã€äººåœ¨ç¯å¥–åŠ±å­¦ä¹ æ¡ˆä¾‹ï¼‰çš„å½’çº³ï¼Œå±•ç°å„åˆ†ç±»ä¸‹æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ï¼ˆå¦‚AlphaGoçš„å†³ç­–å¥–åŠ±ã€InstructGPTçš„å¯¹é½å¥–åŠ±ï¼‰çš„æœ‰æ•ˆæ€§ï¼šæ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹åœ¨ Gym - MuJoCo  walker ä»»åŠ¡ä¸­é€šè¿‡ç»„åˆç”Ÿå­˜ã€ç§»åŠ¨ç­‰å¥–åŠ±å¼•å¯¼æ™ºèƒ½ä½“ï¼›äººåœ¨ç¯å­¦ä¹ ä»äººç±»æ¼”ç¤º/åå¥½ä¸­å­¦ä¹ çš„æ–¹å¼é™ä½äººå·¥è®¾è®¡éš¾åº¦ä¸”æå‡å¯¹é½åº¦ï¼›åŸºç¡€æ¨¡å‹ç”Ÿæˆå¥–åŠ±åœ¨å¤§æ¨¡å‹æ™ºèƒ½ä½“è®­ç»ƒï¼ˆå¦‚OpenAI - o1ï¼‰ä¸­å±•ç°å¼ºå¤§æ¨ç†å¼•å¯¼èƒ½åŠ›ï¼ŒéªŒè¯ä¸åŒå¥–åŠ±å»ºæ¨¡è·¯å¾„åœ¨å„è‡ªé€‚ç”¨åœºæ™¯çš„ä»·å€¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **åˆ†ç±»æ€ç»´**ï¼šä¸‰ç»´åº¦åˆ†ç±»æ¡†æ¶ä¸ºé¢†åŸŸå†…ç ”ç©¶æä¾›æ¸…æ™°â€œåæ ‡â€ï¼Œåç»­ç ”ç©¶å¯å¿«é€Ÿå®šä½è‡ªèº«æ–¹æ³•æ‰€å±ç»´åº¦ï¼Œä¹Ÿä¾¿äºæ¢³ç†é¢†åŸŸå‘å±•è„‰ç»œï¼Œç±»ä¼¼æ€è·¯å¯è¿ç§»è‡³å…¶ä»–AIå­é¢†åŸŸæŠ€æœ¯ç»¼è¿°ä¸åˆ†ç±»ã€‚
2. **æ–°å…´æŠ€æœ¯èåˆè§†è§’**ï¼šå…³æ³¨åŸºç¡€æ¨¡å‹ä¸RLå¥–åŠ±å»ºæ¨¡çš„ç»“åˆï¼Œå¯ç¤ºç ”ç©¶è€…é‡è§†è·¨æ¨¡æ€ã€å¤§æ¨¡å‹ç­‰å‰æ²¿æŠ€æœ¯å¯¹ä¼ ç»ŸRLæ¨¡å—çš„é©æ–°ï¼Œæ¨åŠ¨å­¦ç§‘äº¤å‰ã€‚
3. **ç”Ÿæ€åŒ–æ¢³ç†**ï¼šä»æŠ€æœ¯åˆ°åº”ç”¨å†åˆ°è¯„ä¼°çš„å…¨æµç¨‹è¦†ç›–ï¼Œæé†’ä»ä¸šè€…åšç ”ç©¶æˆ–è½åœ°æ—¶éœ€å…¼é¡¾ä¸Šä¸‹æ¸¸ç¯èŠ‚ï¼Œå¦‚è®¾è®¡å¥–åŠ±æ¨¡å‹è¦è€ƒè™‘åº”ç”¨åœºæ™¯ç‰¹æ€§ã€è¯„ä¼°æ‰‹æ®µåˆç†æ€§ï¼Œä¸ºé¡¹ç›®å…¨å‘¨æœŸè§„åˆ’æä¾›å‚è€ƒã€‚ 
4. **å¡«è¡¥é¢†åŸŸç©ºç™½ä»·å€¼**ï¼šä½œä¸ºé¦–ç¯‡ç³»ç»Ÿèšç„¦æ·±åº¦RLå¥–åŠ±æ¨¡å‹çš„ç»¼è¿°ï¼Œä¸ºå­¦ç•Œæä¾›äº†è¯¥æ–¹å‘çŸ¥è¯†åº•åº§ï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤å±•å¼€æ›´ç»†åˆ†ã€æ·±å…¥çš„æ¢ç´¢ï¼Œå¦‚ç‰¹å®šæ¥æºå¥–åŠ±æ¨¡å‹çš„ä¼˜åŒ–ã€è·¨æœºåˆ¶å¥–åŠ±èåˆç­‰ã€‚

## adaptive-accompaniment-with-realchords
### Abstract
Jamming requires coordination, anticipation, and collaborative creativity
between musicians. Current generative models of music produce expressive output
but are not able to generate in an \emph{online} manner, meaning simultaneously
with other musicians (human or otherwise). We propose ReaLchords, an online
generative model for improvising chord accompaniment to user melody. We start
with an online model pretrained by maximum likelihood, and use reinforcement
learning to finetune the model for online use. The finetuning objective
leverages both a novel reward model that provides feedback on both harmonic and
temporal coherency between melody and chord, and a divergence term that
implements a novel type of distillation from a teacher model that can see the
future melody. Through quantitative experiments and listening tests, we
demonstrate that the resulting model adapts well to unfamiliar input and
produce fitting accompaniment. ReaLchords opens the door to live jamming, as
well as simultaneous co-creation in other modalities.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReaLchordsï¼šå¼€å¯å®æ—¶éŸ³ä¹å³å…´ä¼´å¥æ–°ç¯‡ç« 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éŸ³ä¹å³å…´æ¼”å¥ï¼ˆJammingï¼‰éœ€è¦ä¹æ‰‹é—´çš„åè°ƒã€é¢„åˆ¤ä¸åä½œåˆ›é€ åŠ›ï¼Œä½†å½“å‰éŸ³ä¹ç”Ÿæˆæ¨¡å‹è™½èƒ½äº§å‡ºå¯Œæœ‰è¡¨ç°åŠ›çš„å†…å®¹ï¼Œå´æ— æ³•â€œåœ¨çº¿â€ï¼ˆonlineï¼‰ç”Ÿæˆï¼Œå³æ— æ³•ä¸å…¶ä»–ä¹æ‰‹ï¼ˆäººç±»æˆ–å…¶ä»–ï¼‰åŒæ­¥åˆ›ä½œã€‚åœ¨éŸ³ä¹ä¼´å¥åœºæ™¯ä¸­ï¼Œç°æœ‰æ¨¡å‹å¤šä¸ºç¦»çº¿ç”Ÿæˆï¼ˆå¦‚å…ˆæ‹¿åˆ°å®Œæ•´æ—‹å¾‹å†ç”Ÿæˆä¼´å¥ï¼‰ï¼Œä¸é€‚åº”å®æ—¶äº’åŠ¨ã€åŠ¨æ€è°ƒæ•´çš„éœ€æ±‚ï¼›ä¸”åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰é¢„è®­ç»ƒçš„æ¨¡å‹å­˜åœ¨â€œæš´éœ²åå·®â€ï¼Œåœ¨é¢å¯¹å®æ—¶åœºæ™¯ä¸­å¸¸è§çš„é”™è¯¯ã€é£æ ¼å˜åŒ–ç­‰æƒ…å†µæ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥é€‚é…é™Œç”Ÿè¾“å…¥å¹¶ç”Ÿæˆå¥‘åˆçš„ä¼´å¥ã€‚å› æ­¤ï¼Œæ‰“é€ ä¸€æ¬¾èƒ½åœ¨çº¿é€‚é…ç”¨æˆ·æ—‹å¾‹ã€å³å…´ç”Ÿæˆå’Œå¼¦ä¼´å¥çš„æ¨¡å‹æˆä¸ºå…³é”®éœ€æ±‚ï¼ŒReaLchords åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º ReaLchords åœ¨çº¿ä¼´å¥ç”Ÿæˆæ¨¡å‹  
ReaLchords èšç„¦äºä¸ºç”¨æˆ·æ—‹å¾‹å®æ—¶å³å…´ç”Ÿæˆå’Œå¼¦ä¼´å¥ï¼Œå®ƒå…ˆåŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡å®Œæˆé¢„è®­ç»ƒï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹æ¨¡å‹è¿›è¡Œåœ¨çº¿åœºæ™¯çš„å¾®è°ƒã€‚è¿™ç§æ–¹å¼è®©æ¨¡å‹èƒ½åœ¨ä¸çŸ¥é“åç»­æ—‹å¾‹çš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€å“åº”æ­£åœ¨ç”Ÿæˆçš„éŸ³ä¹å™äº‹ï¼Œæ»¡è¶³å®æ—¶äº’åŠ¨ä¸­â€œæ¡ä»¶ç‹¬ç«‹å‡è®¾â€çš„çº¦æŸï¼Œä¸ºç°åœºå³å…´æ¼”å¥ç­‰åœºæ™¯æä¾›æŠ€æœ¯æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆå¼ºåŒ–å­¦ä¹ ä¸è‡ªç›‘ç£å¥–åŠ±æ¨¡å‹ä¼˜åŒ–  
å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥æ–°é¢–çš„è‡ªç›‘ç£å¥–åŠ±æ¨¡å‹ï¼Œä»å’Œå£°ä¸æ—¶é—´è¿è´¯æ€§ç­‰å¤šä¸ªè§’åº¦ï¼Œä¸ºæ—‹å¾‹å’Œå’Œå¼¦ä¹‹é—´çš„éŸ³ä¹åè°ƒæ€§æä¾›åé¦ˆã€‚è¯¥å¥–åŠ±æ¨¡å‹æ— éœ€äººç±»æ ‡æ³¨ï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒå´èƒ½åœ¨äººç±»å¬è§‰æµ‹è¯•ä¸­å±•ç°å‡ºä¸äººç±»åå¥½çš„é«˜åº¦å¯¹é½ï¼Œæœ‰æ•ˆå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å…·éŸ³ä¹æ€§çš„ä¼´å¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºâ€œæœªæ¥å¯è§â€æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è’¸é¦  
è®¾è®¡äº†ä¸€ç§ä»â€œèƒ½çœ‹åˆ°æœªæ¥æ—‹å¾‹â€çš„æ•™å¸ˆæ¨¡å‹å‘â€œçœ‹ä¸åˆ°æœªæ¥â€çš„å­¦ç”Ÿæ¨¡å‹ï¼ˆå³åœ¨çº¿ç”Ÿæˆæ¨¡å‹ï¼‰è’¸é¦çŸ¥è¯†çš„æ–°é¢–æ–¹å¼ã€‚æ•™å¸ˆæ¨¡å‹å¯åˆ©ç”¨å®Œæ•´æ—‹å¾‹ä¿¡æ¯ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ™æ¨¡æ‹Ÿå®æ—¶åœºæ™¯åªèƒ½ä¾èµ–å†å²ä¿¡æ¯ï¼Œé€šè¿‡è¿™ç§è’¸é¦è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¼šâ€œé¢„åˆ¤â€ï¼Œå¤§å¹…æå‡ç”Ÿæˆè´¨é‡ï¼Œäººç±»è¯„ä¼°ç»“æœä¹ŸéªŒè¯äº†è’¸é¦å¯¹æ¨¡å‹æ•ˆæœçš„å¢ç›Šã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- æŠ—æ‰°åŠ¨ä¸é€‚é…æ€§ï¼šå¯¹æ¯”ä»…ç”¨ MLE è®­ç»ƒçš„åœ¨çº¿æ¨¡å‹ï¼ŒReaLchords åœ¨é¢å¯¹æµ‹è¯•é›†ä¸­æ—‹å¾‹ä¸­é€”çš„çªç„¶è½¬è°ƒç­‰æ‰°åŠ¨æ—¶ï¼Œèƒ½æ›´å¿«é€‚åº”ã€‚å¦‚ç¤ºä¾‹ä¸­ MLE æ¨¡å‹é¢„æµ‹å‡ºä¸åˆé€‚å’Œå¼¦åæ— æ³•è°ƒæ•´ï¼Œè€Œ ReaLchords è™½åˆæœŸä¹Ÿä¼šé¢„æµ‹ä¸ä½³ä½†èƒ½å¿«é€Ÿä¿®æ­£ï¼Œä¸”å®¢è§‚å’Œå£°è´¨é‡æŒ‡æ ‡ä¹Ÿä½è¯äº†è¿™ä¸€ä¼˜åŠ¿ã€‚  
- äººç±»è¯„ä¼°ä¸åå¥½å¯¹é½ï¼šå¥–åŠ±æ¨¡å‹åœ¨æ— äººç±»åé¦ˆè®­ç»ƒä¸‹ï¼Œç»äººç±»å¬è§‰æµ‹è¯•è¯æ˜ä¸äººç±»åå¥½é«˜åº¦å¥‘åˆï¼›åŒæ—¶ç»“åˆè’¸é¦ä¸ RL å¾®è°ƒåçš„æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆä¼´å¥çš„èŠ‚å¥ã€å’Œå£°è´¨é‡ç­‰ç»´åº¦è¡¨ç°æ›´ä¼˜ï¼Œå„ç»„ä»¶å¯¹æœ€ç»ˆæ•ˆæœå‡æœ‰æå‡è´¡çŒ®ã€‚  
- å®šé‡æŒ‡æ ‡éªŒè¯ï¼šé€šè¿‡é¢†åŸŸç‰¹å®šæŒ‡æ ‡åˆ†æï¼ˆå¦‚å’Œå£°ã€èŠ‚å¥ç›¸å…³åº¦é‡ï¼‰ï¼Œè¯å® RL å¾®è°ƒä¸­å„ç»„ä»¶ï¼ˆå¥–åŠ±æ¨¡å‹ã€è’¸é¦ç­‰ï¼‰å¯¹ç”Ÿæˆä¼´å¥åœ¨èŠ‚å¥å’Œå’Œå£°è´¨é‡ä¸Šçš„æ”¹è¿›ä½œç”¨ï¼Œæ—  RL å¾®è°ƒçš„æ¨¡å‹åœ¨åº”å¯¹é”™è¯¯å’Œæ‰°åŠ¨æ—¶è¡¨ç°æ‹‰èƒ¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- å®æ—¶äº’åŠ¨åœºæ™¯å»ºæ¨¡ï¼šä¸ºéŸ³ä¹ä»¥å¤–çš„å®æ—¶ååŒåˆ›ä½œï¼ˆå¦‚è§†è§‰è‰ºæœ¯åŒæ­¥å…±åˆ›ã€äº’åŠ¨å™äº‹ç”Ÿæˆç­‰ï¼‰æä¾›äº†â€œåœ¨çº¿ç”Ÿæˆ + å¼ºåŒ–å­¦ä¹ é€‚é…â€çš„æ€è·¯å‚è€ƒï¼Œå±•ç¤ºäº†å¦‚ä½•è®©ç”Ÿæˆæ¨¡å‹é€‚åº”åŠ¨æ€ã€ä¸å¯è§åç»­è¾“å…¥çš„åœºæ™¯ã€‚  
- è‡ªç›‘ç£å¥–åŠ±æœºåˆ¶ï¼šè¯æ˜æ— éœ€äººç±»æ ‡æ³¨çš„è‡ªç›‘ç£æ–¹å¼ä¹Ÿèƒ½è®­ç»ƒå‡ºä¸äººç±»åå¥½å¯¹é½çš„å¥–åŠ±æ¨¡å‹ï¼Œåœ¨å…¶ä»–åˆ›æ„é¢†åŸŸï¼ˆå¦‚ç»˜ç”»é£æ ¼ä¸€è‡´æ€§è¯„ä¼°ã€æ–‡å­¦åˆ›ä½œè¿è´¯æ€§æ‰“åˆ†ç­‰ï¼‰å¯å€Ÿé‰´è¯¥æ€è·¯æ„å»ºè‡ªç›‘ç£åé¦ˆä¿¡å·ã€‚  
- çŸ¥è¯†è’¸é¦æ–°èŒƒå¼ï¼šâ€œä»èƒ½çœ‹æœªæ¥çš„æ•™å¸ˆåˆ°åªèƒ½çœ‹å†å²çš„å­¦ç”Ÿâ€çš„è’¸é¦æ¨¡å¼ï¼Œä¸ºéœ€è¦â€œé¢„åˆ¤â€èƒ½åŠ›çš„ä»»åŠ¡ï¼ˆå¦‚å®æ—¶å¯¹è¯å›å¤ç”Ÿæˆã€å®æ—¶äº¤é€šè°ƒåº¦ç­–ç•¥å­¦ä¹ ç­‰ï¼‰æä¾›äº†è’¸é¦æ–¹å‘ä¸ç›®æ ‡è®¾å®šçš„åˆ›æ–°èŒƒå¼ï¼Œæ‹“å±•äº†çŸ¥è¯†è’¸é¦çš„åº”ç”¨è¾¹ç•Œã€‚

## senior--efficient-query-selection-and-preference-guided-exploration-in-preference-based-reinforcement-learning
### Abstract
Preference-based Reinforcement Learning (PbRL) methods provide a solution to
avoid reward engineering by learning reward models based on human preferences.
However, poor feedback- and sample- efficiency still remain the problems that
hinder the application of PbRL. In this paper, we present a novel efficient
query selection and preference-guided exploration method, called SENIOR, which
could select the meaningful and easy-to-comparison behavior segment pairs to
improve human feedback-efficiency and accelerate policy learning with the
designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We
designed a Motion-Distinction-based Selection scheme (MDS). It selects segment
pairs with apparent motion and different directions through kernel density
estimation of states, which is more task-related and easy for human preference
labeling; (2) We proposed a novel preference-guided exploration method (PGE).
It encourages the exploration towards the states with high preference and low
visits and continuously guides the agent achieving the valuable samples. The
synergy between the two mechanisms could significantly accelerate the progress
of reward and policy learning. Our experiments show that SENIOR outperforms
other five existing methods in both human feedback-efficiency and policy
convergence speed on six complex robot manipulation tasks from simulation and
four real-worlds.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SENIORï¼šåŸºäºåå¥½å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆæŸ¥è¯¢é€‰æ‹©ä¸åå¥½å¼•å¯¼æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼—å¤šå†³ç­–ä»»åŠ¡ä¸­å–å¾—æˆåŠŸï¼Œä½†å¥–åŠ±å‡½æ•°è®¾è®¡éšä»»åŠ¡å¤æ‚åº¦æå‡å˜å¾—å›°éš¾ã€‚åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡å­¦ä¹ äººç±»åå¥½æ¥é¿å…å¥–åŠ±å·¥ç¨‹ï¼Œç„¶è€Œåé¦ˆæ•ˆç‡ä¸æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜é˜»ç¢äº†å…¶åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©æ˜“æ ‡æ³¨çš„æœ‰æ„ä¹‰ç‰‡æ®µåŠè®©æ¢ç´¢ä¸åå¥½å…³è”æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› æ­¤æœ¬æ–‡æå‡ºSENIORæ–¹æ³•æ¥æå‡PbRLçš„åé¦ˆä¸æ¢ç´¢æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºè¿åŠ¨åŒºåˆ†çš„é€‰æ‹©æ–¹æ¡ˆï¼ˆMDSï¼‰
é€šè¿‡å¯¹çŠ¶æ€è¿›è¡Œæ ¸å¯†åº¦ä¼°è®¡ï¼Œé€‰æ‹©å…·æœ‰æ˜æ˜¾è¿åŠ¨ä¸”æ–¹å‘ä¸åŒçš„è¡Œä¸ºç‰‡æ®µå¯¹ã€‚è¿™ç±»ç‰‡æ®µæ›´ä¸ä»»åŠ¡ç›¸å…³ï¼Œä¹Ÿä¾¿äºäººç±»è¿›è¡Œåå¥½æ ‡æ³¨ï¼Œèƒ½ä¸ºå¥–åŠ±å­¦ä¹ è·å–é«˜è´¨é‡æ ‡ç­¾ï¼Œè§£å†³äº†ä»¥å¾€æŸ¥è¯¢é€‰æ‹©ä¸­æ— æ„ä¹‰æ•°æ®å¯¹å¤šã€æ ‡æ³¨éš¾çš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåå¥½å¼•å¯¼çš„æ¢ç´¢æ–¹æ³•ï¼ˆPGEï¼‰
ä»¥å†…åœ¨å¥–åŠ±çš„å½¢å¼ï¼Œé¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢é«˜åå¥½ä¸”ä½è®¿é—®çš„çŠ¶æ€ï¼ŒæŒç»­å¼•å¯¼æ™ºèƒ½ä½“è·å–æœ‰ä»·å€¼æ ·æœ¬ã€‚è®©æ¢ç´¢å’Œäººç±»åå¥½å…³è”èµ·æ¥ï¼Œå¼¥è¡¥äº†ä»¥å¾€RLæ¢ç´¢ä¸­è¾ƒå°‘å…³æ³¨åå¥½ç›¸å…³æ€§çš„ç¼ºé™·ï¼ŒåŠ é€Ÿç­–ç•¥å­¦ä¹ ã€‚å¹¶ä¸”MDSä¸PGEååŒä½œç”¨ï¼ŒMDSä¸ºPGEæä¾›å‡†ç¡®å¥–åŠ±æŒ‡å¯¼ï¼ŒPGEä¸ºMDSæä¾›æ›´æœ‰ä»·å€¼çš„æŸ¥è¯¢æ ·æœ¬ï¼Œå…±åŒæ¨åŠ¨å¥–åŠ±å’Œç­–ç•¥å­¦ä¹ è¿›ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨6ä¸ªæ¨¡æ‹Ÿçš„å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡å’Œ4ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ï¼ŒSENIORåœ¨äººç±»åé¦ˆæ•ˆç‡å’Œç­–ç•¥æ”¶æ•›é€Ÿåº¦ä¸Šå‡ä¼˜äºå…¶ä»–äº”ç§ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨æå‡PbRLæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åœ¨æŸ¥è¯¢é€‰æ‹©å±‚é¢ï¼Œå…³æ³¨ä»»åŠ¡ç›¸å…³çš„è¿åŠ¨ç­‰ä¿¡æ¯æ¥æŒ‘é€‰æ˜“æ ‡æ³¨ç‰‡æ®µï¼Œä¸ºç±»ä¼¼ä¾èµ–äººç±»åé¦ˆçš„å­¦ä¹ åœºæ™¯æä¾›äº†ä»ä»»åŠ¡ç‰¹å¾è§’åº¦ä¼˜åŒ–æ•°æ®é€‰æ‹©çš„æ€è·¯ã€‚
2. åœ¨æ¢ç´¢æœºåˆ¶å±‚é¢ï¼Œå°†äººç±»åå¥½èå…¥æ¢ç´¢è®¾è®¡ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢ä¸ä»»åŠ¡ç›®æ ‡ã€äººç±»æ„å›¾ç»“åˆæä¾›äº†åˆ›æ–°æ–¹å‘ï¼Œå¯å¯å‘åç»­åœ¨æå‡æ¢ç´¢é’ˆå¯¹æ€§ä¸Šçš„ç ”ç©¶ã€‚
3. å¤šä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿ+çœŸå®ä¸–ç•Œï¼‰çš„å®éªŒéªŒè¯æ¨¡å¼ï¼Œä¸ºæ–¹æ³•æ³›åŒ–æ€§éªŒè¯æä¾›äº†å‚è€ƒèŒƒå¼ï¼Œèƒ½è®©åç»­ç ”ç©¶æ›´æ³¨é‡æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§æ£€éªŒã€‚ 

## tgdpo--harnessing-token-level-reward-guidance-for-enhancing-direct-preference-optimization
### Abstract
Recent advancements in reinforcement learning from human feedback have shown
that utilizing fine-grained token-level reward models can substantially enhance
the performance of Proximal Policy Optimization (PPO) in aligning large
language models. However, it is challenging to leverage such token-level reward
as guidance for Direct Preference Optimization (DPO), since DPO is formulated
as a sequence-level bandit problem. To address this challenge, this work
decomposes the sequence-level PPO into a sequence of token-level proximal
policy optimization problems and then frames the problem of token-level PPO
with token-level reward guidance, from which closed-form optimal token-level
policy and the corresponding token-level reward can be derived. Using the
obtained reward and Bradley-Terry model, this work establishes a framework of
computable loss functions with token-level reward guidance for DPO, and
proposes a practical reward guidance based on the induced DPO reward. This
formulation enables different tokens to exhibit varying degrees of deviation
from reference policy based on their respective rewards. Experiment results
demonstrate that our method achieves substantial performance improvements over
DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on
AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at
https://github.com/dvlab-research/TGDPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TGDPOï¼šç”¨Tokençº§å¥–åŠ±æŒ‡å¯¼å¢å¼ºç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæå‡å¤§æ¨¡å‹å¯¹é½èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰é¢†åŸŸï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•å¸¸è¢«ç”¨äºè®©æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ï¼Œä½†ä¼ ç»Ÿåºåˆ—çº§å¥–åŠ±å­˜åœ¨ç¨€ç–æ€§ï¼ˆå¦‚å»¶è¿Ÿåé¦ˆï¼‰ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ ·æœ¬åˆ©ç”¨ä½æ•ˆã€‚è€Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è™½ç®€åŒ–äº†æµç¨‹ã€æ— éœ€å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä½†å®ƒæ˜¯åºåˆ—çº§çš„â€œè€è™æœºé—®é¢˜â€å½¢å¼ï¼Œéš¾ä»¥ç›´æ¥åˆ©ç”¨ç»†ç²’åº¦çš„Tokençº§å¥–åŠ±æ¥æŒ‡å¯¼ä¼˜åŒ–ã€‚å› æ­¤ï¼Œå¦‚ä½•æŠŠå¯¹PPOæœ‰æ•ˆçš„Tokençº§å¥–åŠ±ä¼˜åŠ¿å¼•å…¥DPOï¼Œæˆä¸ºå¾…è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåºåˆ—çº§PPOçš„Tokençº§åˆ†è§£ä¸æœ€ä¼˜ç­–ç•¥æ¨å¯¼  
è®ºæ–‡å°†åºåˆ—çº§çš„PPOåˆ†è§£ä¸ºä¸€ç³»åˆ—Tokençº§çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å­é—®é¢˜ï¼Œé€šè¿‡ä¸Šç•Œæ–¹æ³•é‡æ–°æ„å»ºé—®é¢˜åï¼Œæ¨å¯¼å‡ºäº†**é—­å¼çš„æœ€ä¼˜Tokençº§ç­–ç•¥**ï¼Œå¹¶å¾—åˆ°ä¸ä¹‹å¯¹åº”çš„Tokençº§å¥–åŠ±è¡¨ç¤ºï¼Œä¸ºåç»­æŠŠTokençº§å¥–åŠ±èå…¥DPOæ‰“ä¸‹åŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢å‘DPOçš„Tokençº§å¥–åŠ±æŒ‡å¯¼æ¡†æ¶ï¼ˆTGDPOï¼‰  
åŸºäºæ¨å¯¼å¾—åˆ°çš„Tokençº§å¥–åŠ±ï¼Œç»“åˆBradley - Terryæ¨¡å‹ä»¥åŠâ€œæ¶ˆé™¤é…åˆ†å‡½æ•°â€çš„æ–°ç†è®ºç»“æœï¼Œæå‡ºäº†**TGDPOæ¡†æ¶**â€”â€”ä¸ºDPOèµ‹äºˆTokençº§å¥–åŠ±æŒ‡å¯¼èƒ½åŠ›çš„åå¥½ä¼˜åŒ–ç®—æ³•ã€‚åŒæ—¶ï¼Œè¿˜è®¾è®¡äº†åŸºäºè¯±å¯¼DPOå¥–åŠ±çš„**å®ç”¨Tokençº§å¥–åŠ±æŒ‡å¯¼æ–¹æ¡ˆ**ï¼Œè®©ä¸åŒTokenèƒ½ä¾æ®è‡ªèº«å¥–åŠ±ï¼Œå‘ˆç°å‡ºä¸å‚è€ƒç­–ç•¥ä¸åŒç¨‹åº¦çš„åç¦»ï¼Œæ›´ç²¾ç»†åœ°è°ƒæ§ç”Ÿæˆè¿‡ç¨‹ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨AlpacaEval 2ã€MT - Benchã€Arena - Hardä¸‰å¤§æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­éªŒè¯æ•ˆæœï¼šä¸æœ€ä¼˜åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTGDPOåœ¨MT - Benchä¸Šèƒœç‡æå‡é«˜è¾¾7.5ä¸ªç™¾åˆ†ç‚¹ï¼ŒAlpacaEval 2ä¸Šæå‡6.2ä¸ªç™¾åˆ†ç‚¹ï¼ŒArena - Hardä¸Šæå‡4.3ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å±•ç°å‡ºTGDPOåœ¨æŸå¤±æ”¶æ•›æ—¶èƒ½å¾—åˆ°æ›´ä¼˜ç­–ç•¥ï¼ˆè¿™åœ¨ä¼ ç»Ÿåå¥½ä¼˜åŒ–æ–¹æ³•ä¸­ä¸å¸¸è§ï¼‰ã€å¯æ§åˆ¶æ”¶æ•›é€Ÿåº¦ä¸”å¯¹Tokençº§å¥–åŠ±å˜åŒ–é²æ£’ç­‰ä¼˜åŠ¿ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†è§£ä¸é‡æ„æ€è·¯ï¼šæŠŠåºåˆ—çº§é—®é¢˜æ‹†åˆ†ä¸ºTokençº§å­é—®é¢˜å¹¶æ¨å¯¼é—­å¼è§£ï¼Œè¿™ç§â€œåŒ–æ•´ä¸ºé›¶ + ç†è®ºæ¨å¯¼â€çš„æ€è·¯ï¼Œä¸ºå¤„ç†å¤§ç²’åº¦å¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†Tokençº§ç²¾ç»†è°ƒæ§çš„æ–°è§†è§’ã€‚  
2. è·¨ç®—æ³•èåˆåˆ›æ–°ï¼šæˆåŠŸå°†å¯¹PPOæœ‰æ•ˆçš„Tokençº§å¥–åŠ±èƒ½åŠ›å¼•å…¥DPOï¼Œè¯æ˜äº†ä¸åŒRLHFç®—æ³•é—´â€œä¼˜åŠ¿ç‰¹æ€§è¿ç§»â€çš„å¯è¡Œæ€§ï¼Œå¯å‘åç»­ç®—æ³•èåˆåˆ›æ–°ã€‚  
3. å®ç”¨å¥–åŠ±è®¾è®¡ï¼šæå‡ºçš„åŸºäºè¯±å¯¼å¥–åŠ±çš„Tokençº§æŒ‡å¯¼æ–¹æ¡ˆï¼Œä¸ºå®é™…è½åœ°æ—¶å¦‚ä½•åˆ©ç”¨ç»†ç²’åº¦åé¦ˆä¼˜åŒ–å¤§æ¨¡å‹ï¼Œæä¾›äº†å¯å‚è€ƒçš„å·¥ç¨‹åŒ–æ€è·¯ã€‚  

## adaptive-data-augmentation-for-thompson-sampling
### Abstract
In linear contextual bandits, the objective is to select actions that
maximize cumulative rewards, modeled as a linear function with unknown
parameters. Although Thompson Sampling performs well empirically, it does not
achieve optimal regret bounds. This paper proposes a nearly minimax optimal
Thompson Sampling for linear contextual bandits by developing a novel estimator
with the adaptive augmentation and coupling of the hypothetical samples that
are designed for efficient parameter learning. The proposed estimator
accurately predicts rewards for all arms without relying on assumptions for the
context distribution. Empirical results show robust performance and significant
improvement over existing methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸ºæ±¤æ™®æ£®é‡‡æ ·èµ‹èƒ½ï¼šè‡ªé€‚åº”æ•°æ®å¢å¼ºç ´è§£çº¿æ€§æƒ…å¢ƒ bandit é—æ†¾ç•Œéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
çº¿æ€§æƒ…å¢ƒ banditï¼ˆLinCBï¼‰æ˜¯åºåˆ—å†³ç­–é¢†åŸŸçš„ç»å…¸æ¡†æ¶ï¼Œç›®æ ‡æ˜¯é€šè¿‡é€‰æ‹©åŠ¨ä½œæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼ˆå¥–åŠ±ç”±å«æœªçŸ¥å‚æ•°çš„çº¿æ€§å‡½æ•°å»ºæ¨¡ï¼‰ã€‚æ±¤æ™®æ£®é‡‡æ ·ï¼ˆLinTSï¼‰åœ¨å®é™…ä¸­è¡¨ç°ä¸é”™ï¼Œä½†ç†è®ºä¸Šé—æ†¾ç•Œæœªè¾¾æœ€ä¼˜ï¼Œä¸æå°æå¤§ä¸‹ç•Œå­˜åœ¨å·®è·ã€‚ç°æœ‰ LinTS æ–¹æ³•è¦ä¹ˆé—æ†¾ç•Œè¾ƒé«˜ï¼ˆå¦‚ $\tilde{O}(d^{3/2}\sqrt{T})$ï¼‰ï¼Œè¦ä¹ˆä¾èµ–å¼ºå‡è®¾ï¼ˆå¦‚ç‹¬ç«‹åŒåˆ†å¸ƒä¸Šä¸‹æ–‡ã€ç‰¹å®šå…ˆéªŒç­‰ï¼‰é™åˆ¶äº†é€‚ç”¨èŒƒå›´ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ç¼ºå¤±æ•°æ®æŠ€æœ¯ä¼°è®¡æœªé€‰è‡‚å¥–åŠ±çš„æ–¹æ³•ä¼šå› é€†æ¦‚ç‡åŠ æƒå¼•å…¥éšè‡‚æ•°å¢é•¿çš„æ–¹å·®ï¼Œéœ€çªç ´è¿™äº›å±€é™æ¥å®ç°æ›´ä¼˜ç†è®ºä¿è¯ä¸æ™®é€‚æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå‡è®¾ bandit é—®é¢˜ä¸è‡ªé€‚åº”æ•°æ®å¢å¼º  
ä¸ºé«˜æ•ˆå­¦ä¹ å‚æ•°ï¼Œè®¾è®¡äº†é€‚é…çš„å‡è®¾ bandit é—®é¢˜ã€‚è¯¥å‡è®¾åœºæ™¯é‡‡ç”¨æ­£äº¤åŸºå‘é‡é›†ï¼Œåœ¨ä¿ç•™åŸå§‹ä¸Šä¸‹æ–‡åæ–¹å·®ç»“æ„çš„åŒæ—¶å¤§å¹…å‡å°‘â€œæœ‰æ•ˆè‡‚æ•°â€ï¼Œå®ç°å¯¹æ‰€æœ‰è‡‚ï¼ˆåŒ…æ‹¬æœªé€‰è‡‚ï¼‰å¥–åŠ±çš„å­¦ä¹ ï¼Œä¸å†ä¾èµ– IID æˆ–å¤šæ ·æ€§ç­‰å¼ºå‡è®¾ï¼Œé€šè¿‡è¿™ç§è‡ªé€‚åº”çš„æ•°æ®å¢å¼ºæ€è·¯æ¥ä¼˜åŒ–å‚æ•°ä¼°è®¡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè€¦åˆå‡è®¾ä¸åŸå§‹é—®é¢˜çš„æ–°å‹ä¼°è®¡å™¨  
å°†å‡è®¾ bandit ä¸åŸå§‹é—®é¢˜è€¦åˆï¼ŒåŸºäºåŒ…å«æ‰€æœ‰è‡‚ï¼ˆå«æœªé€‰ï¼‰ä¸Šä¸‹æ–‡çš„ Gram çŸ©é˜µï¼Œå¾—åˆ°äº†ä¸€ç§è‡ªå½’ä¸€åŒ–çš„æ–°ä¼°è®¡å™¨ã€‚è¯¥ä¼°è®¡å™¨èƒ½ç²¾å‡†é¢„æµ‹æ‰€æœ‰è‡‚çš„å¥–åŠ±ï¼Œä¸ºæ±¤æ™®æ£®é‡‡æ ·æä¾›æ›´å¯é çš„å‚æ•°åéªŒï¼ˆæˆ–ä¼°è®¡åˆ†å¸ƒï¼‰åŸºç¡€ï¼Œè¿›è€Œæ¨åŠ¨é—æ†¾ç•Œçš„ä¼˜åŒ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿‘æå°æå¤§æœ€ä¼˜çš„æ±¤æ™®æ£®é‡‡æ ·ç®—æ³•  
ä¾æ‰˜ä¸Šè¿°æ–°å‹ä¼°è®¡å™¨ï¼Œæå‡ºçš„ LinTS ç®—æ³•åœ¨æ— ä¸Šä¸‹æ–‡åˆ†å¸ƒå¼ºå‡è®¾ä¸‹ï¼Œå®ç°äº†å¯¹æ•°å› å­å†…çš„æå°æå¤§æœ€ä¼˜é—æ†¾ç•Œ $\tilde{O}(d\sqrt{T})$ï¼Œå¡«è¡¥äº†è¿‡å¾€ LinTS ç†è®ºé—æ†¾ç•Œä¸æå°æå¤§ä¸‹ç•Œé—´çš„å…³é”®ç¼ºå£ï¼Œåœ¨ç†è®ºä¿è¯ä¸Šæ›´ä¼˜ä¸”é€‚ç”¨åœºæ™¯æ›´æ™®é€‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­è™½æœªè¯¦ç»†å±•å¼€å®éªŒæ•°æ®å‘ˆç°ï¼Œä½†æ‘˜è¦ä¸å¼•è¨€æåŠâ€œå®è¯ç»“æœæ˜¾ç¤ºé²æ£’æ€§èƒ½ï¼Œç›¸è¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æå‡â€ï¼Œè¯´æ˜åœ¨å„ç±»åŸºå‡†åœºæ™¯ä¸‹ï¼Œæ–°ç®—æ³•åœ¨ç´¯ç§¯å¥–åŠ±ã€é—æ†¾æ§åˆ¶ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸å®ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºä¸å®è·µç»“åˆçš„æ€è·¯**ï¼šé’ˆå¯¹ç»å…¸ç®—æ³•ï¼ˆæ±¤æ™®æ£®é‡‡æ ·ï¼‰çš„ç†è®ºçŸ­æ¿ï¼Œé€šè¿‡ç»Ÿè®¡æ–¹æ³•ï¼ˆå¦‚ç¼ºå¤±æ•°æ®è§†è§’ã€å‡è®¾é—®é¢˜æ„é€ ï¼‰ä¸ç®—æ³•è®¾è®¡ï¼ˆè€¦åˆã€è‡ªé€‚åº”å¢å¼ºï¼‰ç»“åˆï¼Œå®ç°ç†è®ºä¿è¯çªç ´ï¼Œä¸ºæ”¹è¿›æ—¢æœ‰æˆç†Ÿç®—æ³•æä¾›äº†èŒƒæœ¬ã€‚  
2. **æ™®é€‚æ€§è®¾è®¡ç†å¿µ**ï¼šæ‘†è„±å¯¹ä¸Šä¸‹æ–‡åˆ†å¸ƒçš„å¼ºå‡è®¾ï¼ˆå¦‚ IIDã€ç‰¹å®šå…ˆéªŒï¼‰ï¼Œè®©ç®—æ³•èƒ½åœ¨æ›´å¤æ‚çœŸå®åœºæ™¯è½åœ°ï¼Œè¿™ç§â€œå»å‡è®¾åŒ–â€çš„è®¾è®¡æ€è·¯å¯¹é¢å‘å®é™…åº”ç”¨çš„ç®—æ³•ç ”å‘å¾ˆæœ‰å¯å‘ã€‚  
3. **è·¨é¢†åŸŸæŠ€æœ¯èåˆ**ï¼šå€Ÿé‰´é«˜ç»´å‚æ•°ä¼°è®¡ã€æœ€ä¼˜å®éªŒè®¾è®¡ã€ç¼ºå¤±æ•°æ®å¤„ç†ç­‰é¢†åŸŸæŠ€æœ¯é€‚é… bandit åœºæ™¯ï¼Œå±•ç¤ºäº†è·¨å­¦ç§‘æ–¹æ³•æ•´åˆåœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶çš„æ½œåŠ›ï¼Œä¸ºåç»­ç ”ç©¶æ‰“å¼€æ€è·¯ã€‚

## gram--a-generative-foundation-reward-model-for-reward-generalization
### Abstract
In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GRAMï¼šé¢å‘å¥–åŠ±æ³›åŒ–çš„ç”Ÿæˆå¼åŸºç¡€å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½å·¥ä½œä¸­ï¼Œå¥–åŠ±æ¨¡å‹æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¤šä»¥åˆ¤åˆ«å¼æ¨¡å‹å½¢å¼è®­ç»ƒï¼Œä¸”é«˜åº¦ä¾èµ–å¸¦æ ‡ç­¾çš„äººç±»åå¥½æ•°æ®ã€‚è¿™å¸¦æ¥äº†ä¸¤æ–¹é¢é—®é¢˜ï¼šä¸€æ˜¯å¼ºåŒ–å­¦ä¹ ç®—æ³•å¤æ‚åº¦ä¸æ ‡æ³¨æ•°æ®è·å–éš¾åº¦å¯¼è‡´å¥–åŠ±æ¨¡å‹åº”ç”¨æˆæœ¬é«˜æ˜‚ï¼›äºŒæ˜¯ç°æœ‰è®­ç»ƒæ–¹å¼å¯¹å¤§é‡æ— æ ‡ç­¾æ•°æ®åˆ©ç”¨ä¸è¶³ï¼Œéš¾ä»¥å¾—åˆ°èƒ½çµæ´»é€‚é…å¤šä»»åŠ¡çš„é€šç”¨å¥–åŠ±æ¨¡å‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢åˆ©ç”¨æ— æ ‡ç­¾å’Œæœ‰æ ‡ç­¾æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼Œæ‰“é€ å¯æ³›åŒ–çš„åŸºç¡€å¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µè®­ç»ƒçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¶æ„  
åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆå¼èƒ½åŠ›ï¼Œæ„å»ºç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRAMï¼‰ï¼Œåˆ†ä¸¤é˜¶æ®µè®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µåœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾çš„è¾“å…¥ - å“åº”æ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ è¾“å…¥ä¸å“åº”é—´çš„å¯¹åº”å…³ç³»ï¼Œæ— éœ€åå¥½æ ‡æ³¨æ•°æ®ï¼Œå¯è§„æ¨¡åŒ–è·å–å“åº”æ¯”è¾ƒçš„é€šç”¨çŸ¥è¯†ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨äººç±»åå¥½æ•°æ®è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šé¢„æµ‹ä¸¤ä¸ªå“åº”é—´çš„åå¥½å…³ç³»ã€‚æœ€ç»ˆå¾—åˆ°çš„åŸºç¡€å¥–åŠ±æ¨¡å‹èƒ½ç›´æ¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡æˆ–ä»…ç”¨å°‘é‡ä»»åŠ¡ç‰¹å®šæ•°æ®å¾®è°ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ ‡ç­¾å¹³æ»‘ä¸‹çš„æŸå¤±å‡½æ•°ç»Ÿä¸€è§†è§’  
å¼•å…¥æ ‡ç­¾å¹³æ»‘æŠ€æœ¯åˆ°å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¯æ˜æ­¤æ—¶è®­ç»ƒç›®æ ‡å¯è½¬åŒ–ä¸ºæ­£åˆ™åŒ–çš„ pairwise ranking lossï¼ˆBradley - Terry æŸå¤±ï¼‰å½¢å¼ã€‚è¿™ä¸€æˆæœåœ¨ä¸€å®šç¨‹åº¦ä¸Šç»Ÿä¸€äº†ç”Ÿæˆå¼æ¨¡å‹ä¸åˆ¤åˆ«å¼æ¨¡å‹çš„è®­ç»ƒç›®æ ‡è§†è§’ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°è®¤çŸ¥ï¼Œä¸”æ ‡ç­¾å¹³æ»‘å¯¹è®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ååˆ†æœ‰ç›Šï¼Œæå‡äº†æ¨¡å‹æ³›åŒ–æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å“åº”æ’åºã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€ä»»åŠ¡é€‚é…ç­‰å¤šä»»åŠ¡åœºæ™¯ä¸‹å¼€å±•å¤§é‡å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒGRAM åœ¨å‡ ä¹æ— éœ€æˆ–ä»…éœ€å°‘é‡å¾®è°ƒæ—¶ï¼Œåœ¨å„ä»»åŠ¡ä¸Šæ³›åŒ–æ€§å‡ºè‰²ã€‚ä¾‹å¦‚ï¼ŒåŸºäº LLaMA - 3.1 - 8B - Instruct æ¨¡å‹è®­ç»ƒå¥–åŠ±æ¨¡å‹æ—¶ï¼Œåœ¨ RewardBench å¹³å‡å‡†ç¡®ç‡ä¸Šï¼Œç›¸æ¯”æ™®é€šåˆ¤åˆ«å¼å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åˆ†åˆ«æå‡ 11.0 å’Œ 5.1 ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—è¶…è¶Šå¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆ©ç”¨æ€è·¯ï¼šæ‰“ç ´ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å¯¹æœ‰æ ‡ç­¾æ•°æ®çš„å¼ºä¾èµ–ï¼Œç¤ºèŒƒäº†æ— æ ‡ç­¾æ•°æ®åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸ºæ¨¡å‹æ³¨å…¥é€šç”¨çŸ¥è¯†çš„ä»·å€¼ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹ç”šè‡³å…¶ä»–æ¨¡å‹è®­ç»ƒåœ¨æ•°æ®åˆ©ç”¨ä¸Šå¼€è¾Ÿäº†â€œæ— æ ‡ç­¾ + æœ‰æ ‡ç­¾â€ç»“åˆçš„æ€è·¯ã€‚
2. æ¨¡å‹æ¶æ„åˆ›æ–°ï¼šä¸¤é˜¶æ®µçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¶æ„ä¸ºæ‰“é€ é€šç”¨åŸºç¡€å¥–åŠ±æ¨¡å‹æä¾›äº†å¯è¡ŒèŒƒå¼ï¼Œå±•ç¤ºäº†å…ˆæ— ç›‘ç£é¢„è®­ç»ƒå†ç›‘ç£å¾®è°ƒåœ¨å¥–åŠ±æ¨¡å‹é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œå¯å¯å‘åç»­å¤šæ¨¡æ€ã€å…¶ä»–ä»»åŠ¡å¯¼å‘æ¨¡å‹çš„æ¶æ„è®¾è®¡ã€‚
3. æŸå¤±å‡½æ•°ä¸æ­£åˆ™åŒ–ï¼šæ ‡ç­¾å¹³æ»‘ç»“åˆåå¯¹æŸå¤±å‡½æ•°çš„åˆ†æä¸è½¬åŒ–ï¼Œä¸ºç†è§£ç”Ÿæˆå¼å’Œåˆ¤åˆ«å¼æ¨¡å‹åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„è”ç³»æä¾›æ–°è§’åº¦ï¼Œä¹Ÿæé†’å¼€å‘è€…åœ¨æ¨¡å‹è®­ç»ƒä¸­é‡è§†æ­£åˆ™åŒ–æŠ€æœ¯å¯¹æ¨¡å‹æ³›åŒ–ç­‰èƒ½åŠ›çš„æå‡ä½œç”¨ã€‚

## vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training
### Abstract
Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large
language models but remains underexplored for Vision-Language (VL) models. The
Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing
structured feedback, yet training effective VL-RMs faces two major challenges.
First, the bootstrapping dilemma arises as high-quality training data depends
on already strong VL models, creating a cycle where self-generated supervision
reinforces existing biases. Second, modality bias and negative example
amplification occur when VL models hallucinate incorrect visual attributes,
leading to flawed preference data that further misguides training. To address
these issues, we propose an iterative training framework leveraging vision
experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection
Sampling. Our approach refines preference datasets, enhances structured
critiques, and iteratively improves reasoning. Experiments across VL-RM
benchmarks demonstrate superior performance in hallucination detection and
multimodal reasoning, advancing VL model alignment with reinforcement learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VL-GenRMï¼šå€Ÿè§†è§‰ä¸“å®¶ä¸è¿­ä»£è®­ç»ƒï¼Œçªç ´å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è®­ç»ƒå›°å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å·²å–å¾—è¿›å±•ï¼Œä½†åœ¨è§†è§‰-è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹ä¸­ä»å¾…æ·±å…¥æ¢ç´¢ã€‚è§†è§‰-è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼ˆVL-RMï¼‰æ˜¯å¯¹é½VLæ¨¡å‹çš„å…³é”®ï¼Œèƒ½æä¾›ç»“æ„åŒ–åé¦ˆï¼Œç„¶è€Œè®­ç»ƒé«˜æ•ˆVL-RMé¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯â€œè‡ªä¸¾å›°å¢ƒâ€ï¼Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ä¾èµ–å¼ºVLæ¨¡å‹ç”Ÿæˆï¼Œæ˜“å¼ºåŒ–æ¨¡å‹å›ºæœ‰åå·®ï¼›äºŒæ˜¯â€œæ¨¡æ€åå·®ä¸è´Ÿä¾‹æ”¾å¤§â€ï¼ŒVLæ¨¡å‹å¯¹è§†è§‰å±æ€§çš„é”™è¯¯å¹»æƒ³ä¼šäº§ç”Ÿæœ‰ç¼ºé™·çš„åå¥½æ•°æ®ï¼Œè¯¯å¯¼è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºåˆ›æ–°è®­ç»ƒæ¡†æ¶VL-GenRMã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå€Ÿè§†è§‰ä¸“å®¶è‡ªåŠ¨æ„å»ºåå¥½æ•°æ®é›†  
åˆ©ç”¨è§†è§‰ä¸“å®¶ï¼ˆå¦‚æ“…é•¿ç›®æ ‡æ£€æµ‹ã€æ·±åº¦ä¼°è®¡ç­‰çš„æ¨¡å‹ï¼‰ç”Ÿæˆå¤§è§„æ¨¡åå¥½æ•°æ®é›†ï¼Œæå‡VL-GenRMè®­ç»ƒæ—¶çš„ç›‘ç£è´¨é‡ï¼Œæ‰“ç ´â€œè‡ªä¸¾å›°å¢ƒâ€ä¸­ä¾èµ–è‡ªç”Ÿæˆæ•°æ®å¼ºåŒ–åå·®çš„é—®é¢˜ï¼Œä¸ºæ¨¡å‹æä¾›æ›´å¯é çš„è®­ç»ƒä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCoTå¢å¼ºVL-GenRMè®­ç»ƒ  
å¼•å…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºVL-GenRMè®­ç»ƒæä¾›ç³»ç»Ÿæ€§æŒ‡å¯¼ã€‚é€šè¿‡ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¢åŠ æ•°æ®ä¸­æœ‰æ•ˆæ­£ç¡®æè¿°å æ¯”ï¼Œç¼“è§£è‡ªç”Ÿæˆæ•°æ®çš„å±€é™æ€§ï¼Œå¼ºåŒ–å¥–åŠ±å»ºæ¨¡çš„è¿è´¯æ€§ï¼Œè®©æ¨¡å‹å­¦ä¹ æ›´åˆç†çš„è¯„ä¼°é€»è¾‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºè¾¹é™…æ‹’ç»é‡‡æ ·çš„è¿­ä»£è‡ªä¸¾  
é€šè¿‡å¯¹â€œæ­£ä¾‹ä¸è´Ÿä¾‹å¥–åŠ±ä¿¡å·è¾¹é™…â€ç­›é€‰å‡ºçš„ä¼˜è´¨æ¨ç†ä¾æ®ï¼Œè¿›è¡Œè¿­ä»£å¾®è°ƒï¼ŒæŒç»­ä¼˜åŒ–VL-GenRMçš„æ¨ç†èƒ½åŠ›ã€‚è®©æ¨¡å‹åœ¨å¤šè½®è®­ç»ƒä¸­é€æ­¥å‘æ›´ä¼˜è¾“å‡ºé€‚é…ï¼Œä¸æ–­æå‡å¯¹è§†è§‰-è¯­è¨€åœºæ™¯çš„è¯„ä¼°ä¸æ¨ç†æ°´å‡†ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨VL-RMåŸºå‡†æµ‹è¯•ä¸Best-of-Né‡‡æ ·ç­‰å®éªŒä¸­éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œåœ¨å¹»è§‰æ£€æµ‹ï¼ˆè¯†åˆ«VLæ¨¡å‹é”™è¯¯å¹»æƒ³è§†è§‰å±æ€§ç­‰é—®é¢˜ï¼‰ä¸å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´ä¼˜æ€§èƒ½ï¼Œæ¨åŠ¨äº†VLæ¨¡å‹å€ŸåŠ©å¼ºåŒ–å­¦ä¹ å®ç°æ›´å¥½å¯¹é½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨æ¨¡æ€é¢†åŸŸæ•°æ®å¢å¼ºæ€è·¯ï¼šå¼•å…¥é¢†åŸŸä¸“å®¶ï¼ˆå¦‚è§†è§‰ä¸“å®¶ï¼‰è¾…åŠ©æ„å»ºè®­ç»ƒæ•°æ®ï¼Œä¸ºçªç ´â€œè‡ªä¸¾å¾ªç¯â€æä¾›äº†æ–°èŒƒå¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€å¤šæ¨¡æ€åä½œã€ä¾èµ–æ•°æ®è´¨é‡çš„ä»»åŠ¡ã€‚  
2. ç»“æ„åŒ–æ¨ç†èå…¥è®­ç»ƒï¼šå€ŸåŠ©CoTå°†æ¨¡ç³Šçš„è¯„ä¼°è½¬åŒ–ä¸ºå¯è§£é‡Šçš„æ¨ç†æ­¥éª¤ï¼Œä¸ºæå‡æ¨¡å‹å¯è§£é‡Šæ€§ä¸è®­ç»ƒæœ‰æ•ˆæ€§æä¾›äº†å‚è€ƒï¼Œåœ¨å¤æ‚ä»»åŠ¡å‹æ¨¡å‹è®­ç»ƒä¸­å…·å€Ÿé‰´ä»·å€¼ã€‚  
3. è¿­ä»£å¼è®­ç»ƒç­–ç•¥ï¼šåŸºäºå¥–åŠ±è¾¹é™…çš„é‡‡æ ·ä¸è¿­ä»£å¾®è°ƒï¼Œè®©æ¨¡å‹èƒ½åŠ›é€æ­¥è¿­ä»£æå‡ï¼Œè¿™ç§â€œå°æ­¥å¿«è·‘ã€æ•°æ®æ‹©ä¼˜â€çš„è®­ç»ƒé€»è¾‘ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸å¤šè½®ä¼˜åŒ–åœºæ™¯ä¸­å€¼å¾—å¤ç”¨ã€‚  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å‘Šåˆ«ç¹çæ ‡æ³¨ï¼šGAN - RM è®©å¥–åŠ±å»ºæ¨¡â€œä»¥å‡ä¹±çœŸâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒåå¢å¼ºä¸­ï¼Œå¥–åŠ±æ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œå½“å‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•å­˜åœ¨è¯¸å¤šéš¾é¢˜ï¼šä¸€æ˜¯æ„å»ºå¥–åŠ±æ¨¡å‹éœ€å¤§é‡äººå·¥æ ‡æ³¨åå¥½æ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œä¸”åŸºäºç‰¹å®šç”Ÿæˆæ¨¡å‹è¾“å‡ºåŸŸæ ‡æ³¨çš„æ•°æ®ï¼Œåœ¨åº”ç”¨åˆ°ä¸åŒè¾“å‡ºåŸŸæ¨¡å‹æ—¶å­˜åœ¨åŸŸå·®è·ï¼›äºŒæ˜¯ä¸ºå…¨é¢è¯„ä¼°ç”Ÿæˆå†…å®¹è´¨é‡ï¼Œéœ€äººå·¥è®¾è®¡å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¢å¢åŠ å·¥ç¨‹æˆæœ¬ï¼Œåˆéš¾åœ¨ä¸åŒç»´åº¦é—´å–å¾—æœ€ä¼˜å¹³è¡¡ï¼Œè¿˜éš¾ä¿è¯ä¸äººç±»æ™®éåå¥½å¥‘åˆã€‚å› æ­¤ï¼Œæœ¬æ–‡å—ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸­å¯¹æŠ—è®­ç»ƒå¯å‘ï¼Œæå‡º GAN - RM æ¡†æ¶ï¼Œæ—¨åœ¨æ‘†è„±æ‰‹åŠ¨åå¥½æ ‡æ³¨å’Œæ˜¾å¼è´¨é‡ç»´åº¦è®¾è®¡ï¼Œé«˜æ•ˆæ„å»ºå¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— éœ€æ‰‹åŠ¨åå¥½æ ‡æ³¨ï¼Œåˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®
GAN - RM ä»…éœ€å°‘é‡ï¼ˆå‡ ç™¾ä¸ªï¼‰æ— æ ‡æ³¨çš„ä»£è¡¨æ€§æ ·æœ¬ï¼ˆå³åå¥½ä»£ç†æ•°æ®ï¼ŒPreference Proxy Dataï¼‰ä½œä¸ºå¤–éƒ¨æ•°æ®ã€‚é€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹åŒºåˆ†åå¥½ä»£ç†æ•°æ®å’Œç”Ÿæˆæ¨¡å‹è¾“å‡ºï¼Œè®©æ¨¡å‹å­¦ä¹ è¯„ä¼°ç”Ÿæˆæ ·æœ¬ã€‚åŒæ—¶é‡‡ç”¨åŸºäºæ’åçš„è‡ªä¸¾ç­–ç•¥ï¼Œå°† GAN - RM åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„ç½®ä¿¡åˆ†æ•°ä½œä¸ºè½¯æ ‡ç­¾ï¼Œåˆ©ç”¨é¢å¤–æ•°æ®å†è®­ç»ƒ GAN - RMï¼Œä½¿å…¶æ›´å¥½æ•æ‰æ½œåœ¨äººç±»åå¥½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šè½®è®­ç»ƒï¼Œè¿­ä»£å¯¹é½åå¥½
GAN - RM æ”¯æŒå¤šè½®è®­ç»ƒåä¼˜åŒ–ã€‚æ¯ä¸€è½®ä¸­ï¼Œå°†è¢«è¯†åˆ«ä¸ºæ¥è¿‘åå¥½ä»£ç†æ•°æ®çš„æ ·æœ¬ç”¨äºç”Ÿæˆå™¨çš„è®­ç»ƒåä¼˜åŒ–ï¼Œåè¿‡æ¥å†è®­ç»ƒåˆ¤åˆ«å™¨ä»¥åŒºåˆ†è¿™äº›æ›´éš¾çš„æ ·æœ¬ã€‚è¿™ç§è¿­ä»£çš„â€œä»¥å‡ä¹±çœŸâ€è¿‡ç¨‹èƒ½é€æ­¥è®©ç”Ÿæˆè´¨é‡ä¸åå¥½ä»£ç†æ•°æ®ä¸­çš„æ½œåœ¨äººç±»åå¥½å¯¹é½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒåŸºäº GAN - RM çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯ä¸ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼ˆå¦‚ Pickapic çš„ 100 ä¸‡æ ‡æ³¨äººç±»åå¥½æ•°æ®ï¼‰çš„æ–¹æ³•ï¼ˆå¦‚ç›¸å…³å¯¹æ¯”æ–¹æ³•ï¼‰ç›¸å½“ç”šè‡³è¶…è¶Šã€‚åœ¨å›¾åƒè´¨é‡å®éªŒè®¾ç½®ä¸­ï¼ŒGAN - RM ä»…éœ€ 500 ä¸ªåå¥½ä»£ç†æ•°æ®æ ·æœ¬ã€‚é™¤å›¾åƒè´¨é‡æå‡å®éªŒå¤–ï¼Œåœ¨å›¾åƒå®‰å…¨å’Œè§†é¢‘è´¨é‡å¢å¼ºåœºæ™¯ä¸‹çš„å®éªŒä¹Ÿå‡¸æ˜¾äº† GAN - RM æ¡†æ¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆå¦‚ Best - of - N æ ·æœ¬è¿‡æ»¤ï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰è®­ç»ƒåæ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼ŒGAN - RM ä¸ºè§£å†³å¥–åŠ±å»ºæ¨¡ä¸­æ•°æ®è·å–éš¾ã€ä¾èµ–ç‰¹å®šåŸŸã€äººå·¥è®¾è®¡ç»´åº¦éš¾å¥‘åˆäººç±»åå¥½ç­‰é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå…¶åˆ©ç”¨å¯¹æŠ—è®­ç»ƒå’Œå°‘é‡ä»£ç†æ•°æ®çš„æ–¹å¼ï¼Œå‡å°‘äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œé™ä½å·¥ç¨‹æˆæœ¬ï¼›ä»åº”ç”¨æ‹“å±•è§’åº¦ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒã€è§†é¢‘ç­‰å¤šåœºæ™¯çš„æœ‰æ•ˆå®éªŒï¼Œä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è®­ç»ƒåå¢å¼ºæä¾›äº†å¯å¤ç”¨çš„å¥–åŠ±å»ºæ¨¡èŒƒå¼ï¼Œåç»­åœ¨è§†è§‰ç”Ÿæˆç›¸å…³ä»»åŠ¡ä¸­ï¼Œè‹¥éœ€æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œå¯å€Ÿé‰´å…¶åˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®å’Œå¯¹æŠ—è®­ç»ƒçš„æ€è·¯æ¥é™ä½æˆæœ¬ä¸éš¾åº¦ã€‚

## pb$^2$--preference-space-exploration-via-population-based-methods-in-preference-based-reinforcement-learning
### Abstract
Preference-based reinforcement learning (PbRL) has emerged as a promising
approach for learning behaviors from human feedback without predefined reward
functions. However, current PbRL methods face a critical challenge in
effectively exploring the preference space, often converging prematurely to
suboptimal policies that satisfy only a narrow subset of human preferences. In
this work, we identify and address this preference exploration problem through
population-based methods. We demonstrate that maintaining a diverse population
of agents enables more comprehensive exploration of the preference landscape
compared to single-agent approaches. Crucially, this diversity improves reward
model learning by generating preference queries with clearly distinguishable
behaviors, a key factor in real-world scenarios where humans must easily
differentiate between options to provide meaningful feedback. Our experiments
reveal that current methods may fail by getting stuck in local optima,
requiring excessive feedback, or degrading significantly when human evaluators
make errors on similar trajectories, a realistic scenario often overlooked by
methods relying on perfect oracle teachers. Our population-based approach
demonstrates robust performance when teachers mislabel similar trajectory
segments and shows significantly enhanced preference exploration
capabilities,particularly in environments with complex reward landscapes.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PBÂ²ï¼šåŸºäºç¾¤ä½“æ–¹æ³•ç ´è§£åå¥½å¼ºåŒ–å­¦ä¹ ä¸­çš„åå¥½ç©ºé—´æ¢ç´¢éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼—å¤šé¢†åŸŸå–å¾—æˆåŠŸï¼Œä½†ä¼ ç»ŸRLä¾èµ–ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå¤æ‚ä»»åŠ¡ä¸­å› æ¶‰åŠä¸»è§‚ç»“æœæˆ–äººç±»åå¥½ï¼Œå¥–åŠ±å‡½æ•°éš¾ä»¥æŒ‡å®šã€‚åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡äººç±»å¯¹è½¨è¿¹å¯¹çš„åå¥½åé¦ˆå­¦ä¹ è¡Œä¸ºï¼Œæ— éœ€æ‰‹å·¥å¥–åŠ±å‡½æ•°ï¼Œä½†ç°æœ‰PbRLæ–¹æ³•å­˜åœ¨å…³é”®æŒ‘æˆ˜ï¼šæœ‰æ•ˆæ¢ç´¢åå¥½ç©ºé—´ä¸è¶³ï¼Œå¸¸è¿‡æ—©æ”¶æ•›åˆ°ä»…æ»¡è¶³ç‹­çª„äººç±»åå¥½å­é›†çš„æ¬¡ä¼˜ç­–ç•¥ã€‚å•æ™ºèƒ½ä½“æ–¹æ³•æ˜“ç”Ÿæˆç›¸ä¼¼è½¨è¿¹å¯¹ç”¨äºåå¥½ elicitationï¼Œé™åˆ¶åé¦ˆä¿¡æ¯å¤šæ ·æ€§ï¼Œå¥–åŠ±æ¨¡å‹å­¦ä¹ æ ·æœ¬å•ä¸€ï¼›ä¸”äººç±»è¯„ä¼°ç›¸ä¼¼è½¨è¿¹æ—¶åé¦ˆä¸ä¸€è‡´ä¼šä¸¥é‡é™ä½å­¦ä¹ æ€§èƒ½ï¼Œä¾èµ–â€œå®Œç¾å…ˆçŸ¥æ•™å¸ˆâ€çš„æ–¹æ³•å¸¸å¿½ç•¥è¿™ä¸€ç°å®åœºæ™¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯†åˆ«åå¥½æ¢ç´¢é—®é¢˜  
æ˜ç¡®PbRLä¸­åå¥½æ¢ç´¢éš¾é¢˜ï¼Œè®ºè¯å•æ™ºèƒ½ä½“æ–¹æ³•æ˜“åœ¨åå¥½ç©ºé—´æ”¶æ•›åˆ°æ¬¡ä¼˜å±€éƒ¨æå°å€¼ï¼Œå¯¼è‡´æ¢ç´¢ä¸è¶³ä¸ç­–ç•¥ä¸ä½³ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºç¾¤ä½“åŒ–PbRLæ¡†æ¶ï¼ˆPBÂ²ï¼‰  
é‡‡ç”¨ç¾¤ä½“æ–¹æ³•åŒæ—¶è®­ç»ƒå¤šä¸ªä¸åŒç­–ç•¥ï¼Œç›¸æ¯”å•ç­–ç•¥æ–¹æ³•æ›´å…¨é¢æ¢ç´¢åå¥½ landscapeã€‚é€šè¿‡ä¸åŒç­–ç•¥æ”¶é›†ç»éªŒæ„å»ºæ¯”è¾ƒå¯¹ï¼Œæå‡è¯„ä¼°è¡Œä¸ºçš„å¤šæ ·æ€§ï¼Œä¸”ä¸äººç±»åå¥½å¯¹é½ã€‚å€ŸåŠ©åé¦ˆå¾ªç¯å®ç°ï¼šå¤šæ ·ç­–ç•¥ç”Ÿæˆç‹¬ç‰¹è½¨è¿¹â†’äººç±»å¯¹è½¨è¿¹çš„åå¥½è®­ç»ƒå¥–åŠ±æ¨¡å‹â†’åˆ¤åˆ«å™¨ç»´æŒç¾¤ä½“å¤šæ ·æ€§å¹¶é¼“åŠ±ç¬¦åˆå½“å‰åå¥½çš„è¡Œä¸ºï¼Œè®©åå¥½æŸ¥è¯¢æ›´æ˜“åŒºåˆ†ï¼Œè§£å†³ç°æœ‰æ–¹æ³•éšå«å‡è®¾äººç±»èƒ½å¯é è¯„ä¼°ç»†å¾®å·®å¼‚è¡Œä¸ºçš„ä¸è¶³ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡ä¸‰ç±»å®éªŒéªŒè¯ï¼šï¼ˆ1ï¼‰åœ¨DMControl locomotionä»»åŠ¡ä¸Šè®¾ç½®ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼Ïµæ¨¡æ‹Ÿäººç±»åˆ¤æ–­ä¸ä¸€è‡´ï¼Œç³»ç»Ÿè¯„ä¼°æ–¹æ³•é²æ£’æ€§ï¼›ï¼ˆ2ï¼‰å®šæ€§å±•ç¤ºPBÂ²åœ¨å¤æ‚åå¥½ landscape ä¸­æ‘†è„±å±€éƒ¨æœ€ä¼˜ï¼Œè€Œå•æ™ºèƒ½ä½“æ–¹æ³•é™·å…¥å…¶ä¸­ï¼›ï¼ˆ3ï¼‰åœ¨åé¦ˆææœ‰é™çš„å¯¼èˆªä»»åŠ¡ä¸­å¯¹æ¯”åˆ†æï¼Œä½“ç°PBÂ²çš„åé¦ˆæ•ˆç‡ã€‚å®éªŒè¡¨æ˜PBÂ²ç”Ÿæˆæ›´æ˜“åŒºåˆ†çš„æŸ¥è¯¢æå‡å¥–åŠ±å­¦ä¹ æ•ˆç‡ï¼Œåœ¨äººç±»åé¦ˆä¸ä¸€è‡´æ—¶æ›´é²æ£’ï¼Œå°‘åé¦ˆä¸‹æ€§èƒ½æ›´ä¼˜ï¼›é¢å¯¹æ•™å¸ˆå¯¹ç›¸ä¼¼è½¨è¿¹æ®µè¯¯æ ‡åœºæ™¯ï¼Œä¹Ÿå±•ç°ç¨³å¥æ€§èƒ½ä¸æ›´å¼ºåå¥½æ¢ç´¢èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç¾¤ä½“åŒ–æ€è·¯æ‹“å±•ï¼šå°†ç¾¤ä½“æ–¹æ³•å¼•å…¥PbRLï¼Œä¸ºè§£å†³å•æ™ºèƒ½ä½“æ¢ç´¢å±€é™æä¾›æ–°èŒƒå¼ï¼Œå¯ç¤ºåœ¨ä¾èµ–äººç±»åé¦ˆã€éœ€è¡Œä¸ºå¤šæ ·æ€§çš„ä»»åŠ¡ä¸­ï¼Œå¯è€ƒè™‘å¤šæ™ºèƒ½ä½“å¹¶è¡Œæ¢ç´¢æ¨¡å¼ã€‚  
2. ç°å®åœºæ™¯é€‚é…ï¼šå…³æ³¨äººç±»åé¦ˆä¸ä¸€è‡´çš„ç°å®æƒ…å†µï¼Œè®¾è®¡é²æ£’æ–¹æ³•ï¼Œä¸ºå®é™…åº”ç”¨ä¸­å¤„ç†å™ªå£°åé¦ˆæä¾›å‚è€ƒï¼Œå¼ºè°ƒæ–¹æ³•è¦é€‚åº”çœŸå®ä¸–ç•Œéç†æƒ³æ•™å¸ˆçš„åœºæ™¯ã€‚  
3. åå¥½æ¢ç´¢ä¸å¥–åŠ±æ¨¡å‹ååŒï¼šé€šè¿‡è¡Œä¸ºå¤šæ ·æ€§è®©åå¥½æŸ¥è¯¢æ›´æ˜“åŒºåˆ†ï¼Œä¼˜åŒ–å¥–åŠ±æ¨¡å‹å­¦ä¹ ï¼Œæç¤ºåœ¨æ¶‰åŠäººç±»äº¤äº’çš„å­¦ä¹ ä»»åŠ¡é‡Œï¼Œé‡è§†äº¤äº’å†…å®¹çš„â€œå¯åŒºåˆ†æ€§â€ä»¥æå‡åé¦ˆä»·å€¼ã€‚

## $\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts
### Abstract
Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SPECSï¼šç”¨â€œæ¨æµ‹è‰ç¨¿â€åŠ é€Ÿå¤§æ¨¡å‹æ¨ç†ï¼Œå¹³è¡¡å»¶è¿Ÿä¸ç²¾åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æå‡å¸¸ä¾èµ–â€œæµ‹è¯•æ—¶ç®—åŠ›æ‰©å®¹â€ï¼Œæ¯”å¦‚åˆ†é…æ›´å¤šè®¡ç®—èµ„æºåšæ›´å……åˆ†çš„æ¢ç´¢ã€‚ä½†ç®—åŠ›å¢åŠ å¾€å¾€å¯¼è‡´ç”¨æˆ·ä¾§å»¶è¿Ÿå‡é«˜ï¼Œç›´æ¥å½±å“ä½“éªŒã€‚ç°æœ‰æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•å¤šèšç„¦ç®—åŠ›ï¼ˆFLOPSï¼‰ä¼˜åŒ–ç²¾åº¦ï¼Œå´å¿½ç•¥å»¶è¿Ÿçº¦æŸã€‚æ­¤å¤–ï¼ŒåŸºäºTransformerçš„LLMè‡ªå›å½’é‡‡æ ·å»¶è¿Ÿå¸¸å—é™äºå†…å­˜åŠ è½½è€Œéæ€»ç®—åŠ›ï¼Œè€Œæ¨æµ‹è§£ç è™½èƒ½å€Ÿå°æ¨¡å‹æå€™é€‰ token é™å»¶è¿Ÿï¼Œå´ä¼šå¢åŠ æ€»è®¡ç®—é‡ã€‚äºæ˜¯ï¼Œè®ºæ–‡è¯•å›¾å›ç­”ï¼š**èƒ½å¦è®¾è®¡é«˜æ•ˆæµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ï¼Œä¼˜åŒ–å»¶è¿Ÿ - æ•ˆç”¨æƒè¡¡ï¼Ÿ**

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º SPECS ç®—æ³•æ¡†æ¶  
SPECS å—æ¨æµ‹è§£ç å¯å‘ï¼Œæ˜¯ä¸€ç§â€œå»¶è¿Ÿæ„ŸçŸ¥å‹â€æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ã€‚å®ƒç”¨**æ›´å°æ›´å¿«çš„è‰ç¨¿æ¨¡å‹**é«˜æ•ˆç”Ÿæˆå€™é€‰åºåˆ—ï¼Œå†ç»“åˆ**æ›´å¤§çš„ç›®æ ‡æ¨¡å‹**ä¸**ä¸“ç”¨å¥–åŠ±æ¨¡å‹**è¯„ä¼°å€™é€‰ã€‚æ•´ä½“éµå¾ªâ€œè‰ç¨¿ - é€‰æ‹©â€æµç¨‹ï¼šè¿­ä»£ç”Ÿæˆå“åº”å—ï¼Œæ¯è½®ç”¨è‰ç¨¿æ¨¡å‹ç”Ÿæˆå€™é€‰å—ï¼Œç»æ‰“åˆ†é€‰æ‹©åæ‹¼æ¥ï¼Œè¿›å…¥ä¸‹ä¸€è½®ï¼›è‹¥è‰ç¨¿å…¨è¢«æ‹’ï¼Œåˆ™åˆ‡æ¢ç›®æ ‡æ¨¡å‹ç”Ÿæˆå€™é€‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¥–åŠ±å¼•å¯¼çš„è½¯éªŒè¯ä¸å»¶è¿Ÿæœºåˆ¶  
- å¥–åŠ±å¼•å¯¼è½¯éªŒè¯ï¼ˆSUBSAMPLE å­ä¾‹ç¨‹ï¼‰ï¼šåŸºäºè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡å‹è®¡ç®—çš„â€œåˆ†æ•°â€é€‰å€™é€‰å—ï¼Œæ—¢ä¼˜åŒ–æ•ˆç”¨ - å»¶è¿Ÿæƒè¡¡ï¼Œä¹Ÿé¿å…ç®€å•ä¸¢å¼ƒé«˜å¥–åŠ±ä½†å¯èƒ½è¢« naive æ¨æµ‹è§£ç æ¼æ‰çš„è½¨è¿¹ã€‚  
- å¥–åŠ±æ„ŸçŸ¥å»¶è¿Ÿè§„åˆ™ï¼ˆCASCADE å­ä¾‹ç¨‹ï¼‰ï¼šè‡ªé€‚åº”å†³å®šä¸‹ä¸€è½®ç”¨è‰ç¨¿è¿˜æ˜¯ç›®æ ‡æ¨¡å‹ç”Ÿæˆå€™é€‰â€”â€”è®©å¤§æ¨¡å‹å¤„ç†éš¾é¢˜æ­¥éª¤ï¼Œå°æ¨¡å‹å¤„ç†ç®€å•æ­¥éª¤ï¼ŒåŠ¨æ€å¹³è¡¡ç®—åŠ›ä¸å»¶è¿Ÿã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†è®ºåˆ†æä¿éšœæ”¶æ•›æ€§  
ä»ç†è®ºä¸Šåˆ†æï¼ŒSPECS åœ¨ç»“åˆè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡å‹ä¼˜åŒ–â€œKL æ­£åˆ™åŒ–å¥–åŠ±æœ€å¤§åŒ–â€ç›®æ ‡æ—¶ï¼Œå…¶è½¯éªŒè¯æ–¹æ³•éš beam å®½åº¦å¢å¤§ï¼Œèƒ½ä¼˜é›…æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ MATH500ã€AMC23ã€OlympiadBench æ•°æ®é›†æµ‹è¯•ï¼Œç”¨ Qwen - 1.5B - Instructï¼ˆè‰ç¨¿æ¨¡å‹ï¼‰ã€Qwen - 7B - Instructï¼ˆç›®æ ‡æ¨¡å‹ï¼‰ä¸ Qwen - 7B - Math - PRMï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰éªŒè¯ï¼š  
- ç²¾åº¦å±‚é¢ï¼šSPECS åŒ¹é…ç”šè‡³è¶…è¶Š beam search ç²¾åº¦ï¼›  
- å»¶è¿Ÿå±‚é¢ï¼šå»¶è¿Ÿæœ€å¤šé™ä½çº¦ 19.1%ï¼Œåœ¨ç²¾åº¦ä¸å»¶è¿Ÿé—´å®ç°æ›´ä¼˜æƒè¡¡ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å»¶è¿Ÿ - ç²¾åº¦æƒè¡¡æ€è·¯**ï¼šè·³å‡ºâ€œåªçœ‹ç®—åŠ›/ç²¾åº¦â€çš„æ€ç»´å®šå¼ï¼ŒæŠŠå»¶è¿Ÿä½œä¸ºæ ¸å¿ƒçº¦æŸï¼Œä¸ºå¤§æ¨¡å‹è½åœ°ä½å»¶è¿Ÿåœºæ™¯ï¼ˆå¦‚ä¸ªæ€§åŒ–äº¤äº’ï¼‰æä¾›æ–°æ€è·¯ï¼›  
2. **å¤šæ¨¡å‹åä½œèŒƒå¼**ï¼šç”¨â€œå°è‰ç¨¿æ¨¡å‹ + å¤§ç›®æ ‡æ¨¡å‹ + å¥–åŠ±æ¨¡å‹â€åˆ†å±‚åä½œï¼Œæ—¢åˆ©ç”¨å°æ¨¡å‹æé€Ÿï¼Œåˆé å¤§æ¨¡å‹ä¿ç²¾åº¦ï¼Œè¿˜å€Ÿå¥–åŠ±æ¨¡å‹åšçµæ´»é€‰æ‹©ï¼Œè¿™ç§â€œåˆ†å·¥â€æ¨¡å¼å¯è¿ç§»åˆ°å…¶ä»–éœ€å¹³è¡¡èµ„æºä¸æ•ˆæœçš„ä»»åŠ¡ï¼›  
3. **ç†è®º + å®éªŒåŒéªŒè¯**ï¼šä»ç†è®ºè¯æ˜æ”¶æ•›æ€§ï¼Œå†ç”¨çœŸå®æ•°æ®é›†éªŒè¯ï¼Œä¸ºæ–¹æ³•å¯é æ€§èƒŒä¹¦ï¼Œä¹Ÿç¤ºèŒƒäº†å­¦æœ¯ç ”ç©¶ä¸­â€œæ–¹æ³• - ç†è®º - å®éªŒâ€é—­ç¯çš„é‡è¦æ€§ã€‚  


SPECS ä¸ºå¤§æ¨¡å‹æ¨ç†çš„â€œå»¶è¿Ÿ - ç²¾åº¦â€éš¾é¢˜æä¾›äº†ä¸€å¥—å…¼å…·åˆ›æ–°æ€§ä¸å®ç”¨æ€§çš„è§£æ³•ï¼Œæ— è®ºæ˜¯å·¥ä¸šç•Œè½åœ°ä½å»¶è¿Ÿ LLM åº”ç”¨ï¼Œè¿˜æ˜¯å­¦æœ¯ç•Œæ¢ç´¢æµ‹è¯•æ—¶ä¼˜åŒ–æ–°æ–¹å‘ï¼Œéƒ½æœ‰ä¸å°‘å¯å€Ÿé‰´çš„é—ªå…‰ç‚¹~

## from-outcomes-to-processes--guiding-prm-learning-from-orm-for-inference-time-alignment
### Abstract
Inference-time alignment methods have gained significant attention for their
efficiency and effectiveness in aligning large language models (LLMs) with
human preferences. However, existing dominant approaches using reward-guided
search (RGS) primarily rely on outcome reward models (ORMs), which suffer from
a critical granularity mismatch: ORMs are designed to provide outcome rewards
for complete responses, while RGS methods rely on process rewards to guide the
policy, leading to inconsistent scoring and suboptimal alignment. To address
this challenge, we introduce process reward models (PRMs) into RGS and argue
that an ideal PRM should satisfy two objectives: Score Consistency, ensuring
coherent evaluation across partial and complete responses, and Preference
Consistency, aligning partial sequence assessments with human preferences.
Based on these, we propose SP-PRM, a novel dual-consistency framework
integrating score consistency-based and preference consistency-based partial
evaluation modules without relying on human annotation. Extensive experiments
on dialogue, summarization, and reasoning tasks demonstrate that SP-PRM
substantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement
in GPT-4 evaluation scores across all tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»ç»“æœåˆ°è¿‡ç¨‹ï¼šç”¨ORMå¼•å¯¼PRMå®ç°æ¨ç†æ—¶å¯¹é½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸ä¸äººç±»åå¥½ä¸ä¸€è‡´ã€‚è®­ç»ƒåå¯¹é½æ–¹æ³•ï¼ˆå¦‚SFTã€RLHFï¼‰è®¡ç®—æˆæœ¬é«˜ä¸”éœ€é‡æ–°è®­ç»ƒï¼Œæ¨ç†æ—¶å¯¹é½æˆä¸ºæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚å¥–åŠ±å¼•å¯¼æœç´¢ï¼ˆRGSï¼‰æ˜¯ä¸»æµæ¨ç†æ—¶å¯¹é½æ¡†æ¶ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰ï¼Œå­˜åœ¨å…³é”®çš„ç²’åº¦ä¸åŒ¹é…é—®é¢˜ï¼šORMsä¸ºå®Œæ•´å“åº”æä¾›ç»“æœå¥–åŠ±ï¼Œè€ŒRGSæ–¹æ³•ä¾èµ–è¿‡ç¨‹å¥–åŠ±æŒ‡å¯¼ç­–ç•¥ï¼Œå¯¼è‡´è¯„åˆ†ä¸ä¸€è‡´å’Œå¯¹é½æ¬¡ä¼˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¹¶æå‡ºåŒä¸€è‡´æ€§ç›®æ ‡  
ä¸ºè§£å†³ç²’åº¦ä¸åŒ¹é…æŒ‘æˆ˜ï¼Œå¼•å…¥PRMåˆ°RGSä¸­ï¼Œå¹¶æå‡ºç†æƒ³PRMåº”æ»¡è¶³çš„ä¸¤ä¸ªç›®æ ‡ï¼šåˆ†æ•°ä¸€è‡´æ€§ï¼ˆScore Consistencyï¼‰ï¼Œç¡®ä¿å¯¹éƒ¨åˆ†å’Œå®Œæ•´å“åº”çš„è¯„ä¼°è¿è´¯ï¼›åå¥½ä¸€è‡´æ€§ï¼ˆPreference Consistencyï¼‰ï¼Œä½¿éƒ¨åˆ†åºåˆ—è¯„ä¼°ä¸äººç±»åå¥½å¯¹é½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSP - PRMåŒä¸€è‡´æ€§æ¡†æ¶  
SP - PRMæ˜¯ä»ORMè¯±å¯¼PRMçš„æ–°é¢–åŒä¸€è‡´æ€§æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚åˆ†æ•°ä¸€è‡´æ€§æ¨¡å—é€šè¿‡å°†å®Œæ•´å“åº”åˆ†è§£ä¸ºéƒ¨åˆ†åºåˆ—ï¼ŒåŸºäºBradley - Terryæ¨¡å‹å®ç°å¥–åŠ±å»ºæ¨¡ï¼Œè§£å†³ORMå›ºæœ‰çš„ç²’åº¦ä¸åŒ¹é…ï¼Œæ•æ‰é•¿æœŸä¾èµ–ï¼›åå¥½ä¸€è‡´æ€§æ¨¡å—åˆ©ç”¨å¼ºå¥–åŠ±æ¨¡å‹ä½œä¸ºå‚è€ƒæ¨¡å‹è®¡ç®—éƒ¨åˆ†åºåˆ—ç†µï¼Œé‡æ–°åŠ æƒå…¶å¯¹è®­ç»ƒè¿‡ç¨‹çš„è´¡çŒ®ï¼Œä½¿éƒ¨åˆ†åºåˆ—çš„PRMå¥–åŠ±ä¸äººç±»åå¥½å¯¹é½ã€‚è¯¥æ¡†æ¶æ— éœ€äººå·¥æ ‡æ³¨ï¼Œä»ORMè·å–æŒ‡å¯¼ï¼ŒåŒæ—¶å…¼é¡¾éƒ¨åˆ†ä¸Šä¸‹æ–‡çš„é•¿æœŸå¯¹é½å’Œäººç±»åå¥½ä¸€è‡´æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¯¹è¯ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦å’Œå¤æ‚æ¨ç†ä¸‰ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œåº”ç”¨äº1Båˆ°3Bå‚æ•°çš„æ¨¡å‹æ¶æ„ã€‚ç»“æœè¡¨æ˜ï¼ŒSP - PRMæ˜¾è‘—å¢å¼ºç°æœ‰RGSæ–¹æ³•ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡çš„GPT - 4è¯„ä¼°åˆ†æ•°ä¸­å®ç°äº†3.6% - 10.3%çš„æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ç²’åº¦ä¸åŒ¹é…é—®é¢˜ï¼Œæå‡ºä»ç»“æœå¥–åŠ±æ¨¡å‹æ‹“å±•åˆ°è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ€è·¯ï¼Œä¸ºæ¨ç†æ—¶å¯¹é½æ–¹æ³•çš„æ”¹è¿›æä¾›äº†æ–°æ–¹å‘ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨å¥–åŠ±æ¨¡å‹åœ¨ä¸åŒåºåˆ—ç²’åº¦ä¸‹çš„ä¸€è‡´æ€§é—®é¢˜ã€‚  
2. è®¾è®¡çš„åŒä¸€è‡´æ€§æ¡†æ¶SP - PRMï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯åˆ©ç”¨ç°æœ‰ORMèµ„æºæ¥è®­ç»ƒPRMï¼Œåœ¨èµ„æºåˆ©ç”¨å’Œå‡å°‘æ ‡æ³¨æˆæœ¬æ–¹é¢æä¾›äº†å¯å€Ÿé‰´çš„èŒƒå¼ï¼Œå¯¹äºæ¨åŠ¨æ¨ç†æ—¶å¯¹é½æŠ€æœ¯åœ¨å®é™…åœºæ™¯çš„åº”ç”¨æœ‰å‚è€ƒä»·å€¼ã€‚  
3. åœ¨å¤šä¸ªä»»åŠ¡å’Œä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šçš„æœ‰æ•ˆå®éªŒï¼ŒéªŒè¯äº†æ–¹æ³•çš„é€šç”¨æ€§å’Œé²æ£’æ€§ï¼Œä¸ºåç»­ç›¸å…³ä»»åŠ¡ï¼ˆå¦‚å¯¹è¯ã€æ‘˜è¦ã€æ¨ç†ç­‰ï¼‰ä¸­æ”¹è¿›RGSæ–¹æ³•æä¾›äº†å®éªŒè®¾è®¡å’Œæ–¹æ³•åº”ç”¨çš„å‚è€ƒã€‚

## theoretical-tensions-in-rlhf--reconciling-empirical-success-with-inconsistencies-in-social-choice-theory
### Abstract
Despite its empirical success, Reinforcement Learning from Human Feedback
(RLHF) has been shown to violate almost all the fundamental axioms in social
choice theory -- such as majority consistency, pairwise majority consistency,
and Condorcet consistency. This raises a foundational question: why does RLHF
perform so well in practice if it fails these seemingly essential properties?
In this paper, we resolve this paradox by showing that under mild and
empirically plausible assumptions on the preference profile, RLHF does satisfy
pairwise majority and Condorcet consistency. These assumptions are frequently
satisfied in real-world alignment tasks, offering a theoretical explanation for
RLHF's strong practical performance. Furthermore, we show that a slight
modification to the reward modeling objective can ensure pairwise majority or
Condorcet consistency even under general preference profiles, thereby improving
the alignment process. Finally, we go beyond classical axioms in economic and
social choice theory and introduce new alignment criteria -- preference
matching, preference equivalence, and group preference matching -- that better
reflect the goal of learning distributions over responses. We show that while
RLHF satisfies the first two properties, it fails to satisfy the third. We
conclude by discussing how future alignment methods may be designed to satisfy
all three.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RLHFå®è·µæˆåŠŸèƒŒåï¼šè°ƒå’Œç¤¾ä¼šé€‰æ‹©ç†è®ºçŸ›ç›¾çš„ç†è®ºæ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯è®©æ¨¡å‹è¡Œä¸ºä¸äººç±»æœŸæœ›å¯¹é½çš„å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼ŒRLHFåœ¨ç†è®ºå±‚é¢å´ä¸ç¤¾ä¼šé€‰æ‹©ç†è®ºçš„è¯¸å¤šåŸºç¡€å…¬ç†ï¼ˆå¦‚å¤šæ•°ä¸€è‡´æ€§ã€æˆå¯¹å¤šæ•°ä¸€è‡´æ€§ã€å­”å¤šå¡ä¸€è‡´æ€§ï¼‰ç›¸è¿èƒŒã€‚è¿™å°±äº§ç”Ÿäº†ä¸€ä¸ªæ ¸å¿ƒç–‘é—®ï¼šRLHFåœ¨å®è·µä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯ä¸ºä½•åœ¨ç†è®ºä¸Šè¿èƒŒè¿™äº›çœ‹ä¼¼å…³é”®çš„å…¬ç†ï¼Ÿæœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³è¿™ä¸€çŸ›ç›¾ã€ä»ç†è®ºå±‚é¢è§£é‡ŠRLHFå®è·µæˆåŠŸåŸå› å¹¶æ¢ç´¢æ”¹è¿›æ–¹å‘è€Œå±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºRLHFæ»¡è¶³å…¬ç†çš„å‰ææ¡ä»¶  
åœ¨å¯¹åå¥½åˆ†å¸ƒåšæ¸©å’Œä¸”ç¬¦åˆå®è¯çš„å‡è®¾ä¸‹ï¼Œè¯æ˜RLHFèƒ½æ»¡è¶³æˆå¯¹å¤šæ•°ä¸€è‡´æ€§ä¸å­”å¤šå¡ä¸€è‡´æ€§ã€‚ç°å®ä¸­å¯¹é½ä»»åŠ¡é‡Œå¸¸è§çš„åå¥½æ•°æ®ç»“æ„ï¼ˆæ¯”å¦‚æ¯ä¸ªæ¯”è¾ƒæœ€å¤šç”±ä¸€ä¸ªæ ‡æ³¨è€…æ ‡æ³¨ã€å…è®¸å¾ªç¯åå¥½ç­‰æƒ…å†µæ„æˆçš„åå¥½åˆ†å¸ƒï¼‰ï¼Œè®©RLHFå¯ä»¥æ­£ç¡®è¯†åˆ«å­”å¤šå¡èƒœè€…ï¼ˆè‹¥å­˜åœ¨ï¼‰å¹¶èµ‹äºˆæœ€é«˜åˆ†æ•°ï¼Œä»ç†è®ºä¸Šè§£é‡Šäº†RLHFå®è·µæ•ˆæœå¥½çš„åŸå› ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¹è¿›å¥–åŠ±æ¨¡å‹ç›®æ ‡ä»¥ä¿éšœå…¬ç†æ»¡è¶³  
é’ˆå¯¹å¤šä¸ªæ ‡æ³¨è€…å‚ä¸åŒä¸€æ¯”è¾ƒçš„åœºæ™¯ï¼Œå¯¹å¥–åŠ±æ¨¡å‹ç›®æ ‡åšå¾®å°ä¿®æ”¹â€”â€”åŸºäºå¤šæ•°æŠ•ç¥¨èšåˆåå¥½å¹¶èµ‹äºˆäºŒå…ƒåå¥½ã€‚è¿™ç§ä¿®æ”¹è®©å¥–åŠ±æ¨¡å‹èƒ½æ»¡è¶³å¤šæ•°ä¸€è‡´æ€§ã€æˆå¯¹å¤šæ•°ä¸€è‡´æ€§å’Œå­”å¤šå¡ä¸€è‡´æ€§ï¼Œå…¶æœ¬è´¨æ˜¯è®©å¥–åŠ±å»ºæ¨¡éšå¼å®ç°äº†ç¤¾ä¼šé€‰æ‹©ç†è®ºä¸­æ»¡è¶³ä¼˜è‰¯å…¬ç†çš„Copelandè§„åˆ™ï¼Œä¸ºç°æœ‰å®è·µæä¾›ç†è®ºæ”¯æ’‘åŒæ—¶ï¼Œä¹Ÿä¸ºæœªæ¥å¯¹é½ç­–ç•¥æŒ‡æ˜æ–¹å‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºé€‚é…LLMå¯¹é½çš„æ–°å…¬ç†  
çªç ´ç»æµä¸ç¤¾ä¼šé€‰æ‹©ç†è®ºçš„ç»å…¸å…¬ç†æ¡†æ¶ï¼Œæå‡ºåå¥½åŒ¹é…ã€åå¥½ç­‰ä»·ã€ç¾¤ä½“åå¥½åŒ¹é…è¿™ä¸‰ä¸ªæ›´è´´åˆç”Ÿæˆæ¨¡å‹å­¦ä¹ å“åº”åˆ†å¸ƒç›®æ ‡çš„æ–°å¯¹é½å‡†åˆ™ã€‚è¯æ˜ç›®æ ‡åˆ†å¸ƒæ˜¯å®šä¹‰è‰¯å¥½ã€å­˜åœ¨ä¸”å”¯ä¸€çš„ï¼Œè¿˜åˆ†æå‡ºRLHFæ»¡è¶³å‰ä¸¤ä¸ªå‡†åˆ™ä½†ä¸æ»¡è¶³ç¬¬ä¸‰ä¸ªï¼Œä¸ºæœªæ¥æ–¹æ³•å¦‚ä½•æ»¡è¶³å…¨éƒ¨å‡†åˆ™æä¾›æ€è·¯ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªå¼€å±•ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®éªŒéªŒè¯ï¼ˆä¾§é‡ç†è®ºåˆ†ææ¨å¯¼ï¼‰ï¼Œé€šè¿‡ä¸¥è°¨çš„ç†è®ºæ¨å¯¼ä¸è®ºè¯ï¼Œé˜æ˜äº†åœ¨ç‰¹å®šåå¥½åˆ†å¸ƒå‡è®¾ä¸‹RLHFæ»¡è¶³ç»å…¸ç¤¾ä¼šé€‰æ‹©å…¬ç†çš„æƒ…å†µã€ä¿®æ”¹å¥–åŠ±æ¨¡å‹ç›®æ ‡åçš„å…¬ç†æ»¡è¶³æ€§ï¼Œä»¥åŠæ–°æå‡ºçš„ä¸‰ä¸ªåˆ†å¸ƒå±‚é¢å…¬ç†ä¸RLHFçš„å¥‘åˆæƒ…å†µç­‰å…³é”®ç»“è®ºï¼Œä»ç†è®ºè§’åº¦æ”¯æ’‘äº†å„åˆ›æ–°ç‚¹çš„åˆç†æ€§ä¸ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç†è®ºè§£é‡Šå®è·µï¼šä¸ºRLHFåœ¨å®è·µä¸­è¡¨ç°å¥½å´è¿èƒŒç»å…¸å…¬ç†è¿™ä¸€çŸ›ç›¾æä¾›äº†ç†è®ºå±‚é¢çš„è§£é‡Šï¼Œè®©ä»ä¸šè€…ç†è§£å…¶æˆåŠŸèƒŒåçš„åå¥½åˆ†å¸ƒå‡è®¾ç­‰å…³é”®å› ç´ ï¼Œåœ¨å®é™…æ„å»ºåå¥½æ•°æ®ã€è®¾è®¡å¯¹é½æµç¨‹æ—¶æ›´æœ‰ç†è®ºä¾æ®ã€‚  
2. æ”¹è¿›å¥–åŠ±æ¨¡å‹æ€è·¯ï¼šæå‡ºçš„å¥–åŠ±æ¨¡å‹ç›®æ ‡ä¿®æ”¹æ–¹å¼ï¼Œä¸ºå¤„ç†å¤šæ ‡æ³¨è€…åœºæ™¯ä¸‹çš„åå¥½èšåˆæä¾›äº†æ›´ä¼˜è·¯å¾„ï¼Œèƒ½æŒ‡å¯¼åç»­ä¼˜åŒ–å¥–åŠ±å»ºæ¨¡ç¯èŠ‚ä»¥ä¿éšœå¯¹é½è´¨é‡ä¸å…¬ç†æ»¡è¶³æ€§ã€‚  
3. æ–°å…¬ç†æ‹“å±•æ–¹å‘ï¼šæ–°æå‡ºçš„ä¸‰ä¸ªé€‚é…LLMå¯¹é½çš„å…¬ç†ï¼Œä¸ºè¯¥é¢†åŸŸåç»­è¯„ä¼°æ–¹æ³•ã€è®¾è®¡æ–°å¯¹é½ç®—æ³•æä¾›äº†å…¨æ–°çš„è¡¡é‡ç»´åº¦ä¸ç›®æ ‡å¯¼å‘ï¼Œæ¨åŠ¨é¢†åŸŸä»ç»å…¸å…¬ç†æ¡†æ¶å‘æ›´è´´åˆç”Ÿæˆæ¨¡å‹ç‰¹æ€§çš„æ–¹å‘å‘å±•ã€‚

## treerl--llm-reinforcement-learning-with-on-policy-tree-search
### Abstract
Reinforcement learning (RL) with tree search has demonstrated superior
performance in traditional reasoning tasks. Compared to conventional
independent chain sampling strategies with outcome supervision, tree search
enables better exploration of the reasoning space and provides dense, on-policy
process rewards during RL training but remains under-explored in On-Policy LLM
RL. We propose TreeRL, a reinforcement learning framework that directly
incorporates on-policy tree search for RL training. Our approach includes
intermediate supervision and eliminates the need for a separate reward model
training. Existing approaches typically train a separate process reward model,
which can suffer from distribution mismatch and reward hacking. We also
introduce a cost-effective tree search approach that achieves higher search
efficiency under the same generation token budget by strategically branching
from high-uncertainty intermediate steps rather than using random branching.
Experiments on challenging math and code reasoning benchmarks demonstrate that
TreeRL achieves superior performance compared to traditional ChainRL,
highlighting the potential of tree search for LLM. TreeRL is open-sourced at
https://github.com/THUDM/TreeRL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TreeRLï¼šåŸºäºOn - Policyæ ‘æœç´¢çš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯æå‡å…¶æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚å½“å‰LLMçš„RLæ–¹æ³•å¤šç‹¬ç«‹é‡‡æ ·è½¨è¿¹å¹¶åŸºäºæœ€ç»ˆç­”æ¡ˆè·å–å¥–åŠ±ï¼Œè€Œåœ¨å…¶ä»–é¢†åŸŸæˆåŠŸçš„æ ‘æœç´¢åœ¨LLMæ¨ç†çš„å¼ºåŒ–å­¦ä¹ ä¸­å‘å±•ä¸è¶³ã€‚ä¸€æ–¹é¢ï¼Œç»å…¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰åœ¨ç›¸åŒæ¨ç†æˆæœ¬ä¸‹æ•ˆæœå’Œæ•ˆç‡ä¸å¦‚ç‹¬ç«‹é‡‡æ ·å¤šå“åº”ï¼›å¦ä¸€æ–¹é¢ï¼Œæ ‘æœç´¢è™½èƒ½æä¾›ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£ï¼Œä½†ç¦»çº¿è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¯¹RLè®­ç»ƒæ€§èƒ½æå‡è´¡çŒ®å°ã€‚å› æ­¤ï¼Œæ¢ç´¢ç»“åˆæ ‘æœç´¢çš„On - Policy RLè®­ç»ƒä»¥æå‡LLMæ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºTreeRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶
TreeRLæ˜¯ä¸€ç§å°†On - Policyæ ‘æœç´¢ç›´æ¥çº³å…¥RLè®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸­é—´ç›‘ç£ï¼Œæ— éœ€å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•ä¸­å•ç‹¬è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¯èƒ½å‡ºç°çš„åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±é»‘å®¢é—®é¢˜ã€‚é€šè¿‡æ ‘æœç´¢ä¸ºRLè®­ç»ƒæä¾›å¯†é›†çš„ã€On - Policyçš„è¿‡ç¨‹å¥–åŠ±ï¼Œæ›´å¥½åœ°æ¢ç´¢æ¨ç†ç©ºé—´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºé«˜æ•ˆæ ‘æœç´¢ç­–ç•¥EPTree
ä¸åŒäºMCTSå°†ç­”æ¡ˆåˆ†è§£ä¸ºå°éƒ¨åˆ†è®©æ¨¡å‹é€æ­¥æ¢ç´¢ï¼ŒEPTreeåŸºäºç†µä»ç°æœ‰æ ‘ä¸­æœ€ä¸ç¡®å®šçš„ä¸­é—´tokenåˆ†å‰ç”Ÿæˆæ–°å“åº”ï¼Œç›´åˆ°å¾—åˆ°æœ€ç»ˆç­”æ¡ˆã€‚è¿™ç§æ–¹å¼åœ¨ç›¸åŒç”Ÿæˆtokené¢„ç®—ä¸‹ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§ä¸­é—´æ­¥éª¤ç­–ç•¥æ€§åˆ†æ”¯è€Œééšæœºåˆ†æ”¯ï¼Œå®ç°æ›´é«˜æœç´¢æ•ˆç‡ï¼Œä¸”é€šå¸¸åªéœ€çº¦ä¸¤æ¬¡è¿­ä»£å°±èƒ½å½¢æˆç”Ÿæˆæ ‘ï¼Œèƒ½ç”Ÿæˆæ›´å¤šæ ·æœ‰æ•ˆçš„å“åº”ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºæ ‘æœç´¢çš„è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ 
åœ¨æ ‘çš„æ¯ä¸ªæ­¥éª¤åŸºäºä¼˜åŠ¿åˆ†é…ä¿¡ç”¨ï¼Œè®¡ç®—ç»™å®šæ¨ç†æ­¥éª¤çš„è¿‡ç¨‹ä¿¡å·ä¸ºå…¨å±€ä¼˜åŠ¿å’Œå±€éƒ¨ä¼˜åŠ¿çš„åŠ æƒå’Œã€‚å…¨å±€ä¼˜åŠ¿åæ˜ è¯¥æ­¥éª¤å¯¹é—®é¢˜æ•´ä½“æ­£ç¡®ç‡çš„æ½œåŠ›ï¼Œå±€éƒ¨ä¼˜åŠ¿é‡åŒ–è¯¥æ­¥éª¤ä¸å…¶åœ¨æ ‘ä¸­çˆ¶æ­¥éª¤ç›¸æ¯”çš„æ”¹è¿›ã€‚è¿™äº›ä¼˜åŠ¿ä¿¡å·ç›´æ¥ä»On - Policyç”Ÿæˆçš„æ ‘ä¸­æ¨å¯¼ï¼Œèƒ½æŠµæŠ—å¥–åŠ±é»‘å®¢ä¸”ä¸ä¾èµ–é¢å¤–å¥–åŠ±æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆåŸºäºQwenå’ŒGLMæ¨¡å‹ï¼‰ä¸Šè¯„ä¼°TreeRLï¼Œç»“æœè¡¨æ˜TreeRLæ¯”ä¼ ç»ŸChainRLæ€§èƒ½æ›´ä¼˜ï¼Œå±•ç°å‡ºæ ‘æœç´¢å¯¹LLMçš„æ½œåŠ›ã€‚EPTreeåœ¨ä¸åŒæ¨ç†é¢„ç®—ä¸‹æŒç»­ä¼˜äºç‹¬ç«‹åŒåˆ†å¸ƒå¤šé“¾é‡‡æ ·å’ŒMCTSï¼ŒTreeRLç»“åˆEPTreeæ¯”é‡‡ç”¨ç‹¬ç«‹åŒåˆ†å¸ƒå¤šé“¾é‡‡æ ·çš„ChainRLè¡¨ç°æ›´å¥½ï¼Œæ€§èƒ½æå‡å—ç›ŠäºEPTreeè‰¯å¥½çš„PassRateè¡¨ç°å’Œè¿‡ç¨‹ç›‘ç£ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ ‘æœç´¢ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆçš„æ€è·¯ï¼šä¸ºæå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†æ–°æ–¹å‘ï¼Œä¸å†å±€é™äºä¼ ç»Ÿçš„ç‹¬ç«‹è½¨è¿¹é‡‡æ ·å’Œä»…åŸºäºæœ€ç»ˆç»“æœçš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨æ ‘æœç´¢æ›´å¥½æ¢ç´¢æ¨ç†ç©ºé—´ã€‚
2. é«˜æ•ˆæ ‘æœç´¢ç­–ç•¥è®¾è®¡ï¼šEPTreeåŸºäºç†µçš„é«˜ä¸ç¡®å®šæ€§ä¸­é—´æ­¥éª¤åˆ†æ”¯æ–¹å¼ï¼Œä¸ºåœ¨æœ‰é™tokené¢„ç®—ä¸‹æå‡æœç´¢æ•ˆç‡æä¾›äº†å‚è€ƒï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦é«˜æ•ˆæ¢ç´¢çš„ç”Ÿæˆå¼ä»»åŠ¡åœºæ™¯ã€‚
3. è¿‡ç¨‹ç›‘ç£ä¿¡å·æ„å»ºï¼šåŸºäºæ ‘ä¸­æ­¥éª¤ä¼˜åŠ¿è®¡ç®—è¿‡ç¨‹ä¿¡å·ï¼Œæ— éœ€é¢å¤–å¥–åŠ±æ¨¡å‹ä¸”æŠ—å¥–åŠ±é»‘å®¢çš„æ–¹å¼ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±ä¿¡å·è®¾è®¡æä¾›äº†åˆ›æ–°æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°éœ€è¦ç»†ç²’åº¦ç›‘ç£çš„RLä»»åŠ¡ä¸­ã€‚

## personalized-llm-decoding-via-contrasting-personal-preference
### Abstract
As large language models (LLMs) are progressively deployed in various
real-world applications, personalization of LLMs has become increasingly
important. While various approaches to LLM personalization such as prompt-based
and training-based methods have been actively explored, the development of
effective decoding-time algorithms remains largely overlooked, despite their
demonstrated potential. In this paper, we propose CoPe (Contrasting Personal
Preference), a novel decoding-time approach applied after performing
parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is
to leverage reward-guided decoding specifically for personalization by
maximizing each user's implicit reward signal. We evaluate CoPe across five
open-ended personalized text generation tasks. Our empirical results
demonstrate that CoPe achieves strong performance, improving personalization by
an average of 10.57% in ROUGE-L, without relying on external reward models or
additional training procedures.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£ç é˜¶æ®µä¸ªæ€§åŒ–LLMæ–°èŒƒå¼ï¼šCoPeè®©æ¨¡å‹æ›´æ‡‚ä½ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®åº”ç”¨ä¸­å¹¿æ³›éƒ¨ç½²ï¼ŒLLMçš„ä¸ªæ€§åŒ–å˜å¾—æ„ˆå‘é‡è¦ã€‚ç›®å‰å·²æœ‰åŸºäºæç¤ºï¼ˆprompt - basedï¼‰å’ŒåŸºäºè®­ç»ƒï¼ˆtraining - basedï¼‰çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œä½†è§£ç é˜¶æ®µçš„æœ‰æ•ˆç®—æ³•å¼€å‘å´è¢«å¿½è§†ï¼Œå°½ç®¡å…¶æœ‰å¾ˆå¤§æ½œåŠ›ã€‚åŸºäºæç¤ºçš„æ–¹æ³•ç¼ºä¹å¯¹ç”¨æˆ·æ•°æ®çš„ç›´æ¥å­¦ä¹ ï¼Œæ•ˆæœå—é™ï¼›åŸºäºè®­ç»ƒçš„æ–¹æ³•è™½èƒ½æ›´å¥½æ•æ‰ç”¨æˆ·åå¥½ï¼Œä½†å­˜åœ¨ç¾éš¾æ€§é—å¿˜å’Œè®¡ç®—æˆæœ¬å¢åŠ ç­‰é—®é¢˜ã€‚æ‰€ä»¥æœ¬æ–‡æ—¨åœ¨ä»è§£ç é˜¶æ®µå…¥æ‰‹ï¼Œæå‡ºæ–°æ–¹æ³•å®ç°LLMçš„æœ‰æ•ˆä¸ªæ€§åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCoPeï¼ˆContrasting Personal Preferenceï¼‰è§£ç é˜¶æ®µæ–¹æ³•  
CoPeæ˜¯åœ¨å¯¹ç”¨æˆ·ç‰¹å®šæ•°æ®è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ååº”ç”¨çš„è§£ç é˜¶æ®µæ–°æ–¹æ³•ã€‚å®ƒå±äºå¥–åŠ±å¼•å¯¼è§£ç çš„ä¸€ç§ï¼Œä½†æ— éœ€å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨PEFTè°ƒä¼˜æ¨¡å‹å’ŒåŸå§‹åŸºç¡€æ¨¡å‹çš„ä¼¼ç„¶æ¥è¿‘ä¼¼éšå¼ç”¨æˆ·å¥–åŠ±ä¿¡å·ï¼Œå°†è¿™ç§éšå¼å¥–åŠ±ä¸å¯¹æ¯”è§£ç ç›®æ ‡ç›¸è”ç³»ï¼Œå®ç°å¥–åŠ±å¼•å¯¼è§£ç ç”¨äºä¸ªæ€§åŒ–ï¼Œè®©ç”Ÿæˆæ–‡æœ¬æ›´å¥½åœ°ä¸ç”¨æˆ·åå¥½å¯¹é½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¢å¼ºPEFTæ•æ‰éšå¼ç”¨æˆ·å¥–åŠ±  
é€šè¿‡Direct Preference Optimizationï¼ˆDPOï¼‰å¯¹æ¯”æ­£é¢å“åº”ï¼ˆç”¨æˆ·æä¾›ï¼‰å’Œè´Ÿé¢å“åº”ï¼ˆä¸å¤ªå¯èƒ½æ¥è‡ªç”¨æˆ·ï¼Œå¦‚å…¶ä»–ç”¨æˆ·æˆ–åˆæˆçš„ä½éšå¼å¥–åŠ±è¾“å‡ºï¼‰ä¹‹é—´çš„éšå¼å¥–åŠ±ã€‚ä¸ºé¿å…ä¾èµ–å…¶ä»–ç”¨æˆ·æ•°æ®çš„éšç§å’Œå®é™…æŒ‘æˆ˜ï¼Œç”¨Best - of - Né‡‡æ ·ç”Ÿæˆä½éšå¼å¥–åŠ±çš„åˆæˆè´Ÿé¢å“åº”ã€‚è¿™ç§è®­ç»ƒæ–¹æ³•ä¸ä»…æå‡äº†PEFTçš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºå¥–åŠ±å¼•å¯¼è§£ç æä¾›æ›´å‡†ç¡®çš„éšå¼ç”¨æˆ·å¥–åŠ±å»ºæ¨¡ï¼Œè¿›è€Œæå‡æ•´ä½“ä¸ªæ€§åŒ–æ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¥è‡ªLanguage Model Personalizationï¼ˆLaMPï¼‰å’ŒLongLaMPåŸºå‡†çš„äº”ä¸ªä¸åŒä¸ªæ€§åŒ–å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¯„ä¼°CoPeã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ä»»åŠ¡å¾®è°ƒæ¨¡å‹ç›¸æ¯”ï¼ŒCoPeåœ¨ROUGE - Lä¸Šå¹³å‡ç›¸å¯¹æå‡10.57%ï¼›ä¸ç¼ºä¹å¯¹æ¯”æœºåˆ¶çš„ç®€å•ä¸ªæ€§åŒ–æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨å„ä»»åŠ¡ä¸­ROUGE - Lå¹³å‡æå‡5.67%ã€‚è€Œä¸”CoPeåœ¨ä¸åŒè§„æ¨¡å’Œç±»å‹çš„æœ€å…ˆè¿›LLMsä¸Šæ³›åŒ–æ€§è‰¯å¥½ï¼Œè¯æ˜å…¶éšå¼å¥–åŠ±æœ€å¤§åŒ–èƒ½è¿›ä¸€æ­¥å¢å¼ºä¸ä¸ªä½“ç”¨æˆ·åå¥½çš„å¯¹é½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§£ç é˜¶æ®µä¸ªæ€§åŒ–æ€è·¯ï¼šå¼€æ‹“äº†LLMä¸ªæ€§åŒ–åœ¨è§£ç é˜¶æ®µçš„ç ”ç©¶æ–¹å‘ï¼Œå±•ç¤ºäº†æ— éœ€é¢å¤–è®­ç»ƒæµç¨‹å’Œå¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œä»…åœ¨è§£ç æ—¶åˆ©ç”¨æ¨¡å‹è‡ªèº«ä¼¼ç„¶å¯¹æ¯”å®ç°ä¸ªæ€§åŒ–çš„å¯è¡Œæ€§ï¼Œä¸ºåç»­è§£ç é˜¶æ®µä¸ªæ€§åŒ–æ–¹æ³•æä¾›äº†æ–°æ€è·¯ã€‚
2. PEFTå¢å¼ºæ–¹å¼ï¼šåˆ©ç”¨DPOå’Œåˆæˆè´Ÿé¢æ ·æœ¬å¢å¼ºPEFTæ•æ‰ç”¨æˆ·éšå¼å¥–åŠ±çš„æ–¹å¼ï¼Œä¸ºå‚æ•°é«˜æ•ˆå¾®è°ƒåœ¨ä¸ªæ€§åŒ–åœºæ™¯ä¸‹çš„ä¼˜åŒ–æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼Œåœ¨å¤„ç†ç”¨æˆ·æ•°æ®éšç§å’Œé¿å…ä¾èµ–å¤–éƒ¨æ•°æ®æ–¹é¢ç»™å‡ºäº†åˆ›æ–°è§£æ³•ã€‚
3. å¤šä»»åŠ¡å¤šæ¨¡å‹éªŒè¯ï¼šåœ¨å¤šä¸ªä»»åŠ¡å’Œä¸åŒLLMä¸ŠéªŒè¯æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢çš„å®éªŒè®¾è®¡æ€è·¯ä»¥åŠæ‰€å±•ç°å‡ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºç›¸å…³æ–¹æ³•çš„å®ç”¨æ€§éªŒè¯æä¾›äº†èŒƒä¾‹ï¼Œè®©è¯¥æ–¹æ³•åœ¨å®é™…è½åœ°åˆ°ä¸åŒåœºæ™¯æ—¶æ›´å…·å¯ä¿¡åº¦ã€‚

## med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards
### Abstract
Large language models have shown promise in clinical decision making, but
current approaches struggle to localize and correct errors at specific steps of
the reasoning process. This limitation is critical in medicine, where
identifying and addressing reasoning errors is essential for accurate diagnosis
and effective patient care. We introduce Med-PRM, a process reward modeling
framework that leverages retrieval-augmented generation to verify each
reasoning step against established medical knowledge bases. By verifying
intermediate reasoning steps with evidence retrieved from clinical guidelines
and literature, our model can precisely assess the reasoning quality in a
fine-grained manner. Evaluations on five medical QA benchmarks and two
open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art
performance, with improving the performance of base models by up to 13.50%
using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by
integrating it in a plug-and-play fashion with strong policy models such as
Meerkat, achieving over 80\% accuracy on MedQA for the first time using
small-scale models of 8 billion parameters. Our code and data are available at:
https://med-prm.github.io/
### ğŸŒŸ è®ºæ–‡è§£è¯» | Med - PRMï¼šåŒ»ç–—æ¨ç†æ¨¡å‹çš„â€œæ­¥æ­¥ä¸ºè¥â€æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¸´åºŠå†³ç­–åˆ¶å®šï¼ˆCDMï¼‰æ˜¯ä¸€ä¸ªå¤æ‚å¤šæ­¥éª¤è¿‡ç¨‹ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨åŒ»ç–—åº”ç”¨æœ‰è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾åœ¨æ¨ç†ç‰¹å®šæ­¥éª¤å®šä½å’Œçº æ­£é”™è¯¯ï¼Œè€ŒåŒ»ç–—é¢†åŸŸè¯†åˆ«ä¸è§£å†³æ¨ç†é”™è¯¯å¯¹å‡†ç¡®è¯Šæ–­å’Œæœ‰æ•ˆåŒ»ç–—è‡³å…³é‡è¦ã€‚åŒæ—¶ï¼Œè¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ï¼ˆPRMï¼‰åº”ç”¨äºåŒ»ç–—æœ‰æŒ‘æˆ˜ï¼šä¸€æ˜¯é«˜è´¨é‡æ­¥éª¤çº§ç›‘ç£è·å–æˆæœ¬é«˜ä¸”è´¹åŠ›ï¼Œç°æœ‰è‡ªåŠ¨æ ‡æ³¨ç­–ç•¥æ˜“ä½ä¼°åˆç†ä½†æ²¡å¯¼å‘æ­£ç¡®ç»“æœçš„æ—©æœŸæ­¥éª¤ï¼›äºŒæ˜¯åŒ»ç–—æ¨ç†éœ€å¤§é‡é¢†åŸŸçŸ¥è¯†ï¼Œä»…é è¯­è¨€æ¨¡å‹å‚æ•°éš¾å®Œå…¨æ¶µç›–ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ç¼ºåŒ»ç–—ä¸Šä¸‹æ–‡ä¹Ÿä¸å¤Ÿã€‚è¿™äº›ç—›ç‚¹æ¨åŠ¨äº†Med - PRMçš„æå‡ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMed - PRMæ¡†æ¶
Med - PRMæ˜¯æ£€ç´¢å¢å¼ºçš„è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œé‡‡ç”¨RAG - AS - A - JUDGEæ–¹å¼ï¼ŒåŸºäºä¸´åºŠé—®é¢˜å’Œæ£€ç´¢åˆ°çš„åŒ»ç–—æ–‡æ¡£å¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤åšé€æ­¥è¯„ä¼°ã€‚è¯¥è¯„ä¼°ç›¸æ¯”è®­ç»ƒæ—¶åŸºäºé‡‡æ ·çš„è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ï¼Œæ›´è´´è¿‘ä¸“å®¶åŒ»å¸ˆæ ‡æ³¨ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µèå…¥ä¸´åºŠçŸ¥è¯†ï¼Œèƒ½æ›´ç²¾å‡†è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¼å…·é€šç”¨æ€§ä¸é«˜æ•ˆæ€§
Med - PRMå±•ç°å‡ºå³æ’å³ç”¨çš„é€šç”¨æ€§ï¼Œå¯ä¸Meerkatç­‰å¼ºå¤§ç­–ç•¥æ¨¡å‹ç»“åˆã€‚ä¸”è®­ç»ƒæˆæœ¬é«˜æ•ˆï¼Œå¦‚é’ˆå¯¹èŠ±è´¹çº¦2ä¸‡ç¾å…ƒè®­ç»ƒæ•°æ®çš„UltraMedicalæ¨¡å‹ï¼ŒMed - PRMç”¨èŠ±è´¹ä¸åˆ°20ç¾å…ƒçš„ç²¾å¿ƒç­–åˆ’æ•°æ®é›†è®­ç»ƒï¼Œä»èƒ½æå‡æ€§èƒ½ï¼Œä½“ç°æˆæœ¬æ•ˆç›Šä¸æ‰©å±•æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªåŒ»ç–—QAåŸºå‡†å’Œä¸¤ä¸ªå¼€æ”¾å¼è¯Šæ–­ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒMed - PRMå®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œèƒ½å°†åŸºç¡€æ¨¡å‹æ€§èƒ½æå‡é«˜è¾¾13.50%ã€‚ä¸å¼ºç­–ç•¥æ¨¡å‹ç»“åˆåï¼Œç”¨80äº¿å‚æ•°å°è§„æ¨¡æ¨¡å‹åœ¨MedQAé¦–æ¬¡å®ç°è¶…80%å‡†ç¡®ç‡ï¼Œåœ¨ä¸ƒä¸ªåŒ»ç–—åŸºå‡†ä¸­å…­ä¸ªå®ç°SOTAï¼Œåœ¨MedQAï¼ˆ4é€‰é¡¹ï¼‰ä¸Šç”¨8Bå‚æ•°æ¨¡å‹è¾¾åˆ°80.35%å‡†ç¡®ç‡ï¼Œå¹³å‡æ¯”ç°æœ‰PRMåŸºçº¿åœ¨ä¸ƒä¸ªåŒ»ç–—åŸºå‡†ä¸Šé«˜3.44%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æŠ€æœ¯åˆ›æ–°çœ‹ï¼Œæ£€ç´¢å¢å¼ºç»“åˆè¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ç”¨äºå‚ç›´é¢†åŸŸæ¨ç†è¯„ä¼°æ˜¯å¾ˆå¥½æ€è·¯ï¼Œä¸ºé¢†åŸŸç‰¹å®šçš„LLMä¼˜åŒ–æä¾›æ–¹å‘ï¼›åœ¨å·¥ç¨‹å®è·µä¸Šï¼Œå±•ç¤ºäº†ä½æˆæœ¬æ•°æ®è®­ç»ƒé«˜æ•ˆæ¨¡å‹è¾…åŠ©å¼ºæ¨¡å‹æå‡æ€§èƒ½çš„è·¯å¾„ï¼Œä¸ºèµ„æºæœ‰é™ä½†éœ€æå‡æ¨¡å‹åŒ»ç–—æ¨ç†èƒ½åŠ›çš„åœºæ™¯æä¾›å‚è€ƒï¼›ä»åŒ»ç–—AIå‘å±•è§’åº¦ï¼Œå¼ºè°ƒæ­¥éª¤çº§éªŒè¯å’ŒåŒ»ç–—çŸ¥è¯†èå…¥ï¼Œè®©æ¨¡å‹æ¨ç†æ›´é€æ˜å¯é ï¼Œä¸ºåŒ»ç–—AIè´´è¿‘ä¸´åºŠå®é™…åº”ç”¨æ ‡å‡†æä¾›äº†å®è·µèŒƒå¼ï¼Œåç»­åŒ»ç–—æˆ–å…¶ä»–å‚ç›´é¢†åŸŸçš„æ¨ç†æ¨¡å‹ä¼˜åŒ–å¯å€Ÿé‰´å…¶æ­¥éª¤éªŒè¯ã€çŸ¥è¯†èåˆä¸æˆæœ¬æ§åˆ¶ç­‰æ€è·¯ã€‚

## agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards
### Abstract
Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too sparse for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-RLVRï¼šè®©å¤§æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­â€œæ‹œå¸ˆå­¦è‰ºâ€çš„RLæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦ã€ç«èµ›ç¼–ç¨‹ç­‰å¯éªŒè¯é¢†åŸŸæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ™ºèƒ½ä½“ç¯å¢ƒï¼ˆå¤šæ­¥éª¤ã€å¤æ‚é—®é¢˜æ±‚è§£åœºæ™¯ï¼‰ä¸­æ•ˆæœéª¤é™ã€‚è¿™ç±»åœºæ™¯å¥–åŠ±ç¨€ç–ï¼Œå‰æ²¿LLMä¹Ÿæ˜“é«˜å¤±è´¥ç‡ï¼Œä¼ ç»ŸRLVRéš¾ä»¥æœ‰æ•ˆè®­ç»ƒã€‚åŒæ—¶ï¼Œæ™ºèƒ½ä½“ç¯å¢ƒéœ€å¤šè½®æ¨ç†ã€ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ï¼Œè®­ç»ƒå¤æ‚åº¦é«˜ï¼Œä¸ºè®©RLVRåœ¨å¤æ‚çœŸå®åœºæ™¯ï¼ˆå¦‚è½¯ä»¶å·¥ç¨‹ï¼‰ç”Ÿæ•ˆï¼Œå‚¬ç”Ÿäº†Agent - RLVRæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAgent - RLVRæ¡†æ¶é€‚é…æ™ºèƒ½ä½“åœºæ™¯  
å€Ÿé‰´äººç±»æ•™å­¦æ³•å¼•å…¥â€œagent guidanceï¼ˆæ™ºèƒ½ä½“æŒ‡å¯¼ï¼‰â€æœºåˆ¶ï¼Œåˆ©ç”¨ä»é«˜å±‚æˆ˜ç•¥è§„åˆ’åˆ°é”™è¯¯ä¸ç¯å¢ƒäº¤äº’åŠ¨æ€åé¦ˆç­‰å¤šæ ·ä¿¡æ¯çº¿ç´¢ï¼Œå¼•å¯¼æ™ºèƒ½ä½“èµ°å‘æˆåŠŸè½¨è¿¹ï¼Œåƒè€å¸ˆæŒ‡å¯¼æ–°äººä¸€æ ·å¸®æ™ºèƒ½ä½“åœ¨å¤æ‚è§£ç©ºé—´å¯¼èˆªï¼Œè¿˜èƒ½é€šè¿‡ç¯å¢ƒæ¢ç´¢ä¿ƒè¿›è‡ªæˆ‘æå‡ã€‚è®­ç»ƒå¾ªç¯åˆ†ä¸‰æ­¥ï¼šå…ˆè®©æ™ºèƒ½ä½“æ— æŒ‡å¯¼å°è¯•ç”Ÿæˆåˆå§‹è½¨è¿¹ï¼Œç”¨å•å…ƒæµ‹è¯•éªŒè¯å¹¶è¡¥å……æŒ‡å¯¼ï¼›å†è®©æ™ºèƒ½ä½“å¸¦æŒ‡å¯¼é‡è¯•ï¼›æœ€ååŸºäºæŒ‡å¯¼åè½¨è¿¹å¥–åŠ±ç”¨RLVRæ›´æ–°ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºè½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸“å±æ•°æ®é›†  
ç²¾å¿ƒæ•´ç†å«817ä¸ªè®­ç»ƒç¯å¢ƒçš„æ•°æ®é›†ï¼Œæ¶µç›–é—®é¢˜é™ˆè¿°ã€ç¯å¢ƒå’ŒæŒ‡å¯¼ä¿¡æ¯ï¼Œè¶…è¶Šä¼ ç»Ÿè¾“å…¥ - è¾“å‡ºå¯¹ï¼Œæ•æ‰å¸¦é›†æˆæŒ‡å¯¼ä¿¡å·çš„å®Œæ•´ç¼–ç ç¯å¢ƒï¼Œä¸ºè®­ç»ƒè½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“æä¾›ä¸°å¯Œèµ„æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨SWE - Bench VerifiedåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgent - RLVRå°†Qwen - 2.5 - 72B - Instructçš„pass@1æ€§èƒ½ä»9.4%æå‡è‡³22.4%ï¼›æŒ‡å¯¼å¢å¼ºçš„RLVRæ•°æ®ç”¨äºæµ‹è¯•æ—¶å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œèƒ½è¿›ä¸€æ­¥æŠŠpass@1æ¨è‡³27.8%ï¼›æŒ‡å¯¼æ¨¡å‹åœ¨pass@1ï¼ˆ19.8%â†’22.4%ï¼‰å’Œpass@32ï¼ˆ34.2%â†’38.4%ï¼‰ä¸Šéƒ½æœ‰æå‡ï¼ŒéªŒè¯æŒ‡å¯¼æ˜¯å…³é”®ç»„ä»¶ï¼Œä¹Ÿä½“ç°æ–¹æ³•åœ¨å°æ•°æ®é›†ä¸‹æå‡æ™ºèƒ½ä½“å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åº”å¯¹ç¨€ç–å¥–åŠ±åœºæ™¯æ—¶ï¼Œå¼•å…¥ç±»äººç±»æ•™å­¦çš„æŒ‡å¯¼æœºåˆ¶æ˜¯æœ‰æ•ˆæ€è·¯ï¼Œä¸ºå¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­æ¨¡å‹è®­ç»ƒæä¾›æ–°èŒƒå¼å‚è€ƒã€‚
2. æ„å»ºé¢†åŸŸä¸“å±ã€å«ä¸°å¯Œç¯å¢ƒä¸æŒ‡å¯¼ä¿¡æ¯çš„æ•°æ®é›†ï¼Œèƒ½ä¸ºç‰¹å®šé¢†åŸŸæ™ºèƒ½ä½“è®­ç»ƒç­‘ç‰¢æ•°æ®åŸºç¡€ï¼Œè¿™ç§â€œå®šåˆ¶åŒ– + åœºæ™¯åŒ–â€æ•°æ®æ„å»ºæ€ç»´å€¼å¾—å€Ÿé‰´ã€‚
3. å±•ç¤ºäº†RLVRæ•°æ®åœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒç­‰æ–¹é¢çš„é¢å¤–ä»·å€¼ï¼Œå¯å‘åç»­æ¢ç´¢ä¸åŒæ¨¡å—é—´æ•°æ®å¤ç”¨ä¸ååŒå¢æ•ˆï¼Œæ‹“å±•æŠ€æœ¯åº”ç”¨è¾¹ç•Œã€‚

## reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems
### Abstract
There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReGuidanceï¼šä¸ºå›°éš¾é€†é—®é¢˜æå‡é‡‡æ ·è´¨é‡çš„ç®€æ´æ‰©æ•£åŒ…è£…å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒæ¥è§£å†³é€†é—®é¢˜ä»¥åŠé€šè¿‡å¥–åŠ±æ¨¡å‹å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„ç ”ç©¶ååˆ†æ´»è·ƒã€‚åƒæ‰©æ•£åéªŒé‡‡æ ·ï¼ˆDPSï¼‰è¿™ç±»æ— è®­ç»ƒæ–¹æ³•è™½æä¾›äº†çµæ´»çš„å¯å‘å¼ç®—æ³•ï¼Œä½†å½“å¥–åŠ±ä¿¡æ¯ä¸è¶³æ—¶ï¼ˆå¦‚ä½ä¿¡å™ªæ¯”çš„å›°éš¾é€†é—®é¢˜åœºæ™¯ï¼‰ï¼Œè¿™äº›æŠ€æœ¯ä¼šåç¦»æ•°æ®æµå½¢ï¼Œæ— æ³•ç”Ÿæˆé€¼çœŸè¾“å‡ºã€‚ä¾‹å¦‚åœ¨å¤§åŒºåŸŸå›¾åƒä¿®å¤ã€é«˜å€æ•°è¶…åˆ†è¾¨ç‡ç­‰ç¡¬é€†é—®é¢˜ä¸­ï¼Œç°æœ‰å…ˆè¿›åŸºçº¿æ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œæ—¢éš¾ä¿è¯é‡‡æ ·æ¥è‡ªåˆé€‚çš„å€¾æ–œå¯†åº¦ï¼ˆåéªŒåˆ†å¸ƒï¼‰ï¼Œç”Ÿæˆç»“æœçš„çœŸå®æ„Ÿä¹Ÿå¾ˆå·®ã€‚æœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºæå‡æ ·æœ¬çœŸå®æ„Ÿä¸å¥–åŠ±è¡¨ç°çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReGuidanceæ–¹æ³•  
ReGuidanceæ˜¯ä¸€ä¸ªç®€æ´çš„åŒ…è£…å™¨ç®—æ³•ï¼Œç”¨äºæå‡ç°æœ‰åŸºäºæ‰©æ•£çš„é€†é—®é¢˜æ±‚è§£å™¨çš„æ ·æœ¬è´¨é‡ã€‚å®ƒä»¥ç”¨æˆ·é€‰æ‹©çš„é€†é—®é¢˜æ±‚è§£å™¨ç”Ÿæˆçš„å€™é€‰è§£$\hat{x}$ä¸ºè¾“å…¥ï¼Œåˆ†ä¸¤æ­¥æ“ä½œï¼šç¬¬ä¸€æ­¥ï¼Œä»$\hat{x}$å‡ºå‘åå‘è¿è¡Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹é’ˆå¯¹åŸºç¡€å¯†åº¦$q$çš„ç¡®å®šæ€§é‡‡æ ·å™¨ï¼Œæå–ä¸ä¹‹å…³è”çš„æ½œåœ¨å˜é‡$x^*$ï¼›ç¬¬äºŒæ­¥ï¼Œä»¥$x^*$ä¸ºåˆå§‹åŒ–ï¼Œè¿è¡ŒDiffusion Posterior Samplingï¼ˆDPSï¼‰ç®—æ³•æ¥ç”Ÿæˆæ–°çš„é‡å»ºç»“æœ$x_{\text{DPS}}$ ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºä¿éšœ  
ä»ç†è®ºå±‚é¢è¯æ˜ï¼Œåœ¨æŸäº›å¤šæ¨¡æ€æ•°æ®åˆ†å¸ƒä¸‹ï¼ŒReGuidanceèƒ½åŒæ—¶æå‡å¥–åŠ±å¹¶ä½¿å€™é€‰è§£æ›´æ¥è¿‘æ•°æ®æµå½¢ã€‚è¿™æ˜¯é¦–æ¬¡ä¸ºDPSæä¾›ä¸¥æ ¼çš„ç®—æ³•ä¿éšœï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç†è®ºå±‚é¢çš„éƒ¨åˆ†ç©ºç™½ï¼Œä»æ•°å­¦è§’åº¦æ”¯æ’‘äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å›¾åƒä¿®å¤çš„ä¼ ç»Ÿå›°éš¾é€†ä»»åŠ¡ï¼ˆå¦‚å¤§åŒºåŸŸå›¾åƒä¿®å¤ã€é«˜å€æ•°è¶…åˆ†è¾¨ç‡ï¼‰ä¸­å±•å¼€è¯„ä¼°ï¼šå½“ç°æœ‰å…ˆè¿›åŸºçº¿æ–¹æ³•è¡¨ç°ä¸ä½³æ—¶ï¼Œåœ¨è¿™äº›åŸºçº¿ä¹‹ä¸Šåº”ç”¨ReGuidanceï¼Œèƒ½æ˜¾è‘—æå‡æ ·æœ¬è´¨é‡ä¸æµ‹é‡ä¸€è‡´æ€§ã€‚ä»¥å›¾åƒä¿®å¤ä»»åŠ¡ä¸ºä¾‹ï¼Œå¯¹ä¸åŒåŸºçº¿åº”ç”¨ReGuidanceåï¼Œå¥–åŠ±å’ŒçœŸå®æ„ŸæŒ‡æ ‡éƒ½æœ‰æŒç»­ä¸”æ˜¾è‘—çš„æå‡ï¼›å®šæ€§å±‚é¢è¿˜èƒ½å¾—åˆ°å¤šæ ·ä¸”é€¼çœŸã€ä¸åŸå§‹æµ‹é‡æ ·æœ¬æœ‰æ„ä¹‰åŒºåˆ«çš„é‡å»ºç»“æœã€‚ 

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•è®¾è®¡ç®€æ´æ¨¡å—åŒ–ï¼šReGuidanceçš„ä¸¤æ­¥æ“ä½œæ€è·¯æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œæ•´åˆåˆ°ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„é€†é—®é¢˜æ±‚è§£æµç¨‹ä¸­ï¼Œä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•æä¾›äº†è½»é‡ä½†æœ‰æ•ˆçš„æ€è·¯ã€‚  
2. å…¼é¡¾å®è¯ä¸ç†è®ºï¼šæ—¢é€šè¿‡å®éªŒéªŒè¯åœ¨ç¡¬é€†é—®é¢˜åœºæ™¯ä¸‹å¯¹æ ·æœ¬è´¨é‡çš„æå‡ï¼Œåˆç»™å‡ºç†è®ºè¯æ˜æ”¯æ’‘æ–¹æ³•ä¼˜åŠ¿ï¼Œè¿™ç§ä»å®è·µåˆ°ç†è®ºçš„å®Œæ•´ç ”ç©¶èŒƒå¼å€¼å¾—ç›¸å…³é¢†åŸŸç ”ç©¶å€Ÿé‰´ï¼Œå¸®åŠ©åç»­å·¥ä½œåœ¨æ–¹æ³•åˆ›æ–°æ—¶æ›´æ³¨é‡ç†è®ºä¸å®è¯çš„ç»“åˆã€‚ 
3. é’ˆå¯¹ç¡¬é€†é—®é¢˜åœºæ™¯ï¼šèšç„¦ä½ä¿¡å™ªæ¯”ç­‰ç¡¬é€†é—®é¢˜åœºæ™¯å±•å¼€ç ”ç©¶ï¼Œä¸ºè¿™ç±»é•¿æœŸå›°æ‰°çš„éš¾é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹å‘ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨æ›´å…·æŒ‘æˆ˜æ€§çš„é€†é—®é¢˜åœºæ™¯ä¸‹çš„æ–¹æ³•ä¼˜åŒ–ã€‚ 

## eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling
### Abstract
Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EQA - RMï¼šä¸ºå…·èº«é—®ç­”é‡èº«å®šåˆ¶çš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œå®ç°æµ‹è¯•æ—¶å¯æ‰©å±•è¯„ä¼°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§æ¨¡å‹å¯¹é½ä¸­è‡³å…³é‡è¦ï¼Œä½†åœ¨å¤æ‚çš„å…·èº«ä»»åŠ¡ï¼ˆå¦‚å…·èº«é—®ç­”ï¼ŒEQAï¼‰ä¸­å´æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚EQAéœ€è¦æ™ºèƒ½ä½“åœ¨3Dç¯å¢ƒä¸­é€šè¿‡å¤šæ¨¡æ€è§‚å¯Ÿå’ŒåŠ¨ä½œåºåˆ—æ¥æ„ŸçŸ¥ã€äº¤äº’å’Œæ¨ç†ä»¥å›ç­”é—®é¢˜ï¼Œå¯¹æ™ºèƒ½ä½“çš„ç©ºé—´ã€æ—¶é—´å’Œé€»è¾‘ç†è§£è¿›è¡Œç»†è‡´è¯„ä¼°è‡³å…³é‡è¦ï¼Œè€Œé€šç”¨çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•æ— æ³•æ»¡è¶³è¿™ä¸€éœ€æ±‚ã€‚ç°æœ‰é€šç”¨å¥–åŠ±æ¨¡å‹å¤šä¸ºé™æ€è¾“å…¥æˆ–ç®€å•ç»“æœè®¾è®¡ï¼Œéš¾ä»¥æ•æ‰å…·èº«ä»»åŠ¡ä¸­å›ºæœ‰çš„æ—¶ç©ºå’Œé€»è¾‘ä¾èµ–å…³ç³»ï¼Œå› æ­¤è¿«åˆ‡éœ€è¦ä¸“é—¨çš„æœºåˆ¶æ¥å‡†ç¡®è¯„ä¼°EQAçš„å¤šæ–¹é¢æˆåŠŸæŒ‡æ ‡ã€‚åŒæ—¶ï¼ŒEQAé¢†åŸŸç¼ºä¹ç”¨äºä¸¥æ ¼è¯„ä¼°å’Œæ¯”è¾ƒå¥–åŠ±æ¨¡å‹çš„æ ‡å‡†åŒ–åŸºå‡†ï¼Œå½“å‰EQAä»»åŠ¡åŸºå‡†ä¾§é‡äºç²—ç•¥çš„æˆåŠŸæŒ‡æ ‡ï¼Œè€Œéå¯¹å¥–åŠ±æ¨¡å‹å‘å±•è‡³å…³é‡è¦çš„ç»†ç²’åº¦è½¨è¿¹è´¨é‡è¯„ä¼°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEQA - RMç”Ÿæˆå¼å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹
EQA - RMæ˜¯ä¸“ä¸ºè¯„ä¼°EQAè½¨è¿¹è€Œè®¾è®¡çš„æ–°å‹å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œä½œä¸ºç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ï¼Œä¸ä»…èƒ½äº§ç”Ÿæ ‡é‡å¥–åŠ±ï¼Œè¿˜èƒ½ä¸ºè¯„ä¼°æä¾›æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚å…¶å…·æœ‰å¢å¼ºçš„ç©ºé—´ã€æ—¶é—´å’Œæ¨ç†å¤„ç†èƒ½åŠ›ï¼Œä»¥å¤„ç†EQAä»»åŠ¡ä¸­å›ºæœ‰çš„ç‹¬ç‰¹å¤šæ¨¡æ€æ•°æ®æµã€‚é€šè¿‡é«˜æ•ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µç”¨æ ‡å‡†çš„Rejective Finetuningï¼ˆRFTï¼‰æ•™ä¼šæ¨¡å‹æœŸæœ›çš„è¾“å‡ºæ ¼å¼ï¼ˆåŒ…å«æ–‡æœ¬æ‰¹è¯„å’Œæ ‡é‡åˆ†æ•°ï¼‰ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åˆ›æ–°çš„Contrastive Group Relative Policy Optimizationï¼ˆC - GRPOï¼‰å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³ä»…RFTå¯èƒ½åªå­¦æ ¼å¼ä¸å­¦å†…å®¹çš„é—®é¢˜ï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¯¹æ¯”å¥–åŠ±ï¼ˆæºäºé’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºï¼Œå¦‚è§†é¢‘å¸§æ‰“ä¹±ã€ç©ºé—´åŒºåŸŸéšæœºæ©ç ã€æ¨ç†æ­¥éª¤æ··ä¹±ç­‰æ‰°åŠ¨æ–¹å¼ï¼‰ï¼Œè®©æ¨¡å‹åŒºåˆ†åŸå§‹è¿è´¯ä¸Šä¸‹æ–‡å’Œåˆæˆæ‰°åŠ¨ä¸Šä¸‹æ–‡ä¸‹çš„ç­–ç•¥è¾“å‡ºï¼Œä»è€Œå†…åŒ–æ—¶é—´é¡ºåºã€ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚å’Œè¿è´¯é€»è¾‘æµçš„é‡è¦æ€§ï¼ŒåŸ¹å…»å¯¹å…·èº«ä»»åŠ¡å¼ºå¤§ä¸”æ•é”çš„è¯„ä¼°èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºEQARewardBenchåŸºå‡†
ä¸ºè§£å†³EQAé¢†åŸŸå¥–åŠ±æ¨¡å‹è¯„ä¼°åŸºå‡†ç¼ºå¤±çš„é—®é¢˜ï¼ŒåŸºäºOpenEQAæ„å»ºäº†EQARewardBenchã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªHM3Då’ŒScanNetä¸¤ç§å®¶åº­ç¯å¢ƒçš„å…·èº«æƒ…èŠ‚è®°å¿†è§†é¢‘ï¼Œä»åŸå§‹é—®ç­”å¯¹æ„å»ºæ›´å…¨é¢çš„é—®é¢˜ - å“åº” - æ¨ç†è½¨è¿¹ä¸‰å…ƒç»„ï¼Œæœ‰1546ä¸ªæµ‹è¯•å®ä¾‹ï¼Œç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹åœ¨è½¨è¿¹è´¨é‡çš„å…«ä¸ªä¸åŒæ–¹é¢ï¼ˆå¦‚æ­£ç¡®æ€§ã€æ¥åœ°æ€§ã€æ•ˆç‡ç­‰ï¼‰ï¼Œä¸ºEQAä»»åŠ¡ä¸Šçš„å¥–åŠ±æ¨¡å‹æä¾›äº†æ ‡å‡†åŒ–ã€å¯æ¯”è¾ƒçš„è¯„ä¼°å¹³å°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä»¥Qwen2 - VL - 2B - Instructä¸ºåŸºç¡€è¿›è¡Œå¾®è°ƒçš„EQA - RMå±•ç°å‡ºé«˜æ ·æœ¬æ•ˆç‡ï¼Œä»…ç”¨700ä¸ªæ ·æœ¬åœ¨EQA - RM - Benchä¸Šè¾¾åˆ°61.9%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„ä¸“æœ‰åŸºçº¿ï¼ˆå¦‚Gemini - 2.5 - Flashã€GPT - 4oã€Claude - 3.5 - Haikuï¼‰å’Œå¼€æºçš„æœ€å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚RoVRMå’ŒVisualPRMï¼‰ã€‚åŒæ—¶ï¼ŒEQA - RMå±•ç¤ºäº†æµ‹è¯•æ—¶å¯æ‰©å±•æ€§ï¼Œåœ¨æ¨ç†æ—¶å¢åŠ è¯„ä¼°è®¡ç®—é‡ï¼Œå…¶åœ¨EQARewardBenchä¸Šçš„å‡†ç¡®ç‡ä»42.47%æå‡åˆ°61.86%ï¼Œæ€§èƒ½æå‡ååœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢†å…ˆçš„å¤§å‹å•†ä¸šæ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é’ˆå¯¹ç‰¹å®šå¤æ‚ä»»åŠ¡è®¾è®¡ä¸“ç”¨å¥–åŠ±æ¨¡å‹ï¼šå½“é€šç”¨æ¨¡å‹æ— æ³•æ»¡è¶³å¤æ‚ä»»åŠ¡ï¼ˆå¦‚å…·èº«ä»»åŠ¡ï¼‰çš„è¯„ä¼°éœ€æ±‚æ—¶ï¼Œå¯åƒEQA - RMä¸€æ ·é’ˆå¯¹ä»»åŠ¡ç‰¹æ€§ï¼Œè®¾è®¡å…·å¤‡ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚ç©ºé—´ã€æ—¶é—´ã€æ¨ç†å¤„ç†èƒ½åŠ›ï¼‰çš„ä¸“ç”¨æ¨¡å‹ï¼Œè§£å†³é€šç”¨æ¨¡å‹çš„å±€é™æ€§ã€‚
2. åˆ›æ–°çš„è®­ç»ƒç­–ç•¥ï¼šä¸¤é˜¶æ®µè®­ç»ƒï¼ˆRFT + C - GRPOï¼‰ä»¥åŠåˆ©ç”¨æ•°æ®å¢å¼ºçš„å¯¹æ¯”å¥–åŠ±ç­–ç•¥ï¼Œä¸ºè§£å†³æ¨¡å‹åªå­¦å½¢å¼ä¸å­¦å†…å®¹ã€æå‡æ¨¡å‹å¯¹ä»»åŠ¡å…³é”®è¦ç´ çš„ç†è§£æä¾›äº†æ€è·¯ï¼Œå¯å€Ÿé‰´äºå…¶ä»–éœ€è¦æ¨¡å‹æ·±å…¥ç†è§£ä»»åŠ¡ç»†èŠ‚çš„è®­ç»ƒåœºæ™¯ã€‚
3. æ„å»ºé¢†åŸŸåŸºå‡†ï¼šå¯¹äºç¼ºä¹è¯„ä¼°åŸºå‡†çš„é¢†åŸŸï¼Œå¯åƒæ„å»ºEQARewardBenchä¸€æ ·ï¼ŒåŸºäºç°æœ‰æ•°æ®é›†æ„å»ºä¸“é—¨çš„åŸºå‡†ï¼Œæ¨åŠ¨é¢†åŸŸå†…æ¨¡å‹çš„è¯„ä¼°å’Œå‘å±•ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æ¯”è¾ƒå’Œæ”¹è¿›æä¾›æ ‡å‡†å¹³å°ã€‚

## reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning
### Abstract
This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°æ¨¡å‹ä¹Ÿèƒ½æ‰“ï¼å¼ºåŒ–å­¦ä¹ å¾®è°ƒè®©è½»é‡çº§è¯­è¨€æ¨¡å‹ç©è½¬æŒ‡ä»¤éµå¾ªä¸æ•°å­¦æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆé¢†åŸŸå–å¾—äº†äº®çœ¼æˆæœï¼Œä½†å¦‚ä½•è®©å°è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªã€æ•°å­¦æ¨ç†ç­‰ä¸åŒæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ä»æ˜¯éš¾é¢˜ã€‚åŒæ—¶ï¼Œä¸åŒå¾®è°ƒæŠ€æœ¯ï¼ˆå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ ç±»æŠ€æœ¯ï¼‰é—´çš„æ€§èƒ½å¯¹æ¯”ä¹Ÿæœ‰å¾…æ·±å…¥æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶èšç„¦äºè½»é‡çº§å¼€æºæ¨¡å‹Qwen2.5 - 0.5B Baseï¼Œæ¢ç´¢åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•åœ¨åå¥½å¯¹é½ä¸ç‰¹å®šé¢†åŸŸé€‚é…æ–¹é¢çš„è¡¨ç°ï¼Œä»¥æ˜ç¡®è½»é‡çº§æ¨¡å‹åœ¨ç±»äººå¯¹é½å­¦ä¹ åœºæ™¯ä¸‹çš„èƒ½åŠ›ä¸å±€é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šå¾®è°ƒæŠ€æœ¯å¯¹æ¯”ä¸RLOO reward modelæ¢ç´¢  
å¯¹æ¯”äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€Reinforce Leave - One - Outï¼ˆRLOOï¼‰ä¸‰ç§å¾®è°ƒæŠ€æœ¯ã€‚åœ¨RLOOä¸­ï¼Œè¿˜è¯„ä¼°äº†DeBERTaã€DistilBERTã€Siamese DistilBERTç­‰ä¸åŒå¥–åŠ±æ¨¡å‹ï¼Œä»¥æ­¤æ¢ç©¶å¥–åŠ±æ¨¡å‹å¯¹æœ€ç»ˆç­–ç•¥æ€§èƒ½çš„å½±å“ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡çš„æ•°æ®å¢å¼ºä¸æ¨ç†å·¥å…·ç»“åˆ  
ä¸ºæå‡æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŸºäºCountdownæ•°æ®é›†æ„é€ å«1600ä¸ªæ ·æœ¬çš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼ˆå€ŸåŠ©GPT - 4oå®Œæˆé—®é¢˜ç”Ÿæˆä¸ç­”æ¡ˆéªŒè¯ï¼‰ï¼›åŒæ—¶é‡‡ç”¨best - of - Né‡‡æ ·ç­–ç•¥ï¼Œç»“åˆå¤–éƒ¨éªŒè¯å™¨æ¥æå‡æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ä¸æ­£ç¡®æ€§ï¼Œæ¢ç´¢å¾®è°ƒä¸æ¨ç†æ—¶å·¥å…·ç»“åˆçš„æ½œåŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šï¼ŒDPOç›¸æ¯”SFTåœ¨å…¨å‚æ•°ä¸LoRAé…ç½®ä¸‹å‡èƒ½è¿›ä¸€æ­¥æå‡æ•ˆæœï¼›RLOOå˜ä½“é‡Œï¼Œä»¥DeBERTaä¸ºå¥–åŠ±æ¨¡å‹çš„ç‰ˆæœ¬å¯¹é½åˆ†æ•°æœ€é«˜ã€‚æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œåˆæˆæ•°æ®èƒ½å°å¹…æå‡æ€§èƒ½ï¼Œè€Œç»“åˆå¤–éƒ¨éªŒè¯å™¨çš„best - of - Né‡‡æ ·å¸¦æ¥æ˜¾è‘—å¢ç›Šï¼Œå‡†ç¡®ç‡è¶…0.81ï¼Œæ˜¯SFTçš„ä¸¤å€å¤šã€‚æ•´ä½“è¡¨æ˜è½»é‡çº§æ¨¡å‹ç»æœ‰æ•ˆå¾®è°ƒä¸å·¥å…·è¾…åŠ©å¯å®ç°ä¸é”™æ€§èƒ½ï¼Œå¥–åŠ±æ¨¡å‹è´¨é‡ã€é‡‡æ ·å“åº”å¤šæ ·æ€§å¯¹RLOOå¾ˆå…³é”®ï¼Œå¤–éƒ¨éªŒè¯å™¨ + best - of - Né‡‡æ ·ä¸ºæ•°å­¦æ¨ç†æå‡†ç¡®ç‡æä¾›äº†ä½æˆæœ¬æ–¹æ¡ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºæƒ³ä¼˜åŒ–å°æ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„ç ”ç©¶è€…ä¸å¼€å‘è€…ï¼Œå¯å€Ÿé‰´å¤šå¼ºåŒ–å­¦ä¹ å¾®è°ƒæŠ€æœ¯å¯¹æ¯”æ€è·¯ï¼Œæ˜ç¡®ä¸åŒæŠ€æœ¯åœ¨åå¥½å¯¹é½ç­‰åœºæ™¯çš„ä¼˜åŠ£ï¼›åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰ä»»åŠ¡ä¸­ï¼Œå°è¯•åˆæˆæ•°æ®å¢å¼ºä¸æ¨ç†æ—¶å·¥å…·ï¼ˆå¦‚å¤–éƒ¨éªŒè¯å™¨ + best - of - Né‡‡æ ·ï¼‰ç»“åˆçš„æ–¹å¼ï¼Œåœ¨è®¡ç®—èµ„æºå—é™ä¸‹æå‡å°æ¨¡å‹è¡¨ç°ï¼›åŒæ—¶é‡è§†å¥–åŠ±æ¨¡å‹é€‰å‹ä¸é‡‡æ ·å¤šæ ·æ€§ç­‰å› ç´ å¯¹RLOOç±»æ–¹æ³•çš„å½±å“ï¼Œä¸ºè½»é‡çº§è¯­è¨€æ¨¡å‹é€‚é…å¤šä»»åŠ¡æä¾›å®è·µå‚è€ƒã€‚

## unsupervised-elicitation-of-language-models
### Abstract
To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— ç›‘ç£æ¿€å‘è¯­è¨€æ¨¡å‹æ½œåŠ›ï¼šICMç®—æ³•çªç ´äººç±»ç›‘ç£é™åˆ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“ä¸‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„åè®­ç»ƒèŒƒå¼ï¼Œä¾æ—§ä¾èµ–äººç±»æ¥æŒ‡å®šæœŸæœ›è¡Œä¸ºï¼Œåƒé€šè¿‡æ¼”ç¤ºæˆ–è€…åå¥½åé¦ˆç­‰æ–¹å¼ã€‚ç„¶è€Œï¼Œéšç€ä»»åŠ¡å’Œæ¨¡å‹è¡Œä¸ºæ„ˆå‘å¤æ‚ï¼Œäººç±»ç›‘ç£å˜å¾—è¶Šæ¥è¶Šä¸å¯é ï¼Œè¯­è¨€æ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ æ¨¡ä»¿æ¼”ç¤ºé‡Œçš„é”™è¯¯ï¼Œæˆ–è€…åˆ©ç”¨åé¦ˆä¸­çš„ç¼ºé™·ã€‚å¹¶ä¸”ï¼Œå¯¹äºå…·å¤‡è¶…äººç±»èƒ½åŠ›çš„æ¨¡å‹ï¼Œè·å–é«˜è´¨é‡çš„äººç±»ç›‘ç£å­˜åœ¨å›°éš¾ç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®­ç»ƒè¯­è¨€æ¨¡å‹å»å®Œæˆé‚£äº›äººç±»éš¾ä»¥å¯é æ¼”ç¤ºæˆ–è¯„ä¼°çš„ä»»åŠ¡ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ï¼Œæœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³è¯¥æŒ‘æˆ˜è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ— ç›‘ç£ç®—æ³•ICM
å¼•å…¥Internal Coherence Maximizationï¼ˆICMï¼‰è¿™ä¸€æ— ç›‘ç£ç®—æ³•ï¼Œåœ¨æ²¡æœ‰å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ ‡ç­¾å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å…¶ç›®æ ‡æ˜¯åœ¨ç»™å®šç”±å¸¦æ ‡ç­¾è¾“å…¥æŒ‡å®šçš„ä»»åŠ¡æ—¶ï¼Œè®©é¢„è®­ç»ƒæ¨¡å‹åŸºäºè‡ªèº«ç”Ÿæˆçš„æ ‡ç­¾åœ¨è¯¥ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•æä¾›çš„å¤–éƒ¨æ ‡ç­¾ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ scoring function è¡¡é‡æ ‡ç­¾è´¨é‡
ç”¨ç”±ä¸¤éƒ¨åˆ†ç»„æˆçš„è¯„åˆ†å‡½æ•°è¡¡é‡æ¨¡å‹ç”Ÿæˆæ ‡ç­¾é›†çš„è´¨é‡ï¼Œä¸€æ˜¯â€œç›¸äº’å¯é¢„æµ‹æ€§ï¼ˆMutual Predictabilityï¼‰â€ï¼Œè®¡ç®—æ¨¡å‹åœ¨ä»¥æ‰€æœ‰å…¶ä»–æ ‡ç­¾ä¸ºæ¡ä»¶æ—¶æ¨æ–­æ¯ä¸ªæ ‡ç­¾çš„å¯èƒ½æ€§ï¼Œå°†æ‰€æœ‰ç¤ºä¾‹çš„å¯¹æ•°æ¦‚ç‡æ±‚å’Œï¼›äºŒæ˜¯â€œé€»è¾‘ä¸€è‡´æ€§ï¼ˆLogical Consistencyï¼‰â€ï¼Œé€šè¿‡é€»è¾‘ä¸€è‡´æ€§å‡½æ•°æ£€æŸ¥æ ‡ç­¾é›†é‡Œæ•°æ®ç‚¹çš„æ ‡ç­¾ä¹‹é—´æ˜¯å¦é€»è¾‘ä¸€è‡´ï¼Œä»¥æ­¤è¡¡é‡æ ‡ç­¾ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚æœ€ç»ˆç»“åˆè¿™ä¸¤ä¸ªéƒ¨åˆ†å¾—åˆ°æ•´ä½“è¯„åˆ†å‡½æ•° \( U(D) = \alpha Â· P_Î¸(D) âˆ’ I(D) \) ï¼Œå…¶ä¸­ \( \alpha \) æ˜¯å¹³è¡¡ç›¸äº’å¯é¢„æµ‹æ€§å’Œé€»è¾‘ä¸€è‡´æ€§å¼ºåº¦çš„è¶…å‚æ•°ã€‚ 
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡æ‹Ÿé€€ç«å¯å‘çš„è¿‘ä¼¼æœç´¢ç®—æ³•
ç”±äºæ‰¾åˆ°æœ€å¤§åŒ–è¯„åˆ†å‡½æ•°çš„æœ€ä¼˜æ ‡ç­¾é›†åœ¨è®¡ç®—ä¸Šä¸å¯è¡Œï¼ˆç°å®æ•°æ®é›†è§„æ¨¡ä¸‹ï¼‰ï¼ŒICMé‡‡ç”¨å—æ¨¡æ‹Ÿé€€ç«å¯å‘çš„é«˜æ•ˆè¿‘ä¼¼ç®—æ³•ã€‚ä»ç©ºçš„æ ‡è®°é›†å¼€å§‹ï¼Œç”¨Kä¸ªéšæœºæ ‡è®°çš„ç¤ºä¾‹åˆå§‹åŒ–æœç´¢è¿‡ç¨‹ï¼Œç„¶åè¿­ä»£æ·»åŠ æ ‡ç­¾ï¼Œæ¯æ¬¡æ·»åŠ æ ‡ç­¾æ—¶æ‰§è¡Œé‡‡æ ·æ–°ç¤ºä¾‹ã€ç¡®å®šæ ‡ç­¾åŒæ—¶ä¿®å¤å¼•å…¥çš„ä¸ä¸€è‡´ã€åŸºäºè¯„åˆ†å‡½æ•°å†³å®šæ˜¯å¦æ¥å—æ–°æ ‡ç­¾è¿™ä¸‰ä¸ªæ­¥éª¤ï¼Œä»¥æ­¤å¢é‡å¼æ‰©å±•æ ‡ç­¾é›†å¹¶æé«˜åˆ†æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GSM8k - verificationã€TruthfulQAå’ŒAlpaca reward modelingä»»åŠ¡ä¸Šï¼ŒICMæ–¹æ³•åŒ¹é…äº†åŸºäºâ€œé»„é‡‘ç›‘ç£ï¼ˆgolden supervisionï¼‰â€è®­ç»ƒçš„æ€§èƒ½ï¼Œä¸”è¶…è¿‡äº†åŸºäºä¼—åŒ…äººç±»ç›‘ç£è®­ç»ƒçš„æ€§èƒ½ï¼›åœ¨è¯­è¨€æ¨¡å‹èƒ½åŠ›è¿œè¶…äººç±»çš„ä»»åŠ¡ï¼ˆå¦‚ä»å†™ä½œæ ·æœ¬è¯†åˆ«ä½œè€…æ€§åˆ«ï¼‰ä¸Šï¼ŒICMæ¯”åŸºäºäººç±»æ ‡ç­¾çš„è®­ç»ƒèƒ½æ˜¾è‘—æ›´å¥½åœ°æ¿€å‘æ¨¡å‹èƒ½åŠ›ï¼›åœ¨å‰æ²¿æ¨¡å‹è®­ç»ƒæ–¹é¢ï¼Œç”¨ICMè®­ç»ƒæ— ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŸºäºClaude 3.5 Haikuçš„åŠ©æ‰‹ï¼Œç»“æœæ˜¾ç¤ºæ— ç›‘ç£å¥–åŠ±æ¨¡å‹åœ¨Rewardbenchè¯„ä¼°ä¸­è¶…è¿‡åŸºäºç”Ÿäº§çº§é«˜è´¨é‡äººç±»ç›‘ç£è®­ç»ƒçš„å¯¹åº”æ¨¡å‹ï¼Œä¸”æ— ç›‘ç£åŠ©æ‰‹ç­–ç•¥åœ¨ä¸åŸºäºäººç±»ç›‘ç£å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„ç­–ç•¥å¤´å¯¹å¤´æ¯”è¾ƒä¸­ï¼Œèµ¢å¾—60%çš„å¯¹æ¯”ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ICMç®—æ³•ä¸ºçªç ´äººç±»ç›‘ç£é™åˆ¶è®­ç»ƒè¯­è¨€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ï¼Œè¯æ˜åœ¨ç°å®ç”Ÿäº§è§„æ¨¡åœºæ™¯ä¸­æ— ç›‘ç£æ¿€å‘èƒ½è¶…è¶Šäººç±»ç›‘ç£ï¼Œä¸ºåè®­ç»ƒå‰æ²¿æ¨¡å‹æˆä¸ºé€šç”¨åŠ©æ‰‹æä¾›äº†å®ç”¨æ–¹æ³•ï¼›å…¶è®¾è®¡çš„è¡¡é‡æ ‡ç­¾è´¨é‡çš„è¯„åˆ†å‡½æ•°æ€è·¯ï¼Œä»¥åŠå—æ¨¡æ‹Ÿé€€ç«å¯å‘çš„è¿‘ä¼¼æœç´¢ç®—æ³•ï¼Œåœ¨å¤„ç†éœ€æ¨¡å‹è‡ªèº«ç”Ÿæˆæ ‡ç­¾ä¼˜åŒ–ä»»åŠ¡ã€è§£å†³è®¡ç®—ä¸å¯è¡Œçš„ä¼˜åŒ–é—®é¢˜ç­‰åœºæ™¯ä¸­ï¼Œéƒ½æœ‰ä¸€å®šçš„å€Ÿé‰´æ„ä¹‰ï¼Œä¸ºåç»­ç›¸å…³ç ”ç©¶åœ¨æ–¹æ³•è®¾è®¡å’Œç®—æ³•é€‰æ‹©ä¸Šæä¾›äº†å‚è€ƒæ–¹å‘ã€‚

## reward-models-enable-scalable-code-verification-by-trading-accuracy-for-throughput
### Abstract
The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¥–åŠ±æ¨¡å‹ï¼šç”¨ç²¾åº¦æ¢ååé‡ï¼Œè§£é”å¤§è§„æ¨¡ä»£ç éªŒè¯æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³ç¼–ç ä»»åŠ¡æ—¶ï¼Œâ€œç”Ÿæˆ - æ’åºâ€æ˜¯å¸¸ç”¨èŒƒå¼ï¼Œå…¶ä¸­æ’åºç¯èŠ‚ä¾èµ–éªŒè¯å™¨åˆ¤æ–­ç¨‹åºæ­£ç¡®æ€§ã€‚ä»¥å¾€å…±è¯†è®¤ä¸ºæœ‰å…¨é¢éªŒè¯å™¨ï¼ˆå¦‚å®Œæ•´æµ‹è¯•å¥—ä»¶ï¼‰æ—¶åº”ä¼˜å…ˆé€‰ç”¨ï¼Œå´é²œå°‘è€ƒé‡é€Ÿåº¦ä¸ç²¾åº¦çš„æƒè¡¡ã€‚ä½†éšç€æ¨¡å‹èƒ½å¤„ç†æ›´å¤æ‚è½¯ä»¶ä»»åŠ¡ï¼Œå…¨é¢éªŒè¯å™¨çš„éªŒè¯æˆæœ¬ï¼ˆå¦‚è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶è€—æ—¶ï¼‰æ€¥å‰§æ”€å‡ï¼Œç”šè‡³ä¼šæŠµæ¶ˆå¤šé‡‡æ ·å€™é€‰æ–¹æ¡ˆå¸¦æ¥çš„æ€§èƒ½å¢ç›Šã€‚å› æ­¤ï¼Œè®ºæ–‡è¯•å›¾æ¢ç©¶ï¼šåœ¨ç²¾åº¦ä¸é€Ÿåº¦çš„æƒè¡¡ä¸­ï¼Œç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰èƒ½å¦åœ¨ä¸ç‰ºç‰²è¿‡å¤šç²¾åº¦çš„å‰æä¸‹ï¼Œå¤§å¹…æå‡éªŒè¯é€Ÿåº¦ï¼ŒåŠ©åŠ›æ„å»ºå¯æ‰©å±•çš„ä»£ç éªŒè¯ç³»ç»Ÿï¼Ÿ

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ˜ç¡®ORMåœ¨å¯æ‰©å±•ç¨‹åºéªŒè¯ä¸­çš„ä»·å€¼  
æ‰“ç ´â€œæœ‰å…¨é¢éªŒè¯å™¨å°±å¼ƒç”¨ORMâ€çš„å›ºæœ‰è®¤çŸ¥ï¼Œé€šè¿‡å®è¯åˆ†æè¡¨æ˜ï¼šORMå¯åœ¨ç²¾åº¦ä¸ååé‡é—´åšæƒè¡¡ï¼Œå®ç°å¹³å‡9.55å€äºæœ€å¼ºéªŒè¯å™¨ï¼ˆå¦‚å®Œæ•´æµ‹è¯•å¥—ä»¶ï¼‰çš„é€Ÿåº¦æå‡ï¼Œä¸”ç›¸æ¯”ä»…ç”¨ä»£ç æ£€æŸ¥å·¥å…·ï¼ˆlinterï¼‰è¿‡æ»¤å†å–å¸¸è§å“åº”çš„æ–¹å¼ï¼Œç²¾åº¦è¿˜èƒ½é«˜å‡º33.55%ï¼Œä¸ºå¤§è§„æ¨¡ä»£ç éªŒè¯æä¾›äº†é«˜æ•ˆè·¯å¾„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºâ€œç”Ÿæˆ - å‰ªæ - æ’åºâ€ç­–ç•¥  
åœ¨ä¼ ç»Ÿâ€œç”Ÿæˆ - æ’åºâ€åŸºç¡€ä¸Šï¼Œå¼•å…¥â€œå‰ªæâ€ç¯èŠ‚ï¼šå…ˆç”¨å¼±éªŒè¯å™¨ï¼ˆå¦‚è¿è¡Œéƒ¨åˆ†æµ‹è¯•ç”¨ä¾‹ã€è¯­æ³•æ£€æŸ¥ç­‰ï¼‰è¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„å€™é€‰ç¨‹åºï¼Œå†ç”¨ORMè¿›è¡Œæ’åºã€‚è¯¥ç­–ç•¥èƒ½å‡å°‘ç²¾åº¦æŸå¤±å¹¶è¿›ä¸€æ­¥æå‡ååé‡ï¼Œæ¯”å¦‚ä»…ç”¨1ä¸ªæµ‹è¯•ç”¨ä¾‹è¿‡æ»¤ï¼Œç›¸æ¯”çº¯ORMç­–ç•¥ï¼Œç²¾åº¦æå‡2.85%ä¸”é€Ÿåº¦é¢å¤–åŠ å¿«16.93%ï¼›ç”¨10ä¸ªæµ‹è¯•ç”¨ä¾‹æ—¶ï¼Œç²¾åº¦æå‡10.38%ï¼Œååé‡ä»…æŸå¤±16.69%ï¼Œå´ä»æ¯”å®Œæ•´æµ‹è¯•å¥—ä»¶å¿«29.71%ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ­ç¤ºå¼±éªŒè¯å™¨ä½œç”¨æœºåˆ¶  
é€šè¿‡å®éªŒåˆ†æâ€œç”Ÿæˆ - å‰ªæ - æ’åºâ€æœ‰æ•ˆçš„åŸå› ï¼šå¼±éªŒè¯å™¨èƒ½ç§»é™¤é«˜ã€ä½æ’åä¸­é”™è¯¯çš„å€™é€‰æ–¹æ¡ˆï¼Œç¼“è§£ORMçš„ä¸ç²¾ç¡®æ€§ï¼Œé™ä½ç»“æœæ–¹å·®ï¼Œä»æœºåˆ¶å±‚é¢è§£é‡Šäº†è¯¥ç­–ç•¥åœ¨ç²¾åº¦ - é€Ÿåº¦æƒè¡¡ä¸Šçš„ä¼˜åŠ¿ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- é€Ÿåº¦ä¸ç²¾åº¦æƒè¡¡å±‚é¢ï¼šORMåœ¨æ¢å–ååé‡ï¼ˆé€Ÿåº¦ï¼‰æ—¶èƒ½è¾ƒå¥½ä¿ç•™ç²¾åº¦ï¼Œâ€œç”Ÿæˆ - å‰ªæ - æ’åºâ€ç³»ç»Ÿç›¸æ¯”å®Œæ•´æµ‹è¯•å¥—ä»¶ï¼Œé€Ÿåº¦å¿«11.65å€ï¼Œä»…æŸå¤±8.33%çš„ç²¾åº¦ã€‚ 
- ä¸åŒç­–ç•¥å¯¹æ¯”ï¼šâ€œç”Ÿæˆ - å‰ªæ - æ’åºâ€ä¸­ï¼Œå¼±éªŒè¯å™¨ï¼ˆå¦‚å°‘é‡æµ‹è¯•ç”¨ä¾‹ï¼‰è¾…åŠ©ORMçš„æ–¹å¼ï¼Œåœ¨ç²¾åº¦å’Œé€Ÿåº¦æå‡ä¸Šå‡ä¼˜äºçº¯ORMæˆ–ä»…ç”¨å¼±éªŒè¯å™¨çš„æ–¹å¼ï¼›å¢åŠ æµ‹è¯•ç”¨ä¾‹æ•°é‡æå‡å‰ªæç”¨å¼±éªŒè¯å™¨ç²¾åº¦æ—¶ï¼Œååé‡æŸå¤±å¯æ§ä¸”æ•´ä½“ä»æ¯”æœ€å¼ºéªŒè¯å™¨é«˜æ•ˆã€‚
- å¼±éªŒè¯å™¨ä»·å€¼ï¼šå®è¯æ˜¾ç¤ºå…¶èƒ½é€šè¿‡è¿‡æ»¤é”™è¯¯å€™é€‰ï¼ˆæ— è®ºæ’åé«˜ä½ï¼‰ï¼Œå‡è½»ORMè¯¯å·®ï¼Œè®©æ•´ä¸ªéªŒè¯æ’åºæµç¨‹æ›´ç¨³å®šé«˜æ•ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- ç³»ç»Ÿè®¾è®¡è§’åº¦ï¼šåœ¨éœ€éªŒè¯å¤§é‡ç¨‹åºçš„åœºæ™¯ï¼ˆå¦‚å¤§è§„æ¨¡ä»£ç åº“CI/CDã€å¤æ‚ç¼–ç¨‹ä»»åŠ¡æ±‚è§£ï¼‰ï¼Œå¯å€Ÿé‰´â€œç²¾åº¦ - ååé‡â€æƒè¡¡æ€è·¯ï¼Œå¼•å…¥ORMå’Œå¼±éªŒè¯å™¨æ„å»ºåˆ†å±‚éªŒè¯æ¶æ„ï¼Œå¹³è¡¡é€Ÿåº¦ä¸æ­£ç¡®æ€§è¦æ±‚ã€‚ 
- æ–¹æ³•åˆ›æ–°è§’åº¦ï¼šâ€œç”Ÿæˆ - å‰ªæ - æ’åºâ€çš„ä¸¤é˜¶æ®µç­–ç•¥ï¼Œä¸ºå¤„ç†éœ€å¤§é‡å€™é€‰ç­›é€‰çš„ä»»åŠ¡ï¼ˆä¸é™äºä»£ç ï¼Œå¦‚æ•°å­¦é¢˜æ±‚è§£ã€æ¨ç†ä»»åŠ¡ç­‰ï¼‰æä¾›äº†æ€§èƒ½ä¼˜åŒ–èŒƒå¼ï¼Œå³å…ˆç”¨ä½æˆæœ¬å¼±è¿‡æ»¤å·¥å…·ç¼©å°èŒƒå›´ï¼Œå†ç”¨æ›´é«˜æ•ˆä½†ç¨æ¬ ç²¾åº¦çš„æ¨¡å‹åšç²¾ç»†æ’åºã€‚ 
- è®¤çŸ¥çªç ´è§’åº¦ï¼šæ‰“ç ´â€œå…¨é¢éªŒè¯å™¨è‡³ä¸Šâ€çš„æ€ç»´å®šå¼ï¼Œæ„è¯†åˆ°åœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸‹ï¼Œç»“åˆä¸åŒç²¾åº¦ã€é€Ÿåº¦çš„éªŒè¯å·¥å…·åšååŒï¼Œæ˜¯æå‡ç³»ç»Ÿå¯æ‰©å±•æ€§çš„å…³é”®ï¼Œä¸ºåç»­æ¢ç´¢æ›´é«˜æ•ˆçš„AIè¾…åŠ©ç¼–ç¨‹ã€è‡ªåŠ¨éªŒè¯ç­‰æ–¹å‘æ‰“å¼€æ–°æ€è·¯ã€‚

## dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision
### Abstract
While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DreamCSï¼šæ— é…å¯¹3Då¥–åŠ±ç›‘ç£ä¸‹çš„å‡ ä½•æ„ŸçŸ¥æ–‡æœ¬åˆ°3Dç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬åˆ°3Dç”ŸæˆæŠ€æœ¯åœ¨æ•°å­—æ¼«ç”»ã€æ¸¸æˆã€ç”µå½±å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸå±•ç°å‡ºå…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•ç”Ÿæˆçš„3Dèµ„äº§å¾€å¾€éš¾ä»¥å¥‘åˆäººç±»åå¥½ã€‚å½“å‰3Då†…å®¹çš„åå¥½å¯¹é½æŠ€æœ¯ï¼Œé€šå¸¸ä¾èµ–éš¾æ”¶é›†çš„åå¥½é…å¯¹å¤šè§†è§’2Då›¾åƒæ¥è®­ç»ƒ2Då¥–åŠ±æ¨¡å‹ä»¥æŒ‡å¯¼3Dç”Ÿæˆï¼Œç„¶è€Œè¿™ç§æ–¹å¼å› å›ºæœ‰2Dåå·®æ˜“äº§ç”Ÿå‡ ä½•ä¼ªå½±ã€‚åŒæ—¶ï¼Œæ”¶é›†é…å¯¹çš„åå¥½æ ‡è®°æ•°æ®æˆæœ¬é«˜ã€è€—æ—¶ä¸”å›°éš¾ï¼Œä¸”2Dç©ºé—´çš„åé¦ˆä»…åŸºäºå›¾åƒæ¸²æŸ“è€Œé3Dç»“æ„ï¼Œä¼šå¯¼è‡´å¦‚è§†è§’ä¸ä¸€è‡´ã€Janusè„¸é—®é¢˜ç­‰å‡ ä½•ç¼ºé™·ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½æä¾›å‡ ä½•çº§åé¦ˆä¸”æ‘†è„±é…å¯¹æ•°æ®ä¾èµ–çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»º3D - MeshPrefæ•°æ®é›†
ä¸ºè§£å†³é…å¯¹åå¥½æ ‡è®°3Dæ•°æ®æ”¶é›†éš¾çš„é—®é¢˜ï¼Œæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ— é…å¯¹3Dåå¥½æ•°æ®é›†3D - MeshPrefï¼ŒåŒ…å«14000 + æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰æ–‡æœ¬æç¤ºã€3Dèµ„äº§åŠå…¶åå¥½å¥–åŠ±åˆ†æ•°ã€‚ä»Cap3Dç­›é€‰å¤šæ ·é«˜è´¨é‡ç½‘æ ¼ï¼Œç”¨Llama - Meshå¯¹å‡ ä½•ä¿çœŸåº¦ã€è¯­ä¹‰å¯¹é½å’Œç»“æ„åˆç†æ€§è¯„åˆ†ï¼Œå†ç»äººå·¥éªŒè¯ä¼˜åŒ–åˆ†æ•°ï¼Œå°†é«˜å¥–åŠ±3Dèµ„äº§é€‰ä¸ºåå¥½æ ·æœ¬ï¼Œä½å¥–åŠ±é€‰ä¸ºéåå¥½æ ·æœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºRewardCSå¥–åŠ±æ¨¡å‹
é’ˆå¯¹æ— é…å¯¹æ•°æ®è®­ç»ƒ3Då¥–åŠ±æ¨¡å‹çš„éš¾é¢˜ï¼Œæå‡ºé¦–ä¸ªåŸºäºæ— é…å¯¹æ•°æ®è®­ç»ƒçš„3Då‡ ä½•æ„ŸçŸ¥å¥–åŠ±æ¨¡å‹RewardCSã€‚å¼•å…¥åŸºäºæŸ¯è¥¿ - æ–½ç“¦èŒ¨ï¼ˆCSï¼‰æ•£åº¦çš„åˆ†å¸ƒçº§è®­ç»ƒç›®æ ‡ï¼Œå°†åå¥½å’Œéåå¥½èµ„äº§çš„åµŒå…¥è§†ä¸ºä¸¤ä¸ªåˆ†å¸ƒçš„æ ·æœ¬ï¼Œä¼˜åŒ–å®ƒä»¬ä¹‹é—´çš„CSæ•£åº¦ï¼Œä½¿æ¨¡å‹ç»™å‡ ä½•å’Œè¯­ä¹‰æ›´ä¼˜çš„3Dèµ„äº§æ›´é«˜å¥–åŠ±ã€‚è¿˜è¯æ˜äº†æ— é…å¯¹æ•°æ®ä¸Šçš„CSæ•£åº¦ä¸ä¼ ç»Ÿé…å¯¹åå¥½ç›‘ç£æ¸è¿‘ç­‰ä»·ï¼Œä¸ºæ— é…å¯¹å¥–åŠ±å­¦ä¹ èŒƒå¼æä¾›ç†è®ºä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºDreamCSæ¡†æ¶
ç°æœ‰æ–‡æœ¬åˆ°3Dæ¡†æ¶ç¼ºä¹å‡ ä½•çº§åé¦ˆåŸç”Ÿæ”¯æŒï¼Œä¸ºæ­¤å¼€å‘DreamCSï¼Œé¦–ä¸ªé›†æˆRewardCSåˆ°ç°æœ‰æ–‡æœ¬åˆ°3Dç®¡çº¿çš„3Då¥–åŠ±å¼•å¯¼æ¡†æ¶ã€‚é€šè¿‡ä¸‰é¡¹åˆ›æ–°å®ç°ä¸éšå¼å’Œæ˜¾å¼3Dè¡¨ç¤ºæ— ç¼åä½œï¼šå¯å¾®åˆ†ç½‘æ ¼åŒ–ä½¿éšå¼åœºåˆ°ç«¯åˆ°ç«¯æ¢¯åº¦æµæˆä¸ºå¯èƒ½ï¼›è‡ªé€‚åº”ç½‘æ ¼èåˆåœ¨ä¸ç‰ºç‰²å‡ ä½•ç»†èŠ‚çš„æƒ…å†µä¸‹ä¼˜åŒ–æ‹“æ‰‘ä»¥é€‚é…RewardCSï¼›æ¸è¿›å¼å¥–åŠ±å¼•å¯¼è‡ªåŠ¨å¹³è¡¡ç²—ç»“æ„ç»†åŒ–å’Œç»†ç²’åº¦ä¼˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GPTEval3DåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹äºMVDreamã€Dreamfusionã€Magic3Dç­‰å•é˜¶æ®µå’Œä¸¤é˜¶æ®µæ–‡æœ¬åˆ°3Dç”Ÿæˆç®¡çº¿ï¼Œé›†æˆDreamCSååœ¨å‡ ä½•å¯¹é½å’Œç½‘æ ¼è´¨é‡æ–¹é¢æ¯”ä¹‹å‰å¤šè§†è§’å¼•å¯¼æ–¹æ³•å–å¾—æ›´ä¼˜ç»“æœã€‚ä¸”3Då¥–åŠ±å¼•å¯¼ä¸ç°æœ‰2Dæ–¹æ³•äº’è¡¥ï¼Œç»“åˆåèƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºæ€è·¯ï¼šé¢å¯¹æ•°æ®æ”¶é›†éš¾é¢˜æ—¶ï¼Œå¯è€ƒè™‘æ„å»ºå¤§è§„æ¨¡æ— é…å¯¹ä¸”ç»“åˆå¤§è¯­è¨€æ¨¡å‹åˆç­›ä¸äººå·¥ç²¾ä¿®çš„æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›é«˜è´¨é‡æ•°æ®æ”¯æ’‘ã€‚
2. æ¨¡å‹è®­ç»ƒç›®æ ‡åˆ›æ–°ï¼šåœ¨å¤„ç†æ— é…å¯¹æ•°æ®åœºæ™¯æ—¶ï¼Œå€Ÿé‰´åŸºäºåˆ†å¸ƒçº§çš„è®­ç»ƒç›®æ ‡ï¼ˆå¦‚CSæ•£åº¦ï¼‰æ€è·¯ï¼Œæ‘†è„±é…å¯¹æ¯”è¾ƒä¾èµ–ï¼Œæ‹“å±•æ¨¡å‹åœ¨æ— é…å¯¹æ•°æ®ä¸‹çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. æ¡†æ¶é›†æˆåˆ›æ–°ï¼šé’ˆå¯¹é¢†åŸŸå†…ç°æœ‰æ¡†æ¶çŸ­æ¿ï¼ˆå¦‚ç¼ºä¹å‡ ä½•çº§åé¦ˆæ”¯æŒï¼‰ï¼Œé€šè¿‡å¤šé¡¹é’ˆå¯¹æ€§åˆ›æ–°ï¼ˆå¯å¾®åˆ†ç½‘æ ¼åŒ–ã€è‡ªé€‚åº”ç½‘æ ¼èåˆã€æ¸è¿›å¼å¥–åŠ±å¼•å¯¼ç­‰ï¼‰å®ç°æ–°ç»„ä»¶ä¸ç°æœ‰ç®¡çº¿çš„æ— ç¼é›†æˆï¼Œä¸ºç±»ä¼¼é¢†åŸŸå†…æ¡†æ¶å‡çº§æä¾›å‚è€ƒèŒƒå¼ã€‚

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Athenaï¼šç”¨æ•°æ®é«˜æ•ˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œå¤šæ­¥éª¤æ¨ç†ï¼‰ä»å…·æŒ‘æˆ˜ã€‚ä¸ºå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ç­‰æ–¹æ³•è¢«æ¢ç´¢ï¼Œå…¶ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰èƒ½ä¸ºä¸­é—´æ¨ç†æ­¥éª¤æä¾›ç»†ç²’åº¦åé¦ˆï¼Œæ€§èƒ½æ›´ä¼˜ä¸”æ³›åŒ–æ€§å¼ºã€‚ç„¶è€Œï¼ŒPRMs å‘å±•é¢ä¸´ä¸¤å¤§éš¾é¢˜ï¼šä¸€æ˜¯è·å–å¸¦è¿‡ç¨‹æ ‡ç­¾çš„é«˜è´¨é‡æ•°æ®æˆæœ¬é«˜ï¼ˆéœ€å¤§é‡äººå·¥æ ‡æ³¨æˆ–è®¡ç®—æ˜‚è´µçš„è‡ªåŠ¨åŒ–æ ‡æ³¨ï¼‰ï¼›äºŒæ˜¯ä¼ ç»Ÿè‡ªåŠ¨åŒ–æ ‡æ³¨ï¼ˆå¦‚è’™ç‰¹å¡æ´›ä¼°è®¡ï¼‰æ˜“äº§ç”Ÿå™ªå£°æ ‡ç­¾ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶å‡è½»æ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œæå‡ PRMs æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¼ºå¼±å®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç”Ÿæˆé«˜è´¨é‡è¿‡ç¨‹æ ‡ç­¾  
ä¼ ç»Ÿè’™ç‰¹å¡æ´›ç­‰è‡ªåŠ¨åŒ–æ ‡æ³¨æ–¹æ³•æ˜“å—å®Œæˆå™¨æ¨ç†èƒ½åŠ›å½±å“ï¼Œæ ‡ç­¾æœ‰å™ªå£°ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡å‘ç°ï¼Œå¼ºå®Œæˆå™¨å³ä¾¿ä¸­é—´æ­¥éª¤é”™è¯¯ä»èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œå¼±å®Œæˆå™¨åˆ™å¯èƒ½åœ¨ä¸­é—´æ­¥éª¤æ­£ç¡®æ—¶ä¹Ÿå¤±è´¥ã€‚åŸºäºæ­¤ï¼Œæå‡ºç”¨å¼±ã€å¼ºå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ä½œä¸ºç­›é€‰å¯é è¿‡ç¨‹æ ‡ç­¾çš„æ ‡å‡†ï¼Œä¿ç•™ä¸¤è€…æ ‡ç­¾ä¸€è‡´çš„æ­¥éª¤ï¼Œå‡å°‘å®Œæˆå™¨å¸¦æ¥çš„åå·®ï¼Œæå‡æ ‡ç­¾è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œçº¦ 5000 æ¡é«˜è´¨é‡æ ‡ç­¾å°±èƒ½æ¯”ä¼ ç»Ÿæ–¹æ³•çº¦ 30 ä¸‡æ¡å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¡¨ç°æ›´ä¼˜ï¼Œä¸”å¤§å¹…é™ä½æ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒçš„è®¡ç®—æˆæœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ PRMs æ€§èƒ½çš„ä¸¤å¤§ç­–ç•¥  
 - ORM åˆå§‹åŒ–ï¼šPRMs é€šå¸¸åŸºäºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¾®è°ƒï¼Œè€Œç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰åœ¨å¤§è§„æ¨¡å“åº”çº§æ•°æ®ä¸Šè®­ç»ƒï¼Œå…·å¤‡å¼±ç›‘ç£ä¸‹è¯„ä¼°ä¸­é—´æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚å› æ­¤ç”¨ ORMs åˆå§‹åŒ– PRMsï¼Œå°† ORMs ä½œä¸ºå¼±ç›‘ç£é¢„è®­ç»ƒï¼ŒPRMs å†åœ¨é«˜è´¨é‡ç»†ç²’åº¦æ­¥éª¤æ•°æ®ä¸Šå¾®è°ƒï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚  
 - è´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ï¼šè¿‡ç¨‹æ ‡ç­¾æ•°æ®å­˜åœ¨æ ‡ç­¾ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡å¯¹å«è´Ÿæ­¥éª¤æ ‡ç­¾çš„æ•°æ®è¿›è¡Œä¸Šé‡‡æ ·ï¼Œè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡é—®é¢˜ï¼Œä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»º Athena ç³»åˆ—æ¨¡å‹å¹¶å¤šåœºæ™¯éªŒè¯  
åŸºäºä¸Šè¿°æ–¹æ³•æ„å»ºç»“æœå¥–åŠ±æ¨¡å‹ Athena - ORM å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ Athena - PRMï¼Œå†åˆ©ç”¨ Athena - PRM é€šè¿‡å¥–åŠ±æ’åºå¾®è°ƒå¾—åˆ° Athena - 7Bã€‚å¹¶åœ¨ä¸‰ä¸ªåœºæ™¯éªŒè¯ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­å¯¹ç­–ç•¥æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªè¾“å‡ºæ’åºï¼›ç›´æ¥è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§ï¼›å¥–åŠ±æ’åºå¾®è°ƒï¼ˆç”¨é«˜å¥–åŠ±å“åº”å¾®è°ƒç­–ç•¥æ¨¡å‹ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- æµ‹è¯•æ—¶ç¼©æ”¾åœºæ™¯ï¼šåœ¨ 7 ä¸ªå¤šæ¨¡æ€æ•°å­¦å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç”¨ Athena - PRM é…åˆä¸åŒè§„æ¨¡ï¼ˆ7B åˆ° 72Bï¼‰ç­–ç•¥æ¨¡å‹ï¼Œæ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ã€‚å¦‚ç”¨ Qwen2.5 - VL - 7B ä½œä¸ºç­–ç•¥æ¨¡å‹æ—¶ï¼Œåœ¨ WeMath åŸºå‡†ä¸Šé›¶æ ·æœ¬åŸºçº¿æå‡ 10.2 åˆ†ï¼Œåœ¨ MathVista æå‡ 7.1 åˆ†ï¼›åœ¨æ–‡æœ¬-only æ•°å­¦åŸºå‡†ç”¨ Mistral - 8B æ—¶æå‡ 8.9 åˆ†ã€‚  
- æ¨ç†æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°åœºæ™¯ï¼šåœ¨ VisualProcessBench åŸºå‡†ä¸Šï¼ŒAthena - PRM è¡¨ç°å¼ºåŠ²ï¼Œè¶…è¶Šå¼€æºçš„ VisualPRM - 8B ç­‰æ¨¡å‹ï¼ŒF1 åˆ†æ•°æ¯”ä¹‹å‰æœ€ä¼˜ç»“æœé«˜ 3.9ï¼Œå±•ç°å‡†ç¡®è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚  
- å¥–åŠ±æ’åºå¾®è°ƒåœºæ™¯ï¼šåŸºäº Qwen2.5 - VL - 7B å¾®è°ƒå¾—åˆ°çš„ Athena - 7Bï¼Œåœ¨ 7 ä¸ªæ•°å­¦å’Œæ¨ç†åŸºå‡†ä¸Šå¤§å¹…æå‡ç­–ç•¥æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ•°æ®é«˜æ•ˆæ ‡æ³¨æ€è·¯ï¼šåˆ©ç”¨å¤šå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç­›é€‰æ ‡ç­¾ï¼Œä¸ºè§£å†³éœ€ç»†ç²’åº¦æ ‡æ³¨ä¸”æ ‡æ³¨æˆæœ¬é«˜çš„ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ï¼Œåœ¨å‡å°‘æ•°æ®é‡åŒæ—¶æå‡æ•°æ®è´¨é‡ï¼Œå®ç°æ•°æ®é«˜æ•ˆåˆ©ç”¨ã€‚  
- æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼šORM åˆå§‹åŒ–å’Œè´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ç­–ç•¥ï¼Œä¸ºæå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½æä¾›äº†å¯å¤ç”¨æ–¹æ³•ï¼Œå¯å¯å‘å…¶ä»–å¥–åŠ±æ¨¡å‹æˆ–éœ€ç»†ç²’åº¦åé¦ˆæ¨¡å‹çš„è®­ç»ƒä¼˜åŒ–ã€‚  
- å¤šåœºæ™¯éªŒè¯æ¨¡å¼ï¼šåœ¨æµ‹è¯•æ—¶ç¼©æ”¾ã€æ­¥éª¤è¯„ä¼°ã€æ¨¡å‹å¾®è°ƒç­‰å¤šåœºæ™¯éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢éªŒè¯æ€è·¯æœ‰åŠ©äºæ›´å……åˆ†å±•ç¤ºæ–¹æ³•ä»·å€¼ï¼Œä¸ºåç»­ç ”ç©¶æä¾›éªŒè¯èŒƒå¼å‚è€ƒã€‚

## know-what-you-don-t-know--uncertainty-calibration-of-process-reward-models
### Abstract
Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ ¡å‡†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æ¨ç†æ›´â€œè‡ªçŸ¥â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ—¶æ‰©å±•ï¼ˆinference - time scalingï¼‰ç®—æ³•ä¸­ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œå®ƒèƒ½æŒ‡å¯¼æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åšå‡ºå†³ç­–ã€‚ç„¶è€Œï¼Œç°æœ‰å…ˆè¿›çš„PRMå­˜åœ¨æ ¡å‡†ä¸ä½³çš„é—®é¢˜ï¼Œå¸¸å¸¸é«˜ä¼°æˆåŠŸæ¦‚ç‡ï¼Œè¿™ä¼šé™åˆ¶å…¶åœ¨è¯¸å¤šåœºæ™¯çš„æ•ˆç”¨ï¼Œæ¯”å¦‚æä¾›å¯è§£é‡Šçš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€åˆ¤æ–­ä½•æ—¶â€œæ‰¿è®¤ä¸æ‡‚â€ä»¥åŠè‡ªé€‚åº”è°ƒæ•´æ¨ç†è®¡ç®—é¢„ç®—ç­‰ã€‚åŒæ—¶ï¼Œä¼ ç»Ÿæ¨ç†æ—¶æ‰©å±•æ–¹æ³•ï¼ˆå¦‚best - of - Nï¼‰é‡‡ç”¨å›ºå®šé¢„ç®—åˆ†é…æ–¹å¼ï¼Œåœ¨ç®€å•ä»»åŠ¡ä¸Šæµªè´¹è®¡ç®—èµ„æºï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šåˆå¯èƒ½èµ„æºä¸è¶³ï¼Œæ‰€ä»¥éœ€è¦æ›´æ™ºèƒ½çš„è‡ªé€‚åº”ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPRMæ ¡å‡†æ–¹æ³•â€”â€”åˆ†ä½æ•°å›å½’
ç°æœ‰æ ‡å‡†æ ¡å‡†æŠ€æœ¯ï¼ˆå¦‚æ¸©åº¦ç¼©æ”¾ï¼‰æ— æ³•æœ‰æ•ˆæ ¡å‡†PRMï¼Œä¸ºæ­¤æå‡ºåŸºäºåˆ†ä½æ•°å›å½’çš„æ ¡å‡†æ–¹æ¡ˆã€‚é€šè¿‡æ”¶é›†LLMç”Ÿæˆçš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¹¶ç”¨è’™ç‰¹å¡æ´›å±•å¼€å¾—åˆ°æ¯ä¸ªå‰ç¼€è½¨è¿¹çš„çœŸå®æˆåŠŸæ¦‚ç‡ä½œä¸ºæ•°æ®ï¼Œå°†PRMçš„é¢„æµ‹å¤´æ›¿æ¢ä¸ºåˆ†ä½æ•°å›å½’æ¨¡å‹å¹¶å¾®è°ƒï¼Œä½¿PRMè¾“å‡ºèƒ½æ›´å‡†ç¡®åæ˜ LLMå¾—åˆ°æ­£ç¡®ç­”æ¡ˆçš„ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶è¿˜èƒ½ç»™å‡ºç½®ä¿¡è¾¹ç•Œã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®ä¾‹è‡ªé€‚åº”ç¼©æ”¾ï¼ˆIASï¼‰æ¡†æ¶
åˆ©ç”¨æ ¡å‡†åçš„PRMï¼Œä¾æ®å¯¹éƒ¨åˆ†æ¨ç†è½¨è¿¹äº§ç”Ÿæ­£ç¡®æœ€ç»ˆç­”æ¡ˆçš„ä¼°è®¡å¯èƒ½æ€§ï¼ŒåŠ¨æ€è°ƒæ•´æ¨ç†é¢„ç®—ã€‚é’ˆå¯¹å¸¸ç”¨çš„best - of - Nå’Œbeam searchæ¨ç†æ—¶æ‰©å±•æ–¹æ³•ï¼Œè¯¥æ¡†æ¶èƒ½è®©å¤§æ¨¡å‹åœ¨æ¨ç†æ—¶åƒäººç±»è§£å†³é—®é¢˜ä¸€æ ·ï¼Œå¯¹æœ‰æŒ‘æˆ˜æˆ–æœ‰æ½œåŠ›çš„æ¨ç†è·¯å¾„æŠ•å…¥æ›´å¤šè®¡ç®—èµ„æºï¼Œè€Œéåƒä¼ ç»Ÿæ–¹æ³•é‚£æ ·å¯¹æ¯ä¸ªæŸ¥è¯¢åˆ†é…å›ºå®šæ•°é‡çš„æ¨ç†è½¨è¿¹ã€‚å¹¶ä¸”ä»ç†è®ºä¸Šè¯æ˜ï¼Œæ ¡å‡†åçš„å¥–åŠ±åˆ†æ•°èƒ½ä¼°è®¡ç”Ÿæˆè‡³å°‘ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„é¢å¤–è½¨è¿¹æœ€å°æ•°é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼šï¼ˆiï¼‰æå‡ºçš„PRMæ ¡å‡†æ–¹æ³•å®ç°äº†å°çš„æ ¡å‡†è¯¯å·®ï¼Œæ€§èƒ½è¶…è¿‡åŸºçº¿æ–¹æ³•ï¼›ï¼ˆiiï¼‰éªŒè¯äº†æ ¡å‡†å¯¹å®ç°æœ‰æ•ˆè‡ªé€‚åº”ç¼©æ”¾è‡³å…³é‡è¦ï¼›ï¼ˆiiiï¼‰æ‰€æIASç­–ç•¥åœ¨ä¿æŒæœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡çš„åŒæ—¶é™ä½äº†æ¨ç†æˆæœ¬ï¼Œèƒ½åœ¨å¯¹é—®é¢˜æ›´æœ‰ä¿¡å¿ƒæ—¶åˆç†å‡å°‘è®¡ç®—é‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢ï¼Œåˆ†ä½æ•°å›å½’ç”¨äºæ¨¡å‹æ ¡å‡†çš„æ€è·¯ä¸ºè§£å†³ç±»ä¼¼çš„æ¦‚ç‡é¢„æµ‹æ ¡å‡†é—®é¢˜æä¾›äº†å‚è€ƒï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è¦æ¦‚ç‡é¢„æµ‹æ ¡å‡†çš„æ¨¡å‹åœºæ™¯ï¼›ä»åº”ç”¨å±‚é¢ï¼Œå®ä¾‹è‡ªé€‚åº”çš„èµ„æºåˆ†é…ç†å¿µåœ¨å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–ã€èµ„æºé«˜æ•ˆåˆ©ç”¨æ–¹é¢å…·æœ‰å¯å‘ï¼Œæœªæ¥åœ¨å…¶ä»–éœ€è¦åŠ¨æ€èµ„æºè°ƒé…çš„AIä»»åŠ¡ï¼ˆå¦‚å¤šæ­¥éª¤å†³ç­–ã€å¤æ‚é—®é¢˜æ±‚è§£ï¼‰ä¸­ä¹Ÿå¯èƒ½é€‚ç”¨ï¼›ä»£ç å¼€æºï¼ˆhttps://github.com/azizanlab/instance - adaptive - scalingï¼‰æ–¹ä¾¿ç ”ç©¶è€…å¤ç°å’Œè¿›ä¸€æ­¥æ‹“å±•ç›¸å…³å·¥ä½œï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## learning-to-reason-across-parallel-samples-for-llm-reasoning
### Abstract
Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.
### ğŸŒŸ è®ºæ–‡è§£è¯» | èåˆå¹¶è¡Œä¸é¡ºåºæ¨ç†ï¼ŒSSAè®©å¤§æ¨¡å‹æ¨ç†æ›´é«˜æ•ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šèƒ½åŠ›ä¸æ–­æå‡ï¼Œè€Œæµ‹è¯•æ—¶è®¡ç®—èµ„æºçš„åˆ†é…ï¼ˆå³æµ‹è¯•æ—¶ç¼©æ”¾ï¼‰æ˜¯ä¼˜åŒ–æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹å‘ã€‚ç°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åˆ†å¹¶è¡Œå’Œé¡ºåºä¸¤ç±»ï¼šå¹¶è¡Œç¼©æ”¾æ˜¯ç‹¬ç«‹ç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„å†èšåˆï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨ï¼‰ï¼›é¡ºåºç¼©æ”¾åˆ™è¿­ä»£ä¼˜åŒ–å•ä¸ªè§£ï¼ˆå¦‚åŸºäºæç¤ºçš„è‡ªæˆ‘åæ€ï¼‰ã€‚ä½†å¹¶è¡Œæ–¹æ³•å¸¸å­¤ç«‹çœ‹å¾…æ ·æœ¬ï¼Œé¡ºåºæ–¹æ³•è®¡ç®—æˆæœ¬æˆ–é€‚é…æ€§å—é™ã€‚æœ¬æ–‡æ—¨åœ¨æå‡ºæ–°æ–¹æ³•ï¼ŒèåˆäºŒè€…ä¼˜åŠ¿ï¼Œæ›´é«˜æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºæå‡æ¨ç†æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSample Set Aggregatorï¼ˆSSAï¼‰æ¨¡å‹æ¶æ„  
è®¾è®¡è½»é‡çº§çš„SSAæ¨¡å‹ï¼Œå°†å…¶ä¸ç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ï¼ˆLMansï¼‰è§£è€¦ã€‚å…ˆç”±LManså¹¶è¡Œç”ŸæˆKä¸ªå€™é€‰ç­”æ¡ˆï¼Œå†æŠŠè¿™äº›å€™é€‰ç­”æ¡ˆæ‹¼æ¥æˆåºåˆ—è¾“å…¥SSAï¼ŒSSAé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä»¥è¾“å‡ºæœ€ç»ˆæ­£ç¡®ç­”æ¡ˆã€‚è¿™ç§è®¾è®¡è®©SSAèƒ½åŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºçš„åˆ†å¸ƒç‰¹æ€§ï¼Œç›´æ¥ä¼˜åŒ–ç­”æ¡ˆåˆæˆè¿‡ç¨‹ï¼Œè€Œéå­¤ç«‹è¯„ä¼°å•ä¸ªæ ·æœ¬ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºè¾“å‡ºåˆ†å¸ƒæ¨ç†ï¼Œè§£è€¦è®­ç»ƒä¸æ¨ç†  
SSAä¸ç›´æ¥è®­ç»ƒç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ï¼ˆLManså¯è§†ä¸ºé»‘ç›’ï¼‰ï¼Œè€Œæ˜¯é’ˆå¯¹å…¶é‡‡æ ·è¾“å‡ºè¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§â€œæ¨ç†è¾“å‡ºåˆ†å¸ƒè€Œéè°ƒæ•´æ¨¡å‹å†…éƒ¨â€çš„æ€è·¯ï¼Œè®©æ–¹æ³•æ›´çµæ´»â€”â€”å¯é€‚é…ä¸åŒåŸºç¡€æ¨¡å‹ï¼ˆç”šè‡³æ˜¯åªèƒ½é€šè¿‡APIè°ƒç”¨çš„é»‘ç›’å¤§æ¨¡å‹ï¼‰ï¼Œåªéœ€ç”¨å…¶é‡‡æ ·ç­”æ¡ˆè®­ç»ƒSSAå³å¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»Ÿä¸€å¹¶è¡Œä¸é¡ºåºç¼©æ”¾ä¼˜åŠ¿  
å¹¶è¡Œç¼©æ”¾èƒ½å¿«é€Ÿè·å–å¤šè§†è§’ç­”æ¡ˆï¼Œé¡ºåºç¼©æ”¾å¯è¿­ä»£ä¼˜åŒ–æ¨ç†ï¼›SSAé€šè¿‡â€œå¹¶è¡Œé‡‡æ ·+å•æ­¥é¡ºåºRLèšåˆâ€çš„æ–¹å¼ï¼Œåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç»“åˆäºŒè€…é•¿å¤„ï¼šç”¨å¹¶è¡Œè·å–å¤šæ ·æ€§ï¼Œç”¨SSAçš„é¡ºåºæ¨ç†å®ç°ç²¾å‡†èšåˆï¼Œä¸”ä»…éœ€è®­ç»ƒå°æ¨¡å‹å°±èƒ½å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ€§èƒ½è¶…è¶Šå¼ºåŸºçº¿ï¼šåœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒSSAç›¸æ¯”åŸºäºå¥–åŠ±æ¨¡å‹é‡æ’åºç­‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œå¤§å¹…ç¼©å°äº†æ¨¡å‹å®é™…æ€§èƒ½ä¸â€œç†è®ºæœ€ä¼˜ï¼ˆoracle - bestï¼‰â€ç²¾åº¦çš„å·®è·ã€‚  
2. æ³›åŒ–èƒ½åŠ›çªå‡ºï¼šè·¨æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—ï¼ˆå¦‚Qwen 2.5ã€Llama 3.1ï¼‰ã€æ¨¡å‹è§„æ¨¡ï¼ˆ7B/14B/32Bï¼‰å’Œä»»åŠ¡ï¼ŒSSAéƒ½å±•ç°å‡ºè‰¯å¥½æ³›åŒ–æ€§ã€‚æ¯”å¦‚åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šä¸ºç‰¹å®šæ¨¡å‹è®­ç»ƒçš„SSAï¼Œèƒ½æˆåŠŸèšåˆä¸åŒæ¨¡å‹å®¶æ—ã€è§„æ¨¡åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¾“å‡ºã€‚  
3. è½»é‡åŒ–ä¼˜åŠ¿ï¼šç´§å‡‘çš„SSAæ¨¡å‹èƒ½åŒ¹é…é¡ºåºç¼©æ”¾ä¸­ç»å¼ºåŒ–è®­ç»ƒçš„å¤§æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜å…¶ä½œä¸ºè½»é‡é¡ºåºç¼©æ”¾æ–¹å¼çš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¶æ„è§£è€¦æ€è·¯ï¼šå°†â€œç­”æ¡ˆç”Ÿæˆâ€ä¸â€œç­”æ¡ˆèšåˆåˆ†æâ€è§£è€¦ï¼Œä¸ºåˆ©ç”¨é»‘ç›’å¤§æ¨¡å‹ï¼ˆå¦‚è°ƒç”¨APIçš„å•†ç”¨å¤§æ¨¡å‹ï¼‰æä¾›äº†å¯è¡Œè·¯å¾„â€”â€”åªéœ€è·å–å…¶è¾“å‡ºï¼Œç”¨SSAåšåå¤„ç†å³å¯ï¼Œæ— éœ€æ”¹åŠ¨é»‘ç›’æ¨¡å‹æœ¬èº«ã€‚  
2. æµ‹è¯•æ—¶ç¼©æ”¾æ–°èŒƒå¼ï¼šå±•ç¤ºäº†â€œå¹¶è¡Œé‡‡æ · + é’ˆå¯¹æ€§å°æ¨¡å‹èšåˆâ€åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ½œåŠ›ï¼Œä¸ºåç»­ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—æ•ˆç‡ã€å¹³è¡¡èµ„æºä¸æ€§èƒ½æä¾›äº†æ–°æ–¹å‘ã€‚  
3. å¼ºåŒ–å­¦ä¹ åº”ç”¨å¯å‘ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–èšåˆæ¨¡å‹ï¼ˆSSAï¼‰æ¥æå‡æœ€ç»ˆç­”æ¡ˆç²¾åº¦ï¼ŒéªŒè¯äº†åœ¨â€œè¾“å‡ºåˆ†å¸ƒå±‚é¢åšæ¨ç†ä¼˜åŒ–â€çš„ä»·å€¼ï¼Œå¯å¯å‘æ›´å¤šå›´ç»•æ¨¡å‹è¾“å‡ºåå¤„ç†çš„ç ”ç©¶ã€‚

