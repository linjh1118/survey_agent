# Paper List of Terms(Medical+RL)
- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.13973v1)] [[Code/Page]()] [[TLDR/Notes](#toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models)]

- [25/05] **Disentangling Reasoning and Knowledge in Medical Large Language Models**  
[[Paper](http://arxiv.org/pdf/2505.11462v1)] [[Code/Page]()] [[TLDR/Notes](#disentangling-reasoning-and-knowledge-in-medical-large-language-models)]

- [25/05] **Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner**  
[[Paper](http://arxiv.org/pdf/2505.11404v1)] [[Code/Page](https://github.com/Wenchuan-Zhang/Patho-R1.)] [[TLDR/Notes](#patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner)]

- [25/04] **Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation**  
[[Paper](http://arxiv.org/pdf/2504.18453v1)] [[Code/Page]()] [[TLDR/Notes](#reason-like-a-radiologist--chain-of-thought-and-reinforcement-learning-for-verifiable-report-generation)]

- [25/04] **GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.01886v1)] [[Code/Page](https://github.com/uni-medical/GMAI-VL-R1}{this)] [[TLDR/Notes](#gmai-vl-r1--harnessing-reinforcement-learning-for-multimodal-medical-reasoning)]

- [25/04] **Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection**  
[[Paper](http://arxiv.org/pdf/2504.13186v1)] [[Code/Page]()] [[TLDR/Notes](#advanced-deep-learning-and-large-language-models--comprehensive-insights-for-cancer-detection)]

- [25/03] **Reinforcement Learning for Active Matter**  
[[Paper](http://arxiv.org/pdf/2503.23308v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-for-active-matter)]

- [25/03] **RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models**  
[[Paper](http://arxiv.org/pdf/2503.15784v1)] [[Code/Page]()] [[TLDR/Notes](#rl4med-ddpo--reinforcement-learning-for-controlled-guidance-towards-diverse-medical-image-generation-using-vision-language-foundation-models)]

- [25/03] **Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis**  
[[Paper](http://arxiv.org/pdf/2503.16547v1)] [[Code/Page]()] [[TLDR/Notes](#empowering-medical-multi-agents-with-clinical-consultation-flow-for-dynamic-diagnosis)]



# TLDR/Notes
## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œå°‘é‡æ•°æ®è®­ç»ƒæœç´¢ä»£ç†ï¼Œs3æ¡†æ¶å¼•é¢†RLæ–°ç¯‡ç« â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰ç³»ç»Ÿçš„å‡ºç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾—ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¿é—®å¤–éƒ¨çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨ä»…å…³æ³¨æœç´¢çš„æŒ‡æ ‡ï¼ˆå¦‚NDCGï¼‰æ¥ä¼˜åŒ–æ£€ç´¢ï¼Œå¿½ç•¥ä¸‹æ¸¸ä»»åŠ¡çš„å®é™…æ•ˆç”¨ï¼Œè¦ä¹ˆå°†æ•´ä¸ªLLMè¿›è¡Œå¾®è°ƒä»¥å®ç°æ£€ç´¢å’Œç”Ÿæˆçš„è”åˆä¼˜åŒ–ï¼Œè¿™å¯¼è‡´æ£€ç´¢ä¸ç”Ÿæˆç´§å¯†è€¦åˆï¼Œé™åˆ¶äº†æœç´¢çš„å®é™…æ•ˆç”¨å’Œä¸å†»ç»“æˆ–ä¸“æœ‰æ¨¡å‹çš„å…¼å®¹æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä»¥æ›´é«˜æ•ˆã€æ¨¡å—åŒ–çš„æ–¹å¼ä¼˜åŒ–æœç´¢ä»£ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†s3æ¡†æ¶ï¼Œä¸€ç§è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œå®ƒå°†æœç´¢å™¨ä¸ç”Ÿæˆå™¨åˆ†ç¦»ï¼Œå¹¶ä½¿ç”¨ä¸€ç§åä¸ºâ€œGain Beyond RAGâ€çš„å¥–åŠ±ä¿¡å·æ¥è®­ç»ƒæœç´¢å™¨ã€‚è¿™ç§å¥–åŠ±ä¿¡å·é‡åŒ–äº†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç›¸å¯¹äºç®€å•top-kæ£€ç´¢çš„ç”Ÿæˆå‡†ç¡®åº¦æå‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
s3æ¡†æ¶åœ¨ä»…æœ‰2.4kè®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå°±èƒ½è¶…è¶ŠåŸºäºè¶…è¿‡70å€æ•°æ®è®­ç»ƒçš„åŸºçº¿ï¼Œè¿™åœ¨å…­ä¸ªé€šç”¨é—®ç­”å’Œäº”ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸€è‡´ã€‚è¿™è¡¨æ˜äº†s3æ¡†æ¶åœ¨æ•°æ®æ•ˆç‡å’Œæ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼Œs3æ¡†æ¶åœ¨å…­ä¸ªé€šç”¨é—®ç­”å’Œäº”ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨æ¯”ç°æœ‰æ–¹æ³•å°‘å¾—å¤šçš„è®­ç»ƒæ•°æ®ï¼Œå®ç°äº†æ›´ä½³çš„ä¸‹æ¸¸æ€§èƒ½ã€‚è¿™è¯æ˜äº†s3æ¡†æ¶åœ¨ä¼˜åŒ–æœç´¢è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„s3æ¡†æ¶ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿæä¾›äº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ€è·¯ï¼Œå³é€šè¿‡åˆ†ç¦»æœç´¢å™¨å’Œç”Ÿæˆå™¨ï¼Œä¸“æ³¨äºä¼˜åŒ–æœç´¢è´¨é‡ï¼Œè€Œä¸æ˜¯è”åˆä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†æ•°æ®æ•ˆç‡ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å…¼å®¹æ€§å’Œå®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„â€œGain Beyond RAGâ€å¥–åŠ±ä¿¡å·ä¸ºè¯„ä¼°æ£€ç´¢è´¨é‡æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œè¿™äº›åˆ›æ–°ç‚¹å¯¹äºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨éƒ½å…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ã€‚

## toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models
### Abstract
Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "è¿ˆå‘ç²¾å‡†ï¼šå¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—è§†è§‰é—®ç­”ä¸­çš„å¾®è°ƒç­–ç•¥"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¾®è°ƒæ–¹æ³•å·²ç»æ˜¾è‘—æ”¹å˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•è½¨è¿¹ï¼Œå°¤å…¶æ˜¯åœ¨å¼•å…¥äº†ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¹‹åã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ–¹æ³•ç›´æ¥åº”ç”¨äºåŒ»ç–—ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯éœ€è¦ä¸´åºŠç²¾ç¡®è¾“å‡ºçš„åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼Œä»ç„¶é¢ä¸´ç€æŒ‘æˆ˜ã€‚æœ¬æ–‡çš„åŠ¨æœºåœ¨äºå¦‚ä½•å°†æ¨¡å‹å“åº”ä¸ä¸´åºŠæœŸæœ›ç›¸åŒ¹é…ï¼Œæ¢è®¨äº†å½±å“RLåœ¨åŒ»ç–—VQAä¸­æœ‰æ•ˆæ€§çš„å››ä¸ªå…³é”®ç»´åº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡å¯¹æ¯”äº†ä»é›¶å¼€å§‹è®­ç»ƒä¸åŸºäºæŒ‡ä»¤çš„å¾®è°ƒç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä»é›¶å¼€å§‹è®­ç»ƒå¯ä»¥é¼“åŠ±æ›´å¤šçš„æ¨ç†ï¼Œä½†å®ƒç¼ºä¹æŒ‡ä»¤å¾®è°ƒæ‰€æä¾›çš„åŒ»å­¦çŸ¥è¯†å’Œè¯­è¨€æµç•…æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥äº†åŒ»å­¦è¯­ä¹‰å¯¹é½çš„æ¦‚å¿µï¼Œé€šè¿‡è®¾è®¡ä¸€ä¸ªè¯­ä¹‰å¯¹é½å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹å“åº”ä¸é¢„å®šä¹‰ä¸“å®¶LLMçš„åˆ¤æ–­ç›¸åŒ¹é…ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œæ¨ç†è´¨é‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¢è®¨äº†é•¿åº¦åŸºäºå¥–åŠ±å¯¹é•¿é“¾æ¨ç†çš„å½±å“ï¼Œå‘ç°ä»…ä¾èµ–é•¿åº¦å¥–åŠ±å¾€å¾€å¯¼è‡´å†—é•¿ä¸”ä¸å¤Ÿå‡†ç¡®çš„ç­”æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šç ”ç©¶äº†åŒ»å­¦MLLMä¸­çš„åè§é—®é¢˜ï¼Œé€šè¿‡å®æ–½Dr.GRPOæ–¹æ³•ï¼Œæé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ ‡è®°æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šé€šè¿‡å¯¹æ¯”æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒGRPO-based RLå¾®è°ƒï¼Œå‘ç°åè€…åœ¨å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ä¸Šå‡å…·æœ‰ä¼˜åŠ¿ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨PMC-VQAåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†ä¸Šè¿°åˆ›æ–°ç‚¹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGRPO-based RLå¾®è°ƒåœ¨åŒ»ç–—VQAä»»åŠ¡ä¸­ï¼Œæ— è®ºæ˜¯åœ¨å‡†ç¡®æ€§è¿˜æ˜¯åœ¨æ¨ç†è´¨é‡ä¸Šéƒ½ä¼˜äºä¼ ç»Ÿçš„SFTæ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºåŒ»ç–—é¢†åŸŸä¸­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒæä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°åŒ»å­¦è¯­ä¹‰å¯¹é½å’Œå‡å°‘åè§æä¾›äº†å®è·µæŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼ŒGRPO-based RLå¾®è°ƒåœ¨å¼€å‘æ›´ç¬¦åˆä¸´åºŠéœ€æ±‚çš„èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚

## disentangling-reasoning-and-knowledge-in-medical-large-language-models
### Abstract
Medical reasoning in large language models (LLMs) aims to emulate clinicians'
diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and
PubMedQA often mix reasoning with factual recall. We address this by separating
11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using
a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human
performance. Our analysis shows that only 32.8 percent of questions require
complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)
and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent
gaps between knowledge and reasoning performance. For example, m1 scores 60.5
on knowledge but only 47.1 on reasoning. In adversarial tests where models are
misled with incorrect initial reasoning, biomedical models degrade sharply,
while larger or RL-trained general models show more robustness. To address
this, we train BioMed-R1 using fine-tuning and reinforcement learning on
reasoning-heavy examples. It achieves the strongest performance among similarly
sized models. Further gains may come from incorporating clinical case reports
and training with adversarial and backtracking scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "ç²¾å‡†æ‹†è§£åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹ï¼šæ¨ç†ä¸çŸ¥è¯†çš„åˆ†ç¦»ä¹‹é“"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŒ»å­¦é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯•å›¾æ¨¡æ‹Ÿä¸´åºŠåŒ»ç”Ÿçš„è¯Šæ–­æ€ç»´è¿‡ç¨‹ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ ‡å‡†å¦‚MedQA-USMLEã€MedMCQAå’ŒPubMedQAç­‰ï¼Œå¾€å¾€å°†éœ€è¦æ¨ç†çš„é—®é¢˜ä¸ä»…éœ€äº‹å®å›å¿†çš„é—®é¢˜æ··åˆåœ¨ä¸€èµ·ã€‚è¿™ä½¿å¾—è¯„ä¼°LLMçš„çœŸæ­£æ¨ç†èƒ½åŠ›å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ†ç¦»å‡ºä¸“æ³¨äºæ¨ç†å’Œä¸“æ³¨äºçŸ¥è¯†çš„é—®é¢˜ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°LLMåœ¨åŒ»å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡ä½¿ç”¨PubMedBERTåˆ†ç±»å™¨ï¼Œå°†11ä¸ªç”Ÿç‰©åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­çš„é—®é¢˜åˆ†ä¸ºæ¨ç†å‹å’ŒçŸ¥è¯†å‹ï¼Œåˆ†ç±»å™¨è¾¾åˆ°äº†çº¦81%çš„å‡†ç¡®ç‡ï¼Œä¸äººç±»è¡¨ç°ç›¸å½“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
é€šè¿‡å¯¹ç”Ÿç‰©åŒ»å­¦æ¨¡å‹å’Œé€šç”¨é¢†åŸŸæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨çŸ¥è¯†å‹å’Œæ¨ç†å‹é—®é¢˜ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜æ˜¾å·®è·ã€‚ä¸ºäº†æé«˜æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡è®­ç»ƒäº†BioMed-R1æ¨¡å‹ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†å‹ç¤ºä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶åœ¨åŒç­‰è§„æ¨¡æ¨¡å‹ä¸­å–å¾—äº†æœ€ä½³è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œåªæœ‰32.8%çš„é—®é¢˜éœ€è¦å¤æ‚çš„æ¨ç†ï¼Œå¤§å¤šæ•°é—®é¢˜é›†ä¸­åœ¨äº‹å®ç†è§£ä¸Šã€‚åœ¨å¯¹æŠ—æ€§æµ‹è¯•ä¸­ï¼Œç”Ÿç‰©åŒ»å­¦æ¨¡å‹åœ¨ç»™å‡ºé”™è¯¯åˆå§‹æ¨ç†åè¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œè€Œç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„é€šç”¨é¢†åŸŸæ¨¡å‹åˆ™è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚BioMed-R1æ¨¡å‹åœ¨æ¨ç†å‹å’Œå¯¹æŠ—æ€§æµ‹è¯•ä¸­å‡å–å¾—äº†è¾ƒå¼ºçš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•ä¸ºåŒ»å­¦LLMçš„è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¯¹æŠ—æ€§è®­ç»ƒæå‡æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆé€”å¾„ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥é€šè¿‡æ•´åˆæ›´å¤šæ¨ç†ä¸°å¯Œçš„æ•°æ®æºï¼Œå¦‚ä¸´åºŠç—…ä¾‹æŠ¥å‘Šï¼Œä»¥åŠè®­ç»ƒæ¨¡å‹è¿›è¡Œå¯¹æŠ—æ€§æˆ–å›æº¯åœºæ™¯ï¼Œæ¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå¯é æ€§ã€‚

## patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner
### Abstract
Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose PathoCLIP, trained on the same figure-caption corpus used for continued
pretraining. Comprehensive experimental results demonstrate that both PathoCLIP
and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è·¯å¾„å­¦è¯Šæ–­æ–°ç¯‡ç« ï¼šPatho-R1 å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¨ç†ä¸“å®¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç—…ç†å­¦ä½œä¸ºç°ä»£ä¸´åºŠè¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œå…¶AIè¾…åŠ©ç³»ç»Ÿçš„æ„å»ºå´é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„ç—…ç†å­¦ç‰¹å®šVLMsåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†åˆç†æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸»è¦åŸå› æ˜¯å½“å‰ç—…ç†å­¦æ•°æ®é›†ä¸»è¦ç”±ç¼ºä¹æ·±åº¦å’Œç»“æ„åŒ–è¯Šæ–­èŒƒå¼çš„å›¾åƒæè¿°å¯¹ç»„æˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ„å»ºé«˜è´¨é‡ã€é¢å‘æ¨ç†çš„æ•°æ®é›†ï¼Œå¹¶å¼€å‘ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¨ç†ä¸“å®¶Patho-R1ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®ç­–å±•æµç¨‹ï¼Œè¯¥æµç¨‹åœ¨æœ€å°åŒ–äººåŠ›æŠ•å…¥çš„åŒæ—¶ï¼Œç¡®ä¿äº†é«˜è´¨é‡ç»“æ„åŒ–æ¨ç†æ•°æ®ï¼ˆSFTæ•°æ®ï¼‰çš„å¯æ‰©å±•ç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡ä»‹ç»äº†PathoCLIPï¼Œä¸€ä¸ªå¼€æºçš„ç—…ç†å­¦é€‚åº”ç‰ˆCLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æœ¬æ–‡æ¢ç´¢äº†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¢†åŸŸé€‚åº”ä¸­çš„ç«¯åˆ°ç«¯è®­ç»ƒè¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼šç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŠ¨æ€å‰ªè¾‘ä¸é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
PathoCLIPå’ŒPatho-R1åœ¨å¹¿æ³›çš„ç—…ç†å­¦ç›¸å…³ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå¤šé¡¹é€‰æ‹©é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºç—…ç†å­¦AIç³»ç»Ÿçš„å¼€å‘æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯ä»¥ä¸‹æ–¹é¢å€¼å¾—å€Ÿé‰´ï¼š
- åˆ©ç”¨ç—…ç†å­¦æ•™ç§‘ä¹¦å’ŒçœŸå®ä¸–ç•Œç—…ç†å­¦ä¸“å®¶çš„çŸ¥è¯†æ„å»ºé«˜è´¨é‡æ¨ç†æ•°æ®é›†çš„æ–¹æ³•ã€‚
- é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆæŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ï¼‰æ¥è®­ç»ƒå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚
- åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨GRPOå’ŒDAPOç­–ç•¥æ¥ä¼˜åŒ–å¤šæ¨¡æ€æ¨ç†è´¨é‡ã€‚
- å¼€æºæ¨¡å‹çš„å‘å¸ƒï¼Œä¸ºåç»­ç ”ç©¶å’Œåº”ç”¨æä¾›äº†åŸºç¡€ã€‚

## reason-like-a-radiologist--chain-of-thought-and-reinforcement-learning-for-verifiable-report-generation
### Abstract
Radiology report generation is critical for efficiency but current models
lack the structured reasoning of experts, hindering clinical trust and
explainability by failing to link visual findings to precise anatomical
locations. This paper introduces BoxMed-RL, a groundbreaking unified training
framework for generating spatially verifiable and explainable radiology
reports. Built on a large vision-language model, BoxMed-RL revolutionizes
report generation through two integrated phases: (1) In the Pretraining Phase,
we refine the model via medical concept learning, using Chain-of-Thought
supervision to internalize the radiologist-like workflow, followed by spatially
verifiable reinforcement, which applies reinforcement learning to align medical
findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze
the pretrained weights and train a downstream adapter to ensure fluent and
clinically credible reports. This framework precisely mimics radiologists'
workflow, compelling the model to connect high-level medical concepts with
definitive anatomical evidence. Extensive experiments on public datasets
demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR
and ROUGE-L metrics compared to state-of-the-art methods. An average 5%
improvement in large language model-based metrics further underscores
BoxMed-RL's robustness in generating high-quality radiology reports.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œBoxMed-RLï¼šåƒæ”¾å°„ç§‘åŒ»ç”Ÿä¸€æ ·æ¨ç†â€”â€”ç”¨äºå¯éªŒè¯æŠ¥å‘Šç”Ÿæˆçš„é“¾å¼æ€ç»´ä¸å¼ºåŒ–å­¦ä¹ â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€åŒ»å­¦å½±åƒåœ¨ä¸´åºŠå†³ç­–ä¸­çš„é‡è¦ä½œç”¨æ—¥ç›Šå‡¸æ˜¾ï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„éœ€æ±‚ä¹Ÿéšä¹‹å¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æŠ¥å‘Šç”Ÿæˆæ¨¡å‹å¾€å¾€ç¼ºä¹ç»“æ„åŒ–çš„æ¨ç†èƒ½åŠ›ï¼Œä¸èƒ½æœ‰æ•ˆåœ°å°†è§†è§‰å‘ç°ä¸ç²¾ç¡®çš„è§£å‰–ä½ç½®è”ç³»èµ·æ¥ï¼Œå¯¼è‡´ä¸´åºŠä¿¡ä»»åº¦å’Œè§£é‡Šæ€§ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBoxMed-RLçš„è®­ç»ƒæ¡†æ¶ï¼Œä»¥ç”Ÿæˆç©ºé—´ä¸Šå¯éªŒè¯ä¸”è§£é‡Šæ€§å¼ºçš„æ”¾å°„å­¦æŠ¥å‘Šã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
BoxMed-RLæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå’Œä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡åŒ»å­¦æ¦‚å¿µå­¦ä¹ å’Œé“¾å¼æ€ç»´ç›‘ç£æ¥å†…åŒ–æ”¾å°„ç§‘åŒ»ç”Ÿçš„æ¨ç†æµç¨‹ï¼Œå¹¶é€šè¿‡ç©ºé—´å¯éªŒè¯çš„å¼ºåŒ–å­¦ä¹ å°†åŒ»å­¦å‘ç°ä¸è¾¹ç•Œæ¡†å¯¹é½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åœ¨ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µï¼Œé¢„è®­ç»ƒçš„æƒé‡è¢«å†»ç»“ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªä¸‹æ¸¸é€‚é…å™¨ä»¥ç¡®ä¿ç”Ÿæˆçš„æŠ¥å‘Šæµç•…ä¸”å…·æœ‰ä¸´åºŠå¯ä¿¡åº¦ã€‚è¿™ç§æ¡†æ¶ç²¾ç¡®åœ°æ¨¡ä»¿äº†æ”¾å°„ç§‘åŒ»ç”Ÿçš„æµç¨‹ï¼Œè¿«ä½¿æ¨¡å‹å°†é«˜çº§åŒ»å­¦æ¦‚å¿µä¸ç¡®åˆ‡çš„è§£å‰–è¯æ®è”ç³»èµ·æ¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBoxMed-RLåœ¨METEORå’ŒROUGE-LæŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†7%ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†5%ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†BoxMed-RLåœ¨ç”Ÿæˆé«˜è´¨é‡æ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„ç¨³å¥æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
BoxMed-RLæ–¹æ³•é€šè¿‡ç»“åˆé“¾å¼æ€ç»´å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä¸ºåŒ»å­¦å½±åƒæŠ¥å‘Šçš„è‡ªåŠ¨åŒ–ç”Ÿæˆæä¾›äº†æ–°çš„è§†è§’ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºä¸ä»…æ¨¡ä»¿äº†æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹ï¼Œè¿˜é€šè¿‡ç©ºé—´å¯¹é½ç¡®ä¿äº†æŠ¥å‘Šçš„è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•å¯¹äºæé«˜åŒ»å­¦æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿçš„ä¸´åºŠé€‚ç”¨æ€§å’Œä¿¡ä»»åº¦å…·æœ‰é‡è¦ä»·å€¼ã€‚

## gmai-vl-r1--harnessing-reinforcement-learning-for-multimodal-medical-reasoning
### Abstract
Recent advances in general medical AI have made significant strides, but
existing models often lack the reasoning capabilities needed for complex
medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical
reasoning model enhanced by reinforcement learning (RL) to improve its
reasoning abilities. Through iterative training, GMAI-VL-R1 optimizes
decision-making, significantly boosting diagnostic accuracy and clinical
support. We also develop a reasoning data synthesis method, generating
step-by-step reasoning data via rejection sampling, which further enhances the
model's generalization. Experimental results show that after RL training,
GMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question
answering. While the model demonstrates basic memorization with supervised
fine-tuning, RL is crucial for true generalization. Our work establishes new
evaluation benchmarks and paves the way for future advancements in medical
reasoning models. Code, data, and model will be released at
\href{https://github.com/uni-medical/GMAI-VL-R1}{this link}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "GMAI-VL-R1ï¼šå¼ºåŒ–å­¦ä¹ åŠ©åŠ›åŒ»ç–—å¤šæ¨¡æ€æ¨ç†æ–°ç¯‡ç« "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€åŒ»ç–—AIçš„å¿«é€Ÿå‘å±•ï¼Œå°½ç®¡å¤šæ¨¡æ€åŒ»ç–—æ•°æ®æ¨¡å‹åœ¨æé«˜åŒ»ç–—è´¨é‡å’Œæ•ˆç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†ç°æœ‰çš„æ¨¡å‹åœ¨å¤„ç†å¤æ‚åŒ»ç–—å†³ç­–æ—¶å¾€å¾€ç¼ºä¹å¿…è¦çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGMAI-VL-R1çš„å¤šæ¨¡æ€åŒ»ç–—æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œæå‡æ¨¡å‹åœ¨åŒ»ç–—å†³ç­–ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
GMAI-VL-R1æ¨¡å‹ç›´æ¥åœ¨åŸºç¡€æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ è°ƒä¼˜ï¼ˆRLTï¼‰ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€šè¿‡åå¤è®­ç»ƒå’Œè‡ªæˆ‘æ ¡æ­£ï¼Œè·å¾—å®é™…çš„æ¨ç†ç»éªŒï¼Œä»è€Œè¶…è¶Šç®€å•çš„æ¨¡å¼è¯†åˆ«ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†GMAI-Reasoning10Kï¼ŒåŒ…å«äº†æ¥è‡ª12ç§æˆåƒæ¨¡æ€çš„è¯¦ç»†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ³¨é‡Šã€‚è¿™ä¸€æ•°æ®é›†çš„æ„å»ºç¡®ä¿äº†åœ¨å…¬å¹³æ¯”è¾ƒä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨çœŸå®æ¨ç†èƒ½åŠ›ä¸Šçš„å·®å¼‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
GMAI-VL-R1æ¨¡å‹åœ¨å¤šä¸ªå¤§è§„æ¨¡åŒ»ç–—å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ä»…åœ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œè€Œä¸”åœ¨ç–¾ç—…è¯Šæ–­å’Œè¯†åˆ«ä»»åŠ¡ä¸Šä¹Ÿå±•ç°äº†å…¶ä¼˜è¶Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ è°ƒä¼˜ç­–ç•¥åœ¨å¤šç§åŒ»ç–—å¤šæ¨¡æ€ä»»åŠ¡ä¸­å‡ä¼˜äºåŸºçº¿æ¨¡å‹å’Œç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºåŒ»ç–—AIé¢†åŸŸæä¾›äº†æ–°çš„è§†è§’ï¼Œè¡¨æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡åŒ»ç–—å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚åŒæ—¶ï¼Œæ„å»ºçš„é«˜è´¨é‡æ•°æ®é›†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å¯é çš„åŸºç¡€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å‡å°‘è®­ç»ƒæ•°æ®éœ€æ±‚çš„åŒæ—¶ï¼Œå®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™å¯¹äºå®é™…ä¸´åºŠåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## advanced-deep-learning-and-large-language-models--comprehensive-insights-for-cancer-detection
### Abstract
The rapid advancement of deep learning (DL) has transformed healthcare,
particularly in cancer detection and diagnosis. DL surpasses traditional
machine learning and human accuracy, making it a critical tool for identifying
diseases. Despite numerous reviews on DL in healthcare, a comprehensive
analysis of its role in cancer detection remains limited. Existing studies
focus on specific aspects, leaving gaps in understanding its broader impact.
This paper addresses these gaps by reviewing advanced DL techniques, including
transfer learning (TL), reinforcement learning (RL), federated learning (FL),
Transformers, and large language models (LLMs). These approaches enhance
accuracy, tackle data scarcity, and enable decentralized learning while
maintaining data privacy. TL adapts pre-trained models to new datasets,
improving performance with limited labeled data. RL optimizes diagnostic
pathways and treatment strategies, while FL fosters collaborative model
development without sharing sensitive data. Transformers and LLMs,
traditionally used in natural language processing, are now applied to medical
data for improved interpretability. Additionally, this review examines these
techniques' efficiency in cancer diagnosis, addresses challenges like data
imbalance, and proposes solutions. It serves as a resource for researchers and
practitioners, providing insights into current trends and guiding future
research in advanced DL for cancer detection.
#### è®ºæ–‡æ ‡é¢˜è§£è¯»ï¼šã€ŠAdvanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detectionã€‹

###### è®ºæ–‡æ‘˜è¦æ€»ç»“

æœ¬æ–‡æ·±å…¥æ¢è®¨äº†æ·±åº¦å­¦ä¹ ï¼ˆDeep Learning, DLï¼‰åœ¨ç™Œç—‡æ£€æµ‹é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜è¯Šæ–­å‡†ç¡®åº¦ã€è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜å’Œç»´æŠ¤æ•°æ®éšç§æ–¹é¢çš„å…ˆè¿›æŠ€æœ¯ã€‚è®ºæ–‡å›é¡¾äº†è½¬ç§»å­¦ä¹ ï¼ˆTransfer Learning, TLï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰ã€è”é‚¦å­¦ä¹ ï¼ˆFederated Learning, FLï¼‰ã€Transformeræ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰ç­‰å…ˆè¿›DLæŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯æ­£åœ¨æ¨åŠ¨ç™Œç—‡æ£€æµ‹çš„è¾¹ç•Œã€‚è½¬ç§»å­¦ä¹ é€šè¿‡è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹æ¥æé«˜æœ‰é™æ ‡è®°æ•°æ®çš„æ€§èƒ½ï¼›å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–äº†è¯Šæ–­è·¯å¾„å’Œæ²»ç–—ç­–ç•¥ï¼›è”é‚¦å­¦ä¹ ä¿ƒè¿›äº†æ¨¡å‹çš„æ— å…±äº«æ•æ„Ÿæ•°æ®åˆä½œå¼€å‘ã€‚Transformerå’ŒLLMsï¼Œé€šå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œç°åœ¨è¢«åº”ç”¨äºåŒ»å­¦æ•°æ®ä»¥æé«˜è§£é‡Šæ€§å’Œä¸Šä¸‹æ–‡é¢„æµ‹ã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†è¿™äº›æŠ€æœ¯åœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„æ•ˆç‡ï¼Œè®¨è®ºäº†æ•°æ®ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚

###### è®ºæ–‡å…¨æ–‡è¯¦ç»†è§£è¯»

1. **å¼•è¨€**ï¼š
   - ç™Œç—‡æ˜¯å…¨çƒæ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸæ£€æµ‹å¯¹æŒ½æ•‘ç”Ÿå‘½è‡³å…³é‡è¦ã€‚
   - ä¼ ç»Ÿçš„ç™Œç—‡è¯Šæ–­æ–¹æ³•å­˜åœ¨è¯¯å·®å’Œæ•ˆç‡é—®é¢˜ï¼Œè€Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å› å…¶åˆ†å±‚ç»“æ„åœ¨åŒ»ç–—é¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚
   - æœ¬æ–‡æ—¨åœ¨å¡«è¡¥æ·±åº¦å­¦ä¹ åœ¨ç™Œç—‡æ£€æµ‹é¢†åŸŸåº”ç”¨çš„ç»¼è¿°æ€§ç ”ç©¶ç©ºç™½ã€‚

2. **æ·±åº¦å­¦ä¹ åœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„åº”ç”¨**ï¼š
   - æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ï¼Œåœ¨æå–åŒ»å­¦å›¾åƒç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
   - DLç®—æ³•èƒ½å¤Ÿç›´æ¥ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ ç‰¹å¾ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼Œè¿™å¯¹äºå›¾åƒåˆ†æéå¸¸å®è´µã€‚

3. **å…ˆè¿›æ·±åº¦å­¦ä¹ æŠ€æœ¯**ï¼š
   - **è½¬ç§»å­¦ä¹ ï¼ˆTLï¼‰**ï¼šé€šè¿‡é€‚åº”é¢„è®­ç»ƒæ¨¡å‹æ¥æé«˜æ–°æ•°æ®é›†çš„æ€§èƒ½ï¼Œå°¤å…¶é€‚ç”¨äºæ ‡è®°æ•°æ®æœ‰é™çš„æƒ…å†µã€‚
   - **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼šä¼˜åŒ–è¯Šæ–­å’Œæ²»ç–—ç­–ç•¥ï¼Œé€šè¿‡å­¦ä¹ æœ€ä½³å†³ç­–è·¯å¾„ã€‚
   - **è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰**ï¼šå…è®¸å¤šä¸ªæœºæ„åˆä½œå¼€å‘æ¨¡å‹ï¼Œè€Œæ— éœ€å…±äº«æ•æ„Ÿæ•°æ®ï¼Œæœ‰åŠ©äºä¿æŠ¤éšç§ã€‚
   - **Transformerå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**ï¼šè¿™äº›é€šå¸¸ç”¨äºNLPçš„æŠ€æœ¯ï¼Œç°åœ¨è¢«åº”ç”¨äºåŒ»å­¦æ•°æ®ï¼Œä»¥æé«˜è§£é‡Šæ€§å’Œä¸Šä¸‹æ–‡é¢„æµ‹èƒ½åŠ›ã€‚

4. **æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ**ï¼š
   - æ•°æ®ä¸å¹³è¡¡å’Œæœ‰é™çš„æ•°æ®é›†æ˜¯ç™Œç—‡ç ”ç©¶ä¸­çš„ä¸»è¦æŒ‘æˆ˜ã€‚
   - æå‡ºäº†æ•°æ®å¢å¼ºå’Œè½¬ç§»å­¦ä¹ ç­‰æ–¹æ³•æ¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚
   - è®¨è®ºäº†å¦‚ä½•é€šè¿‡è”é‚¦å­¦ä¹ ç­‰æŠ€æœ¯ä¿æŠ¤æ•°æ®éšç§ã€‚

5. **ç»“è®º**ï¼š
   - æœ¬æ–‡ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†ä¸€ä¸ªå®è´µçš„èµ„æºï¼Œæä¾›äº†å½“å‰è¶‹åŠ¿çš„è§è§£ï¼Œå¹¶æŒ‡å¯¼æœªæ¥åœ¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ç”¨äºç™Œç—‡æ£€æµ‹çš„ç ”ç©¶ã€‚

è¿™ç¯‡è®ºæ–‡å¯¹äºç†è§£æ·±åº¦å­¦ä¹ åœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„åº”ç”¨å’Œæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†å…¨é¢çš„æ¦‚è¿°ï¼Œå¯¹äºåŒ»ç–—é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## reinforcement-learning-for-active-matter
### Abstract
Active matter refers to systems composed of self-propelled entities that
consume energy to produce motion, exhibiting complex non-equilibrium dynamics
that challenge traditional models. With the rapid advancements in machine
learning, reinforcement learning (RL) has emerged as a promising framework for
addressing the complexities of active matter. This review systematically
introduces the integration of RL for guiding and controlling active matter
systems, focusing on two key aspects: optimal motion strategies for individual
active particles and the regulation of collective dynamics in active swarms. We
discuss the use of RL to optimize the navigation, foraging, and locomotion
strategies for individual active particles. In addition, the application of RL
in regulating collective behaviors is also examined, emphasizing its role in
facilitating the self-organization and goal-directed control of active swarms.
This investigation offers valuable insights into how RL can advance the
understanding, manipulation, and control of active matter, paving the way for
future developments in fields such as biological systems, robotics, and medical
science.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "æ·±åº¦å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¼•é¢†æ´»è·ƒç‰©è´¨ç³»ç»Ÿçš„æ–°ç¯‡ç« "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ´»è·ƒç‰©è´¨ç³»ç»Ÿç”±å…·æœ‰è‡ªæ¨è¿›æœºåˆ¶çš„å®ä½“ç»„æˆï¼Œè¿™äº›å®ä½“é€šè¿‡æ¶ˆè€—èƒ½é‡äº§ç”Ÿè¿åŠ¨ï¼Œå±•ç°å‡ºå¤æ‚çš„éå¹³è¡¡åŠ¨åŠ›å­¦è¡Œä¸ºï¼Œè¿™å¯¹ä¼ ç»Ÿæ¨¡å‹æå‡ºäº†æŒ‘æˆ˜ã€‚å°½ç®¡å·²æœ‰ç†è®ºæ¨¡å‹å°è¯•è§£é‡Šè¿™äº›å¼‚å¸¸è¡Œä¸ºï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨äºç°è±¡çš„è§£é‡Šè€Œéå®é™…çš„æ§åˆ¶å’Œå¼•å¯¼ã€‚éšç€æœºå™¨å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ¡†æ¶ï¼Œè¢«æå‡ºç”¨äºè§£å†³æ´»è·ƒç‰©è´¨ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡ç³»ç»Ÿåœ°ä»‹ç»äº†å¦‚ä½•å°†å¼ºåŒ–å­¦ä¹ æ•´åˆåˆ°æ´»è·ƒç‰©è´¨ç³»ç»Ÿçš„æŒ‡å¯¼å’Œæ§åˆ¶ä¸­ï¼Œä¸»è¦å…³æ³¨ä¸¤ä¸ªæ–¹é¢ï¼šå•ä¸ªæ´»è·ƒç²’å­çš„æœ€ä¼˜è¿åŠ¨ç­–ç•¥å’Œæ´»è·ƒç¾¤ä½“é›†ä½“åŠ¨æ€çš„è°ƒèŠ‚ã€‚å¯¹äºå•ä¸ªç²’å­ï¼ŒRLè¢«ç”¨æ¥ä¼˜åŒ–å¯¼èˆªã€è§…é£Ÿå’Œè¿åŠ¨ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†RLåœ¨è°ƒèŠ‚é›†ä½“è¡Œä¸ºä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒå…¶åœ¨ä¿ƒè¿›æ´»è·ƒç¾¤ä½“çš„è‡ªæˆ‘ç»„ç»‡å’Œç›®æ ‡å¯¼å‘æ§åˆ¶ä¸­çš„ä½œç”¨ã€‚è¿™ç§æ•´åˆRLçš„æ–¹æ³•ä¸ä»…æ·±åŒ–äº†æˆ‘ä»¬å¯¹æ´»è·ƒç‰©è´¨ç³»ç»Ÿçš„ç†è§£ï¼Œä¹Ÿä¸ºå®é™…åº”ç”¨ä¸­å¤æ‚ç³»ç»Ÿçš„æ“çºµå’Œä¼˜åŒ–æä¾›äº†é€”å¾„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡è¯¦ç»†è®¨è®ºäº†RLåœ¨ä¸åŒç±»å‹çš„æ´»è·ƒç‰©è´¨ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬äººå·¥æ´»æ€§èƒ¶ä½“ã€å¾®ç”Ÿç‰©ç³»ç»Ÿã€åŠ¨ç‰©ç¾¤ä½“å’Œæœºå™¨äººç¾¤ä½“ã€‚é€šè¿‡å®éªŒç»“æœï¼Œå±•ç¤ºäº†RLå¦‚ä½•å¸®åŠ©è¿™äº›ç³»ç»Ÿåœ¨ä¸ç¡®å®šç¯å¢ƒä¸­å®ç°è‡ªé€‚åº”å†³ç­–å’Œä¼˜åŒ–ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå®è´µçš„è§†è§’ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ç†è§£å’Œæ§åˆ¶æ´»è·ƒç‰©è´¨ç³»ç»Ÿã€‚å…¶ç ”ç©¶æˆæœå¯¹äºç”Ÿç‰©å­¦ç³»ç»Ÿã€æœºå™¨äººæŠ€æœ¯å’ŒåŒ»å­¦ç§‘å­¦ç­‰é¢†åŸŸçš„å‘å±•å…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç¡®æ“çºµå’Œä¼˜åŒ–å¤æ‚ç³»ç»Ÿçš„åº”ç”¨ä¸­ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢RLåœ¨æ´»è·ƒç‰©è´¨ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œä»¥æ¨åŠ¨è¿™ä¸€æ–°å…´é¢†åŸŸçš„è¿›æ­¥ã€‚

## rl4med-ddpo--reinforcement-learning-for-controlled-guidance-towards-diverse-medical-image-generation-using-vision-language-foundation-models
### Abstract
Vision-Language Foundation Models (VLFM) have shown a tremendous increase in
performance in terms of generating high-resolution, photorealistic natural
images. While VLFMs show a rich understanding of semantic content across
modalities, they often struggle with fine-grained alignment tasks that require
precise correspondence between image regions and textual descriptions a
limitation in medical imaging, where accurate localization and detection of
clinical features are essential for diagnosis and analysis. To address this
issue, we propose a multi-stage architecture where a pre-trained VLFM provides
a cursory semantic understanding, while a reinforcement learning (RL) algorithm
refines the alignment through an iterative process that optimizes for
understanding semantic context. The reward signal is designed to align the
semantic information of the text with synthesized images. We demonstrate the
effectiveness of our method on a medical imaging skin dataset where the
generated images exhibit improved generation quality and alignment with prompt
over the fine-tuned Stable Diffusion. We also show that the synthesized samples
could be used to improve disease classifier performance for underrepresented
subgroups through augmentation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–åŒ»å­¦å›¾åƒç”Ÿæˆçš„RL4Med-DDPOæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLFMï¼‰åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é€¼çœŸçš„è‡ªç„¶å›¾åƒæ–¹é¢çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå…¶åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ä¹Ÿå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†éœ€è¦ç²¾ç¡®å¯¹åº”å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬æè¿°çš„ç»†ç²’åº¦å¯¹é½ä»»åŠ¡æ—¶å­˜åœ¨å±€é™ï¼Œè¿™åœ¨åŒ»å­¦æˆåƒä¸­å°¤å…¶å…³é”®ï¼Œå› ä¸ºå‡†ç¡®çš„ä¸´åºŠç‰¹å¾å®šä½å’Œæ£€æµ‹å¯¹äºè¯Šæ–­å’Œåˆ†æè‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šé˜¶æ®µæ¶æ„ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„VLFMå’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥ä¼˜åŒ–åŒ»å­¦å›¾åƒçš„ç”Ÿæˆè´¨é‡å’Œè¯­ä¹‰å¯¹é½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRL4Med-DDPOçš„å¤šé˜¶æ®µæ¶æ„ï¼Œå…¶ä¸­é¢„è®­ç»ƒçš„VLFMæä¾›åˆæ­¥çš„è¯­ä¹‰ç†è§£ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ç®—æ³•åˆ™é€šè¿‡è¿­ä»£è¿‡ç¨‹ä¼˜åŒ–å¯¹è¯­ä¹‰ä¸Šä¸‹æ–‡çš„ç†è§£ï¼Œä»è€Œç»†åŒ–å¯¹é½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†è¯„ä¼°ç”Ÿæˆçš„å›¾åƒæ˜¯å¦ä¸æ–‡æœ¬æç¤ºè¯­ä¹‰å¯¹é½ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†â€”â€” artifact prevalence rate (APR)ï¼Œç”¨äºè®¡ç®—åˆæˆå›¾åƒä¸­æœŸæœ›å±æ€§çš„å­˜åœ¨æƒ…å†µã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ISIC2019çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ä»…ç»è¿‡å¾®è°ƒçš„Stable Diffusionç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ç”Ÿæˆçš„å›¾åƒåœ¨è´¨é‡å’Œå¯¹æç¤ºçš„è¯­ä¹‰å¯¹é½æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ•°æ®å¢å¼ºï¼Œè¿™äº›åˆæˆæ ·æœ¬è¿˜èƒ½æé«˜ç–¾ç—…åˆ†ç±»å™¨å¯¹ä»£è¡¨æ€§ä¸è¶³çš„äºšç»„çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºåŒ»å­¦å›¾åƒç”Ÿæˆæä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒå›¾åƒè´¨é‡å’Œè¯­ä¹‰å¯¹é½æ–¹é¢ã€‚å¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–æ‰©æ•£è¿‡ç¨‹ä¸­çš„åº”ç”¨ä¸ºå‡å°‘åå·®å’Œæ”¹å–„å›¾åƒä¸æ–‡æœ¬æç¤ºä¹‹é—´çš„å¯¹é½æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚æ­¤å¤–ï¼ŒAPRä½œä¸ºä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œä¸ºè¯„ä¼°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ€è·¯ã€‚

## empowering-medical-multi-agents-with-clinical-consultation-flow-for-dynamic-diagnosis
### Abstract
Traditional AI-based healthcare systems often rely on single-modal data,
limiting diagnostic accuracy due to incomplete information. However, recent
advancements in foundation models show promising potential for enhancing
diagnosis combining multi-modal information. While these models excel in static
tasks, they struggle with dynamic diagnosis, failing to manage multi-turn
interactions and often making premature diagnostic decisions due to
insufficient persistence in information collection.To address this, we propose
a multi-agent framework inspired by consultation flow and reinforcement
learning (RL) to simulate the entire consultation process, integrating multiple
clinical information for effective diagnosis. Our approach incorporates a
hierarchical action set, structured from clinic consultation flow and medical
textbook, to effectively guide the decision-making process. This strategy
improves agent interactions, enabling them to adapt and optimize actions based
on the dynamic state. We evaluated our framework on a public dynamic diagnosis
benchmark. The proposed framework evidentially improves the baseline methods
and achieves state-of-the-art performance compared to existing foundation
model-based methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ™ºèƒ½ä½“åŒ»ç–—å’¨è¯¢æ¡†æ¶åŠ©åŠ›åŠ¨æ€è¯Šæ–­

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„åŸºäºäººå·¥æ™ºèƒ½çš„åŒ»ç–—ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå•ä¸€æ¨¡æ€çš„æ•°æ®ï¼Œå¦‚åŒ»å­¦å½±åƒæˆ–å®éªŒå®¤æ£€æµ‹ç»“æœï¼Œè¿™é™åˆ¶äº†è¯Šæ–­çš„å‡†ç¡®æ€§ï¼Œå› ä¸ºä¿¡æ¯ä¸å®Œæ•´ã€‚å°½ç®¡æœ€è¿‘çš„åŸºç¡€æ¨¡å‹è¿›å±•åœ¨ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯å¢å¼ºè¯Šæ–­æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€è¯Šæ–­ä»»åŠ¡æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæ— æ³•æœ‰æ•ˆç®¡ç†å¤šè½®äº¤äº’ï¼Œå¹¶ä¸”ç”±äºä¿¡æ¯æ”¶é›†ä¸è¶³è€Œå¸¸å¸¸åšå‡ºè¿‡æ—©çš„è¯Šæ–­å†³ç­–ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸´åºŠå’¨è¯¢æµç¨‹å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥æ¨¡æ‹Ÿæ•´ä¸ªå’¨è¯¢è¿‡ç¨‹ï¼Œæ•´åˆå¤šç§ä¸´åºŠä¿¡æ¯è¿›è¡Œæœ‰æ•ˆè¯Šæ–­ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¨¡æ‹Ÿäº†è¯Šæ‰€çš„å’¨è¯¢è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„å†³ç­–è¿‡ç¨‹ã€‚è¿™ç§æ¡†æ¶èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“æ ¹æ®åŠ¨æ€çŠ¶æ€é€‚åº”å¹¶ä¼˜åŒ–è¡ŒåŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚çš„è¡ŒåŠ¨é›†ï¼Œè¯¥è¡ŒåŠ¨é›†åŸºäºä¸´åºŠå’¨è¯¢æµç¨‹å’ŒåŒ»å­¦æ•™ç§‘ä¹¦æ„å»ºï¼Œæœ‰æ•ˆåœ°æŒ‡å¯¼äº†å†³ç­–è¿‡ç¨‹ã€‚è¿™ç§ç­–ç•¥æé«˜äº†æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©æœ€åˆé€‚çš„è¡ŒåŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æå‡ºçš„æ¡†æ¶åœ¨å…¬å¼€çš„åŠ¨æ€è¯Šæ–­åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¸ç°æœ‰çš„åŸºäºåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºåŠ¨æ€è¯Šæ–­æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œå³é€šè¿‡æ¨¡æ‹Ÿä¸´åºŠå’¨è¯¢æµç¨‹å’Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å¤šæ™ºèƒ½ä½“çš„äº¤äº’ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”ä¸ºå¤„ç†å¤æ‚çš„åŒ»ç–—å’¨è¯¢åœºæ™¯æä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„æ¡†æ¶ã€‚æ­¤å¤–ï¼Œåˆ†å±‚è¡ŒåŠ¨é›†çš„è®¾è®¡ä¸ºå†³ç­–è¿‡ç¨‹æä¾›äº†ç»“æ„åŒ–çš„æŒ‡å¯¼ï¼Œè¿™å¯¹äºæé«˜äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ•ˆç‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚

