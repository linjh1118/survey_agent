# Paper List of Terms(Reward Model+Test Time Scaling)
- [25/07] **Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS**  
[[Paper](http://arxiv.org/pdf/2507.05557v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts)]

- [25/07] **Test-Time Scaling with Reflective Generative Model**  
[[Paper](http://arxiv.org/pdf/2507.01951v2)] [[Code/Page](https://github.com/MetaStone-AI/MetaStone-S1.)] [[TLDR/Notes](#test-time-scaling-with-reflective-generative-model)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts**  
[[Paper](http://arxiv.org/pdf/2506.15733v1)] [[Code/Page]()] [[TLDR/Notes](#$\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts)]

- [25/06] **EQA-RM: A Generative Embodied Reward Model with Test-time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.10389v1)] [[Code/Page](https://github.com/UNITES-Lab/EQA-RM.)] [[TLDR/Notes](#eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **Learning to Reason Across Parallel Samples for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.09014v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-across-parallel-samples-for-llm-reasoning)]

- [25/06] **Guided Speculative Inference for Efficient Test-Time Alignment of LLMs**  
[[Paper](http://arxiv.org/pdf/2506.04118v1)] [[Code/Page](https://github.com/j-geuter/GSI)] [[TLDR/Notes](#guided-speculative-inference-for-efficient-test-time-alignment-of-llms)]

- [25/06] **Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.03136v1)] [[Code/Page](https://github.com/Gen-Verse/CURE)] [[TLDR/Notes](#co-evolving-llm-coder-and-unit-tester-via-reinforcement-learning)]

- [25/06] **Incentivizing LLMs to Self-Verify Their Answers**  
[[Paper](http://arxiv.org/pdf/2506.01369v1)] [[Code/Page](https://github.com/mansicer/self-verification.)] [[TLDR/Notes](#incentivizing-llms-to-self-verify-their-answers)]

- [25/05] **DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.20241v2)] [[Code/Page](https://github.com/coder-qicao/DreamPRM.)] [[TLDR/Notes](#dreamprm--domain-reweighted-process-reward-model-for-multimodal-reasoning)]

- [25/06] **From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.00027v1)] [[Code/Page]()] [[TLDR/Notes](#from-mathematical-reasoning-to-code--generalization-of-process-reward-models-in-test-time-scaling)]

- [25/05] **Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence**  
[[Paper](http://arxiv.org/pdf/2505.20325v1)] [[Code/Page]()] [[TLDR/Notes](#guided-by-gut--efficient-test-time-scaling-with-reinforced-intrinsic-confidence)]

- [25/05] **Value-Guided Search for Efficient Chain-of-Thought Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.17373v1)] [[Code/Page]()] [[TLDR/Notes](#value-guided-search-for-efficient-chain-of-thought-reasoning)]

- [25/05] **J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge**  
[[Paper](http://arxiv.org/pdf/2505.11875v1)] [[Code/Page]()] [[TLDR/Notes](#j1--exploring-simple-test-time-scaling-for-llm-as-a-judge)]

- [25/05] **Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards**  
[[Paper](http://arxiv.org/pdf/2505.02686v2)] [[Code/Page](https://github.com/bobxwu/learning-from-rewards-llm-papers.)] [[TLDR/Notes](#sailing-by-the-stars--a-survey-on-reward-models-and-learning-strategies-for-learning-from-rewards)]

- [25/05] **A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law**  
[[Paper](http://arxiv.org/pdf/2505.02665v2)] [[Code/Page]()] [[TLDR/Notes](#a-survey-of-slow-thinking-based-reasoning-llms-using-reinforced-learning-and-inference-time-scaling-law)]

- [25/04] **Process Reward Models That Think**  
[[Paper](http://arxiv.org/pdf/2504.16828v3)] [[Code/Page](https://github.com/mukhal/thinkprm.)] [[TLDR/Notes](#process-reward-models-that-think)]

- [25/04] **Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.15275v2)] [[Code/Page](https://github.com/CJReinforce/PURE.)] [[TLDR/Notes](#stop-summation--min-form-credit-assignment-is-all-process-reward-model-needs-for-reasoning)]

- [25/04] **Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators**  
[[Paper](http://arxiv.org/pdf/2504.15253v2)] [[Code/Page]()] [[TLDR/Notes](#evaluating-judges-as-evaluators--the-jetts-benchmark-of-llm-as-judges-as-test-time-scaling-evaluators)]

- [25/04] **Adaptive Rectification Sampling for Test-Time Compute Scaling**  
[[Paper](http://arxiv.org/pdf/2504.01317v1)] [[Code/Page]()] [[TLDR/Notes](#adaptive-rectification-sampling-for-test-time-compute-scaling)]

- [25/04] **When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.01005v1)] [[Code/Page](https://github.com/nishadsinghi/sc-genrm-scaling.)] [[TLDR/Notes](#when-to-solve--when-to-verify--compute-optimal-problem-solving-and-generative-verification-for-llm-reasoning)]

- [25/04] **GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.00891v2)] [[Code/Page](https://ryanliu112.github.io/GenPRM.)] [[TLDR/Notes](#genprm--scaling-test-time-compute-of-process-reward-models-via-generative-reasoning)]

- [25/03] **Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute**  
[[Paper](http://arxiv.org/pdf/2503.23803v2)] [[Code/Page](https://github.com/yingweima2022/SWE-Reasoner)] [[TLDR/Notes](#thinking-longer--not-larger--enhancing-software-engineering-agents-via-scaling-test-time-compute)]

- [25/03] **MetaScale: Test-Time Scaling with Evolving Meta-Thoughts**  
[[Paper](http://arxiv.org/pdf/2503.13447v1)] [[Code/Page]()] [[TLDR/Notes](#metascale--test-time-scaling-with-evolving-meta-thoughts)]

- [25/03] **Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding**  
[[Paper](http://arxiv.org/pdf/2503.01422v1)] [[Code/Page]()] [[TLDR/Notes](#sampling-efficient-test-time-scaling--self-estimating-the-best-of-n-sampling-in-early-decoding)]

- [25/02] **AgentRM: Enhancing Agent Generalization with Reward Modeling**  
[[Paper](http://arxiv.org/pdf/2502.18407v1)] [[Code/Page]()] [[TLDR/Notes](#agentrm--enhancing-agent-generalization-with-reward-modeling)]

- [25/02] **Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2502.17407v1)] [[Code/Page]()] [[TLDR/Notes](#linguistic-generalizability-of-test-time-scaling-in-mathematical-reasoning)]

- [25/02] **Process Reward Models for LLM Agents: Practical Framework and Directions**  
[[Paper](http://arxiv.org/pdf/2502.10325v1)] [[Code/Page](https://github.com/sanjibanc/agent_prm.)] [[TLDR/Notes](#process-reward-models-for-llm-agents--practical-framework-and-directions)]

- [25/02] **Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2502.06703v1)] [[Code/Page]()] [[TLDR/Notes](#can-1b-llm-surpass-405b-llm--rethinking-compute-optimal-test-time-scaling)]

- [25/02] **Teaching Language Models to Critique via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2502.03492v1)] [[Code/Page]()] [[TLDR/Notes](#teaching-language-models-to-critique-via-reinforcement-learning)]

- [25/02] **STAIR: Improving Safety Alignment with Introspective Reasoning**  
[[Paper](http://arxiv.org/pdf/2502.02384v2)] [[Code/Page](https://github.com/thu-ml/STAIR.)] [[TLDR/Notes](#stair--improving-safety-alignment-with-introspective-reasoning)]

- [25/01] **SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2501.19306v3)] [[Code/Page]()] [[TLDR/Notes](#sets--leveraging-self-verification-and-self-correction-for-improved-test-time-scaling)]

- [25/01] **PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament**  
[[Paper](http://arxiv.org/pdf/2501.13007v2)] [[Code/Page]()] [[TLDR/Notes](#pairjudge-rm--perform-best-of-n-sampling-with-knockout-tournament)]

- [25/01] **InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model**  
[[Paper](http://arxiv.org/pdf/2501.12368v2)] [[Code/Page](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward)] [[TLDR/Notes](#internlm-xcomposer2-5-reward--a-simple-yet-effective-multi-modal-reward-model)]

- [25/01] **MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow Thinking**  
[[Paper](http://arxiv.org/pdf/2501.12051v2)] [[Code/Page](https://github.com/pixas/MedSSS.)] [[TLDR/Notes](#meds$^3$--towards-medical-small-language-models-with-self-evolved-slow-thinking)]

- [25/01] **ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding**  
[[Paper](http://arxiv.org/pdf/2501.07861v1)] [[Code/Page]()] [[TLDR/Notes](#rearter--retrieval-augmented-reasoning-with-trustworthy-process-rewarding)]

- [25/01] **URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics**  
[[Paper](http://arxiv.org/pdf/2501.04686v5)] [[Code/Page](https://github.com/URSA-MATH.)] [[TLDR/Notes](#ursa--understanding-and-verifying-chain-of-thought-reasoning-in-multimodal-mathematics)]

- [24/11] **Enhancing LLM Reasoning with Reward-guided Tree Search**  
[[Paper](http://arxiv.org/pdf/2411.11694v4)] [[Code/Page]()] [[TLDR/Notes](#enhancing-llm-reasoning-with-reward-guided-tree-search)]



# TLDR/Notes
## enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts
### Abstract
Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ†å±‚æ£€ç´¢å¢å¼ºMCTSï¼Œè§£é”å¤§æ¨¡å‹æ¨ç†æ—¶ç¼©æ”¾æ–°å§¿åŠ¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æå‡ä¼ ç»Ÿä¸Šä¾èµ–è®­ç»ƒæ—¶å¤§è§„æ¨¡è®¡ç®—ï¼Œè€Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - Time Scalingï¼ŒTTSï¼‰ä½œä¸ºäº’è¡¥èŒƒå¼ï¼Œé€šè¿‡æ¨ç†æ—¶åˆ†é…é¢å¤–è®¡ç®—èµ„æºå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰åŸºäºæœç´¢çš„TTSæ–¹æ³•ï¼ˆå¦‚MCTSç»“åˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹PRMï¼‰å­˜åœ¨å±€é™ï¼šä¾èµ–é¢„è®­ç»ƒä¿¡æ¯æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜æˆ–æ¢ç´¢ç›²åŒºï¼Œä¸”ä»…é PRMè¯„ä¼°æ­¥éª¤éš¾ä»¥æ•æ‰å…¨å±€ç­–ç•¥å’Œè¯­ä¹‰å…³ç³»ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ç¨€ç–æˆ–æ¬¡ä¼˜ï¼Œå½±å“å¤æ‚æ¨ç†ä»»åŠ¡æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚å› æ­¤éœ€è¦æ›´æœ‰æ•ˆé€šç”¨çš„æ¨ç†ç¼©æ”¾æ–¹æ³•ï¼Œåœ¨æ— éœ€å¤§é‡é¢å¤–è®­ç»ƒä¸‹å¢å¼ºæ¨ç†èƒ½åŠ›å¹¶æå‡é²æ£’æ€§ä¸é€‚åº”æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒå±‚æ¬¡æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆç²—ç²’åº¦å±‚é¢ï¼‰
æå‡ºæ·±åº¦é€»è¾‘æ£€ç´¢ï¼ˆDeep Logical Retrievalï¼‰ï¼Œä»å¤æ‚æ¨ç†é—®é¢˜ä¸­æå–æŠ½è±¡æ¨¡æ¿ï¼Œæ£€ç´¢ç›¸ä¼¼é—®é¢˜ - ç­”æ¡ˆå¯¹ã€‚è¿™äº›ç›¸ä¼¼å¯¹ä¸ºæ¨¡å‹æä¾›å¤šæ ·ç¤ºä¾‹ï¼ŒåŠ©åŠ›æ¨¡å‹æ•æ‰é—®é¢˜ç»“æ„çš„æ½œåœ¨æ¨¡å¼ä¸å˜å¼‚æ€§ï¼Œè¿›è€Œæå‡ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœï¼Œå¢å¼ºå¯¹æœªè§è¿‡é—®é¢˜çš„é€‚åº”æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†å±‚å¢å¼ºæ¨ç†MCTSï¼ˆç»†ç²’åº¦å±‚é¢ï¼‰
åœ¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿‡ç¨‹ä¸­ï¼ŒR2 - LLMsä»å¤–éƒ¨æ•°å­¦é—®é¢˜æ•°æ®é›†åŠ¨æ€æ£€ç´¢ç›¸å…³ä¸­é—´è§£å†³æ­¥éª¤ï¼Œç”¨ç›¸ä¼¼å…ˆéªŒçŸ¥è¯†ä¸°å¯Œæ¨ç†è¿‡ç¨‹ã€‚ç»“åˆè¿™äº›æ£€ç´¢æ­¥éª¤åï¼ŒPRMèƒ½åšå‡ºæ›´å…·ä¿¡æ¯æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§çš„è¯„ä¼°ï¼Œé™ä½æ— æ•ˆæ¢ç´¢é£é™©ï¼ŒåŒæ—¶è¯¥æ–¹æ³•æ— ç¼æ•´åˆä¸Šä¸‹æ–‡çº§æ¨ç†å¢å¼ºä¸æ­¥éª¤çº§æ ‘æœç´¢æ–¹æ³•ï¼Œåˆ©ç”¨PRMä¼˜åŒ–å€™é€‰ç”Ÿæˆä¸å†³ç­–ä»¥æå‡æ¨ç†å‡†ç¡®æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MATH500ã€GSM8Kå’ŒOlympiadBench - TOæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨LLaMA - 3.1 - 8Bæ¨¡å‹æ—¶ï¼Œä¸åŸºçº¿ç›¸æ¯”R2 - LLMså®ç°äº†æ˜¾è‘—ç›¸å¯¹æå‡ï¼Œæå‡å¹…åº¦æœ€é«˜è¾¾16%ï¼›åœ¨LLaMA 3.1 - 8Bå’ŒQwen 2 - 7Bç­‰ç­–ç•¥æ¨¡å‹ä¸Šè¯„ä¼°ï¼Œä¹Ÿä¼˜äºåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’ŒåŸºäºæ ‘æœç´¢çš„åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†å±‚æ£€ç´¢å¢å¼ºæ€è·¯ï¼šå°†æ£€ç´¢åœ¨æ¨ç†æ—¶çš„ä½œç”¨åˆ†å±‚è®¾è®¡ï¼Œç²—ç²’åº¦æŠ“é—®é¢˜ç»“æ„æ¨¡å¼ã€ç»†ç²’åº¦è¡¥ä¸­é—´æ­¥éª¤çŸ¥è¯†ï¼Œä¸ºå¤šç²’åº¦åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†è¾…åŠ©æ¨ç†æä¾›äº†å‚è€ƒèŒƒå¼ã€‚
2. æ£€ç´¢ä¸MCTSç»“åˆï¼šæŠŠå¤–éƒ¨æ£€ç´¢å¼•å…¥MCTSè¿‡ç¨‹æ¥è¾…åŠ©PRMè¯„ä¼°ï¼Œä¸ºæ”¹è¿›åŸºäºæœç´¢çš„TTSæ–¹æ³•ä¸­å¥–åŠ±ä¿¡å·ä¸è¶³ã€æ¢ç´¢ä½æ•ˆç­‰é—®é¢˜æä¾›äº†åˆ›æ–°è§£æ³•ï¼Œåç»­å¯å€Ÿé‰´è¿™ç§å¤–éƒ¨çŸ¥è¯†èµ‹èƒ½æœç´¢è¿‡ç¨‹çš„æ€è·¯æ‹“å±•æ›´å¤šæ¨ç†åœºæ™¯ã€‚
3. æ— CoTè’¸é¦ä¾èµ–ï¼šæ— éœ€ä»æ›´å…ˆè¿›æ¨¡å‹è’¸é¦è·å–æ€ç»´é“¾è®­ç»ƒæ•°æ®ï¼Œé™ä½äº†æ–¹æ³•åº”ç”¨é—¨æ§›ï¼Œåœ¨èµ„æºæœ‰é™æˆ–éš¾è·å–é«˜çº§æ¨¡å‹è’¸é¦æ•°æ®æ—¶ï¼Œè¯¥è½»é‡ï¼ˆç›¸å¯¹ï¼‰å¢å¼ºæ¨ç†çš„æ–¹å¼å€¼å¾—å‚è€ƒã€‚

## test-time-scaling-with-reflective-generative-model
### Abstract
We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3-mini's performance via the new Reflective Generative Form.
The new form focuses on high-quality reasoning trajectory selection and
contains two novelties: 1) A unified interface for policy and process reward
model: we share the backbone network and use task-specific heads for reasoning
trajectory predicting and scoring respectively, introducing only 53M extra
parameters for trajectory scoring. 2) Eliminating the reliance on process-level
annotation: we provide a self-supervised process reward model, which can
directly learn the high-quality reasoning trajectory selection from the outcome
reward. Equipped with the reflective generative form, MetaStone-S1 is naturally
suitable for test-time scaling, and we provide three reasoning effort modes
(low, medium, and high) based on the controllable thinking length. Experiments
demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI
o3-mini's series with only 32B parameter size. To support the research
community, we have open-sourced MetaStone-S1 at
https://github.com/MetaStone-AI/MetaStone-S1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å…¨æ–°åå°„ç”Ÿæˆæ¨¡å‹MetaStone - S1ï¼šå°å‚æ•°å®ç°å¤§æ€§èƒ½ï¼Œæ¢ç´¢æµ‹è¯•æ—¶ç¼©æ”¾æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸå¿«é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ï¼ŒOpenAIçš„o3æ¨¡å‹ç­‰å€ŸåŠ©æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - Time Scalingï¼ŒTTSï¼‰æŠ€æœ¯å®ç°äº†å…ˆè¿›çš„æ¨ç†å’Œç¼–ç èƒ½åŠ›ã€‚TTSåˆ†ä¸ºå†…éƒ¨TTSå’Œå¤–éƒ¨TTSï¼Œå†…éƒ¨TTSå­˜åœ¨è®­ç»ƒé˜¶æ®µç»“æœå¥–åŠ±è¯¯åˆ†ç±»æ­£ç¡®ç­”æ¡ˆï¼ˆå³å‡é˜³æ€§æ¨ç†è¿‡ç¨‹ï¼‰çš„é—®é¢˜ï¼›å¤–éƒ¨TTSè™½èƒ½æ›´æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹å¼ä¸Šä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œæ¯”å¦‚è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è®­ç»ƒæˆæœ¬é«˜ã€ä¾èµ–ç‰¹å®šæ ‡æ³¨ç­‰ã€‚æœ¬æ–‡èšç„¦å¤–éƒ¨TTSï¼Œæ—¨åœ¨æå‡ºæ–°çš„åå°„ç”Ÿæˆå½¢å¼æ¥å®ç°é«˜è´¨é‡æ¨ç†è½¨è¿¹é€‰æ‹©ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå…¨æ–°åå°„ç”Ÿæˆå½¢å¼ï¼ˆReflective Generative Formï¼‰
ç³»ç»Ÿæ¢³ç†ç°æœ‰TTSèŒƒå¼åï¼Œå®šä¹‰äº†ç”¨äºé«˜è´¨é‡æ¨ç†è½¨è¿¹é€‰æ‹©çš„åå°„ç”Ÿæˆå½¢å¼ã€‚è¯¥å½¢å¼è®©å•ä¸ªç½‘ç»œèƒ½åŒæ—¶å®ç°æ¨ç†è½¨è¿¹é¢„æµ‹ä¸é€‰æ‹©ï¼Œä¸”æ— éœ€è¿‡ç¨‹çº§æ ‡æ³¨ã€‚å…·ä½“è€Œè¨€ï¼Œä¸ºç­–ç•¥å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ‰“é€ ç»Ÿä¸€æ¥å£ï¼Œå…±äº«éª¨å¹²ç½‘ç»œï¼Œé’ˆå¯¹æ¨ç†è½¨è¿¹é¢„æµ‹å’Œè¯„åˆ†åˆ†åˆ«ä½¿ç”¨ç‰¹å®šä»»åŠ¡å¤´ï¼Œä»…ä¸ºè½¨è¿¹è¯„åˆ†å¼•å…¥53Mé¢å¤–å‚æ•°ï¼Œåœ¨æ¨¡å‹æ¶æ„å±‚é¢å®ç°é«˜æ•ˆå¤ç”¨ä¸åˆ›æ–°è®¾è®¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªç›‘ç£è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ¶ˆé™¤è¿‡ç¨‹çº§æ ‡æ³¨ä¾èµ–
æä¾›è‡ªç›‘ç£çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿä»ç»“æœå¥–åŠ±ä¸­ç›´æ¥å­¦ä¹ é«˜è´¨é‡æ¨ç†è½¨è¿¹é€‰æ‹©ã€‚ä¸å†ä¾èµ–æ˜‚è´µä¸”éš¾ä»¥è·å–çš„è¿‡ç¨‹çº§æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡è‡ªç›‘ç£æ–¹å¼è®©æ¨¡å‹è‡ªä¸»å­¦ä¹ æ¨ç†è½¨è¿¹çš„ä¼˜åŠ£åˆ¤æ–­ï¼Œé™ä½äº†æ•°æ®æ ‡æ³¨æˆæœ¬ä¸éš¾åº¦ï¼ŒåŒæ—¶æå‡äº†æ¨¡å‹å¯¹æ¨ç†è¿‡ç¨‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¯æŒå¯æ§æ€è€ƒé•¿åº¦çš„å¤šæ¨ç†åŠªåŠ›æ¨¡å¼
åŸºäºåå°„ç”Ÿæˆå½¢å¼ï¼ŒMetaStone - S1è®¾ç½®äº†ä½ã€ä¸­ã€é«˜ä¸‰ç§æ¨ç†åŠªåŠ›æ¨¡å¼ï¼ˆä¾æ®å¯æ§çš„æ€è€ƒé•¿åº¦åŒºåˆ†ï¼‰ï¼Œè®©æ¨¡å‹èƒ½æ ¹æ®ä¸åŒä»»åŠ¡åœºæ™¯å’Œèµ„æºé™åˆ¶çµæ´»è°ƒæ•´æ¨ç†ç­–ç•¥ï¼Œè‡ªç„¶é€‚é…æµ‹è¯•æ—¶ç¼©æ”¾éœ€æ±‚ï¼Œæå‡æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ä¸æ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œä»…32Bå‚æ•°è§„æ¨¡çš„MetaStone - S1åœ¨æ€§èƒ½ä¸Šèƒ½ä¸OpenAI o3 - miniç³»åˆ—ç›¸åª²ç¾ã€‚å…·ä½“æ¥çœ‹ï¼ŒMetaStone - S1 - lowåœ¨æ•°å­¦ï¼ˆAIME24&25ï¼‰ã€ç¼–ç ï¼ˆLiveCodeBenchï¼‰å’Œä¸­æ–‡æ¨ç†ï¼ˆC - Evalï¼‰ä»»åŠ¡ä¸Šåˆ†åˆ«è¶…è¶ŠOpenAI o3 - mini - lowï¼›MetaStone - S1 - mediumå–å¾—ä¸OpenAI o3 - mini - mediumç›¸è¿‘çš„ç»“æœï¼›MetaStone - S1 - highè¿›ä¸€æ­¥æå‡äº†æ™ºèƒ½ä¸Šé™ï¼Œåœ¨ä¸€ç³»åˆ—å¼€æºå’Œé—­æºæ¨¡å‹ä¸­å–å¾—SOTAç»“æœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹æ¶æ„å¤ç”¨ä¸åˆ›æ–°ï¼šåå°„ç”Ÿæˆå½¢å¼ä¸­å…±äº«éª¨å¹²ç½‘ç»œã€è®¾è®¡ç‰¹å®šä»»åŠ¡å¤´çš„æ€è·¯ï¼Œä¸ºåç»­å¤šä»»åŠ¡æ¨¡å‹æˆ–éœ€è¦åŒæ—¶å®ç°ç”Ÿæˆä¸è¯„ä¼°åŠŸèƒ½çš„æ¨¡å‹æ¶æ„è®¾è®¡æä¾›äº†å‚è€ƒï¼Œå¯åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆæ§åˆ¶å‚æ•°è§„æ¨¡ä¸è®­ç»ƒæˆæœ¬ã€‚
2. è‡ªç›‘ç£å­¦ä¹ åœ¨å¥–åŠ±æ¨¡å‹çš„åº”ç”¨ï¼šå…¶è‡ªç›‘ç£è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ‘†è„±è¿‡ç¨‹çº§æ ‡æ³¨ä¾èµ–çš„åšæ³•ï¼Œä¸ºè§£å†³å¥–åŠ±æ¨¡å‹è®­ç»ƒæ•°æ®è·å–éš¾ã€æˆæœ¬é«˜çš„é—®é¢˜æä¾›äº†æ–°æ–¹å‘ï¼Œåç»­åœ¨æ„å»ºå„ç±»å¥–åŠ±æ¨¡å‹ï¼ˆå¦‚ç”¨äºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æ¨¡å‹ç­‰ï¼‰æ—¶å¯å€Ÿé‰´è‡ªç›‘ç£å­¦ä¹ æ€è·¯æ¥å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚
3. å¯æ§æ¨ç†æ¨¡å¼è®¾è®¡ï¼šé’ˆå¯¹ä¸åŒåœºæ™¯è®¾è®¡å¤šæ¨ç†åŠªåŠ›æ¨¡å¼çš„ç­–ç•¥ï¼Œè®©æ¨¡å‹èƒ½çµæ´»é€‚é…æµ‹è¯•æ—¶éœ€æ±‚ï¼Œè¿™å¯¹æ‰“é€ é¢å‘å®é™…å¤æ‚åœºæ™¯ã€èµ„æºå¯å˜çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ç³»ç»Ÿå…·æœ‰å¯å‘æ„ä¹‰ï¼Œå¯æ ¹æ®ç”¨æˆ·è®¾å¤‡æ€§èƒ½ã€ä»»åŠ¡ç´§æ€¥ç¨‹åº¦ç­‰åŠ¨æ€è°ƒæ•´æ¨¡å‹æ¨ç†ç­–ç•¥ã€‚
4. å°å‚æ•°æ¨¡å‹è¿½å¹³å¤§æ¨¡å‹æ€§èƒ½ï¼šè¯æ˜äº†32Bå‚æ•°è§„æ¨¡æ¨¡å‹èƒ½åœ¨æ€§èƒ½ä¸Šæ¯”è‚©åŒç±»çŸ¥åæ¨¡å‹ï¼Œè¯´æ˜æ¨¡å‹æ€§èƒ½æå‡ä¸åªæ˜¯å•çº¯å †å‚æ•°ï¼Œåˆç†çš„æ¶æ„è®¾è®¡ã€è®­ç»ƒç­–ç•¥ç­‰åŒæ ·å…³é”®ï¼Œä¸ºåç»­ä¸­å°è§„æ¨¡æ¨¡å‹çš„ç ”å‘æä¾›äº†ä¿¡å¿ƒä¸æ–¹å‘ï¼Œå¼•å¯¼ç ”ç©¶è€…å…³æ³¨æ¨¡å‹å†…åœ¨æœºåˆ¶ä¼˜åŒ–è€Œéä¸€å‘³è¿½æ±‚å‚æ•°è§„æ¨¡ã€‚

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨çŸ¥è¯†å¢å¼ºæ ‘æœç´¢æ¨ç†åŠ©åŠ›å¤§æ¨¡å‹æ”»å…‹åˆ†å­ç»“æ„è§£æéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åˆ†å­ç»“æ„è§£ææ˜¯åŒ–å­¦å®éªŒåˆ†æé‡Œçš„å…³é”®ä»»åŠ¡ï¼Œè¦ä»æ ¸ç£ã€çº¢å¤–ç­‰å…‰è°±æ•°æ®æ¨å¯¼åˆ†å­ç»“æ„ï¼Œä¸“ä¸šäººå‘˜éƒ½å¾—èŠ±10 - 15åˆ†é’Ÿåˆ†æå•ä¸ªåˆ†å­ï¼Œæ‰€ä»¥ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–è§£æå¾ˆæœ‰å¿…è¦ã€‚ä½†LLMåœ¨è¿™ä»»åŠ¡ä¸Šæœ‰æŒ‘æˆ˜ï¼šä¸€æ˜¯å¯¹åŒ–å­¦åˆ†å­ç»“æ„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œåƒå™»å©è¿™ç±»ç‰¹æ®Šæ‚ç¯ç»“æ„ï¼ŒLLMå¸¸å› ç¼ºä¹å­ç»“æ„çŸ¥è¯†è¯¯åˆ¤ï¼›äºŒæ˜¯æ²¡æ³•å‡†ç¡®è¯„ä¼°å’Œä¿®æ­£æ¨ç†è¿‡ç¨‹ï¼Œæ ‘æœç´¢æ¨ç†éœ€è¦æœ‰æ•ˆè¯„ä¼°åé¦ˆï¼Œå¯LLMç¼ºé¢†åŸŸçŸ¥è¯†ï¼Œåšä¸å¥½ reward model è§’è‰²ã€‚äºæ˜¯è®ºæ–‡è¦è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæå‡LLMåœ¨åˆ†å­ç»“æ„è§£æçš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºK - MSEæ¡†æ¶  
æ„å»ºçŸ¥è¯†å¢å¼ºçš„åˆ†å­ç»“æ„è§£ææ¨ç†æ¡†æ¶K - MSEï¼ŒæŠŠè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºæ’ä»¶å®ç°æµ‹è¯•æ—¶çš„èƒ½åŠ›æ‰©å±•ï¼Œèƒ½é€‚é…ä»»æ„LLMã€‚å€ŸåŠ©MCTSå¹³è¡¡æ–°è§£æ¢ç´¢å’Œå·²æœ‰è§£åˆ©ç”¨ï¼Œè¿˜ç»“åˆSelf - Refineè®©LLMåŠæ—¶ä¼˜åŒ–ä¹‹å‰çš„è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“  
ä¸ºå¼¥è¡¥LLMåŒ–å­¦ç»“æ„ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œæ„å»ºå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“ã€‚å­ç»“æ„æ˜¯åŒ–å­¦ç©ºé—´åŸºç¡€å…ƒç´ ï¼ŒçŸ¥è¯†åº“é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ•´åˆå­ç»“æ„å’Œç»“æ„æè¿°ï¼Œç»™LLMè¡¥å……é¢†åŸŸçŸ¥è¯†ï¼Œæå‡ç‰¹æ®Šç»“æ„æ¨ç†å‡†ç¡®æ€§ï¼Œå‡å°‘ atypical æ¡ˆä¾‹é”™è¯¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸“å±åˆ†å­ - å…‰è°±è¯„åˆ†å™¨  
è®¾è®¡åˆ†å­ - å…‰è°±è¯„åˆ†å™¨å½“ reward modelï¼Œè§£å†³LLMè§£è¯„ä¼°ä¸å‡†é—®é¢˜ã€‚è¯„åˆ†å™¨æœ‰åˆ†å­ç¼–ç å™¨å’Œå…‰è°±ç¼–ç å™¨ï¼Œè¯„ä¼°åˆ†å­ç»“æ„å’Œå…‰è°±æ•°æ®åŒ¹é…åº¦ç»™å¥–åŠ±åˆ†ã€‚å®ƒè¿˜ä½œä¸ºLLMå’ŒçŸ¥è¯†åº“é—´çš„æ£€ç´¢å™¨ï¼Œç”¨è¾“å…¥å…‰è°±æŸ¥æœ€ç›¸å…³å­ç»“æ„ï¼Œå‡å°‘å­ç»“æ„æ£€ç´¢è¯¯å·®ï¼Œå¢å¼ºæ¨ç†ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MolPuzzleåŸºå‡†æµ‹è¯•ä¸Šï¼ŒK - MSEæ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œå¯¹GPT - 4o - miniå’ŒGPT - 4oéƒ½å¸¦æ¥è¶…20%çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¡†æ¶åœ¨å¢å¼ºLLMåˆ†å­ç»“æ„è§£æèƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸçŸ¥è¯†å¢å¼ºæ€è·¯ï¼šé¢å¯¹ä¸“ä¸šé¢†åŸŸä»»åŠ¡ï¼ŒLLMé€šç”¨çŸ¥è¯†ä¸è¶³æ—¶ï¼Œæ„å»ºé¢†åŸŸå­ç»“æ„çŸ¥è¯†åº“è¡¥å……ï¼Œè¿™ç§â€œå¤–éƒ¨çŸ¥è¯† + LLMâ€æ¨¡å¼å¯å¤ç”¨åœ¨å…¶ä»–ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©ã€ææ–™ï¼‰ä»»åŠ¡ã€‚  
2. æ¨ç†è¿‡ç¨‹è¯„ä¼°ä¼˜åŒ–ï¼šè®¾è®¡é¢†åŸŸä¸“å±è¯„åˆ†å™¨åš reward modelï¼Œç»“åˆæ ‘æœç´¢æ¡†æ¶ä¼˜åŒ–æ¨ç†ï¼Œä¸ºéœ€è¦æ·±åº¦æ¨ç†ã€éœ€è¯„ä¼°åé¦ˆçš„å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è¯æ˜ã€ä»£ç è°ƒè¯•ï¼‰æä¾›äº†â€œè¯„åˆ†å™¨ + æ ‘æœç´¢â€çš„æ¨ç†å¢å¼ºèŒƒå¼ã€‚  
3. æ’ä»¶åŒ–æ¡†æ¶è®¾è®¡ï¼šK - MSEä½œä¸ºæ’ä»¶é€‚é…ä»»æ„LLMï¼Œè¿™ç§è§£è€¦å¼è®¾è®¡æ–¹ä¾¿æŠ€æœ¯è½åœ°ï¼Œä¸åŒåœºæ™¯ä¸‹å¯å¿«é€Ÿé›†æˆåˆ°ç°æœ‰LLMå·¥ä½œæµé‡Œï¼Œé™ä½æŠ€æœ¯è¿ç§»æˆæœ¬ã€‚

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonFlux-PRMï¼šé¢å‘å¤§æ¨¡å‹é•¿æ€ç»´é“¾æ¨ç†çš„è½¨è¿¹æ„ŸçŸ¥å‹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ¨ç†åœºæ™¯ï¼ˆå¦‚æ•°å­¦è§£é¢˜ï¼‰ä¸­ï¼ŒProcess Reward Modelsï¼ˆPRMsï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰æ˜¯ç›‘ç£ä¸­é—´æ¨ç†æ­¥éª¤çš„æœ‰åŠ›å·¥å…·ã€‚ä¸è¿‡ç°æœ‰PRMså­˜åœ¨æ˜æ˜¾å±€é™ï¼šå®ƒä»¬ä¸»è¦åŸºäºæ¨¡å‹æœ€ç»ˆè¾“å‡ºè®­ç»ƒï¼Œéš¾ä»¥å¯¹**è½¨è¿¹ - å“åº”ï¼ˆtrajectory - responseï¼‰**è¿™ç±»æ–°å…´è¾“å‡ºå½¢å¼çš„ä¸­é—´æ¨ç†è½¨è¿¹è¿›è¡Œé²æ£’è¯„ä¼°ã€‚åƒDeepseek - R1ç­‰å‰æ²¿æ¨ç†æ¨¡å‹ä¼šç”Ÿæˆâ€œå†—é•¿ã€æ¬ è§„æ•´çš„ä¸­é—´æ€è€ƒè½¨è¿¹ + ç®€æ´æœ€ç»ˆå“åº”â€çš„è½¨è¿¹ - å“åº”å¯¹ï¼Œè¿™ç±»æ•°æ®å¸¸è¢«ç”¨äºå°æ¨¡å‹è’¸é¦ï¼Œä½†ç°æœ‰PRMså› ä¸ä¸­é—´è½¨è¿¹åœ¨ç»“æ„ã€æ ¼å¼ä¸Šä¸åŒ¹é…ï¼Œä¸”è®­ç»ƒæ—¶ç¼ºä¹å¸¦å¥–åŠ±çš„è½¨è¿¹ - å“åº”æ•°æ®ï¼Œåœ¨ç›‘ç£è¿™ç±»æ•°æ®æ—¶æ•ˆæœä¸ä½³ç”šè‡³ä¼šæŸå®³ä¸‹æ¸¸è®­ç»ƒã€‚æ‰€ä»¥ï¼Œå¦‚ä½•è®©PRMsæ—¢èƒ½ç›‘ç£æœ€ç»ˆå“åº”ï¼Œåˆèƒ½æœ‰æ•ˆè¯„ä¼°ä¸­é—´æ€è€ƒè½¨è¿¹ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè½¨è¿¹æ„ŸçŸ¥çš„PRMâ€”â€”ReasonFlux - PRM  
ReasonFlux - PRMä¸“ä¸ºè¯„ä¼°è½¨è¿¹ - å“åº”å‹æ¨ç†ç—•è¿¹è®¾è®¡ï¼Œèåˆäº†**æ­¥éª¤çº§ï¼ˆstep - levelï¼‰**å’Œ**è½¨è¿¹çº§ï¼ˆtrajectory - levelï¼‰**ç›‘ç£ã€‚å®ƒåœ¨æ¶µç›–æ•°å­¦å’Œç§‘å­¦æ¨ç†çš„10ké«˜è´¨é‡è½¨è¿¹ - å“åº”å¯¹ curated æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½ä¸ºæ€è€ƒè½¨è¿¹å†…çš„æ¯ä¸ªæ­¥éª¤æä¾›ç»†ç²’åº¦å¥–åŠ±ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®©æ¨¡å‹ä¸­é—´æ€è€ƒè½¨è¿¹ä¸æœ€ç»ˆå“åº”æ›´å¯¹é½ï¼Œè§£å†³äº†ç°æœ‰PRMså¯¹ä¸­é—´è½¨è¿¹ç›‘ç£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šåœºæ™¯é€‚é…çš„å¥–åŠ±ç›‘ç£  
ReasonFlux - PRMé€‚é…ç¦»çº¿å’Œåœ¨çº¿å¤šç§åœºæ™¯ï¼š  
- ç¦»çº¿åœºæ™¯ï¼šä¸ºè½¨è¿¹ - å“åº”å¯¹æ‰“åˆ†ï¼Œç­›é€‰é«˜è´¨é‡æ•°æ®ï¼ŒåŠ©åŠ›å°æ¨¡å‹ä¸‹æ¸¸æœ‰ç›‘ç£å¾®è°ƒçš„è®­ç»ƒæ•°æ®ç²¾é€‰ï¼›  
- åœ¨çº¿åœºæ™¯ï¼šèå…¥GRPOç­‰ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ç­–ç•¥ä¼˜åŒ–æä¾›ç»†ç²’åº¦è¿‡ç¨‹å¥–åŠ±ï¼›  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest - time scalingï¼‰ï¼šé€šè¿‡å¥–åŠ±å¼•å¯¼çš„Best - of - Nç­–ç•¥ï¼Œè¯„ä¼°å¤šä¸ªç”Ÿæˆå“åº”å¹¶é€‰æœ€ä¼˜ï¼Œæå‡æ¨ç†æ€§èƒ½ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AIMEã€MATH500ã€GPQA - Diamondç­‰æŒ‘æˆ˜æ€§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux - PRMå±•ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼š  
- æ•°æ®é€‰æ‹©æ–¹é¢ï¼šReasonFlux - PRM - 7Bæ¯”å¼ºåŸºçº¿ï¼ˆå¦‚Qwen2.5 - Math - PRM - 72Bï¼‰å’Œäººå·¥ç­–åˆ’åŸºçº¿é€‰å‡ºçš„æ•°æ®é›†è´¨é‡æ›´é«˜ï¼›  
- æ€§èƒ½æå‡æ–¹é¢ï¼šReasonFlux - PRM - 7Båœ¨æœ‰ç›‘ç£å¾®è°ƒä¸­å¹³å‡æå‡12.1%ï¼Œå¼ºåŒ–å­¦ä¹ ä¸­å¹³å‡æå‡4.5%ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ä¸­å¹³å‡æå‡6.3%ï¼›  
- èµ„æºå‹å¥½å‹å‘å¸ƒï¼šè¿˜å‘å¸ƒäº†ReasonFlux - PRM - 1.5Bï¼Œé€‚é…èµ„æºå—é™åœºæ™¯ä¸è¾¹ç¼˜éƒ¨ç½²ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å®šä¹‰ä¸åˆ†æè§’åº¦ï¼šé’ˆå¯¹æ–°å…´çš„è½¨è¿¹ - å“åº”è’¸é¦æ•°æ®è¶‹åŠ¿ï¼Œæ·±å…¥åˆ†æç°æœ‰PRMsåœ¨ç›‘ç£ä¸­é—´è½¨è¿¹æ—¶çš„é—®é¢˜ï¼ˆç»“æ„æ ¼å¼ä¸åŒ¹é…ã€è®­ç»ƒæ•°æ®ç¼ºå¤±ï¼‰ï¼Œè¿™ç§ä»äº§ä¸šæ–°æ•°æ®å½¢æ€åæ¨æŠ€æœ¯ç—›ç‚¹çš„æ€è·¯ï¼Œä¸ºåç»­ç ”ç©¶é”šå®šæ–¹å‘æä¾›å‚è€ƒï¼›  
2. å¤šç²’åº¦ç›‘ç£èåˆï¼šå°†æ­¥éª¤çº§å’Œè½¨è¿¹çº§ç›‘ç£ç»“åˆï¼Œä¸ºå¤„ç†â€œé•¿é“¾æ¡ã€å¤šé˜¶æ®µâ€çš„æ¨ç†ç±»ä»»åŠ¡æä¾›äº†ç»†ç²’åº¦å¥–åŠ±è®¾è®¡çš„èŒƒä¾‹ï¼Œå¯è¿ç§»åˆ°ä»£ç ç”Ÿæˆã€å¤æ‚å†³ç­–ç­‰éœ€åˆ†æ­¥è¯„ä¼°çš„åœºæ™¯ï¼›  
3. å¤šåœºæ™¯å·¥ç¨‹è½åœ°ï¼šä»ç¦»çº¿æ•°æ®ç­›é€‰ã€åœ¨çº¿RLä¼˜åŒ–åˆ°æµ‹è¯•æ—¶å¢å¼ºï¼Œå®Œæ•´è¦†ç›–å¤§æ¨¡å‹è®­ç»ƒ - æ¨ç†å…¨æµç¨‹çš„å¥–åŠ±ç›‘ç£ï¼Œå±•ç¤ºäº†æŠ€æœ¯æ–¹æ¡ˆåœ¨äº§ä¸šçº§è½åœ°ä¸­çš„å¤šç»´åº¦ä»·å€¼ï¼Œä¸ºæ‰“é€ ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹æ¨ç†å¢å¼ºç®¡çº¿æä¾›äº†å®è·µæ¨¡æ¿ï¼›  
4. èµ„æºåˆ†å±‚å‘å¸ƒï¼šåŒæ—¶æä¾›7Bå’Œ1.5Bè§„æ¨¡æ¨¡å‹ï¼Œå…¼é¡¾é«˜æ€§èƒ½ä¸èµ„æºå—é™åœºæ™¯ï¼Œä½“ç°äº†æŠ€æœ¯æ™®æƒ æ€§ï¼Œåœ¨å®é™…ä¸šåŠ¡ä¸­å¯æ ¹æ®ç®—åŠ›ã€å»¶è¿Ÿç­‰éœ€æ±‚çµæ´»é€‰æ‹©ï¼Œå¹³è¡¡æ•ˆæœä¸æˆæœ¬ã€‚  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å‘Šåˆ«ç¹çæ ‡æ³¨ï¼šGAN - RM è®©å¥–åŠ±å»ºæ¨¡â€œä»¥å‡ä¹±çœŸâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒåå¢å¼ºä¸­ï¼Œå¥–åŠ±æ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œå½“å‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•å­˜åœ¨è¯¸å¤šéš¾é¢˜ï¼šä¸€æ˜¯æ„å»ºå¥–åŠ±æ¨¡å‹éœ€å¤§é‡äººå·¥æ ‡æ³¨åå¥½æ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œä¸”åŸºäºç‰¹å®šç”Ÿæˆæ¨¡å‹è¾“å‡ºåŸŸæ ‡æ³¨çš„æ•°æ®ï¼Œåœ¨åº”ç”¨åˆ°ä¸åŒè¾“å‡ºåŸŸæ¨¡å‹æ—¶å­˜åœ¨åŸŸå·®è·ï¼›äºŒæ˜¯ä¸ºå…¨é¢è¯„ä¼°ç”Ÿæˆå†…å®¹è´¨é‡ï¼Œéœ€äººå·¥è®¾è®¡å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¢å¢åŠ å·¥ç¨‹æˆæœ¬ï¼Œåˆéš¾åœ¨ä¸åŒç»´åº¦é—´å–å¾—æœ€ä¼˜å¹³è¡¡ï¼Œè¿˜éš¾ä¿è¯ä¸äººç±»æ™®éåå¥½å¥‘åˆã€‚å› æ­¤ï¼Œæœ¬æ–‡å—ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸­å¯¹æŠ—è®­ç»ƒå¯å‘ï¼Œæå‡º GAN - RM æ¡†æ¶ï¼Œæ—¨åœ¨æ‘†è„±æ‰‹åŠ¨åå¥½æ ‡æ³¨å’Œæ˜¾å¼è´¨é‡ç»´åº¦è®¾è®¡ï¼Œé«˜æ•ˆæ„å»ºå¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— éœ€æ‰‹åŠ¨åå¥½æ ‡æ³¨ï¼Œåˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®
GAN - RM ä»…éœ€å°‘é‡ï¼ˆå‡ ç™¾ä¸ªï¼‰æ— æ ‡æ³¨çš„ä»£è¡¨æ€§æ ·æœ¬ï¼ˆå³åå¥½ä»£ç†æ•°æ®ï¼ŒPreference Proxy Dataï¼‰ä½œä¸ºå¤–éƒ¨æ•°æ®ã€‚é€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹åŒºåˆ†åå¥½ä»£ç†æ•°æ®å’Œç”Ÿæˆæ¨¡å‹è¾“å‡ºï¼Œè®©æ¨¡å‹å­¦ä¹ è¯„ä¼°ç”Ÿæˆæ ·æœ¬ã€‚åŒæ—¶é‡‡ç”¨åŸºäºæ’åçš„è‡ªä¸¾ç­–ç•¥ï¼Œå°† GAN - RM åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„ç½®ä¿¡åˆ†æ•°ä½œä¸ºè½¯æ ‡ç­¾ï¼Œåˆ©ç”¨é¢å¤–æ•°æ®å†è®­ç»ƒ GAN - RMï¼Œä½¿å…¶æ›´å¥½æ•æ‰æ½œåœ¨äººç±»åå¥½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šè½®è®­ç»ƒï¼Œè¿­ä»£å¯¹é½åå¥½
GAN - RM æ”¯æŒå¤šè½®è®­ç»ƒåä¼˜åŒ–ã€‚æ¯ä¸€è½®ä¸­ï¼Œå°†è¢«è¯†åˆ«ä¸ºæ¥è¿‘åå¥½ä»£ç†æ•°æ®çš„æ ·æœ¬ç”¨äºç”Ÿæˆå™¨çš„è®­ç»ƒåä¼˜åŒ–ï¼Œåè¿‡æ¥å†è®­ç»ƒåˆ¤åˆ«å™¨ä»¥åŒºåˆ†è¿™äº›æ›´éš¾çš„æ ·æœ¬ã€‚è¿™ç§è¿­ä»£çš„â€œä»¥å‡ä¹±çœŸâ€è¿‡ç¨‹èƒ½é€æ­¥è®©ç”Ÿæˆè´¨é‡ä¸åå¥½ä»£ç†æ•°æ®ä¸­çš„æ½œåœ¨äººç±»åå¥½å¯¹é½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒåŸºäº GAN - RM çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯ä¸ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼ˆå¦‚ Pickapic çš„ 100 ä¸‡æ ‡æ³¨äººç±»åå¥½æ•°æ®ï¼‰çš„æ–¹æ³•ï¼ˆå¦‚ç›¸å…³å¯¹æ¯”æ–¹æ³•ï¼‰ç›¸å½“ç”šè‡³è¶…è¶Šã€‚åœ¨å›¾åƒè´¨é‡å®éªŒè®¾ç½®ä¸­ï¼ŒGAN - RM ä»…éœ€ 500 ä¸ªåå¥½ä»£ç†æ•°æ®æ ·æœ¬ã€‚é™¤å›¾åƒè´¨é‡æå‡å®éªŒå¤–ï¼Œåœ¨å›¾åƒå®‰å…¨å’Œè§†é¢‘è´¨é‡å¢å¼ºåœºæ™¯ä¸‹çš„å®éªŒä¹Ÿå‡¸æ˜¾äº† GAN - RM æ¡†æ¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆå¦‚ Best - of - N æ ·æœ¬è¿‡æ»¤ï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰è®­ç»ƒåæ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼ŒGAN - RM ä¸ºè§£å†³å¥–åŠ±å»ºæ¨¡ä¸­æ•°æ®è·å–éš¾ã€ä¾èµ–ç‰¹å®šåŸŸã€äººå·¥è®¾è®¡ç»´åº¦éš¾å¥‘åˆäººç±»åå¥½ç­‰é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå…¶åˆ©ç”¨å¯¹æŠ—è®­ç»ƒå’Œå°‘é‡ä»£ç†æ•°æ®çš„æ–¹å¼ï¼Œå‡å°‘äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œé™ä½å·¥ç¨‹æˆæœ¬ï¼›ä»åº”ç”¨æ‹“å±•è§’åº¦ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒã€è§†é¢‘ç­‰å¤šåœºæ™¯çš„æœ‰æ•ˆå®éªŒï¼Œä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è®­ç»ƒåå¢å¼ºæä¾›äº†å¯å¤ç”¨çš„å¥–åŠ±å»ºæ¨¡èŒƒå¼ï¼Œåç»­åœ¨è§†è§‰ç”Ÿæˆç›¸å…³ä»»åŠ¡ä¸­ï¼Œè‹¥éœ€æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œå¯å€Ÿé‰´å…¶åˆ©ç”¨å°‘é‡ä»£ç†æ•°æ®å’Œå¯¹æŠ—è®­ç»ƒçš„æ€è·¯æ¥é™ä½æˆæœ¬ä¸éš¾åº¦ã€‚

## $\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts
### Abstract
Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SPECSï¼šç”¨â€œæ¨æµ‹è‰ç¨¿â€åŠ é€Ÿå¤§æ¨¡å‹æ¨ç†ï¼Œå¹³è¡¡å»¶è¿Ÿä¸ç²¾åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æå‡å¸¸ä¾èµ–â€œæµ‹è¯•æ—¶ç®—åŠ›æ‰©å®¹â€ï¼Œæ¯”å¦‚åˆ†é…æ›´å¤šè®¡ç®—èµ„æºåšæ›´å……åˆ†çš„æ¢ç´¢ã€‚ä½†ç®—åŠ›å¢åŠ å¾€å¾€å¯¼è‡´ç”¨æˆ·ä¾§å»¶è¿Ÿå‡é«˜ï¼Œç›´æ¥å½±å“ä½“éªŒã€‚ç°æœ‰æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•å¤šèšç„¦ç®—åŠ›ï¼ˆFLOPSï¼‰ä¼˜åŒ–ç²¾åº¦ï¼Œå´å¿½ç•¥å»¶è¿Ÿçº¦æŸã€‚æ­¤å¤–ï¼ŒåŸºäºTransformerçš„LLMè‡ªå›å½’é‡‡æ ·å»¶è¿Ÿå¸¸å—é™äºå†…å­˜åŠ è½½è€Œéæ€»ç®—åŠ›ï¼Œè€Œæ¨æµ‹è§£ç è™½èƒ½å€Ÿå°æ¨¡å‹æå€™é€‰ token é™å»¶è¿Ÿï¼Œå´ä¼šå¢åŠ æ€»è®¡ç®—é‡ã€‚äºæ˜¯ï¼Œè®ºæ–‡è¯•å›¾å›ç­”ï¼š**èƒ½å¦è®¾è®¡é«˜æ•ˆæµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ï¼Œä¼˜åŒ–å»¶è¿Ÿ - æ•ˆç”¨æƒè¡¡ï¼Ÿ**

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º SPECS ç®—æ³•æ¡†æ¶  
SPECS å—æ¨æµ‹è§£ç å¯å‘ï¼Œæ˜¯ä¸€ç§â€œå»¶è¿Ÿæ„ŸçŸ¥å‹â€æµ‹è¯•æ—¶æ‰©å®¹æ–¹æ³•ã€‚å®ƒç”¨**æ›´å°æ›´å¿«çš„è‰ç¨¿æ¨¡å‹**é«˜æ•ˆç”Ÿæˆå€™é€‰åºåˆ—ï¼Œå†ç»“åˆ**æ›´å¤§çš„ç›®æ ‡æ¨¡å‹**ä¸**ä¸“ç”¨å¥–åŠ±æ¨¡å‹**è¯„ä¼°å€™é€‰ã€‚æ•´ä½“éµå¾ªâ€œè‰ç¨¿ - é€‰æ‹©â€æµç¨‹ï¼šè¿­ä»£ç”Ÿæˆå“åº”å—ï¼Œæ¯è½®ç”¨è‰ç¨¿æ¨¡å‹ç”Ÿæˆå€™é€‰å—ï¼Œç»æ‰“åˆ†é€‰æ‹©åæ‹¼æ¥ï¼Œè¿›å…¥ä¸‹ä¸€è½®ï¼›è‹¥è‰ç¨¿å…¨è¢«æ‹’ï¼Œåˆ™åˆ‡æ¢ç›®æ ‡æ¨¡å‹ç”Ÿæˆå€™é€‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¥–åŠ±å¼•å¯¼çš„è½¯éªŒè¯ä¸å»¶è¿Ÿæœºåˆ¶  
- å¥–åŠ±å¼•å¯¼è½¯éªŒè¯ï¼ˆSUBSAMPLE å­ä¾‹ç¨‹ï¼‰ï¼šåŸºäºè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡å‹è®¡ç®—çš„â€œåˆ†æ•°â€é€‰å€™é€‰å—ï¼Œæ—¢ä¼˜åŒ–æ•ˆç”¨ - å»¶è¿Ÿæƒè¡¡ï¼Œä¹Ÿé¿å…ç®€å•ä¸¢å¼ƒé«˜å¥–åŠ±ä½†å¯èƒ½è¢« naive æ¨æµ‹è§£ç æ¼æ‰çš„è½¨è¿¹ã€‚  
- å¥–åŠ±æ„ŸçŸ¥å»¶è¿Ÿè§„åˆ™ï¼ˆCASCADE å­ä¾‹ç¨‹ï¼‰ï¼šè‡ªé€‚åº”å†³å®šä¸‹ä¸€è½®ç”¨è‰ç¨¿è¿˜æ˜¯ç›®æ ‡æ¨¡å‹ç”Ÿæˆå€™é€‰â€”â€”è®©å¤§æ¨¡å‹å¤„ç†éš¾é¢˜æ­¥éª¤ï¼Œå°æ¨¡å‹å¤„ç†ç®€å•æ­¥éª¤ï¼ŒåŠ¨æ€å¹³è¡¡ç®—åŠ›ä¸å»¶è¿Ÿã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†è®ºåˆ†æä¿éšœæ”¶æ•›æ€§  
ä»ç†è®ºä¸Šåˆ†æï¼ŒSPECS åœ¨ç»“åˆè‰ç¨¿ã€ç›®æ ‡ã€å¥–åŠ±æ¨¡å‹ä¼˜åŒ–â€œKL æ­£åˆ™åŒ–å¥–åŠ±æœ€å¤§åŒ–â€ç›®æ ‡æ—¶ï¼Œå…¶è½¯éªŒè¯æ–¹æ³•éš beam å®½åº¦å¢å¤§ï¼Œèƒ½ä¼˜é›…æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ MATH500ã€AMC23ã€OlympiadBench æ•°æ®é›†æµ‹è¯•ï¼Œç”¨ Qwen - 1.5B - Instructï¼ˆè‰ç¨¿æ¨¡å‹ï¼‰ã€Qwen - 7B - Instructï¼ˆç›®æ ‡æ¨¡å‹ï¼‰ä¸ Qwen - 7B - Math - PRMï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰éªŒè¯ï¼š  
- ç²¾åº¦å±‚é¢ï¼šSPECS åŒ¹é…ç”šè‡³è¶…è¶Š beam search ç²¾åº¦ï¼›  
- å»¶è¿Ÿå±‚é¢ï¼šå»¶è¿Ÿæœ€å¤šé™ä½çº¦ 19.1%ï¼Œåœ¨ç²¾åº¦ä¸å»¶è¿Ÿé—´å®ç°æ›´ä¼˜æƒè¡¡ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **å»¶è¿Ÿ - ç²¾åº¦æƒè¡¡æ€è·¯**ï¼šè·³å‡ºâ€œåªçœ‹ç®—åŠ›/ç²¾åº¦â€çš„æ€ç»´å®šå¼ï¼ŒæŠŠå»¶è¿Ÿä½œä¸ºæ ¸å¿ƒçº¦æŸï¼Œä¸ºå¤§æ¨¡å‹è½åœ°ä½å»¶è¿Ÿåœºæ™¯ï¼ˆå¦‚ä¸ªæ€§åŒ–äº¤äº’ï¼‰æä¾›æ–°æ€è·¯ï¼›  
2. **å¤šæ¨¡å‹åä½œèŒƒå¼**ï¼šç”¨â€œå°è‰ç¨¿æ¨¡å‹ + å¤§ç›®æ ‡æ¨¡å‹ + å¥–åŠ±æ¨¡å‹â€åˆ†å±‚åä½œï¼Œæ—¢åˆ©ç”¨å°æ¨¡å‹æé€Ÿï¼Œåˆé å¤§æ¨¡å‹ä¿ç²¾åº¦ï¼Œè¿˜å€Ÿå¥–åŠ±æ¨¡å‹åšçµæ´»é€‰æ‹©ï¼Œè¿™ç§â€œåˆ†å·¥â€æ¨¡å¼å¯è¿ç§»åˆ°å…¶ä»–éœ€å¹³è¡¡èµ„æºä¸æ•ˆæœçš„ä»»åŠ¡ï¼›  
3. **ç†è®º + å®éªŒåŒéªŒè¯**ï¼šä»ç†è®ºè¯æ˜æ”¶æ•›æ€§ï¼Œå†ç”¨çœŸå®æ•°æ®é›†éªŒè¯ï¼Œä¸ºæ–¹æ³•å¯é æ€§èƒŒä¹¦ï¼Œä¹Ÿç¤ºèŒƒäº†å­¦æœ¯ç ”ç©¶ä¸­â€œæ–¹æ³• - ç†è®º - å®éªŒâ€é—­ç¯çš„é‡è¦æ€§ã€‚  


SPECS ä¸ºå¤§æ¨¡å‹æ¨ç†çš„â€œå»¶è¿Ÿ - ç²¾åº¦â€éš¾é¢˜æä¾›äº†ä¸€å¥—å…¼å…·åˆ›æ–°æ€§ä¸å®ç”¨æ€§çš„è§£æ³•ï¼Œæ— è®ºæ˜¯å·¥ä¸šç•Œè½åœ°ä½å»¶è¿Ÿ LLM åº”ç”¨ï¼Œè¿˜æ˜¯å­¦æœ¯ç•Œæ¢ç´¢æµ‹è¯•æ—¶ä¼˜åŒ–æ–°æ–¹å‘ï¼Œéƒ½æœ‰ä¸å°‘å¯å€Ÿé‰´çš„é—ªå…‰ç‚¹~

## eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling
### Abstract
Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EQA - RMï¼šä¸ºå…·èº«é—®ç­”é‡èº«å®šåˆ¶çš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œå®ç°æµ‹è¯•æ—¶å¯æ‰©å±•è¯„ä¼°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§æ¨¡å‹å¯¹é½ä¸­è‡³å…³é‡è¦ï¼Œä½†åœ¨å¤æ‚çš„å…·èº«ä»»åŠ¡ï¼ˆå¦‚å…·èº«é—®ç­”ï¼ŒEQAï¼‰ä¸­å´æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚EQAéœ€è¦æ™ºèƒ½ä½“åœ¨3Dç¯å¢ƒä¸­é€šè¿‡å¤šæ¨¡æ€è§‚å¯Ÿå’ŒåŠ¨ä½œåºåˆ—æ¥æ„ŸçŸ¥ã€äº¤äº’å’Œæ¨ç†ä»¥å›ç­”é—®é¢˜ï¼Œå¯¹æ™ºèƒ½ä½“çš„ç©ºé—´ã€æ—¶é—´å’Œé€»è¾‘ç†è§£è¿›è¡Œç»†è‡´è¯„ä¼°è‡³å…³é‡è¦ï¼Œè€Œé€šç”¨çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•æ— æ³•æ»¡è¶³è¿™ä¸€éœ€æ±‚ã€‚ç°æœ‰é€šç”¨å¥–åŠ±æ¨¡å‹å¤šä¸ºé™æ€è¾“å…¥æˆ–ç®€å•ç»“æœè®¾è®¡ï¼Œéš¾ä»¥æ•æ‰å…·èº«ä»»åŠ¡ä¸­å›ºæœ‰çš„æ—¶ç©ºå’Œé€»è¾‘ä¾èµ–å…³ç³»ï¼Œå› æ­¤è¿«åˆ‡éœ€è¦ä¸“é—¨çš„æœºåˆ¶æ¥å‡†ç¡®è¯„ä¼°EQAçš„å¤šæ–¹é¢æˆåŠŸæŒ‡æ ‡ã€‚åŒæ—¶ï¼ŒEQAé¢†åŸŸç¼ºä¹ç”¨äºä¸¥æ ¼è¯„ä¼°å’Œæ¯”è¾ƒå¥–åŠ±æ¨¡å‹çš„æ ‡å‡†åŒ–åŸºå‡†ï¼Œå½“å‰EQAä»»åŠ¡åŸºå‡†ä¾§é‡äºç²—ç•¥çš„æˆåŠŸæŒ‡æ ‡ï¼Œè€Œéå¯¹å¥–åŠ±æ¨¡å‹å‘å±•è‡³å…³é‡è¦çš„ç»†ç²’åº¦è½¨è¿¹è´¨é‡è¯„ä¼°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEQA - RMç”Ÿæˆå¼å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹
EQA - RMæ˜¯ä¸“ä¸ºè¯„ä¼°EQAè½¨è¿¹è€Œè®¾è®¡çš„æ–°å‹å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œä½œä¸ºç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ï¼Œä¸ä»…èƒ½äº§ç”Ÿæ ‡é‡å¥–åŠ±ï¼Œè¿˜èƒ½ä¸ºè¯„ä¼°æä¾›æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚å…¶å…·æœ‰å¢å¼ºçš„ç©ºé—´ã€æ—¶é—´å’Œæ¨ç†å¤„ç†èƒ½åŠ›ï¼Œä»¥å¤„ç†EQAä»»åŠ¡ä¸­å›ºæœ‰çš„ç‹¬ç‰¹å¤šæ¨¡æ€æ•°æ®æµã€‚é€šè¿‡é«˜æ•ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µç”¨æ ‡å‡†çš„Rejective Finetuningï¼ˆRFTï¼‰æ•™ä¼šæ¨¡å‹æœŸæœ›çš„è¾“å‡ºæ ¼å¼ï¼ˆåŒ…å«æ–‡æœ¬æ‰¹è¯„å’Œæ ‡é‡åˆ†æ•°ï¼‰ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åˆ›æ–°çš„Contrastive Group Relative Policy Optimizationï¼ˆC - GRPOï¼‰å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³ä»…RFTå¯èƒ½åªå­¦æ ¼å¼ä¸å­¦å†…å®¹çš„é—®é¢˜ï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¯¹æ¯”å¥–åŠ±ï¼ˆæºäºé’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºï¼Œå¦‚è§†é¢‘å¸§æ‰“ä¹±ã€ç©ºé—´åŒºåŸŸéšæœºæ©ç ã€æ¨ç†æ­¥éª¤æ··ä¹±ç­‰æ‰°åŠ¨æ–¹å¼ï¼‰ï¼Œè®©æ¨¡å‹åŒºåˆ†åŸå§‹è¿è´¯ä¸Šä¸‹æ–‡å’Œåˆæˆæ‰°åŠ¨ä¸Šä¸‹æ–‡ä¸‹çš„ç­–ç•¥è¾“å‡ºï¼Œä»è€Œå†…åŒ–æ—¶é—´é¡ºåºã€ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚å’Œè¿è´¯é€»è¾‘æµçš„é‡è¦æ€§ï¼ŒåŸ¹å…»å¯¹å…·èº«ä»»åŠ¡å¼ºå¤§ä¸”æ•é”çš„è¯„ä¼°èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºEQARewardBenchåŸºå‡†
ä¸ºè§£å†³EQAé¢†åŸŸå¥–åŠ±æ¨¡å‹è¯„ä¼°åŸºå‡†ç¼ºå¤±çš„é—®é¢˜ï¼ŒåŸºäºOpenEQAæ„å»ºäº†EQARewardBenchã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªHM3Då’ŒScanNetä¸¤ç§å®¶åº­ç¯å¢ƒçš„å…·èº«æƒ…èŠ‚è®°å¿†è§†é¢‘ï¼Œä»åŸå§‹é—®ç­”å¯¹æ„å»ºæ›´å…¨é¢çš„é—®é¢˜ - å“åº” - æ¨ç†è½¨è¿¹ä¸‰å…ƒç»„ï¼Œæœ‰1546ä¸ªæµ‹è¯•å®ä¾‹ï¼Œç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹åœ¨è½¨è¿¹è´¨é‡çš„å…«ä¸ªä¸åŒæ–¹é¢ï¼ˆå¦‚æ­£ç¡®æ€§ã€æ¥åœ°æ€§ã€æ•ˆç‡ç­‰ï¼‰ï¼Œä¸ºEQAä»»åŠ¡ä¸Šçš„å¥–åŠ±æ¨¡å‹æä¾›äº†æ ‡å‡†åŒ–ã€å¯æ¯”è¾ƒçš„è¯„ä¼°å¹³å°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä»¥Qwen2 - VL - 2B - Instructä¸ºåŸºç¡€è¿›è¡Œå¾®è°ƒçš„EQA - RMå±•ç°å‡ºé«˜æ ·æœ¬æ•ˆç‡ï¼Œä»…ç”¨700ä¸ªæ ·æœ¬åœ¨EQA - RM - Benchä¸Šè¾¾åˆ°61.9%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„ä¸“æœ‰åŸºçº¿ï¼ˆå¦‚Gemini - 2.5 - Flashã€GPT - 4oã€Claude - 3.5 - Haikuï¼‰å’Œå¼€æºçš„æœ€å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚RoVRMå’ŒVisualPRMï¼‰ã€‚åŒæ—¶ï¼ŒEQA - RMå±•ç¤ºäº†æµ‹è¯•æ—¶å¯æ‰©å±•æ€§ï¼Œåœ¨æ¨ç†æ—¶å¢åŠ è¯„ä¼°è®¡ç®—é‡ï¼Œå…¶åœ¨EQARewardBenchä¸Šçš„å‡†ç¡®ç‡ä»42.47%æå‡åˆ°61.86%ï¼Œæ€§èƒ½æå‡ååœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢†å…ˆçš„å¤§å‹å•†ä¸šæ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é’ˆå¯¹ç‰¹å®šå¤æ‚ä»»åŠ¡è®¾è®¡ä¸“ç”¨å¥–åŠ±æ¨¡å‹ï¼šå½“é€šç”¨æ¨¡å‹æ— æ³•æ»¡è¶³å¤æ‚ä»»åŠ¡ï¼ˆå¦‚å…·èº«ä»»åŠ¡ï¼‰çš„è¯„ä¼°éœ€æ±‚æ—¶ï¼Œå¯åƒEQA - RMä¸€æ ·é’ˆå¯¹ä»»åŠ¡ç‰¹æ€§ï¼Œè®¾è®¡å…·å¤‡ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚ç©ºé—´ã€æ—¶é—´ã€æ¨ç†å¤„ç†èƒ½åŠ›ï¼‰çš„ä¸“ç”¨æ¨¡å‹ï¼Œè§£å†³é€šç”¨æ¨¡å‹çš„å±€é™æ€§ã€‚
2. åˆ›æ–°çš„è®­ç»ƒç­–ç•¥ï¼šä¸¤é˜¶æ®µè®­ç»ƒï¼ˆRFT + C - GRPOï¼‰ä»¥åŠåˆ©ç”¨æ•°æ®å¢å¼ºçš„å¯¹æ¯”å¥–åŠ±ç­–ç•¥ï¼Œä¸ºè§£å†³æ¨¡å‹åªå­¦å½¢å¼ä¸å­¦å†…å®¹ã€æå‡æ¨¡å‹å¯¹ä»»åŠ¡å…³é”®è¦ç´ çš„ç†è§£æä¾›äº†æ€è·¯ï¼Œå¯å€Ÿé‰´äºå…¶ä»–éœ€è¦æ¨¡å‹æ·±å…¥ç†è§£ä»»åŠ¡ç»†èŠ‚çš„è®­ç»ƒåœºæ™¯ã€‚
3. æ„å»ºé¢†åŸŸåŸºå‡†ï¼šå¯¹äºç¼ºä¹è¯„ä¼°åŸºå‡†çš„é¢†åŸŸï¼Œå¯åƒæ„å»ºEQARewardBenchä¸€æ ·ï¼ŒåŸºäºç°æœ‰æ•°æ®é›†æ„å»ºä¸“é—¨çš„åŸºå‡†ï¼Œæ¨åŠ¨é¢†åŸŸå†…æ¨¡å‹çš„è¯„ä¼°å’Œå‘å±•ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æ¯”è¾ƒå’Œæ”¹è¿›æä¾›æ ‡å‡†å¹³å°ã€‚

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Athenaï¼šç”¨æ•°æ®é«˜æ•ˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œå¤šæ­¥éª¤æ¨ç†ï¼‰ä»å…·æŒ‘æˆ˜ã€‚ä¸ºå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ç­‰æ–¹æ³•è¢«æ¢ç´¢ï¼Œå…¶ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰èƒ½ä¸ºä¸­é—´æ¨ç†æ­¥éª¤æä¾›ç»†ç²’åº¦åé¦ˆï¼Œæ€§èƒ½æ›´ä¼˜ä¸”æ³›åŒ–æ€§å¼ºã€‚ç„¶è€Œï¼ŒPRMs å‘å±•é¢ä¸´ä¸¤å¤§éš¾é¢˜ï¼šä¸€æ˜¯è·å–å¸¦è¿‡ç¨‹æ ‡ç­¾çš„é«˜è´¨é‡æ•°æ®æˆæœ¬é«˜ï¼ˆéœ€å¤§é‡äººå·¥æ ‡æ³¨æˆ–è®¡ç®—æ˜‚è´µçš„è‡ªåŠ¨åŒ–æ ‡æ³¨ï¼‰ï¼›äºŒæ˜¯ä¼ ç»Ÿè‡ªåŠ¨åŒ–æ ‡æ³¨ï¼ˆå¦‚è’™ç‰¹å¡æ´›ä¼°è®¡ï¼‰æ˜“äº§ç”Ÿå™ªå£°æ ‡ç­¾ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶å‡è½»æ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œæå‡ PRMs æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¼ºå¼±å®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç”Ÿæˆé«˜è´¨é‡è¿‡ç¨‹æ ‡ç­¾  
ä¼ ç»Ÿè’™ç‰¹å¡æ´›ç­‰è‡ªåŠ¨åŒ–æ ‡æ³¨æ–¹æ³•æ˜“å—å®Œæˆå™¨æ¨ç†èƒ½åŠ›å½±å“ï¼Œæ ‡ç­¾æœ‰å™ªå£°ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡å‘ç°ï¼Œå¼ºå®Œæˆå™¨å³ä¾¿ä¸­é—´æ­¥éª¤é”™è¯¯ä»èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œå¼±å®Œæˆå™¨åˆ™å¯èƒ½åœ¨ä¸­é—´æ­¥éª¤æ­£ç¡®æ—¶ä¹Ÿå¤±è´¥ã€‚åŸºäºæ­¤ï¼Œæå‡ºç”¨å¼±ã€å¼ºå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ä½œä¸ºç­›é€‰å¯é è¿‡ç¨‹æ ‡ç­¾çš„æ ‡å‡†ï¼Œä¿ç•™ä¸¤è€…æ ‡ç­¾ä¸€è‡´çš„æ­¥éª¤ï¼Œå‡å°‘å®Œæˆå™¨å¸¦æ¥çš„åå·®ï¼Œæå‡æ ‡ç­¾è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œçº¦ 5000 æ¡é«˜è´¨é‡æ ‡ç­¾å°±èƒ½æ¯”ä¼ ç»Ÿæ–¹æ³•çº¦ 30 ä¸‡æ¡å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¡¨ç°æ›´ä¼˜ï¼Œä¸”å¤§å¹…é™ä½æ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒçš„è®¡ç®—æˆæœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ PRMs æ€§èƒ½çš„ä¸¤å¤§ç­–ç•¥  
 - ORM åˆå§‹åŒ–ï¼šPRMs é€šå¸¸åŸºäºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¾®è°ƒï¼Œè€Œç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰åœ¨å¤§è§„æ¨¡å“åº”çº§æ•°æ®ä¸Šè®­ç»ƒï¼Œå…·å¤‡å¼±ç›‘ç£ä¸‹è¯„ä¼°ä¸­é—´æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚å› æ­¤ç”¨ ORMs åˆå§‹åŒ– PRMsï¼Œå°† ORMs ä½œä¸ºå¼±ç›‘ç£é¢„è®­ç»ƒï¼ŒPRMs å†åœ¨é«˜è´¨é‡ç»†ç²’åº¦æ­¥éª¤æ•°æ®ä¸Šå¾®è°ƒï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚  
 - è´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ï¼šè¿‡ç¨‹æ ‡ç­¾æ•°æ®å­˜åœ¨æ ‡ç­¾ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡å¯¹å«è´Ÿæ­¥éª¤æ ‡ç­¾çš„æ•°æ®è¿›è¡Œä¸Šé‡‡æ ·ï¼Œè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡é—®é¢˜ï¼Œä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»º Athena ç³»åˆ—æ¨¡å‹å¹¶å¤šåœºæ™¯éªŒè¯  
åŸºäºä¸Šè¿°æ–¹æ³•æ„å»ºç»“æœå¥–åŠ±æ¨¡å‹ Athena - ORM å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ Athena - PRMï¼Œå†åˆ©ç”¨ Athena - PRM é€šè¿‡å¥–åŠ±æ’åºå¾®è°ƒå¾—åˆ° Athena - 7Bã€‚å¹¶åœ¨ä¸‰ä¸ªåœºæ™¯éªŒè¯ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­å¯¹ç­–ç•¥æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªè¾“å‡ºæ’åºï¼›ç›´æ¥è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§ï¼›å¥–åŠ±æ’åºå¾®è°ƒï¼ˆç”¨é«˜å¥–åŠ±å“åº”å¾®è°ƒç­–ç•¥æ¨¡å‹ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- æµ‹è¯•æ—¶ç¼©æ”¾åœºæ™¯ï¼šåœ¨ 7 ä¸ªå¤šæ¨¡æ€æ•°å­¦å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç”¨ Athena - PRM é…åˆä¸åŒè§„æ¨¡ï¼ˆ7B åˆ° 72Bï¼‰ç­–ç•¥æ¨¡å‹ï¼Œæ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ã€‚å¦‚ç”¨ Qwen2.5 - VL - 7B ä½œä¸ºç­–ç•¥æ¨¡å‹æ—¶ï¼Œåœ¨ WeMath åŸºå‡†ä¸Šé›¶æ ·æœ¬åŸºçº¿æå‡ 10.2 åˆ†ï¼Œåœ¨ MathVista æå‡ 7.1 åˆ†ï¼›åœ¨æ–‡æœ¬-only æ•°å­¦åŸºå‡†ç”¨ Mistral - 8B æ—¶æå‡ 8.9 åˆ†ã€‚  
- æ¨ç†æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°åœºæ™¯ï¼šåœ¨ VisualProcessBench åŸºå‡†ä¸Šï¼ŒAthena - PRM è¡¨ç°å¼ºåŠ²ï¼Œè¶…è¶Šå¼€æºçš„ VisualPRM - 8B ç­‰æ¨¡å‹ï¼ŒF1 åˆ†æ•°æ¯”ä¹‹å‰æœ€ä¼˜ç»“æœé«˜ 3.9ï¼Œå±•ç°å‡†ç¡®è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚  
- å¥–åŠ±æ’åºå¾®è°ƒåœºæ™¯ï¼šåŸºäº Qwen2.5 - VL - 7B å¾®è°ƒå¾—åˆ°çš„ Athena - 7Bï¼Œåœ¨ 7 ä¸ªæ•°å­¦å’Œæ¨ç†åŸºå‡†ä¸Šå¤§å¹…æå‡ç­–ç•¥æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ•°æ®é«˜æ•ˆæ ‡æ³¨æ€è·¯ï¼šåˆ©ç”¨å¤šå®Œæˆå™¨é¢„æµ‹ä¸€è‡´æ€§ç­›é€‰æ ‡ç­¾ï¼Œä¸ºè§£å†³éœ€ç»†ç²’åº¦æ ‡æ³¨ä¸”æ ‡æ³¨æˆæœ¬é«˜çš„ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ï¼Œåœ¨å‡å°‘æ•°æ®é‡åŒæ—¶æå‡æ•°æ®è´¨é‡ï¼Œå®ç°æ•°æ®é«˜æ•ˆåˆ©ç”¨ã€‚  
- æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼šORM åˆå§‹åŒ–å’Œè´Ÿæ ·æœ¬ä¸Šé‡‡æ ·ç­–ç•¥ï¼Œä¸ºæå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½æä¾›äº†å¯å¤ç”¨æ–¹æ³•ï¼Œå¯å¯å‘å…¶ä»–å¥–åŠ±æ¨¡å‹æˆ–éœ€ç»†ç²’åº¦åé¦ˆæ¨¡å‹çš„è®­ç»ƒä¼˜åŒ–ã€‚  
- å¤šåœºæ™¯éªŒè¯æ¨¡å¼ï¼šåœ¨æµ‹è¯•æ—¶ç¼©æ”¾ã€æ­¥éª¤è¯„ä¼°ã€æ¨¡å‹å¾®è°ƒç­‰å¤šåœºæ™¯éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿™ç§å…¨é¢éªŒè¯æ€è·¯æœ‰åŠ©äºæ›´å……åˆ†å±•ç¤ºæ–¹æ³•ä»·å€¼ï¼Œä¸ºåç»­ç ”ç©¶æä¾›éªŒè¯èŒƒå¼å‚è€ƒã€‚

## learning-to-reason-across-parallel-samples-for-llm-reasoning
### Abstract
Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.
### ğŸŒŸ è®ºæ–‡è§£è¯» | èåˆå¹¶è¡Œä¸é¡ºåºæ¨ç†ï¼ŒSSAè®©å¤§æ¨¡å‹æ¨ç†æ›´é«˜æ•ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šèƒ½åŠ›ä¸æ–­æå‡ï¼Œè€Œæµ‹è¯•æ—¶è®¡ç®—èµ„æºçš„åˆ†é…ï¼ˆå³æµ‹è¯•æ—¶ç¼©æ”¾ï¼‰æ˜¯ä¼˜åŒ–æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹å‘ã€‚ç°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åˆ†å¹¶è¡Œå’Œé¡ºåºä¸¤ç±»ï¼šå¹¶è¡Œç¼©æ”¾æ˜¯ç‹¬ç«‹ç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„å†èšåˆï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨ï¼‰ï¼›é¡ºåºç¼©æ”¾åˆ™è¿­ä»£ä¼˜åŒ–å•ä¸ªè§£ï¼ˆå¦‚åŸºäºæç¤ºçš„è‡ªæˆ‘åæ€ï¼‰ã€‚ä½†å¹¶è¡Œæ–¹æ³•å¸¸å­¤ç«‹çœ‹å¾…æ ·æœ¬ï¼Œé¡ºåºæ–¹æ³•è®¡ç®—æˆæœ¬æˆ–é€‚é…æ€§å—é™ã€‚æœ¬æ–‡æ—¨åœ¨æå‡ºæ–°æ–¹æ³•ï¼ŒèåˆäºŒè€…ä¼˜åŠ¿ï¼Œæ›´é«˜æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºæå‡æ¨ç†æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSample Set Aggregatorï¼ˆSSAï¼‰æ¨¡å‹æ¶æ„  
è®¾è®¡è½»é‡çº§çš„SSAæ¨¡å‹ï¼Œå°†å…¶ä¸ç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ï¼ˆLMansï¼‰è§£è€¦ã€‚å…ˆç”±LManså¹¶è¡Œç”ŸæˆKä¸ªå€™é€‰ç­”æ¡ˆï¼Œå†æŠŠè¿™äº›å€™é€‰ç­”æ¡ˆæ‹¼æ¥æˆåºåˆ—è¾“å…¥SSAï¼ŒSSAé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä»¥è¾“å‡ºæœ€ç»ˆæ­£ç¡®ç­”æ¡ˆã€‚è¿™ç§è®¾è®¡è®©SSAèƒ½åŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºçš„åˆ†å¸ƒç‰¹æ€§ï¼Œç›´æ¥ä¼˜åŒ–ç­”æ¡ˆåˆæˆè¿‡ç¨‹ï¼Œè€Œéå­¤ç«‹è¯„ä¼°å•ä¸ªæ ·æœ¬ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºè¾“å‡ºåˆ†å¸ƒæ¨ç†ï¼Œè§£è€¦è®­ç»ƒä¸æ¨ç†  
SSAä¸ç›´æ¥è®­ç»ƒç”Ÿæˆç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ï¼ˆLManså¯è§†ä¸ºé»‘ç›’ï¼‰ï¼Œè€Œæ˜¯é’ˆå¯¹å…¶é‡‡æ ·è¾“å‡ºè¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§â€œæ¨ç†è¾“å‡ºåˆ†å¸ƒè€Œéè°ƒæ•´æ¨¡å‹å†…éƒ¨â€çš„æ€è·¯ï¼Œè®©æ–¹æ³•æ›´çµæ´»â€”â€”å¯é€‚é…ä¸åŒåŸºç¡€æ¨¡å‹ï¼ˆç”šè‡³æ˜¯åªèƒ½é€šè¿‡APIè°ƒç”¨çš„é»‘ç›’å¤§æ¨¡å‹ï¼‰ï¼Œåªéœ€ç”¨å…¶é‡‡æ ·ç­”æ¡ˆè®­ç»ƒSSAå³å¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»Ÿä¸€å¹¶è¡Œä¸é¡ºåºç¼©æ”¾ä¼˜åŠ¿  
å¹¶è¡Œç¼©æ”¾èƒ½å¿«é€Ÿè·å–å¤šè§†è§’ç­”æ¡ˆï¼Œé¡ºåºç¼©æ”¾å¯è¿­ä»£ä¼˜åŒ–æ¨ç†ï¼›SSAé€šè¿‡â€œå¹¶è¡Œé‡‡æ ·+å•æ­¥é¡ºåºRLèšåˆâ€çš„æ–¹å¼ï¼Œåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç»“åˆäºŒè€…é•¿å¤„ï¼šç”¨å¹¶è¡Œè·å–å¤šæ ·æ€§ï¼Œç”¨SSAçš„é¡ºåºæ¨ç†å®ç°ç²¾å‡†èšåˆï¼Œä¸”ä»…éœ€è®­ç»ƒå°æ¨¡å‹å°±èƒ½å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. æ€§èƒ½è¶…è¶Šå¼ºåŸºçº¿ï¼šåœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒSSAç›¸æ¯”åŸºäºå¥–åŠ±æ¨¡å‹é‡æ’åºç­‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œå¤§å¹…ç¼©å°äº†æ¨¡å‹å®é™…æ€§èƒ½ä¸â€œç†è®ºæœ€ä¼˜ï¼ˆoracle - bestï¼‰â€ç²¾åº¦çš„å·®è·ã€‚  
2. æ³›åŒ–èƒ½åŠ›çªå‡ºï¼šè·¨æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—ï¼ˆå¦‚Qwen 2.5ã€Llama 3.1ï¼‰ã€æ¨¡å‹è§„æ¨¡ï¼ˆ7B/14B/32Bï¼‰å’Œä»»åŠ¡ï¼ŒSSAéƒ½å±•ç°å‡ºè‰¯å¥½æ³›åŒ–æ€§ã€‚æ¯”å¦‚åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šä¸ºç‰¹å®šæ¨¡å‹è®­ç»ƒçš„SSAï¼Œèƒ½æˆåŠŸèšåˆä¸åŒæ¨¡å‹å®¶æ—ã€è§„æ¨¡åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¾“å‡ºã€‚  
3. è½»é‡åŒ–ä¼˜åŠ¿ï¼šç´§å‡‘çš„SSAæ¨¡å‹èƒ½åŒ¹é…é¡ºåºç¼©æ”¾ä¸­ç»å¼ºåŒ–è®­ç»ƒçš„å¤§æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜å…¶ä½œä¸ºè½»é‡é¡ºåºç¼©æ”¾æ–¹å¼çš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¶æ„è§£è€¦æ€è·¯ï¼šå°†â€œç­”æ¡ˆç”Ÿæˆâ€ä¸â€œç­”æ¡ˆèšåˆåˆ†æâ€è§£è€¦ï¼Œä¸ºåˆ©ç”¨é»‘ç›’å¤§æ¨¡å‹ï¼ˆå¦‚è°ƒç”¨APIçš„å•†ç”¨å¤§æ¨¡å‹ï¼‰æä¾›äº†å¯è¡Œè·¯å¾„â€”â€”åªéœ€è·å–å…¶è¾“å‡ºï¼Œç”¨SSAåšåå¤„ç†å³å¯ï¼Œæ— éœ€æ”¹åŠ¨é»‘ç›’æ¨¡å‹æœ¬èº«ã€‚  
2. æµ‹è¯•æ—¶ç¼©æ”¾æ–°èŒƒå¼ï¼šå±•ç¤ºäº†â€œå¹¶è¡Œé‡‡æ · + é’ˆå¯¹æ€§å°æ¨¡å‹èšåˆâ€åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ½œåŠ›ï¼Œä¸ºåç»­ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—æ•ˆç‡ã€å¹³è¡¡èµ„æºä¸æ€§èƒ½æä¾›äº†æ–°æ–¹å‘ã€‚  
3. å¼ºåŒ–å­¦ä¹ åº”ç”¨å¯å‘ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–èšåˆæ¨¡å‹ï¼ˆSSAï¼‰æ¥æå‡æœ€ç»ˆç­”æ¡ˆç²¾åº¦ï¼ŒéªŒè¯äº†åœ¨â€œè¾“å‡ºåˆ†å¸ƒå±‚é¢åšæ¨ç†ä¼˜åŒ–â€çš„ä»·å€¼ï¼Œå¯å¯å‘æ›´å¤šå›´ç»•æ¨¡å‹è¾“å‡ºåå¤„ç†çš„ç ”ç©¶ã€‚

## guided-speculative-inference-for-efficient-test-time-alignment-of-llms
### Abstract
We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜æ•ˆLLMæµ‹è¯•æ—¶å¯¹é½çš„å¼•å¯¼å¼æ¨æµ‹æ¨ç†ï¼ˆGSIï¼‰

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç±»ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†æ¨¡å‹ä¸æ•°æ®è§„æ¨¡çš„æ‰©å¤§å¸¦æ¥äº†é«˜æ˜‚çš„è®¡ç®—ä¸ç»æµæˆæœ¬ï¼Œå› æ­¤éœ€è¦æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚æµ‹è¯•æ—¶æ‰©å±•ï¼ˆtest - time scalingï¼‰èšç„¦äºæ¨ç†æ—¶çš„è®¡ç®—æ‰©å±•ï¼Œè€Œæ¨¡å‹å¯¹é½åˆ™è‡´åŠ›äºè®©æ¨¡å‹ä¼˜åŒ–ä»¥æœ€å¤§åŒ–ç»™å®šå¥–åŠ±æ¨¡å‹çš„å›æŠ¥ã€‚ç°æœ‰æ–¹æ³•å¦‚å¥–åŠ±å¼•å¯¼çš„æ¨æµ‹è§£ç ï¼ˆRSDï¼‰ç¼ºä¹åˆ†å¸ƒä¿çœŸåº¦çš„ç†è®ºä¿è¯ï¼Œè½¯æœ€ä½³né‡‡æ ·ï¼ˆsoft best - of - nï¼‰è™½æœ‰ä¸€å®šä½œç”¨ä½†ä¹Ÿå­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡æå‡ºå¼•å¯¼å¼æ¨æµ‹æ¨ç†ï¼ˆGSIï¼‰ç®—æ³•ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„å¥–åŠ±å¼•å¯¼è§£ç ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç®—æ³•èåˆä¸ç›®æ ‡è¿‘ä¼¼
GSIç»“åˆäº†è½¯best - of - næµ‹è¯•æ—¶æ‰©å±•ã€å¥–åŠ±æ¨¡å‹\( r(x,y) \)ä»¥åŠæ¥è‡ªå°å‹è¾…åŠ©æ¨¡å‹\( \pi_S(y\mid x) \)çš„æ¨æµ‹æ ·æœ¬ã€‚é€šè¿‡å¯¹\( \pi_B \)å’Œ\( \pi_S \)ä¸‹çš„å¯¹æ•°ä¼¼ç„¶è°ƒæ•´å¥–åŠ±ï¼ˆå€¾æ–œï¼‰ï¼ŒGSIå¯è¯æ˜åœ°è¿‘ä¼¼\( \pi_B \)ä¸‹è½¯best - of - nçš„æœ€ä¼˜å€¾æ–œç­–ç•¥\( \pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta\,r(x,y)) \)ã€‚å°†å€¾æ–œåˆ†å¸ƒé‡å†™ä¸ºåŸºäº\( \pi_S \)çš„åˆ†å¸ƒå½¢å¼ï¼Œé€šè¿‡å¯¹\( \pi_S \)é‡‡æ ·å¹¶é‡æ–°åŠ æƒå€™é€‰æ¥è¿‘ä¼¼\( \pi_{\beta,B} \)ï¼Œå®šä¹‰äº†å¥–åŠ± - ä¼¼ç„¶å€¾æ–œçš„è½¯best - of - nï¼ˆReward - Likelihood Tilted S - BoNï¼‰ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºä¿è¯ä¸ç®—æ³•æµç¨‹
æ¨å¯¼äº†è¯±å¯¼åˆ†å¸ƒä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´KLæ•£åº¦çš„ç†è®ºç•Œã€‚æå‡ºçš„GSIç®—æ³•åœ¨æ¯ä¸€æ­¥æ¨ç†æ—¶ï¼Œå…ˆä»\( \pi_S \)é‡‡æ ·ï¼Œè®¡ç®—å¥–åŠ±ä¸ä¼¼ç„¶è°ƒæ•´åçš„å€¾æ–œå€¼ï¼Œé€šè¿‡softmaxé‡‡æ ·å€™é€‰ï¼›è‹¥å€™é€‰æ»¡è¶³é˜ˆå€¼åˆ™æ¥å—ï¼Œå¦åˆ™å›é€€åˆ°ä»\( \pi_B \)è¿›è¡Œè½¯best - of - né‡‡æ ·ã€‚åŒæ—¶å‡è®¾è¦†ç›–æ¡ä»¶\( C_{\infty}(x) := \sup_{y\in Y:\pi_B(y|x)>0} \frac{\pi_B(y | x)}{\pi_S(y | x)} < \infty \)ï¼Œåœ¨æ­¤æ¡ä»¶ä¸‹è¯æ˜Reward - Likelihood Tilted S - BoNå¯¹å€¾æ–œåˆ†å¸ƒçš„è¿‘ä¼¼æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆMATH500ã€OlympiadBenchã€Minerva Mathï¼‰ä¸Šï¼ŒGSIæ–¹æ³•æ¯”ä½¿ç”¨\( \pi_S \)çš„æ ‡å‡†è½¯best - of - nå’Œå¥–åŠ±å¼•å¯¼çš„æ¨æµ‹è§£ç ï¼ˆLiao et al., 2025ï¼‰å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œåœ¨æŸäº›è®¾ç½®ä¸‹ç”šè‡³è¶…è¿‡äº†ä½¿ç”¨\( \pi_B \)çš„è½¯best - of - nã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•åˆ›æ–°è§’åº¦ï¼ŒGSIèåˆå¤šç§æŠ€æœ¯å¹¶æä¾›ç†è®ºä¿è¯çš„æ€è·¯ï¼Œä¸ºåç»­é«˜æ•ˆLLMæ¨ç†ä¸å¯¹é½æ–¹æ³•ç ”ç©¶æä¾›äº†å‚è€ƒï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç»“åˆä¸åŒç»„ä»¶ï¼ˆå°æ¨¡å‹æ¨æµ‹ã€å¥–åŠ±æ¨¡å‹ã€è½¯é‡‡æ ·ç­‰ï¼‰å¹¶è¿›è¡Œç†è®ºåˆ†ææ¥æå‡æ€§èƒ½ï¼›ä»å®éªŒè§’åº¦ï¼Œåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šçš„éªŒè¯ä¸ºç±»ä¼¼æ–¹æ³•çš„æ•ˆæœè¯„ä¼°æä¾›äº†èŒƒä¾‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å®é™…ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´å…¶ä»»åŠ¡é€‰æ‹©ä¸å¯¹æ¯”å®éªŒè®¾ç½®æ–¹å¼ã€‚

## co-evolving-llm-coder-and-unit-tester-via-reinforcement-learning
### Abstract
We propose CURE, a novel reinforcement learning framework with a dedicated
reward design that co-evolves coding and unit test generation capabilities
based on their interaction outcomes, without any ground-truth code as
supervision. This approach enables flexible and scalable training and allows
the unit tester to learn directly from the coder's mistakes. Our derived
ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and
Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,
outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They
naturally extend to downstream tasks such as test-time scaling and agentic
coding-achieving a 8.1% improvement over the base model. For the long-CoT
model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while
achieving 64.8% inference efficiency in unit test generation. Notably, we also
find that our model can serve as an effective reward model for reinforcement
learning on base models. Project: https://github.com/Gen-Verse/CURE
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— çœŸå€¼ä»£ç ç›‘ç£ä¸‹ï¼Œè®©LLMç¼–ç ä¸å•å…ƒæµ‹è¯•èƒ½åŠ›ååŒè¿›åŒ–çš„CUREæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ä¸­ï¼Œæå‡å…¶ç¼–ç èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæå‡ç¼–ç èƒ½åŠ›çš„æ–¹å¼å­˜åœ¨å±€é™ï¼Œæ¯”å¦‚è®­ç»ƒå•å…ƒæµ‹è¯•ç”Ÿæˆå™¨ä¾èµ–çœŸå€¼ä»£ç ç›‘ç£ï¼Œæ”¶é›†çœŸå€¼ä»£ç æˆæœ¬é«˜ä¸” labor - intensiveï¼Œé™åˆ¶äº†è®­ç»ƒæ•°æ®è§„æ¨¡å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œå•å…ƒæµ‹è¯•å¯¹æå‡ç¼–ç æ€§èƒ½æ˜¯å…³é”®å› ç´ ï¼Œä½†å¦‚ä½•åœ¨æ— çœŸå€¼ä»£ç ä¸‹è®©ä»£ç ç”Ÿæˆå™¨å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆå™¨ååŒè¿›åŒ–æ¥æå‡ç¼–ç èƒ½åŠ›æ˜¯å¾…è§£å†³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œé•¿é“¾æ€ç»´ï¼ˆlong - CoTï¼‰æ¨¡å‹æ¨ç†é€Ÿåº¦ææ…¢ä¹Ÿéœ€ä¼˜åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCUREå¼ºåŒ–å­¦ä¹ æ¡†æ¶
CUREæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ— éœ€ä»»ä½•çœŸå€¼ä»£ç ç›‘ç£ï¼ŒåŸºäºä»£ç ç”Ÿæˆå™¨å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆå™¨çš„äº¤äº’ç»“æœæ¥ååŒè¿›åŒ–å®ƒä»¬çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ„å»ºæˆå¯¹å¥–åŠ±çŸ©é˜µï¼Œå®ç°ç›¸äº’ç›‘ç£å’ŒæŒç»­æ”¹è¿›ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä»£ç ç”Ÿæˆå™¨äº§ç”Ÿçš„æ­£ç¡®å’Œé”™è¯¯è§£å†³æ–¹æ¡ˆèƒ½è®©å•å…ƒæµ‹è¯•ç”Ÿæˆå™¨å­¦ä¹ åŒºåˆ†å¥½åä»£ç ï¼ŒåŒæ—¶ä»£ç ç”Ÿæˆå™¨ä¹Ÿèƒ½å¾—åˆ°ä¼˜åŒ–ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé’ˆå¯¹é•¿CoTæ¨¡å‹çš„å¥–åŠ±å˜æ¢
å¯¹äºé•¿é“¾æ€ç»´ï¼ˆlong - CoTï¼‰æ¨¡å‹ï¼Œå¼•å…¥å“åº”é•¿åº¦å¼•å¯¼çš„å¥–åŠ±å˜æ¢ï¼Œä½¿é•¿CoTå•å…ƒæµ‹è¯•ç”Ÿæˆå™¨åœ¨æµ‹è¯•æ—¶æ›´é«˜æ•ˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®­ç»ƒåçš„å•å…ƒæµ‹è¯•ç”Ÿæˆå™¨ä½œå¥–åŠ±æ¨¡å‹
è®­ç»ƒåçš„å•å…ƒæµ‹è¯•ç”Ÿæˆå™¨å¯ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒLLMsï¼Œåœ¨æ— äººå·¥æ ‡æ³¨æˆ–çœŸå€¼å•å…ƒæµ‹è¯•ç›‘ç£ä¸‹æå‡ç¼–ç æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Qwen2.5 - Instructæ¨¡å‹ä¸Šä¼˜åŒ–åï¼ŒReasonFlux - Coder 7Bå’Œ14Bæ¨¡å‹ä»£ç ç”Ÿæˆå‡†ç¡®ç‡æå‡5.3%ï¼ŒBest of Nå‡†ç¡®ç‡æå‡9.0%ï¼Œè¶…è¿‡åŒè§„æ¨¡çš„Qwen - Coderã€DeepSeek - Coderå’ŒSeed - Coderï¼›åœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚æµ‹è¯•æ—¶ç¼©æ”¾å’Œæ™ºèƒ½ç¼–ç ä¸Šï¼Œæ¯”åŸºç¡€æ¨¡å‹æå‡8.1%ï¼›é•¿CoTçš„ReasonFlux - Coder - 4Båœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­æ¨ç†æ•ˆç‡è¾¾64.8%ï¼Œä¸”æŒç»­è¶…è¿‡Qwen3 - 4Bï¼›è®­ç»ƒåçš„å•å…ƒæµ‹è¯•ç”Ÿæˆå™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹ç”¨äºåŸºç¡€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¹Ÿæœ‰ç«äº‰åŠ›çš„æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ååŒè¿›åŒ–æ€è·¯ï¼šCUREæ¡†æ¶å±•ç°äº†è®©ä¸¤ä¸ªç›¸å…³èƒ½åŠ›ï¼ˆç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆï¼‰é€šè¿‡äº¤äº’ååŒè¿›åŒ–çš„æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–å­˜åœ¨ç›¸äº’å½±å“èƒ½åŠ›çš„æ¨¡å‹è®­ç»ƒåœºæ™¯ï¼Œæ¯”å¦‚ä¸åŒç±»å‹çš„ç”Ÿæˆä»»åŠ¡é—´çš„ååŒä¼˜åŒ–ã€‚
2. æ— ç›‘ç£ç›‘ç£æ–¹å¼ï¼šæ— éœ€çœŸå€¼ä»£ç ç›‘ç£æ¥è®­ç»ƒå•å…ƒæµ‹è¯•ç”Ÿæˆå™¨å’Œä»£ç ç”Ÿæˆå™¨ï¼Œä¸ºæ•°æ®è·å–å›°éš¾åœºæ™¯ä¸‹çš„æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°æ–¹å‘ï¼Œå¯æ€è€ƒåœ¨å…¶ä»–ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®çš„ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•æ„é€ ç±»ä¼¼çš„è‡ªç›‘ç£æˆ–æ— ç›‘ç£ç›‘ç£æœºåˆ¶ã€‚
3. é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„ä¼˜åŒ–ç­–ç•¥ï¼šå¯¹é•¿CoTæ¨¡å‹çš„å“åº”é•¿åº¦å¼•å¯¼å¥–åŠ±å˜æ¢ï¼Œæç¤ºæˆ‘ä»¬åœ¨é¢å¯¹æœ‰ç‰¹æ®Šæ€§èƒ½ç“¶é¢ˆï¼ˆå¦‚æ¨ç†æ…¢ï¼‰çš„æ¨¡å‹æ—¶ï¼Œå¯ä»ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ï¼ˆå¦‚å“åº”é•¿åº¦ï¼‰å…¥æ‰‹è®¾è®¡ä¼˜åŒ–ç­–ç•¥ã€‚

## incentivizing-llms-to-self-verify-their-answers
### Abstract
Large Language Models (LLMs) have demonstrated remarkable progress in complex
reasoning tasks through both post-training and test-time scaling laws. While
prevalent test-time scaling approaches are often realized by using external
reward models to guide the model generation process, we find only marginal
gains can be acquired when scaling a model post-trained on specific reasoning
tasks. We identify that the limited improvement stems from distribution
discrepancies between the specific post-trained generator and the general
reward model. To address this, we propose a framework that incentivizes LLMs to
self-verify their own answers. By unifying answer generation and verification
within a single reinforcement learning (RL) process, we train models that can
effectively assess the correctness of their own solutions. The trained model
can further scale its performance during inference time by verifying its
generations, without the need for external verifiers. We train our
self-verification models based on Qwen2.5-Math-7B and
DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying
reasoning context lengths. Experiments on multiple mathematical reasoning
benchmarks show that our models can not only improve post-training performance
but also enable effective test-time scaling. Our code is available at
https://github.com/mansicer/self-verification.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤§æ¨¡å‹è‡ªæˆ‘éªŒè¯ç­”æ¡ˆï¼Œè§£é”æ¨ç†æ€§èƒ½æ–°é«˜åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œè™½èƒ½é€šè¿‡è®­ç»ƒåé˜¶æ®µå’Œæµ‹è¯•æ—¶çš„ç¼©æ”¾å®šå¾‹æå‡è¡¨ç°ï¼Œä½†ç°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚ç”¨å¤–éƒ¨å¥–åŠ±æ¨¡å‹å¼•å¯¼ç”Ÿæˆï¼‰åœ¨ç‰¹å®šæ¨ç†ä»»åŠ¡è®­ç»ƒåçš„æ¨¡å‹ä¸Šæ•ˆæœæœ‰é™ã€‚åŸå› åœ¨äºç‰¹å®šè®­ç»ƒåçš„ç”Ÿæˆå™¨ä¸é€šç”¨å¥–åŠ±æ¨¡å‹å­˜åœ¨åˆ†å¸ƒå·®å¼‚ï¼Œå¯¼è‡´éªŒè¯ä¸å‡†ç¡®ï¼Œéš¾ä»¥è®©è®­ç»ƒåé˜¶æ®µå’Œæµ‹è¯•æ—¶ç¼©æ”¾ååŒå¢æ•ˆã€‚æ‰€ä»¥ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºä¸€ç§è®©LLMsè‡ªæˆ‘éªŒè¯ç­”æ¡ˆçš„æ¡†æ¶ï¼Œå¼¥åˆè¿™ä¸¤ä¸ªé˜¶æ®µçš„å·®è·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªéªŒè¯æ¡†æ¶è®¾è®¡  
æå‡ºæ¿€åŠ±å¤§æ¨¡å‹è‡ªæˆ‘éªŒè¯ç­”æ¡ˆçš„æ¡†æ¶ï¼Œå°†ç­”æ¡ˆç”Ÿæˆä¸éªŒè¯ç»Ÿä¸€åˆ°å•ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ä¸­ã€‚è®©æ¨¡å‹æ—¢å­¦ä¹ ç”Ÿæˆç­”æ¡ˆï¼Œåˆå­¦ä¹ è¯„ä¼°è‡ªèº«è§£ç­”çš„æ­£ç¡®æ€§ï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨ï¼Œè§£å†³äº†ç‰¹å®šç”Ÿæˆå™¨ä¸é€šç”¨å¥–åŠ±æ¨¡å‹åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒæœºåˆ¶ä¼˜åŒ–  
åŸºäºGRPOç®—æ³•è®¾è®¡åœ¨çº¿ç­–ç•¥å¯¹é½ç¼“å†²åŒºå’ŒåŠ¨æ€éªŒè¯å¥–åŠ±ã€‚åœ¨çº¿ç¼“å†²åŒºä¿è¯éªŒè¯å™¨è¾“å…¥åˆ†å¸ƒä¸æ¨¡å‹æœ€æ–°è¾“å‡ºå¯¹é½ï¼›åŠ¨æ€å¥–åŠ±å‡½æ•°åˆ©ç”¨GRPOå¤šè½®æ¬¡è¾“å‡ºè‡ªåŠ¨è°ƒæ•´å¥–åŠ±ä¿¡å·ï¼Œç¨³å®šç­”æ¡ˆç”Ÿæˆä¸éªŒè¯çš„è”åˆè®­ç»ƒï¼Œæå‡éªŒè¯ä»»åŠ¡è¡¨ç°ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨ç†æ—¶æ€§èƒ½ç¼©æ”¾  
æ¨ç†é˜¶æ®µï¼Œç”¨è®­ç»ƒåçš„æ¨¡å‹åŒæ—¶åšç­”æ¡ˆç”Ÿæˆä¸éªŒè¯ï¼ŒåŸºäºéªŒè¯åˆ†æ•°åšåŠ æƒç­”æ¡ˆèšåˆã€‚è¯¥æ–¹å¼å¯åœ¨ç°æœ‰LLMæ¨ç†å¼•æ“ä¸­è½»æ¾éƒ¨ç½²ï¼Œå€ŸåŠ©è‡ªæˆ‘éªŒè¯å®ç°æµ‹è¯•æ—¶æ€§èƒ½æœ‰æ•ˆç¼©æ”¾ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MATH500ã€AIME24ï¼‰ä¸Šï¼ŒåŸºäºQwen2.5 - Math - 7Bå’ŒDeepSeek - R1 - Distill - Qwen - 1.5Bè®­ç»ƒè‡ªéªŒè¯æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºï¼šæ¨¡å‹è®­ç»ƒåæ€§èƒ½æå‡ï¼Œä¸”åœ¨æ¨ç†æ—¶éšç€ç”Ÿæˆæ¬¡æ•°å¢åŠ ï¼Œèƒ½é€šè¿‡è‡ªæˆ‘éªŒè¯å®ç°æœ‰æ•ˆæµ‹è¯•æ—¶ç¼©æ”¾ï¼Œç›¸æ¯”ä¼ ç»Ÿæµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚best - of - Nã€å¸¦å¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„æŸæœç´¢ï¼‰è¡¨ç°æ›´ä¼˜ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªéªŒè¯æ€è·¯ï¼šä¸ºè§£å†³æ¨¡å‹ä¸åŒé˜¶æ®µåˆ†å¸ƒå·®å¼‚é—®é¢˜æä¾›äº†æ–°æ–¹å‘ï¼Œå¯å¯å‘åœ¨å…¶ä»–éœ€è¦éªŒè¯çš„ä»»åŠ¡ï¼ˆå¦‚é€»è¾‘æ¨ç†ã€ä»£ç ç”Ÿæˆï¼‰ä¸­è®¾è®¡ç±»ä¼¼è‡ªéªŒè¯æœºåˆ¶ã€‚  
2. RLç»“åˆç”Ÿæˆä¸éªŒè¯ï¼šå°†å¼ºåŒ–å­¦ä¹ ç”¨äºç»Ÿä¸€ç”Ÿæˆå’ŒéªŒè¯çš„è®­ç»ƒæµç¨‹ï¼Œä¸ºå¤šä»»åŠ¡è”åˆè®­ç»ƒã€æå‡æ¨¡å‹é²æ£’æ€§æä¾›äº†å‚è€ƒèŒƒå¼ã€‚  
3. æ¨ç†æ—¶éƒ¨ç½²ï¼šæ¨ç†æ—¶åŸºäºè‡ªæˆ‘éªŒè¯çš„æ€§èƒ½ç¼©æ”¾æ–¹å¼ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨å¤æ‚ç»„ä»¶ï¼Œå¯¹å·¥ç¨‹åŒ–éƒ¨ç½²å‹å¥½ï¼Œå¯æŒ‡å¯¼å®é™…ç”Ÿäº§ä¸­LLMæ¨ç†æ€§èƒ½ä¼˜åŒ–ã€‚

## dreamprm--domain-reweighted-process-reward-model-for-multimodal-reasoning
### Abstract
Reasoning has improved the performance of large language models (LLMs) on
complicated tasks. Central to the current reasoning studies, Process Reward
Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps
and guide the reasoning process. However, extending PRMs to multimodal large
language models (MLLMs) introduces challenges. Since multimodal reasoning
covers a wider range of tasks compared to text-only scenarios, the resulting
distribution shift from the training to testing sets is more severe, leading to
greater generalization difficulty. Training a reliable multimodal PRM,
therefore, demands large and diverse datasets to ensure sufficient coverage.
However, current multimodal reasoning datasets suffer from quality imbalance,
which degrades PRM performance and highlights the need for data selection
strategy. To address the issues, we introduce DreamPRM, a domain-reweighted
training framework for multimodal PRMs which employs bi-level optimization. In
the lower-level optimization, DreamPRM performs fine-tuning on multiple
datasets with domain weights, allowing the PRM to prioritize high-quality
reasoning signals and alleviating the impact of dataset quality imbalance. In
the upper-level optimization, the PRM is evaluated on a separate meta-learning
dataset; this feedback updates the domain weights through an aggregation loss
function, thereby improving the generalization capability of trained PRM.
Extensive experiments on multiple multimodal reasoning benchmarks covering both
mathematical and general reasoning show that test-time scaling with DreamPRM
consistently improves performance of state-of-the-art MLLMs. Further
comparisons reveal that DreamPRM's domain-reweighting strategy surpasses data
selection methods and yields higher accuracy gains than existing test-time
scaling approaches. Codes are available at
https://github.com/coder-qicao/DreamPRM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DreamPRMï¼šä¸ºå¤šæ¨¡æ€æ¨ç†é‡èº«å®šåˆ¶çš„é¢†åŸŸåŠ æƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ¨ç†èƒ½åŠ›è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºæ¨ç†ç ”ç©¶çš„æ ¸å¿ƒï¼Œèƒ½å¯¹ä¸­é—´æ¨ç†æ­¥éª¤åšç»†ç²’åº¦è¯„ä¼°å¹¶å¼•å¯¼æ¨ç†è¿‡ç¨‹ã€‚ä½†å°†PRMsæ‹“å±•åˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ï¼šå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡èŒƒå›´æ¯”çº¯æ–‡æœ¬æ›´å¹¿ï¼Œè®­ç»ƒåˆ°æµ‹è¯•çš„åˆ†å¸ƒåç§»æ›´ä¸¥é‡ï¼Œæ³›åŒ–éš¾åº¦å¤§ï¼›è®­ç»ƒå¯é å¤šæ¨¡æ€PRMéœ€å¤§é‡å¤šæ ·æ•°æ®é›†ï¼Œå¯ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†å­˜åœ¨è´¨é‡ä¸å‡è¡¡é—®é¢˜ï¼ˆå¦‚å«ä¸å¿…è¦æ¨¡æ€ã€éš¾åº¦è¿‡ä½ç­‰å™ªå£°æ•°æ®ï¼‰ï¼Œä¼šé™ä½PRMæ€§èƒ½ï¼Œå› æ­¤æ€¥éœ€æœ‰æ•ˆæ•°æ®é€‰æ‹©ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDreamPRMæ¡†æ¶  
DreamPRMæ˜¯é¢å‘å¤šæ¨¡æ€PRMçš„é¢†åŸŸåŠ æƒè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨åŒå±‚ä¼˜åŒ–ï¼ˆbi - level optimizationï¼‰ã€‚å—é¢†åŸŸåŠ æƒæŠ€æœ¯å¯å‘ï¼Œå®ƒä¸ºæ¯ä¸ªå¤šæ¨¡æ€æ¨ç†æ•°æ®é›†åŠ¨æ€å­¦ä¹ åˆé€‚æƒé‡ï¼Œè®©ä¸åŒæ•°æ®é›†åœ¨è®­ç»ƒä¸­è´¡çŒ®ä¸åŒã€‚ä½è´¨é‡ã€å«å¤šå™ªå£°æ ·æœ¬çš„æ•°æ®é›†æƒé‡ä½ï¼Œå‡å°‘å¯¹PRMå‚æ•°æ›´æ–°çš„å½±å“ï¼›é«˜è´¨é‡æ•°æ®é›†æƒé‡é«˜ï¼Œåœ¨ä¼˜åŒ–ä¸­èµ·æ›´é‡è¦ä½œç”¨ï¼Œä»¥æ­¤ç¼“è§£æ•°æ®é›†è´¨é‡ä¸å‡è¡¡é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒå±‚ä¼˜åŒ–æœºåˆ¶  
åœ¨ä¸‹å±‚ä¼˜åŒ–ä¸­ï¼ŒPRMå‚æ•°åœ¨å¤šä¸ªè®­ç»ƒé¢†åŸŸä¸‹ï¼Œç»“åˆé¢†åŸŸæƒé‡ä¸è’™ç‰¹å¡æ´›ä¿¡å·è¿›è¡Œå¾®è°ƒï¼›åœ¨ä¸Šå±‚ä¼˜åŒ–ä¸­ï¼Œç”¨å•ç‹¬çš„å…ƒå­¦ä¹ æ•°æ®é›†è¯„ä¼°ä¼˜åŒ–åçš„PRMï¼Œé€šè¿‡èšåˆæŸå¤±å‡½æ•°åé¦ˆæ›´æ–°é¢†åŸŸæƒé‡ï¼Œæå‡è®­ç»ƒåPRMçš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¶µç›–æ•°å­¦å’Œé€šç”¨æ¨ç†çš„å¤šä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»“åˆDreamPRMçš„æµ‹è¯•æ—¶ç¼©æ”¾æŒç»­æå‡äº†æœ€å…ˆè¿›MLLMsçš„æ€§èƒ½ï¼›å¯¹æ¯”æ˜¾ç¤ºï¼ŒDreamPRMçš„é¢†åŸŸåŠ æƒç­–ç•¥è¶…è¶Šå…¶ä»–æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä¸”æ¯”ç°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•å¸¦æ¥æ›´é«˜çš„å‡†ç¡®ç‡æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œé’ˆå¯¹æ•°æ®è´¨é‡ä¸å‡è¡¡é—®é¢˜æå‡ºçš„é¢†åŸŸåŠ æƒ + åŒå±‚ä¼˜åŒ–æ€è·¯ï¼Œä¸ºå¤„ç†å¤šæ¨¡æ€åœºæ™¯ä¸‹æ•°æ®åˆ†å¸ƒä¸è´¨é‡éš¾é¢˜æä¾›äº†æ–°é¢–èŒƒå¼ï¼Œå¯å¯å‘åç»­å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒä¸­æ•°æ®åˆ©ç”¨ä¸ä¼˜åŒ–ç­–ç•¥è®¾è®¡ï¼›ä»åº”ç”¨è§’åº¦ï¼Œå…¶åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­éªŒè¯æœ‰æ•ˆæ€§ï¼Œè¯´æ˜è¯¥æ¡†æ¶å¯¹æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›åˆ‡å®å¯è¡Œï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†é¢†åŸŸæ¨¡å‹ä¼˜åŒ–æä¾›äº†æœ‰ä»·å€¼çš„æŠ€æœ¯è·¯çº¿å‚è€ƒã€‚

## from-mathematical-reasoning-to-code--generalization-of-process-reward-models-in-test-time-scaling
### Abstract
Recent advancements in improving the reasoning capabilities of Large Language
Models have underscored the efficacy of Process Reward Models (PRMs) in
addressing intermediate errors through structured feedback mechanisms. This
study analyzes PRMs from multiple perspectives, including training
methodologies, scalability, and generalization capabilities. We investigate the
interplay between pre-training and reward model training FLOPs to assess their
influence on PRM efficiency and accuracy in complex reasoning tasks. Our
analysis reveals a pattern of diminishing returns in performance with
increasing PRM scale, highlighting the importance of balancing model size and
computational cost. Furthermore, the diversity of training datasets
significantly impacts PRM performance, emphasizing the importance of diverse
data to enhance both accuracy and efficiency. We further examine test-time
scaling strategies, identifying Monte Carlo Tree Search as the most effective
method when computational resources are abundant, while Best-of-N Sampling
serves as a practical alternative under resource-limited conditions. Notably,
our findings indicate that PRMs trained on mathematical datasets exhibit
performance comparable to those tailored for code generation, suggesting robust
cross-domain generalization. Employing a gradient-based metric, we observe that
PRMs exhibit a preference for selecting responses with similar underlying
patterns, further informing their optimization.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»æ•°å­¦æ¨ç†åˆ°ä»£ç ç”Ÿæˆï¼šæµ‹è¯•æ—¶ç¼©æ”¾ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ³›åŒ–æ€§æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œèƒ½é€šè¿‡ç»“æ„åŒ–åé¦ˆæœºåˆ¶è§£å†³æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´é”™è¯¯ã€‚ä½†ç›®å‰å¯¹äºPRMsåœ¨è®­ç»ƒæ–¹æ³•ã€å¯æ‰©å±•æ€§ã€æ³›åŒ–èƒ½åŠ›ç­‰å¤šç»´åº¦çš„åˆ†æä»æœ‰å¾…æ·±å…¥ï¼Œä¸”äººä»¬ä¹Ÿå…³å¿ƒå…¶åœ¨æ•°å­¦æ¨ç†å¤–ï¼ˆå¦‚ä»£ç ç”Ÿæˆé¢†åŸŸï¼‰çš„è¡¨ç°ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»è®­ç»ƒè®¡ç®—é‡ã€æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ã€è·¨é¢†åŸŸæ³›åŒ–ç­‰è§’åº¦å¯¹PRMså±•å¼€ç ”ç©¶ï¼Œä»¥ä¼˜åŒ–PRMè®­ç»ƒå¹¶æ¢ç´¢å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç»´åº¦åˆ†æPRMsæ€§èƒ½å½±å“å› ç´   
ä»è®­ç»ƒæ–¹æ³•è®ºã€å¯æ‰©å±•æ€§ã€æ³›åŒ–èƒ½åŠ›ç­‰è§’åº¦å‰–æPRMsã€‚ç ”ç©¶é¢„è®­ç»ƒä¸å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»¥æ­¤è¯„ä¼°å¯¹PRMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å½±å“ï¼›åŒæ—¶åˆ†æè®­ç»ƒæ•°æ®é›†å¤šæ ·æ€§å¯¹PRMæ€§èƒ½çš„ä½œç”¨ï¼Œä¸ºæ¨¡å‹ä¼˜åŒ–æä¾›ä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¢ç´¢æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥  
è¯„ä¼°å¤šç§æœç´¢ç­–ç•¥ï¼ˆå¦‚Best - of - N Samplingã€Beam Searchã€Monte Carlo Tree Searchï¼ˆMCTSï¼‰ã€Majority Votingç­‰ï¼‰åœ¨æµ‹è¯•æ—¶å¯¹PRMæ¨ç†å‡†ç¡®æ€§çš„ä¼˜åŒ–æ•ˆæœï¼Œæ˜ç¡®ä¸åŒè®¡ç®—èµ„æºæ¡ä»¶ä¸‹æœ€æœ‰æ•ˆçš„ç­–ç•¥ï¼šè®¡ç®—èµ„æºå……è¶³æ—¶MCTSæ•ˆæœæœ€ä½³ï¼Œèµ„æºå—é™ä¸‹Best - of - N Samplingæ˜¯å®ç”¨é€‰æ‹©ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªåŠ¨æ­¥éª¤çº§æ ‡æ³¨ä¸è¿‡æ»¤æœºåˆ¶  
åœ¨PRMè®­ç»ƒä¸­ï¼Œé€šè¿‡æ”¶é›†å¤šæ ·æ¨ç†ä»»åŠ¡ã€åˆ©ç”¨LLMå¯¹æ­¥éª¤è¿›è¡Œæ ‡æ³¨ï¼ˆå€ŸåŠ©è’™ç‰¹å¡æ´›ä¼°è®¡ã€äºŒåˆ†æœç´¢ç­‰å¯å‘å¼æ–¹æ³•ï¼‰ã€åŸºäºé›†æˆçš„è¿‡æ»¤æœºåˆ¶ï¼ˆå¤šLLMäº¤å‰éªŒè¯æ­¥éª¤æ­£ç¡®æ€§ï¼‰è¿™ä¸€ç³»åˆ—æµç¨‹ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œæå‡PRMè®­ç»ƒæ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›æ¢ç©¶  
æ¢ç´¢PRMsä»æ•°å­¦æ¨ç†åˆ°ä»£ç ç”Ÿæˆçš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯åœ¨æ•°å­¦è¯­æ–™ä¸Šè®­ç»ƒçš„PRMsä¸é’ˆå¯¹ä»£ç ç”Ÿæˆä¼˜åŒ–çš„æ¨¡å‹æ€§èƒ½ç›¸å½“ï¼Œå±•ç°å‡ºè‰¯å¥½çš„è·¨é¢†åŸŸé€‚åº”æ€§ï¼›è¿˜é€šè¿‡åŸºäºæ¢¯åº¦çš„åº¦é‡å‘ç°PRMsåå¥½é€‰æ‹©å…·æœ‰ç›¸ä¼¼æ½œåœ¨æ¨¡å¼çš„å“åº”ï¼Œä¸ºæ¨¡å‹ä¼˜åŒ–æä¾›æ–°è§†è§’ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
- æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½å…³ç³»ï¼šPRMè§„æ¨¡å¢å¤§æ—¶æ€§èƒ½å­˜åœ¨æ”¶ç›Šé€’å‡ç°è±¡ï¼Œè¯´æ˜è¦å¹³è¡¡æ¨¡å‹å¤§å°ä¸è®¡ç®—æˆæœ¬ã€‚  
- è®­ç»ƒæ•°æ®é›†å½±å“ï¼šè®­ç»ƒæ•°æ®é›†çš„å¤šæ ·æ€§å¯¹PRMæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œä¸°å¯Œå¤šæ ·çš„æ•°æ®æœ‰åŠ©äºæå‡å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚  
- æµ‹è¯•æ—¶ç­–ç•¥æ•ˆæœï¼šMCTSåœ¨è®¡ç®—èµ„æºå……è¶³æ—¶æ˜¯æå‡æ¨ç†å‡†ç¡®æ€§æœ€æœ‰æ•ˆçš„æµ‹è¯•æ—¶ç­–ç•¥ï¼›èµ„æºæœ‰é™æ—¶ï¼ŒBest - of - N Samplingå› ç®€å•å¿«é€Ÿæˆä¸ºå®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚  
- è·¨é¢†åŸŸè¡¨ç°ï¼šåœ¨æ•°å­¦æ•°æ®é›†ä¸Šè®­ç»ƒçš„PRMsåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¸ä¸“é—¨ä¸ºä»£ç ç”Ÿæˆè®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼Œä½“ç°å‡ºå¼ºå¤§çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- è®­ç»ƒèµ„æºå¹³è¡¡ï¼šåœ¨å¼€å‘ç±»ä¼¼å¥–åŠ±æ¨¡å‹æ—¶ï¼Œéœ€å…³æ³¨æ¨¡å‹å¤§å°ä¸è®¡ç®—èµ„æºçš„å¹³è¡¡ï¼Œé¿å…ç›²ç›®è¿½æ±‚å¤§æ¨¡å‹è€Œå¿½ç•¥æ”¶ç›Šé€’å‡é—®é¢˜ã€‚  
- æ•°æ®å¤šæ ·æ€§é‡è§†ï¼šæ„å»ºè®­ç»ƒæ•°æ®é›†æ—¶è¦æ³¨é‡å¤šæ ·æ€§ï¼Œä»¥æ­¤æå‡æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚  
- æµ‹è¯•ç­–ç•¥é€‰æ‹©ï¼šéƒ¨ç½²æ¨¡å‹æ—¶è¦æ ¹æ®è®¡ç®—èµ„æºæƒ…å†µé€‰æ‹©åˆé€‚çš„æµ‹è¯•æ—¶æœç´¢ç­–ç•¥ï¼Œèµ„æºå……è¶³é€‰MCTSï¼Œèµ„æºæœ‰é™è€ƒè™‘Best - of - N Samplingç­‰ã€‚  
- è·¨é¢†åŸŸè¿ç§»æ€è·¯ï¼šå¯¹äºå…·æœ‰æ­¥éª¤æ€§æ¨ç†ç‰¹å¾çš„ä¸åŒé¢†åŸŸä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œä»£ç ï¼‰ï¼Œå¯å°è¯•åˆ©ç”¨åœ¨æŸä¸€é¢†åŸŸè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹è¿ç§»åˆ°å¦ä¸€é¢†åŸŸï¼Œå€ŸåŠ©å…¶æ³›åŒ–èƒ½åŠ›å‡å°‘é‡å¤è®­ç»ƒæˆæœ¬ã€‚  
- è‡ªåŠ¨æ•°æ®æ„å»ºæµç¨‹ï¼šPRMè®­ç»ƒä¸­è‡ªåŠ¨æ­¥éª¤çº§æ ‡æ³¨ä¸è¿‡æ»¤çš„æµç¨‹å¯å€Ÿé‰´ï¼Œç”¨äºå‡å°‘äººå·¥æ ‡æ³¨æˆæœ¬å¹¶æå‡è®­ç»ƒæ•°æ®è´¨é‡ï¼Œä¸ºå…¶ä»–éœ€è¦æ­¥éª¤çº§åé¦ˆçš„æ¨¡å‹è®­ç»ƒæä¾›å‚è€ƒã€‚

## guided-by-gut--efficient-test-time-scaling-with-reinforced-intrinsic-confidence
### Abstract
Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)
reasoning often incur substantial computational costs, primarily due to
extensive reliance on external Process Reward Models (PRMs) or sampling methods
like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient
self-guided TTS framework that achieves PRM-level performance without costly
external verifier models. Our method employs a lightweight tree search guided
solely by intrinsic LLM signals, token-level confidence and step novelty. One
critical innovation is improving the reliability of internal confidence
estimates via a targeted reinforcement learning fine-tuning phase. Empirical
evaluations on challenging mathematical reasoning benchmarks demonstrate that
GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching
or surpassing significantly larger models (e.g., 32B-70B parameters), while
reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG
achieves comparable accuracy with 8x faster inference speeds and 4-5x lower
memory usage. Additionally, GG reduces KV cache memory usage by approximately
50% compared to the BoN strategy, facilitating more efficient and practical
deployment of TTS techniques.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨â€œç›´è§‰â€å¼•å¯¼ï¼šé«˜æ•ˆæµ‹è¯•æ—¶ç¼©æ”¾çš„å¼ºåŒ–å†…åœ¨ç½®ä¿¡åº¦æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ–¹æ³•ï¼Œå¸¸å› è¿‡åº¦ä¾èµ–å¤–éƒ¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æˆ–å¦‚Best - of - Nï¼ˆBoNï¼‰è¿™ç±»é‡‡æ ·æ–¹æ³•ï¼Œè€Œäº§ç”Ÿé«˜æ˜‚è®¡ç®—æˆæœ¬ã€‚ç°æœ‰TTSæ–¹æ³•åœ¨å¢å¼ºå°æ¨¡å‹æ¨ç†èƒ½åŠ›æ—¶ï¼Œè¦ä¹ˆåƒBoNé‚£æ ·ç”Ÿæˆå¤§é‡å€™é€‰è§£å¯¼è‡´æ¨ç†æˆæœ¬è¿‡é«˜ï¼Œè¦ä¹ˆåƒåŸºäºPRMçš„æ–¹æ³•å­˜åœ¨è®­ç»ƒéƒ¨ç½²æ˜‚è´µå’Œæ³›åŒ–æ€§é—®é¢˜ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†TTSåœ¨å°æ¨¡å‹ä¸Šçš„å®é™…åº”ç”¨ï¼Œå› æ­¤éœ€è¦æ›´å…·æˆæœ¬æ•ˆç›Šçš„TTSæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨Tokenç½®ä¿¡åº¦ä¸æ–°é¢–æ€§ä¿¡å·
ä¸å†ä¾èµ–æ˜‚è´µçš„å¤–éƒ¨éªŒè¯æ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºçš„å†…åœ¨çº¿ç´¢ï¼Œå¦‚tokenæ¦‚ç‡ï¼Œå°†å…¶è§£é‡Šä¸ºç½®ä¿¡åº¦åˆ†æ•°å¹¶è¡¡é‡æ¨ç†æ­¥éª¤çš„æ–°é¢–æ€§ã€‚è¿™ä¸ºæ¨ç†æ—¶çš„æœç´¢æä¾›äº†è½»é‡çš„æŒ‡å¯¼é€”å¾„ï¼Œä¸”èƒ½æ•´åˆåˆ°ç°æœ‰æ¨¡å‹å’Œç®—æ³•ä¸­ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¢å¼ºç½®ä¿¡åº¦å¯é æ€§
å°†åŸºäºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ èå…¥æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ï¼Œä¸“é—¨ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹å†…éƒ¨ç½®ä¿¡åº¦ä¼°è®¡çš„å¯é æ€§ï¼Œä¸ºæµ‹è¯•æ—¶çš„æœç´¢ç­–ç•¥æä¾›æ›´å¯é çš„æŒ‡å¯¼ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªå¼•å¯¼çš„é«˜æ•ˆæµ‹è¯•æ—¶æœç´¢
å¼•å…¥åŸºäºDiverse Verifier Tree Searchï¼ˆDVTSï¼‰çš„æ ‘æœç´¢ç®—æ³•ï¼Œç”±å¤§è¯­è¨€æ¨¡å‹çš„å†…åœ¨ä¿¡å·ï¼ˆtokenæ¦‚ç‡/ç½®ä¿¡åº¦ã€æ–°é¢–æ€§ï¼‰å¼•å¯¼ã€‚è¯¥ç®—æ³•åœ¨æ¨ç†æ—¶é’ˆå¯¹æœ€å°è®¡ç®—æˆæœ¬è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ï¼Œå®ç°é«˜æ•ˆçš„TTSã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†ï¼ˆå¦‚AIME24/25ã€MATH500ã€AMCç­‰ï¼‰ä¸Šè¯„ä¼°è¡¨æ˜ï¼š
1. ä½¿å°æ¨¡å‹ï¼ˆå¦‚1.5Bå‚æ•°æ¨¡å‹ï¼‰åœ¨å‡†ç¡®ç‡ä¸ŠåŒ¹é…ç”šè‡³è¶…è¿‡å¤§æ¨¡å‹ï¼ˆå¦‚32B - 70Bå‚æ•°æ¨¡å‹ï¼‰ï¼ŒåŒæ—¶å°†GPUå†…å­˜ä½¿ç”¨é‡é™ä½å¤šè¾¾10å€ï¼›
2. ä¸åŸºäºPRMçš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å‡†ç¡®ç‡ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œæ¨ç†é€Ÿåº¦å¿«8å€ï¼Œå†…å­˜ä½¿ç”¨é‡é™ä½4 - 5å€ï¼›
3. ä¸BoNç­–ç•¥ç›¸æ¯”ï¼ŒKVç¼“å­˜å†…å­˜ä½¿ç”¨é‡é™ä½çº¦50%ï¼Œæœ‰åŠ©äºTTSæŠ€æœ¯æ›´é«˜æ•ˆå®ç”¨çš„éƒ¨ç½²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å†…åœ¨ä¿¡å·åˆ©ç”¨ï¼šå±•ç¤ºäº†ä»æ¨¡å‹è‡ªèº«ç”Ÿæˆè¿‡ç¨‹ä¸­æŒ–æ˜æœ‰ç”¨ä¿¡å·ï¼ˆå¦‚tokenæ¦‚ç‡ï¼‰æ¥æŒ‡å¯¼æ¨ç†ï¼Œä¸ºè½»é‡å‹æ¨ç†å¼•å¯¼æä¾›æ€è·¯ï¼Œå¯å¯å‘åç»­ç ”ç©¶åœ¨ä¸ä¾èµ–å¤–éƒ¨å¤æ‚ç»„ä»¶ä¸‹æå‡æ¨¡å‹èƒ½åŠ›ï¼›
2. å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¼˜åŒ–ç½®ä¿¡åº¦ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ é’ˆå¯¹æ€§ä¼˜åŒ–æ¨¡å‹å†…éƒ¨ç½®ä¿¡åº¦ä¼°è®¡ï¼Œè¿™ç§å¯¹æ¨¡å‹å†…åœ¨â€œåˆ¤æ–­â€èƒ½åŠ›ä¼˜åŒ–çš„æ€è·¯ï¼Œå¯ç”¨äºå…¶ä»–éœ€è¦æ¨¡å‹è‡ªæˆ‘è¯„ä¼°æŒ‡å¯¼çš„ä»»åŠ¡åœºæ™¯ï¼›
3. é«˜æ•ˆæœç´¢ç®—æ³•è®¾è®¡ï¼šåŸºäºå†…åœ¨ä¿¡å·è®¾è®¡è½»é‡ä¸”é«˜æ•ˆçš„æ ‘æœç´¢ç®—æ³•ï¼Œä¸ºåœ¨æ¨ç†é˜¶æ®µé«˜æ•ˆåˆ†é…è®¡ç®—èµ„æºã€æå‡æ¨ç†æ€§èƒ½æä¾›äº†å¯å‚è€ƒçš„ç®—æ³•è®¾è®¡èŒƒå¼ï¼Œå¯¹èµ„æºå—é™åœºæ™¯ä¸‹çš„æ¨¡å‹æ¨ç†ä¼˜åŒ–å¾ˆæœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## value-guided-search-for-efficient-chain-of-thought-reasoning
### Abstract
In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of "step,"
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­é«˜æ•ˆçš„ä»·å€¼å¼•å¯¼æœç´¢ï¼šè®©æ¨ç†æ›´èªæ˜ã€æ›´é«˜æ•ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåƒOpenAI o1 & o3ã€DeepSeek R1è¿™ç±»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå‰è¿›è¡Œå¤šæ­¥æ¨ç†ä¸è‡ªæˆ‘ä¿®æ­£ï¼Œåœ¨ç«èµ›æ•°å­¦ã€ç¼–ç ç­‰é¢†åŸŸè¡¨ç°å“è¶Šã€‚ä½†è¿™ç§èƒ½åŠ›éœ€ä»˜å‡ºä»£ä»·ï¼šæ¯è½®ç”Ÿæˆæ¶‰åŠé•¿é“¾æ€è€ƒï¼ˆCoTï¼‰ï¼Œæ¨ç†è®¡ç®—é‡å‰§å¢ï¼Œä¸”æ€è€ƒè¿‡ç¨‹æ˜“é‡å¤ã€é™·å…¥æ— æ•ˆå¾ªç¯ã€‚åŒæ—¶ï¼Œç°æœ‰ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰å¼•å¯¼æœç´¢çš„æ–¹æ³•ï¼Œå› éœ€é¢„å®šä¹‰â€œæ­¥éª¤â€ï¼ˆé•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­éš¾å®šä¹‰ï¼‰ã€æ ‡æ³¨æˆæœ¬é«˜ï¼Œéš¾ä»¥æ‰©å±•åˆ°é•¿ä¸Šä¸‹æ–‡æ¨ç†æ¨¡å‹ã€‚äºæ˜¯ï¼Œæœ¬æ–‡æƒ³è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼šèƒ½å¦ç”¨æ›´å°‘æ¨ç†è®¡ç®—è·å–åŒç­‰æ€§èƒ½ï¼Ÿèƒ½å¦ç”¨é«˜æ•ˆæœç´¢æ–¹æ³•æå‡æ¨¡å‹æ€§èƒ½ä¸Šé™ï¼Ÿ

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºä»·å€¼å¼•å¯¼æœç´¢ï¼ˆVGSï¼‰ä¸æ— â€œæ­¥éª¤â€ä¾èµ–çš„ä»·å€¼æ¨¡å‹è®­ç»ƒæ³•  
ç°æœ‰PRMséœ€ç²¾ç»†â€œæ­¥éª¤â€å®šä¹‰ï¼Œæœ¬æ–‡æå‡ºçš„ä»·å€¼æ¨¡å‹è®­ç»ƒæ— éœ€æ­¤ã€‚æ„å»ºæ•°æ®ç®¡é“ï¼šä»å„ç±»æ¨¡å‹æ”¶é›†è§£å‰ç¼€ï¼Œç”¨è½»é‡æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek - R1 - Distill - 1.5Bï¼‰ä»éšæœºå‰ç¼€ç”Ÿæˆå®Œæ•´è§£ï¼Œæ”¶é›†250ä¸‡æ¡æ•°å­¦æ¨ç†è½¨è¿¹ï¼ˆè¶…300äº¿tokenï¼‰æ•°æ®é›†ï¼Œè®­ç»ƒ15äº¿å‚æ•°çš„tokençº§ä»·å€¼æ¨¡å‹DeepSeek - VM - 1.5Bï¼Œé€šè¿‡åˆ†ç±»å›å½’å®Œæ•´è§£æœ€ç»ˆå¥–åŠ±æ¥è®­ç»ƒï¼Œé¿å¼€â€œæ­¥éª¤â€å®šä¹‰éš¾é¢˜ï¼Œæ•°æ®æ”¶é›†ä¹Ÿæ¯”ç°æœ‰æŠ€æœ¯é«˜æ•ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå—çº§ä»·å€¼å¼•å¯¼æœç´¢ï¼ˆVGSï¼‰æå‡æ¨ç†æ•ˆç‡ä¸æ€§èƒ½  
å°†è®­ç»ƒå¥½çš„ä»·å€¼æ¨¡å‹ç”¨äºDeepSeekæ¨¡å‹çš„å—çº§æœç´¢ã€‚åœ¨ç«èµ›æ•°å­¦ä»»åŠ¡ï¼ˆå¦‚AIME 2024 & 2025ã€HMMT Feb 2024 & 2025ï¼‰ä¸Šè¯„ä¼°ï¼Œå—çº§VGSç»“åˆåŠ æƒå¤šæ•°æŠ•ç¥¨ï¼Œæ¯”å¤šæ•°æŠ•ç¥¨ã€best - of - nç­‰æ ‡å‡†æ–¹æ³•åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—æ‰©å±•ï¼ˆTTCï¼‰è¡¨ç°æ›´å¥½ï¼Œæ—¢æå‡æ¨ç†æ¨¡å‹æ€§èƒ½ä¸Šé™ï¼Œåˆå‡å°‘è¾¾åˆ°æ ‡å‡†TTCæ–¹æ³•æ€§èƒ½æ‰€éœ€çš„æ¨ç†è®¡ç®—é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- æ€§èƒ½æå‡ï¼šåœ¨64æ¬¡ç”Ÿæˆçš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒDeepSeek - R1 - Distill - 1.5Bç»“åˆVGSåœ¨å››ä¸ªç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å¹³å‡å‡†ç¡®ç‡è¾¾45.7%ï¼Œä¸o3 - mini - mediumæŒå¹³ï¼›ä¸åŒè§„æ¨¡DeepSeek - R1 - Distillæ¨¡å‹ç»“åˆVGSåï¼Œåœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ•´ä½“å“åº”è´¨é‡æå‡ï¼ˆå¦‚å·¦å›¾æ‰€ç¤ºï¼‰ã€‚
- æ•ˆç‡æå‡ï¼šVGSæ¯”å¤šæ•°æŠ•ç¥¨ç­‰æ ‡å‡†æ–¹æ³•ï¼Œèƒ½æ˜¾è‘—å‡å°‘è¾¾åˆ°ç›¸åŒå‡†ç¡®ç‡æ‰€éœ€çš„æ¨ç†FLOPsï¼ˆå¦‚å³å›¾æ‰€ç¤ºï¼‰ï¼Œè¯æ˜ä»·å€¼å¼•å¯¼å¯¹æå‡æ•ˆç‡å¾ˆæœ‰å‰æ™¯ã€‚ä¸”VGSæ¯”ç”¨ç°æœ‰PRMsæœç´¢è¡¨ç°æ›´å¥½ï¼Œè¯´æ˜ä»·å€¼æ¨¡å‹èƒ½æä¾›æ›´ä¼˜åé¦ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æŠ€æœ¯å±‚é¢ï¼šæ— â€œæ­¥éª¤â€ä¾èµ–çš„ä»·å€¼æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡æ¨ç†æ¨¡å‹çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæä¾›æ–°æ€è·¯ï¼Œé¿å¼€â€œæ­¥éª¤â€å®šä¹‰ä¸æ ‡æ³¨éš¾é¢˜ï¼›å—çº§æœç´¢ç»“åˆä»·å€¼æ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨æå‡æ€§èƒ½ä¸é™ä½æ¨ç†æˆæœ¬ä¸Šçš„å®è·µï¼Œå¯è¿ç§»åˆ°å…¶ä»–æœ‰è‡ªåŠ¨åŒ–ç»“æœç›‘ç£çš„ä»»åŠ¡ï¼ˆå¦‚ç¼–ç ã€ç§‘å­¦ç ”ç©¶æ¨ç†ç­‰ï¼‰ã€‚
- å¼€æºå±‚é¢ï¼šå…¬å¼€250ä¸‡æ¨ç†è½¨è¿¹æ•°æ®é›†ã€ä»·å€¼æ¨¡å‹ä¸ä»£ç åº“ï¼Œä¸ºåç»­åœ¨å…¶ä»–é¢†åŸŸåº”ç”¨VGSæä¾›äº†æ•°æ®ã€æ¨¡å‹ä¸ä»£ç åŸºç¡€ï¼Œåˆ©äºæ¨åŠ¨ç›¸å…³ç ”ç©¶å‘å±•ã€‚

## j1--exploring-simple-test-time-scaling-for-llm-as-a-judge
### Abstract
The current focus of AI research is shifting from emphasizing model training
towards enhancing evaluation quality, a transition that is crucial for driving
further advancements in AI systems. Traditional evaluation methods typically
rely on reward models assigning scalar preference scores to outputs. Although
effective, such approaches lack interpretability, leaving users often uncertain
about why a reward model rates a particular response as high or low. The advent
of LLM-as-a-Judge provides a more scalable and interpretable method of
supervision, offering insights into the decision-making process. Moreover, with
the emergence of large reasoning models, which consume more tokens for deeper
thinking and answer refinement, scaling test-time computation in the
LLM-as-a-Judge paradigm presents an avenue for further boosting performance and
providing more interpretability through reasoning traces. In this paper, we
introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on
reflection-enhanced datasets collected via rejection-sampling and subsequently
trained using Reinforcement Learning (RL) with verifiable rewards. At inference
time, we apply Simple Test-Time Scaling (STTS) strategies for additional
performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$
surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and
exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally,
we present three key findings: (1) Existing LLM-as-a-Judge does not inherently
exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced
datasets continues to demonstrate similarly weak scaling behavior. (3)
Significant scaling trend emerges primarily during the RL phase, suggesting
that effective STTS capability is acquired predominantly through RL training.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLM-as-a-Judgeæ–°çªç ´ï¼šJ1-7Bä¸æµ‹è¯•æ—¶ç®€å•ç¼©æ”¾ç­–ç•¥çš„æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
AIç ”ç©¶é‡å¿ƒæ­£ä»æ¨¡å‹è®­ç»ƒè½¬å‘æå‡è¯„ä¼°è´¨é‡ï¼Œä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ï¼ˆå¦‚æ ‡é‡å¥–åŠ±æ¨¡å‹ï¼‰è™½æœ‰æ•ˆä½†ç¼ºä¹å¯è§£é‡Šæ€§ã€‚LLM - as - a - JudgeèŒƒå¼æä¾›äº†æ›´å…·æ‰©å±•æ€§ä¸å¯è§£é‡Šæ€§çš„ç›‘ç£æ–¹å¼ï¼Œç„¶è€Œç°æœ‰LLM - as - a - Judgeåœ¨å¤æ‚æ¨ç†è¯„ä¼°ä»»åŠ¡ä¸­å­˜åœ¨ä¸è¶³ï¼Œä¸”æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯åœ¨è¯„ä¼°åœºæ™¯çš„æœ‰æ•ˆæ€§å°šæœªå……åˆ†æ¢ç´¢ã€‚åŒæ—¶ï¼Œå¤§æ¨ç†æ¨¡å‹å‡ºç°åï¼Œæµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾ä¸ºæå‡LLM - as - a - Judgeæ€§èƒ½å’Œå¯è§£é‡Šæ€§æä¾›äº†é€”å¾„ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç©¶æµ‹è¯•æ—¶ç®€å•ç¼©æ”¾ï¼ˆSTTSï¼‰å¯¹LLM - as - a - Judgeçš„ä½œç”¨ï¼Œä»¥æå‡å…¶è´¨é‡ä¸å¯é æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼æ‰“é€ J1 - 7B
é¦–å…ˆé€šè¿‡æ‹’ç»é‡‡æ ·æ”¶é›†åå°„å¢å¼ºæ•°æ®é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¯¥æ•°æ®é›†æ˜¾å¼åŠ å…¥STTS tokensï¼Œä¸ºæ¨¡å‹å†·å¯åŠ¨åˆå§‹åŒ–ï¼Œæ•™æ¨¡å‹æœ€ä¼˜åˆ©ç”¨åå°„æ¨ç†tokensï¼›éšåä½¿ç”¨å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œè®©æ¨¡å‹è‡ªä¸»ä¼˜åŒ–åå°„èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåº”ç”¨ç®€å•æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSTTSï¼‰ç­–ç•¥
æ¨ç†æ—¶ï¼Œé€šè¿‡å¤šæ¬¡é™„åŠ åƒâ€œWait,â€è¿™æ ·çš„æ€è€ƒtokensï¼Œè¿«ä½¿æ¨¡å‹åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆå‰æ›´æ·±å…¥æ€è€ƒï¼Œä»¥æ­¤æå‡æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
J1 - 7Bè¶…è¶Šäº†ä¹‹å‰æœ€å…ˆè¿›çš„LLM - as - a - Judgeæ¨¡å‹ï¼Œåœ¨æ•´ä½“åˆ¤æ–­æ€§èƒ½ä¸Šæå‡äº†4.8%ï¼›åœ¨STTSä¸‹å±•ç°å‡ºæ›´å¼ºçš„ç¼©æ”¾è¶‹åŠ¿ï¼Œç›¸æ¯”ä¹‹å‰æå‡äº†5.1%ã€‚åŒæ—¶æœ‰ä¸‰ä¸ªå…³é”®å‘ç°ï¼šç°æœ‰LLM - as - a - Judgeæœ¬èº«æ²¡æœ‰è¿™ç§ç¼©æ”¾è¶‹åŠ¿ï¼›ä»…åœ¨åå°„å¢å¼ºæ•°æ®é›†ä¸Šå¾®è°ƒçš„æ¨¡å‹ç¼©æ”¾è¡Œä¸ºä»å¼±ï¼›æ˜¾è‘—ç¼©æ”¾è¶‹åŠ¿ä¸»è¦åœ¨RLé˜¶æ®µå‡ºç°ï¼Œå³æœ‰æ•ˆSTTSèƒ½åŠ›ä¸»è¦é€šè¿‡RLè®­ç»ƒè·å¾—ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢ï¼Œä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ä¸ºæ‰“é€ æ›´ä¼˜çš„LLM - as - a - Judgeæ¨¡å‹æä¾›äº†æ€è·¯ï¼Œå…ˆåˆ©ç”¨ç‰¹å®šæ•°æ®é›†åšSFTåˆå§‹åŒ–ï¼Œå†ç”¨RLä¼˜åŒ–èƒ½åŠ›ï¼›ä»ç ”ç©¶è§†è§’ï¼Œå¯¹LLM - as - a - Judgeåœ¨æµ‹è¯•æ—¶ç¼©æ”¾æ–¹é¢çš„åˆ†æï¼Œä¸ºè¯¥é¢†åŸŸåç»­ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ï¼Œå¦‚å…³æ³¨ä¸åŒè®­ç»ƒé˜¶æ®µå¯¹æ¨¡å‹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚STTSä¸‹çš„ç¼©æ”¾èƒ½åŠ›ï¼‰çš„å½±å“ï¼›ä»åº”ç”¨è§’åº¦ï¼ŒJ1 - 7Bçš„æˆåŠŸè¡¨æ˜é€šè¿‡åˆç†è®­ç»ƒå’Œæµ‹è¯•æ—¶ç­–ç•¥è°ƒæ•´ï¼Œèƒ½æå‡LLM - as - a - Judgeçš„æ€§èƒ½ä¸å¯è§£é‡Šæ€§ï¼Œå¯¹æ„å»ºæ›´é²æ£’ã€å¯æ‰©å±•çš„AIè¯„ä¼°ç³»ç»Ÿæœ‰å‚è€ƒä»·å€¼ã€‚

## sailing-by-the-stars--a-survey-on-reward-models-and-learning-strategies-for-learning-from-rewards
### Abstract
Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (RLHF, RLAIF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities for diverse
tasks. In this survey, we present a comprehensive overview of learning from
rewards, from the perspective of reward models and learning strategies across
training, inference, and post-inference stages. We further discuss the
benchmarks for reward models and the primary applications. Finally we highlight
the challenges and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹â€œå¾ªå¥–è€Œå­¦â€ï¼šå¥–åŠ±æ¨¡å‹ä¸å­¦ä¹ ç­–ç•¥å…¨æ™¯æ¢ç§˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‘å±•é‡å¿ƒä»é¢„è®­ç»ƒè§„æ¨¡æ‰©å¼ è½¬å‘è®­ç»ƒåä¸æ¨ç†æ—¶çš„èƒ½åŠ›å¢å¼ºã€‚ä½†é¢„è®­ç»ƒå­˜åœ¨ä¸äººç±»ä»·å€¼è§‚é”™ä½ã€é€‚é…ä»»åŠ¡ç›®æ ‡éš¾ã€æ·±åº¦æ¨ç†ä¸è¶³ç­‰å±€é™ï¼Œéš¾ä»¥æ”¯æ’‘é€šç”¨æ™ºèƒ½é•¿æœŸç›®æ ‡ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œâ€œä»å¥–åŠ±ä¸­å­¦ä¹ ï¼ˆLearning from Rewardsï¼‰â€èŒƒå¼å´›èµ·ï¼Œå®ƒä»¥å¥–åŠ±ä¿¡å·ä¸ºæŒ‡å¼•ï¼Œè®©æ¨¡å‹ä»é™æ€æ•°æ®è¢«åŠ¨å­¦ä¹ è½¬å‘åŠ¨æ€åé¦ˆä¸»åŠ¨å­¦ä¹ ï¼Œæ”¯æ’‘äº†RLHFã€DPOç­‰ä¸»æµæŠ€æœ¯ï¼Œèµ‹èƒ½æ¨¡å‹åå¥½å¯¹é½ä¸å¤æ‚ä»»åŠ¡æ¨ç†ã€‚æœ¬æ–‡æ—¨åœ¨å…¨é¢æ¢³ç†è¯¥èŒƒå¼ä¸‹å¥–åŠ±æ¨¡å‹ä¸å­¦ä¹ ç­–ç•¥çš„å‰æ²¿è¿›å±•ï¼Œä¸ºé¢†åŸŸç ”ç©¶æä¾›å…¨æ™¯å‚è€ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºç»Ÿä¸€æ¦‚å¿µæ¡†æ¶  
æå‡ºæ¶µç›–è¯­è¨€æ¨¡å‹ã€å¥–åŠ±æ¨¡å‹ã€å­¦ä¹ ç­–ç•¥ä¸‰è¦ç´ çš„ç»Ÿä¸€æ¡†æ¶ï¼ˆå¦‚å›¾2ï¼‰ã€‚è¯­è¨€æ¨¡å‹ç”Ÿæˆè¾“å‡ºï¼Œå¥–åŠ±æ¨¡å‹åŸºäºè¾“å…¥è¯„ä¼°è¾“å‡ºè´¨é‡å¹¶ç»™å‡ºå¥–åŠ±ï¼Œå­¦ä¹ ç­–ç•¥åˆ©ç”¨å¥–åŠ±æ›´æ–°æ¨¡å‹æˆ–ä¼˜åŒ–è¾“å‡ºï¼Œæ¸…æ™°æ‹†è§£â€œä»å¥–åŠ±ä¸­å­¦ä¹ â€çš„ç³»ç»Ÿé€»è¾‘ï¼Œä¸ºåç»­åˆ†ç±»åˆ†æå¥ åŸºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç»´ taxonomy åˆ†ç±»ä½“ç³»  
ä»**å¥–åŠ±æ¥æº**ï¼ˆäººç±»åé¦ˆ/è‡ªåŠ¨åŒ–åé¦ˆï¼‰ã€**å¥–åŠ±æ¨¡å‹**ï¼ˆæ¶æ„ã€æ ¼å¼ã€è¯„åˆ†æ¨¡å¼ã€ç²’åº¦ç­‰ç»´åº¦ï¼Œå¦‚å›¾3ï¼‰ã€**å­¦ä¹ é˜¶æ®µ**ï¼ˆè®­ç»ƒæ—¶/æ¨ç†æ—¶/æ¨ç†åï¼‰ã€**å­¦ä¹ ç­–ç•¥**ï¼ˆåŸºäºè®­ç»ƒè°ƒå‚/æ— è®­ç»ƒç›´æ¥ä¼˜åŒ–è¾“å‡ºï¼‰å››å¤§ç»´åº¦ï¼Œç³»ç»Ÿæ¢³ç†ç°æœ‰æ–¹æ³•å·®å¼‚ï¼Œè®©ç¹æ‚æŠ€æœ¯æœ‰äº†æ¸…æ™°å½’ç±»é€»è¾‘ï¼Œä¾¿äºç ”ç©¶è€…å¿«é€Ÿå®šä½æ–¹å‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå…¨é˜¶æ®µæŠ€æœ¯è¦†ç›–  
å¯¹è®­ç»ƒï¼ˆå¦‚RLHF/RLAIF/DPOï¼‰ã€æ¨ç†ï¼ˆå¦‚å¥–åŠ±å¼•å¯¼è§£ç ã€Generate - then - Rankï¼‰ã€æ¨ç†åï¼ˆå¦‚äº‹åä¿®æ­£ï¼‰ä¸‰é˜¶æ®µâ€œä»å¥–åŠ±å­¦ä¹ â€æŠ€æœ¯é€ä¸€å‰–æï¼Œå±•ç°ä¸åŒé˜¶æ®µå¦‚ä½•ç”¨å¥–åŠ±èµ‹èƒ½æ¨¡å‹ï¼Œæ¯”å¦‚è®­ç»ƒé˜¶æ®µå€Ÿäººç±»/AIåé¦ˆå¯¹é½åå¥½ï¼Œæ¨ç†é˜¶æ®µç”¨å¥–åŠ±å¼•å¯¼ç”Ÿæˆæ›´ä¼˜ç»“æœï¼Œæ¨ç†åç”¨å¥–åŠ±ä¿®æ­£è¾“å‡ºç¼ºé™·ï¼Œå½¢æˆæŠ€æœ¯é“¾è·¯é—­ç¯ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªèšç„¦ä¼ ç»Ÿâ€œå®éªŒæŒ‡æ ‡ - æ•°å€¼å¯¹æ¯”â€å¼å®éªŒï¼Œè€Œæ˜¯é€šè¿‡å¯¹å¤§é‡å‰æ²¿æŠ€æœ¯ï¼ˆå¦‚RLHFã€DPOã€GRPOç­‰ï¼‰çš„æ¢³ç†ï¼Œå±•ç°â€œä»å¥–åŠ±ä¸­å­¦ä¹ â€èŒƒå¼åœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šæ¨¡æ€ã€æ™ºèƒ½ä½“ç­‰åœºæ™¯çš„å¹¿æ³›åº”ç”¨ä»·å€¼ï¼Œä¾§é¢éªŒè¯å„æŠ€æœ¯åˆ†æ”¯åœ¨å®é™…ä»»åŠ¡ä¸­æ¨åŠ¨æ¨¡å‹èƒ½åŠ›å‡çº§çš„æ•ˆæœï¼ŒåŒæ—¶æ€»ç»“å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•è¿›å±•ï¼Œä¸ºè¯¥é¢†åŸŸæŠ€æœ¯æœ‰æ•ˆæ€§è¯„ä¼°æä¾›å‚è€ƒæ–¹å‘ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶ä¸åˆ†ç±»æ€ç»´ï¼šå…¶æ„å»ºçš„ç»Ÿä¸€æ¦‚å¿µæ¡†æ¶å’Œå¤šç»´åˆ†ç±»ä½“ç³»ï¼Œä¸ºé¢†åŸŸå†…æ¢³ç†æŠ€æœ¯ã€å¼€å±•è°ƒç ”ç±»å·¥ä½œæä¾›äº†æ¨¡æ¿ï¼Œå¸®åŠ©ç ”ç©¶è€…å¿«é€Ÿé”šå®šæŠ€æœ¯å®šä½ä¸ç©ºç™½ç‚¹ï¼›  
2. å…¨æµç¨‹è§†è§’ï¼šä»è®­ç»ƒåˆ°æ¨ç†å†åˆ°æ¨ç†åå…¨é˜¶æ®µè¦†ç›–çš„åˆ†ææ–¹å¼ï¼Œå¯å‘ä»ä¸šè€…æ€è€ƒä¸åŒç¯èŠ‚å¦‚ä½•ååŒç”¨å¥–åŠ±å¢å¼ºæ¨¡å‹ï¼ŒåŠ©åŠ›æ‰“é€ æ›´æ™ºèƒ½çš„å¤§æ¨¡å‹åº”ç”¨ï¼›  
3. æŠ€æœ¯å…¨æ™¯æ¢³ç†ï¼šå¯¹RLHFã€DPOç­‰ä¸»æµæŠ€æœ¯åŠè‡ªåŠ¨åŒ–åé¦ˆï¼ˆè‡ªå¥–åŠ±ã€é¢„å®šä¹‰è§„åˆ™ç­‰ï¼‰çš„è¯¦ç»†ç›˜ç‚¹ï¼Œä¸ºå·¥ç¨‹å®è·µä¸­é€‰æ‹©æŠ€æœ¯è·¯çº¿ã€è®¾è®¡å¥–åŠ±æœºåˆ¶æä¾›ä¸°å¯Œå‚è€ƒæ¡ˆä¾‹ï¼Œé™ä½æŠ€æœ¯é€‰å‹ä¸åˆ›æ–°è¯•é”™æˆæœ¬ã€‚  

## a-survey-of-slow-thinking-based-reasoning-llms-using-reinforced-learning-and-inference-time-scaling-law
### Abstract
This survey explores recent advancements in reasoning large language models
(LLMs) designed to mimic "slow thinking" - a reasoning process inspired by
human cognition, as described in Kahneman's Thinking, Fast and Slow. These
models, like OpenAI's o1, focus on scaling computational resources dynamically
during complex tasks, such as math reasoning, visual reasoning, medical
diagnosis, and multi-agent debates. We present the development of reasoning
LLMs and list their key technologies. By synthesizing over 100 studies, it
charts a path toward LLMs that combine human-like deep thinking with scalable
efficiency for reasoning. The review breaks down methods into three categories:
(1) test-time scaling dynamically adjusts computation based on task complexity
via search and sampling, dynamic verification; (2) reinforced learning refines
decision-making through iterative improvement leveraging policy networks,
reward models, and self-evolution strategies; and (3) slow-thinking frameworks
(e.g., long CoT, hierarchical processes) that structure problem-solving with
manageable steps. The survey highlights the challenges and further directions
of this domain. Understanding and advancing the reasoning abilities of LLMs is
crucial for unlocking their full potential in real-world applications, from
scientific discovery to decision support systems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜ç±»â€œæ…¢æ€è€ƒâ€æ¨ç†å¤§æ¨¡å‹ï¼šå¼ºåŒ–å­¦ä¹ ä¸æ¨ç†æ—¶ç¼©æ”¾å¾‹è§†è§’ä¸‹çš„å‰æ²¿è¿›å±•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPT - 4ã€Deepseekç­‰è™½åœ¨è‡ªç„¶è¯­è¨€ç†è§£ã€ä»£ç ç”Ÿæˆç­‰é¢†åŸŸå–å¾—çªç ´ï¼Œä½†åœ¨éœ€è¦æ·±åº¦ã€å®¡æ…æ¨ç†çš„ä»»åŠ¡ï¼ˆç±»ä¼¼äººç±»â€œç³»ç»Ÿ2â€è®¤çŸ¥ï¼‰ä¸Šè¡¨ç°ä¸è¶³ã€‚äººç±»è®¤çŸ¥ä¸­â€œæ…¢æ€è€ƒâ€å¼ºè°ƒé€æ­¥ã€è°¨æ…æ¨ç†ï¼Œä¸ºè§£å†³LLMsæ¨ç†çŸ­æ¿ï¼Œç ”ç©¶è€…è½¬å‘â€œæ…¢æ€è€ƒâ€èŒƒå¼ï¼Œç»“åˆæ¨ç†æ—¶åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºä¸å¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯æå‡æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡æ—¨åœ¨ç»¼è¿°è¯¥é¢†åŸŸè¿›å±•ï¼Œä¸ºåç»­ç ”ç©¶æŒ‡è·¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸‰ç»´åº¦åˆ†ç±»ç»¼è¿°æ–¹æ³•
å°†ç±»â€œæ…¢æ€è€ƒâ€æ¨ç†LLMsçš„æŠ€æœ¯æ–¹æ³•åˆ†ä¸ºä¸‰ç±»ã€‚å…¶ä¸€ä¸ºæµ‹è¯•æ—¶ç¼©æ”¾ï¼Œä¾æ®ä»»åŠ¡å¤æ‚åº¦ï¼Œé€šè¿‡æœç´¢ã€é‡‡æ ·ã€åŠ¨æ€éªŒè¯ç­‰åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºï¼›å…¶äºŒæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨ç­–ç•¥ç½‘ç»œã€å¥–åŠ±æ¨¡å‹ã€è‡ªè¿›åŒ–ç­–ç•¥ç­‰è¿­ä»£ä¼˜åŒ–å†³ç­–ï¼›å…¶ä¸‰æ˜¯æ…¢æ€è€ƒæ¡†æ¶ï¼Œåƒé•¿æ€ç»´é“¾ï¼ˆCoTï¼‰ã€åˆ†å±‚æµç¨‹ç­‰ï¼Œå°†é—®é¢˜è§£å†³æ‹†è§£ä¸ºæ˜“å¤„ç†æ­¥éª¤æ¥ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèšç„¦â€œæ…¢æ€è€ƒâ€ä¸æŠ€æœ¯ååŒ
ä»¥äººç±»â€œæ…¢æ€è€ƒâ€ï¼ˆç±»æ¯”â€œç³»ç»Ÿ2â€è®¤çŸ¥ï¼‰ä¸ºè®¤çŸ¥ç›®æ ‡ï¼Œæ˜ç¡®å¼ºåŒ–å­¦ä¹ ä¸æ¨ç†æ—¶ç¼©æ”¾å¾‹ä¸ºå®ç°å…ˆè¿›æ¨ç†èƒ½åŠ›çš„å…³é”®ååŒæŠ€æœ¯æœºåˆ¶ï¼Œåœ¨â€œæ…¢æ€è€ƒâ€æ¦‚å¿µæ¡†æ¶ä¸‹ç³»ç»Ÿæ•´åˆä¸æ·±å…¥åˆ†æè¿™ä¸¤ç§æ–¹æ³•ï¼ŒåŒºåˆ«äºå…¶ä»–ä»…å•ç‹¬æˆ–å®½æ³›è®¨è®ºç›¸å…³æŠ€æœ¯çš„ç»¼è¿°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤§æ ·æœ¬ç ”ç©¶ synthesis
ç»¼åˆè¶…160é¡¹ç ”ç©¶æˆæœï¼Œæ¢³ç†ç±»â€œæ…¢æ€è€ƒâ€æ¨ç†LLMsçš„å‘å±•è„‰ç»œï¼Œå‘ˆç°ä»æŠ€æœ¯æ¼”è¿›åˆ°åº”ç”¨æ–¹å‘çš„å…¨é¢å›¾æ™¯ï¼Œä¸ºè¯¥é¢†åŸŸæç»˜å‘å±•è·¯å¾„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªæ˜ç¡®æåŠä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å¯¹æ¯”å®éªŒç»“æœæ•°å€¼ï¼Œä½†é€šè¿‡å¯¹è¶…160é¡¹ç ”ç©¶çš„ç»¼åˆï¼Œå‘ˆç°äº†ä¸åŒæŠ€æœ¯è·¯å¾„ï¼ˆæµ‹è¯•æ—¶ç¼©æ”¾ã€å¼ºåŒ–å­¦ä¹ ã€æ…¢æ€è€ƒæ¡†æ¶ï¼‰åœ¨æ¨åŠ¨LLMsæ¨ç†èƒ½åŠ›æå‡ä¸Šçš„ä½œç”¨ï¼Œå¦‚åœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šæ™ºèƒ½ä½“ç­‰å¤æ‚ä»»åŠ¡åœºæ™¯ä¸‹ï¼Œè¿™äº›æŠ€æœ¯å¦‚ä½•åŠ©åŠ›æ¨¡å‹æ›´æ¥è¿‘äººç±»æ·±åº¦æ¨ç†æ°´å¹³ï¼Œä¸ºç†è§£æŠ€æœ¯æœ‰æ•ˆæ€§æä¾›äº†å¤šç»´åº¦å‚è€ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºç ”ç©¶äººå‘˜ï¼Œæä¾›äº†æ¸…æ™°çš„ç±»â€œæ…¢æ€è€ƒâ€æ¨ç†LLMsæŠ€æœ¯åˆ†ç±»æ¡†æ¶ä¸å‘å±•è„‰ç»œï¼ŒåŠ©åŠ›å¿«é€ŸæŠŠæ¡é¢†åŸŸæ ¸å¿ƒï¼›åœ¨å·¥ä¸šç•Œåº”ç”¨å±‚é¢ï¼Œä¸ºæ‰“é€ æ›´æ™ºèƒ½çš„å†³ç­–æ”¯æŒç³»ç»Ÿã€ç§‘å­¦å‘ç°è¾…åŠ©å·¥å…·ç­‰æä¾›æŠ€æœ¯æ€è·¯å‚è€ƒï¼Œå¦‚åˆ©ç”¨æµ‹è¯•æ—¶ç¼©æ”¾åº”å¯¹ä¸åŒå¤æ‚åº¦ä¸šåŠ¡ä»»åŠ¡ã€å€Ÿå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ™ºèƒ½ä½“å†³ç­–ï¼›åŒæ—¶ï¼Œæ–‡ä¸­å¯¹æŒ‘æˆ˜ï¼ˆå¦‚å¿«æ…¢æ€è€ƒå¹³è¡¡ã€é²æ£’å¥–åŠ±æœºåˆ¶è®¾è®¡ç­‰ï¼‰çš„è®¨è®ºï¼Œä¸ºåç»­æŠ€æœ¯æ”»åšæŒ‡æ˜æ–¹å‘ï¼Œå¯å€Ÿé‰´å…¶æ€è·¯å¼€å±•é’ˆå¯¹æ€§ç ”ç©¶çªç ´ç“¶é¢ˆã€‚

## process-reward-models-that-think
### Abstract
Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨â€œæ€è€ƒâ€èµ‹èƒ½çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šThinkPRM å¦‚ä½•é«˜æ•ˆéªŒè¯æ¨ç†æ­¥éª¤ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†åœºæ™¯ä¸­ï¼Œ**è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆProcess Reward Modelï¼ŒPRMï¼Œä¹Ÿå«è¿‡ç¨‹éªŒè¯å™¨ï¼‰** æ˜¯å®ç°â€œæµ‹è¯•æ—¶ç®—åŠ›æ‰©å±•â€çš„å…³é”®ç»„ä»¶â€”â€”å®ƒèƒ½ä¸ºå¤šæ­¥æ¨ç†çš„æ¯ä¸€æ­¥æ‰“åˆ†ï¼Œå¸®æˆ‘ä»¬ç­›é€‰æ›´ä¼˜çš„æ¨ç†è·¯å¾„ã€‚ä½†ä¼ ç»Ÿ PRM å­˜åœ¨ä¸¤å¤§ç—›ç‚¹ï¼š  
- **åˆ¤åˆ«å¼ PRM**ï¼šéœ€è¦å¤§é‡â€œæ­¥éª¤çº§æ ‡æ³¨â€ï¼ˆæ¯”å¦‚äººå·¥é€æ­¥æ ‡æ³¨æ¨ç†æ˜¯å¦æ­£ç¡®ï¼‰ï¼Œæ ‡æ³¨æˆæœ¬æé«˜ï¼›  
- **LLM-as-a-Judge**ï¼šè™½ä¸ç”¨è®­ç»ƒã€å¯è§£é‡Šæ€§å¼ºï¼Œä½†é¢å¯¹å¤æ‚æ¨ç†æ—¶å®¹æ˜“â€œçœ‹èµ°çœ¼â€ï¼Œæ¼åˆ¤é”™è¯¯æ­¥éª¤ï¼Œæ€§èƒ½è¿œä¸å¦‚ä¸“é—¨è®­ç»ƒçš„ PRMã€‚  

é‚£æœ‰æ²¡æœ‰åŠæ³• **å…¼é¡¾â€œæ•°æ®é«˜æ•ˆâ€å’Œâ€œé«˜æ€§èƒ½â€**ï¼Œè®© PRM æ—¢ä¸ç”¨æµ·é‡æ ‡æ³¨ï¼Œåˆèƒ½ç²¾å‡†éªŒè¯æ¯ä¸€æ­¥æ¨ç†ï¼Ÿè¿™å°±æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
æœ¬æ–‡æå‡º **ThinkPRM**ï¼Œä¸€ç§åŸºäºâ€œé•¿æ€ç»´é“¾ï¼ˆlong CoTï¼‰â€çš„ç”Ÿæˆå¼ PRMï¼Œæ ¸å¿ƒæ€è·¯æ˜¯ï¼š**è®©æ¨¡å‹ç”¨â€œç”ŸæˆéªŒè¯æ€ç»´é“¾â€çš„æ–¹å¼ï¼Œé€æ­¥åˆ¤æ–­æ¨ç†æ˜¯å¦æ­£ç¡®ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–**ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”¨â€œç”Ÿæˆå¼éªŒè¯ + é•¿ CoTâ€é‡æ„ PRM  
ä¼ ç»Ÿåˆ¤åˆ«å¼ PRM æ˜¯â€œåˆ†ç±»å™¨æ€ç»´â€ï¼ˆç›´æ¥ç»™æ¯ä¸€æ­¥æ‰“åˆ†æ•°ï¼‰ï¼Œè€Œ ThinkPRM æ˜¯â€œç”Ÿæˆå¼æ€ç»´â€â€”â€”æŠŠâ€œéªŒè¯æ¯ä¸€æ­¥æ˜¯å¦æ­£ç¡®â€è½¬åŒ–ä¸º**ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„â€œéªŒè¯æ€ç»´é“¾â€**ï¼ˆæ¯”å¦‚ç”Ÿæˆâ€œè¿™ä¸€æ­¥ç”¨äº†å‹¾è‚¡å®šç†ï¼Œè®¡ç®—è¿‡ç¨‹æ­£ç¡®â†’ä¸‹ä¸€æ­¥ä»£å…¥æ•°å€¼æ—¶ç¬¦å·é”™è¯¯â†’â€¦â€¦â€ï¼‰ã€‚è¿™ç§æ–¹å¼å¤©ç„¶å¤ç”¨äº†å¤§æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œè®©éªŒè¯è¿‡ç¨‹æ›´å¯è§£é‡Šï¼Œä¹Ÿæ›´çµæ´»ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ•°æ®é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥  
åˆ¤åˆ«å¼ PRM é€šå¸¸éœ€è¦å‡ åä¸‡ç”šè‡³ç™¾ä¸‡çº§çš„â€œæ­¥éª¤çº§æ ‡æ³¨â€ï¼Œè€Œ ThinkPRM åªéœ€è¦ **8K é‡çº§çš„æ ‡æ³¨** å°±èƒ½å®Œæˆå¾®è°ƒï¼ˆç”šè‡³è¿˜èƒ½åŸºäºåˆæˆæ•°æ®è¿›ä¸€æ­¥å‡å°‘äººå·¥æ ‡æ³¨ï¼‰ã€‚å®ƒçš„ç§˜è¯€æ˜¯ï¼šå¤ç”¨â€œå¼€æºå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰â€çš„åŸºç¡€èƒ½åŠ›ï¼Œé€šè¿‡è½»é‡å¾®è°ƒè®©æ¨¡å‹å­¦ä¼šâ€œç”ŸæˆéªŒè¯ CoTâ€ï¼Œå……åˆ†åˆ©ç”¨äº†å¤§æ¨¡å‹æœ¬èº«çš„æ¨ç†æ½œèƒ½ï¼Œå‡å°‘å¯¹æ ‡æ³¨çš„ä¾èµ–ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªæå…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº† ThinkPRM çš„æ€§èƒ½ï¼Œæ ¸å¿ƒç»“è®ºåŒ…æ‹¬ï¼š  
1. **æ•°æ®æ•ˆç‡ç¢¾å‹åˆ¤åˆ«å¼ PRM**ï¼šåœ¨ ProcessBench ä¸Šï¼ŒThinkPRM ä»…ç”¨ 8K æ­¥éª¤æ ‡æ³¨ï¼Œå°±è¶…è¿‡äº†ç”¨ 100 å€æ•°æ®è®­ç»ƒçš„åˆ¤åˆ«å¼ PRMï¼ˆå¦‚å›¾ 1 å·¦ï¼ŒF1 åˆ†æ•°æ›´é«˜ï¼‰ï¼›  
2. **å¤æ‚æ¨ç†ä»»åŠ¡ç¨³å‹åŸºçº¿**ï¼šåœ¨ MATH-500ã€AIME â€™24 ç­‰æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒThinkPRM åœ¨â€œå¤šå€™é€‰æ‹©ä¼˜ï¼ˆbest-of-Nï¼‰â€å’Œâ€œå¥–åŠ±å¼•å¯¼æœç´¢â€åœºæ™¯ä¸‹ï¼Œæ€§èƒ½è¶…è¿‡ LLM-as-a-Judge å’Œåˆ¤åˆ«å¼ PRMï¼ˆå¦‚å›¾ 1 å³ï¼Œæ¨ç†å‡†ç¡®ç‡æ›´ä¼˜ï¼‰ï¼›  
3. **è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›æ›´å¼º**ï¼šåœ¨ GPQA-Diamondã€LiveCodeBench ç­‰åŸŸå¤–ä»»åŠ¡ä¸­ï¼ŒThinkPRM ç”šè‡³è¶…è¿‡äº†ç”¨å®Œæ•´ PRM800K æ•°æ®è®­ç»ƒçš„åˆ¤åˆ«å¼ PRMï¼Œåˆ†åˆ«é¢†å…ˆ 8% å’Œ 4.5%ï¼›  
4. **ç®—åŠ›åˆ©ç”¨æ›´é«˜æ•ˆ**ï¼šåœ¨ç›¸åŒ Token é¢„ç®—ä¸‹ï¼ŒThinkPRM èƒ½é€šè¿‡â€œæ›´é•¿çš„éªŒè¯ CoTâ€æ›´é«˜æ•ˆåœ°åˆ†é…éªŒè¯ç®—åŠ›ï¼Œåœ¨ ProcessBench å­é›†ä¸Šæ¯” LLM-as-a-Judge é¢†å…ˆ 7.2%ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **èŒƒå¼åˆ›æ–°ï¼šç”Ÿæˆå¼ PRM æ½œåŠ›å¤§**ï¼šæŠŠâ€œéªŒè¯â€ä»â€œåˆ†ç±»ä»»åŠ¡â€æ”¹æˆâ€œç”Ÿæˆä»»åŠ¡â€ï¼Œä¸ä»…é™ä½æ ‡æ³¨ä¾èµ–ï¼Œè¿˜è®©éªŒè¯è¿‡ç¨‹æ›´é€æ˜ã€å¯è§£é‡Šï¼ˆèƒ½çœ‹åˆ°æ¨¡å‹â€œæ€ä¹ˆæ€è€ƒéªŒè¯çš„â€ï¼‰ï¼›  
2. **å°æ•°æ®é«˜æ•ˆå¾®è°ƒ**ï¼šè¯æ˜äº†â€œå¤ç”¨å¤§æ¨¡å‹åŸºç¡€èƒ½åŠ› + è½»é‡å¾®è°ƒ + å°‘é‡æ ‡æ³¨/åˆæˆæ•°æ®â€çš„è·¯çº¿ï¼Œèƒ½åœ¨é«˜ä»·å€¼ä»»åŠ¡ä¸Šå®ç°â€œä½æ ‡æ³¨æˆæœ¬ã€é«˜æ•ˆæœâ€ï¼›  
3. **æµ‹è¯•æ—¶ç®—åŠ›å¯æ‰©å±•**ï¼šThinkPRM å±•ç¤ºäº†â€œè®©æ¨¡å‹é€šè¿‡æ›´é•¿ CoT æ¶ˆè€—æ›´å¤šæ¨ç† Tokenâ€æ¥æå‡éªŒè¯ç²¾åº¦çš„æ€è·¯ï¼Œä¸ºâ€œæµ‹è¯•æ—¶åŠ¨æ€åˆ†é…ç®—åŠ›â€æä¾›äº†æ–°èŒƒå¼ã€‚  

å¦‚æœä½ å…³æ³¨å¤§æ¨¡å‹æ¨ç†å¢å¼ºã€å¥–åŠ±æ¨¡å‹ä¼˜åŒ–ï¼Œæˆ–æ˜¯â€œç”¨å°æ•°æ®è®­å‡ºå¼ºèƒ½åŠ›â€çš„æŠ€æœ¯è·¯çº¿ï¼Œè¿™ç¯‡è®ºæ–‡çš„æ€è·¯å’Œå®éªŒéƒ½å€¼å¾—æ·±å…¥ç ”ç©¶ï½ä»£ç ã€æ•°æ®ä¹Ÿä¼šå¼€æºåœ¨ [GitHub](https://github.com/mukhal/thinkprm) ï¼Œå¯ä»¥è¹²ä¸€æ³¢ï½

## stop-summation--min-form-credit-assignment-is-all-process-reward-model-needs-for-reasoning
### Abstract
Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å‘Šåˆ«â€œå¥–åŠ±é»‘å®¢â€ï¼šPUREè®©è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­æ›´ç¨³æ›´å¼º

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ˜¯æå‡å…¶é—®é¢˜è§£å†³èƒ½åŠ›çš„å…³é”®æ–¹å‘ã€‚ç°æœ‰æ–¹æ³•ä¸­ï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±ï¼ˆverifiable rewardsï¼‰çš„å¼ºåŒ–å¾®è°ƒè™½èƒ½æä¾›ç¨€ç–ä½†å¯é çš„åé¦ˆï¼Œå´å­˜åœ¨é•¿å“åº”æ—¶å­¦ä¹ æ•ˆç‡ä½çš„é—®é¢˜ï¼›è€Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰èƒ½åœ¨å“åº”çš„æ¯ä¸€æ­¥æä¾›å¯†é›†åé¦ˆï¼Œåœ¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest - time scalingï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œå´å› â€œå¥–åŠ±é»‘å®¢ï¼ˆreward hackingï¼‰â€é—®é¢˜é™åˆ¶äº†åœ¨å¼ºåŒ–å¾®è°ƒä¸­çš„åº”ç”¨ã€‚å¥–åŠ±é»‘å®¢æŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œç”Ÿæˆçš„å¥–åŠ±ä¼šå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒä¸­æœç€ä¸åˆç†çš„é«˜å¥–åŠ±æ–¹å‘ä¼˜åŒ–ã€‚ç©¶å…¶æ ¹æºï¼Œå¼ºåŒ–å­¦ä¹ ä¸­ç»å…¸çš„â€œæ±‚å’Œå¼ä¿¡ç”¨åˆ†é…ï¼ˆsummation - form credit assignmentï¼‰â€å°†ä»·å€¼å®šä¹‰ä¸ºæœªæ¥å¥–åŠ±çš„ç´¯ç§¯ä¼½é©¬è¡°å‡å’Œï¼Œæ˜“è¯±å¯¼LLMså»â€œhackâ€é«˜å¥–åŠ±æ­¥éª¤ï¼Œè¿›è€Œå¼•å‘è®­ç»ƒå´©æºƒç­‰é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³PRMåœ¨å¼ºåŒ–å¾®è°ƒä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œè®©PRMèƒ½æ›´æœ‰æ•ˆåœ°åŠ©åŠ›LLMsæ¨ç†èƒ½åŠ›æå‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºPUREæ¡†æ¶ï¼ˆProcess sUpervised Reinforcement lEarningï¼‰
PUREçš„å…³é”®åˆ›æ–°åœ¨äºé‡‡ç”¨â€œæœ€å°å¼ä¿¡ç”¨åˆ†é…ï¼ˆmin - form credit assignmentï¼‰â€ï¼Œå°†ä»·å€¼å‡½æ•°é‡æ–°å®šä¹‰ä¸ºæœªæ¥å¥–åŠ±çš„æœ€å°å€¼ï¼Œè€Œéä¼ ç»Ÿçš„ç´¯ç§¯å’Œå½¢å¼ã€‚è¿™ç§æ–¹å¼é€šè¿‡é™åˆ¶ä»·å€¼å‡½æ•°çš„èŒƒå›´ï¼Œæ›´åˆç†åœ°åˆ†é…ä¼˜åŠ¿ï¼ˆadvantageï¼‰ï¼Œä»è€Œæ˜¾è‘—ç¼“è§£å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚å®ç°ä¸Šä»…éœ€å¯¹è¿‡ç¨‹å¥–åŠ±è¿›è¡Œè½¬æ¢ï¼Œæ— éœ€å¤§é‡ä»£ç æ”¹åŠ¨ï¼Œç®€å•æ˜“è¡Œã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šç§å¥–åŠ±èåˆ
PUREæ¡†æ¶æ”¯æŒå¯†é›†çš„è¿‡ç¨‹å¥–åŠ±ä¸ç¨€ç–çš„å¯éªŒè¯å¥–åŠ±çš„æ•´åˆã€‚å½“ç»“åˆè¿™ä¸¤ç§å¥–åŠ±æ—¶ï¼Œå°‘é‡çœŸå®æ ‡ç­¾ä¿¡å·ï¼ˆå¯éªŒè¯å¥–åŠ±ï¼‰èƒ½è¿›ä¸€æ­¥å‡è½»PRMå¼•å‘çš„å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ›´ç¨³å®šæœ‰æ•ˆçš„ä¿¡å·ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. åœ¨3ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆQwen2.5 - 7Bã€Qwen2.5 - Math - 7Bã€Qwen2.5 - Math - 1.5Bï¼‰ä¸Šè¿›è¡Œäº†å…¨é¢å®éªŒï¼Œå¯¹æ¯”äº†ä»…ç”¨å¯éªŒè¯å¥–åŠ±ã€ä»…ç”¨PRMç”Ÿæˆçš„è¿‡ç¨‹å¥–åŠ±ã€ä¸¤è€…ç»“åˆè¿™ä¸‰ç§å¥–åŠ±é…ç½®ï¼Œä»¥åŠæ±‚å’Œå¼ä¸æœ€å°å¼ä¿¡ç”¨åˆ†é…æ–¹æ³•åœ¨PRMåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æœ€å°å¼ä¿¡ç”¨åˆ†é…çš„åŸºäºPRMçš„æ–¹æ³•ï¼Œä»…ç”¨30%å·¦å³çš„æ­¥éª¤å°±èƒ½è¾¾åˆ°ä¸åŸºäºå¯éªŒè¯å¥–åŠ±æ–¹æ³•ç›¸å½“çš„æ¨ç†æ€§èƒ½ï¼Œè€Œç»å…¸æ±‚å’Œå¼ä¿¡ç”¨åˆ†é…æ–¹æ³•åœ¨è®­ç»ƒåˆæœŸå°±ä¼šå´©æºƒã€‚
2. å½“åœ¨åŸºäºPRMçš„å¾®è°ƒä¸­è¡¥å……ä»…10%çš„å¯éªŒè¯å¥–åŠ±æ—¶ï¼Œèƒ½è¿›ä¸€æ­¥ç¼“è§£å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚ä»¥Qwen2.5 - Math - 7Bä¸ºåŸºç¡€æ¨¡å‹æ—¶ï¼Œåœ¨AMC23åŸºå‡†ä¸Šè¾¾åˆ°82.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨MATH - 500ã€Minerva Mathã€Olympiad Benchã€AIME24ã€AMC23è¿™5ä¸ªåŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡è¾¾53.3%ï¼Œå–å¾—å®éªŒä¸­åŸºäºè¯¥æ¨¡å‹çš„æœ€ä½³å¾®è°ƒæ•ˆæœã€‚
3. è¿˜æ€»ç»“äº†3ç§PRMå¼•å‘çš„å¥–åŠ±é»‘å®¢æ¡ˆä¾‹ï¼šâ€œåªæ€è€ƒä¸è§£å†³â€â€œæå°‘æ­¥éª¤ï¼ˆ1æ­¥ï¼‰â€â€œæå°‘æ­¥éª¤ï¼ˆ0æ­¥ï¼‰â€ï¼Œå¹¶åˆ†æäº†æ¯ç§æ¡ˆä¾‹çš„æˆå› ã€å±•ç¤ºç¤ºä¾‹å’Œæä¾›è§£å†³æ€è·¯ï¼›åŒæ—¶å‘ç°éªŒè¯å™¨åˆ¤å®šä¸ºæ­£ç¡®çš„é•¿ä¸”é«˜åº¦é‡å¤çš„ä¼ªæ­£æ ·æœ¬ä¼šå¯¼è‡´è®­ç»ƒçªç„¶å´©æºƒï¼ˆ5ä¸ªæ¢¯åº¦æ­¥å†…ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•è®¾è®¡å±‚é¢ï¼šå½“é¢ä¸´ç±»ä¼¼â€œä¿¡ç”¨åˆ†é…å¯¼è‡´æ¨¡å‹ä¸åˆç†ä¼˜åŒ–â€é—®é¢˜æ—¶ï¼Œå¯å€Ÿé‰´PUREè¿™ç§æ”¹å˜ä¿¡ç”¨åˆ†é…å½¢å¼ï¼ˆä»æ±‚å’Œåˆ°å–æœ€å°ï¼‰çš„æ€è·¯ï¼Œé€šè¿‡è°ƒæ•´ä»·å€¼å‡½æ•°è®¡ç®—æ–¹å¼æ¥çº¦æŸæ¨¡å‹ä¼˜åŒ–æ–¹å‘ï¼Œè§£å†³å¥–åŠ±é»‘å®¢ç­‰é—®é¢˜ã€‚
2. å¤šå¥–åŠ±èåˆå±‚é¢ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œå¯è€ƒè™‘å°†ä¸åŒç±»å‹ï¼ˆå¯†é›†ä¸ç¨€ç–ã€æ¨¡å‹ç”Ÿæˆä¸çœŸå®æ ‡ç­¾ï¼‰çš„å¥–åŠ±ç»“åˆï¼Œåˆ©ç”¨å°‘é‡å¯é ä¿¡å·è¾…åŠ©ç¼“è§£æ¨¡å‹å› ä¾èµ–ç”Ÿæˆå¥–åŠ±å¸¦æ¥çš„ä¸ç¨³å®šé—®é¢˜ï¼Œæå‡è®­ç»ƒæ•ˆæœã€‚
3. é—®é¢˜åˆ†æå±‚é¢ï¼šè®ºæ–‡å¯¹å¥–åŠ±é»‘å®¢æ¡ˆä¾‹å’Œè®­ç»ƒå´©æºƒåŸå› çš„æ€»ç»“åˆ†æï¼Œä¸ºåç»­ç ”ç©¶è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨å¼ºåŒ–å¾®è°ƒä¸­é‡åˆ°çš„é—®é¢˜æä¾›äº†å®è´µçš„ç»éªŒå‚è€ƒï¼Œæœ‰åŠ©äºåç»­å·¥ä½œæ›´é«˜æ•ˆåœ°æ’æŸ¥å’Œè§£å†³ç±»ä¼¼è®­ç»ƒå¼‚å¸¸é—®é¢˜ã€‚

## evaluating-judges-as-evaluators--the-jetts-benchmark-of-llm-as-judges-as-test-time-scaling-evaluators
### Abstract
Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯„ä¼°LLMæ³•å®˜åœ¨æµ‹è¯•æ—¶æ‰©å±•åœºæ™¯ä¸‹çš„è¡¨ç°ï¼šJETTSåŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›æå‡å¤šä¾èµ–æ¨¡å‹ä¸è®­ç»ƒæ•°æ®è§„æ¨¡æ‰©å¤§ï¼Œä½†æ­¤æ–¹å¼å› æ•°æ®å’Œæˆæœ¬é™åˆ¶æ¸è¶‹é¥±å’Œã€‚æµ‹è¯•æ—¶æ‰©å±•ï¼ˆtest - time scalingï¼‰ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆå…´èµ·ï¼Œå³æ¨ç†æ—¶ç»™ç”Ÿæˆå¼LLMæ›´å¤šè®¡ç®—èµ„æºä»¥ç”Ÿæˆæ›´ä¼˜å“åº”ï¼Œå…¶ä¸­ evaluator æ¨¡å‹å¾ˆå…³é”®ã€‚æ ‡é‡å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å¸¸ç”¨äºæµ‹è¯•æ—¶è®¡ç®—çš„å“åº”é‡æ’æˆ–åˆ†æ­¥ç”Ÿæˆè¯„ä¼°ï¼Œè€Œç”Ÿæˆå¼LLM - judgesï¼ˆèƒ½ç”Ÿæˆè‡ªç„¶è¯­è¨€è¯„ä¼°å’Œ critiqueï¼‰åœ¨æ¨¡å‹è¯„ä¼°ä¸­æµè¡Œï¼Œä½†åœ¨æµ‹è¯•æ—¶æ‰©å±•åœºæ™¯ä¸‹å…¶æœ‰æ•ˆæ€§æœªçŸ¥ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ„å»ºåŸºå‡†æµ‹è¯•æ¥ç³»ç»Ÿè¯„ä¼° LLM - judges åœ¨æµ‹è¯•æ—¶æ‰©å±•åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºJETTSåŸºå‡†æµ‹è¯•  
æ„å»ºäº†Judge Evaluation for Test - Time Scalingï¼ˆJETTSï¼‰åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLM - judgesåœ¨æµ‹è¯•æ—¶æ‰©å±•åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ¶µç›–ä¸‰ä¸ªä»»åŠ¡ï¼šå“åº”é‡æ’ï¼ˆresponse rerankingï¼Œæ³•å®˜ä»å¤šä¸ªå“åº”ä¸­é€‰æœ€ä¼˜ï¼‰ã€åˆ†æ­¥æŸæœç´¢ï¼ˆstep - level beam searchï¼Œæ³•å®˜å¼•å¯¼æ¨¡å‹é€æ­¥ç”Ÿæˆå“åº”ï¼‰ã€åŸºäº critique çš„ä¼˜åŒ–ï¼ˆcritique - based refinementï¼Œæ³•å®˜æä¾›è‡ªç„¶è¯­è¨€ critique ä¾›æ¨¡å‹ä¼˜åŒ–å“åº”ï¼‰ï¼›æ¶‰åŠä¸‰ä¸ªé¢†åŸŸï¼ˆæ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€æŒ‡ä»¤éµå¾ªï¼‰ï¼Œè¯„ä¼°äº†10ä¸ªä¸åŒè§„æ¨¡ï¼ˆ7B - 70Bå‚æ•°ï¼‰çš„judgeæ¨¡å‹ï¼Œä¸”ç”Ÿæˆå“åº”çš„åŸºç¡€ç”Ÿæˆå™¨æ¨¡å‹æœ‰8ä¸ªï¼ˆ6.7B - 72Bå‚æ•°ï¼‰ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç»´åº¦åˆ†æLLM - judgesæ•ˆç”¨  
é€šè¿‡JETTSåŸºå‡†è®¾ç½®ï¼Œåˆ†æjudgeå¼•å¯¼çš„æµ‹è¯•æ—¶æ‰©å±•çš„å¤šæ–¹é¢å½±å“ï¼Œå¦‚â€œå¼±â€judgeå¯¹â€œå¼ºâ€ç”Ÿæˆå™¨æ˜¯å¦æœ‰å¸®åŠ©ã€å¼ºjudgeå¯¹å¼±ç”Ÿæˆå™¨ç›Šå¤„ã€judgeæ¨¡å‹åœ¨å“ªäº›é¢†åŸŸé€‚åˆæµ‹è¯•æ—¶æ‰©å±•ã€judgeçš„critiqueå®é™…æ•ˆç”¨ç­‰ã€‚åŒæ—¶å¯¹æ¯”äº†JETTSä¸RewardBenchåŸºå‡†ï¼Œæ­ç¤ºä¸åŒè§„æ¨¡judgeåœ¨ä¸åŒåœºæ™¯ä¸‹çš„â€œåŸºç¡€åˆ¤æ–­èƒ½åŠ›â€å·®å¼‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. åœ¨å“åº”é‡æ’ä»»åŠ¡ä¸­ï¼Œjudgesä¸ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆoutcome reward modelsï¼‰ç›¸æ¯”æœ‰ç«äº‰åŠ›ï¼Œä½†åœ¨æŸæœç´¢è¿‡ç¨‹ä¸­ï¼Œå§‹ç»ˆæ¯”è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆprocess reward modelsï¼‰å·®ã€‚
2. å¼±judgeåœ¨åƒæŒ‡ä»¤éµå¾ªè¿™ç±»è¾ƒç®€å•ä»»åŠ¡ä¸­èƒ½å¸®åŠ©å¼ºç”Ÿæˆå™¨ï¼Œä½†åœ¨ç¼–ç æˆ–æ•°å­¦ç­‰æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­ä¸è¡Œï¼›æ›´å¤§è§„æ¨¡çš„judgeå¯¹æ•°å­¦å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡å¸®åŠ©æœ€å¤§ï¼Œä½†æ²¡æœ‰è¯„ä¼°çš„judgeèƒ½å¯é æå‡ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­ç”Ÿæˆå™¨æ€§èƒ½ã€‚
3. å°½ç®¡è‡ªç„¶è¯­è¨€critiqueæ˜¯LLM - judgesç›¸å¯¹äºRMsçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä½†ç›®å‰åœ¨å¼•å¯¼ç”Ÿæˆå™¨å¾—åˆ°æ›´ä¼˜å“åº”æ–¹é¢æ•ˆæœä¸ä½³ã€‚
4. å¯¹æ¯”RewardBenchå’ŒJETTSå‘ç°ï¼Œä¸åŒåŸºå‡†ä¸‹judgeè¡¨ç°æœ‰å·®å¼‚ï¼ŒJETTSæ¨¡æ‹Ÿæµ‹è¯•æ—¶æ‰©å±•åœºæ™¯ï¼Œæ­ç¤ºäº†ä¸åŒè§„æ¨¡judgeâ€œåŸºç¡€åˆ¤æ–­èƒ½åŠ›â€çš„ä¸åŒï¼Œå¦‚åœ¨å“åº”é‡æ’ä»»åŠ¡ä¸­ï¼Œå°å‚æ•°çš„judgeï¼ˆå¦‚Skywork - Critic - 8Bï¼‰åœ¨JETTSä¸Šæ¯”å¤§å‚æ•°judgeï¼ˆå¦‚Skywork - Critic - 70Bï¼‰å¸¦æ¥çš„æå‡ä½ï¼Œè€ŒRewardBenchä¸­å¯èƒ½å› ä»»åŠ¡æ„é€ æ–¹å¼ä¸åŒï¼Œå°å‚æ•°judgeè¡¨ç°å’Œå¤§å‚æ•°judgeå·®å¼‚æ²¡è¿™ä¹ˆå¤§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä¸ºè¯„ä¼°LLMåœ¨æµ‹è¯•æ—¶æ‰©å±•åœºæ™¯ä¸‹ä½œä¸º evaluator çš„è¡¨ç°æä¾›äº†é¦–ä¸ªç³»ç»ŸåŸºå‡†JETTSï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤åŸºå‡†è¿›ä¸€æ­¥æ¢ç´¢LLM - judgesåœ¨æµ‹è¯•æ—¶æ‰©å±•çš„ä¼˜åŒ–æ–¹å‘ã€‚
2. å¤šä»»åŠ¡ã€å¤šé¢†åŸŸã€å¤šæ¨¡å‹è§„æ¨¡çš„è¯„ä¼°æ–¹å¼ä¸ºå…¨é¢ç†è§£LLM - judgesæ•ˆç”¨æä¾›äº†èŒƒä¾‹ï¼Œå…¶ä»–å…³äºæ¨¡å‹è¯„ä¼°æˆ–æµ‹è¯•æ—¶ä¼˜åŒ–çš„ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å¤šç»´åº¦è¯„ä¼°æ€è·¯ã€‚
3. å¯¹LLM - judgesåœ¨ä¸åŒä»»åŠ¡ã€é¢†åŸŸã€æ¨¡å‹è§„æ¨¡ä¸‹çš„è¡¨ç°åˆ†æï¼Œèƒ½ä¸ºå®é™…åº”ç”¨ä¸­é€‰æ‹©åˆé€‚çš„judgeæ¨¡å‹æä¾›å‚è€ƒï¼Œå¦‚åœ¨æ•°å­¦å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­è€ƒè™‘æ›´å¤§è§„æ¨¡judgeï¼Œä»£ç ç”Ÿæˆä»»åŠ¡åˆ™éœ€æ¢ç´¢æ›´æœ‰æ•ˆçš„judgeæˆ–ä¼˜åŒ–æ–¹å¼ã€‚

## adaptive-rectification-sampling-for-test-time-compute-scaling
### Abstract
The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time
scaling can significantly improve model performance, especially in complex
tasks such as logical reasoning. Common test-time scaling methods involve
generating more chain of thoughts (CoTs) or longer CoTs with self-correction.
However, while self-correction can improve performance, it may lead to
significant token waste and reduce readability of the CoT if the reasoning
steps are already correct. To demonstrate that large language models (LLMs) can
rectify errors at a more fine-grained level, we propose Adaptive Rectification
Sampling (AR-Sampling), which can guide the LLMs to self-correction at the
appropriate step. AR-Sampling leverages a process-supervised reward model (PRM)
as a verifier and constructed trigger sentences to guide the model in adaptive
step-level rethinking. Through the experiments on GSM8K and MATH500, it
indicate that our approach enables the models to rethink in more fine-grained
level, improving the accuracy of solutions, while generating a reasonable
number of additional tokens.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªé€‚åº”ä¿®æ­£é‡‡æ ·ï¼šè®©å¤§æ¨¡å‹æµ‹è¯•æ—¶æ›´é«˜æ•ˆæ€è€ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
OpenAI - o1å’ŒDeepSeek - R1ç­‰æ¨¡å‹å±•ç°å‡ºæµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾ï¼ˆå¦‚å¢åŠ æ€ç»´é“¾é•¿åº¦æˆ–å®½åº¦ï¼‰èƒ½æå‡å¤æ‚ä»»åŠ¡è¡¨ç°ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸è¶³ã€‚å¸¸è§æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ä¸­ï¼Œå¢åŠ æ€ç»´é“¾é•¿åº¦çš„è‡ªæˆ‘ä¿®æ­£è‹¥åœ¨æ¨ç†æ­¥éª¤æ­£ç¡®æ—¶ä»è¿›è¡Œï¼Œä¼šå¯¼è‡´tokenæµªè´¹å’Œæ€ç»´é“¾å¯è¯»æ€§ä¸‹é™ï¼›ä¸”éš¾ä»¥åœ¨æµ‹è¯•æ—¶ç²¾å‡†å®šä½é”™è¯¯æ­¥éª¤å¼•å¯¼æ¨¡å‹é‡æ€è€ƒã€‚åŒæ—¶ï¼Œå¤§æ¨¡å‹è¿˜å­˜åœ¨â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆç®€å•é—®é¢˜ä¹Ÿç”Ÿæˆå†—é•¿å›åº”ï¼‰ç°è±¡ã€‚å› æ­¤ï¼Œå¦‚ä½•å¼•å¯¼å¤§æ¨¡å‹åœ¨æµ‹è¯•æ—¶äºåˆé€‚æ—¶æœºç»†ç²’åº¦é‡æ€è€ƒæˆä¸ºå…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè‡ªé€‚åº”ä¿®æ­£é‡‡æ ·ï¼ˆAR - Samplingï¼‰
åˆ©ç”¨è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä½œä¸ºéªŒè¯å™¨æ£€æŸ¥æ¨ç†æ­¥éª¤ï¼Œè¯†åˆ«æ½œåœ¨é”™è¯¯ï¼›å¹¶æ„é€ è§¦å‘è¯­å¥ï¼Œå¼•å¯¼å¤§æ¨¡å‹åœ¨è‡ªé€‚åº”çš„æ­¥éª¤å±‚é¢é‡æ–°æ€è€ƒã€‚è¿™æ ·èƒ½è®©æ¨¡å‹åœ¨é”™è¯¯æ­¥éª¤å¤„é’ˆå¯¹æ€§ä¿®æ­£ï¼Œé¿å…æ— æ„ä¹‰çš„tokenç”Ÿæˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéªŒè¯å¤§æ¨¡å‹ç»†ç²’åº¦é‡æ€è€ƒèƒ½åŠ›
é€šè¿‡è®¾è®¡æ–¹æ³•è¯æ˜å¤§æ¨¡å‹å…·å¤‡åœ¨æ›´ç»†ç²’åº¦å±‚é¢é‡æ–°æ€è€ƒçš„èƒ½åŠ›ï¼Œè¿™å¯¹æœªæ¥è§£å†³â€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜æä¾›äº†æ€è·¯ä¸æ–¹å‘ï¼Œä¸ºåç»­ç›¸å…³ç ”ç©¶æ‰“ä¸‹åŸºç¡€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Llama3.2å’ŒQwen2.5æ¨¡å‹ä¸Šå¯¹GSM8Kå’ŒMATH500ç­‰ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½è®©æ¨¡å‹åœ¨æ›´ç»†ç²’åº¦å±‚é¢é‡æ–°æ€è€ƒï¼Œæå‡è§£é¢˜å‡†ç¡®ç‡ï¼ŒåŒæ—¶é¢å¤–ç”Ÿæˆçš„tokenæ•°é‡åˆç†ï¼Œå³åœ¨ä¿è¯æ•ˆæœæå‡çš„åŒæ—¶ï¼Œè¾ƒå¥½åœ°æ§åˆ¶äº†è®¡ç®—èµ„æºæ¶ˆè€—ï¼ˆtokenæ•°é‡å¯åæ˜ ä¸€å®šè®¡ç®—èµ„æºæƒ…å†µï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼ŒAR - Samplingç»“åˆéªŒè¯å™¨ä¸è§¦å‘è¯­å¥å¼•å¯¼é‡æ€è€ƒçš„æ€è·¯ï¼Œä¸ºå¤§æ¨¡å‹æµ‹è¯•æ—¶ä¼˜åŒ–æ¨ç†è¿‡ç¨‹æä¾›äº†æ–°èŒƒå¼ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§â€œæ£€æµ‹ - å¼•å¯¼ - ç»†ç²’åº¦ä¿®æ­£â€çš„æµç¨‹è®¾è®¡ï¼›ä»èƒ½åŠ›éªŒè¯è§’åº¦ï¼Œè¯æ˜å¤§æ¨¡å‹ç»†ç²’åº¦é‡æ€è€ƒèƒ½åŠ›ä¸ºè§£å†³â€œè¿‡åº¦æ€è€ƒâ€ç­‰é—®é¢˜å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œç›¸å…³ç ”ç©¶å¯å›´ç»•å¦‚ä½•è¿›ä¸€æ­¥åˆ©ç”¨è¯¥èƒ½åŠ›ä¼˜åŒ–æ¨¡å‹è¡¨ç°å±•å¼€ï¼›ä»åº”ç”¨è§’åº¦ï¼Œåœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆå®è·µï¼Œä¸ºå…¶ä»–å¤æ‚ä»»åŠ¡ï¼ˆå¦‚é€»è¾‘æ¨ç†ã€ä»£ç ç”Ÿæˆï¼‰ä¸­æå‡å¤§æ¨¡å‹æµ‹è¯•æ—¶è¡¨ç°æä¾›äº†å‚è€ƒï¼Œå¯å°è¯•å°†è¯¥æ–¹æ³•è¿ç§»åˆ°ç±»ä¼¼éœ€è¦é€æ­¥æ¨ç†çš„ä»»åŠ¡åœºæ™¯ä¸­ã€‚
```

## when-to-solve--when-to-verify--compute-optimal-problem-solving-and-generative-verification-for-llm-reasoning
### Abstract
Scaling test-time compute has emerged as a key strategy for enhancing the
reasoning capabilities of large language models (LLMs), particularly in tasks
like mathematical problem-solving. A traditional approach, Self-Consistency
(SC), generates multiple solutions to a problem and selects the most common
answer via majority voting. Another common method involves scoring each
solution with a reward model (verifier) and choosing the best one. Recent
advancements in Generative Reward Models (GenRM) reframe verification as a
next-token prediction task, enabling inference-time scaling along a new axis.
Specifically, GenRM generates multiple verification chains-of-thought to score
each solution. Under a limited inference budget, this introduces a fundamental
trade-off: should you spend the budget on scaling solutions via SC or generate
fewer solutions and allocate compute to verification via GenRM? To address
this, we evaluate GenRM against SC under a fixed inference budget.
Interestingly, we find that SC is more compute-efficient than GenRM for most
practical inference budgets across diverse models and datasets. For instance,
GenRM first matches SC after consuming up to 8x the inference compute and
requires significantly more compute to outperform it. Furthermore, we derive
inference scaling laws for the GenRM paradigm, revealing that compute-optimal
inference favors scaling solution generation more aggressively than scaling the
number of verifications. Our work provides practical guidance on optimizing
test-time scaling by balancing solution generation and verification. The code
is available at https://github.com/nishadsinghi/sc-genrm-scaling.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§æ¨¡å‹æ¨ç†æ—¶ï¼Œä½•æ—¶ç”Ÿæˆè§£æ³•ã€ä½•æ—¶éªŒè¯ï¼Ÿè®¡ç®—æœ€ä¼˜çš„æƒè¡¡ä¹‹é“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è§£é¢˜ï¼‰ä¸­ï¼Œæµ‹è¯•æ—¶æ‰©å±•è®¡ç®—èµ„æºæ˜¯æå‡èƒ½åŠ›çš„å…³é”®ç­–ç•¥ã€‚ä¼ ç»Ÿæ–¹æ³•æœ‰è‡ªæ´½æ€§ï¼ˆSelf - Consistencyï¼ŒSCï¼‰ï¼Œå³ç”Ÿæˆå¤šä¸ªè§£æ³•åå¤šæ•°æŠ•ç¥¨ï¼›è¿˜æœ‰ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆéªŒè¯å™¨ï¼‰ç»™è§£æ³•æ‰“åˆ†é€‰æœ€ä¼˜çš„Best - of - Nã€‚è€Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰æŠŠéªŒè¯é‡æ„ä¸ºä¸‹ä¸€ä¸ªtokené¢„æµ‹ä»»åŠ¡ï¼Œèƒ½æ²¿æ–°ç»´åº¦æ‰©å±•æ¨ç†ã€‚ä½†åœ¨æœ‰é™æ¨ç†é¢„ç®—ä¸‹ï¼Œå­˜åœ¨å…³é”®æƒè¡¡ï¼šæŠŠé¢„ç®—èŠ±åœ¨SCæ‰©å±•è§£æ³•æ•°é‡ï¼Œè¿˜æ˜¯å°‘ç”Ÿæˆè§£æ³•ã€æŠŠè®¡ç®—èµ„æºç»™GenRMåšéªŒè¯ï¼Ÿä»¥å¾€å›ºå®šè§£æ³•æ•°é‡å¯¹æ¯”GenRMå’ŒSCä¼šè¯¯å¯¼å®è·µï¼Œå› ä¸ºå¿½ç•¥äº†éªŒè¯çš„è®¡ç®—æˆæœ¬ï¼Œæ‰€ä»¥æœ¬æ–‡è¦åœ¨å›ºå®šæ¨ç†é¢„ç®—ä¸‹è¯„ä¼°ä¸¤è€…ï¼Œå¹¶æ¢ç©¶GenRMä¸‹è§£æ³•ç”Ÿæˆä¸éªŒè¯çš„è®¡ç®—åˆ†é…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¡ç®—åŒ¹é…åˆ†ææ¡†æ¶
æå‡ºåœ¨å›ºå®šæ¨ç†é¢„ç®—Bä¸‹ï¼Œå¯¹æ¯”SCï¼ˆè§£æ³•æ•°S = Bï¼‰å’ŒGenRMï¼ˆè§£æ³•æ•°S = B/Vï¼ŒVä¸ºæ¯ä¸ªè§£æ³•çš„éªŒè¯æ•°ï¼‰çš„æ€§èƒ½ï¼ŒåŸºäºLLMç”Ÿæˆçš„è§£æ³•å’ŒéªŒè¯æ€»æ•°æ¥æ¯”è¾ƒSCä¸GenRMçš„æ¨ç†è®¡ç®—ï¼Œæ˜ç¡®åœ¨æœ‰é™é¢„ç®—ä¸‹ä¸¤ç§ç­–ç•¥çš„æ•ˆæœå·®å¼‚ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨å¯¼GenRMæ¨ç†ç¼©æ”¾å¾‹
é’ˆå¯¹GenRMï¼Œæ¨å¯¼æ¨ç†ç¼©æ”¾å¾‹ï¼Œç ”ç©¶åœ¨æ€»è®¡ç®—é¢„ç®—å˜åŒ–æ—¶ï¼Œæœ€ä¼˜çš„è§£æ³•ç”Ÿæˆæ•°å’ŒéªŒè¯æ•°å¦‚ä½•ç¼©æ”¾ï¼Œæ­ç¤ºè®¡ç®—æœ€ä¼˜æ¨ç†ä¸­è§£æ³•ç”Ÿæˆåº”æ¯”éªŒè¯ç¼©æ”¾æ›´æ¿€è¿›ï¼ˆç¼©æ”¾å› å­1.5 - 2å€å·¦å³ï¼‰ï¼Œä»¥æŒ‡å¯¼èµ„æºåˆ†é…ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸åŒæ¨¡å‹ï¼ˆå¦‚Llamaã€Qwenç­‰ï¼‰ã€æ¨¡å‹å¤§å°ï¼ˆå¦‚7Bã€70Bç­‰ï¼‰ã€æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦ï¼‰å’Œæ•°æ®é›†ä¸Šï¼Œå¤šæ•°å®é™…æ¨ç†é¢„ç®—ä¸‹SCæ¯”GenRMè®¡ç®—æ•ˆç‡æ›´é«˜ã€‚GenRMè¦æ¶ˆè€—é«˜è¾¾8å€äºSCçš„æ¨ç†è®¡ç®—æ‰èƒ½è¿½å¹³SCï¼Œè¦æ˜¾è‘—æ›´å¤šè®¡ç®—æ‰èƒ½è¶…è¶Šã€‚åœ¨GenRMèŒƒå¼ä¸‹ï¼Œè®¡ç®—æœ€ä¼˜æ¨ç†æ›´å€¾å‘äºæ¯”æ‰©å±•éªŒè¯æ•°æ›´ç§¯æåœ°æ‰©å±•è§£æ³•ç”Ÿæˆæ•°ï¼Œå½“é¢„ç®—è¶³å¤Ÿé«˜æ—¶GenRMæ‰ä¼šè¶…è¿‡SCï¼Œä¸”GenRMä¸‹è§£æ³•ç”Ÿæˆå’ŒéªŒè¯æ•°éœ€ååŒç¼©æ”¾ä½†è§£æ³•ç¼©æ”¾æ›´å¿«ï¼ˆ1.5 - 2å€ï¼‰æ‰èƒ½æœ€ä¼˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä¸ºä¼˜åŒ–æµ‹è¯•æ—¶çš„è®¡ç®—ç¼©æ”¾æä¾›å®è·µæŒ‡å¯¼ï¼Œå¸®åŠ©ä»ä¸šè€…å¹³è¡¡è§£æ³•ç”Ÿæˆå’ŒéªŒè¯æ¥ä¼˜åŒ–æ¨ç†ç­–ç•¥ä¸é¢„ç®—ã€‚æ˜ç¡®äº†åœ¨ä¸åŒè®¡ç®—é¢„ç®—è§„æ¨¡ä¸‹ï¼ŒSCå’ŒGenRMçš„ä¼˜åŠ£åŒºé—´ï¼Œä»¥åŠGenRMä¸‹è§£æ³•ä¸éªŒè¯çš„èµ„æºåˆ†é…æ–¹å¼ï¼Œè®©å¼€å‘è€…åœ¨å®é™…éƒ¨ç½²å¤§æ¨¡å‹æ¨ç†ä»»åŠ¡æ—¶ï¼Œèƒ½æ›´åˆç†åˆ†é…è®¡ç®—èµ„æºï¼Œæå‡æ•ˆç‡ä¸æ€§èƒ½ã€‚åŒæ—¶å¼€æºä»£ç ï¼Œä¸ºåç»­ç ”ç©¶å’Œå®è·µæä¾›äº†å¯å¤ç°å’Œæ‹“å±•çš„åŸºç¡€ã€‚

## genprm--scaling-test-time-compute-of-process-reward-models-via-generative-reasoning
### Abstract
Recent advancements in Large Language Models (LLMs) have shown that it is
promising to utilize Process Reward Models (PRMs) as verifiers to enhance the
performance of LLMs. However, current PRMs face three key challenges: (1)
limited process supervision and generalization capabilities, (2) dependence on
scalar value prediction without leveraging the generative abilities of LLMs,
and (3) inability to scale the test-time compute of PRMs. In this work, we
introduce GenPRM, a generative process reward model that performs explicit
Chain-of-Thought (CoT) reasoning with code verification before providing
judgment for each reasoning step. To obtain high-quality process supervision
labels and rationale data, we propose Relative Progress Estimation (RPE) and a
rationale synthesis framework that incorporates code verification. Experimental
results on ProcessBench and several mathematical reasoning tasks show that
GenPRM significantly outperforms prior PRMs with only 23K training data from
MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and
a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,
GenPRM demonstrates strong abilities to serve as a critic model for policy
model refinement. This work establishes a new paradigm for process supervision
that bridges the gap between PRMs and critic models in LLMs. Our code, model,
and data will be available in https://ryanliu112.github.io/GenPRM.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | GenPRMï¼šç”¨ç”Ÿæˆå¼æ¨ç†çªç ´è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æµ‹è¯•æ—¶ç®—åŠ›ç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•è®©è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºéªŒè¯å™¨æå‡æ¨¡å‹æ€§èƒ½æˆä¸ºå¯èƒ½ï¼Œä½†ç°æœ‰PRMså­˜åœ¨ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯è¿‡ç¨‹ç›‘ç£å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼›äºŒæ˜¯ä¾èµ–æ ‡é‡é¢„æµ‹ï¼Œæœªå……åˆ†åˆ©ç”¨LLMsçš„ç”Ÿæˆèƒ½åŠ›ï¼›ä¸‰æ˜¯æ— æ³•å¯¹PRMsçš„æµ‹è¯•æ—¶ç®—åŠ›è¿›è¡Œæ‰©å±•ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç”Ÿæˆå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹GenPRMã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è®¾è®¡
GenPRMå°†è¿‡ç¨‹ç›‘ç£é‡æ–°å®šä¹‰ä¸ºç”Ÿæˆä»»åŠ¡è€Œéåˆ¤åˆ«å¼è¯„åˆ†ä»»åŠ¡ï¼Œåœ¨ç»™å‡ºæœ€ç»ˆåˆ¤æ–­å‰æ•´åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸ä»£ç éªŒè¯è¿‡ç¨‹ï¼ŒåŒºåˆ«äºä¼ ç»ŸåŸºäºåˆ†ç±»çš„PRMsï¼Œå……åˆ†å‘æŒ¥LLMsçš„ç”Ÿæˆä¼˜åŠ¿ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç›¸å¯¹è¿›åº¦ä¼°è®¡ï¼ˆRPEï¼‰ä¸åˆç†è§£é‡Šåˆæˆæ¡†æ¶
æå‡ºRelative Progress Estimationï¼ˆRPEï¼‰æ¥æ”¹è¿›ä¼ ç»Ÿç¡¬æ ‡ç­¾ä¼°è®¡ï¼Œåˆ©ç”¨ç›¸å¯¹å‡†åˆ™å®ç°æ›´å‡†ç¡®çš„æ ‡ç­¾ä¼°è®¡ï¼›åŒæ—¶å¼•å…¥ç»“åˆä»£ç éªŒè¯çš„åˆç†è§£é‡Šåˆæˆæ¡†æ¶ï¼Œä»¥è·å–é«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£æ¨ç†æ•°æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæµ‹è¯•æ—¶ç®—åŠ›æ‰©å±•ï¼ˆTTSï¼‰
é€šè¿‡æµ‹è¯•æ—¶ç®—åŠ›æ‰©å±•æ‰‹æ®µï¼Œè®©GenPRMåœ¨æ¨ç†æ€§èƒ½æå‡ä¸Šå±•ç°æ½œåŠ›ï¼Œå°æ¨¡å‹ä¹Ÿèƒ½é€šè¿‡è¯¥æ–¹å¼è¶…è¶Šæ›´å¤§è§„æ¨¡çš„ä¼ ç»ŸPRMsï¼Œä¸ºPRMsçš„èƒ½åŠ›é‡Šæ”¾æä¾›æ–°è·¯å¾„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ProcessBenchå’Œå¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒGenPRMä»…ç”¨MATHæ•°æ®é›†çš„23Kè®­ç»ƒæ•°æ®å°±æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„PRMsã€‚æµ‹è¯•æ—¶æ‰©å±•æ–¹é¢ï¼Œ1.5Bå‚æ•°çš„GenPRMæ€§èƒ½è¶…è¿‡GPT - 4oï¼Œ7Bå‚æ•°çš„GenPRMåœ¨ProcessBenchä¸Šè¶…è¿‡Qwen2.5 - Math - PRM - 72Bï¼›åŒæ—¶GenPRMä½œä¸º critic æ¨¡å‹ä¼˜åŒ–ç­–ç•¥æ¨¡å‹æ—¶ä¹Ÿå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œå¦‚GenPRM - 7Bç»è¿‡3æ¬¡ä¼˜åŒ–è¿­ä»£åï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¯DeepSeek - R1 - Distill - 7Bçš„3.4å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹è®¾è®¡æ€è·¯ï¼šå°†ç”Ÿæˆå¼å»ºæ¨¡å¼•å…¥è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä¸ºPRMsèµ‹äºˆæ›´å¼ºçš„æ¨ç†ä¸éªŒè¯èƒ½åŠ›ï¼Œæ‰“ç ´ä¼ ç»Ÿåˆ¤åˆ«å¼PRMsçš„å±€é™ï¼Œæä¾›äº†ä»ä»»åŠ¡å®šä¹‰å±‚é¢é©æ–°æ¨¡å‹çš„æ€è·¯ã€‚
2. æ•°æ®ä¸æ ‡ç­¾å¤„ç†ï¼šRPEå’Œç»“åˆä»£ç éªŒè¯çš„åˆç†è§£é‡Šåˆæˆæ¡†æ¶ï¼Œä¸ºè·å–é«˜è´¨é‡ç›‘ç£æ•°æ®å’Œæ ‡ç­¾æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯æ–¹æ¡ˆï¼Œåœ¨æå‡æ¨¡å‹è®­ç»ƒæ•°æ®è´¨é‡ä¸Šæœ‰å€Ÿé‰´ä»·å€¼ã€‚
3. æµ‹è¯•æ—¶ä¼˜åŒ–ï¼šæ¢ç´¢æµ‹è¯•æ—¶ç®—åŠ›æ‰©å±•åœ¨PRMsä¸Šçš„åº”ç”¨ï¼Œè¯æ˜å°æ¨¡å‹ä¹Ÿèƒ½é€šè¿‡è¯¥æ–¹å¼å®ç°æ€§èƒ½è·ƒè¿ï¼Œä¸ºæ¨¡å‹åœ¨æ¨ç†é˜¶æ®µçš„æ€§èƒ½æå‡å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå¯å‘åç»­å…³äºæµ‹è¯•æ—¶ä¼˜åŒ–ç­–ç•¥çš„ç ”ç©¶ã€‚ 
```

## thinking-longer--not-larger--enhancing-software-engineering-agents-via-scaling-test-time-compute
### Abstract
Recent advancements in software engineering agents have demonstrated
promising capabilities in automating program improvements. However, their
reliance on closed-source or resource-intensive models introduces significant
deployment challenges in private environments, prompting a critical question:
\textit{How can personally deployable open-source LLMs achieve comparable code
reasoning performance?}
  To this end, we propose a unified Test-Time Compute scaling framework that
leverages increased inference-time computation instead of larger models. Our
framework incorporates two complementary strategies: internal TTC and external
TTC. Internally, we introduce a \textit{development-contextualized trajectory
synthesis} method leveraging real-world software repositories to bootstrap
multi-stage reasoning processes, such as fault localization and patch
generation. We further enhance trajectory quality through rejection sampling,
rigorously evaluating trajectories along accuracy and complexity. Externally,
we propose a novel \textit{development-process-based search} strategy guided by
reward models and execution verification. This approach enables targeted
computational allocation at critical development decision points, overcoming
limitations of existing "end-point only" verification methods.
  Evaluations on SWE-bench Verified demonstrate our \textbf{32B model achieves
a 46\% issue resolution rate}, surpassing significantly larger models such as
DeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical
validation of the test-time scaling phenomenon within SWE agents, revealing
that \textbf{models dynamically allocate more tokens to increasingly
challenging problems}, effectively enhancing reasoning capabilities. We
publicly release all training data, models, and code to facilitate future
research. https://github.com/yingweima2022/SWE-Reasoner
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸æ‹¼æ¨¡å‹å¤§å°ï¼Œæ‹¼æ¨ç†æ—¶é•¿ï¼šç”¨æµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾æå‡è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–ç¨‹åºæ”¹è¿›ä»»åŠ¡ï¼ˆå¦‚ bug ä¿®å¤ã€åŠŸèƒ½æ–°å¢ï¼‰ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å½“å‰è¿›å±•ä¾èµ–é—­æºæˆ–èµ„æºå¯†é›†å‹å¤§æ¨¡å‹ï¼Œå¸¦æ¥éƒ¨ç½²éš¾é¢˜ï¼šä¸€æ–¹é¢ï¼Œåƒ DeepSeek V3 671B è¿™ç±»å¤§æ¨¡å‹éœ€å¤š GPU é«˜æ˜¾å­˜é…ç½®ï¼Œå¤šæ•°æœºæ„éš¾ä»¥è´Ÿæ‹…ï¼›å¦ä¸€æ–¹é¢ï¼Œé—­æºæ¨¡å‹ï¼ˆå¦‚ Claude 3.5ï¼‰é€šè¿‡ API ä½¿ç”¨æ—¶ï¼Œç§æœ‰ä»£ç ä»“åº“å­˜åœ¨éšç§é£é™©ã€‚å› æ­¤ï¼Œ**å¦‚ä½•è®©å¯ä¸ªäººéƒ¨ç½²çš„å¼€æºå°æ¨¡å‹ï¼ˆå¦‚å• GPU èƒ½è·‘çš„ 32B æ¨¡å‹ï¼‰ä¹Ÿè¾¾åˆ°ä¼˜å¼‚çš„ä»£ç æ¨ç†æ€§èƒ½**ï¼Œæˆä¸ºæ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
æœ¬æ–‡æå‡º**æµ‹è¯•æ—¶è®¡ç®—ï¼ˆTest - Time Computeï¼ŒTTCï¼‰ç¼©æ”¾æ¡†æ¶**ï¼Œä¸ä¾èµ–æ¨¡å‹å‚æ•°è§„æ¨¡æ‰©å¼ ï¼Œè€Œæ˜¯é€šè¿‡æå‡æ¨ç†æ—¶è®¡ç®—é‡æ¥å¢å¼ºèƒ½åŠ›ï¼ŒåŒ…å«å†…éƒ¨ TTC å’Œå¤–éƒ¨ TTC ä¸¤å¤§äº’è¡¥ç­–ç•¥ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå†…éƒ¨ TTC - å¼€å‘ä¸Šä¸‹æ–‡è½¨è¿¹åˆæˆä¸ä¼˜åŒ–
ä¸ºè§£å†³ç¼ºä¹çœŸå®å¤šé˜¶æ®µæ¨ç†æ•°æ®çš„é—®é¢˜ï¼Œå›¢é˜Ÿä»é«˜æ˜Ÿ GitHub ä»“åº“æŠ“å– `<issue, repository, pull - request>` ä¸‰å…ƒç»„æ„å»ºå¯æ‰§è¡ŒéªŒè¯ç¯å¢ƒï¼Œç”¨ DeepSeek R1 ç”Ÿæˆæ¶µç›–â€œä»“åº“ç†è§£ã€æ•…éšœå®šä½ã€è¡¥ä¸ç”Ÿæˆã€è¡¥ä¸éªŒè¯â€çš„å®Œæ•´æ¨ç†è½¨è¿¹ã€‚å†é€šè¿‡**å¼€å‘ä¸Šä¸‹æ–‡æ‹’ç»é‡‡æ ·**ï¼Œä»å‡†ç¡®æ€§å’Œå¤æ‚åº¦ç»´åº¦è¿‡æ»¤è½¨è¿¹ï¼ˆç­›é™¤åŸºç¡€å°æ¨¡å‹æ— éœ€ä¼˜åŒ–å°±èƒ½è§£å†³çš„é—®é¢˜ï¼‰ï¼Œä¿è¯è½¨è¿¹è´¨é‡ã€‚è®­ç»ƒæ—¶ä¿ç•™æ¯ä¸ªæ¨ç†æ­¥éª¤çš„â€œæ€è€ƒç»„ä»¶ï¼ˆè§„åˆ’ã€åæ€ã€ä¿®æ­£ï¼‰â€å’Œâ€œç­”æ¡ˆç»„ä»¶ï¼ˆæœ€ç»ˆæ–¹æ¡ˆï¼‰â€ï¼Œè®©æ¨¡å‹å†…åŒ–è½¯ä»¶å·¥ç¨‹å¤æ‚ä»»åŠ¡çš„å¤šæ­¥å†³ç­–é€»è¾‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤–éƒ¨ TTC - åŸºäºå¼€å‘æµç¨‹çš„æœç´¢ç­–ç•¥
ç°æœ‰æ–¹æ³•å¤šä»…åœ¨æœ€ç»ˆé˜¶æ®µéªŒè¯ï¼Œæœ¬æ–‡åˆ™åœ¨â€œä»“åº“ç†è§£ã€æ•…éšœå®šä½ã€è¡¥ä¸ç”Ÿæˆâ€ä¸‰ä¸ªå…³é”®å¼€å‘é˜¶æ®µé’ˆå¯¹æ€§åˆ†é…è®¡ç®—èµ„æºã€‚è®­ç»ƒ**è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰**è¯„ä¼°ä¸­é—´è¾“å‡ºï¼Œæå‰å‰ªæå·®çš„è·¯å¾„ï¼›è¡¥ä¸ç”Ÿæˆé˜¶æ®µç”¨è‡ªåŠ¨ç”Ÿæˆçš„å¤ç°ä»£ç åšæ‰§è¡ŒéªŒè¯ï¼›æœ€ç»ˆç”¨**ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰**åŸºäºéªŒè¯è¿‡çš„è¡¥ä¸å¯¹æ’åºé€‰ä¼˜ã€‚è¯¥ç­–ç•¥è®©è®¡ç®—èµ„æºèšç„¦å¼€å‘å…³é”®å†³ç­–ç‚¹ï¼Œçªç ´â€œåªéªŒè¯ç»ˆç‚¹â€çš„å±€é™ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- åœ¨ SWE - bench Verified åŸºå‡†æµ‹è¯•ä¸­ï¼Œå›¢é˜Ÿçš„ 32B æ¨¡å‹è¾¾æˆ 46% çš„é—®é¢˜è§£å†³ç‡ï¼Œè¶…è¿‡ DeepSeek R1 671Bã€OpenAI o1 ç­‰å¤§å¾—å¤šçš„æ¨¡å‹ã€‚ 
- éªŒè¯äº†è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ä¸­çš„â€œæµ‹è¯•æ—¶ç¼©æ”¾ç°è±¡â€ï¼šæ¨¡å‹é¢å¯¹æ›´å…·æŒ‘æˆ˜çš„é—®é¢˜æ—¶ï¼Œä¼šåŠ¨æ€åˆ†é…æ›´å¤š tokensï¼Œæ¨ç†èƒ½åŠ›æœ‰æ•ˆå¢å¼ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- **èŒƒå¼è½¬ç§»æ€è·¯**ï¼šè¯æ˜ä¸ç›²ç›®å †æ¨¡å‹å‚æ•°ï¼Œè½¬è€Œä¼˜åŒ–æ¨ç†æ—¶è®¡ç®—åˆ©ç”¨ï¼ˆTTC æ¡†æ¶ï¼‰ï¼Œå°æ¨¡å‹ä¹Ÿèƒ½åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸Šæ¯”è‚©å¤§æ¨¡å‹ï¼Œä¸ºèµ„æºå—é™åœºæ™¯çš„ AI éƒ¨ç½²æä¾›æ–°æ–¹å‘ã€‚ 
- **æ•°æ®ä¸è®­ç»ƒåˆ›æ–°**ï¼šä»çœŸå®è½¯ä»¶ä»“åº“æŒ–æ˜å¤šé˜¶æ®µæ¨ç†æ•°æ®ï¼Œç»“åˆæ‹’ç»é‡‡æ ·ä¼˜åŒ–è½¨è¿¹è´¨é‡ï¼Œä¸ºè®­ç»ƒâ€œç†è§£å¼€å‘æµç¨‹â€çš„æ¨¡å‹æä¾›äº†å¯å¤ç”¨çš„æ•°æ®æ„å»ºä¸è¿‡æ»¤æ–¹æ³•ã€‚ 
- **åˆ†é˜¶æ®µå†³ç­–ä¼˜åŒ–**ï¼šå¤–éƒ¨ TTC ä¸­æŒ‰å¼€å‘æµç¨‹å…³é”®èŠ‚ç‚¹åˆ†é…è®¡ç®—ã€ç”¨å¥–åŠ±æ¨¡å‹ + æ‰§è¡ŒéªŒè¯å¼•å¯¼æœç´¢ï¼Œè¿™ç§â€œæ‹†è§£ä»»åŠ¡é˜¶æ®µ + é’ˆå¯¹æ€§ç­–ç•¥â€çš„æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€å¤šæ­¥éª¤å†³ç­–çš„ AI ä»»åŠ¡ï¼ˆå¦‚è‡ªåŠ¨åŒ–è¿ç»´ã€å¤æ‚æ–‡æ¡£å¤„ç†ï¼‰ã€‚ 
- **å¼€æºç”Ÿæ€è´¡çŒ®**ï¼šå…¬å¼€è®­ç»ƒæ•°æ®ã€æ¨¡å‹å’Œä»£ç ï¼ˆhttps://github.com/yingweima2022/SWE - Reasonerï¼‰ï¼Œé™ä½åç»­ç ”ç©¶çš„å¤ç°é—¨æ§›ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## metascale--test-time-scaling-with-evolving-meta-thoughts
### Abstract
One critical challenge for large language models (LLMs) for making complex
reasoning is their reliance on matching reasoning patterns from training data,
instead of proactively selecting the most appropriate cognitive strategy to
solve a given task. Existing approaches impose fixed cognitive structures that
enhance performance in specific tasks but lack adaptability across diverse
scenarios. To address this limitation, we introduce METASCALE, a test-time
scaling framework based on meta-thoughts -- adaptive thinking strategies
tailored to each task. METASCALE initializes a pool of candidate meta-thoughts,
then iteratively selects and evaluates them using a multi-armed bandit
algorithm with upper confidence bound selection, guided by a reward model. To
further enhance adaptability, a genetic algorithm evolves high-reward
meta-thoughts, refining and extending the strategy pool over time. By
dynamically proposing and optimizing meta-thoughts at inference time, METASCALE
improves both accuracy and generalization across a wide range of tasks.
Experimental results demonstrate that MetaScale consistently outperforms
standard inference approaches, achieving an 11% performance gain in win rate on
Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,
METASCALE scales more effectively with increasing sampling budgets and produces
more structured, expert-level responses.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MetaScaleï¼šç”¨è¿›åŒ–å…ƒæ€ç»´å®ç°æµ‹è¯•æ—¶æ¨ç†èƒ½åŠ›æ‰©å±•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå®ƒä»¬ä¾èµ–ä»è®­ç»ƒæ•°æ®ä¸­åŒ¹é…æ¨ç†æ¨¡å¼ï¼Œè€Œéä¸»åŠ¨é€‰æ‹©æœ€é€‚åˆçš„è®¤çŸ¥ç­–ç•¥æ¥è§£å†³ç‰¹å®šä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ–½åŠ å›ºå®šè®¤çŸ¥ç»“æ„æå‡ç‰¹å®šä»»åŠ¡è¡¨ç°ï¼Œä½†åœ¨å¤šæ ·åœºæ™¯ä¸‹ç¼ºä¹é€‚åº”æ€§ã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œè®ºæ–‡æå‡º MetaScale æ¡†æ¶ï¼Œæ—¨åœ¨è®© LLM åœ¨æµ‹è¯•æ—¶èƒ½åŸºäºâ€œå…ƒæ€ç»´ï¼ˆmeta - thoughtsï¼‰â€åŠ¨æ€é€‚é…æ¨ç†ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º MetaScale æµ‹è¯•æ—¶æ‰©å±•æ¡†æ¶  
MetaScale åŸºäºâ€œå…ƒæ€ç»´â€â€”â€”ä¸ºæ¯ä¸ªä»»åŠ¡å®šåˆ¶çš„è‡ªé€‚åº”æ€ç»´ç­–ç•¥ã€‚æ¡†æ¶å…ˆåˆå§‹åŒ–å€™é€‰å…ƒæ€ç»´æ± ï¼Œå…ƒæ€ç»´åŒ…å«â€œè®¤çŸ¥å¿ƒæ€ï¼ˆCognitive Mindsetï¼Œå¦‚é‡‡ç”¨çš„è§’è‰²ã€ä¸“ä¸šè§†è§’ï¼‰â€å’Œâ€œé—®é¢˜è§£å†³ç­–ç•¥ï¼ˆProblem - Solving Strategyï¼ŒåŸºäºå¿ƒæ€çš„ç»“æ„åŒ–æ±‚è§£æ¨¡å¼ï¼‰â€ä¸¤éƒ¨åˆ†ï¼Œè®© LLM å…ˆæ€è€ƒâ€œå¦‚ä½•æ€è€ƒâ€å†ç”Ÿæˆå“åº”ï¼Œçªç ´é™æ€é¢„å®šä¹‰å¯å‘å¼ç­–ç•¥çš„é™åˆ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆå¤šè‡‚è€è™æœºä¸é—ä¼ ç®—æ³•å®ç°åŠ¨æ€ä¼˜åŒ–  
 - å¤šè‡‚è€è™æœºï¼ˆMABï¼‰ç®—æ³•ï¼šé‡‡ç”¨ Upper Confidence Boundï¼ˆUCBï¼‰é€‰æ‹©ç­–ç•¥ï¼Œåœ¨æ¨ç†æ—¶è¿­ä»£é€‰æ‹©å’Œè¯„ä¼°å…ƒæ€ç»´ï¼Œå¹³è¡¡æ¢ç´¢ï¼ˆå°è¯•æ–°å…ƒæ€ç»´ï¼‰ä¸åˆ©ç”¨ï¼ˆå¤ç”¨é«˜å¥–åŠ±å…ƒæ€ç»´ï¼‰ï¼Œç”±å¥–åŠ±æ¨¡å‹æŒ‡å¯¼è¯„ä¼°å“åº”è´¨é‡ã€‚  
 - é—ä¼ ç®—æ³•ï¼šå¯¹é«˜å¥–åŠ±å…ƒæ€ç»´è¿›è¡Œè¿›åŒ–ï¼Œéšæ—¶é—´ç²¾ç‚¼å’Œæ‰©å±•ç­–ç•¥æ± ï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¡†æ¶çš„é€‚åº”æ€§ï¼Œè®©å…ƒæ€ç»´èƒ½åœ¨å¤šè½®è¿­ä»£ä¸­æŒç»­ä¼˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ MetaScale æŒç»­è¶…è¶Šæ ‡å‡†æ¨ç†æ–¹æ³•ï¼šåœ¨ GPT - 4o çš„ Arena - Hard ä»»åŠ¡ä¸Šèƒœç‡æå‡ 11%ï¼›åœ¨é£æ ¼æ§åˆ¶ä»»åŠ¡ä¸­è¶…è¶Š o1 - mini 0.9%ï¼›ä¸”éšç€é‡‡æ ·é¢„ç®—å¢åŠ ï¼Œèƒ½æ›´é«˜æ•ˆæ‰©å±•ï¼Œç”Ÿæˆæ›´ç»“æ„åŒ–ã€ä¸“å®¶çº§çš„å“åº”ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - æ€ç»´èŒƒå¼å±‚é¢ï¼šå¼•å…¥â€œå…ƒæ€ç»´â€æ¦‚å¿µï¼Œè®©æ¨¡å‹å…ˆåæ€æ€è€ƒæ–¹å¼å†æ±‚è§£ï¼Œä¸º LLM æ¨ç†è¿‡ç¨‹çš„è‡ªé€‚åº”è°ƒæ§æä¾›æ–°æ€è·¯ï¼Œå¯å‘åç»­ç ”ç©¶å…³æ³¨â€œæ€è€ƒå¦‚ä½•æ€è€ƒâ€çš„å…ƒè®¤çŸ¥èƒ½åŠ›å¢å¼ºã€‚  
 - æŠ€æœ¯èåˆå±‚é¢ï¼šå°†å¤šè‡‚è€è™æœºçš„æ¢ç´¢ - åˆ©ç”¨å¹³è¡¡ã€é—ä¼ ç®—æ³•çš„è¿­ä»£è¿›åŒ–ä¸ LLM æ¨ç†ç»“åˆï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä¸è¿›åŒ–ç®—æ³•åœ¨æå‡ LLM ä»»åŠ¡é€‚é…æ€§ä¸Šçš„æ½œåŠ›ï¼Œä¸ºè·¨é¢†åŸŸæŠ€æœ¯æ•´åˆä¼˜åŒ– LLM æ€§èƒ½æä¾›å‚è€ƒã€‚  
 - åº”ç”¨ä»·å€¼å±‚é¢ï¼šåœ¨å¤šæ ·ä»»åŠ¡ä¸­æå‡å‡†ç¡®ç‡ä¸æ³›åŒ–æ€§ï¼Œä¸”èƒ½é«˜æ•ˆåˆ©ç”¨é‡‡æ ·é¢„ç®—ï¼Œå¯¹å®é™…åœºæ™¯ä¸­éœ€è¦çµæ´»æ¨ç†ã€é«˜è´¨é‡å“åº”ç”Ÿæˆçš„ä»»åŠ¡ï¼ˆå¦‚å¤æ‚é—®ç­”ã€åˆ›æ„ç”Ÿæˆç­‰ï¼‰å…·æœ‰å®è·µæŒ‡å¯¼æ„ä¹‰ã€‚

## sampling-efficient-test-time-scaling--self-estimating-the-best-of-n-sampling-in-early-decoding
### Abstract
Test-time scaling improves large language model performance by adding extra
compute during decoding. Best-of-N (BoN) sampling serves as a common scaling
technique, broadening the search space for finding better solutions from the
model distribution. However, traditional BoN requires N full generations,
leading to high GPU memory overhead and time latency. Moreover, some methods
depend on reward models, adding computational cost and limiting domain
generalization.
  In this paper, we propose Self-Truncation Best-of-N (ST-BoN), a novel
decoding method that avoids fully generating all samplings and eliminates the
need for reward models. ST-BoN introduces early sampling consistency to
estimate the most promising sample, truncating suboptimal ones to free memory
and accelerate inference. This pushes the sampling-efficient test-time scaling.
Compared to traditional BoN, ST-BoN can reduce dynamic GPU memory overhead by
over 90% and time latency by 50%, while achieving comparable or even better
performance across reasoning and open-ended domains.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£ç é˜¶æ®µé«˜æ•ˆé‡‡æ ·ï¼šST - BoN è®©å¤§æ¨¡å‹æ¨ç†åˆå¿«åˆçœ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½æœ‰å¼ºç”Ÿæˆä¸æ¨ç†èƒ½åŠ›ï¼Œä½†è‡ªå›å½’è§£ç æ˜“èšç„¦å±€éƒ¨æœ€ä¼˜ã€‚æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ï¼ˆå¦‚ Best - of - Nï¼ˆBoNï¼‰é‡‡æ ·ï¼‰èƒ½æ‹“å±•æœç´¢ç©ºé—´æ‰¾æ›´ä¼˜è§£ï¼Œç„¶è€Œä¼ ç»Ÿ BoN å­˜åœ¨ä¸¤å¤§é—®é¢˜ï¼šä¸€æ˜¯å…¨ç”Ÿæˆå¼€é”€ï¼Œéœ€å®Œæ•´ç”Ÿæˆ N ä¸ªæ ·æœ¬ï¼Œå¸¦æ¥é«˜ GPU å†…å­˜å ç”¨ä¸æ—¶é—´å»¶è¿Ÿï¼Œå¤æ‚é•¿åºåˆ—æ¨ç†æ—¶æ›´ç”šï¼›äºŒæ˜¯ä¾èµ–å¥–åŠ±æ¨¡å‹ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹éœ€é«˜è´¨é‡åé¦ˆæ•°æ®ï¼Œæˆæœ¬é«˜ä¸”é¢†åŸŸæ³›åŒ–æ€§å—é™ã€‚ä¸ºæ¨åŠ¨é‡‡æ ·é«˜æ•ˆçš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼Œæœ¬æ–‡æå‡º Self - Truncation Best - of - Nï¼ˆST - BoNï¼‰è§£ç æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç¡®å®šæœ€æ—©ä¼°è®¡æ—¶é—´ c
ä¸ºå®ç°æ•ˆç‡æœ€å¤§åŒ–æå‡ï¼Œæœ€æ—©ä¼°è®¡æ—¶é—´ c æ˜¯æ‰€æœ‰é‡‡æ ·åœ¨è¯¥æ—¶é—´ç‚¹åä¸¤ä¸¤ä¸ä¸€è‡´ï¼Œè€Œåœ¨ä¹‹å‰æ—¶é—´ç‚¹å­˜åœ¨ä¸¤ä¸¤ä¸€è‡´çš„æ—¶åˆ»ã€‚æ­¤æ—¶æ»¡è¶³ç‰¹å®šæ¡ä»¶ï¼ˆå¦‚å…¬å¼4æ‰€ç¤ºï¼Œåœ¨æ—¶é—´ c æ—¶æ‰€æœ‰ä¸åŒé‡‡æ ·çš„å‰ c ä¸ª token åºåˆ—æ— ç›¸åŒï¼Œè€Œåœ¨ c ä¹‹å‰å­˜åœ¨ä¸åŒé‡‡æ ·å‰ t ä¸ª token åºåˆ—ç›¸åŒçš„æƒ…å†µï¼‰ï¼Œä»¥æ­¤ç¡®å®šè‡ªä¼°è®¡æœ€æ—©èƒ½å¼€å±•çš„æ—¶é—´ç‚¹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰æ­¥éª¤çš„ ST - BoN è§£ç æµç¨‹
ST - BoN åŒ…å«ä¸‰æ­¥ï¼šç¬¬ä¸€æ­¥ï¼Œè‡ªå›å½’ç”Ÿæˆ N ä¸ªé‡‡æ ·ç›´åˆ°æœ€æ—©ä¼°è®¡æ—¶é—´ cï¼›ç¬¬äºŒæ­¥ï¼Œæ¯ä¸ªé‡‡æ ·åœ¨æ—¶é—´ c åç»§ç»­ç”Ÿæˆ Ï„ æ­¥ï¼ŒæœŸé—´é€æ¬¡è¿›è¡Œè‡ªä¼°è®¡æ“ä½œï¼Œç»¼åˆ Ï„ + 1 æ¬¡è‡ªä¼°è®¡ç¡®å®šæœ€ä¼˜é‡‡æ ·ï¼›ç¬¬ä¸‰æ­¥ï¼Œæˆªæ–­å‰©ä½™ N - 1 ä¸ªé‡‡æ ·ï¼Œä»…ç”Ÿæˆè‡ªä¼°è®¡æœ€ä¼˜çš„æ ·æœ¬ç›´åˆ° [EOS]ã€‚è¯¥æµç¨‹é¿å…å®Œæ•´ç”Ÿæˆæ‰€æœ‰é‡‡æ ·ï¼Œä¸”æ— éœ€å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡æ—©æœŸé‡‡æ ·ä¸€è‡´æ€§ä¼°è®¡æœ€æœ‰æ½œåŠ›æ ·æœ¬ï¼Œæˆªæ–­æ¬¡ä¼˜æ ·æœ¬é‡Šæ”¾å†…å­˜ã€åŠ é€Ÿæ¨ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è®¡ç®—æˆæœ¬æ–¹é¢ï¼Œä¸ä¼ ç»Ÿ Full - BoN ç›¸æ¯”ï¼ŒST - BoN èƒ½å‡å°‘è¶… 90% çš„åŠ¨æ€ GPU å†…å­˜å¼€é”€ä¸ 50% çš„æ—¶é—´å»¶è¿Ÿï¼›åœ¨æ¨ç†å’Œå¼€æ”¾å¼ä»»åŠ¡é¢†åŸŸçš„æ€§èƒ½æµ‹è¯•ä¸­ï¼ŒST - BoN èƒ½è¾¾åˆ°ä¸ Full - BoN ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ï¼Œåœ¨ä¸åŒé¢†åŸŸå’Œæ¨¡å‹ä¸‹ï¼Œç­‰æ•ˆæˆæœ¬æ—¶è¡¨ç°æ›´ä¼˜ï¼Œä½æˆæœ¬æ—¶ä¹Ÿèƒ½å®ç°ç›¸ä¼¼æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»†ç²’åº¦æ¶ˆèå®éªŒéªŒè¯äº†æ—©æœŸè‡ªä¼°è®¡ä¸æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„ä¸€è‡´æ€§ï¼Œè¿˜è¯¦ç»†è®¨è®ºè¶…å‚æ•°ä¸é²æ£’æ€§ï¼Œç¡®è®¤æ–¹æ³•çš„æ­£ç¡®æ€§ä¸æ³›åŒ–æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼ŒST - BoN å¯å‘æˆ‘ä»¬åœ¨å¤§æ¨¡å‹è§£ç ç­‰éœ€é‡‡æ ·ä¼˜åŒ–çš„åœºæ™¯ä¸­ï¼Œå¯æ¢ç´¢æ—©æœŸé˜¶æ®µçš„è‡ªä¼°è®¡æœºåˆ¶ï¼Œåˆ©ç”¨æ¨¡å‹å†…éƒ¨çŸ¥è¯†å‡å°‘ä¸å¿…è¦è®¡ç®—ï¼Œä¸ºæå‡æ•ˆç‡æä¾›æ–°æ€è·¯ï¼›åœ¨å·¥ç¨‹è½åœ°å±‚é¢ï¼Œå…¶å¯¹å†…å­˜ä¸æ—¶é—´æˆæœ¬çš„å¤§å¹…ä¼˜åŒ–æ€è·¯ï¼Œå¯æŒ‡å¯¼å¤§æ¨¡å‹æ¨ç†æœåŠ¡ç­‰åœºæ™¯çš„æ€§èƒ½ä¼˜åŒ–ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼›ä»ç ”ç©¶è§†è§’ï¼Œå…¶å¯¹é‡‡æ ·æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡çš„æ¢ç´¢ï¼Œä¸ºåç»­æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ç›¸å…³ç ”ç©¶æä¾›äº†æ–°èŒƒå¼ä¸å®éªŒéªŒè¯æ–¹å‘ï¼Œå¦‚è¿›ä¸€æ­¥æŒ–æ˜ä¸åŒä»»åŠ¡ä¸‹æœ€æ—©ä¼°è®¡æ—¶é—´ c å’Œç¼“å†²çª—å£ Ï„ ç­‰è¶…å‚æ•°çš„ä¼˜åŒ–ç­–ç•¥ç­‰ã€‚

## agentrm--enhancing-agent-generalization-with-reward-modeling
### Abstract
Existing LLM-based agents have achieved strong performance on held-in tasks,
but their generalizability to unseen tasks remains poor. Hence, some recent
work focus on fine-tuning the policy model with more diverse tasks to improve
the generalizability. In this work, we find that finetuning a reward model to
guide the policy model is more robust than directly finetuning the policy
model. Based on this finding, we propose AgentRM, a generalizable reward model,
to guide the policy model for effective test-time search. We comprehensively
investigate three approaches to construct the reward model, including explicit
reward modeling, implicit reward modeling and LLM-as-a-judge. We then use
AgentRM to guide the answer generation with Best-of-N sampling and step-level
beam search. On four types of nine agent tasks, AgentRM enhances the base
policy model by $8.8$ points on average, surpassing the top general agent by
$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding
greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the
specializability, AgentRM can also boost a finetuned policy model and
outperform the top specialized agent by $11.4$ on three held-in tasks. Further
analysis verifies its effectiveness in test-time scaling. Codes will be
released to facilitate the research in this area.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AgentRMï¼šç”¨å¥–åŠ±å»ºæ¨¡æå‡æ™ºèƒ½ä½“æ³›åŒ–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“åœ¨å¤æ‚äº¤äº’ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ™ºèƒ½ä½“åœ¨å·²çŸ¥ä»»åŠ¡ï¼ˆheld - in tasksï¼‰ä¸Šè¡¨ç°å°šå¯ï¼Œå¯¹ unseen tasksï¼ˆæœªè§è¿‡çš„ä»»åŠ¡ï¼‰çš„æ³›åŒ–èƒ½åŠ›å´å¾ˆå·®ã€‚ä»¥å¾€ä¸€äº›å·¥ä½œè¯•å›¾é€šè¿‡åœ¨æ›´å¤šæ ·åŒ–ä»»åŠ¡ä¸Šå¾®è°ƒç­–ç•¥æ¨¡å‹ï¼ˆpolicy modelï¼‰æ¥æå‡æ³›åŒ–æ€§ï¼Œç„¶è€Œç ”ç©¶å‘ç°ç›´æ¥å¾®è°ƒç­–ç•¥æ¨¡å‹å­˜åœ¨ç¼ºé™·ï¼šå¾®è°ƒç­–ç•¥æ¨¡å‹ä¼šå¢åŠ è§è¿‡çš„åŠ¨ä½œ token æ¦‚ç‡ï¼Œé™ä½æœªè§è¿‡åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¯¼è‡´åœ¨ unseen tasks ä¸Šæ€§èƒ½ä¸‹é™ï¼ˆå¦‚å›¾ 1(a) æ‰€ç¤ºï¼Œå¾®è°ƒç­–ç•¥æ¨¡å‹å held - out tasks æ€§èƒ½ä¸¥é‡é€€åŒ–ï¼‰ã€‚äºæ˜¯æœ¬æ–‡æå‡ºçŒœæƒ³ï¼šå¾®è°ƒå¥–åŠ±æ¨¡å‹ï¼ˆreward modelï¼‰æ¥å¼•å¯¼ç­–ç•¥æ¨¡å‹ï¼Œä¼šæ¯”ç›´æ¥å¾®è°ƒç­–ç•¥æ¨¡å‹æ›´ç¨³å¥ï¼Œå› ä¸ºå¥–åŠ±å‡½æ•°çš„å›å½’è®­ç»ƒç›®æ ‡å¯¹åŠ¨ä½œ token ç‰¹å®šåˆ†å¸ƒçš„æ•æ„Ÿæ€§æ›´ä½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º AgentRM æ¡†æ¶
AgentRM æ˜¯ä¸€ä¸ªå¯æ³›åŒ–çš„å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºåœ¨æµ‹è¯•æ—¶å¼•å¯¼ç­–ç•¥æ¨¡å‹è¿›è¡Œæœ‰æ•ˆæœç´¢ã€‚ä¸ºæ„å»ºè¯¥å¥–åŠ±æ¨¡å‹ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä¸‰ç§æœ‰ä»£è¡¨æ€§çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼š
 - **æ˜¾å¼å¥–åŠ±å»ºæ¨¡ï¼ˆExplicit reward modelingï¼‰**ï¼šå€ŸåŠ©æ ‘æœç´¢ï¼ˆå¦‚ç±»è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•ï¼‰è¿›è¡Œè‡ªåŠ¨çš„è¿‡ç¨‹å¥–åŠ±æ ‡æ³¨ï¼Œå°†ç¨€ç–çš„æœ€ç»ˆç»“æœå¥–åŠ±ä»¥å¯è§£é‡Šçš„æ–¹å¼åˆ†é…åˆ°æ¯ä¸ªæ­¥éª¤ï¼ŒæŠŠè¿‡ç¨‹å¥–åŠ±å®šä¹‰ä¸º Q - valueï¼ˆä»æŸä¸€çŠ¶æ€å¼€å§‹çš„é¢„æœŸç´¯ç§¯å¥–åŠ±ï¼‰ï¼Œå¹¶é€šè¿‡å¯¹æœç´¢è½¨è¿¹æ„å»ºæ ‘ç»“æ„ç­‰æ–¹å¼è®¡ç®—ã€‚
 - **éšå¼å¥–åŠ±å»ºæ¨¡ï¼ˆImplicit reward modelingï¼‰**ï¼šæ— éœ€å¯¹æ­¥éª¤çº§å¥–åŠ±è¿›è¡Œæ ‡æ³¨ï¼Œè€Œæ˜¯é€šè¿‡å¯¹ç»“æœå¥–åŠ±è®­ç»ƒæ¥éšå¼å­¦ä¹ æ­¥éª¤çº§å¥–åŠ±ã€‚
 - **LLM - as - a - judge**ï¼šè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œä¾èµ–å¤§è¯­è¨€æ¨¡å‹æœ¬èº«çš„é€šç”¨è¯„åˆ¤èƒ½åŠ›ï¼Œç›´æ¥æç¤º LLM æ¥è¯„ä¼°æ™ºèƒ½ä½“çš„è½¨è¿¹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæµ‹è¯•æ—¶æœç´¢ç­–ç•¥
ä½¿ç”¨ AgentRM é€šè¿‡ Best - of - N sampling å’Œ step - level beam search æ¥å¼•å¯¼ç­”æ¡ˆç”Ÿæˆï¼Œä»¥æ­¤å¢å¼ºç­–ç•¥æ¨¡å‹åœ¨ unseen tasks ä¸Šçš„å†³ç­–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå…ˆé€šè¿‡è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼‰å¾—åˆ°å…·æœ‰åŸºæœ¬ä»»åŠ¡èƒ½åŠ›çš„åˆå§‹ç­–ç•¥ Ï€initï¼Œç”¨äºæ”¶é›†é«˜è´¨é‡çŠ¶æ€ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹è®­ç»ƒç­‰ç¯èŠ‚å¥ åŸºã€‚è¡Œä¸ºå…‹éš†æ˜¯åœ¨ä¸“å®¶è½¨è¿¹ Dexpert ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ŒæŸå¤±å‡½æ•°å¦‚å…¬å¼ï¼ˆ1ï¼‰æ‰€ç¤ºï¼Œä»¥æ­¤å¾—åˆ°åˆå§‹ç­–ç•¥æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åŒ…æ‹¬ç½‘é¡µå¯¼èˆªã€å…·èº«è§„åˆ’ã€æ–‡æœ¬æ¸¸æˆå’Œå·¥å…·ä½¿ç”¨è¿™å››ç±»å…±ä¹ä¸ªæ™ºèƒ½ä½“ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼š
 - æ˜¾å¼å¥–åŠ±å»ºæ¨¡åœ¨è¿™äº›ä»»åŠ¡ä¸­æŒç»­å–å¾—æœ€æ˜¾è‘—çš„æå‡ï¼ŒAgentRM å¹³å‡èƒ½ä½¿åŸºç¡€ç­–ç•¥æ¨¡å‹æå‡ 8.8 ä¸ªç‚¹ï¼Œè¶…è¿‡é¡¶å°–çš„é€šç”¨æ™ºèƒ½ä½“ 4.0 ä¸ªç‚¹ã€‚
 - ä½“ç°äº†å¼±åˆ°å¼ºçš„æ³›åŒ–æ€§ï¼Œåœ¨ LLaMA - 3 - 8B ç­–ç•¥æ¨¡å‹é‡‡æ ·çŠ¶æ€ä¸Šè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œåº”ç”¨åˆ° LLaMA - 3 - 70B ç­–ç•¥æ¨¡å‹æ—¶ï¼Œèƒ½å¸¦æ¥ 12.6 ä¸ªç‚¹çš„æå‡ã€‚
 - åœ¨ä¸“é¡¹èƒ½åŠ›æ–¹é¢ï¼ŒAgentRM ä¹Ÿèƒ½æå‡å¾®è°ƒåçš„ç­–ç•¥æ¨¡å‹ï¼Œåœ¨ä¸‰ä¸ª held - in tasks ä¸Šè¶…è¿‡é¡¶å°–çš„ç‰¹å®šä»»åŠ¡æ™ºèƒ½ä½“ 11.4 ä¸ªç‚¹ã€‚æ­¤å¤–ï¼Œå¯¹è®­ç»ƒæ•°æ®è§„æ¨¡è¶‹åŠ¿çš„åˆ†æä»¥åŠå¯¹çŠ¶æ€è¡¨ç¤ºçš„æ¶ˆèå®éªŒç­‰è¿›ä¸€æ­¥éªŒè¯äº†æ˜¾å¼å¥–åŠ±å»ºæ¨¡çš„æœ‰æ•ˆæ€§ç­‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - æ€è·¯åˆ›æ–°ï¼šæå‡ºé€šè¿‡å¥–åŠ±æ¨¡å‹å¼•å¯¼ç­–ç•¥æ¨¡å‹æ¥æå‡æ³›åŒ–æ€§ï¼Œä¸ºè§£å†³æ™ºèƒ½ä½“æ³›åŒ–éš¾é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œä¸åŒäºä»¥å¾€ç›´æ¥å¾®è°ƒç­–ç•¥æ¨¡å‹çš„æ–¹å¼ï¼Œå¼€è¾Ÿäº†åˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„æ–°æ–¹å‘ã€‚
 - æ–¹æ³•å…¨é¢æ€§ï¼šå…¨é¢æ¢ç´¢ä¸‰ç§å¥–åŠ±å»ºæ¨¡æ–¹å¼ï¼Œä¸ºåç»­ç ”ç©¶è€…åœ¨å¥–åŠ±æ¨¡å‹æ„å»ºä¸Šæä¾›äº†ä¸°å¯Œçš„å‚è€ƒèŒƒå¼ï¼Œæ— è®ºæ˜¯æƒ³é‡‡ç”¨æœ‰ç›‘ç£æ ‡æ³¨å¼çš„æ˜¾å¼å»ºæ¨¡ï¼Œè¿˜æ˜¯éšå¼å­¦ä¹ æˆ–è€…åˆ©ç”¨ LLM è¯„åˆ¤èƒ½åŠ›çš„æ–¹å¼ï¼Œéƒ½æœ‰å¯¹åº”çš„å®è·µè·¯å¾„å‚è€ƒã€‚
 - å®éªŒä¸°å¯Œæ€§ï¼šåœ¨å¤šç±»ä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œä¸”å¯¹æ³›åŒ–æ€§ã€ä¸“é¡¹èƒ½åŠ›ã€æµ‹è¯•æ—¶ç¼©æ”¾ç­‰å¤šæ–¹é¢è¿›è¡Œåˆ†æï¼Œå…¶å®éªŒè®¾è®¡å’Œåˆ†ææ€è·¯å¯ä¸ºç›¸å…³é¢†åŸŸå®éªŒç ”ç©¶æä¾›å€Ÿé‰´ï¼Œå¸®åŠ©åç»­å·¥ä½œæ›´å…¨é¢åœ°è¯„ä¼°æ–¹æ³•æœ‰æ•ˆæ€§ã€‚åŒæ—¶è®ºæ–‡è¿˜è®¡åˆ’å‘å¸ƒä»£ç ï¼Œå°†ä¸ºè¯¥é¢†åŸŸç ”ç©¶æä¾›å·¥å…·å±‚é¢çš„ä¾¿åˆ©ï¼Œä¿ƒè¿›ç›¸å…³ç ”ç©¶å‘å±•ã€‚

## linguistic-generalizability-of-test-time-scaling-in-mathematical-reasoning
### Abstract
Scaling pre-training compute has proven effective for achieving
mulitlinguality, but does the same hold for test-time scaling? In this work, we
introduce MCLM, a multilingual math benchmark featuring competition-level
problems in 55 languages. We test three test-time scaling methods-Outcome
Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing
(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for
extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM
achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although
"thinking LLMs" have recently garnered significant attention, we find that
their performance is comparable to traditional scaling methods like best-of-N
once constrained to similar levels of inference FLOPs. Moreover, while BF
yields a 20-point improvement on English AIME, it provides only a 1.94-point
average gain across other languages-a pattern consistent across the other
test-time scaling methods we studied-higlighting that test-time scaling may not
generalize as effectively to multilingual tasks. To foster further research, we
release MCLM, MR1-1.5B, and evaluation results.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ•°å­¦æ¨ç†ä¸­æµ‹è¯•æ—¶ç¼©æ”¾çš„è¯­è¨€æ³›åŒ–æ€§ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡è®¡ç®—é‡ç¼©æ”¾å®ç°äº†å¤šè¯­è¨€èƒ½åŠ›æå‡ï¼Œä½†æµ‹è¯•æ—¶ç¼©æ”¾æ˜¯å¦ä¹Ÿèƒ½å¸¦æ¥ç±»ä¼¼è·¨è¯­è¨€æ”¶ç›Šï¼Ÿæ­¤å‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ç­‰é¢†åŸŸæ¢ç´¢è¾ƒå°‘ï¼Œä¸”æ•°å­¦æ¨ç†å› æœç´¢ç©ºé—´å¤§æ›´å…·æŒ‘æˆ˜ï¼ŒåŒæ—¶ç°æœ‰å¤šè¯­è¨€æ•°å­¦åŸºå‡†è¦ä¹ˆç®€å•é—®é¢˜é¥±å’Œã€è¦ä¹ˆè¯­è¨€è¦†ç›–æœ‰é™ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç©¶æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åœ¨å¤šè¯­è¨€æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„è¯­è¨€æ³›åŒ–æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤šè¯­è¨€æ•°å­¦åŸºå‡†MCLM  
åˆ›å»ºäº†MCLMï¼ˆMultilingual Competition Level Mathï¼‰å¤šè¯­è¨€æ•°å­¦æ¨ç†åŸºå‡†ï¼Œæ¶µç›–55ç§è¯­è¨€ï¼ŒåŒ…å«æœºå™¨ç¿»è¯‘å’Œäººå·¥æ ‡æ³¨çš„ç«èµ›çº§æ•°å­¦é¢˜ï¼Œåˆ†ä¸ºMT - MATH100ã€MT - AIME2024ã€M - IMOã€M - MOå››ä¸ªå­é›†ï¼Œè¦†ç›–ä¸åŒéš¾åº¦ä¸è¯­è¨€æ¥æºï¼Œèƒ½æ›´å…¨é¢è¯„ä¼°å¤šè¯­è¨€æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæµ‹è¯•ä¸‰ç±»æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•  
é€‰å–Outcome Reward Modelingï¼ˆORMï¼‰ã€Process Reward Modelingï¼ˆPRMï¼‰ã€Budget Forcingï¼ˆBFï¼‰ä¸‰ç§æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œåœ¨Qwen2.5 - 1.5B Mathå’Œè‡ªç ”å¤šè¯­è¨€LLMï¼ˆMR1 - 1.5Bï¼‰ä¸Šæµ‹è¯•ï¼Œæ¢ç©¶ä¸åŒæ–¹æ³•åœ¨å¤šè¯­è¨€åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®­ç»ƒå¤šè¯­è¨€æ€è€ƒå‹LLM MR1 - 1.5B  
åŸºäºDeepseek - R1 - 1.5Bï¼Œç”¨10ä¸‡æ¡ç”±GPT - 4oç¿»è¯‘çš„R1 - distilledå®ä¾‹è®­ç»ƒå¾—åˆ°MR1 - 1.5Bï¼Œè™½ä»…1.5Bå‚æ•°ï¼Œä½†åœ¨å¤šè¯­è¨€æ•°å­¦æ¨ç†ä¸Šèƒ½ä¸GPT - 4o - Miniåª²ç¾ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. æ€§èƒ½å¯¹æ¯”ï¼šQwen2.5 - 1.5B Mathç»“åˆORMåœ¨MCLMä¸Šå¾—35.8åˆ†ï¼ŒMR1 - 1.5Bç»“åˆBFå¾—35.2åˆ†ï¼›å½“æ¨ç†è®¡ç®—é‡ï¼ˆFLOPsï¼‰ç›¸è¿‘æ—¶ï¼Œâ€œæ€è€ƒå‹LLMâ€è¡¨ç°ä¸best - of - Nç­‰ä¼ ç»Ÿç¼©æ”¾æ–¹æ³•ç›¸å½“ã€‚  
2. è¯­è¨€æ³›åŒ–æ€§ï¼šä»¥BFä¸ºä¾‹ï¼Œåœ¨è‹±è¯­AIMEä»»åŠ¡ä¸Šæå‡20åˆ†ï¼Œä½†åœ¨å…¶ä»–è¯­è¨€ä¸Šå¹³å‡ä»…æå‡1.94åˆ†ï¼Œå…¶ä»–æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ä¹Ÿæœ‰ç±»ä¼¼æ¨¡å¼ï¼Œè¯´æ˜æµ‹è¯•æ—¶ç¼©æ”¾åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šæ³›åŒ–æ•ˆæœä¸ä½³ï¼Œéš¾ä»¥åœ¨å¤šè¯­è¨€é—´ç¨³å®šæå‡æ€§èƒ½ã€‚  
3. ä¸åŒä»»åŠ¡éš¾åº¦ï¼šORMå’ŒPRMåœ¨è¾ƒç®€å•æ•°æ®é›†æœ‰æ˜æ˜¾å¢ç›Šï¼Œä½†åœ¨é«˜éš¾åº¦ä»»åŠ¡å’Œä¸åŒè¯­è¨€é—´æå‡å¾®å¼±ä¸”ä¸ä¸€è‡´ï¼›BFä»…å¯¹è‹±è¯­éš¾é¢˜æå‡æ˜æ˜¾ï¼Œå¯¹å…¶ä»–è¯­è¨€å½±å“å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŸºå‡†æ„å»ºï¼šMCLMä¸ºå¤šè¯­è¨€å¤æ‚æ•°å­¦æ¨ç†ç ”ç©¶æä¾›äº†é«˜è´¨é‡åŸºå‡†ï¼Œå…¶æ¶µç›–å¤šè¯­è¨€ã€å¤šéš¾åº¦ã€å¤šæ¥æºçš„æ„å»ºæ€è·¯ï¼Œå¯ä¸ºå…¶ä»–å¤šè¯­è¨€ä»»åŠ¡åŸºå‡†å»ºè®¾æä¾›å‚è€ƒã€‚  
2. æ–¹æ³•æ¢ç´¢ï¼šå¯¹ä¸‰ç±»æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åœ¨å¤šè¯­è¨€åœºæ™¯çš„ç³»ç»Ÿæµ‹è¯•ï¼Œæ¸…æ™°å±•ç°äº†ä¸åŒæ–¹æ³•åœ¨å¤šè¯­è¨€ä»»åŠ¡çš„è¡¨ç°å·®å¼‚ï¼Œä¸ºåç»­ä¼˜åŒ–æµ‹è¯•æ—¶ç­–ç•¥ä»¥é€‚é…å¤šè¯­è¨€ä»»åŠ¡æä¾›äº†å®éªŒä¾æ®ã€‚  
3. æ¨¡å‹è®­ç»ƒï¼šMR1 - 1.5Bçš„è®­ç»ƒæ–¹å¼è¯æ˜å°å‚æ•°æ¨¡å‹é€šè¿‡åˆé€‚æ•°æ®ä¹Ÿèƒ½åœ¨å¤šè¯­è¨€æ¨ç†ä»»åŠ¡æœ‰ç«äº‰åŠ›ï¼Œä¸ºèµ„æºæœ‰é™ä¸‹çš„å¤šè¯­è¨€æ¨¡å‹è®­ç»ƒæä¾›äº†æ€è·¯ã€‚

## process-reward-models-for-llm-agents--practical-framework-and-directions
### Abstract
We introduce Agent Process Reward Models (AgentPRM), a simple and scalable
framework for training LLM agents to continually improve through interactions.
AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo
rollouts to compute reward targets and optimize policies. It requires minimal
modifications to existing RLHF pipelines, making it easy to integrate at scale.
Beyond AgentPRM, we propose InversePRM, which learns process rewards directly
from demonstrations without explicit outcome supervision. We also explore key
challenges and opportunities, including exploration, process reward shaping,
and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that
small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o
baselines, and analyze test-time scaling, reward hacking, and more. Our code is
available at: https://github.com/sanjibanc/agent_prm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLMæ™ºèƒ½ä½“è®­ç»ƒæ–°èŒƒå¼ï¼šAgentPRMä¸InversePRMæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“åœ¨å†³ç­–ç±»ä»»åŠ¡ï¼ˆå¦‚ç½‘é¡µå¯¼èˆªã€æœºå™¨äººæ§åˆ¶ã€äº¤äº’å¼ä»£ç ç”Ÿæˆï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å½“å‰ä¾èµ–äººå·¥æç¤ºæˆ–æœ‰ç›‘ç£å¾®è°ƒçš„æ–¹å¼å­˜åœ¨æ˜æ˜¾å±€é™ï¼šæç¤ºå·¥ç¨‹éœ€å¤§é‡äººå·¥æŠ•å…¥ä¸”æ— æ³•è‡ªä¸»è¿­ä»£ä¼˜åŒ–ï¼›æœ‰ç›‘ç£å¾®è°ƒå—é™äºæ¼”ç¤ºè´¨é‡ï¼Œæµ‹è¯•é˜¶æ®µä¹Ÿç¼ºä¹è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½èƒ½é€šè¿‡ç»éªŒä¼˜åŒ–ç­–ç•¥ï¼Œå´é¢ä¸´é•¿æ—¶ç¨‹å†³ç­–ï¼ˆå¤šæ­¥éª¤æ¨ç†+ç»“æ„åŒ–å¤štokenè¾“å‡ºï¼‰ã€ç¨€ç–å¥–åŠ±ï¼ˆåé¦ˆå»¶è¿Ÿå¯¼è‡´ credit assignment éš¾é¢˜ï¼‰ä¸é«˜æ ·æœ¬å¤æ‚åº¦ç­‰æŒ‘æˆ˜ï¼Œéš¾ä»¥ç›´æ¥è½åœ°ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLMæ™ºèƒ½ä½“åœ¨å°‘äººå·¥ç›‘ç£ä¸‹é€šè¿‡äº¤äº’è‡ªä¸»æå‡ï¼Œæˆä¸ºæ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAgent Process Reward Modelsï¼ˆAgentPRMï¼‰æ¡†æ¶  
AgentPRMåŸºäºè½»é‡çš„â€œæ¼”å‘˜-è¯„è®ºå®¶â€èŒƒå¼ï¼Œä¸ºLLMæ™ºèƒ½ä½“æä¾›æŒç»­äº¤äº’ä¼˜åŒ–èƒ½åŠ›ã€‚å®ƒé€šè¿‡**å¼‚æ­¥è’™ç‰¹å¡æ´›rollouts**è‡ªåŠ¨ç”Ÿæˆè¿‡ç¨‹å¥–åŠ±ï¼ˆPRMï¼‰çš„ç›‘ç£ç›®æ ‡ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å¥–åŠ±ï¼›å¹¶é‡‡ç”¨**è¿­ä»£è®­ç»ƒ**æœºåˆ¶ï¼Œè®©PRMï¼ˆè¯„è®ºå®¶è§’è‰²ï¼Œæä¾›ç»†ç²’åº¦ä¸­é—´åé¦ˆï¼‰ä¸æ™ºèƒ½ä½“ç­–ç•¥ï¼ˆæ¼”å‘˜è§’è‰²ï¼‰ç›¸äº’è¿­ä»£ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶ä»…éœ€å¯¹ç°æœ‰RLHFæµæ°´çº¿åšæå°æ”¹åŠ¨ï¼Œå°±èƒ½è§„æ¨¡åŒ–é›†æˆï¼Œè§£å†³é•¿æ—¶ç¨‹å†³ç­–ä¸­ç¨€ç–å¥–åŠ±ä¸æ ·æœ¬æ•ˆç‡é—®é¢˜â€”â€”PRMé€šè¿‡è¯„ä¼°ä¸­é—´åŠ¨ä½œè€Œéä¾èµ–æœ€ç»ˆç¨€ç–å¥–åŠ±ï¼Œå¤§å¹…æå‡æ ·æœ¬åˆ©ç”¨æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºInversePRMï¼šæ— æ˜¾å¼ç»“æœç›‘ç£çš„è¿‡ç¨‹å¥–åŠ±å­¦ä¹   
åœ¨AgentPRMåŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºInversePRMã€‚å®ƒ**ç›´æ¥ä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ è¿‡ç¨‹å¥–åŠ±**ï¼Œæ— éœ€æ˜¾å¼çš„â€œç»“æœå¥–åŠ±â€ç›‘ç£ã€‚é€šè¿‡åŒºåˆ†â€œä¸“å®¶ä¼˜è´¨è½¨è¿¹ï¼ˆæ­£æ ·æœ¬ï¼‰â€ä¸â€œæ™ºèƒ½ä½“ç”Ÿæˆçš„éä¼˜è´¨è½¨è¿¹ï¼ˆè´Ÿæ ·æœ¬ï¼‰â€æ¥è®­ç»ƒPRMï¼Œå†ç”¨PRMå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚ç›¸æ¯”AgentPRMï¼ŒInversePRMåœ¨ä¸å¢åŠ å¤æ‚åº¦çš„å‰æä¸‹ï¼Œå®ç°äº†æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¢ç´¢è§„æ¨¡åŒ–ä¸å®ç”¨æŒ‘æˆ˜çš„è§£æ³•æ–¹å‘  
é’ˆå¯¹AgentPRMè§„æ¨¡åŒ–è¿‡ç¨‹ä¸­çš„æ¢ç´¢ï¼ˆExplorationï¼‰ã€è¿‡ç¨‹å¥–åŠ±å¡‘é€ ï¼ˆProcess Reward Shapingï¼‰ã€æ¨¡å‹é¢„æµ‹æ¨ç†ï¼ˆModel-Predictive Reasoningï¼‰ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè®ºæ–‡ç»“åˆç»å…¸RLæŠ€æœ¯ï¼ˆå¦‚é‡ç½®åˆ†å¸ƒã€å¥–åŠ±å¡‘é€ ï¼‰ä¸LLMé©±åŠ¨ç­–ç•¥ï¼ˆå¦‚å¼•å¯¼å¼æ¢ç´¢ã€æ¨¡å‹é¢„æµ‹æ¨ç†ï¼‰å±•å¼€åˆ†æï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜æ–¹å‘ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨æ–‡æœ¬æ¸¸æˆåŸºå‡†æµ‹è¯•ALFWorldä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼š  
- AgentPRMè®­ç»ƒçš„30äº¿å‚æ•°å°æ¨¡å‹ï¼Œæ€§èƒ½**è¶…è¶Šå¼ºåŸºçº¿GPT-4o**ï¼›å¹¶å¯¹è®­ç»ƒæ›²çº¿ã€æµ‹è¯•æ—¶æ¨¡å‹ç¼©æ”¾ã€å¥–åŠ±é»‘å®¢æ”»å‡»ï¼ˆReward Hackingï¼‰ã€ç»å¯¹/ç›¸å¯¹æŸå¤±ç­‰ç»´åº¦åšäº†æ·±å…¥åˆ†æã€‚  
- InversePRMåœ¨å•è½®è¿­ä»£ä¸­å°±èƒ½æ¥è¿‘ä¸“å®¶æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä¸”æ¯”AgentPRMæ›´å…·æ ·æœ¬æ•ˆç‡ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡è½»é‡åŒ–ä¸å…¼å®¹æ€§ï¼šAgentPRMå’ŒInversePRMå¯¹ç°æœ‰RLHFæµæ°´çº¿ä¾µå…¥æ€§æå°ï¼Œä»…æ–°å¢â€œè‡ªåŠ¨å¥–åŠ±æ ‡æ³¨â€ç¯èŠ‚ï¼Œä¾¿äºå·¥ä¸šç•Œå¿«é€Ÿé›†æˆå¤ç”¨ã€‚  
2. è§£å†³RLè½åœ°LLMæ™ºèƒ½ä½“çš„å…³é”®ç—›ç‚¹ï¼šé€šè¿‡è¿‡ç¨‹å¥–åŠ±æ‹†è§£é•¿æ—¶ç¨‹ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œç”¨è’™ç‰¹å¡æ´›rolloutsè‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å·ï¼Œé™ä½å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä¸ºRLä¸LLMç»“åˆæä¾›äº†æ›´é«˜æ•ˆçš„èŒƒå¼ã€‚  
3. å¼€æºä¸å¯å¤ç°ï¼šä»£ç å¼€æºï¼ˆhttps://github.com/sanjibanc/agent_prmï¼‰ï¼ŒåŸºäºGymå°è£…çš„è½»é‡ wrapper èƒ½å¿«é€Ÿå¯¹æ¥OpenInstructç­‰ç°æœ‰æ¡†æ¶ï¼Œé™ä½ç ”ç©¶ä¸å·¥ç¨‹é—¨æ§›ã€‚  
4. æŒ‘æˆ˜ä¸æ–¹å‘çš„å‰ç»æ€§ï¼šå¯¹æ¢ç´¢ã€å¥–åŠ±å¡‘é€ ã€æ¨¡å‹é¢„æµ‹æ¨ç†ç­‰æ–¹å‘çš„åˆ†æï¼Œä¸ºåç»­ä¼˜åŒ–LLMæ™ºèƒ½ä½“çš„é•¿æ—¶å†³ç­–ã€æ ·æœ¬æ•ˆç‡ç­‰é—®é¢˜æä¾›äº†æ¸…æ™°çš„ç ”ç©¶è·¯å¾„å‚è€ƒã€‚  

## can-1b-llm-surpass-405b-llm--rethinking-compute-optimal-test-time-scaling
### Abstract
Test-Time Scaling (TTS) is an important method for improving the performance
of Large Language Models (LLMs) by using additional computation during the
inference phase. However, current studies do not systematically analyze how
policy models, Process Reward Models (PRMs), and problem difficulty influence
TTS. This lack of analysis limits the understanding and practical use of TTS
methods. In this paper, we focus on two core questions: (1) What is the optimal
approach to scale test-time computation across different policy models, PRMs,
and problem difficulty levels? (2) To what extent can extended computation
improve the performance of LLMs on complex tasks, and can smaller language
models outperform larger ones through this approach? Through comprehensive
experiments on MATH-500 and challenging AIME24 tasks, we have the following
observations: (1) The compute-optimal TTS strategy is highly dependent on the
choice of policy model, PRM, and problem difficulty. (2) With our
compute-optimal TTS strategy, extremely small policy models can outperform
larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.
Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM
surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher
inference efficiency. These findings show the significance of adapting TTS
strategies to the specific characteristics of each task and model and indicate
that TTS is a promising approach for enhancing the reasoning abilities of LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°æ¨¡å‹ä¹Ÿèƒ½é€†è¢­ï¼Ÿé‡æ–°æ€è€ƒæ¨ç†æ—¶è®¡ç®—æœ€ä¼˜çš„æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä½œä¸ºæå‡æ¨ç†æ€§èƒ½çš„é‡è¦æ–¹æ³•ï¼Œå½“å‰ç ”ç©¶æœªç³»ç»Ÿåˆ†æç­–ç•¥æ¨¡å‹ã€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰å’Œé—®é¢˜éš¾åº¦å¯¹å…¶çš„å½±å“ï¼Œé™åˆ¶äº†å¯¹TTSæ–¹æ³•çš„ç†è§£ä¸å®è·µåº”ç”¨ã€‚æœ¬æ–‡èšç„¦ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼šä¸åŒç­–ç•¥æ¨¡å‹ã€PRMså’Œé—®é¢˜éš¾åº¦ä¸‹ï¼Œå¦‚ä½•æœ€ä¼˜ç¼©æ”¾æµ‹è¯•æ—¶è®¡ç®—ï¼›æ‰©å±•è®¡ç®—èƒ½åœ¨å¤šå¤§ç¨‹åº¦æå‡å¤æ‚ä»»åŠ¡æ€§èƒ½ï¼Œå°æ¨¡å‹èƒ½å¦å€Ÿæ­¤è¶…è¶Šå¤§æ¨¡å‹ï¼Œä»¥æ­¤å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢è¯„ä¼°TTSæ–¹æ³•
ä½¿ç”¨å¤šç§å‰æ²¿ç­–ç•¥æ¨¡å‹ã€å¤šä¸ªPRMsã€ä¸åŒç¼©æ”¾æ–¹æ³•ï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ˆå¦‚MATH - 500å’ŒAIME24ï¼‰ä¸Šå¯¹ä¸åŒTTSæ–¹æ³•è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæ¶µç›–ä»0.5Båˆ°72Bä¸åŒè§„æ¨¡çš„ç­–ç•¥æ¨¡å‹ä¸PRMsã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºå¥–åŠ±æ„ŸçŸ¥çš„è®¡ç®—æœ€ä¼˜TTS
åˆ†æTTSè¿‡ç¨‹ä¸­å¥–åŠ±çš„å½±å“ï¼Œå¼•å…¥å¥–åŠ±æ„ŸçŸ¥çš„è®¡ç®—æœ€ä¼˜TTSï¼Œè¡¨æ˜è®¡ç®—æœ€ä¼˜ç¼©æ”¾ç­–ç•¥ä¼šéšç­–ç•¥æ¨¡å‹ã€PRMå’Œé—®é¢˜éš¾åº¦ä¸åŒè€Œå˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MATH - 500å’ŒAIME24ä»»åŠ¡ä¸Šçš„å®éªŒæœ‰é‡è¦å‘ç°ï¼šè®¡ç®—æœ€ä¼˜çš„TTSç­–ç•¥é«˜åº¦ä¾èµ–ç­–ç•¥æ¨¡å‹ã€PRMå’Œé—®é¢˜éš¾åº¦é€‰æ‹©ï¼›å€ŸåŠ©è®¡ç®—æœ€ä¼˜TTSç­–ç•¥ï¼Œæå°çš„ç­–ç•¥æ¨¡å‹èƒ½è¶…è¶Šå¤§æ¨¡å‹ï¼Œå¦‚1Bæ¨¡å‹åœ¨MATH - 500ä¸Šèƒ½è¶…è¿‡405Bæ¨¡å‹ï¼›åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šï¼Œ0.5Bæ¨¡å‹è¡¨ç°ä¼˜äºGPT - 4oï¼Œ3Bæ¨¡å‹è¶…è¿‡405Bæ¨¡å‹ï¼Œ7Bæ¨¡å‹å‡»è´¥o1å’ŒDeepSeek - R1ä¸”æ¨ç†æ•ˆç‡æ›´é«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡å¼ºè°ƒäº†æ ¹æ®ä»»åŠ¡å’Œæ¨¡å‹ç‰¹æ€§é€‚é…TTSç­–ç•¥çš„é‡è¦æ€§ï¼Œä¸ºæå‡LLMæ¨ç†èƒ½åŠ›æä¾›äº†æ–°æ–¹å‘ï¼Œè®©å°æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹é€šè¿‡TTSå®ç°æ€§èƒ½é€†è¢­æˆä¸ºå¯èƒ½ï¼Œä¸ºåç»­ç ”ç©¶TTSåœ¨ä¸åŒæ¨¡å‹ä¸ä»»åŠ¡çš„ä¼˜åŒ–åº”ç”¨æä¾›äº†æ€è·¯ï¼Œä¹Ÿå¯å‘ä»ä¸šè€…å…³æ³¨æ¨ç†é˜¶æ®µè®¡ç®—åˆ†é…å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæ¢ç´¢æ›´é«˜æ•ˆçš„æµ‹è¯•æ—¶ç­–ç•¥ã€‚

## teaching-language-models-to-critique-via-reinforcement-learning
### Abstract
Teaching large language models (LLMs) to critique and refine their outputs is
crucial for building systems that can iteratively improve, yet it is
fundamentally limited by the ability to provide accurate judgments and
actionable suggestions. In this work, we study LLM critics for code generation
and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic
$\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which
trains a critic model to generate feedback that maximizes correction
performance for a fixed generator model without human supervision. Our results
demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance
pass rates and mitigate compounding errors across both base and stronger
generator models. Furthermore, we show that these critic models act as accurate
generative reward models and enable test-time scaling through iterative
critique-revision, achieving up to 106.1% relative improvements across
challenging code generation benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¼ºåŒ–å­¦ä¹ æ•™è¯­è¨€æ¨¡å‹â€œæŒ‘åˆºâ€ï¼šCTRLæ¡†æ¶åŠ©åŠ›ä»£ç ç”Ÿæˆè¿­ä»£ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡è¿­ä»£åé¦ˆå®ç°è‡ªæˆ‘æ”¹è¿›æ˜¯å½“å‰ç ”ç©¶çƒ­ç‚¹ï¼Œä½†ç°æœ‰æœºåˆ¶å­˜åœ¨æ˜æ˜¾å±€é™ï¼šä¸€æ–¹é¢ï¼Œå¥–åŠ±æ¨¡å‹æŠŠå¤æ‚è¯„ä¼°å‹ç¼©æˆç®€å•æ•°å€¼ä¿¡å·ï¼Œè‡ªåŠ¨åŒ–éªŒè¯å·¥å…·ç»™å‡ºçš„æ‰§è¡Œè½¨è¿¹ä¹Ÿéš¾ç›´æ¥è½¬åŒ–ä¸ºé«˜å±‚ä¿®å¤æ–¹æ¡ˆï¼ŒäºŒè€…éƒ½éš¾æä¾›â€œæ—¢ç²¾å‡†åˆ¤æ–­åˆèƒ½æŒ‡å¯¼æ”¹è¿›â€çš„åé¦ˆï¼›å¦ä¸€æ–¹é¢ï¼Œæ²¡æœ‰åˆé€‚å¤–éƒ¨åé¦ˆæ—¶ï¼Œè‡ªæˆ‘æ”¹è¿›å¾ªç¯ç”šè‡³ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œâ€œåé¦ˆç“¶é¢ˆâ€æˆäº†é˜»ç¢LLMè¿­ä»£ä¼˜åŒ–çš„æ ¸å¿ƒé—®é¢˜ã€‚åœ¨ä»£ç ç”Ÿæˆè¿™ç±»åé¦ˆæœºåˆ¶ç›¸å¯¹æˆç†Ÿçš„é¢†åŸŸï¼Œä»…é ç°æœ‰åé¦ˆä¹Ÿå¾ˆéš¾æ¨åŠ¨å®è´¨æ€§æå‡ã€‚å› æ­¤ï¼Œå¦‚ä½•è®­ç»ƒå‡ºèƒ½æä¾›æœ‰æ•ˆåé¦ˆçš„â€œæ‰¹è¯„å®¶ï¼ˆcriticï¼‰â€æ¨¡å‹ï¼Œæˆäº†çªç ´ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCTRLæ¡†æ¶ï¼Œè§£è€¦â€œæ‰¹è¯„å®¶â€ä¸â€œä»»åŠ¡æ‰§è¡Œæ¨¡å‹â€  
CTRLï¼ˆCritic Training via Reinforcement Learningï¼‰æ¡†æ¶å°†ä¸“æ³¨äºâ€œæŒ‘åˆº+ææ”¹è¿›å»ºè®®â€çš„æ‰¹è¯„å®¶æ¨¡å‹ï¼Œå’Œè´Ÿè´£ç”Ÿæˆä»£ç çš„ä»»åŠ¡æ‰§è¡Œæ¨¡å‹è§£è€¦ã€‚ä¸å†è®©æ¨¡å‹â€œè‡ªæˆ‘æ‰¹è¯„â€ï¼Œè€Œæ˜¯è®­ç»ƒä¸“é—¨çš„æ‰¹è¯„å®¶ï¼Œé€šè¿‡è¿­ä»£çš„â€œæ‰¹è¯„ - ä¿®æ­£â€è¿‡ç¨‹ï¼Œå¼•å¯¼ä»»åŠ¡æ‰§è¡Œæ¨¡å‹ç”Ÿæˆæ›´ä¼˜è§£ã€‚è¿™ç§è§£è€¦è®©æ‰¹è¯„å®¶è®­ç»ƒæœ‰äº†æ¸…æ™°ä»£ç†ä»»åŠ¡ï¼šä»¥â€œèƒ½å¦é©±åŠ¨ä»»åŠ¡æ‰§è¡Œæ¨¡å‹äº§å‡ºæ­£ç¡®ç»“æœâ€æ¥è¡¡é‡æ‰¹è¯„å®¶çš„æœ‰æ•ˆæ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸¤é˜¶æ®µè®­ç»ƒ pipeline + GRPO ä¼˜åŒ–  
è®­ç»ƒæ‰¹è¯„å®¶æ—¶ï¼Œå…ˆåˆ©ç”¨æ‰§è¡Œåé¦ˆåˆæˆé«˜è´¨é‡æ‰¹è¯„æ ·ä¾‹ï¼Œåšæœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Finetuningï¼‰ï¼›å†ç”¨ Group Relative Policy Optimizationï¼ˆGRPOï¼‰åšå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚å‰è€…ä¸ºæ‰¹è¯„å®¶æä¾›ä¼˜è´¨åé¦ˆçš„â€œç¤ºèŒƒâ€ï¼Œåè€…åˆ™åœ¨å¤§ç©ºé—´ã€é«˜æ–¹å·®çš„æ‰¹è¯„ç”Ÿæˆåœºæ™¯ä¸­ï¼Œä¼˜åŒ–æ‰¹è¯„å®¶çš„åé¦ˆèƒ½åŠ›ï¼Œè®©å…¶è¾“å‡ºæ›´èƒ½æ¨åŠ¨ä»»åŠ¡æ‰§è¡Œæ¨¡å‹ä¿®æ­£é”™è¯¯ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
1. è·¨åŸºå‡†æµ‹è¯•å…¨é¢é¢†å…ˆï¼šåœ¨CodeContestsã€LiveCodeBenchã€MBPP+ã€JudgeBenchç­‰å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼Œç»CTRLè®­ç»ƒçš„æ‰¹è¯„å®¶ï¼Œè¡¨ç°è¿œè¶…â€œè‡ªæˆ‘æ‰¹åˆ¤â€ç±»æ–¹æ³•å’Œç”¨æ›´å¼ºæ‰¹è¯„æ¨¡å‹çš„æ–¹æ³•ã€‚  
2. å¼±åˆ°å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼šç›¸å¯¹å¼±çš„æ‰¹è¯„å®¶æ¨¡å‹ï¼Œèƒ½æœ‰æ•ˆæŒ‡å¯¼æ›´å¼ºçš„ä»»åŠ¡æ‰§è¡Œæ¨¡å‹ï¼ˆå¦‚GPT - 4oï¼‰ï¼Œç±»ä¼¼â€œå¼±ç›‘ç£å¼ºæ¨¡å‹â€çš„æ³›åŒ–ç°è±¡ï¼Œè¯æ˜æ‰¹è¯„å®¶è®­ç»ƒçš„é«˜æ•ˆæ€§ã€‚  
3. æµ‹è¯•æ—¶é«˜æ•ˆè¿­ä»£ï¼šé€šè¿‡é’ˆå¯¹æ€§åé¦ˆï¼Œå¤§å¹…å‡å°‘ä¿®æ­£è¿­ä»£æ¬¡æ•°ï¼Œé™ä½tokenæ¶ˆè€—åŒæ—¶æå‡æˆåŠŸç‡ã€‚ä¸”èƒ½ç¼“è§£â€œé”™è¯¯å¤åˆ©â€â€”â€”æ—©æœŸç²¾å‡†æªå‡ºå¹¶ä¿®æ­£é”™è¯¯ï¼Œå¼•å¯¼æ¨¡å‹èµ°æ›´ç›´æ¥çš„è§£é¢˜è·¯å¾„ã€‚  
4. è·¨æ¨¡å‹æå‡æ•ˆæœï¼šæ— è®ºåŸºç¡€è¿˜æ˜¯æ›´å¼ºçš„ç”Ÿæˆæ¨¡å‹ï¼Œç»CTRLè®­ç»ƒçš„æ‰¹è¯„å®¶éƒ½èƒ½æ˜¾è‘—æå‡â€œé€šè¿‡ç‡ï¼ˆpass ratesï¼‰â€ï¼Œå‡å°‘é”™è¯¯ç´¯ç§¯ã€‚åœ¨éƒ¨åˆ†åœºæ™¯ç›¸å¯¹æå‡è¾¾106.1%ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§£è€¦æ€è·¯çš„å¯å‘ï¼šæŠŠâ€œè¯„ä¼°åé¦ˆâ€å’Œâ€œä»»åŠ¡æ‰§è¡Œâ€è§£è€¦ï¼Œèƒ½æ›´èšç„¦åœ°ä¼˜åŒ–åé¦ˆè´¨é‡ï¼Œè¿™ç§æ¨¡å—åŒ–è®¾è®¡åœ¨LLMå·¥å…·é“¾ã€è¿­ä»£ç³»ç»Ÿæ„å»ºä¸­å€¼å¾—å€Ÿé‰´ã€‚  
2. ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼šå…ˆç›‘ç£å¾®è°ƒæ³¨å…¥ä¼˜è´¨åé¦ˆæ¨¡å¼ï¼Œå†å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å®ç”¨æ€§ï¼Œä¸ºè®­ç»ƒâ€œè¾…åŠ©å‹â€LLMï¼ˆå¦‚æ‰¹è¯„å®¶ã€è§„åˆ’å™¨ç­‰ï¼‰æä¾›äº†ä¸€å¥—å¯å‚è€ƒçš„ pipelineã€‚  
3. å¼±ç›‘ç£å¼ºæ¨¡å‹çš„å®è·µï¼šè¯æ˜å¼±æ¨¡å‹ç»è®­ç»ƒåå¯æœ‰æ•ˆç›‘ç£å¼ºæ¨¡å‹ï¼Œä¸ºèµ„æºæœ‰é™æ—¶è®­ç»ƒé«˜æ•ˆâ€œè¾…åŠ©ç»„ä»¶â€æä¾›äº†æ–°æ€è·¯ï¼Œå°¤å…¶åœ¨ä»£ç ã€æ•°å­¦æ¨ç†ç­‰éœ€è¦è¿­ä»£ä¿®æ­£çš„åœºæ™¯ã€‚  
4. è¿­ä»£åé¦ˆçš„æ•ˆç‡ä¼˜åŒ–ï¼šé€šè¿‡å‡å°‘é”™è¯¯å¤åˆ©å®ç°æµ‹è¯•æ—¶é«˜æ•ˆè¿­ä»£ï¼Œæç¤ºåœ¨æ„å»ºLLMè¿­ä»£ç³»ç»Ÿæ—¶ï¼Œè¦é‡è§†â€œåé¦ˆç²¾å‡†æ€§â€å¯¹æ•´ä½“æ•ˆç‡çš„å½±å“ï¼Œå¯ä»åé¦ˆå†…å®¹è®¾è®¡ã€æ‰¹è¯„å®¶è®­ç»ƒç­‰è§’åº¦åˆ‡å…¥ä¼˜åŒ–ã€‚

## stair--improving-safety-alignment-with-introspective-reasoning
### Abstract
Ensuring the safety and harmlessness of Large Language Models (LLMs) has
become equally critical as their performance in applications. However, existing
safety alignment methods typically suffer from safety-performance trade-offs
and the susceptibility to jailbreak attacks, primarily due to their reliance on
direct refusals for malicious queries. In this paper, we propose STAIR, a novel
framework that integrates SafeTy Alignment with Itrospective Reasoning. We
enable LLMs to identify safety risks through step-by-step analysis by
self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR
first equips the model with a structured reasoning capability and then advances
safety alignment via iterative preference optimization on step-level reasoning
data generated using our newly proposed Safety-Informed Monte Carlo Tree Search
(SI-MCTS). We further train a process reward model on this data to guide
test-time searches for improved responses. Extensive experiments show that
STAIR effectively mitigates harmful outputs while better preserving
helpfulness, compared to instinctive alignment strategies. With test-time
scaling, STAIR achieves a safety performance comparable to Claude-3.5 against
popular jailbreak attacks. Relevant resources in this work are available at
https://github.com/thu-ml/STAIR.
### ğŸŒŸ è®ºæ–‡è§£è¯» | STAIRï¼šç”¨â€œè‡ªçœå¼æ¨ç†â€é©æ–°å¤§æ¨¡å‹å®‰å…¨å¯¹é½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—ã€æ•™è‚²ã€æ³•å¾‹ç­‰è¯¸å¤šé«˜é£é™©é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œä½¿å…¶å®‰å…¨æ€§ä¸æ— å®³æ€§å˜å¾—å’Œæ€§èƒ½åŒç­‰å…³é”®ã€‚ç„¶è€Œç°æœ‰å®‰å…¨å¯¹é½æ–¹æ³•å­˜åœ¨ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ˜¯å®‰å…¨ä¸æ€§èƒ½çš„æƒè¡¡å›°å¢ƒï¼ˆä¸ºäº†å®‰å…¨ç‰ºç‰²å®ç”¨æ€§ï¼Œåä¹‹äº¦ç„¶ï¼‰ï¼›äºŒæ˜¯æ˜“å—â€œè¶Šç‹±æ”»å‡»â€ï¼ˆjailbreak attacksï¼‰â€”â€”æ”»å‡»è€…é€šè¿‡å¯¹æŠ—æ€§åç¼€ã€ä¼ªè£…ç­‰æ‰‹æ®µç»•è¿‡æ¨¡å‹çš„ç›´æ¥æ‹’ç­”æœºåˆ¶ï¼Œè¯±å¯¼æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚æœ¬è´¨ä¸Šï¼Œç°æœ‰æ–¹æ³•ä¾èµ–â€œç›´è§‰å¼æ‹’ç­”â€ï¼ˆç±»ä¼¼å¿ƒç†å­¦ä¸­â€œç³»ç»Ÿ1â€çš„æœ¬èƒ½ååº”ï¼‰ï¼Œç¼ºä¹å¯¹é£é™©çš„æ·±åº¦åˆ†æï¼Œå¯¼è‡´é¢å¯¹å¤æ‚ä¼ªè£…æ”»å‡»æ—¶é˜²çº¿å´©æºƒã€‚å› æ­¤ï¼Œè®ºæ–‡æå‡º**STAIR**æ¡†æ¶ï¼Œè¯•å›¾ç”¨â€œè‡ªçœå¼æ¨ç†â€ï¼ˆå¯¹åº”â€œç³»ç»Ÿ2â€çš„æ·±æ€ç†Ÿè™‘ï¼‰è®©æ¨¡å‹åœ¨æ‹’ç­”å‰å…ˆåˆ†æé£é™©ï¼Œä»æ ¹æºæå‡å®‰å…¨å¯¹é½èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»“æ„åŒ–CoTæ ¼å¼å¯¹é½ï¼Œèµ‹äºˆæ¨¡å‹â€œç³»ç»Ÿ2â€æ¨ç†èƒ½åŠ›  
ä¼ ç»Ÿå®‰å…¨å¯¹é½è®©æ¨¡å‹ç›´æ¥æ‹’ç­”æ¶æ„è¯·æ±‚ï¼Œè€ŒSTAIRç¬¬ä¸€æ­¥æ˜¯**æ•™ä¼šæ¨¡å‹ç”¨ç»“æ„åŒ–æ€ç»´é“¾ï¼ˆCoTï¼‰åˆ†æé£é™©**ã€‚é€šè¿‡åœ¨å®‰å…¨ä¸æœ‰ç”¨æ€§æ··åˆæ•°æ®é›†ä¸Šå¾®è°ƒï¼Œè®©æ¨¡å‹ä»â€œç›´æ¥è¯´æŠ±æ­‰â€è½¬å‘â€œé€æ­¥æ¨ç†é£é™©â€ï¼Œä¸ºåç»­æ·±åº¦å®‰å…¨åˆ†ææ‰“ä¸‹åŸºç¡€ã€‚è¿™ä¸€æ­¥æ˜¯è®©æ¨¡å‹ä»â€œæœ¬èƒ½ååº”â€è¿‡æ¸¡åˆ°â€œé€»è¾‘æ¨ç†â€çš„å…³é”®é“ºå«ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå®‰å…¨æ„ŸçŸ¥çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆSI-MCTSï¼‰çš„è¿­ä»£è‡ªæ”¹è¿›  
åœ¨ç»“æ„åŒ–CoTåŸºç¡€ä¸Šï¼ŒSTAIRå¼•å…¥**Safety-Informed MCTSï¼ˆSI-MCTSï¼‰**ç”Ÿæˆâ€œæ­¥éª¤çº§æ¨ç†æ•°æ®â€ï¼Œå¹¶åŸºäºè¿™äº›æ•°æ®åšè¿­ä»£åå¥½ä¼˜åŒ–ï¼ˆå¦‚æ­¥éª¤çº§DPOï¼‰ã€‚SI-MCTSçš„æ ¸å¿ƒæ˜¯è®¾è®¡â€œå®‰å…¨æ„ŸçŸ¥å¥–åŠ±â€ï¼šä¸ä»…è€ƒè™‘â€œæœ‰ç”¨æ€§â€ï¼Œè¿˜å°†å®‰å…¨ç›¸å…³ä¿¡æ¯æ³¨å…¥æœç´¢èŠ‚ç‚¹ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨æ¨ç†æ­¥éª¤ï¼‰ï¼Œå¼•å¯¼æ¨¡å‹æ¢ç´¢æ›´å®‰å…¨çš„æ¨ç†è·¯å¾„ã€‚è¿™ç§è¿­ä»£æœºåˆ¶è®©æ¨¡å‹æ— éœ€é¢å¤–äººå·¥æ ‡æ³¨ï¼Œå°±èƒ½æŒç»­è‡ªæˆ‘æå‡å®‰å…¨æ¨ç†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä¸æµ‹è¯•æ—¶æœç´¢ï¼Œæ”¾å¤§å®‰å…¨ä¸æ€§èƒ½æ”¶ç›Š  
åˆ©ç”¨SI-MCTSç”Ÿæˆçš„åå¥½æ•°æ®ï¼Œè®­ç»ƒ**è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰**ã€‚æµ‹è¯•é˜¶æ®µï¼Œç»“åˆBest-of-Næˆ–Beam Searchç­‰æœç´¢ç®—æ³•ï¼Œè®©PRMå¼•å¯¼æ¨¡å‹åœ¨ç”Ÿæˆå“åº”æ—¶è¿›è¡Œæ›´å®¡æ…çš„æ¨ç†ï¼Œè¿›ä¸€æ­¥æå‡è¾“å‡ºè´¨é‡ï¼ˆå®‰å…¨+æœ‰ç”¨æ€§ï¼‰ã€‚è¿™ä¸€æ­¥å®ç°äº†â€œè®­ç»ƒ-æ¨ç†â€é—­ç¯ï¼Œè®©å®‰å…¨å¯¹é½çš„æ”¶ç›Šåœ¨å®é™…éƒ¨ç½²æ—¶æŒç»­ç”Ÿæ•ˆã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. å®‰å…¨æŠ—æ€§æ˜¾è‘—æå‡ï¼šåœ¨â€œStrongRejectâ€ä»»åŠ¡ä¸­ï¼ŒSTAIRä¸ºLLaMAæ¨¡å‹å®ç°äº†0.88çš„â€œå®‰å…¨è¯„åˆ†â€ï¼Œæ¯”æœ€ä¼˜åŸºçº¿é«˜0.15ï¼Œå¯¹å„ç±»æ¶æ„æŸ¥è¯¢çš„æŠµæŠ—èƒ½åŠ›å¤§å¹…å¢å¼ºã€‚  
2. ç¼“è§£å®‰å…¨-æ€§èƒ½æƒè¡¡ï¼šç›¸æ¯”ä»…åšå®‰å…¨å¯¹é½çš„åŸºçº¿ï¼ŒSTAIRåœ¨â€œæœ‰ç”¨æ€§ã€çœŸå®æ€§ã€é²æ£’æ€§ã€éšç§æ„ŸçŸ¥â€ç­‰ç»´åº¦å‡æœ‰æå‡ã€‚ä¾‹å¦‚åœ¨AlpacaEvalä¸Šï¼ŒLLaMAå’ŒQwenç›¸å¯¹äºå…¶åŸºç¡€æ¨¡å‹ï¼Œå¯¹GPT-4çš„èƒœç‡åˆ†åˆ«æå‡13.11%å’Œ6.25%ï¼Œè€Œå¤šæ•°åŸºçº¿ä»…èƒ½â€œä¿å®‰å…¨ä½†ä¸¢æ€§èƒ½â€ã€‚  
3. å¯¹æŠ—è¶Šç‹±æ”»å‡»æ¯”è‚© Claude-3.5ï¼šå½“æµ‹è¯•æ—¶å¯ç”¨æ¨ç†å¢å¼ºï¼ˆtest-time scalingï¼‰ï¼ŒSTAIRåœ¨â€œStrongRejectâ€ä¸Šçš„å®‰å…¨è¯„åˆ†è¾¾åˆ°0.94ï¼Œä¸Claude-3.5ç›¸å½“ï¼Œè¯æ˜å…¶åœ¨å®æˆ˜çº§æ”»å‡»ä¸‹çš„ç«äº‰åŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€ç»´é“¾ï¼ˆCoTï¼‰ä¸å®‰å…¨å¯¹é½çš„ç»“åˆï¼šæ‰“ç ´â€œç›´æ¥æ‹’ç­”â€çš„æƒ¯æ€§ï¼Œç”¨â€œæ¨ç†åˆ†æâ€æ›¿ä»£â€œç›´è§‰ååº”â€ï¼Œä¸ºå®‰å…¨å¯¹é½æä¾›äº†â€œæ·±åº¦ç†è§£é£é™©â€çš„æ–°æ€è·¯ï¼Œå¯æ¨å¹¿åˆ°éœ€è¦å¤æ‚é£é™©åˆ¤æ–­çš„åœºæ™¯ï¼ˆå¦‚é‡‘èã€åŒ»ç–—é—®ç­”ï¼‰ã€‚  
2. è‡ªæ”¹è¿›ä¸æœç´¢æœºåˆ¶çš„é—­ç¯ï¼šSI-MCTS+è¿­ä»£ä¼˜åŒ–+è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„ç»„åˆï¼Œå±•ç¤ºäº†â€œè®©æ¨¡å‹è‡ªå·±ç”Ÿæˆæ•°æ®ã€è‡ªå·±ä¼˜åŒ–ã€è‡ªå·±å¼•å¯¼æ¨ç†â€çš„è‡ªé©±åŠ¨èŒƒå¼ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œé€‚åˆå¤§è§„æ¨¡æ¨¡å‹çš„æŒç»­è¿­ä»£ã€‚  
3. å¤šç›®æ ‡å¹³è¡¡çš„å®è·µï¼šé€šè¿‡â€œå®‰å…¨æ„ŸçŸ¥å¥–åŠ±â€â€œæ­¥éª¤çº§ä¼˜åŒ–â€ç­‰è®¾è®¡ï¼Œè¯æ˜å®‰å…¨ä¸æ€§èƒ½å¹¶éé›¶å’Œåšå¼ˆï¼Œä¸ºåç»­å¤šç›®æ ‡å¯¹é½ä»»åŠ¡æä¾›äº†å¯å¤ç”¨çš„æ–¹æ³•è®ºï¼ˆå¦‚ä½•æ‹†è§£ç›®æ ‡ã€è®¾è®¡ä¸­é—´æ­¥éª¤å¥–åŠ±ï¼‰ã€‚  


STAIRçš„æ ¸å¿ƒçªç ´åœ¨äºï¼šæŠŠâ€œå®‰å…¨å¯¹é½â€ä»â€œç›´è§‰æ‹’ç­”â€å‡çº§ä¸ºâ€œè‡ªçœæ¨ç†â€ï¼Œç”¨ç»“æ„åŒ–æ€ç»´é“¾+è‡ªæ”¹è¿›æœç´¢+è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ‰“é€ äº†ä¸€å¥—â€œå…ˆæƒ³æ¸…æ¥šé£é™©ï¼Œå†å®‰å…¨å“åº”â€çš„æœºåˆ¶ã€‚è¿™ä¸ä»…æå‡äº†å¯¹æŠ—æ”»å‡»çš„èƒ½åŠ›ï¼Œæ›´è®©å¤§æ¨¡å‹åœ¨é«˜é£é™©åœºæ™¯ä¸‹çš„â€œå®‰å…¨æ€§-å®ç”¨æ€§â€å¹³è¡¡æˆä¸ºå¯èƒ½ï¼Œä¸ºä¸‹ä¸€ä»£å®‰å…¨å¯¹é½æŠ€æœ¯æŒ‡æ˜äº†â€œæ·±åº¦æ¨ç†+è‡ªé©±åŠ¨ä¼˜åŒ–â€çš„æ–¹å‘ã€‚

## sets--leveraging-self-verification-and-self-correction-for-improved-test-time-scaling
### Abstract
Recent advancements in Large Language Models (LLMs) have created new
opportunities to enhance performance on complex reasoning tasks by leveraging
test-time computation. However, existing parallel scaling methods, such as
repeated sampling or reward model scoring, often suffer from premature
convergence and high costs due to task-specific reward model training, while
sequential methods like SELF-REFINE cannot effectively leverage increased
compute. This paper introduces Self-Enhanced Test-Time Scaling (SETS), a new
approach that overcomes these limitations by strategically combining parallel
and sequential techniques. SETS exploits the inherent self-verification and
self-correction capabilities of LLMs, unifying sampling, verification, and
correction within a single framework. This innovative design facilitates
efficient and scalable test-time computation for enhanced performance on
complex tasks. Our comprehensive experimental results on challenging benchmarks
spanning planning, reasoning, math, and coding demonstrate that SETS achieves
significant performance improvements and more advantageous test-time scaling
behavior than the alternatives.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SETSï¼šå€Ÿè‡ªéªŒè¯ä¸è‡ªä¿®æ­£ä¹‹åŠ›ï¼Œé©æ–°æµ‹è¯•æ—¶ç®—åŠ›ç¼©æ”¾

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒä¸æµ‹è¯•é˜¶æ®µçš„ç®—åŠ›åˆ©ç”¨æ¨åŠ¨äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½†ç°æœ‰æµ‹è¯•æ—¶ç®—åŠ›ç¼©æ”¾æ–¹æ³•å­˜åœ¨å±€é™ã€‚å¹¶è¡Œç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚é‡å¤é‡‡æ ·ã€å¥–åŠ±æ¨¡å‹æ‰“åˆ†ï¼‰æ˜“è¿‡æ—©æ”¶æ•›ä¸”å› ç‰¹å®šä»»åŠ¡å¥–åŠ±æ¨¡å‹è®­ç»ƒæˆæœ¬é«˜ï¼›é¡ºåºç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚SELF - REFINEï¼‰éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¢åŠ çš„ç®—åŠ›ï¼Œæ€§èƒ½æ˜“å¿«é€Ÿé¥±å’Œã€‚åŒæ—¶ï¼Œæ—©æœŸLLMè‡ªä¿®æ­£èƒ½åŠ›æœ‰é™é™åˆ¶äº†æ–¹æ³•åˆ›æ–°ï¼Œè€Œå¦‚ä»ŠLLMè‡ªéªŒè¯ä¸è‡ªä¿®æ­£èƒ½åŠ›æå‡ï¼Œä¸ºé‡æ–°æ€è€ƒæµ‹è¯•æ—¶ç¼©æ”¾æä¾›äº†å¥‘æœºï¼Œå› æ­¤æœ¬æ–‡æ—¨åœ¨æå‡ºç»“åˆå¹¶è¡Œä¸é¡ºåºç¼©æ”¾çš„æ–°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSETSæ¡†æ¶
SETSï¼ˆSelf - Enhanced Test - Time Scalingï¼‰åˆ›æ–°æ€§åœ°ç»“åˆå¹¶è¡Œä¸é¡ºåºç¼©æ”¾æŠ€æœ¯ï¼Œå°†é‡‡æ ·ã€è‡ªéªŒè¯ã€è‡ªä¿®æ­£ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶å†…ã€‚åˆ©ç”¨LLMå›ºæœ‰çš„è‡ªéªŒè¯å’Œè‡ªä¿®æ­£èƒ½åŠ›ï¼Œå…‹æœäº†ç°æœ‰å¹¶è¡Œä¸é¡ºåºæ–¹æ³•å„è‡ªçš„ç¼ºé™·ï¼Œå®ç°æ›´é«˜æ•ˆä¸”å¯æ‰©å±•çš„æµ‹è¯•æ—¶è®¡ç®—ï¼Œä»¥æå‡å¤æ‚ä»»åŠ¡è¡¨ç°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— éœ€å¤–éƒ¨å¥–åŠ±æ¨¡å‹
ä¸ä¸€äº›éœ€è®­ç»ƒé¢å¤–ç‰¹å®šä»»åŠ¡éªŒè¯å™¨æˆ–ä¿®æ­£æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒSETSä¸ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œä¾é LLMè‡ªèº«çš„è‡ªéªŒè¯ä¸è‡ªä¿®æ­£èƒ½åŠ›æ¥è¿›è¡Œæµ‹è¯•æ—¶ç®—åŠ›ç¼©æ”¾ï¼Œé™ä½äº†è®­ç»ƒå¼€é”€ä¸ä»»åŠ¡ç‰¹å¼‚æ€§é™åˆ¶ï¼Œå¢å¼ºäº†é€šç”¨æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¶µç›–è§„åˆ’ï¼ˆNATURAL PLANï¼‰ã€æ¨ç†ï¼ˆLiveBench Reasoningï¼‰ã€æ•°å­¦ï¼ˆMATH 500ã€AIME 2024 - 2025ï¼‰ã€ç¼–ç ï¼ˆLiveCodeBench TestOutputPredï¼‰çš„äº”å¤§æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSETSå±•ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚ç›¸æ¯”é‡å¤é‡‡æ ·ç­‰å¹¶è¡Œæ–¹æ³•å’ŒSELF - REFINEç­‰é¡ºåºæ–¹æ³•ï¼ŒSETSåœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­ä¿æŒæ›´é«˜æœ‰æ•ˆæ€§ï¼Œæ€§èƒ½å¢ç›Šä¸‹é™æ›´å°‘ï¼Œåœ¨è§„åˆ’ã€æ¨ç†ã€æ•°å­¦å’Œç¼–ç åŸºå‡†ä¸Šï¼Œä½¿ç”¨éæ€è€ƒå‹å’Œæ€è€ƒå‹æ¨¡å‹ï¼ˆå¦‚GEMINI - 1.5 - Proå’ŒGEMINI - 2.5 - Flashï¼‰æ—¶ï¼Œå‡†ç¡®ç‡æœ€å¤šæå‡10.9%ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒè¡¨æ˜SETSå¯¹å…³é”®è¶…å‚æ•°ï¼ˆå¦‚è‡ªä¿®æ­£è½®æ¬¡æœ€å¤§å€¼ã€LLMæ¨ç†æ¸©åº¦ï¼‰é²æ£’ï¼Œåªéœ€å°‘é‡è¶…å‚æ•°è°ƒä¼˜å°±èƒ½å®ç°å¼ºæ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•èåˆæ€è·¯ï¼šå½“é¢ä¸´éœ€ç»“åˆä¸åŒæŠ€æœ¯ä¼˜åŠ¿è§£å†³é—®é¢˜æ—¶ï¼Œå¯å€Ÿé‰´SETSè¿™ç§å°†å¹¶è¡Œä¸é¡ºåºæŠ€æœ¯ç­–ç•¥æ€§ç»“åˆçš„æ€è·¯ï¼Œçªç ´å•ä¸€æ–¹æ³•çš„å±€é™ã€‚
2. åˆ©ç”¨æ¨¡å‹è‡ªèº«èƒ½åŠ›ï¼šåœ¨æ„å»ºAIç³»ç»Ÿè§£å†³ä»»åŠ¡æ—¶ï¼Œå……åˆ†æŒ–æ˜æ¨¡å‹è‡ªèº«çš„è‡ªéªŒè¯ã€è‡ªä¿®æ­£ç­‰å†…åœ¨èƒ½åŠ›ï¼Œå‡å°‘å¯¹å¤–éƒ¨å¤æ‚ç»„ä»¶ï¼ˆå¦‚ç‰¹å®šä»»åŠ¡å¥–åŠ±æ¨¡å‹ï¼‰çš„ä¾èµ–ï¼Œæå‡é€šç”¨æ€§ä¸æ•ˆç‡ã€‚
3. å®éªŒä¸è°ƒä¼˜è§’åº¦ï¼šè¿›è¡Œæ–¹æ³•è¯„ä¼°æ—¶ï¼Œä¸ä»…è¦åœ¨å¤šä»»åŠ¡åŸºå‡†ä¸ŠéªŒè¯æ•ˆæœï¼Œä¹Ÿå¯é€šè¿‡æ¶ˆèå®éªŒåˆ†æå…³é”®è¶…å‚æ•°å½±å“ï¼Œä¸ºæ–¹æ³•çš„é²æ£’æ€§ä¸æ˜“ç”¨æ€§æä¾›æ”¯æ’‘ï¼Œè¿™å¯¹åç»­æ–¹æ³•æ”¹è¿›ä¸è½åœ°æœ‰å‚è€ƒä»·å€¼ã€‚

## pairjudge-rm--perform-best-of-n-sampling-with-knockout-tournament
### Abstract
Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large
Language Models (LLMs), relies on reward models to select the best candidate
solution from multiple generations. However, traditional reward models often
assign arbitrary and inconsistent scores, limiting their effectiveness. To
address this, we propose a Pairwise Judge Reward Model (PariJudge RM) combined
with a knockout tournament for BoN sampling. Instead of assigning absolute
scores, given one math problem, PariJudge RM judges two candidate solutions'
correctness with chain-of-thought reasoning simultaneously. This approach
eliminates the need for scoring and enables cross-validation of solutions
through parallel judgment. In the knockout tournament, PariJudge RM conducts
pairwise Judgment between candidate solutions and eliminates the incorrect ones
iteratively. We construct PairJudge-432K, a large-scale dataset of 432K
pairwise judgments derived from NumiaMath and annotated using
\texttt{gemini-1.5-flash}, and train the PariJudge RM via supervised
fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate
significant improvements over baseline reward models. And a 40\% to 60\%
relative improvement is achieved on the top 50\% challenging problems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PairJudge RMï¼šç”¨æ·˜æ±°èµ›æœºåˆ¶é©æ–°å¤§æ¨¡å‹Best-of-Né‡‡æ ·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥ä¸­ï¼ŒBest-of-Nï¼ˆBoNï¼‰é‡‡æ ·æ˜¯å¸¸ç”¨æ‰‹æ®µï¼Œå®ƒä¾èµ–å¥–åŠ±æ¨¡å‹ä»å¤šä¸ªç”Ÿæˆç»“æœé‡Œé€‰æœ€ä¼˜è§£ã€‚ä½†ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹å­˜åœ¨**åˆ†æ•°éšæ„æ€§ä¸ä¸ä¸€è‡´æ€§**é—®é¢˜â€”â€”å³ä¾¿äººç±»ä¸“å®¶æŒ‰åŒä¸€æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœä¹Ÿå¯èƒ½å·®å¼‚å¾ˆå¤§ï¼Œæ›´ä¸ç”¨è¯´æ¨¡å‹è®­ç»ƒå‡ºçš„ç›¸å¯¹åˆ†æ•°å¾€å¾€ç¼ºä¹ç»å¯¹å‚è€ƒä»·å€¼ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†BoNé‡‡æ ·çš„æ•ˆæœã€‚å› æ­¤ï¼Œå¦‚ä½•è®©å¥–åŠ±æ¨¡å‹æ›´å¯é åœ°å®Œæˆå€™é€‰è§£ç­›é€‰ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPairwise Judge Reward Modelï¼ˆPairJudge RMï¼‰è®¾è®¡  
ä¸å†ç»™å•ä¸ªå€™é€‰è§£æ‰“â€œç»å¯¹åˆ†æ•°â€ï¼Œè€Œæ˜¯**å¯¹åŒä¸€æ•°å­¦é—®é¢˜çš„ä¸¤ä¸ªå€™é€‰è§£ï¼Œç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†åŒæ—¶åˆ¤æ–­æ­£ç¡®æ€§**ã€‚é€šè¿‡ pairwiseï¼ˆæˆå¯¹ï¼‰åˆ¤æ–­ï¼Œæ—¢é¿å…äº†ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹â€œæ‰“åˆ†éšæ„â€çš„ç¼ºé™·ï¼Œåˆèƒ½è®©å€™é€‰è§£ä¹‹é—´é€šè¿‡å¹¶è¡Œåˆ¤æ–­å®ç°äº¤å‰éªŒè¯ï¼Œä»æœºåˆ¶ä¸Šæå‡åˆ¤æ–­å¯é æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ·˜æ±°èµ›ï¼ˆKnockout Tournamentï¼‰æµç¨‹  
ä¸ºå®ç°BoNé‡‡æ ·ï¼Œå°†æ‰€æœ‰å€™é€‰è§£ç»„ç»‡æˆâ€œæ·˜æ±°èµ›â€å½¢å¼ï¼šPairJudge RM å¯¹å€™é€‰è§£ä¸¤ä¸¤é…å¯¹åšåˆ¤æ–­ï¼Œè¿­ä»£æ·˜æ±°é”™è¯¯è§£ï¼Œç›´åˆ°åªå‰©ä¸€ä¸ªå€™é€‰è§£ä½œä¸ºæœ€ç»ˆè¾“å‡ºã€‚è¿™ç§æ–¹å¼æŠŠâ€œé€‰æœ€ä¼˜â€è½¬åŒ–ä¸ºâ€œé€æ­¥æ·˜æ±°æ¬¡ä¼˜â€ï¼Œç”¨ pairwise åˆ¤æ–­çš„â€œèƒœè´Ÿâ€é€»è¾‘æ›¿ä»£ä¼ ç»Ÿçš„â€œåˆ†æ•°æ’åºâ€ï¼Œè®©é€‰æ‹©è¿‡ç¨‹æ›´ç›´è§‚ä¸”é²æ£’ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šPAIRJUDGE - 432K å¤§è§„æ¨¡æ•°æ®é›†æ„å»º  
ä» NumiaMath è¡ç”Ÿå‡º 43.2 ä¸‡æ¡æˆå¯¹åˆ¤æ–­æ•°æ®ï¼Œå¹¶ç”¨ gemini - 1.5 - flash å®Œæˆæ ‡æ³¨ï¼Œä¸º PairJudge RM çš„æœ‰ç›‘ç£å¾®è°ƒæä¾›é«˜è´¨é‡è®­ç»ƒç´ æã€‚æ•°æ®é›†+æ¨¡å‹è®­ç»ƒ pipeline çš„å¼€æºï¼Œä¹Ÿä¸ºåç»­ç ”ç©¶æ‰“ä¸‹åŸºç¡€ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ MATH - 500 å’Œ Olympiad Bench ç­‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPairJudge RM å±•ç°å‡ºå¯¹ä¼ ç»Ÿåˆ¤åˆ«å¼å¥–åŠ±æ¨¡å‹çš„æ˜¾è‘—ä¼˜åŠ¿ï¼š  
- **æ•´ä½“æ€§èƒ½**ï¼šåœ¨ BoN é‡‡æ ·ä»»åŠ¡ä¸Šå…¨é¢è¶…è¶ŠåŸºçº¿å¥–åŠ±æ¨¡å‹ï¼›  
- **éš¾é¢˜è¡¨ç°**ï¼šé’ˆå¯¹ MATH - 500 ä¸­éš¾åº¦å‰ 50% çš„é—®é¢˜ï¼Œç›¸å¯¹åŸºçº¿å®ç°äº† 40% - 60% çš„æ€§èƒ½æå‡ï¼›  
- **å¯¹æ¯”å‰æ²¿**ï¼šåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹ï¼Œæ•ˆæœä¼˜äºè¿‘æœŸæå‡ºçš„ Critic Model ç­‰æ–¹æ³•ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ€è·¯é©æ–°**ï¼šç”¨â€œæˆå¯¹åˆ¤æ–­+æ·˜æ±°æœºåˆ¶â€æ›¿ä»£â€œç»å¯¹æ‰“åˆ†+æ’åºâ€ï¼Œä¸ºå¥–åŠ±æ¨¡å‹è®¾è®¡æä¾›äº†è·³å‡ºâ€œåˆ†æ•°é™·é˜±â€çš„æ–°æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è¦â€œé€‰ä¼˜â€çš„åœºæ™¯ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€åˆ›æ„å†™ä½œç­‰ï¼‰ï¼›  
2. **æ•°æ®æ„å»º**ï¼šå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æˆå¯¹æ ‡æ³¨æ•°æ®é›†ï¼ˆPAIRJUDGE - 432Kï¼‰è¯æ˜äº†â€œåŸºäºç°æœ‰èµ„æºè¡ç”Ÿ+å¼ºæ¨¡å‹æ ‡æ³¨â€çš„é«˜æ•ˆæ•°æ®æ„å»ºèŒƒå¼ï¼Œä¸ºé¢†åŸŸç‰¹å®šæ¨¡å‹è®­ç»ƒæä¾›å‚è€ƒï¼›  
3. **å·¥ç¨‹è½åœ°**ï¼šå°†â€œæ·˜æ±°èµ›â€è¿™ç§ç›´è§‚çš„ç«äº‰æœºåˆ¶å¼•å…¥æ¨¡å‹æ¨ç†æµç¨‹ï¼Œç®€åŒ–äº†â€œé€‰æœ€ä¼˜â€çš„å·¥ç¨‹å®ç°é€»è¾‘ï¼Œé™ä½äº†å¯¹â€œç²¾ç¡®åˆ†æ•°æ ¡å‡†â€çš„ä¾èµ–ï¼Œæ›´æ˜“åœ¨å®é™…äº§å“ä¸­è½åœ°ã€‚  


è¿™ç¯‡è®ºæ–‡ä»æ ¸å¿ƒæ¨¡å‹è®¾è®¡ã€æ•°æ®æ„å»ºåˆ°æ¨ç†æµç¨‹éƒ½åšäº†åˆ›æ–°æ€§çªç ´ï¼Œä¸ºå¤§æ¨¡å‹æµ‹è¯•æ—¶çš„â€œé€‰ä¼˜â€ç¯èŠ‚æä¾›äº†ä¸€å¥—æ›´å¯é ã€æ›´æ˜“è½åœ°çš„æ–¹æ¡ˆï¼Œå€¼å¾—å…³æ³¨å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–çš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆæ·±å…¥ç ”ç©¶~

## internlm-xcomposer2-5-reward--a-simple-yet-effective-multi-modal-reward-model
### Abstract
Despite the promising performance of Large Vision Language Models (LVLMs) in
visual understanding, they occasionally generate incorrect outputs. While
reward models (RMs) with reinforcement learning or test-time scaling offer the
potential for improving generation quality, a critical gap remains: publicly
available multi-modal RMs for LVLMs are scarce, and the implementation details
of proprietary models are often unclear. We bridge this gap with
InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective
multi-modal reward model that aligns LVLMs with human preferences. To ensure
the robustness and versatility of IXC-2.5-Reward, we set up a high-quality
multi-modal preference corpus spanning text, image, and video inputs across
diverse domains, such as instruction following, general understanding,
text-rich documents, mathematical reasoning, and video understanding.
IXC-2.5-Reward achieves excellent results on the latest multi-modal reward
model benchmark and shows competitive performance on text-only reward model
benchmarks. We further demonstrate three key applications of IXC-2.5-Reward:
(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward
with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows
consistent improvements in instruction following and multi-modal open-ended
dialogue; (2) Selecting the best response from candidate responses for
test-time scaling; and (3) Filtering outlier or noisy samples from existing
image and video instruction tuning training data. To ensure reproducibility and
facilitate further research, we have open-sourced all model weights and
training recipes at
https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward
### ğŸŒŸ è®ºæ–‡è§£è¯» | InternLM-XComposer2.5-Rewardï¼šå¡«è¡¥å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ç©ºç™½çš„é«˜æ•ˆæ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¶å°”ä¹Ÿä¼šç”Ÿæˆé”™è¯¯è¾“å‡ºã€‚å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ç»“åˆå¼ºåŒ–å­¦ä¹ æˆ–æµ‹è¯•æ—¶ç¼©æ”¾è™½æœ‰æå‡ç”Ÿæˆè´¨é‡çš„æ½œåŠ›ï¼Œç„¶è€Œå…¬å¼€å¯ç”¨çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ç¨€ç¼ºï¼Œä¸”ä¸“æœ‰æ¨¡å‹å®ç°ç»†èŠ‚ä¸æ˜ã€‚é’ˆå¯¹æ­¤ï¼Œè®ºæ–‡æå‡ºInternLM - XComposer2.5 - Rewardï¼ˆIXC - 2.5 - Rewardï¼‰æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè®©LVLMsä¸äººç±»åå¥½å¯¹é½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºé«˜è´¨é‡å¤šæ¨¡æ€åå¥½è¯­æ–™åº“
ä¸ºç¡®ä¿IXC - 2.5 - Rewardçš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œæ„å»ºäº†æ¶µç›–æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘è¾“å…¥ï¼Œè·¨æŒ‡ä»¤éµå¾ªã€é€šç”¨ç†è§£ã€æ–‡æœ¬ä¸°å¯Œæ–‡æ¡£ã€æ•°å­¦æ¨ç†ã€è§†é¢‘ç†è§£ç­‰å¤šæ ·é¢†åŸŸçš„é«˜è´¨é‡å¤šæ¨¡æ€åå¥½è¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“æ„å»º pipeline ä¼šä¸ºä¸åŒæ¨¡æ€è¾“å…¥é€‰æ‹©å¤šæ ·é¢†åŸŸçš„æç¤ºï¼Œç”Ÿæˆå¯¹åº”å“åº”ï¼Œå†ç”¨GPT - 4oæˆ–éªŒè¯å™¨åšåå¥½åˆ¤æ–­ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ç®€å•æœ‰æ•ˆçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ¶æ„
ä¸ç›´æ¥å°†å•æ¨¡æ€ï¼ˆæ–‡æœ¬ï¼‰å¥–åŠ±æ¨¡å‹è¿ç§»åˆ°è§†è§‰æ¨¡æ€ï¼Œè€Œæ˜¯åœ¨ç°æœ‰LVLMï¼ˆInternLM - XComposer2.5ï¼‰åŸºç¡€ä¸Šå¢åŠ ä¸€ä¸ªè¯„åˆ†å¤´æ¥é¢„æµ‹å¥–åŠ±åˆ†æ•°ï¼Œä½¿å…¶èƒ½æœ‰æ•ˆè¯„ä¼°è§†è§‰ï¼ˆå›¾åƒå’Œè§†é¢‘ï¼‰å’Œæ–‡æœ¬è¾“å…¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå±•ç¤ºå¥–åŠ±æ¨¡å‹ä¸‰å¤§å…³é”®åº”ç”¨
ä¸€æ˜¯ä¸ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›ç›‘ç£ä¿¡å·ï¼Œå°†IXC - 2.5 - Rewardä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç»“åˆå¾—åˆ°IXC - 2.5 - Chatï¼Œæå‡æŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡æ€å¼€æ”¾å¼å¯¹è¯èƒ½åŠ›ï¼›äºŒæ˜¯åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­ä»å€™é€‰å“åº”é‡Œé€‰æœ€ä½³å“åº”ï¼›ä¸‰æ˜¯ä»ç°æœ‰å›¾åƒå’Œè§†é¢‘æŒ‡ä»¤è°ƒä¼˜è®­ç»ƒæ•°æ®ä¸­è¿‡æ»¤å¼‚å¸¸æˆ–å™ªå£°æ ·æœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
IXC - 2.5 - Rewardåœ¨æœ€æ–°å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹åŸºå‡†VL - RewardBenchä¸Šè¡¨ç°å‡ºè‰²ï¼Œå–å¾—70.0%çš„æˆç»©ï¼Œå‡»è´¥åŒ…æ‹¬Gemini - 1.5 - Proï¼ˆ62.5%ï¼‰å’ŒGPT - 4oï¼ˆ62.4%ï¼‰åœ¨å†…çš„ä¹‹å‰æ‰€æœ‰ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼›åœ¨çº¯æ–‡æœ¬å¥–åŠ±æ¨¡å‹åŸºå‡†ä¸Šä¹Ÿæœ‰ç«äº‰åŠ›ï¼Œåœ¨Reward - Benchä¸Šå¹³å‡å¾—åˆ†88.6%ï¼Œåœ¨RM - Benchä¸Šå¾—åˆ†68.8%ã€‚åŒæ—¶ï¼Œåœ¨RLè®­ç»ƒã€æµ‹è¯•æ—¶ç¼©æ”¾ã€æ•°æ®æ¸…ç†ä¸‰å¤§åº”ç”¨åœºæ™¯çš„å®éªŒä¹ŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¦‚IXC - 2.5 - Chatåœ¨å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªå’Œé‡å¤–èŠå¤©åŸºå‡†ä¸Šæœ‰æ˜æ˜¾æ”¹è¿›ç­‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€æ•°æ®æ„å»ºæ€è·¯ï¼šè®ºæ–‡æ„å»ºè·¨æ¨¡æ€ã€è·¨é¢†åŸŸå¤šæ¨¡æ€åå¥½è¯­æ–™åº“çš„æ–¹å¼ï¼Œä¸ºåç»­å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ•°æ®æ„å»ºæä¾›äº†å‚è€ƒï¼Œå¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å’Œé«˜è´¨é‡æ ‡æ³¨çš„é‡è¦æ€§ã€‚
2. æ¨¡å‹æ‰©å±•ä¸åº”ç”¨æ–¹å‘ï¼šä»å•æ¨¡æ€å¥–åŠ±æ¨¡å‹æ‰©å±•åˆ°å¤šæ¨¡æ€çš„æ€è·¯ï¼Œä»¥åŠå±•ç¤ºçš„å¥–åŠ±æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ã€æµ‹è¯•æ—¶ä¼˜åŒ–ã€æ•°æ®æ¸…ç†ç­‰å¤šåœºæ™¯åº”ç”¨ï¼Œä¸ºå¥–åŠ±æ¨¡å‹ä¹ƒè‡³å¤§æ¨¡å‹ç”Ÿæ€ä¸­ä¸åŒæ¨¡å—çš„åä½œå’ŒåŠŸèƒ½æ‹“å±•æä¾›äº†èŒƒä¾‹ã€‚
3. å¼€æºä¸å¯å¤ç°æ€§ï¼šå°†æ¨¡å‹æƒé‡å’Œè®­ç»ƒæ–¹æ¡ˆå¼€æºï¼Œåˆ©äºç¤¾åŒºåŸºäºæ­¤è¿›ä¸€æ­¥ç ”ç©¶ï¼Œè¿™ç§å¼€æºå…±äº«çš„åšæ³•ä¹Ÿå€¼å¾—ç›¸å…³ç ”ç©¶å€Ÿé‰´ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## meds$^3$--towards-medical-small-language-models-with-self-evolved-slow-thinking
### Abstract
Medical language models (MLMs) have become pivotal in advancing medical
natural language processing. However, prior models that rely on pre-training or
supervised fine-tuning often exhibit low data efficiency and limited
practicality in real-world clinical applications. While OpenAI's o1 highlights
test-time scaling in mathematics, attempts to replicate this approach in
medicine typically distill responses from GPT-series models to open-source
models, focusing primarily on multiple-choice tasks. This strategy, though
straightforward, neglects critical concerns like data privacy and realistic
deployment in clinical settings. In this work, we present a deployable,
small-scale medical reasoning system, MedS3, designed for long-chain reasoning
in clinical tasks using a self-evolution paradigm. Starting with a seed dataset
of around 8,000 instances spanning five domains and 16 datasets, we prompt a
base policy model to perform Monte Carlo Tree Search (MCTS) to construct
rule-verifiable reasoning chains. Each reasoning step is assigned an evolution
rollout value, allowing verified trajectories to train the policy model and the
process reward model (PRM). During inference, the policy model generates
multiple responses, and the reward model selects the one with a newly proposed
PRM-guided Vote-Sum (P-VS) strategy. Experiments on eleven evaluation datasets
demonstrate that MedS3 outperforms not only the prior strongest medical model
by 6.59, but also 32B-level general reasoning models by 8.71 points. Code and
data are available at https://github.com/pixas/MedSSS.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MedSÂ³ï¼šç”¨è‡ªè¿›åŒ–æ…¢æ€è€ƒæ‰“é€ å¯éƒ¨ç½²çš„åŒ»ç–—å°è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŒ»ç–—è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨æ¨åŠ¨åŒ»ç–—è‡ªç„¶è¯­è¨€å¤„ç†å‘å±•ä¸­è‡³å…³é‡è¦ï¼Œä½†ä»¥å¾€ä¾èµ–é¢„è®­ç»ƒæˆ–æœ‰ç›‘ç£å¾®è°ƒçš„æ¨¡å‹å­˜åœ¨æ•°æ®æ•ˆç‡ä½ã€å®é™…ä¸´åºŠåº”ç”¨å®ç”¨æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒéœ€å¤§é‡è®¡ç®—èµ„æºä¸”ä¸‹æ¸¸ä»»åŠ¡å¢ç›Šæœ‰é™ï¼›æœ‰ç›‘ç£å¾®è°ƒä¾èµ–çš„äººå·¥æ ‡æ³¨æ•°æ®é›†å¸¸æä¾›ç®€æ´å“åº”ï¼Œä¼šé™ä½æ¨¡å‹è¯­è¨€æµç•…æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆè¯­æ–™å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œç›´æ¥ç”¨äºè®­ç»ƒé™åˆ¶æ¨¡å‹ä¼˜åŒ–ç©ºé—´ã€‚åŒæ—¶ï¼Œå°è¯•å¤åˆ¶OpenAI o1åœ¨æ•°å­¦é¢†åŸŸâ€œæ…¢æ€è€ƒâ€æ¨ç†çš„åŒ»å­¦é¢†åŸŸå·¥ä½œï¼Œå¤šæ˜¯è’¸é¦GPTç³»åˆ—æ¨¡å‹å“åº”åˆ°å¼€æºæ¨¡å‹ï¼Œèšç„¦å¤šé€‰ä»»åŠ¡ï¼Œè¿˜å¿½è§†äº†æ•°æ®éšç§ä¸ä¸´åºŠéƒ¨ç½²ç­‰å…³é”®é—®é¢˜ã€‚å› æ­¤ï¼Œæ‰“é€ èƒ½åœ¨ä¸´åºŠä»»åŠ¡ä¸­è¿›è¡Œé•¿é“¾æ¨ç†ã€å¯éƒ¨ç½²çš„å°è§„æ¨¡åŒ»ç–—æ¨ç†ç³»ç»Ÿå¾ˆæœ‰å¿…è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªè¿›åŒ–æ¡†æ¶èµ‹èƒ½å°æ¨¡å‹é•¿é“¾æ¨ç†  
æå‡ºé¦–ä¸ªä¸“ä¸ºå°è§„æ¨¡åŒ»ç–—æ¨¡å‹è®¾è®¡çš„è‡ªè¿›åŒ–æ¡†æ¶ï¼Œä»¥å®ç°é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚ä»æ¶µç›–5ä¸ªé¢†åŸŸã€16ä¸ªæ•°æ®é›†çš„çº¦8000ä¸ªå®ä¾‹çš„ç§å­æ•°æ®é›†å…¥æ‰‹ï¼Œå¼•å¯¼åŸºç¡€ç­–ç•¥æ¨¡å‹æ‰§è¡Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æ„å»ºå¯è§„åˆ™éªŒè¯çš„æ¨ç†é“¾ã€‚æ¯ä¸ªæ¨ç†æ­¥éª¤åˆ†é…è¿›åŒ–rolloutå€¼ï¼Œè®©ç»è¿‡éªŒè¯çš„è½¨è¿¹å»è®­ç»ƒç­–ç•¥æ¨¡å‹å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œå®ç°æ•°æ®é«˜æ•ˆçš„æ€§èƒ½æå‡ï¼Œé€‚ç”¨äºå¹¿æ³›ä¸´åºŠåº”ç”¨åœºæ™¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºç­–ç•¥æ¨¡å‹ä¸è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ååŒä½“ç³»  
 - ç­–ç•¥æ¨¡å‹Ï€ï¼šé€šè¿‡åœ¨MCTSç”Ÿæˆçš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ å¾®è°ƒåŸºç¡€ç­–ç•¥Ï€â‚€å¾—åˆ°ï¼Œç”¨äºç”Ÿæˆæ¨ç†è¿‡ç¨‹ã€‚  
 - è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰VÎ¸ï¼šç”¨å¸¦è½¯æ ‡ç­¾çš„é€æ­¥ç›‘ç£è¿›è¡Œå¾®è°ƒï¼Œç»™æ¯ä¸ªæ¨ç†æ­¥éª¤åˆ†é…[0,1]èŒƒå›´çš„å€¼è¡¨ç¤ºæ­¥éª¤æ­£ç¡®æ€§ï¼Œä¸ºæ¨ç†å‡†ç¡®æ€§æä¾›ç»†ç²’åº¦æŒ‡å¯¼ã€‚  
 - æ¨ç†é˜¶æ®µé‡‡ç”¨PRMå¼•å¯¼çš„Vote - Sumï¼ˆP - VSï¼‰ç­–ç•¥ï¼šç­–ç•¥æ¨¡å‹ç”Ÿæˆå¤šä¸ªå“åº”åï¼Œå¥–åŠ±æ¨¡å‹åŸºäºè¯¥ç­–ç•¥é€‰å‡ºæœ€ä¼˜è§£ï¼Œä»è®­ç»ƒåˆ°æ¨ç†ç¯èŠ‚å½¢æˆååŒä¼˜åŒ–ä¸é€‰æ‹©æœºåˆ¶ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨11ä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMedSÂ³ä¸ä»…æ¯”ä¹‹å‰æœ€å¼ºçš„åŒ»ç–—æ¨¡å‹æ€§èƒ½é«˜å‡º6.59åˆ†ï¼Œè¿˜è¶…è¿‡äº†32Bè§„æ¨¡çš„é€šç”¨æ¨ç†æ¨¡å‹8.71åˆ†ï¼Œåœ¨ä¸´åºŠæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå…¨é¢çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è‡ªè¿›åŒ–æ¡†æ¶ä¸è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ç­‰è®¾è®¡åœ¨åŒ»ç–—é•¿é“¾æ¨ç†ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡å±‚é¢ï¼šè‡ªè¿›åŒ–æ¡†æ¶ä¸ºèµ„æºå—é™åœºæ™¯ä¸‹å°æ¨¡å‹æå‡ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰é•¿é“¾æ¨ç†èƒ½åŠ›æä¾›äº†æ–°æ€è·¯ï¼Œæ‘†è„±å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒæˆ–é—­æºå¤§æ¨¡å‹è’¸é¦çš„å¼ºä¾èµ–ï¼Œå®ç°æ•°æ®é«˜æ•ˆåˆ©ç”¨ã€‚  
2. æ¨¡å‹ååŒå±‚é¢ï¼šç­–ç•¥æ¨¡å‹ä¸è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„ååŒè®¾è®¡ï¼Œä»¥åŠæ¨ç†æ—¶çš„å¼•å¯¼é€‰æ‹©ç­–ç•¥ï¼Œä¸ºæ„å»ºæ›´ç²¾å‡†ã€å¯è§£é‡Šçš„æ¨ç†å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ¨¡å—åŒ–çš„å‚è€ƒèŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è¦é•¿é“¾æ¨ç†çš„å‚ç›´é¢†åŸŸã€‚  
3. å¼€æºèµ„æºå±‚é¢ï¼šå…¬å¼€é‡Šæ”¾ç­–ç•¥å¾®è°ƒè¯­æ–™å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹è¯­æ–™ï¼Œä¸ºåŒ»ç–—AIé¢†åŸŸåç»­ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºï¼Œåˆ©äºè¡Œä¸šå…±å»ºç”Ÿæ€æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ã€‚

## rearter--retrieval-augmented-reasoning-with-trustworthy-process-rewarding
### Abstract
Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)
hold promise in knowledge-intensive tasks but face limitations in complex
multi-step reasoning. While recent methods have integrated RAG with
chain-of-thought reasoning or test-time search using Process Reward Models
(PRMs), these approaches encounter challenges such as a lack of explanations,
bias in PRM training data, early-step bias in PRM scores, and insufficient
post-training optimization of reasoning potential. To address these issues, we
propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding
(ReARTeR), a framework that enhances RAG systems' reasoning capabilities
through post-training and test-time scaling. At test time, ReARTeR introduces
Trustworthy Process Rewarding via a Process Reward Model for accurate scalar
scoring and a Process Explanation Model (PEM) for generating natural language
explanations, enabling step refinement. During post-training, it utilizes Monte
Carlo Tree Search guided by Trustworthy Process Rewarding to collect
high-quality step-level preference data, optimized through Iterative Preference
Optimization. ReARTeR addresses three core challenges: (1) misalignment between
PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM
training data, mitigated by balanced annotation methods and stronger
annotations for challenging examples; and (3) early-step bias in PRM, resolved
through a temporal-difference-based look-ahead search strategy. Experimental
results on multi-step reasoning benchmarks demonstrate significant
improvements, underscoring ReARTeR's potential to advance the reasoning
capabilities of RAG systems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReARTeRï¼šç”¨å¯ä¿¡è¿‡ç¨‹å¥–åŠ±å¢å¼ºæ£€ç´¢å¢å¼ºæ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤æ‚å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸Šä»å­˜åœ¨å±€é™ã€‚ç°æœ‰ç»“åˆRAGä¸æ€ç»´é“¾æ¨ç†æˆ–ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åšæµ‹è¯•æ—¶æœç´¢çš„æ–¹æ³•ï¼Œé¢ä¸´ç¼ºä¹è§£é‡Šæ€§ã€PRMè®­ç»ƒæ•°æ®å­˜åœ¨åå·®ã€PRMåˆ†æ•°çš„æ—©æœŸæ­¥éª¤åå·®ä»¥åŠæ¨ç†æ½œåŠ›åœ¨è®­ç»ƒåä¼˜åŒ–ä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºReARTeRæ¡†æ¶ï¼Œä»è®­ç»ƒåé˜¶æ®µå’Œæµ‹è¯•æ—¶æ‰©å±•ä¸¤æ–¹é¢å¢å¼ºRAGç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¯ä¿¡è¿‡ç¨‹å¥–åŠ±æœºåˆ¶ï¼ˆæµ‹è¯•æ—¶é˜¶æ®µï¼‰
ReARTeRåœ¨æµ‹è¯•æ—¶å¼•å…¥å¯ä¿¡è¿‡ç¨‹å¥–åŠ±ï¼Œç”±è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å’Œè¿‡ç¨‹è§£é‡Šæ¨¡å‹ï¼ˆPEMï¼‰å…±åŒå®ç°ã€‚PRMæä¾›ç²¾ç¡®çš„æ ‡é‡åˆ†æ•°ç”¨äºè¯„ä¼°æ¨ç†æ­¥éª¤ï¼ŒPEMç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šæ¥è¾…åŠ©å¯¹ä½åˆ†æ­¥éª¤è¿›è¡Œä¼˜åŒ–ï¼Œè®©æ¨ç†æ­¥éª¤å¯ç²¾ç»†åŒ–è°ƒæ•´ï¼Œè§£å†³äº†ç°æœ‰PRMç¼ºä¹è§£é‡Šæ€§ã€ä¸åˆ©äºæµ‹è¯•æ—¶æ¨ç†ä¼˜åŒ–çš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒåé˜¶æ®µçš„ä¼˜åŒ–ç­–ç•¥
åœ¨è®­ç»ƒåé˜¶æ®µï¼Œåˆ©ç”¨å¯ä¿¡è¿‡ç¨‹å¥–åŠ±å¼•å¯¼çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æ”¶é›†é«˜è´¨é‡çš„æ­¥éª¤çº§åå¥½æ•°æ®ï¼Œå†é€šè¿‡è¿­ä»£åå¥½ä¼˜åŒ–å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚é’ˆå¯¹PRMä¸PEMä¸ä¸€è‡´é—®é¢˜ï¼Œé‡‡ç”¨ç¦»ç­–ç•¥åå¥½å­¦ä¹ ï¼Œä¾æ®PEMè§£é‡Šä¼˜åŒ–å‰åPRMåˆ†æ•°çš„å˜åŒ–æ¥æ„å»ºåå¥½æ ‡ç­¾ï¼Œè®©äºŒè€…å¯¹é½ï¼›é’ˆå¯¹PRMè®­ç»ƒæ•°æ®åå·®ï¼Œå€ŸåŠ©OmegaPRMå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼Œå¹¶å¯¹éš¾é¢˜å¼•å…¥æ›´å¼ºæ¨¡å‹æˆ–äººç±»ä¸“å®¶æ ‡æ³¨æ¥æå‡PRMå¯¹å¤æ‚åœºæ™¯çš„åˆ¤åˆ«åŠ›ï¼›é’ˆå¯¹PRMæ—©æœŸæ­¥éª¤åå·®ï¼Œæå‡ºåŸºäºæ—¶åºå·®åˆ†ï¼ˆTDï¼‰çš„å‰ç»æœç´¢ç­–ç•¥ï¼Œç”¨æ¨¡æ‹Ÿæœªæ¥æ¨ç†æ­¥éª¤è®¡ç®—é¢„æœŸå¥–åŠ±æ¥æ›´æ–°å½“å‰æ­¥éª¤å¥–åŠ±ä¼°è®¡ï¼Œå¹³è¡¡åå·®ä¸æ–¹å·®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šæ­¥æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReARTeRå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶åœ¨æå‡RAGç³»ç»Ÿæ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹å¤æ‚å¤šæ­¥æ¨ç†åœºæ™¯ä¸‹çš„å„ç±»æŒ‘æˆ˜ï¼Œè®©RAGç³»ç»Ÿåœ¨æ¨ç†è¡¨ç°ä¸Šæ›´ä¸Šä¸€å±‚æ¥¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šé˜¶æ®µååŒä¼˜åŒ–æ€è·¯ï¼šå°†è®­ç»ƒåä¼˜åŒ–ä¸æµ‹è¯•æ—¶å¢å¼ºç›¸ç»“åˆï¼Œä¸ºæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†å…¨æµç¨‹ä¼˜åŒ–çš„å‚è€ƒèŒƒå¼ï¼Œå¯ç¤ºåç»­å·¥ä½œå¯ä»ä¸åŒé˜¶æ®µå…¥æ‰‹ç³»ç»Ÿæ€§å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚
2. è§£å†³æ¨¡å‹ç»„ä»¶ä¸ä¸€è‡´é—®é¢˜çš„æ–¹æ³•ï¼šç¦»ç­–ç•¥åå¥½å­¦ä¹ ä¸ºè§£å†³ä¸åŒæ¨¡å‹ç»„ä»¶ï¼ˆå¦‚è¿™é‡Œçš„PRMå’ŒPEMï¼‰ä¹‹é—´çš„å¯¹é½é—®é¢˜æä¾›äº†æ–°é¢–æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å¤šç»„ä»¶åä½œçš„AIç³»ç»Ÿä¼˜åŒ–ä¸­ã€‚
3. æ•°æ®åå·®ä¸æ­¥éª¤åå·®å¤„ç†ï¼šé’ˆå¯¹è®­ç»ƒæ•°æ®åå·®é‡‡ç”¨çš„å¹³è¡¡æ ‡æ³¨å’Œå¼ºæ ‡æ³¨æ–¹æ³•ã€é’ˆå¯¹æ—©æœŸæ­¥éª¤åå·®çš„æ—¶åºå·®åˆ†å‰ç»æœç´¢ï¼Œä¸ºå¤„ç†æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å„ç±»åå·®é—®é¢˜æä¾›äº†å…·ä½“æŠ€æœ¯æ‰‹æ®µï¼Œåœ¨ç±»ä¼¼å­˜åœ¨æ•°æ®åˆ†å¸ƒä¸å‡ã€æ­¥éª¤ç‰¹æ€§å·®å¼‚å¯¼è‡´æ€§èƒ½é—®é¢˜çš„åœºæ™¯ä¸­å¯å€Ÿé‰´ã€‚ 

## ursa--understanding-and-verifying-chain-of-thought-reasoning-in-multimodal-mathematics
### Abstract
Process Reward Models (PRMs) have shown promise in enhancing the mathematical
reasoning capabilities of Large Language Models (LLMs) through Test-Time
Scaling (TTS). However, their integration into multimodal reasoning remains
largely unexplored. In this work, we take the first step toward unlocking the
potential of PRMs in multimodal mathematical reasoning. We identify three key
challenges: (1) the scarcity of high-quality reasoning data constrains the
capabilities of foundation Multimodal Large Language Models (MLLMs), which
imposes further limitations on the upper bounds of TTS and reinforcement
learning (RL); (2) a lack of automated methods for process labeling within
multimodal contexts persists; (3) the employment of process rewards in unimodal
RL faces issues like reward hacking, which may extend to multimodal scenarios.
To address these issues, we introduce URSA, a three-stage Unfolding multimodal
Process-Supervision Aided training framework. We first construct MMathCoT-1M, a
high-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset,
to build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we
go through an automatic process to synthesize process supervision data, which
emphasizes both logical correctness and perceptual consistency. We introduce
DualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose
Process-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a
multimodal PRM-aided online RL method that outperforms vanilla GRPO. With
PS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4%
and 2.7% on average across 6 benchmarks. Code, data and checkpoint can be found
at https://github.com/URSA-MATH.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | URSAï¼šè§£é”å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ½œåŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•°å­¦æ¨ç†å–å¾—è¿›å±•åï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ä¹Ÿå¤‡å—å…³æ³¨ã€‚è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨æå‡LLMsæ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„åº”ç”¨ä»æœªå……åˆ†æ¢ç´¢ã€‚å½“å‰å­˜åœ¨ä¸‰å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯é«˜è´¨é‡æ¨ç†æ•°æ®ç¨€ç¼ºï¼Œé™åˆ¶äº†åŸºç¡€MLLMsèƒ½åŠ›ï¼Œè¿›è€Œå½±å“æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸Šé™ï¼›äºŒæ˜¯å¤šæ¨¡æ€æƒ…å¢ƒä¸‹ç¼ºä¹è‡ªåŠ¨åŒ–è¿‡ç¨‹æ ‡æ³¨æ–¹æ³•ï¼›ä¸‰æ˜¯å•æ¨¡æ€RLä¸­è¿‡ç¨‹å¥–åŠ±å­˜åœ¨å¥–åŠ±é»‘å®¢ç­‰é—®é¢˜ï¼Œå¯èƒ½å»¶ä¼¸åˆ°å¤šæ¨¡æ€åœºæ™¯ã€‚æœ¬æ–‡æ—¨åœ¨è¿ˆå‡ºå°†PRMsèå…¥å¤šæ¨¡æ€æ•°å­¦æ¨ç†çš„ç¬¬ä¸€æ­¥ï¼Œåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºURSAä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶  
æå‡ºURSAï¼Œä¸€ä¸ªä¸‰é˜¶æ®µçš„â€œå±•å¼€å¼å¤šæ¨¡æ€è¿‡ç¨‹ç›‘ç£è¾…åŠ©â€è®­ç»ƒæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µæ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ•°æ®é›†MMathCoT - 1Mï¼Œç”¨äºæ‰“é€ æ›´å¼ºçš„æ•°å­¦æ¨ç†åŸºç¡€MLLMï¼ˆURSA - 8Bï¼‰ã€‚ä»143ä¸‡å¼€æºç¤ºä¾‹åˆæˆè¯¥æ•°æ®é›†ï¼Œé€šè¿‡é’ˆå¯¹æ€§æŒ‡ä»¤è°ƒä¼˜å¢å¼ºåŸºç¡€æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µè‡ªåŠ¨åˆæˆè¿‡ç¨‹ç›‘ç£æ•°æ®ï¼Œæ„å»ºDualMath - 1.1Mç”¨äºè®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ•°æ®åˆæˆç­–ç•¥ç»“åˆäºŒè¿›åˆ¶é”™è¯¯å®šä½å¼•æ“å’Œè¯¯è§£æ’å…¥å¼•æ“ï¼Œå¼ºè°ƒé€»è¾‘æ­£ç¡®æ€§å’Œæ„ŸçŸ¥ä¸€è‡´æ€§ï¼›ç¬¬ä¸‰é˜¶æ®µæå‡ºProcess - Supervised Group - Relative - Policy - Optimizationï¼ˆPS - GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€PRMè¾…åŠ©çš„åœ¨çº¿RLæ–¹æ³•ï¼Œä¼˜äºåŸå§‹GRPOï¼Œé€šè¿‡åœ¨ç­–ç•¥ä¼˜åŒ–ä¸­éšå¼æƒ©ç½šè¿‡ç¨‹çº§ä¸ä¸€è‡´ï¼Œç¼“è§£å¥–åŠ±é»‘å®¢å’Œé•¿åº¦åå·®é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå‘å¸ƒä¸¤å¤§å¼€æºæ•°æ®é›†  
å‘å¸ƒMMathCoT - 1Må’ŒDualMath - 1.1Mä¸¤ä¸ªå¤§è§„æ¨¡å¼€æºæ•°æ®é›†ã€‚MMathCoT - 1Mè§£å†³é«˜è´¨é‡å¤šæ¨¡æ€CoTæ¨ç†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒDualMath - 1.1Mè§£å†³è¿‡ç¨‹ç›‘ç£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€æ•°å­¦æ¨ç†ç ”ç©¶æä¾›æ•°æ®æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºPS - GRPOç®—æ³•  
æå‡ºPS - GRPOåœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡æ¯”è¾ƒè½¨è¿¹çš„ç›¸å¯¹è´¨é‡æ¥æ•´åˆå¤šæ¨¡æ€PRMsï¼Œè€Œéä¾èµ–æ ‡é‡å¥–åŠ±å»ºæ¨¡ï¼Œæœ‰æ•ˆç¼“è§£äº†PRMçš„å¥–åŠ±é»‘å®¢å’Œå¥–åŠ±ä¸­çš„é•¿åº¦åå·®é—®é¢˜ï¼Œåœ¨åœ¨çº¿è®­ç»ƒä¸­å‘æŒ¥ä½œç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨6ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œåº”ç”¨PS - GRPOçš„URSA - 8B - PS - GRPOè¡¨ç°å‡ºè‰²ã€‚å¹³å‡è€Œè¨€ï¼Œå®ƒåœ¨è¿™6ä¸ªåŸºå‡†ä¸Šè¶…è¶ŠGemma3 - 12B 8.4%ï¼Œè¶…è¶ŠGPT - 4o 2.7%ï¼Œåœ¨åŒç±»è§„æ¨¡å¼€æºMLLMsä¸­è¾¾åˆ° state - of - the - art æ€§èƒ½ï¼Œä¸”è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æå‡äº†æµ‹è¯•æ—¶éªŒè¯æ•ˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºæ–¹é¢ï¼šé€šè¿‡æ•´åˆä¸å¤„ç†å¼€æºæ•°æ®æ¥æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›åšå®åŸºç¡€ï¼Œè¿™ç§æ•°æ®é©±åŠ¨çš„æ€è·¯åœ¨å¤šæ¨¡æ€é¢†åŸŸæ•°æ®ç¨€ç¼ºæ—¶å€¼å¾—å€Ÿé‰´ï¼Œå¯ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡æ•°æ®æ„å»ºã€‚
2. æ¡†æ¶è®¾è®¡æ–¹é¢ï¼šä¸‰é˜¶æ®µçš„URSAæ¡†æ¶ä¸ºå¤šæ¨¡æ€ä¸‹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„åº”ç”¨æä¾›äº†å®Œæ•´çš„ pipeline å‚è€ƒï¼Œä»åŸºç¡€æ¨¡å‹å¢å¼ºåˆ°è¿‡ç¨‹ç›‘ç£æ•°æ®åˆæˆå†åˆ°å¼ºåŒ–å­¦ä¹ æ–¹æ³•åˆ›æ–°ï¼Œå„é˜¶æ®µç¯ç¯ç›¸æ‰£ï¼Œä¸ºè§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­å¤šä¸ªç—›ç‚¹æä¾›äº†ä½“ç³»åŒ–æ€è·¯ã€‚
3. ç®—æ³•åˆ›æ–°æ–¹é¢ï¼šPS - GRPOé’ˆå¯¹å¤šæ¨¡æ€ä¸‹å¼ºåŒ–å­¦ä¹ åº”ç”¨è¿‡ç¨‹å¥–åŠ±çš„é—®é¢˜ï¼Œæå‡ºç›¸å¯¹è´¨é‡æ¯”è¾ƒçš„æ–¹å¼æ›¿ä»£æ ‡é‡å¥–åŠ±å»ºæ¨¡ï¼Œä¸ºè§£å†³å¥–åŠ±é»‘å®¢ç­‰é—®é¢˜æä¾›äº†æ–°çš„ç®—æ³•è§†è§’ï¼Œå¯å¯å‘åç»­å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡ã€‚
```

## enhancing-llm-reasoning-with-reward-guided-tree-search
### Abstract
Recently, test-time scaling has garnered significant attention from the
research community, largely due to the substantial advancements of the o1 model
released by OpenAI. By allocating more computational resources during the
inference phase, large language models~(LLMs) can extensively explore the
solution space by generating more thought tokens or diverse solutions, thereby
producing more accurate responses. However, developing an o1-like reasoning
approach is challenging, and researchers have been making various attempts to
advance this open area of research. In this paper, we present a preliminary
exploration into enhancing the reasoning abilities of LLMs through
reward-guided tree search algorithms. This framework is implemented by
integrating the policy model, reward model, and search algorithm. It is
primarily constructed around a tree search algorithm, where the policy model
navigates a dynamically expanding tree guided by a specially trained reward
model. The implemented framework is denoted as \textbf{STILL-1}. We thoroughly
explore various design considerations necessary for implementing this framework
and provide a detailed report of the technical aspects. To assess the
effectiveness of our approach, we focus on mathematical reasoning tasks and
conduct extensive evaluations on four challenging datasets, significantly
enhancing the reasoning abilities of LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¥–åŠ±å¼•å¯¼æ ‘æœç´¢å¢å¼ºå¤§æ¨¡å‹æ¨ç†ï¼šSTILL - 1æ¡†æ¶æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒæ—¶é€šè¿‡æ•°æ®å’Œå‚æ•°è§„æ¨¡æ‰©å±•å–å¾—è¿›å±•ï¼Œä½†å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚STEMé¢†åŸŸä»»åŠ¡ï¼‰è¡¨ç°æœ‰é™ã€‚æµ‹è¯•æ—¶æ‰©å±•ï¼ˆtest - time scalingï¼‰å—å…³æ³¨ï¼ŒOpenAIçš„o1æ¨¡å‹å±•ç¤ºäº†é€šè¿‡æ¨ç†æ—¶æ›´å¤šè®¡ç®—èµ„æºæå‡æ€§èƒ½çš„æ½œåŠ›ï¼Œç„¶è€Œo1æ ¸å¿ƒæŠ€æœ¯æœªå…¬å¼€ï¼Œå¤åˆ»ç±»ä¼¼æ¨ç†æ–¹æ³•å…·æŒ‘æˆ˜ã€‚ç ”ç©¶ç¤¾åŒºå°è¯•æ¢ç´¢åŸºäºæœç´¢çš„æ¨ç†æ¡†æ¶ï¼Œè€Œå®ç°è¿™ç±»æ¡†æ¶éœ€è¯¸å¤šè®¾è®¡è€ƒé‡ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¥–åŠ±å¼•å¯¼æ ‘æœç´¢ç®—æ³•å¢å¼ºLLMsæ¨ç†èƒ½åŠ›çš„åˆæ­¥æ–¹æ¡ˆï¼Œå¡«è¡¥æŠ€æœ¯ç»†èŠ‚æŠ¥å‘Šçš„éƒ¨åˆ†ç©ºç™½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSTILL - 1æ¡†æ¶æ¶æ„
æ„å»ºæ•´åˆç­–ç•¥æ¨¡å‹ï¼ˆpolicy modelï¼‰ã€å¥–åŠ±æ¨¡å‹ï¼ˆreward modelï¼‰å’Œæœç´¢ç®—æ³•çš„å¥–åŠ±å¼•å¯¼æ ‘æœç´¢æ¡†æ¶ã€‚æ¡†æ¶å›´ç»•æ ‘æœç´¢ç®—æ³•æ„å»ºï¼Œç­–ç•¥æ¨¡å‹åœ¨ç»ç‰¹æ®Šè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹å¼•å¯¼ä¸‹ï¼Œåœ¨åŠ¨æ€æ‰©å±•çš„æ ‘ä¸­å¯¼èˆªæ¨ç†æ­¥éª¤ã€‚
- ç­–ç•¥æ¨¡å‹ï¼šåŒ…å«æ¨ç†æ ¼å¼é€‚é…çš„æŒ‡ä»¤å¾®è°ƒä¸ç­–ç•¥æ”¹è¿›çš„åå¥½ä¼˜åŒ–ä¸¤æ­¥è®­ç»ƒã€‚æ¢ç´¢é€‚é…æœç´¢æ ‘ç»“æ„å®šä¹‰çš„æ¨ç†æ ¼å¼ï¼Œä»¥åŠåœ¨å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ä¸‹ç”¨æ„é€ è®­ç»ƒæ•°æ®åšåå¥½ä¼˜åŒ–ï¼ˆå¦‚DPOç­‰ï¼‰ã€‚
- å¥–åŠ±æ¨¡å‹ï¼šæ¢ç©¶åˆ¤åˆ«å¼/ç”Ÿæˆå¼å½¢å¼é€‰æ‹©ã€ç»“æœ/è¿‡ç¨‹ç›‘ç£è®­ç»ƒã€æ’åº/åˆ†æ•°ä¼˜åŒ–ç­‰å…³é”®è®¾è®¡ç‚¹ï¼Œè¿˜æ¢ç´¢ä¸ç­–ç•¥æ¨¡å‹è¿­ä»£äº’ç²¾ï¼ˆiterative mutual refinementï¼‰ï¼Œå¹¶ç»™å‡ºè¯¦ç»†è®­ç»ƒç»†èŠ‚ã€‚
- æ ‘æœç´¢ï¼šå®ç°ç±»MCTSç®—æ³•è¾…åŠ©ç­–ç•¥æ¨¡å‹æ¨ç†ï¼Œä»æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸¤æ–¹é¢ä¼˜åŒ–ä»¥é€‚é…æ•°å­¦æ¨ç†ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèšç„¦æ•°å­¦æ¨ç†ä»»åŠ¡çš„å…¨é¢æŠ€æœ¯æ¢ç´¢
é’ˆå¯¹æ•°å­¦æ¨ç†ä»»åŠ¡ï¼ˆæ–‡æœ¬æè¿°çš„æ•°å­¦é—®é¢˜ï¼‰ï¼Œå…¨é¢æ¢ç´¢æ¡†æ¶å„ç»„ä»¶å®ç°ç»†èŠ‚ã€‚å¯¹ç­–ç•¥æ¨¡å‹ï¼Œç ”ç©¶å¦‚ä½•é€‚é…æ¨ç†æ ¼å¼ä¸åšåå¥½ä¼˜åŒ–ï¼›å¯¹å¥–åŠ±æ¨¡å‹ï¼Œæ·±å…¥å…³é”®è®¾è®¡è€ƒé‡ä¸è®­ç»ƒï¼›å¯¹æ ‘æœç´¢ï¼Œä¼˜åŒ–ç®—æ³•é€‚é…æ•°å­¦æ¨ç†ï¼Œä¸ºè¯¥é¢†åŸŸåŸºäºæœç´¢çš„æ¨ç†æ¡†æ¶å®ç°æä¾›æŠ€æœ¯å‚è€ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªå…·æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æ•°æ®é›†ï¼ˆMATH - OAIã€GSM - Hardã€Olympiad Benchã€College Mathï¼‰ä¸Šå¼€å±•å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„STILL - 1æ¨ç†æ¡†æ¶æ˜¾è‘—æå‡äº†ç­–ç•¥æ¨¡å‹åœ¨è¿™äº›æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œå¯¹ç­–ç•¥æ¨¡å‹ã€å¥–åŠ±æ¨¡å‹å’Œæ ‘æœç´¢ç®—æ³•è®¾è®¡å¼€å±•æ·±å…¥å®è¯åˆ†æï¼Œèƒ½ä¸ºç ”ç©¶äººå‘˜æä¾›æœ‰æ„ä¹‰çš„æŒ‡å¯¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šæ•´åˆç­–ç•¥ã€å¥–åŠ±æ¨¡å‹ä¸æœç´¢ç®—æ³•çš„æ€è·¯ï¼Œä¸ºæå‡å¤§æ¨¡å‹å¤æ‚æ¨ç†èƒ½åŠ›æä¾›äº†åŸºäºâ€œæœç´¢ + åé¦ˆå¼•å¯¼â€çš„èŒƒå¼å‚è€ƒï¼Œå¯å¯å‘åç»­åœ¨å…¶ä»–å¤æ‚ä»»åŠ¡ï¼ˆå¦‚ codingã€åŒ»ç–—è¯Šæ–­ç­‰ï¼‰ä¸Šçš„æ¨ç†æ¡†æ¶è®¾è®¡ã€‚
2. ç»„ä»¶å®ç°æ¢ç´¢ï¼šå¯¹ç­–ç•¥æ¨¡å‹çš„æ¨ç†æ ¼å¼é€‚é…ä¸åå¥½ä¼˜åŒ–ã€å¥–åŠ±æ¨¡å‹çš„å¤šç§è®¾è®¡è€ƒé‡ï¼ˆå½¢å¼ã€ç›‘ç£æ–¹å¼ã€ä¼˜åŒ–ç›®æ ‡ç­‰ï¼‰ã€æ ‘æœç´¢çš„ç±»MCTSå®ç°ä¸ä¼˜åŒ–ç­‰æŠ€æœ¯ç»†èŠ‚çš„æ¢ç´¢ï¼Œä¸ºç ”ç©¶è€…åœ¨å¤ç°æˆ–æ”¹è¿›è¿™ç±»æœç´¢å¢å¼ºæ¨ç†æ¡†æ¶æ—¶æä¾›äº†å®è·µå±‚é¢çš„å‚è€ƒï¼Œé™ä½æŠ€æœ¯å°è¯•é—¨æ§›ã€‚
3. ä»»åŠ¡é€‚é…ä¸è¯„ä¼°ï¼šèšç„¦æ•°å­¦æ¨ç†ä»»åŠ¡å¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†éªŒè¯æœ‰æ•ˆæ€§ï¼Œè¿™ç§â€œç‰¹å®šå¤æ‚ä»»åŠ¡ + å¤šæ•°æ®é›†è¯„ä¼°â€çš„æ¨¡å¼ï¼Œå¯ä¸ºå…¶ä»–ç ”ç©¶åœ¨ä»»åŠ¡é€‰æ‹©ä¸æ•ˆæœéªŒè¯æ–¹é¢æä¾›å€Ÿé‰´ï¼ŒåŠ©åŠ›é¢†åŸŸå†…å¯¹ä¸åŒå¤æ‚ä»»åŠ¡æ¨ç†èƒ½åŠ›æå‡çš„ç ”ç©¶æ¨è¿›ã€‚

