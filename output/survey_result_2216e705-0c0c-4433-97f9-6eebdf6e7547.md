# Paper List of Terms(RL+entropy)
- [25/10] **BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping**  
[[Paper](http://arxiv.org/pdf/2510.18927v1)] [[Code/Page]()] [[TLDR/Notes](#bapo--stabilizing-off-policy-reinforcement-learning-for-llms-via-balanced-policy-optimization-with-adaptive-clipping)]

- [25/10] **CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections**  
[[Paper](http://arxiv.org/pdf/2510.17921v1)] [[Code/Page]()] [[TLDR/Notes](#claws-creativity-detection-for-llm-generated-solutions-using-attention-window-of-sections)]

- [25/10] **D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks**  
[[Paper](http://arxiv.org/pdf/2510.17212v1)] [[Code/Page]()] [[TLDR/Notes](#d2c-hrhr--discrete-actions-with-double-distributional-critics-for-high-risk-high-return-tasks)]

- [25/10] **Video Reasoning without Training**  
[[Paper](http://arxiv.org/pdf/2510.17045v1)] [[Code/Page]()] [[TLDR/Notes](#video-reasoning-without-training)]

- [25/10] **The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling**  
[[Paper](http://arxiv.org/pdf/2510.15502v1)] [[Code/Page](https://github.com/MuLabPKU/sesa.)] [[TLDR/Notes](#the-road-less-traveled--enhancing-exploration-in-llms-via-sequential-sampling)]

- [25/10] **Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization**  
[[Paper](http://arxiv.org/pdf/2510.15165v1)] [[Code/Page]()] [[TLDR/Notes](#policy-transfer-ensures-fast-learning-for-continuous-time-lqr-with-entropy-regularization)]

- [25/10] **DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2510.15110v1)] [[Code/Page]()] [[TLDR/Notes](#dler--doing-length-penalty-right---incentivizing-more-intelligence-per-token-via-reinforcement-learning)]

- [25/10] **Agentic Entropy-Balanced Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2510.14545v1)] [[Code/Page]()] [[TLDR/Notes](#agentic-entropy-balanced-policy-optimization)]

- [25/10] **DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping**  
[[Paper](http://arxiv.org/pdf/2510.12979v1)] [[Code/Page]()] [[TLDR/Notes](#deepplanner--scaling-planning-capability-for-deep-research-agents-via-advantage-shaping)]

- [25/10] **Finite-time Convergence Analysis of Actor-Critic with Evolving Reward**  
[[Paper](http://arxiv.org/pdf/2510.12334v1)] [[Code/Page]()] [[TLDR/Notes](#finite-time-convergence-analysis-of-actor-critic-with-evolving-reward)]

- [25/10] **Reinforced sequential Monte Carlo for amortised sampling**  
[[Paper](http://arxiv.org/pdf/2510.11711v1)] [[Code/Page]()] [[TLDR/Notes](#reinforced-sequential-monte-carlo-for-amortised-sampling)]

- [25/10] **Demystifying Reinforcement Learning in Agentic Reasoning**  
[[Paper](http://arxiv.org/pdf/2510.11701v1)] [[Code/Page](https://github.com/Gen-Verse/Open-AgentRL)] [[TLDR/Notes](#demystifying-reinforcement-learning-in-agentic-reasoning)]

- [25/10] **QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs**  
[[Paper](http://arxiv.org/pdf/2510.11696v1)] [[Code/Page]()] [[TLDR/Notes](#qerl--beyond-efficiency----quantization-enhanced-reinforcement-learning-for-llms)]

- [25/10] **Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning**  
[[Paper](http://arxiv.org/pdf/2510.10207v2)] [[Code/Page]()] [[TLDR/Notes](#adaptive-dual-reasoner--large-reasoning-models-can-think-efficiently-by-hybrid-reasoning)]

- [25/10] **Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models**  
[[Paper](http://arxiv.org/pdf/2510.09259v1)] [[Code/Page]()] [[TLDR/Notes](#detecting-data-contamination-from-reinforcement-learning-post-training-for-large-language-models)]

- [25/10] **Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2510.08526v1)] [[Code/Page]()] [[TLDR/Notes](#convergence-theorems-for-entropy-regularized-and-distributional-reinforcement-learning)]

- [25/10] **Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2510.08233v1)] [[Code/Page](https://github.com/yuchen-zhu-zyc/DMPO.)] [[TLDR/Notes](#enhancing-reasoning-for-diffusion-llms-via-distribution-matching-policy-optimization)]

- [25/10] **TTRV: Test-Time Reinforcement Learning for Vision Language Models**  
[[Paper](http://arxiv.org/pdf/2510.06783v1)] [[Code/Page]()] [[TLDR/Notes](#ttrv--test-time-reinforcement-learning-for-vision-language-models)]

- [25/10] **Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them**  
[[Paper](http://arxiv.org/pdf/2510.06534v1)] [[Code/Page]()] [[TLDR/Notes](#beneficial-reasoning-behaviors-in-agentic-search-and-effective-post-training-to-obtain-them)]

- [25/10] **MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations**  
[[Paper](http://arxiv.org/pdf/2510.04508v1)] [[Code/Page](https://github.com/xiewilliams/MARCO.)] [[TLDR/Notes](#marco--a-cooperative-knowledge-transfer-framework-for-personalized-cross-domain-recommendations)]

- [25/10] **Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners**  
[[Paper](http://arxiv.org/pdf/2510.04454v1)] [[Code/Page]()] [[TLDR/Notes](#mitigating-forgetting-between-supervised-and-reinforcement-learning-yields-stronger-reasoners)]

- [25/10] **A KL-regularization framework for learning to plan with adaptive priors**  
[[Paper](http://arxiv.org/pdf/2510.04280v1)] [[Code/Page]()] [[TLDR/Notes](#a-kl-regularization-framework-for-learning-to-plan-with-adaptive-priors)]

- [25/10] **Global Convergence of Policy Gradient for Entropy Regularized Linear-Quadratic Control with multiplicative noise**  
[[Paper](http://arxiv.org/pdf/2510.02896v1)] [[Code/Page]()] [[TLDR/Notes](#global-convergence-of-policy-gradient-for-entropy-regularized-linear-quadratic-control-with-multiplicative-noise)]

- [25/10] **ExGRPO: Learning to Reason from Experience**  
[[Paper](http://arxiv.org/pdf/2510.02245v1)] [[Code/Page]()] [[TLDR/Notes](#exgrpo--learning-to-reason-from-experience)]

- [25/10] **Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning**  
[[Paper](http://arxiv.org/pdf/2510.01656v3)] [[Code/Page]()] [[TLDR/Notes](#asymmetric-proximal-policy-optimization--mini-critics-boost-llm-reasoning)]

- [25/10] **VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2510.01444v1)] [[Code/Page]()] [[TLDR/Notes](#vogue--guiding-exploration-with-visual-uncertainty-improves-multimodal-reasoning)]

- [25/09] **Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2509.26114v1)] [[Code/Page]()] [[TLDR/Notes](#clip-low-increases-entropy-and-clip-high-decreases-entropy-in-reinforcement-learning-of-large-language-models)]

- [25/09] **Machine Learning Algorithms for Improving Black Box Optimization Solvers**  
[[Paper](http://arxiv.org/pdf/2509.25592v1)] [[Code/Page]()] [[TLDR/Notes](#machine-learning-algorithms-for-improving-black-box-optimization-solvers)]

- [25/09] **Rethinking Entropy Regularization in Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2509.25133v1)] [[Code/Page]()] [[TLDR/Notes](#rethinking-entropy-regularization-in-large-reasoning-models)]

- [25/09] **AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation**  
[[Paper](http://arxiv.org/pdf/2509.24387v1)] [[Code/Page](https://github.com/xinding-sys/AdaNav.)] [[TLDR/Notes](#adanav--adaptive-reasoning-with-uncertainty-for-vision-language-navigation)]

- [25/09] **Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation**  
[[Paper](http://arxiv.org/pdf/2509.23866v1)] [[Code/Page]()] [[TLDR/Notes](#efficient-multi-turn-rl-for-gui-agents-via-decoupled-training-and-adaptive-data-curation)]

- [25/09] **C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning**  
[[Paper](http://arxiv.org/pdf/2509.23129v1)] [[Code/Page](https://github.com/HaotianLiu123/CCGSPG.)] [[TLDR/Notes](#c$^2$gspg--confidence-calibrated-group-sequence-policy-gradient-towards-self-aware-reasoning)]

- [25/09] **Quantile Advantage Estimation for Entropy-Safe Reasoning**  
[[Paper](http://arxiv.org/pdf/2509.22611v1)] [[Code/Page]()] [[TLDR/Notes](#quantile-advantage-estimation-for-entropy-safe-reasoning)]

- [25/09] **Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2509.22601v2)] [[Code/Page]()] [[TLDR/Notes](#learn-the-ropes--then-trust-the-wins--self-imitation-with-progressive-exploration-for-agentic-reinforcement-learning)]

- [25/09] **EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2509.22576v1)] [[Code/Page]()] [[TLDR/Notes](#epo--entropy-regularized-policy-optimization-for-llm-agents-reinforcement-learning)]

- [25/09] **Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2509.22115v1)] [[Code/Page]()] [[TLDR/Notes](#learning-more-with-less--a-dynamic-dual-level-down-sampling-framework-for-efficient-policy-optimization)]

- [25/09] **Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2509.21942v1)] [[Code/Page]()] [[TLDR/Notes](#structural-information-based-hierarchical-diffusion-for-offline-reinforcement-learning)]

- [25/09] **ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models**  
[[Paper](http://arxiv.org/pdf/2509.21826v1)] [[Code/Page]()] [[TLDR/Notes](#rest--reshaping-token-level-policy-gradients-for-tool-use-large-language-models)]

- [25/09] **Learning to Reason with Mixture of Tokens**  
[[Paper](http://arxiv.org/pdf/2509.21482v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-with-mixture-of-tokens)]

- [25/09] **Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns**  
[[Paper](http://arxiv.org/pdf/2509.21124v2)] [[Code/Page]()] [[TLDR/Notes](#expanding-reasoning-potential-in-foundation-model-by-learning-diverse-chains-of-thought-patterns)]

- [25/09] **Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs**  
[[Paper](http://arxiv.org/pdf/2509.21044v1)] [[Code/Page](https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.)] [[TLDR/Notes](#reinforcement-learning-fine-tuning-enhances-activation-intensity-and-diversity-in-the-internal-circuitry-of-llms)]

- [25/09] **ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2509.21010v1)] [[Code/Page]()] [[TLDR/Notes](#exmolrl--phenotype-target-joint-generation-of-de-novo-molecules-via-multi-objective-reinforcement-learning)]

- [25/09] **CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2509.20712v4)] [[Code/Page]()] [[TLDR/Notes](#ce-gppo--coordinating-entropy-via-gradient-preserving-clipping-policy-optimization-in-reinforcement-learning)]

- [25/09] **Failure Modes of Maximum Entropy RLHF**  
[[Paper](http://arxiv.org/pdf/2509.20265v1)] [[Code/Page]()] [[TLDR/Notes](#failure-modes-of-maximum-entropy-rlhf)]

- [25/09] **Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers**  
[[Paper](http://arxiv.org/pdf/2509.15498v1)] [[Code/Page]()] [[TLDR/Notes](#mental-accounts-for-actions--ewa-inspired-attention-in-decision-transformers)]

- [25/09] **Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation**  
[[Paper](http://arxiv.org/pdf/2509.15194v2)] [[Code/Page](https://github.com/YujunZhou/EVOL-RL.)] [[TLDR/Notes](#evolving-language-models-without-labels--majority-drives-selection--novelty-promotes-variation)]

- [25/09] **GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2509.12108v2)] [[Code/Page]()] [[TLDR/Notes](#gta--supervised-guided-reinforcement-learning-for-text-classification-with-large-language-models)]

- [25/09] **Mutual Information Tracks Policy Coherence in Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2509.10423v1)] [[Code/Page]()] [[TLDR/Notes](#mutual-information-tracks-policy-coherence-in-reinforcement-learning)]

- [25/09] **Inpainting-Guided Policy Optimization for Diffusion Large Language Models**  
[[Paper](http://arxiv.org/pdf/2509.10396v1)] [[Code/Page]()] [[TLDR/Notes](#inpainting-guided-policy-optimization-for-diffusion-large-language-models)]

- [25/09] **CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2509.09675v1)] [[Code/Page]()] [[TLDR/Notes](#cde--curiosity-driven-exploration-for-efficient-reinforcement-learning-in-large-language-models)]



# TLDR/Notes
## bapo--stabilizing-off-policy-reinforcement-learning-for-llms-via-balanced-policy-optimization-with-adaptive-clipping
### Abstract
Reinforcement learning (RL) has recently become the core paradigm for
aligning and strengthening large language models (LLMs). Yet, applying RL in
off-policy settings--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: policy entropy
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
policy gradient, suppressing useful behaviors and risking gradient explosions;
and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping
mechanism in PPO-like objectives systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including sample replay and partial rollout--BAPO
achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like o3-mini and Gemini-2.5-Flash-Thinking.
### ğŸŒŸ è®ºæ–‡è§£è¯» | BAPOï¼šå¤§è¯­è¨€æ¨¡å‹ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šå™¨ï¼Œè‡ªé€‚åº”è£å‰ªå®ç°å¹³è¡¡ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½ä¸å¢å¼ºçš„æ ¸å¿ƒèŒƒå¼ï¼Œå…¶ä¸­ç¦»ç­–ç•¥RLå› èƒ½åˆ©ç”¨è¿‡å¾€ç­–ç•¥çš„â€œ stale dataï¼ˆè¿‡æ—¶æ•°æ®ï¼‰â€æå‡æ ·æœ¬æ•ˆç‡ï¼Œåœ¨é•¿å‘¨æœŸã€é«˜æŒ‘æˆ˜åœºæ™¯åŠç°ä»£AIåŸºç¡€è®¾æ–½é€‚é…æ€§ä¸Šä¼˜åŠ¿æ˜¾è‘—ã€‚ä½†ç¦»ç­–ç•¥RLåº”ç”¨äºLLMsæ—¶é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼šç­–ç•¥ç†µæ€¥å‰§ä¸‹é™ï¼Œä¼˜åŒ–ä¸ç¨³å®šç”šè‡³â€œå´©æºƒâ€ã€‚æ­¤å‰ç ”ç©¶è¡¨æ˜ï¼Œç¦»ç­–ç•¥è®­ç»ƒä¸­æ•°æ®è¿‡æ—¶ç¨‹åº¦è¶Šé«˜ï¼Œè®­ç»ƒå¥–åŠ±æ³¢åŠ¨ã€ç†µè¡°å‡ã€æ¢¯åº¦çˆ†ç‚¸ç­‰é—®é¢˜è¶Šçªå‡ºï¼Œè€Œon - policyè®­ç»ƒåˆ™ç›¸å¯¹ç¨³å®šã€‚ä¸ºè§£å†³ç¦»ç­–ç•¥RLçš„ä¸ç¨³å®šæ€§ï¼Œè®ºæ–‡ä»ç†è®ºä¸å®è¯åˆ†æå…¥æ‰‹æ¢å¯»æ ¹æºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºç¦»ç­–ç•¥RLä¸ç¨³å®šçš„ä¸¤å¤§å…³é”®è¯±å›  
é€šè¿‡åˆ†æå‘ç°ï¼Œå…¶ä¸€å­˜åœ¨**ä¼˜åŒ–ä¸å¹³è¡¡**é—®é¢˜ï¼šè´Ÿä¼˜åŠ¿æ ·æœ¬åœ¨ç­–ç•¥æ¢¯åº¦ä¸­å ä¸»å¯¼ï¼Œä¼šæŠ‘åˆ¶æœ‰æ•ˆè¡Œä¸ºä¸”æœ‰æ¢¯åº¦çˆ†ç‚¸é£é™©ï¼›å…¶äºŒæ¨å¯¼å¹¶éªŒè¯äº†**ç†µ - è£å‰ªè§„åˆ™ï¼ˆEntropy - Clip Ruleï¼‰**ï¼šç±»ä¼¼PPOçš„å›ºå®šè£å‰ªæœºåˆ¶ä¼šç³»ç»Ÿæ€§é˜»ç¢ç†µå¢æ›´æ–°ï¼Œè®©ç­–ç•¥åå‘â€œè¿‡åº¦åˆ©ç”¨ï¼ˆexploitationï¼‰â€è€Œç‰ºç‰²â€œæ¢ç´¢ï¼ˆexplorationï¼‰â€ï¼Œæ¯”å¦‚PPOç±»æ–¹æ³•çš„è£å‰ªä¼šåœ¨å¼ºåŒ–é«˜æ¦‚ç‡æ­£tokenæ—¶è¿‡åº¦æƒ©ç½šä½æ¦‚ç‡è´Ÿtokenï¼Œå¯¼è‡´åˆ†å¸ƒå°–é”åŒ–ã€ç†µåå¡Œã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºBAPOæ–¹æ³• 
BAPOå…¨ç§°Balanced Policy Optimization with Adaptive Clippingï¼Œæ—¨åœ¨åŠ¨æ€è°ƒæ•´è£å‰ªè¾¹ç•Œæ¥é‡æ–°å¹³è¡¡æ­£è´Ÿè´¡çŒ®ã€ä¿ç•™ç†µå¹¶ç¨³å®šRLä¼˜åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒä¾æ®æ­£tokençš„æŸå¤±è´¡çŒ®åŠ¨æ€è°ƒæ•´`c_low`å’Œ`c_high`è¿™ä¸¤ä¸ªè£å‰ªè¾¹ç•Œï¼Œè¿‡æ»¤è¿‡åº¦è´Ÿå‘çš„tokenä»¥ç»´æŒåˆ†å¸ƒå¹³æ»‘ï¼ŒåŒæ—¶çº³å…¥æ­¤å‰è¢«è£å‰ªçš„æ­£å‘tokenæ¥ä¿æŒç†µå¹³è¡¡ï¼Œåœ¨æ¢ç´¢ä¸åˆ©ç”¨é—´æ‰¾åˆ°æ›´å¥½çš„å¹³è¡¡ç‚¹ï¼Œè®©ç¦»ç­–ç•¥è®­ç»ƒæ›´ç¨³å®šé«˜æ•ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šæ ·çš„ç¦»ç­–ç•¥åœºæ™¯ï¼ˆæ ·æœ¬é‡æ”¾ã€éƒ¨åˆ†rolloutç­‰ï¼‰ä¸‹ï¼ŒBAPOå±•ç°å‡ºå¿«é€Ÿã€ç¨³å®šä¸”æ•°æ®é«˜æ•ˆçš„è®­ç»ƒç‰¹æ€§ã€‚åœ¨AIME 2024å’ŒAIME 2025åŸºå‡†æµ‹è¯•ä¸­ï¼š7Bè§„æ¨¡çš„BAPOæ¨¡å‹è¶…è¿‡SkyWork - OR1 - 7Bç­‰å¼€æºç«å“ï¼›32Bè§„æ¨¡çš„BAPOæ¨¡å‹ä¸ä»…åœ¨åŒè§„æ¨¡æ¨¡å‹ä¸­è¾¾åˆ°SOTAï¼Œè¿˜è¶…è¶Šäº†o3 - miniã€Gemini - 2.5 - Flash - Thinkingç­‰å¤´éƒ¨é—­æºç³»ç»Ÿã€‚ä¾‹å¦‚åœ¨AIME24ä¸Š7B BAPOæ¨¡å‹å¾—70.8åˆ†ï¼Œ32B BAPOæ¨¡å‹å¾—87.1åˆ†ï¼›AIME25ä¸Š7B BAPOæ¨¡å‹å¾—62.5åˆ†ï¼Œ32B BAPOæ¨¡å‹å¾—80.0åˆ† ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜è¯Šæ–­å±‚é¢ï¼šè®ºæ–‡é€šè¿‡ç†è®º + å®è¯ç»“åˆçš„æ–¹å¼ï¼Œæ·±å…¥å‰–æç¦»ç­–ç•¥RLåœ¨LLMsåœºæ™¯ä¸‹ä¸ç¨³å®šçš„æ ¹å› ï¼ˆä¼˜åŒ–ä¸å¹³è¡¡ä¸ç†µ - è£å‰ªè§„åˆ™é™åˆ¶ï¼‰ï¼Œè¿™ç§â€œå…ˆæ‰¾å‡†é—®é¢˜æœ¬è´¨å†è®¾è®¡æ–¹æ¡ˆâ€çš„ç ”ç©¶æ€è·¯ï¼Œä¸ºåç»­RLåœ¨LLMsæˆ–å…¶ä»–é¢†åŸŸçš„ä¼˜åŒ–å·¥ä½œæä¾›äº†ä¼˜ç§€çš„é—®é¢˜åˆ†æèŒƒå¼ã€‚
2. æ–¹æ³•è®¾è®¡å±‚é¢ï¼šBAPOèšç„¦â€œåŠ¨æ€è‡ªé€‚åº”è£å‰ªå¹³è¡¡æ­£è´Ÿè´¡çŒ®ã€ä¿ç•™ç†µâ€è¿™ä¸€ç®€æ´å´æœ‰æ•ˆçš„æ€è·¯æ¥è§£å†³å¤æ‚çš„ç¦»ç­–ç•¥è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œè¯æ˜äº†åœ¨ç»å…¸æ–¹æ³•ï¼ˆå¦‚PPOï¼‰åŸºç¡€ä¸Šåšé’ˆå¯¹æ€§æœºåˆ¶æ”¹è¿›ï¼ˆçªç ´å›ºå®šè£å‰ªæ€ç»´ï¼‰èƒ½å¸¦æ¥æ˜¾è‘—æ”¶ç›Šï¼Œä¸ºç®—æ³•æ”¹è¿›æä¾›äº†â€œç²¾å‡†å®šä½ç—›ç‚¹ + è½»é‡æœ‰æ•ˆä¿®æ”¹â€çš„å‚è€ƒæ–¹å‘ã€‚
3. å®éªŒéªŒè¯å±‚é¢ï¼šåœ¨å¤šæ¨¡å‹ backboneã€å¤šè§„æ¨¡ã€å¤šç¦»ç­–ç•¥è®¾ç½®ä¸‹éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿˜ä¸å¼€æºã€é—­æºå¼ºåŸºçº¿å¯¹æ¯”å‡¸æ˜¾ä¼˜åŠ¿ï¼Œè¿™ç§å…¨é¢çš„å®éªŒéªŒè¯æ¨¡å¼èƒ½æœ‰åŠ›æ”¯æ’‘æ–¹æ³•çš„æ™®é€‚æ€§ä¸ä¼˜è¶Šæ€§ç»“è®ºï¼Œå€¼å¾—åç»­ç ”ç©¶å€Ÿé‰´ä»¥å¢å¼ºæˆæœè¯´æœåŠ›ã€‚

## claws-creativity-detection-for-llm-generated-solutions-using-attention-window-of-sections
### Abstract
Recent advances in enhancing the reasoning ability of large language models
(LLMs) have been remarkably successful. LLMs trained with reinforcement
learning (RL) for reasoning demonstrate strong performance in challenging tasks
such as mathematics and coding, even with relatively small model sizes.
However, despite these improvements in task accuracy, the assessment of
creativity in LLM generations has been largely overlooked in reasoning tasks,
in contrast to writing tasks. The lack of research on creativity assessment in
reasoning primarily stems from two challenges: (1) the difficulty of defining
the range of creativity, and (2) the necessity of human evaluation in the
assessment process. To address these challenges, we propose CLAWS, a method
that defines and classifies mathematical solutions into typical, creative, and
hallucinated categories without human evaluation, by leveraging attention
weights across prompt sections and output. CLAWS outperforms five existing
white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden
Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,
Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems
collected from 181 math contests (AJHSME, AMC, AIME).
### ğŸŒŸ è®ºæ–‡è§£è¯» | CLAWSï¼šåŸºäºåˆ†æ®µæ³¨æ„åŠ›çª—å£æ£€æµ‹å¤§æ¨¡å‹æ¨ç†çš„åˆ›é€ æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›æå‡ä¸Šæˆæœæ˜¾è‘—ï¼Œå°¤å…¶åœ¨æ•°å­¦ã€ç¼–ç ç­‰ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚ä½†åœ¨æ¨ç†ä»»åŠ¡é‡Œï¼Œå¯¹å¤§æ¨¡å‹ç”Ÿæˆå†…å®¹â€œåˆ›é€ æ€§â€çš„è¯„ä¼°å´è¿œä¸å¦‚å†™ä½œä»»åŠ¡å—é‡è§†ã€‚ç©¶å…¶åŸå› ï¼Œä¸€æ˜¯â€œåˆ›é€ æ€§â€çš„ç•Œå®šèŒƒå›´éš¾ç¡®å®šï¼ŒäºŒæ˜¯è¯„ä¼°è¿‡ç¨‹å¾€å¾€ä¾èµ–äººå·¥ï¼Œæˆæœ¬é«˜ä¸”éš¾æ ‡å‡†åŒ–ã€‚è€Œäººç±»æ™ºèƒ½ä¸­åˆ›é€ æ€§æ˜¯é‡è¦ç»´åº¦ï¼Œæ•°å­¦æ¨ç†é‡Œè¯„ä¼°åˆ›é€ æ€§åˆéœ€ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ï¼Œè¿™è®©å¤§è§„æ¨¡è¯„ä¼°æ›´å…·æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œç°æœ‰å¹»è§‰æ£€æµ‹ç ”ç©¶è‹¥è¿‡åº¦é™åˆ¶ç”Ÿæˆä¼šæŠ‘åˆ¶åˆ›é€ æ€§ï¼Œæ‰€ä»¥è¯†åˆ«åˆ›é€ æ€§å›åº”å¯¹æå‡å¤§æ¨¡å‹ç”Ÿæˆå¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§å¾ˆå…³é”®ï¼Œåœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè®ºæ–‡æå‡ºCLAWSæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCLAWSæ–¹æ³•å®ç°æ— äººå·¥è¯„ä¼°çš„åˆ†ç±»  
CLAWSå€ŸåŠ©å¯¹æç¤ºè¯å„éƒ¨åˆ†ï¼ˆGuidelineã€Problemã€Reference Solutionsã€Instructionï¼‰å’Œè¾“å‡ºä¹‹é—´æ³¨æ„åŠ›æƒé‡çš„åˆ†æï¼ŒæŠŠæ•°å­¦è§£å†³æ–¹æ¡ˆå®šä¹‰å¹¶åˆ†ç±»ä¸ºå…¸å‹ï¼ˆTypicalï¼‰ã€åˆ›é€ æ€§ï¼ˆCreativeï¼‰ã€å¹»è§‰ï¼ˆHallucinatedï¼‰ä¸‰ç±»ï¼Œæ— éœ€äººå·¥å‚ä¸è¯„ä¼°ï¼Œçªç ´äº†ä¼ ç»Ÿä¾èµ–äººå·¥è¯„ä¼°åˆ›é€ æ€§çš„å±€é™ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå®éªŒæ¡†æ¶æ”¯æ’‘åˆ†ç±»æ£€æµ‹  
æ­å»ºçš„å®éªŒæ¡†æ¶æ¶µç›–ç”Ÿæˆã€ç‰¹å¾æå–ã€æ ‡æ³¨ã€è¯„ä¼°ç­‰ç¯èŠ‚ã€‚ç”Ÿæˆç¯èŠ‚ç”±å¤§æ¨¡å‹äº§å‡ºè§£å†³æ–¹æ¡ˆï¼›ç‰¹å¾æå–ä»æ¨¡å‹å†…éƒ¨è¡¨å¾è·å–ä¿¡æ¯ï¼›LLM Evaluatorè´Ÿè´£ç»™ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆæ‰“æ ‡ç­¾ï¼ˆåŒºåˆ†å¹»è§‰ã€åˆ›é€ æ€§ã€å…¸å‹ï¼‰ï¼›æœ€ååˆ©ç”¨å‚è€ƒé›†è®©æ£€æµ‹æ–¹æ³•åŸºäºæå–ç‰¹å¾åšåˆ†ç±»ï¼Œä¸ºåˆ›é€ æ€§å’Œå¹»è§‰æ£€æµ‹æä¾›äº†å®Œæ•´æµç¨‹æ”¯æ’‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºå…¨é¢è¯„ä¼°åè®®  
è®¾è®¡åŒ…å«äº”ç§è¯„ä¼°ç­–ç•¥å’Œå››ä¸ªæŒ‡æ ‡çš„è¯„ä¼°åè®®ï¼Œç”¨äºè¯„ä¼°æ£€æµ‹æ–¹æ³•æå–çš„ç‰¹å¾ï¼Œèƒ½æ›´å…¨é¢ã€ç²¾å‡†åœ°è¡¡é‡æ£€æµ‹æ–¹æ³•æ€§èƒ½ï¼Œä¸ºè¯¥é¢†åŸŸè¯„ä¼°æä¾›äº†æ›´å®Œå–„çš„æ ‡å‡†å’Œæ‰‹æ®µã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ª7 - 8Bè§„æ¨¡ã€ç»å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼ˆDeepSeekã€Qwenã€Mathstralã€OpenMath2ã€Orealï¼‰ä¸Šï¼ŒCLAWSåœ¨å¹»è§‰æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½è¶…è¿‡äº†Perplexityã€Logit Entropyã€Window Entropyã€Hidden Scoreã€Attention Scoreè¿™äº”ç§ç°æœ‰ç™½ç›’æ£€æµ‹æ–¹æ³•ã€‚å¹¶ä¸”åŸºäºä»181åœºæ•°å­¦ç«èµ›ï¼ˆå¦‚AJHSMEã€AMCã€AIMEï¼‰æ”¶é›†çš„4545é“æ•°å­¦é¢˜ç»„æˆçš„æ•°æ®é›†éªŒè¯ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šå°†æ³¨æ„åŠ›åˆ†æä¸promptåˆ†æ®µç»“åˆæ¥æ£€æµ‹åˆ›é€ æ€§å’Œå¹»è§‰ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†è´¨é‡è¯„ä¼°å¼€è¾Ÿäº†æ–°è§†è§’ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§ç»“åˆæ¨¡å‹å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶åˆ†æçš„æ€è·¯æ‹“å±•è¯„ä¼°ç»´åº¦ã€‚  
2. æ¡†æ¶æ„å»ºï¼šæ­å»ºçš„ä»ç”Ÿæˆåˆ°è¯„ä¼°çš„å®éªŒæ¡†æ¶ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†ä»»åŠ¡ä¸­åˆ›é€ æ€§ç­‰å¤æ‚ç»´åº¦çš„ç ”ç©¶æä¾›äº†æµç¨‹å‚è€ƒï¼Œæ–¹ä¾¿åç»­ç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šä¼˜åŒ–æˆ–æ‹“å±•ç ”ç©¶ã€‚  
3. è¯„ä¼°åè®®ï¼šæå‡ºçš„å¤šç­–ç•¥å¤šæŒ‡æ ‡è¯„ä¼°åè®®ï¼Œè®©æ£€æµ‹æ–¹æ³•æ€§èƒ½è¯„ä¼°æ›´å…¨é¢ç§‘å­¦ï¼Œå…¶ä»–é¢†åŸŸåœ¨è¯„ä¼°æ¨¡å‹èƒ½åŠ›æˆ–æ£€æµ‹æ–¹æ³•æ—¶ï¼Œå¯å‚è€ƒè¿™ç§ä¸°å¯Œè¯„ä¼°ç»´åº¦çš„æ–¹å¼æ¥è®¾è®¡è¯„ä¼°ä½“ç³»ã€‚

## d2c-hrhr--discrete-actions-with-double-distributional-critics-for-high-risk-high-return-tasks
### Abstract
Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜é£é™©é«˜å›æŠ¥ä»»åŠ¡çš„æ–°è§£æ³•ï¼šD2C - HRHRæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢†åŸŸï¼Œåƒéšœç¢è·¨è¶Šè¿™ç±»æ¶‰åŠé«˜é£é™©é«˜å›æŠ¥ï¼ˆHRHRï¼‰çš„ä»»åŠ¡ï¼Œå¾€å¾€å­˜åœ¨å¤šå³°çš„åŠ¨ä½œåˆ†å¸ƒä¸éšæœºå›æŠ¥ç‰¹æ€§ã€‚ç„¶è€Œï¼Œå¤šæ•°RLæ–¹æ³•é‡‡ç”¨å•å³°é«˜æ–¯ç­–ç•¥å¹¶ä¾èµ–æ ‡é‡å€¼çš„è¯„è®ºå®¶ï¼ˆcriticï¼‰ï¼Œè¿™åœ¨HRHRåœºæ™¯ä¸‹æ•ˆæœå—é™ã€‚å› ä¸ºé«˜æ–¯ç­–ç•¥éš¾ä»¥ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜è§£ï¼Œä¸”ä¼šåå‘æ›´å®‰å…¨åŠ¨ä½œè€Œå¿½ç•¥é«˜å›æŠ¥åŒºåŸŸã€‚åŒæ—¶ï¼Œç°å®ä¸­å¾ˆå¤šRLä»»åŠ¡ï¼ˆå¦‚è·‘é…·ç§»åŠ¨ã€å¯Œæ¥è§¦æœºå™¨äººæ“ä½œï¼‰å¤„äºHRHRåœºæ™¯ï¼Œæ ‡å‡†æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰é«˜å›æŠ¥åŠ¨ä½œï¼Œæ‰€ä»¥éœ€è¦æ–°æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç¦»æ•£åŒ–è¿ç»­åŠ¨ä½œç©ºé—´ä»¥è¿‘ä¼¼å¤šå³°åˆ†å¸ƒ  
é’ˆå¯¹HRHRä»»åŠ¡ä¸­åŠ¨ä½œçš„å¤šå³°åˆ†å¸ƒç‰¹ç‚¹ï¼Œå°†è¿ç»­åŠ¨ä½œç©ºé—´è¿›è¡Œç¦»æ•£åŒ–å¤„ç†ã€‚è¿™æ ·èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰é‚£äº›åˆ†å¸ƒåœ¨ä¸åŒâ€œå³°â€å¤„çš„åŠ¨ä½œï¼Œé¿å…é«˜æ–¯ç­–ç•¥åœ¨å¤„ç†å¤šå³°æƒ…å†µæ—¶çš„ä¸è¶³ï¼Œä»è€Œè¿‘ä¼¼é«˜é£é™©é«˜å›æŠ¥åŒºåŸŸçš„å¤šå³°åŠ¨ä½œåˆ†å¸ƒï¼Œä¸ºåç»­å­¦ä¹ é«˜å›æŠ¥åŠ¨ä½œæ‰“ä¸‹åŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡‡ç”¨ç†µæ­£åˆ™åŒ–æ¢ç´¢æå‡ risky ä½† rewarding åŠ¨ä½œçš„è¦†ç›–åº¦  
å¼•å…¥ç†µæ­£åˆ™åŒ–çš„æ¢ç´¢æœºåˆ¶ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å»æ¢ç´¢é‚£äº›é£é™©é«˜ä½†æ½œåœ¨å›æŠ¥ä¹Ÿé«˜çš„åŠ¨ä½œã€‚åœ¨HRHRåœºæ™¯ä¸­ï¼Œé«˜å›æŠ¥å¾€å¾€éšè—åœ¨é«˜é£é™©åŒºåŸŸï¼Œé€šè¿‡è¿™ç§æ¢ç´¢æ–¹å¼ï¼Œèƒ½è®©æ™ºèƒ½ä½“æ›´å……åˆ†åœ°è¦†ç›–è¿™äº›å…³é”®åŠ¨ä½œåŒºåŸŸï¼Œé¿å…åªèšç„¦äºå®‰å…¨ä½†ä½å›æŠ¥çš„åŠ¨ä½œï¼Œè¿›è€Œæå‡åœ¨é«˜é£é™©é«˜å›æŠ¥ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥åŒè¯„è®ºå®¶æ¶æ„å®ç°æ›´ç²¾å‡†çš„ç¦»æ•£å€¼åˆ†å¸ƒä¼°è®¡  
è®¾è®¡äº†åŒè¯„è®ºå®¶ï¼ˆdual - criticï¼‰æ¶æ„ï¼Œç”¨äºå¯¹ç¦»æ•£åŠ¨ä½œçš„ä»·å€¼åˆ†å¸ƒè¿›è¡Œæ›´å‡†ç¡®çš„ä¼°è®¡ã€‚ä¸åŒäºä¼ ç»Ÿçš„æ ‡é‡è¯„è®ºå®¶ï¼ŒåŒè¯„è®ºå®¶æ¶æ„èƒ½å¤Ÿä»ä¸åŒè§’åº¦å¯¹åŠ¨ä½œä»·å€¼è¿›è¡Œè¯„ä¼°ï¼Œåœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´ä¸‹ï¼Œæ›´ç²¾å‡†åœ°æ•æ‰åŠ¨ä½œå¯¹åº”çš„ä»·å€¼åˆ†å¸ƒæƒ…å†µï¼Œä¸ºæ™ºèƒ½ä½“é€‰æ‹©é«˜å›æŠ¥åŠ¨ä½œæä¾›æ›´å¯é çš„ä»·å€¼åˆ¤æ–­ä¾æ®ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰é«˜å¤±è´¥é£é™©çš„ locomotionï¼ˆ locomotion ä»»åŠ¡ï¼Œå¦‚é«˜æ‘”å€’é£é™©çš„ç§»åŠ¨ä»»åŠ¡ï¼‰å’Œ manipulationï¼ˆæ“ä½œä»»åŠ¡ï¼Œå¦‚é«˜å¤±è´¥é£é™©çš„æ“ä½œä»»åŠ¡ï¼‰åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œè¿™ä¹Ÿå‡¸æ˜¾äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ˜¾å¼åœ°å¯¹å¤šå³°æ€§å’Œé£é™©è¿›è¡Œå»ºæ¨¡çš„é‡è¦æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œé’ˆå¯¹ç‰¹æ®Šåœºæ™¯ï¼ˆHRHRï¼‰å¯¹åŠ¨ä½œç©ºé—´å¤„ç†ã€æ¢ç´¢æœºåˆ¶ã€ä»·å€¼ä¼°è®¡æ¶æ„è¿›è¡Œåˆ›æ–°çš„æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œå½“é¢å¯¹å…·æœ‰ç‰¹æ®Šåˆ†å¸ƒæˆ–å›æŠ¥ç‰¹æ€§çš„ä»»åŠ¡æ—¶ï¼Œå¯ä»¥æ€è€ƒå¯¹åŠ¨ä½œç©ºé—´çš„è½¬æ¢ã€æ¢ç´¢ç­–ç•¥çš„è°ƒæ•´ä»¥åŠä»·å€¼ä¼°è®¡æ–¹å¼çš„ä¼˜åŒ–ã€‚ä»é—®é¢˜å®šä¹‰è§’åº¦ï¼Œæœ¬æ–‡å…ˆå¯¹HRHRä»»åŠ¡è¿›è¡Œå½¢å¼åŒ–å®šä¹‰å¹¶ç†è®ºåˆ†æç°æœ‰æ–¹æ³•ä¸è¶³ï¼Œè¿™ç§å…ˆæ˜ç¡®é—®é¢˜æœ¬è´¨å†æå‡ºè§£æ³•çš„ç ”ç©¶æ€è·¯ï¼Œèƒ½å¸®åŠ©ç ”ç©¶è€…åœ¨é¢å¯¹æ–°åœºæ™¯æ–°é—®é¢˜æ—¶ï¼Œæ›´ç³»ç»Ÿåœ°å¼€å±•ç ”ç©¶ã€‚ä»å®éªŒéªŒè¯è§’åº¦ï¼Œé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„é«˜é£é™©åŸºå‡†ä»»åŠ¡æ¥éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œä¸ºæ–¹æ³•çš„å®ç”¨æ€§æä¾›äº†æœ‰åŠ›æ”¯æ’‘ï¼Œåœ¨åç»­ç ”ç©¶ä¸­é€‰æ‹©åˆé€‚çš„åŸºå‡†ä»»åŠ¡éªŒè¯æ–°æ–¹æ³•ä¹Ÿè‡³å…³é‡è¦ã€‚

## video-reasoning-without-training
### Abstract
Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— éœ€è®­ç»ƒä¹Ÿèƒ½åšè§†é¢‘æ¨ç†ï¼ŸV-Reasonå¸¦æ¥æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åˆ©ç”¨å¤§ multimodal æ¨¡å‹ï¼ˆLMMsï¼‰è¿›è¡Œè§†é¢‘æ¨ç†æ—¶ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–æˆæœ¬é«˜æ˜‚çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå†—é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œè®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½å­˜åœ¨å¤§é‡è®¡ç®—å¼€é”€ã€‚è€Œä¸”è¿™äº›æ¨ç†æ¨¡å‹ä¸­æ§åˆ¶æ€è€ƒè¿‡ç¨‹çš„æœºåˆ¶å¾ˆæœ‰é™ã€‚åŒæ—¶ï¼Œå¯¹äºè§†é¢‘æ¨ç†ä»»åŠ¡ï¼Œé«˜åˆ†è¾¨ç‡å’Œå¤šå¸§ç‰¹æ€§è®©è®¡ç®—æˆæœ¬é—®é¢˜æ›´çªå‡ºï¼Œä¸”äººä»¬å¯¹æ§åˆ¶æ¨ç†è¿‡ç¨‹æ·±åº¦ä¸è´¨é‡çš„å› ç´ ç†è§£ä¸è¶³ã€‚äºæ˜¯æœ¬æ–‡å›´ç»•â€œæ¨ç†æ—¶çš„æŒ‡æ ‡èƒ½å¦åˆ»ç”»è§†é¢‘æ¨ç†æ¨¡å‹æ€è€ƒè¿‡ç¨‹ã€èƒ½å¦ç”¨è¿™äº›æŒ‡æ ‡åˆ¶å®šæ¨ç†æ—¶ä¼˜åŒ–ç›®æ ‡æ¥å¢å¼ºè§†é¢‘æ¨ç†ä¸”æ— éœ€é¢å¤–è®­ç»ƒâ€ç­‰é—®é¢˜å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç†µåˆ†ææ¨ç†è¿‡ç¨‹è§„å¾‹  
é€šè¿‡åˆ†ææ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„ç†µï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹ç”Ÿæˆ token æ—¶ç†µå…ˆå¢åå‡ï¼Œå¯¹åº”â€œå®è§‚æ¢ç´¢ï¼ˆmacro - explorationï¼Œæ¨¡å‹å¼€å§‹ç”Ÿæˆå“åº”æ—¶ä¸ç¡®å®šï¼Œæœç´¢å¤šç§è§£è½¨è¿¹ï¼Œç†µä¸Šå‡ï¼‰â€å’Œâ€œå®è§‚åˆ©ç”¨ï¼ˆmacro - exploitationï¼Œæ¨¡å‹é€æ¸ç¡®å®šè§£ï¼Œç†µä¸‹é™ï¼‰â€é˜¶æ®µï¼›è¿˜å‘ç°æ›´å¤§æ›´å‡†ç¡®çš„æ¨¡å‹ç†µæœ€å¤§å€¼æ›´ä½ä¸”å‡ºç°æ›´æ™šï¼Œåç»­æ”¶æ•›åˆ°æ›´ä½æœ€ç»ˆç†µï¼Œä¸”æ¨¡å‹åœ¨æ€è€ƒè¿‡ç¨‹çš„å®è§‚é˜¶æ®µä¸­å­˜åœ¨å¾®æ¢ç´¢ï¼ˆmicro - explorationï¼‰å’Œå¾®åˆ©ç”¨ï¼ˆmicro - exploitationï¼‰å¾ªç¯ï¼ˆç†µå°å¹…åº¦å¢å‡ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºV - Reasonæ¨ç†æ—¶ä¼˜åŒ–æ–¹æ³•  
æå‡ºV - Reasonæ–¹æ³•ï¼Œåœ¨æ¨ç†æ—¶å¼•å…¥å°å‹å¯è®­ç»ƒæ§åˆ¶å™¨åˆ°LMMçš„value cacheï¼Œä»…é€šè¿‡åŸºäºç†µçš„ç›®æ ‡è¿›è¡Œå°‘é‡ä¼˜åŒ–æ­¥éª¤æ¥è°ƒæ•´æ¨¡å‹è¡Œä¸ºï¼Œæ— éœ€æ•°æ®ç›‘ç£æˆ–RLã€‚è¯¥ç›®æ ‡é¼“åŠ±æ›´æ˜æ˜¾çš„å¾®æ¢ç´¢å’Œå¾®åˆ©ç”¨å¾ªç¯ï¼Œå…ˆè®©æ¨¡å‹åœ¨å¾ªç¯ä¸­æ›´å¼ºåœ°å¢å‡ç†µï¼Œæ¥ç€è¿›å…¥æœ€ç»ˆç†µæœ€å°åŒ–é˜¶æ®µã€‚è¿™æ ·èƒ½é˜²æ­¢å®è§‚æ¢ç´¢é˜¶æ®µç†µä¸Šå‡è¿‡å¿«ï¼Œè®©æ¨¡å‹åœ¨å®è§‚åˆ©ç”¨é˜¶æ®µå¾—åˆ°æ›´ä½æœ€ç»ˆç†µï¼Œä½¿åŸºçº¿æ¨¡å‹è¡¨ç°å¾—æ›´åƒæ›´å¼ºçš„æ¨ç†æ¨¡å‹ã€‚è¿˜æå‡ºV - Reason(Lite)å˜ä½“ï¼Œé€šè¿‡ç§»é™¤KV - cacheä¸­50%æœ€ä½èŒƒæ•°çš„è§†é¢‘tokenæ¥å‡å°‘å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒV - Reasonå’ŒV - Reason(Lite)åœ¨å¤šä¸ªè§†é¢‘æ¨ç†æ•°æ®é›†ä¸Šï¼Œç›¸æ¯”åŸºçº¿æŒ‡ä»¤å¾®è°ƒæ¨¡å‹æœ‰æ˜¾è‘—ç²¾åº¦æå‡ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†ä¸RLè®­ç»ƒæ¨¡å‹çš„ç²¾åº¦å·®è·ç¼©å°åˆ°å¹³å‡å‡†ç¡®ç‡0.6%ä»¥å†…ï¼›åŒæ—¶è¾“å‡ºtokenç›¸æ¯”RLæ¨¡å‹å‡å°‘58.6%ï¼Œæ¨ç†æ•ˆç‡å¤§å¹…æå‡ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç ”ç©¶æ€è·¯çœ‹ï¼Œé€šè¿‡åˆ†ææ¨¡å‹è¾“å‡ºç†µæ¥æŒ–æ˜æ¨ç†è¿‡ç¨‹è§„å¾‹ï¼Œä¸ºç†è§£æ¨¡å‹æ¨ç†æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œè¿™ç§ä»è¾“å‡ºåˆ†å¸ƒç»Ÿè®¡ç‰¹æ€§å…¥æ‰‹åˆ†ææ¨¡å‹è¡Œä¸ºçš„æ–¹å¼å€¼å¾—å€Ÿé‰´ï¼›åœ¨æ–¹æ³•å±‚é¢ï¼Œæå‡ºæ¨ç†æ—¶æ— éœ€è®­ç»ƒä»…åŸºäºç†µä¼˜åŒ–æ¥å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ€è·¯ï¼Œä¸ºé™ä½å¤§æ¨¡å‹æ¨ç†æˆæœ¬ã€æå‡æ¨ç†è¡¨ç°æä¾›äº†æ–°æ–¹å‘ï¼Œå°¤å…¶æ˜¯V - Reasonè¿™ç§è½»é‡æ¨ç†æ—¶ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨èµ„æºå—é™åœºæ™¯æˆ–è¿½æ±‚é«˜æ•ˆæ¨ç†åœºæ™¯æœ‰å¾ˆå¤§åº”ç”¨æ½œåŠ›ï¼›å¯¹äºæ¨¡å‹è¡Œä¸ºè°ƒæ§ï¼Œåˆ©ç”¨å¾®æ¢ç´¢å’Œå¾®åˆ©ç”¨å¾ªç¯ç­‰æ¦‚å¿µæ¥å¼•å¯¼æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼Œä¸ºè®¾è®¡æ›´æ™ºèƒ½çš„æ¨¡å‹æ¨ç†æ§åˆ¶æœºåˆ¶æä¾›äº†å‚è€ƒã€‚

## the-road-less-traveled--enhancing-exploration-in-llms-via-sequential-sampling
### Abstract
Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¦è¾Ÿè¹Šå¾„ï¼šç”¨é¡ºåºé‡‡æ ·å¢å¼ºå¤§æ¨¡å‹æ¢ç´¢èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢è‡³å…³é‡è¦ï¼Œä½†å½“å‰RLèŒƒå¼å¸¸é¢ä¸´æ¢ç´¢ä¸è¶³ä¸ç†µåç¼©é—®é¢˜ã€‚æ¨¡å‹ä¼šè¿‡åº¦åˆ©ç”¨ç‹­çª„çš„è§£å†³æ–¹æ¡ˆé›†åˆï¼ŒæŸå¤±é‡‡æ ·å¤šæ ·æ€§ï¼Œé˜»ç¢æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚å¹¶è¡Œé‡‡æ ·æ–¹æ³•ä½¿å¤šä¸ªè¾“å‡ºä»åŒä¸€åˆ†å¸ƒç”Ÿæˆï¼Œæ˜“è®©æ¨¡å‹æ”¶æ•›åˆ°ç›¸ä¼¼è§£ï¼ŒåŠ å‰§äº†è¯¥é—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°æ–¹æ³•æ¥å¢å¼ºæ¨¡å‹æ¢ç´¢æ€§ï¼Œå¹³è¡¡åˆ©ç”¨ä¸æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSESAé¡ºåºé‡‡æ ·æ¡†æ¶
SESAé‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼Œå…ˆé¡ºåºç”Ÿæˆä¸åŒçš„â€œæ–¹æ³•è‰å›¾â€æ¥å‹¾å‹’ç‹¬ç‰¹ç­–ç•¥ï¼Œå†åŸºäºè¿™äº›è‰å›¾å¹¶è¡Œæ‰©å±•ä¸ºå®Œæ•´æ¨ç†è·¯å¾„ã€‚ä¸å¹¶è¡Œé‡‡æ ·ï¼ˆåŒä¸€ç­–ç•¥ç‹¬ç«‹ç”Ÿæˆå¤šä¸ªè¾“å‡ºï¼‰ä¸åŒï¼Œé¡ºåºé‡‡æ ·è®©æ¯ä¸ªæ–°è¾“å‡ºä»¥ä¹‹å‰çš„è¾“å‡ºä¸ºæ¡ä»¶ï¼Œä¸»åŠ¨å¼•å¯¼æ¨¡å‹è¿œç¦»å·²ç”Ÿæˆè§£ï¼Œä¿è¯æ–°å€™é€‰ä¸ä¹‹å‰è¶³å¤Ÿä¸åŒï¼Œæå‡é‡‡æ ·å¤šæ ·æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“æ„åŒ–æ¢ç´¢æ–¹å¼
é€šè¿‡é¡ºåºç”Ÿæˆè‰å›¾å†æ‰©å±•çš„æ–¹å¼ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å…¼é¡¾å¤šæ ·æ€§ä¸æ•ˆç‡ã€‚æ—¢ä¿ç•™æ ·æœ¬å¤šæ ·æ€§ï¼Œåˆç»´æŒè®¡ç®—æ•ˆç‡ï¼Œä¸ºçœŸå®ä¸–ç•ŒRLåº”ç”¨æä¾›é²æ£’è§£å†³æ–¹æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åˆæˆä»»åŠ¡ä¸­ï¼Œé¡ºåºé‡‡æ ·å±•ç°ä¼˜åŠ¿ï¼šå‘ç°äº†å¹¶è¡Œé‡‡æ ·æœªå‘ç°çš„ç­–ç•¥ï¼Œä¸”ç›¸æ¯”ä¼ ç»ŸRLæ–¹æ³•ä¿ç•™äº†æ›´å¤šæ­£ç¡®è§£ï¼›åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ï¼ˆåŒ…å«æ™ºèƒ½ä½“ã€æ¨ç†ã€æ•°å­¦ç­‰äº”ç±»ä»»åŠ¡ï¼‰ä¸­ï¼ŒSESAæŒç»­è¶…è¶Šå¦‚DAPOã€RAGENç­‰å¼ºå¤§å¹¶è¡Œé‡‡æ ·åŸºçº¿ã€‚åœ¨ä¸‰ä¸ªæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSESAä½¿æˆåŠŸç‡åˆ†åˆ«ç»å¯¹æå‡0.25ã€0.42ã€0.07ï¼Œç›¸å¯¹åŸºçº¿RLæœ€å¤šæœ‰211%çš„é¢å¤–æå‡ï¼Œå‡¸æ˜¾æ¢ç´¢ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œåˆæˆä»»åŠ¡é‡Œé¡ºåºé‡‡æ ·åœ¨è·¯å¾„å¤šæ ·æ€§å’Œä»åç¼©ä¸­æ¢å¤æ–¹é¢ä¹Ÿä¼˜äºä¼ ç»ŸRLæ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è¯¥å·¥ä½œå¼•å…¥çš„ç»“æ„åŒ–æ¢ç´¢æ–¹æ³•ï¼Œä¸ºRLè®­ç»ƒçš„å¤§æ¨¡å‹å®ç°æ›´æœ‰æ•ˆã€æ›´å¤šæ ·åŒ–çš„æ¨ç†é“ºè·¯ã€‚å…¶é¡ºåºé‡‡æ ·æ€è·¯ä¸ºè§£å†³RLä¸­æ¢ç´¢ä¸è¶³é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œåç»­åœ¨æå‡å¤§æ¨¡å‹æ¨ç†å¤šæ ·æ€§ã€å¤„ç†å¤æ‚ä»»åŠ¡æ¢ç´¢ç­‰æ–¹é¢ï¼Œå¯å€Ÿé‰´è¿™ç§â€œå…ˆé¡ºåºç”Ÿæˆç­–ç•¥è‰å›¾å†æ‰©å±•â€çš„ä¸¤é˜¶æ®µæ¨¡å¼ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œé¿å…ç­–ç•¥åç¼©ï¼Œæ¨åŠ¨æ¨¡å‹æŒç»­å­¦ä¹ ä¸æ€§èƒ½æå‡ã€‚ä»£ç å¼€æºä¹Ÿä¸ºç›¸å…³ç ”ç©¶å’Œå·¥ç¨‹å®è·µæä¾›äº†å¯å¤ç°ä¸æ‹“å±•çš„åŸºç¡€ã€‚
```

## policy-transfer-ensures-fast-learning-for-continuous-time-lqr-with-entropy-regularization
### Abstract
Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¿ç»­æ—¶é—´å¸¦ç†µæ­£åˆ™LQRä¸­ç­–ç•¥è¿ç§»å¦‚ä½•ä¿éšœå¿«é€Ÿå­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½è®©æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜å†³ç­–ç­–ç•¥ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸Šä»å¤´è®­ç»ƒæ•ˆç‡æä½ã€‚è¿ç§»å­¦ä¹ ï¼ˆTLï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­æˆåŠŸåº”ç”¨ï¼Œä¸ºæå‡RLæ•ˆç‡æä¾›äº†æ–¹å‘ã€‚åœ¨å¼ºåŒ–å­¦ä¹ é‡Œï¼Œç­–ç•¥è¿ç§»æ˜¯æŠŠæºä»»åŠ¡å­¦åˆ°çš„ç­–ç•¥ç”¨äºåˆå§‹åŒ–ç›®æ ‡ä»»åŠ¡å­¦ä¹ ï¼Œç¦»æ•£æ—¶é—´LQæ¡†æ¶å·²æœ‰ç›¸å…³åˆ†æï¼Œä½†è¿ç»­æ—¶é—´RLæ¡†æ¶ä¸‹çš„ç­–ç•¥è¿ç§»åˆ†æä»æ˜¯ç©ºç™½ï¼Œä¸”é¢ä¸´å—æ§éšæœºè¿‡ç¨‹å’Œæ— é™ç»´å‡½æ•°ç©ºé—´ç­‰æŠ€æœ¯æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œè¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸéœ€æ±‚å¤§ï¼Œå› æ­¤æ¢ç´¢è¿ç»­æ—¶é—´ä¸‹çš„ç­–ç•¥è¿ç§»å¾ˆæœ‰å¿…è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡ç»™å‡ºè¿ç»­æ—¶é—´RLç­–ç•¥è¿ç§»çš„ç†è®ºè¯æ˜  
é’ˆå¯¹å¸¦ç†µæ­£åˆ™çš„è¿ç»­æ—¶é—´çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆLQRï¼‰ï¼Œè¯æ˜äº†ä¸€ä¸ªLQRçš„æœ€ä¼˜ç­–ç•¥å¯ä½œä¸ºå¯†åˆ‡ç›¸å…³LQRçš„è¿‘æœ€ä¼˜åˆå§‹åŒ–ï¼Œä¸”èƒ½ä¿ç•™åŸç®—æ³•æ”¶æ•›é€Ÿç‡ã€‚å¡«è¡¥äº†è¿ç»­æ—¶é—´RLä¸­è¿ç§»å­¦ä¹ ç†è®ºåˆ†æçš„ç©ºç™½ï¼Œå°†ç¦»æ•£æ—¶é—´çš„ç›¸å…³å·¥ä½œæ‹“å±•åˆ°è¿ç»­æ—¶é—´åœºæ™¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºè¿ç»­æ—¶é—´LQRçš„æ–°å‹ç­–ç•¥å­¦ä¹ ç®—æ³•  
è¯¥ç®—æ³•å®ç°äº†å…¨å±€çº¿æ€§å’Œå±€éƒ¨è¶…çº¿æ€§æ”¶æ•›ã€‚åˆ©ç”¨LQRæœ€ä¼˜ç­–ç•¥çš„é«˜æ–¯ç»“æ„ä»¥åŠç›¸å…³Riccatiæ–¹ç¨‹çš„é²æ£’æ€§ï¼Œä¸ºè¿ç»­æ—¶é—´LQRçš„é«˜æ•ˆå­¦ä¹ æä¾›äº†ç®—æ³•æ”¯æ’‘ï¼Œæ„å‘³ç€å¯†åˆ‡ç›¸å…³çš„LQRéƒ½èƒ½æœ‰è¶…çº¿æ€§æ”¶æ•›çš„å­¦ä¹ ç®—æ³•ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨å¯¼ä¸€ç±»è¿ç»­æ—¶é—´åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ€§ï¼ˆå‰¯äº§å“ï¼‰  
é€šè¿‡å°†åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ä¸LQRå»ºç«‹è”ç³»ï¼Œæ¨å¯¼å‡ºè¿™ç±»æ¨¡å‹çš„ç¨³å®šæ€§ã€‚å€ŸåŠ©LQRç›¸å…³åˆ†æï¼Œä¸ºæ‰©æ•£æ¨¡å‹ç¨³å®šæ€§ç ”ç©¶æä¾›äº†æ–°è§†è§’ä¸ç†è®ºä¾æ®ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡ä¸»è¦ä¾§é‡äºç†è®ºåˆ†æï¼Œé€šè¿‡å¯¹è¿ç»­æ—¶é—´LQRç­–ç•¥è¿ç§»çš„ç†è®ºæ¨å¯¼ï¼Œè®ºè¯äº†ç­–ç•¥è¿ç§»åœ¨è¿ç»­æ—¶é—´RLä¸­çš„ç†è®ºä¿éšœï¼ˆè¿‘æœ€ä¼˜åˆå§‹åŒ–ä¸æ”¶æ•›é€Ÿç‡ä¿ç•™ï¼‰ï¼›æ‰€ææ–°ç®—æ³•çš„æ”¶æ•›æ€§ä¹Ÿé€šè¿‡ç†è®ºåˆ†æå¾—ä»¥éªŒè¯ï¼ˆå…¨å±€çº¿æ€§ã€å±€éƒ¨è¶…çº¿æ€§æ”¶æ•›ï¼‰ï¼›åŒæ—¶ä½œä¸ºåˆ†æå‰¯äº§å“ï¼Œå¾—åˆ°äº†è¿ç»­æ—¶é—´åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ç¨³å®šæ€§ç»“æœï¼Œä»ç†è®ºå±‚é¢æ”¯æ’‘äº†å„åˆ›æ–°ç‚¹çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¿ç§»å­¦ä¹ åœ¨RLé¢†åŸŸçš„æ‹“å±•æ€è·¯ï¼šæŠŠå¤§è¯­è¨€æ¨¡å‹ç­‰é¢†åŸŸæˆåŠŸçš„è¿ç§»å­¦ä¹ æ€è·¯å¼•å…¥è¿ç»­æ—¶é—´RLï¼Œä¸ºæå‡å¤æ‚åœºæ™¯ä¸‹RLè®­ç»ƒæ•ˆç‡æä¾›äº†æ–¹å‘ï¼Œåç»­å¯å€Ÿé‰´è¿™ç§è·¨ä»»åŠ¡/è·¨åœºæ™¯çš„çŸ¥è¯†è¿ç§»æ€è·¯åˆ°æ›´å¤šRLå­é¢†åŸŸã€‚  
2. ç†è®ºåˆ†æä¸ç®—æ³•è®¾è®¡ç»“åˆï¼šé’ˆå¯¹è¿ç»­æ—¶é—´ç³»ç»Ÿçš„ç‰¹æ€§ï¼Œç»“åˆLQRè‡ªèº«ç»“æ„ï¼ˆé«˜æ–¯ç»“æ„ã€Riccatiæ–¹ç¨‹ï¼‰è¿›è¡Œç†è®ºåˆ†æå’Œç®—æ³•åˆ›æ–°ï¼Œè¿™ç§åŸºäºé¢†åŸŸç‰¹æœ‰ç»“æ„åšç ”ç©¶çš„æ–¹å¼ï¼Œå¯ä¸ºå…¶ä»–è¿ç»­æ—¶é—´ç³»ç»Ÿï¼ˆå¦‚æœºå™¨äººæ§åˆ¶ä¸­çš„è¿ç»­åŠ¨åŠ›å­¦ç³»ç»Ÿï¼‰çš„RLç ”ç©¶æä¾›æ–¹æ³•è®ºå‚è€ƒã€‚  
3. è·¨é¢†åŸŸè”ç³»æŒ–æ˜ï¼šé€šè¿‡å»ºç«‹LQRä¸æ‰©æ•£æ¨¡å‹çš„è”ç³»æ¥æ¨å¯¼æ‰©æ•£æ¨¡å‹ç¨³å®šæ€§ï¼Œå±•ç¤ºäº†ä¸åŒé¢†åŸŸæ¨¡å‹é—´è·¨ç•Œè”ç³»æŒ–æ˜çš„ä»·å€¼ï¼Œå¯å‘åœ¨AIä¸åŒåˆ†æ”¯ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ ä¸ç”Ÿæˆæ¨¡å‹ï¼‰é—´å¯»æ‰¾å…³è”ä»¥è§£å†³é—®é¢˜ã€‚  

## dler--doing-length-penalty-right---incentivizing-more-intelligence-per-token-via-reinforcement-learning
### Abstract
Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DLERï¼šç”¨å¼ºåŒ–å­¦ä¹ æŠŠé•¿åº¦æƒ©ç½šåšå¯¹ï¼Œæ¯Tokenæ›´æ™ºèƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ¨ç†å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚OpenAI - o1ã€DeepSeek - R1ã€Qwenï¼‰ä¾é é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½å–å¾—å¼ºæ€§èƒ½ï¼Œä½†å¸¸ç”Ÿæˆä¸å¿…è¦çš„å†—é•¿è¾“å‡ºã€‚å¦‚ä½•æœ€å¤§åŒ–â€œæ¯Tokenæ™ºèƒ½â€ï¼ˆå³å“åº”é•¿åº¦å¯¹åº”çš„å‡†ç¡®ç‡ï¼‰ä»æ˜¯å¼€æ”¾é—®é¢˜ã€‚è¿‡å¾€åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆé•¿åº¦æƒ©ç½šçš„æ–¹æ³•ï¼Œè™½èƒ½å‡çŸ­è¾“å‡ºé•¿åº¦ï¼Œå´å¸¸å‡ºç°å‡†ç¡®ç‡ä¸‹é™é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™å¹¶éæ˜¯é•¿åº¦æƒ©ç½šè®¾è®¡ä¸ç²¾å·§ï¼Œè€Œæ˜¯RLä¼˜åŒ–ä¸åˆ°ä½ã€‚åŒæ—¶è¿˜å­˜åœ¨ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¼˜åŠ¿ä¼°è®¡åå·®å¤§ã€ç†µåç¼©ã€å¥–åŠ±ä¿¡å·ç¨€ç–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDLERè®­ç»ƒæ–¹æ¡ˆ  
æ•´åˆæ‰¹é‡å¥–åŠ±å½’ä¸€åŒ–ã€æ›´é«˜çš„è£å‰ªé˜ˆå€¼ã€åŠ¨æ€é‡‡æ ·ä»¥åŠç®€å•çš„æˆªæ–­é•¿åº¦æƒ©ç½šï¼Œè§£å†³RLä¼˜åŒ–ä¸­çš„ä¸‰å¤§é—®é¢˜ï¼ˆä¼˜åŠ¿ä¼°è®¡åå·®ã€ç†µåç¼©ã€å¥–åŠ±ä¿¡å·ç¨€ç–ï¼‰ã€‚ç”¨æœ€ç®€å•çš„æˆªæ–­é•¿åº¦æƒ©ç½šï¼Œç»“åˆè¯¥æ–¹æ¡ˆå®ç°äº†å½“å‰æœ€ä¼˜çš„å‡†ç¡®ç‡ - æ•ˆç‡æƒè¡¡ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºéš¾åº¦æ„ŸçŸ¥DLERï¼ˆDA - DLERï¼‰  
æ ¹æ®æ¨¡å‹è§£å†³é—®é¢˜çš„èƒ½åŠ›ä¼°è®¡ï¼Œè‡ªé€‚åº”è°ƒæ•´æˆªæ–­ç›®æ ‡é•¿åº¦ã€‚å¯¹æ¨¡å‹èƒ½å¯é å›ç­”çš„ç®€å•é—®é¢˜è¿›ä¸€æ­¥ç¼©çŸ­æˆªæ–­é•¿åº¦ï¼Œå¯¹éš¾é¢˜æ”¾å®½é•¿åº¦é™åˆ¶ï¼Œåœ¨DeepSeek - R1 - 1.5Bå’Œ7Bä¸Šåˆ†åˆ«é¢å¤–å‡å°‘15%å’Œ11%çš„å“åº”é•¿åº¦ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºæ›´æ–°é€‰æ‹©æ€§åˆå¹¶æ–¹æ³•  
åœ¨RLè®­ç»ƒæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹ï¼Œç»“åˆåŸå§‹åŸºçº¿æ¨¡å‹ä¸DLERè®­ç»ƒåçš„æ¨¡å‹ï¼Œæ¢å¤åŸºçº¿å‡†ç¡®ç‡åŒæ—¶ä¿ç•™DLERæ¨¡å‹çš„ç®€æ´æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ— é«˜è´¨é‡ä¸“æœ‰æ•°æ®æ—¶ï¼Œä¸ºç²¾å‡†é«˜æ•ˆçš„æ¨ç†æ¨¡å‹æä¾›æ— è®­ç»ƒè·¯å¾„ï¼Œèƒ½åœ¨æ¢å¤å‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘47%å¹³å‡è¾“å‡ºé•¿åº¦ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
DLERå®ç°äº†å½“å‰æœ€ä¼˜çš„å‡†ç¡®ç‡ - æ•ˆç‡æƒè¡¡ï¼Œåœ¨å‡å°‘è¶…70%è¾“å‡ºé•¿åº¦çš„åŒæ—¶è¶…è¶Šæ‰€æœ‰å…ˆå‰åŸºçº¿å‡†ç¡®ç‡ï¼›åœ¨æµ‹è¯•æ—¶çš„å¹¶è¡Œç”Ÿæˆè¡¨ç°ä¸Šï¼ŒDLER - 7Bå¯¹æ¯”DeepSeek - R1 - 7Bï¼Œå¹¶è¡Œç”Ÿæˆå¤šä¸ªç®€æ´å“åº”æ—¶å‡†ç¡®ç‡é«˜28%ä¸”å»¶è¿Ÿæ›´ä½ï¼›DA - DLERåœ¨DeepSeek - R1 - 1.5Bå’Œ7Bä¸Šåˆ†åˆ«é¢å¤–å‡å°‘15%å’Œ11%å“åº”é•¿åº¦ï¼›æ›´æ–°é€‰æ‹©æ€§åˆå¹¶æ–¹æ³•åœ¨æ¢å¤å‡†ç¡®ç‡çš„åŒæ—¶èƒ½å‡å°‘47%å¹³å‡è¾“å‡ºé•¿åº¦ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é‡æ–°å®¡è§†ç®€å•æŠ€æœ¯ï¼ˆå¦‚æˆªæ–­é•¿åº¦æƒ©ç½šï¼‰çš„ä»·å€¼ï¼šè¡¨æ˜æ€§èƒ½æå‡å…³é”®æœ‰æ—¶ä¸åœ¨å¤æ‚è®¾è®¡ï¼Œè€Œåœ¨ä¼˜åŒ–ç®—æ³•é€‰æ‹©ï¼Œç®€å•æ–¹æ³•ç»“åˆå¥½çš„ä¼˜åŒ–æ–¹æ¡ˆèƒ½è¶…å¤æ‚æ–¹æ³•ã€‚  
2. è‡ªé€‚åº”ä¸åœºæ™¯æ„ŸçŸ¥è®¾è®¡ï¼šDA - DLERæ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´ç­–ç•¥ï¼Œä¸ºä¸åŒéš¾åº¦ä»»åŠ¡çš„èµ„æºåˆ†é…æä¾›æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°éœ€åŠ¨æ€è°ƒæ•´èµ„æºæˆ–ç­–ç•¥çš„åœºæ™¯ã€‚  
3. æ•°æ®ç¨€ç¼ºä¸‹çš„æ¨¡å‹èåˆç­–ç•¥ï¼šæ›´æ–°é€‰æ‹©æ€§åˆå¹¶æ–¹æ³•ä¸ºæ•°æ®ä¸è¶³æ—¶æ¨¡å‹ä¼˜åŒ–æä¾›äº†æ— è®­ç»ƒå¼çš„æœ‰æ•ˆè·¯å¾„ï¼Œåœ¨ä¼ä¸šç§æœ‰æ•°æ®éš¾è·å–ç­‰åœºæ™¯æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## agentic-entropy-balanced-policy-optimization
### Abstract
Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¹³è¡¡ç†µæŒ‘æˆ˜ï¼ŒåŠ©åŠ›æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼šAEPOç®—æ³•æ¥è¢­

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆAgentic RLï¼‰é¢†åŸŸï¼Œå°½ç®¡ä¸»æµç®—æ³•å€ŸåŠ©ç†µå¼•å¯¼æ¢ç´¢é«˜ä¸ç¡®å®šæ€§çš„å·¥å…·è°ƒç”¨æ­¥éª¤ä»¥æå‡å¤šè½®ã€é•¿å‘¨æœŸå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä½†è¿‡åº¦ä¾èµ–ç†µä¿¡å·ä¼šå¼•å‘è®­ç»ƒå´©æºƒç­‰é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºä¸¤æ–¹é¢æŒ‘æˆ˜ï¼šä¸€æ˜¯é«˜ç†µrolloutå´©æºƒï¼Œrollouté˜¶æ®µé«˜ç†µå·¥å…·è°ƒç”¨æ­¥éª¤è¿ç»­å‡ºç°æ—¶ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šæ²¿å•ä¸€è·¯å¾„è¿‡åº¦åˆ†æ”¯ï¼Œè€—å°½å…¶ä»–è½¨è¿¹åˆ†æ”¯é¢„ç®—ï¼Œé™åˆ¶é‡‡æ ·å¤šæ ·æ€§ï¼›äºŒæ˜¯é«˜ç†µtokenæ¢¯åº¦è£å‰ªï¼Œç­–ç•¥æ›´æ–°é˜¶æ®µä¼ ç»Ÿç®—æ³•ä¼šè¿‡åº¦è£å‰ªé«˜ç†µtokenæ¢¯åº¦ï¼Œå¯¼è‡´LLMæ¢ç´¢è¿‡æ—©ç»ˆæ­¢ã€‚å› æ­¤ï¼Œåœ¨æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­é«˜æ•ˆå¹³è¡¡ç†µæˆä¸ºå®ç°é€šç”¨æ™ºèƒ½ä½“è®­ç»ƒçš„å…³é”®æŒ‘æˆ˜ï¼Œæœ¬æ–‡æ­£æ˜¯ä¸ºè§£å†³è¿™äº›é—®é¢˜è€Œæå‡ºAgentic Entropy - Balanced Policy Optimizationï¼ˆAEPOï¼‰ç®—æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€ç†µå¹³è¡¡rolloutæœºåˆ¶
ä¸ºç¼“è§£â€œé«˜ç†µrolloutå´©æºƒâ€é—®é¢˜ï¼ŒAEPOæå‡ºç†µé¢„ç›‘æµ‹æœºåˆ¶ï¼Œä»¥æ­¤è‡ªé€‚åº”åˆ†é…å…¨å±€å’Œåˆ†æ”¯é‡‡æ ·é¢„ç®—ï¼Œç¡®ä¿æ ‘çŠ¶rolloutè¿‡ç¨‹ä¸­æ¢ç´¢çš„å¹³è¡¡æ€§ã€‚åŒæ—¶ï¼Œé’ˆå¯¹è¿ç»­é«˜ç†µå·¥å…·è°ƒç”¨æ­¥éª¤å¼•å…¥åˆ†æ”¯æƒ©ç½šç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³ç‰¹å®šé“¾è·¯ä¸Šçš„è¿‡åº¦åˆ†æ”¯é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†µå¹³è¡¡ç­–ç•¥ä¼˜åŒ–
å—è¿‘æœŸè£å‰ªä¼˜åŒ–å·¥ä½œå¯å‘ï¼Œä¸ºè§£å†³â€œé«˜ç†µtokenæ¢¯åº¦è£å‰ªâ€é—®é¢˜ï¼ŒAEPOåœ¨ç­–ç•¥æ›´æ–°æ—¶å°†åœæ­¢æ¢¯åº¦æ“ä½œèå…¥é«˜ç†µè£å‰ªé¡¹ã€‚è¿™ä¸€æ“ä½œåœ¨åå‘ä¼ æ’­ä¸­ä¿ç•™å¹¶åˆç†ç¼©æ”¾é«˜ç†µtokençš„æ¢¯åº¦ï¼Œå‰å‘ä¼ æ’­ä¸å—å½±å“ã€‚æ­¤å¤–ï¼ŒAEPOæå‡ºç†µæ„ŸçŸ¥ä¼˜åŠ¿ä¼°è®¡ï¼Œå°†ç†µä¼˜åŠ¿æ•´åˆåˆ°ä¼ ç»Ÿä¼˜åŠ¿ä¼°è®¡ä¸­ï¼Œè®©æ¨¡å‹ä¼˜å…ˆå­¦ä¹ é«˜ä¸ç¡®å®šæ€§tokenã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¶µç›–æ·±åº¦ä¿¡æ¯æœç´¢ã€çŸ¥è¯†å¯†é›†å‹æ¨ç†å’Œè®¡ç®—æ¨ç†çš„14ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºAEPOæŒç»­è¶…è¶Š7ç§ä¸»æµRLç®—æ³•ã€‚ä»…ç”¨1K RLæ ·æœ¬æ—¶ï¼Œæ­è½½AEPOçš„Qwen3 - 14Bå–å¾—äº®çœ¼æˆç»©ï¼šPass@1æŒ‡æ ‡ä¸‹ï¼ŒGAIAè¾¾47.6%ã€Humanity's Last Examè¾¾11.2%ã€WebWalkerQAè¾¾43.0%ï¼›Pass@5æŒ‡æ ‡ä¸‹ï¼ŒGAIAè¾¾65.0%ã€Humanity's Last Examè¾¾26.0%ã€WebWalkerQAè¾¾70.0%ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒAEPOåœ¨æå‡rollouté‡‡æ ·å¤šæ ·æ€§çš„åŒæ—¶èƒ½ç»´æŒç¨³å®šçš„ç­–ç•¥ç†µï¼Œä¸ºå¯æ‰©å±•çš„ç½‘ç»œæ™ºèƒ½ä½“è®­ç»ƒæä¾›æ”¯æŒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä¸­å› ç†µå¼•å‘çš„è®­ç»ƒé—®é¢˜ï¼Œä»rolloutå’Œç­–ç•¥æ›´æ–°ä¸¤é˜¶æ®µå…¥æ‰‹è®¾è®¡å¹³è¡¡æœºåˆ¶çš„æ€è·¯å…·æœ‰åˆ›æ–°æ€§ï¼Œä¸ºè§£å†³ç±»ä¼¼å¤šé˜¶æ®µã€æ˜“å—å•ä¸€ä¿¡å·å½±å“å¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜æä¾›äº†åˆ†é˜¶æ®µä¼˜åŒ–çš„å‚è€ƒèŒƒå¼ã€‚
2. ç†µé¢„ç›‘æµ‹ã€åˆ†æ”¯æƒ©ç½šã€åœæ­¢æ¢¯åº¦ç»“åˆé«˜ç†µè£å‰ªã€ç†µæ„ŸçŸ¥ä¼˜åŠ¿ä¼°è®¡ç­‰å…·ä½“æŠ€æœ¯æ‰‹æ®µï¼Œåœ¨å¤„ç†èµ„æºåˆ†é…ã€æ¢¯åº¦ä¿ç•™ä¸ç¼©æ”¾ã€å­¦ä¹ ä¼˜å…ˆçº§ç­‰æ–¹é¢æä¾›äº†æ–°é¢–çš„å®ç°æ–¹å¼ï¼Œå¯å¯å‘åç»­åœ¨å¼ºåŒ–å­¦ä¹ æˆ–å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–æ–¹å‘çš„ç ”ç©¶ï¼Œç”¨äºè§£å†³ä¸åŒåœºæ™¯ä¸‹å› ä¸ç¡®å®šæ€§å¼•å¯¼å¸¦æ¥çš„è®­ç»ƒéš¾é¢˜ã€‚
3. åœ¨å¤šç±»å‹å¤æ‚æ•°æ®é›†ä¸ŠéªŒè¯ç®—æ³•æœ‰æ•ˆæ€§çš„å®éªŒè®¾è®¡ï¼Œä¸ºç®—æ³•æ³›åŒ–æ€§éªŒè¯æä¾›äº†èŒƒä¾‹ï¼Œåç»­ç ”ç©¶åœ¨è¯„ä¼°æ–°æ–¹æ³•æ—¶å¯å€Ÿé‰´è¿™ç§å¤šåœºæ™¯ã€å¤šä»»åŠ¡çš„å…¨é¢æµ‹è¯•æ–¹å¼ã€‚

## deepplanner--scaling-planning-capability-for-deep-research-agents-via-advantage-shaping
### Abstract
Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | DeepPlannerï¼šé€šè¿‡ä¼˜åŠ¿å¡‘é€ æå‡æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“çš„è§„åˆ’èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆå¤šæ­¥æ¨ç†ä¸åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨åˆ©ç”¨å¤–éƒ¨å·¥å…·å¤„ç†éœ€é•¿æœŸè§„åˆ’çš„å¤æ‚ä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåŠ›ã€‚ä½†ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–æ¨ç†é˜¶æ®µçš„éšå¼è§„åˆ’ï¼Œè¦ä¹ˆå¼•å…¥æ˜¾å¼è§„åˆ’å™¨å´æœªç³»ç»Ÿä¼˜åŒ–è§„åˆ’é˜¶æ®µã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨åŸå§‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸‹ï¼Œè§„åˆ’tokençš„ç†µæ˜¾è‘—é«˜äºå…¶ä»–åŠ¨ä½œtokenï¼Œæ„å‘³ç€è§„åˆ’é˜¶æ®µå­˜åœ¨æœªå……åˆ†ä¼˜åŒ–çš„ä¸ç¡®å®šå†³ç­–ç‚¹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºDeepPlanneræ¡†æ¶æ¥å¢å¼ºæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“çš„è§„åˆ’èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç†µçš„token - çº§ä¼˜åŠ¿å¡‘é€   
å—ç›¸å…³å·¥ä½œå¯å‘ï¼Œç»™åŸå§‹token - çº§ä¼˜åŠ¿é™„åŠ ä¸€ä¸ªåŸºäºç†µçš„é¡¹ã€‚è¿™æ ·èƒ½åœ¨ä¸ç¡®å®štokenï¼ˆä¸»è¦æ˜¯è§„åˆ’é˜¶æ®µçš„tokenï¼‰ä¸Šæ”¾å¤§æ¢¯åº¦ï¼ŒåŒæ—¶é€šè¿‡è£å‰ªé˜²æ­¢å¼ºè´Ÿä¼˜åŠ¿æ—¶çš„ç¬¦å·ç¿»è½¬ã€‚è¯¥åˆ†ç¦»çš„å¡‘é€ é¡¹ä¸»è¦å¼ºåŒ–æœ‰åˆ©çš„è§„åˆ’è½¨è¿¹ï¼Œé˜²æ­¢ç†µåç¼©ï¼Œä¿æŒæŒç»­æ¢ç´¢ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€‰æ‹©æ€§ä¼˜åŠ¿åŠ æƒ  
ä¸ºå¼ºåŒ–è§„åˆ’å¯†é›†å‹ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¼•å…¥é€‰æ‹©æ€§ä¼˜åŠ¿åŠ æƒã€‚åœ¨åŒä¸€æŸ¥è¯¢ä¸‹çš„æ¯ä¸ªrolloutç»„ä¸­ï¼Œç¡®å®šæœ€æœ‰æ•ˆçš„rolloutï¼ˆæ­£ç¡®ç­”æ¡ˆä¸”å·¥å…·è°ƒç”¨æœ€å°‘ï¼‰ï¼Œå¹¶å°†å…¶å·¥å…·è°ƒç”¨æ•°å®šä¹‰ä¸ºæŸ¥è¯¢å¤æ‚åº¦ã€‚ç„¶åå¯¹è¶…è¿‡å¤æ‚åº¦é˜ˆå€¼ç»„ä¸­çš„æœ€æœ‰æ•ˆrolloutï¼Œæå‡æ ·æœ¬çº§ä¼˜åŠ¿çš„æƒé‡ï¼Œåœ¨ä¿æŒç«¯åˆ°ç«¯ç®€å•æ€§çš„åŒæ—¶è·å¾—æ€§èƒ½æå‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸ƒä¸ªæ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeepPlanneræé«˜äº†è§„åˆ’è´¨é‡ï¼Œåœ¨è®­ç»ƒé¢„ç®—å¤§å¹…é™ä½çš„æƒ…å†µä¸‹å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ä¸ä¹‹å‰çš„SOTAæ¡†æ¶EvolveSearchç›¸æ¯”ï¼ŒDeepPlannerä»…ç”¨3072ä¸ªæŸ¥è¯¢ä¸”æ¯ä¸ªæŸ¥è¯¢8æ¬¡rolloutsï¼Œè€ŒEvolveSearchéœ€è¦å¤š10å€çš„è®­ç»ƒæ ·æœ¬å’Œå¤§çº¦2å€çš„rolloutsã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼šæ˜¾å¼è§„åˆ’æé«˜äº†é•¿æ—¶ä»»åŠ¡çš„æ€§èƒ½ï¼›åŸºäºç†µçš„ä¼˜åŠ¿å¡‘é€ åœ¨æ— ç†µåç¼©çš„æƒ…å†µä¸‹åŠ é€Ÿäº†æœ‰æ•ˆè§„åˆ’ä¼˜åŒ–ï¼›é€‰æ‹©æ€§ä¼˜åŠ¿åŠ æƒèƒ½æ›´å¥½åœ°åˆ©ç”¨éœ€è¦å¯†é›†è§„åˆ’çš„å¤æ‚rolloutsã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¯Šæ–­è§„åˆ’èƒ½åŠ›çŸ­æ¿çš„æ€è·¯ï¼šé€šè¿‡å°†ç°æœ‰æ¡†æ¶æ‰©å±•ä¸ºâ€œå…ˆè§„åˆ’åæ‰§è¡Œâ€ç»“æ„ï¼Œé‡åŒ–è§„åˆ’tokençš„é«˜ç†µé—®é¢˜ï¼Œä¸ºæå‡è§„åˆ’èƒ½åŠ›æŒ‡æ˜æ–¹å‘ï¼Œè¿™ç§è¯Šæ–­æ–¹æ³•å¯ç”¨äºç±»ä¼¼çš„æ™ºèƒ½ä½“èƒ½åŠ›ä¼˜åŒ–åœºæ™¯ã€‚
2. ä¼˜åŠ¿å¡‘é€ æœºåˆ¶ï¼šåŸºäºç†µçš„token - çº§ä¼˜åŠ¿å¡‘é€ å’Œé€‰æ‹©æ€§ä¼˜åŠ¿åŠ æƒè¿™ä¸¤ç§æœºåˆ¶ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹ä¼˜åŒ–æ™ºèƒ½ä½“ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚è§„åˆ’èƒ½åŠ›ï¼‰æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–éœ€è¦ä¼˜åŒ–ç‰¹å®šé˜¶æ®µå†³ç­–çš„æ™ºèƒ½ä½“è®­ç»ƒä»»åŠ¡ä¸­ã€‚
3. é«˜æ•ˆè®­ç»ƒæ¨¡å¼ï¼šåœ¨å¤§å¹…é™ä½è®­ç»ƒèµ„æºçš„æƒ…å†µä¸‹å®ç°SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ–¹æ³•åœ¨èµ„æºåˆ©ç”¨ä¸Šçš„é«˜æ•ˆæ€§ï¼Œä¸ºèµ„æºå—é™ä¸‹çš„æ™ºèƒ½ä½“è®­ç»ƒæä¾›äº†å‚è€ƒã€‚
```

## finite-time-convergence-analysis-of-actor-critic-with-evolving-reward
### Abstract
Many popular practical reinforcement learning (RL) algorithms employ evolving
reward functions-through techniques such as reward shaping, entropy
regularization, or curriculum learning-yet their theoretical foundations remain
underdeveloped. This paper provides the first finite-time convergence analysis
of a single-timescale actor-critic algorithm in the presence of an evolving
reward function under Markovian sampling. We consider a setting where the
reward parameters may change at each time step, affecting both policy
optimization and value estimation. Under standard assumptions, we derive
non-asymptotic bounds for both actor and critic errors. Our result shows that
an $O(1/\sqrt{T})$ convergence rate is achievable, matching the best-known rate
for static rewards, provided the reward parameters evolve slowly enough. This
rate is preserved when the reward is updated via a gradient-based rule with
bounded gradient and on the same timescale as the actor and critic, offering a
theoretical foundation for many popular RL techniques. As a secondary
contribution, we introduce a novel analysis of distribution mismatch under
Markovian sampling, improving the best-known rate by a factor of $\log^2T$ in
the static-reward case.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŠ¨æ€å¥–åŠ±ä¸‹Actor - Criticç®—æ³•çš„æœ‰é™æ—¶é—´æ”¶æ•›æ€§åˆ†æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å®è¯æ–¹é¢å·²å–å¾—è¯¸å¤šæˆåŠŸï¼Œä½†ç†è®ºåŸºç¡€ä»éœ€å®Œå–„ã€‚è®¸å¤šå®ç”¨RLç®—æ³•é‡‡ç”¨åŠ¨æ€å¥–åŠ±æŠ€æœ¯ï¼ˆå¦‚å¥–åŠ±å¡‘é€ ã€ç†µæ­£åˆ™åŒ–ã€è¯¾ç¨‹å­¦ä¹ ç­‰ï¼‰ï¼Œç„¶è€Œå…¶ç†è®ºåŸºç¡€å‘å±•ä¸è¶³ã€‚RLç†è®ºå¸¸åŸºäºé™æ€å¥–åŠ±çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œä½†å®é™…åº”ç”¨ä¸­è®¾è®¡åˆé€‚å¥–åŠ±å‡½æ•°å›°éš¾ï¼Œå‚¬ç”Ÿäº†åŠ¨æ€å¥–åŠ±æŠ€æœ¯ã€‚éœ€æ˜ç¡®å¥–åŠ±å¤šå¿«å˜åŒ–èƒ½ä¿è¯RLç®—æ³•æ”¶æ•›ï¼Œæœ¬æ–‡æ—¨åœ¨ä¸ºå«åŠ¨æ€å¥–åŠ±çš„Actor - Criticç®—æ³•æä¾›é¦–ä¸ªæœ‰é™æ—¶é—´æ”¶æ•›æ€§åˆ†æï¼Œå¡«è¡¥ç†è®ºç©ºç™½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé—®é¢˜å½¢å¼åŒ–å®šä¹‰
å°†å«åŠ¨æ€å¥–åŠ±çš„Actor - Criticé—®é¢˜å½¢å¼åŒ–ï¼Œå¥–åŠ±å‚æ•°\(\phi_t\)ï¼ˆåŒ…å«çœŸå®å¥–åŠ±å’Œæ­£åˆ™é¡¹ç­‰ï¼‰å¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥ç”±ä»»æ„â€œ oracle â€æ›´æ–°ï¼Œæ¶µç›–äº†å¤šç§åŠ¨æ€å¥–åŠ±åœºæ™¯ï¼Œä¸ºåç»­åˆ†æå¥ å®šåŸºç¡€ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéæ¸è¿‘æ”¶æ•›ç»“æœæ¨å¯¼
åœ¨æ ‡å‡†å‡è®¾ï¼ˆçº¿æ€§å‡½æ•°è¿‘ä¼¼çš„criticã€ç­–ç•¥å’Œå¥–åŠ±çš„ Lipschitz è¿ç»­æ€§ã€å……åˆ†æ¢ç´¢ç­‰ï¼‰ä¸‹ï¼Œæ¨å¯¼äº†å•æ ·æœ¬å•æ—¶é—´å°ºåº¦Actor - Criticç®—æ³•åœ¨é©¬å°”å¯å¤«é‡‡æ ·ä¸‹çš„æ”¶æ•›é€Ÿç‡ã€‚è¯æ˜äº†è‹¥å¥–åŠ±å‚æ•°æ¼”åŒ–è¶³å¤Ÿæ…¢ï¼Œèƒ½è¾¾åˆ°\(O(1/\sqrt{T})\)çš„æ”¶æ•›é€Ÿç‡ï¼Œä¸é™æ€å¥–åŠ±ä¸‹å·²çŸ¥æœ€ä¼˜é€Ÿç‡åŒ¹é…ï¼›ä¸”å½“å¥–åŠ±é€šè¿‡æœ‰ç•Œæ¢¯åº¦çš„åŸºäºæ¢¯åº¦çš„è§„åˆ™æ›´æ–°ä¸”ä¸actorå’ŒcriticåŒæ—¶é—´å°ºåº¦æ—¶ï¼Œè¯¥é€Ÿç‡ä»ä¿æŒï¼Œä¸ºä¼—å¤šå®ç”¨RLæŠ€æœ¯ï¼ˆå¦‚å¥½å¥‡å¿ƒé©±åŠ¨çš„å¥–åŠ±å¡‘é€ ã€éšæœºç½‘ç»œè’¸é¦æ–¹æ³•ã€å¸¦è‡ªåŠ¨ç†µè°ƒæ•´çš„è½¯Actor - Criticç­‰ï¼‰æä¾›ç†è®ºä¿éšœã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé©¬å°”å¯å¤«é‡‡æ ·ä¸‹åˆ†å¸ƒä¸åŒ¹é…çš„æ–°é¢–åˆ†æ
å¼•å…¥å¯¹é©¬å°”å¯å¤«é‡‡æ ·å¯¼è‡´çš„åˆ†å¸ƒä¸åŒ¹é…çš„æ–°é¢–åˆ†æï¼Œåœ¨é™æ€å¥–åŠ±æƒ…å†µä¸‹ï¼Œå°†å·²çŸ¥æ”¶æ•›é€Ÿç‡æå‡äº†\(\log^2 T\)å€ï¼Œç‹¬ç«‹æ”¹è¿›äº†é™æ€å¥–åŠ±åœºæ™¯ä¸‹çš„æ”¶æ•›é€Ÿç‡ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ç›®æ ‡å‡½æ•°å’Œæœ€ä¼˜criticå‚æ•°å…³äºå¥–åŠ±å‚æ•°çš„ Lipschitz è¿ç»­æ€§æ¥å¤„ç†åŠ¨æ€å¥–åŠ±çš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªæåŠä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®éªŒéƒ¨åˆ†ï¼ˆå¦‚åœ¨ç‰¹å®šç¯å¢ƒä¸‹çš„å®éªŒå¯¹æ¯”ç­‰ï¼‰ï¼Œä¸»è¦æ˜¯ç†è®ºæ¨å¯¼å¾—å‡ºæ”¶æ•›é€Ÿç‡ç­‰ç†è®ºç»“æœï¼Œè¯æ˜äº†åœ¨åŠ¨æ€å¥–åŠ±åœºæ™¯ä¸‹Actor - Criticç®—æ³•çš„æ”¶æ•›æ€§åŠé€Ÿç‡ç›¸å…³ç»“è®ºï¼Œä»¥åŠåœ¨é™æ€å¥–åŠ±åœºæ™¯ä¸‹å¯¹åˆ†å¸ƒä¸åŒ¹é…åˆ†æå¸¦æ¥çš„é€Ÿç‡æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜å»ºæ¨¡æ–¹é¢ï¼šå°†åŠ¨æ€å¥–åŠ±ä¸‹çš„Actor - Criticé—®é¢˜å½¢å¼åŒ–ï¼Œä¸ºåç»­ç ”ç©¶ç±»ä¼¼å«åŠ¨æ€å…ƒç´ çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†å»ºæ¨¡å‚è€ƒï¼Œå¯å¯å‘ç ”ç©¶è€…å…³æ³¨å®é™…åº”ç”¨ä¸­å¥–åŠ±åŠ¨æ€å˜åŒ–è¿™ä¸€å¸¸è§å´ç†è®ºç ”ç©¶ä¸è¶³çš„åœºæ™¯ã€‚
2. ç†è®ºåˆ†ææŠ€å·§ï¼šåˆ©ç”¨ Lipschitz è¿ç»­æ€§å¤„ç†åŠ¨æ€å¥–åŠ±å½±å“ã€å¯¹é©¬å°”å¯å¤«é‡‡æ ·ä¸‹åˆ†å¸ƒä¸åŒ¹é…çš„åˆ†ææ–¹æ³•ç­‰ï¼Œè¿™äº›æŠ€æœ¯å·¥å…·å¯ä¸ºåˆ†æå…¶ä»–éå¹³ç¨³ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æˆ–å¸¦é‡‡æ ·åå·®çš„ç®—æ³•æä¾›æ€è·¯ï¼Œå¸®åŠ©æå‡ç®—æ³•æ”¶æ•›æ€§åˆ†æçš„æ•ˆç‡å’Œæ•ˆæœã€‚
3. å®é™…åº”ç”¨æ”¯æ’‘ï¼šä¸ºä¼—å¤šå®ç”¨RLæŠ€æœ¯æä¾›äº†ç†è®ºåŸºç¡€ï¼Œè®©å®è·µè€…åœ¨ä½¿ç”¨å¥–åŠ±å¡‘é€ ã€ç†µæ­£åˆ™åŒ–ç­‰æŠ€æœ¯æ—¶æœ‰æ›´åšå®çš„ç†è®ºä¾æ®ï¼Œå¢å¼ºäº†è¿™äº›æŠ€æœ¯åœ¨å®é™…åœºæ™¯ï¼ˆå¦‚å¤æ‚ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€æœºå™¨äººæ§åˆ¶ç­‰ï¼‰ä¸­åº”ç”¨çš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ã€‚

## reinforced-sequential-monte-carlo-for-amortised-sampling
### Abstract
This paper proposes a synergy of amortised and particle-based methods for
sampling from distributions defined by unnormalised density functions. We state
a connection between sequential Monte Carlo (SMC) and neural sequential
samplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein
learnt sampling policies and value functions define proposal kernels and twist
functions. Exploiting this connection, we introduce an off-policy RL training
procedure for the sampler that uses samples from SMC -- using the learnt
sampler as a proposal -- as a behaviour policy that better explores the target
distribution. We describe techniques for stable joint training of proposals and
twist functions and an adaptive weight tempering scheme to reduce training
signal variance. Furthermore, building upon past attempts to use experience
replay to guide the training of neural samplers, we derive a way to combine
historical samples with annealed importance sampling weights within a replay
buffer. On synthetic multi-modal targets (in both continuous and discrete
spaces) and the Boltzmann distribution of alanine dipeptide conformations, we
demonstrate improvements in approximating the true distribution as well as
training stability compared to both amortised and Monte Carlo methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | èåˆ amortised ä¸ç²’å­æ–¹æ³•ï¼šå¼ºåŒ–åºè´¯è’™ç‰¹å¡æ´›å®ç°é«˜æ•ˆé‡‡æ ·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä»é«˜ç»´éå½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·æ˜¯è¯¸å¤šé¢†åŸŸï¼ˆå¦‚è´å¶æ–¯æ¨æ–­ã€è®¡ç®—åŒ–å­¦ä¸­åˆ†å­æ„è±¡é‡‡æ ·ï¼‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚ä¼ ç»Ÿè’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ–¹æ³•ï¼ˆå¦‚ MCMCã€é‡è¦æ€§é‡‡æ ·ç­‰ï¼‰è™½èƒ½é€æ­¥é€¼è¿‘ç›®æ ‡åˆ†å¸ƒï¼Œä½†åœ¨é«˜ç»´å¤æ‚åœºæ™¯ä¸‹å¸¸éœ€å¤§é‡æ­¥éª¤æˆ–æ ·æœ¬ï¼›è€Œ amortised é‡‡æ ·æ–¹æ³•ï¼ˆåŸºäºç¥ç»ç½‘ç»œçš„åˆ†å±‚ latent æ¨¡å‹ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ã€è‡ªå›å½’æ¨¡å‹ï¼‰è™½å¯åˆ©ç”¨æ·±åº¦ç½‘ç»œæ³›åŒ–æ€§å¿«é€Ÿç”Ÿæˆæ ·æœ¬ï¼Œå´æ˜“å› æ¨¡å‹å®¹é‡ä¸è¶³æˆ–ä¼˜åŒ–é—®é¢˜ï¼ˆå¦‚æ¨¡å¼åå¡Œï¼‰æ— æ³•æ”¶æ•›åˆ°ç›®æ ‡åˆ†å¸ƒã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨èåˆäºŒè€…ä¼˜åŠ¿ï¼Œè®© MC æ–¹æ³•å€ŸåŠ© amortised é‡‡æ ·çš„å­¦ä¹ ç»„ä»¶ï¼ŒåŒæ—¶ amortised é‡‡æ ·å™¨åˆ©ç”¨ MC æ–¹æ³•çš„ç¦»ç­–ç•¥æ ·æœ¬è®­ç»ƒï¼Œå®ç°æ›´ä¼˜é‡‡æ ·æ•ˆæœä¸è®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€ HVIã€MaxEnt RL ä¸ SMC/AIS ç†è®ºæ¡†æ¶  
é¦–æ¬¡åœ¨å•ä¸€æ¡†æ¶ä¸‹é˜è¿°åˆ†å±‚å˜åˆ†æ¨æ–­ï¼ˆHVIï¼‰ã€æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ï¼ˆMaxEnt RLï¼‰ä¸å¸¦é€€ç«é‡è¦æ€§é‡‡æ ·ï¼ˆAISï¼‰çš„åºè´¯è’™ç‰¹å¡æ´›ï¼ˆSMCï¼‰ä¹‹é—´çš„æ•°å­¦è”ç³»ã€‚æ˜ç¡®åœ¨ MaxEnt RL è®­ç»ƒçš„ç¥ç»åºè´¯é‡‡æ ·å™¨ä¸­ï¼Œå­¦ä¹ åˆ°çš„é‡‡æ ·ç­–ç•¥å’Œå€¼å‡½æ•°å¯å¯¹åº” SMC ä¸­çš„æè®®æ ¸ï¼ˆproposal kernelsï¼‰ä¸æ‰­æ›²å‡½æ•°ï¼ˆtwist functionsï¼‰ï¼Œä¸ºåç»­ç®—æ³•è®¾è®¡ç­‘ç‰¢ç†è®ºæ ¹åŸºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç¦»ç­–ç•¥ RL è®­ç»ƒæµç¨‹ä¸ç¨³å®šè®­ç»ƒæŠ€æœ¯  
æå‡ºåˆ©ç”¨ SMC æ ·æœ¬ï¼ˆä»¥å­¦ä¹ åˆ°çš„é‡‡æ ·å™¨ä½œä¸ºæè®®ï¼‰ä½œä¸ºè¡Œä¸ºç­–ç•¥ï¼Œå¼€å±• amortised é‡‡æ ·å™¨çš„ç¦»ç­–ç•¥ RL è®­ç»ƒï¼Œè®©é‡‡æ ·å™¨æ›´é«˜æ•ˆæ¢ç´¢ç›®æ ‡åˆ†å¸ƒã€‚åŒæ—¶ï¼Œè®¾è®¡ææ¡ˆæ ¸ä¸æ‰­æ›²å‡½æ•°çš„ç¨³å®šè”åˆè®­ç»ƒæŠ€æœ¯ã€è‡ªé€‚åº”æƒé‡å›ç«ï¼ˆweight temperingï¼‰æ–¹æ¡ˆä»¥é™ä½è®­ç»ƒä¿¡å·æ–¹å·®ï¼›è¿˜åŸºäºç»éªŒå›æ”¾æ€æƒ³ï¼Œæ¨å¯¼åœ¨å›æ”¾ç¼“å†²åŒºä¸­ç»“åˆå†å²æ ·æœ¬ä¸é€€ç«é‡è¦æ€§é‡‡æ ·æƒé‡çš„æ–¹æ³•ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ä¸æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è¿ç»­ä¸ç¦»æ•£ç©ºé—´çš„åˆæˆå¤šæ¨¡æ€ç›®æ ‡åˆ†å¸ƒï¼Œä»¥åŠä¸™æ°¨é…¸äºŒè‚½æ„è±¡çš„ç»å°”å…¹æ›¼åˆ†å¸ƒç­‰ä»»åŠ¡ä¸Šï¼Œå¯¹æ¯”çº¯ amortised å’Œ Monte Carlo æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨é€¼è¿‘çœŸå®åˆ†å¸ƒç²¾åº¦ä¸è®­ç»ƒç¨³å®šæ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨æ¨¡å¼è¦†ç›–èƒ½åŠ›ç­‰æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºèåˆè§†è§’**ï¼šå°†ä¸åŒé¢†åŸŸï¼ˆå˜åˆ†æ¨æ–­ã€å¼ºåŒ–å­¦ä¹ ã€è’™ç‰¹å¡æ´›ï¼‰çš„æ–¹æ³•ç»Ÿä¸€åˆ†æï¼Œä¸ºè·¨é¢†åŸŸç®—æ³•åˆ›æ–°æä¾›æ€è·¯ï¼Œå¯å‘ç ”ç©¶è€…ä»ç†è®ºè”ç³»ä¸­æŒ–æ˜æ–¹æ³•èåˆçš„å¯èƒ½æ€§ã€‚  
2. **è®­ç»ƒç¨³å®šæ€§æŠ€æœ¯**ï¼šè‡ªé€‚åº”æƒé‡å›ç«ã€åŸºäºç»éªŒå›æ”¾ç»“åˆ AIS æƒé‡ç­‰æŠ€æœ¯ï¼Œä¸ºè®­ç»ƒå«éšæœºç»„ä»¶æˆ–éœ€å¤„ç†é«˜æ–¹å·®ä¿¡å·çš„æ¨¡å‹æä¾›äº†å¯å¤ç”¨çš„ç¨³å®šè®­ç»ƒæ‰‹æ®µã€‚  
3. **è·¨æ¨¡æ€ä¸çœŸå®åœºæ™¯éªŒè¯**ï¼šåœ¨è¿ç»­ã€ç¦»æ•£ç©ºé—´åŠçœŸå®åˆ†å­æ¨¡æ‹ŸåŸºå‡†ä»»åŠ¡ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºæ–¹æ³•åœ¨å¤šé¢†åŸŸå¤æ‚åˆ†å¸ƒé‡‡æ ·ä¸­çš„æ™®é€‚æ€§ï¼Œä¸ºç›¸å…³é¢†åŸŸï¼ˆå¦‚è®¡ç®—åŒ–å­¦ã€è´å¶æ–¯æ¨æ–­ï¼‰çš„é‡‡æ ·é—®é¢˜æä¾›æ–°è§£æ³•å‚è€ƒã€‚

## demystifying-reinforcement-learning-in-agentic-reasoning
### Abstract
Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­å¼€æ™ºèƒ½ä½“æ¨ç†ä¸­å¼ºåŒ–å­¦ä¹ çš„ç¥ç§˜é¢çº±

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆagentic RLï¼‰çš„å‡ºç°è¡¨æ˜å¼ºåŒ–å­¦ä¹ å¯æœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›ï¼Œä½†å…³é”®è®¾è®¡åŸåˆ™ä¸æœ€ä¼˜å®è·µä»ä¸æ¸…æ™°ã€‚åŒæ—¶ï¼Œåœ¨æ™ºèƒ½ä½“æ¨ç†ä¸­æ‰©å±•å¼ºåŒ–å­¦ä¹ é¢ä¸´æŒ‘æˆ˜ï¼šæ•°æ®å±‚é¢ï¼Œç°æœ‰æ•°æ®ç­–åˆ’ä¾èµ–æ‹¼æ¥å¼åˆæˆæ•°æ®ï¼Œå¿½ç•¥æ¨ç†ä¸å·¥å…·ä½¿ç”¨çš„è‡ªç„¶å…³è”æ€§ï¼›ç®—æ³•å±‚é¢ï¼ŒåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨æ™ºèƒ½ä½“æ¨ç†ä¸­çš„æœ€ä¼˜é…æ–¹ä¸æ˜ï¼›æ¨ç†æ¨¡å¼å±‚é¢ï¼Œå…³äºè½®æ¬¡é¢„ç®—åˆ†é…ã€å“åº”é•¿åº¦ä¸å·¥å…·è°ƒç”¨æ•ˆç‡æƒè¡¡ç­‰é—®é¢˜ä»å¾…è§£å†³ã€‚è¿™äº›æŒ‘æˆ˜ä¿ƒä½¿ä½œè€…ä»æ•°æ®ã€ç®—æ³•ã€æ¨ç†æ¨¡å¼ä¸‰ä¸ªå…³é”®è§†è§’å¯¹æ™ºèƒ½ä½“æ¨ç†ä¸­çš„å¼ºåŒ–å­¦ä¹ å±•å¼€ç³»ç»Ÿç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ•°æ®å±‚é¢çš„ä¼˜åŒ–
ç°æœ‰æ•°æ®ç­–åˆ’å¸¸é‡‡ç”¨æ‹¼æ¥å¼åˆæˆè½¨è¿¹ï¼Œå¿½ç•¥æ¨ç†ä¸å·¥å…·ä½¿ç”¨çš„è‡ªç„¶è”ç³»ä¸”æ•°æ®å¤šæ ·æ€§ä¸è¶³ã€‚æœ¬æ–‡é€šè¿‡ä½¿ç”¨çœŸå®ç«¯åˆ°ç«¯å·¥å…·ä½¿ç”¨è½¨è¿¹æ›¿ä»£æ‹¼æ¥å¼åˆæˆè½¨è¿¹ï¼Œä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›æ›´å¼ºåˆå§‹åŒ–ï¼›åŒæ—¶æ„å»ºé«˜å¤šæ ·æ€§ã€æ¨¡å‹æ„ŸçŸ¥çš„æ•°æ®é›†ï¼Œç»´æŒæ¢ç´¢å¹¶æ˜¾è‘—æå‡å¼ºåŒ–å­¦ä¹ æ€§èƒ½ã€‚è¿˜è´¡çŒ®äº†é«˜è´¨é‡çœŸå®ç«¯åˆ°ç«¯æ™ºèƒ½ä½“SFTæ•°æ®é›†ä¸RLæ•°æ®é›†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç®—æ³•å±‚é¢çš„æ¢ç´¢å‹å¥½æŠ€æœ¯
åœ¨æ™ºèƒ½ä½“RLä¸­ï¼Œæ¢ç´¢å‹å¥½æŠ€æœ¯è‡³å…³é‡è¦ã€‚å¯¹æ¯”åŸºäºGRPOçš„RLç®—æ³•å‘ç°ï¼Œä¿å®ˆè£å‰ªä¸KLæ•£åº¦æƒ©ç½šè¿‡åº¦çº¦æŸè®­ç»ƒä¸­çš„æ¢ç´¢ã€‚ç ”ç©¶è¡¨æ˜å¦‚æ›´é«˜çš„è£å‰ªã€è¶…é•¿å¥–åŠ±å¡‘é€ ä»¥åŠç»´æŒè¶³å¤Ÿçš„ç­–ç•¥ç†µç­‰æŠ€æœ¯å¯æå‡è®­ç»ƒæ•ˆç‡ï¼Œå…¶ä¸­å¯¹è¾ƒå¼±æ¨¡å‹è€Œè¨€ï¼Œç»´æŒæ›´é«˜ç†µæ˜¯æå‡RLæ•ˆç‡çš„å…³é”®ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨ç†æ¨¡å¼çš„æ·±æ€ç†Ÿè™‘ç­–ç•¥
æ¢ç©¶å·¥å…·è°ƒç”¨æ¬¡æ•°ã€å“åº”é•¿åº¦ç­‰æ¨ç†æ¨¡å¼ç»„ä»¶ä¸æ€§èƒ½çš„å…³ç³»ï¼Œå‘ç°å·¥å…·è°ƒç”¨æ›´å°‘çš„æ·±æ€ç†Ÿè™‘ç­–ç•¥ä¼˜äºé¢‘ç¹å·¥å…·è°ƒç”¨æˆ–å†—é•¿è‡ªæˆ‘æ¨ç†ï¼Œæå‡äº†å·¥å…·æ•ˆç‡ä¸æœ€ç»ˆå‡†ç¡®ç‡ï¼Œå³æœ‰æ•ˆä¸”å‡†ç¡®çš„å·¥å…·è°ƒç”¨èå…¥æ¨¡å‹æ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹æ‰æ˜¯å…³é”®ï¼Œè€Œéè¿‡åº¦ä¾èµ–å¤–éƒ¨è°ƒç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AIME2024/AIME2025ã€GPQA - Diamondã€LiveCodeBench - v6ç­‰å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒéªŒè¯äº†ä¸Šè¿°è§è§£åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨æœ¬æ–‡æ–¹æ³•ï¼Œ4Bè§„æ¨¡çš„æ¨¡å‹åœ¨æ™ºèƒ½ä½“æ¨ç†æ€§èƒ½ä¸Šèƒ½è¶…è¶Š32Bè§„æ¨¡çš„æ¨¡å‹ï¼Œè¿˜æå‡ºäº†æ€§èƒ½è¾¾SOTAæ°´å¹³çš„åŸºçº¿æ¨¡å‹DemyAgent - 4Bã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ•°æ®ç­–åˆ’ä¸Šï¼Œé‡è§†çœŸå®åœºæ™¯ä¸‹çš„ç«¯åˆ°ç«¯è½¨è¿¹æ•°æ®ä»¥åŠé«˜å¤šæ ·æ€§ã€æ¨¡å‹æ„ŸçŸ¥æ•°æ®é›†çš„æ„å»ºï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ›´ä¼˜åˆå§‹åŒ–ä¸æ¢ç´¢åŠ¨åŠ›ï¼›ç®—æ³•ä¼˜åŒ–æ—¶ï¼Œå…³æ³¨æ¢ç´¢å‹å¥½å‹æŠ€æœ¯ï¼Œåˆç†åˆ©ç”¨è£å‰ªã€å¥–åŠ±å¡‘é€ ã€ç†µç»´æŒç­‰æ‰‹æ®µæå‡è®­ç»ƒæ•ˆç‡ï¼›æ¨ç†æ¨¡å¼è®¾è®¡ä¸­ï¼Œè¿½æ±‚å°‘è€Œç²¾çš„å·¥å…·è°ƒç”¨ç­–ç•¥ï¼Œå¹³è¡¡å·¥å…·ä½¿ç”¨ä¸è‡ªæˆ‘æ¨ç†ï¼Œæå‡æ•´ä½“æ€§èƒ½ã€‚è¿™äº›å®è·µä¸ºæœªæ¥æ™ºèƒ½ä½“RLç ”ç©¶å»ºç«‹äº†å®ç”¨åŸºçº¿ï¼Œä¹Ÿä¸ºæ¨¡å‹åœ¨ä¸åŒè§„æ¨¡ä¸‹å®ç°é«˜æ•ˆæ™ºèƒ½ä½“æ¨ç†æä¾›äº†æ€è·¯ã€‚

## qerl--beyond-efficiency----quantization-enhanced-reinforcement-learning-for-llms
### Abstract
We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
large language models (LLMs). While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL
while reducing memory overhead. Beyond efficiency, our findings show that
quantization noise increases policy entropy, enhancing exploration, and
enabling the discovery of better strategies during RL. To further optimize
exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is
the first framework to enable RL training of a 32B LLM on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | QeRLï¼šå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ä¸æ€§èƒ½åŒçªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»ŸRLåœ¨LLMsè®­ç»ƒä¸­å­˜åœ¨èµ„æºæ¶ˆè€—å¤§ã€è®­ç»ƒé€Ÿåº¦æ…¢ç­‰é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼ŒRLéœ€è¦å¤šæ¨¡å‹å¹¶å‘è¿è¡Œï¼Œå¯¹GPUå†…å­˜éœ€æ±‚æé«˜ï¼›å¦ä¸€æ–¹é¢ï¼Œè®­ç»ƒä¸­çš„å¤šé˜¶æ®µæµç¨‹ï¼ˆå¦‚rolloutã€å¥–åŠ±è®¡ç®—ç­‰ï¼‰è€—æ—¶ä¹…ï¼Œå°¤å…¶æ˜¯rollouté˜¶æ®µå› å¤æ‚ä»»åŠ¡çš„é•¿åºåˆ—é‡‡æ ·å’Œå¤„ç†æˆæœ¬é«˜æ˜‚ã€‚æ­¤å‰æ–¹æ³•ï¼ˆå¦‚LoRAã€QLoRAï¼‰è™½å°è¯•ä¼˜åŒ–ï¼Œä½†LoRAæœªè§£å†³rollouté€Ÿåº¦ç“¶é¢ˆï¼ŒQLoRAä¾èµ–çš„NF4é‡åŒ–åœ¨è®¡ç®—æ—¶éœ€é¢å¤–æ“ä½œæ‹–æ…¢é€Ÿåº¦ï¼Œä¸”ä¼ ç»Ÿé‡åŒ–å™ªå£°å¯¹RLåæœŸè®­ç»ƒæ— ç›Šå¤„ç­‰é—®é¢˜ä»å¾…è§£å†³ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å…¼é¡¾æ•ˆç‡ä¸æ€§èƒ½çš„RLæ¡†æ¶æ¥ä¼˜åŒ–LLMsçš„è®­ç»ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šNVFP4é‡åŒ–ä¸LoRAç»“åˆ  
QeRLé‡‡ç”¨NVFP4é‡åŒ–å¤„ç†LLMæƒé‡ï¼Œå¹¶åœ¨rolloutå’Œprefillingé˜¶æ®µèå…¥åŸºäºMarlinçš„æ–¹æ³•ã€‚NVFP4å€ŸåŠ©ç¡¬ä»¶æ”¯æŒå®ç°æ›´ç»†ç²’åº¦çš„ç¼©æ”¾è°ƒæ•´ï¼Œç»“åˆLoRAåœ¨å‡å°‘å†…å­˜å¼€é”€çš„åŒæ—¶ï¼Œè®©rollouté˜¶æ®µåŠ é€Ÿï¼Œä¸”é€šè¿‡LoRAå±‚æ”¯æŒæ¢¯åº¦åå‘ä¼ æ’­ï¼Œä¿è¯è®­ç»ƒç²¾åº¦ä¸å—æŸã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªé€‚åº”é‡åŒ–å™ªå£°ï¼ˆAQNï¼‰æœºåˆ¶  
å‘ç°é‡åŒ–å™ªå£°è‹¥æ§åˆ¶å¾—å½“ï¼Œèƒ½å¢åŠ ç­–ç•¥ç†µä»¥å¢å¼ºæ¢ç´¢æ€§ï¼ˆç±»ä¼¼RLä¸­å‚æ•°å™ªå£°çš„ä½œç”¨ï¼Œå¸®åŠ©æ¨¡å‹å‘ç°æ›´ä¼˜ç­–ç•¥ï¼‰ã€‚AQNåœ¨è®­ç»ƒæ—¶æŒ‰é€šé“æ³¨å…¥éšæœºå™ªå£°ï¼Œå¹¶é€šè¿‡æŒ‡æ•°è°ƒåº¦åŠ¨æ€è°ƒæ•´æ¢ç´¢å™ªå£°ï¼›è¿˜é‡‡ç”¨å™ªå£°å…±äº«ç­–ç•¥ï¼Œå°†å™ªå£°å‘é‡èå…¥å±‚å½’ä¸€åŒ–å±‚ï¼Œå®ç°é›¶å‚æ•°å¼€é”€çš„å™ªå£°æ³¨å…¥ï¼Œè§£å†³ä¼ ç»Ÿé™æ€é‡åŒ–å™ªå£°å¯¹RLåæœŸè®­ç»ƒä¸åˆ©çš„é—®é¢˜ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
- é€Ÿåº¦æå‡ï¼šåœ¨rollouté˜¶æ®µå®ç°è¶…1.5å€åŠ é€Ÿï¼›ç«¯åˆ°ç«¯è®­ç»ƒç›¸æ¯”QLoRAçº¦æœ‰1.8å€åŠ é€Ÿï¼Œä¸”æ˜¯é¦–ä¸ªæ”¯æŒåœ¨å•å¼ H100 80GB GPUä¸Šå¯¹32Bè§„æ¨¡LLMè¿›è¡ŒRLè®­ç»ƒçš„æ¡†æ¶ã€‚  
- æ€§èƒ½è¡¨ç°ï¼šåœ¨7Bæ¨¡å‹çš„æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆå¦‚GSM8Kè¾¾90.8%ã€MATH 500è¾¾77.4%ï¼‰ä¸­ï¼Œå¥–åŠ±å¢é•¿æ›´å¿«ã€æœ€ç»ˆç²¾åº¦æ›´é«˜ï¼Œè¶…è¿‡16ä½LoRAå’ŒQLoRAï¼Œä¸”èƒ½åŒ¹é…å…¨å‚æ•°å¾®è°ƒæ€§èƒ½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- é‡åŒ–ä¸é«˜æ•ˆå¾®è°ƒç»“åˆæ€è·¯ï¼šå°†å…ˆè¿›é‡åŒ–æŠ€æœ¯ï¼ˆå¦‚NVFP4ï¼‰å’ŒLoRAè¿™ç±»å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç»“åˆï¼Œä¸ºå¤§æ¨¡å‹è®­ç»ƒåœ¨å†…å­˜ä¸é€Ÿåº¦é—´æ‰¾åˆ°å¹³è¡¡æä¾›äº†å‚è€ƒã€‚  
- å™ªå£°åˆ©ç”¨æ–°è§†è§’ï¼šçªç ´ä¼ ç»Ÿå¯¹é‡åŒ–å™ªå£°â€œæœ‰å®³â€çš„è®¤çŸ¥ï¼Œå‘ç°å¹¶åˆ©ç”¨å…¶å¯¹RLæ¢ç´¢æ€§çš„å¢ç›Šï¼Œè®¾è®¡è‡ªé€‚åº”æœºåˆ¶è°ƒæ§å™ªå£°ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­åˆ©ç”¨å™ªå£°ä¼˜åŒ–è®­ç»ƒå¼€è¾Ÿæ–°æ–¹å‘ã€‚  
- å·¥ç¨‹è½åœ°å¯å‘ï¼šå®ç°å•å¡è®­ç»ƒå¤§å‚æ•°é‡æ¨¡å‹ï¼ˆ32Bï¼‰ï¼Œä¸ºå¤§æ¨¡å‹åœ¨ç¡¬ä»¶èµ„æºæœ‰é™åœºæ™¯ä¸‹çš„RLè®­ç»ƒæä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œæ¨åŠ¨å¤§æ¨¡å‹æ›´é«˜æ•ˆåœ°è½åœ°å¤æ‚æ¨ç†ä»»åŠ¡è®­ç»ƒã€‚

## adaptive-dual-reasoner--large-reasoning-models-can-think-efficiently-by-hybrid-reasoning
### Abstract
Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªé€‚åº”åŒæ¨ç†å™¨ï¼šè®©å¤§æ¨¡å‹æ¨ç†æ•ˆç‡ä¸æ€§èƒ½å…¼å¾—

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€é•¿æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å„ç±»æ¨ç†åœºæ™¯ä¸­å±•ç°å“è¶Šæ€§èƒ½ï¼Œâ€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜å´æ—¥ç›Šå‡¸æ˜¾â€”â€”æ¨¡å‹ä¼šç”Ÿæˆå†—ä½™æ¨ç†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬ä¸æ¨ç†å»¶è¿Ÿæ”€å‡ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚æç¤ºå·¥ç¨‹ã€é•¿åº¦é©±åŠ¨ä¼˜åŒ–ç­‰ï¼‰è¦ä¹ˆéš¾ä»¥é€‚é…ä¸åŒå¤æ‚åº¦å­é—®é¢˜ï¼Œè¦ä¹ˆä¾èµ–é™æ€ç­–ç•¥é™åˆ¶å¯¹éš¾é¢˜çš„æ¢ç´¢æ·±åº¦ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©æ¨¡å‹ä¾æ®ä¸Šä¸‹æ–‡å¤æ‚åº¦çµæ´»åˆ†é…â€œå¿«æ€è€ƒâ€ä¸â€œæ…¢æ€è€ƒâ€èµ„æºï¼Œå¹³è¡¡æ¨ç†æ€§èƒ½ä¸æ•ˆç‡ï¼Œæˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªé€‚åº”åŒæ¨ç†èŒƒå¼ï¼ˆADRï¼‰  
æå‡ºæ”¯æŒâ€œå¿«æ€è€ƒâ€ä¸â€œæ…¢æ€è€ƒâ€åŒæ¨¡å¼çš„æ¨ç†æ¡†æ¶ï¼Œè®©æ¨¡å‹èƒ½ä¾æ®æ¨ç†è¿‡ç¨‹ä¸­ä¸Šä¸‹æ–‡å¤æ‚åº¦**åŠ¨æ€åˆ‡æ¢**ä¸¤ç§æ¨¡å¼ï¼šç®€å•åœºæ™¯ç”¨å¿«æ€è€ƒå‹ç¼©æ¨ç†é•¿åº¦ï¼Œå¤æ‚ä¾èµ–åœºæ™¯ç”¨æ…¢æ€è€ƒä¿è¯æ¨ç†æ·±åº¦ï¼Œä¸ºçµæ´»åˆ†é…æ¨ç†èµ„æºå¥ å®šåŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªåŠ¨åŒ–æ··åˆæ¨ç†æ•°æ®æ„å»ºæµæ°´çº¿  
è®¾è®¡ä¸€å¥—å¯æ‰©å±•çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œä»å¼€æºæ¨ç†æ•°æ®é›†ä¸­åˆ†è§£ã€é‡å†™æ¨ç†è½¨è¿¹ï¼šå°†é«˜ç†µï¼ˆåæ˜ æ€è€ƒæ·±åº¦ï¼‰çš„æ¨ç†å•å…ƒæ ‡è®°ä¸ºâ€œhardâ€ä¿ç•™å®Œæ•´æ¨ç†ï¼Œä½ç†µå•å…ƒæ ‡è®°ä¸ºâ€œeasyâ€ç”¨å‹ç¼©æ ¼å¼å¤„ç†ï¼Œå†é€šè¿‡ç‰¹æ®Štokenå°è£…æˆ`<easy>...</easy><hard>......</hard>`ç­‰æ··åˆæ¨ç†æ ¼å¼ï¼Œè®©ç°æœ‰LRMsèƒ½æ— ç¼æ¥å…¥æ–°èŒƒå¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†µå¼•å¯¼çš„æ··åˆç­–ç•¥ä¼˜åŒ–ï¼ˆEHPOï¼‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶  
ä¸ºä¼˜åŒ–æ¨ç†â€œæŠ•å…¥äº§å‡ºæ¯”â€ï¼Œæå‡ºEHPOæ¡†æ¶ï¼š  
- **å¥–åŠ±è®¾è®¡**ï¼šèåˆæ ¼å¼åˆè§„ã€å‡†ç¡®æ€§ã€å•å…ƒè¯­ä¹‰ã€æ¨¡å¼æ§åˆ¶4ç±»å¥–åŠ±ä¿¡å·ï¼Œæ—¢ä¿è¯æ¨ç†ç»“æ„ä¸æ­£ç¡®æ€§ï¼Œåˆå¼•å¯¼æ¨¡å‹åŒºåˆ†åŒæ¨¡å¼ï¼ˆå¦‚â€œeasyâ€ä¸å«åæ€å…³é”®è¯ã€â€œhardâ€å«åæ€å…³é”®è¯ï¼‰ï¼Œè¿˜èƒ½æ ¹æ®ä»»åŠ¡éš¾åº¦åå¥½å¿«/æ…¢æ¨¡å¼ï¼›  
- **ç†µå¼•å¯¼åŠ¨æ€å±•å¼€ç­–ç•¥**ï¼šé€šè¿‡åˆ†ææ¨ç†å•å…ƒé¦–å°¾ç†µå€¼ï¼ˆæ˜“â†’éš¾æ¨¡å¼åˆ‡æ¢æ—¶ç†µæ›´é«˜ï¼Œéœ€æ·±åº¦æ¢ç´¢ï¼‰ï¼Œè®¾è®¡åŸºäºç†µå·®çš„åˆ†æ”¯æ¦‚ç‡ï¼ˆSP=Î±+âˆ†Hï¼‰ï¼Œè®©æ¨¡å‹åœ¨é«˜ç†µå•å…ƒå¤„åŠ¨æ€æ‰©å±•æ¨ç†è·¯å¾„ï¼Œå¹³è¡¡æ¢ç´¢ä¸æ•ˆç‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒADRåœ¨ä¸»æµæ–¹æ³•é‡Œå®ç°äº†æ¨ç†æ€§èƒ½ä¸æ•ˆç‡çš„æœ‰æ•ˆå¹³è¡¡ï¼šæ€§èƒ½æå‡æœ€é«˜è¾¾6.1%ï¼ŒåŒæ—¶æ¨ç†è¾“å‡ºé•¿åº¦å‡å°‘49.5% - 59.3%ï¼ŒéªŒè¯äº†â€œå¿«/æ…¢æ€è€ƒåŠ¨æ€åˆ‡æ¢â€åœ¨é™æœ¬ææ•ˆä¸Šçš„æ˜¾è‘—ä½œç”¨ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **åŒæ¨¡å¼åŠ¨æ€æ¨ç†**ï¼šä¸ºå¤§æ¨¡å‹æ¨ç†æ•ˆç‡ä¼˜åŒ–æä¾›æ–°æ€è·¯â€”â€”ä¸å†â€œä¸€åˆ€åˆ‡â€å‹ç¼©/å»¶é•¿æ¨ç†ï¼Œè€Œæ˜¯æŒ‰å­é—®é¢˜å¤æ‚åº¦åŠ¨æ€åˆ†é…èµ„æºï¼›  
2. **æ•°æ®æ„å»ºè‡ªåŠ¨åŒ–**ï¼šé€šè¿‡â€œåˆ†è§£-æ ‡è®°-é‡å†™â€æµæ°´çº¿ï¼Œä½æˆæœ¬å°†ç°æœ‰å•æ¨¡æ€æ¨ç†æ•°æ®è½¬åŒ–ä¸ºæ··åˆæ¨ç†æ•°æ®ï¼Œé™ä½æ–°èŒƒå¼è½åœ°é—¨æ§›ï¼›  
3. **ç†µ+å¼ºåŒ–å­¦ä¹ çš„ç»†ç²’åº¦æ§åˆ¶**ï¼šç”¨ç†µé‡åŒ–æ¨ç†å•å…ƒå¤æ‚åº¦ï¼Œç»“åˆRLå®ç°æ¨ç†è·¯å¾„çš„åŠ¨æ€æ¢ç´¢ä¸éš¾åº¦æ„ŸçŸ¥å¥–åŠ±ï¼Œä¸ºå¤æ‚ä»»åŠ¡ä¸‹çš„æ¨¡å‹è¡Œä¸ºè°ƒæ§æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯èŒƒå¼ã€‚  


è¿™ç¯‡è®ºæ–‡ç›´å‡»é•¿æ¨ç†æ¨¡å‹â€œè¿‡åº¦æ€è€ƒâ€ç—›ç‚¹ï¼Œä»èŒƒå¼ã€æ•°æ®ã€è®­ç»ƒæ¡†æ¶ä¸‰å±‚é¢åˆ›æ–°ï¼Œä¸ºå¤§æ¨¡å‹é«˜æ•ˆæ¨ç†å¼€è¾Ÿäº†ä¸€æ¡å…¼å…·çµæ´»æ€§ä¸å¯æ“ä½œæ€§çš„è·¯å¾„ï¼Œå€¼å¾—å…³æ³¨æ¨ç†æ•ˆç‡ä¼˜åŒ–çš„ç ”ç©¶è€…ä¸å·¥ç¨‹å¸ˆæ·±å…¥å‚è€ƒï½

## detecting-data-contamination-from-reinforcement-learning-post-training-for-large-language-models
### Abstract
Data contamination poses a significant threat to the reliable evaluation of
Large Language Models (LLMs). This issue arises when benchmark samples may
inadvertently appear in training sets, compromising the validity of reported
performance. While detection methods have been developed for the pre-training
and Supervised Fine-Tuning stages, a critical research gap exists for the
increasingly significant phase of Reinforcement Learning (RL) post-training. As
RL post-training becomes pivotal for advancing LLM reasoning, the absence of
specialized contamination detection methods in this paradigm presents a
critical vulnerability. To address this, we conduct the first systematic study
of data detection within RL post-training scenario and propose Self-Critique.
Our method is motivated by a key observation: after RL phase, the output
entropy distribution of LLMs tends to collapse into highly specific and sparse
modes. Self-Critique probes for the underlying policy collapse, i.e., the
model's convergence to a narrow reasoning path, which causes this entropy
reduction. To facilitate this research, we also introduce RL-MIA, a benchmark
constructed to simulate this specific contamination scenario. Extensive
experiments show that Self-Critique significantly outperforms baseline methods
across multiple models and contamination tasks, achieving an AUC improvement of
up to 30%. Whereas existing methods are close to a random guess for RL-phase
contamination, our method makes detection possible.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ åè®­ç»ƒé˜¶æ®µçš„æ•°æ®æ±¡æŸ“æ£€æµ‹æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°çš„å¯é æ€§æ·±å—æ•°æ®æ±¡æŸ“å¨èƒï¼Œå½“åŸºå‡†æµ‹è¯•æ ·æœ¬æ„å¤–å‡ºç°åœ¨è®­ç»ƒé›†æ—¶ï¼Œæ¨¡å‹æŠ¥å‘Šçš„æ€§èƒ½æœ‰æ•ˆæ€§ä¼šå—æŸã€‚æ­¤å‰æ£€æµ‹æ–¹æ³•å¤šèšç„¦äºé¢„è®­ç»ƒå’Œæœ‰ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œè€Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒä½œä¸ºæå‡LLMæ¨ç†èƒ½åŠ›çš„å…³é”®é˜¶æ®µï¼Œå´ç¼ºä¹ä¸“é—¨çš„æ±¡æŸ“æ£€æµ‹æ–¹æ³•ã€‚RLåè®­ç»ƒåŸºäºå¥–åŠ±æœ€å¤§åŒ–åŸåˆ™ï¼Œä¸é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒçš„ä¼¼ç„¶æœ€å¤§åŒ–èŒƒå¼ä¸åŒï¼Œä¼ ç»Ÿä¾èµ–ä¼¼ç„¶ä¿¡å·çš„æ£€æµ‹æ–¹æ³•åœ¨æ­¤å¤±æ•ˆï¼Œå› æ­¤é’ˆå¯¹RLåè®­ç»ƒé˜¶æ®µæ•°æ®æ±¡æŸ“æ£€æµ‹çš„ç ”ç©¶è¿«åœ¨çœ‰ç«ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡ç³»ç»Ÿç ”ç©¶RLåè®­ç»ƒé˜¶æ®µæ•°æ®æ±¡æŸ“æ£€æµ‹  
è®ºæ–‡å¡«è¡¥äº†RLåè®­ç»ƒé˜¶æ®µæ•°æ®æ±¡æŸ“æ£€æµ‹çš„ç ”ç©¶ç©ºç™½ï¼ŒæŒ‡å‡ºè¯¥é˜¶æ®µå› è®­ç»ƒç›®æ ‡ä»ä¼¼ç„¶æœ€å¤§åŒ–è½¬å‘å¥–åŠ±æœ€å¤§åŒ–ï¼Œä¼ ç»Ÿæ£€æµ‹æ–¹æ³•å¤±æ•ˆï¼Œè€Œè¿™ä¸€é˜¶æ®µå¯¹LLMæ¨ç†èƒ½åŠ›æå‡è‡³å…³é‡è¦ï¼Œå…¶æ½œåœ¨æ±¡æŸ“é£é™©éœ€é‡è§†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSelf - Critiqueæ£€æµ‹æ–¹æ³•  
åŸºäºRLåè®­ç»ƒä¸­æ¨¡å‹è¾“å‡ºç†µåˆ†å¸ƒä¼šåç¼©åˆ°ç‰¹å®šç¨€ç–æ¨¡å¼è¿™ä¸€è§‚å¯Ÿï¼ŒSelf - Critiqueé€šè¿‡æ¢æµ‹â€œç­–ç•¥åç¼©â€ï¼ˆæ¨¡å‹æ”¶æ•›åˆ°ç‹­çª„æ¨ç†è·¯å¾„å¯¼è‡´ç†µé™ä½ï¼‰æ¥æ£€æµ‹æ±¡æŸ“ã€‚å…·ä½“æ˜¯è®©æ¨¡å‹å¯¹åŒä¸€é—®é¢˜ç”Ÿæˆä¸¤ä¸ªå“åº”ï¼Œè‹¥å“åº”åœ¨ç†µç©ºé—´ç›¸ä¼¼åº¦é«˜åˆ™æ ‡è®°ä¸ºæ±¡æŸ“æ ·æœ¬ï¼Œé€šè¿‡ä¸»åŠ¨æ¢æµ‹æš´éœ²æ±¡æŸ“ä¸‹æ¨¡å‹æ¨ç†è·¯å¾„éš¾ä»¥å˜åŒ–çš„ç‰¹æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºRL - MIAåŸºå‡†æµ‹è¯•é›†  
ä¸ºæ¨¡æ‹ŸRLé˜¶æ®µç‰¹å®šæ±¡æŸ“åœºæ™¯ä»¥è¯„ä¼°æ£€æµ‹æ–¹æ³•ï¼Œè®ºæ–‡æ„å»ºäº†RL - MIAåŸºå‡†ã€‚è¯¥åŸºå‡†é’ˆå¯¹æ•°å­¦å’Œé€»è¾‘ä»»åŠ¡è®¾è®¡ï¼Œèƒ½ä¸ºRLåè®­ç»ƒé˜¶æ®µæ•°æ®æ±¡æŸ“æ£€æµ‹ç ”ç©¶æä¾›å¯é è¯„ä¼°ç¯å¢ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ¨¡å‹å’Œæ±¡æŸ“ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSelf - Critiqueæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•å¯¹RLé˜¶æ®µæ±¡æŸ“æ£€æµ‹æ¥è¿‘éšæœºçŒœæµ‹ï¼Œè€ŒSelf - Critiqueèƒ½å®ç°æœ‰æ•ˆæ£€æµ‹ï¼ŒAUCæå‡æœ€é«˜å¯è¾¾30%ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç ”ç©¶è§†è§’åˆ›æ–°ï¼šå…³æ³¨è¢«å¿½è§†çš„RLåè®­ç»ƒé˜¶æ®µæ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œä¸ºLLMå…¨æµç¨‹è¯„ä¼°å®Œæ•´æ€§æä¾›æ€è·¯ï¼Œåç»­ç ”ç©¶å¯æ‹“å±•ä¸åŒè®­ç»ƒé˜¶æ®µç»“åˆçš„æ±¡æŸ“æ£€æµ‹ã€‚  
2. æ–¹æ³•è®¾è®¡æ€è·¯ï¼šä»RLè®­ç»ƒå†…åœ¨ç‰¹æ€§ï¼ˆç­–ç•¥åç¼©ã€ç†µå˜åŒ–ï¼‰å‡ºå‘è®¾è®¡æ£€æµ‹æ–¹æ³•ï¼Œå¯ç¤ºé’ˆå¯¹ä¸åŒè®­ç»ƒèŒƒå¼ç‹¬ç‰¹æ€§è´¨å¼€å‘ä¸“å±æ£€æµ‹æ‰‹æ®µã€‚  
3. åŸºå‡†æ„å»ºï¼šRL - MIAä¸ºç‰¹å®šåœºæ™¯ç ”ç©¶æä¾›äº†æœ‰æ•ˆè¯„ä¼°å·¥å…·ï¼Œç±»ä¼¼åœ°ï¼Œå¯é’ˆå¯¹å…¶ä»–LLMè®­ç»ƒæ¨ç†ç¯èŠ‚æ„å»ºä¸“ç”¨åŸºå‡†ä»¥æ¨åŠ¨ç›¸å…³ç ”ç©¶ã€‚

## convergence-theorems-for-entropy-regularized-and-distributional-reinforcement-learning
### Abstract
In the pursuit of finding an optimal policy, reinforcement learning (RL)
methods generally ignore the properties of learned policies apart from their
expected return. Thus, even when successful, it is difficult to characterize
which policies will be learned and what they will do. In this work, we present
a theoretical framework for policy optimization that guarantees convergence to
a particular optimal policy, via vanishing entropy regularization and a
temperature decoupling gambit. Our approach realizes an interpretable,
diversity-preserving optimal policy as the regularization temperature vanishes
and ensures the convergence of policy derived objects--value functions and
return distributions. In a particular instance of our method, for example, the
realized policy samples all optimal actions uniformly. Leveraging our
temperature decoupling gambit, we present an algorithm that estimates, to
arbitrary accuracy, the return distribution associated to its interpretable,
diversity-preserving optimal policy.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç†µæ­£åˆ™åŒ–ä¸åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›æ€§å®šç†ï¼šè®©æœ€ä¼˜ç­–ç•¥â€œå¯è§£é‡Šä¸”ä¿å¤šæ ·æ€§â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œä¸ºæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨ç­–ç•¥çš„æœŸæœ›å›æŠ¥ï¼Œå´å¿½ç•¥äº†æ‰€å­¦ç­–ç•¥çš„å…¶ä»–å±æ€§ã€‚è€Œåœ¨ä¸€èˆ¬çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰é‡Œï¼Œæœ€ä¼˜ç­–ç•¥å¹¶ä¸å”¯ä¸€ï¼Œè¿™å°±å¯¼è‡´å¾ˆéš¾åˆ»ç”»æœ€ç»ˆå­¦åˆ°çš„æ˜¯å“ªç±»ç­–ç•¥ã€ç­–ç•¥ä¼šæœ‰æ€æ ·çš„è¡Œä¸ºã€‚ç†µæ­£åˆ™åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆERLï¼‰è™½èƒ½é€šè¿‡æ­£åˆ™åŒ–è®©æœ€ä¼˜ç­–ç•¥å”¯ä¸€ï¼Œä½†å½“æ­£åˆ™åŒ–æ¸©åº¦ï¼ˆÏ„ï¼‰è¶‹è¿‘äº0æ—¶ï¼ŒÏ„ - æœ€ä¼˜ç›¸å…³é‡ï¼ˆå¦‚ç­–ç•¥ã€ä»·å€¼å‡½æ•°ç­‰ï¼‰çš„æ¼”åŒ–è§„å¾‹åœ¨éè¡¨æ ¼å‹MDPä¸­å¹¶ä¸æ˜æ™°ï¼Œåˆå›åˆ°äº†â€œç­–ç•¥å±æ€§æ¨¡ç³Šâ€çš„å›°å¢ƒã€‚æ­¤å¤–ï¼Œåˆ†å¸ƒå¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä¸­ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶åœºæ™¯ä¸‹ä¹Ÿéš¾ä»¥å¾—åˆ°æ”¶æ•›çš„è¿­ä»£ç»“æœï¼Œå¯¹å›æŠ¥åˆ†å¸ƒçš„ç†è§£å’Œå‡†ç¡®ä¼°è®¡å­˜åœ¨ä¸è¶³ã€‚äºæ˜¯ï¼Œæœ¬æ–‡å¸Œæœ›æ„å»ºç†è®ºæ¡†æ¶ï¼Œè§£å†³ç­–ç•¥åŠè¡ç”Ÿå¯¹è±¡åœ¨æ¸©åº¦è¶‹è¿‘0æ—¶çš„æ”¶æ•›æ€§ï¼ŒåŒæ—¶è®©æœ€ä¼˜ç­–ç•¥æ›´å…·å¯è§£é‡Šæ€§ä¸å¤šæ ·æ€§ï¼Œå¹¶ä¸ºå›æŠ¥åˆ†å¸ƒä¼°è®¡æä¾›æ–°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ¸©åº¦è§£è€¦ç­–ç•¥ï¼ˆtemperature decoupling gambitï¼‰  
ä¸ºä¿è¯æ¸©åº¦è¶‹è¿‘0æ—¶ç­–ç•¥åŠå…¶è¡ç”Ÿå¯¹è±¡çš„æ”¶æ•›æ€§ï¼Œè®¾è®¡äº†æ¸©åº¦è§£è€¦ç­–ç•¥ã€‚ç±»ä¼¼å›½é™…è±¡æ£‹ä¸­â€œå¼ƒå°åˆ©è°‹å¤§åŠ¿â€ï¼Œä¸ºäº†è®©Ï„â†’0æ—¶æ”¶æ•›åˆ°RLæœ€ä¼˜æ€§ï¼Œåœ¨Ï„ - ERLç›®æ ‡ä¸‹é‡‡ç”¨æ˜æ˜¾æ¬¡ä¼˜çš„ç­–ç•¥ã€‚å…·ä½“æ˜¯åœ¨ç›®æ ‡æ­£åˆ™åŒ–æ¸©åº¦ä¸‹ä¼°è®¡åŠ¨ä½œä»·å€¼ï¼Œå´ç”¨æ”¾å¤§æ¸©åº¦çš„ç­–ç•¥å»æ‰§è¡ŒåŠ¨ä½œï¼Œä»¥æ­¤ä¿éšœæ”¶æ•›ã€‚é€šè¿‡è¯¥ç­–ç•¥å¾—åˆ°çš„æé™ç­–ç•¥æ˜¯å¯¹å‚è€ƒç­–ç•¥çš„â€œæ”¹é€ â€â€”â€”èƒ½è¿‡æ»¤æ‰æ¬¡ä¼˜åŠ¨ä½œï¼Œä¸”ç›¸æ¯”å…¶ä»–æ–¹å¼å¾—åˆ°çš„æé™ç­–ç•¥ï¼Œåœ¨çŠ¶æ€å±‚é¢åŠ¨ä½œå¤šæ ·æ€§ä¸Šä¿ç•™å¾—æ›´å……åˆ†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ»ç”»å‚è€ƒæœ€ä¼˜æ€§ä¸æ–°Bellmanå‹æ–¹ç¨‹  
å°†æ¸©åº¦è§£è€¦ç­–ç•¥å¾—åˆ°çš„æé™ç­–ç•¥å®šä¹‰ä¸ºä¸€ç§â€œå‚è€ƒæœ€ä¼˜â€ï¼ˆreference - optimalityï¼‰ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªæ–°çš„ç±»Bellmanæ–¹ç¨‹æ¥åˆ»ç”»ã€‚è¿™ä¸ªæ–¹ç¨‹çš„å”¯ä¸€ä¸åŠ¨ç‚¹èƒ½åœ¨ä¸€èˆ¬æƒ…å†µä¸‹å¯¹Ï„ - æœ€ä¼˜ç­–ç•¥çš„ï¼ˆRLï¼‰æ€§èƒ½å½¢æˆä¸Šç•Œï¼Œä»ç†è®ºå±‚é¢é”šå®šäº†æé™ç­–ç•¥çš„æ€§èƒ½ä¸æ€§è´¨ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºERLæ”¶æ•›æ€§çš„å›æŠ¥åˆ†å¸ƒä¼°è®¡ç®—æ³•  
å€ŸåŠ©ERLä¸­ç­–ç•¥çš„æ”¶æ•›ç»“æœï¼Œæå‡ºäº†é¦–ä¸ªèƒ½ç²¾ç¡®ä¼°è®¡â€œå‚è€ƒæœ€ä¼˜å›æŠ¥åˆ†å¸ƒâ€çš„ç®—æ³•ã€‚è¯¥å›æŠ¥åˆ†å¸ƒä¸æ¸©åº¦è§£è€¦ç­–ç•¥å¾—åˆ°çš„â€œå¯è§£é‡Šã€ä¿å¤šæ ·æ€§â€æœ€ä¼˜ç­–ç•¥ç›¸å…³è”ï¼Œè§£å†³äº†DRLåœ¨æ§åˆ¶åœºæ™¯ä¸‹å›æŠ¥åˆ†å¸ƒä¼°è®¡æ”¶æ•›æ€§ä¸ä½³çš„é—®é¢˜ï¼Œä¸ºå®‰å…¨å…³é”®ç­‰åœºæ™¯ä¸­ç†è§£å›æŠ¥åˆ†å¸ƒæä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡æœªæåŠä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®éªŒéƒ¨åˆ†ï¼ˆå¦‚åœ¨æŸç¯å¢ƒä¸‹å¯¹æ¯”ç®—æ³•æ€§èƒ½ç­‰ï¼‰ï¼Œä¸»è¦èšç„¦äºç†è®ºæ¨å¯¼ä¸åˆ†æï¼Œé€šè¿‡ä¸¥æ ¼çš„æ•°å­¦è¯æ˜ï¼Œé˜è¿°äº†æ¸©åº¦è§£è€¦ç­–ç•¥ä¸‹ç­–ç•¥ã€ä»·å€¼å‡½æ•°ã€å›æŠ¥åˆ†å¸ƒç­‰çš„æ”¶æ•›æ€§ï¼Œä»¥åŠå‚è€ƒæœ€ä¼˜æ€§çš„åˆ»ç”»ã€å›æŠ¥åˆ†å¸ƒä¼°è®¡ç®—æ³•çš„åˆç†æ€§ç­‰ç†è®ºç»“æœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç†è®ºæ¡†æ¶æ„å»ºæ€è·¯ï¼šé¢å¯¹RLä¸­æœ€ä¼˜ç­–ç•¥ä¸å”¯ä¸€ã€å±æ€§æ¨¡ç³Šçš„é—®é¢˜ï¼Œé€šè¿‡ç†µæ­£åˆ™åŒ– + æ¸©åº¦è§£è€¦çš„æ€è·¯ï¼Œä¸ºçº¦æŸå’Œå¼•å¯¼æœ€ä¼˜ç­–ç•¥çš„å±æ€§ï¼ˆå¦‚å¤šæ ·æ€§ã€å¯è§£é‡Šæ€§ï¼‰æä¾›äº†ç†è®ºèŒƒå¼ï¼Œåç»­ç ”ç©¶åœ¨å¤„ç†ç­–ç•¥å”¯ä¸€æ€§ä¸å±æ€§æ§åˆ¶æ—¶å¯å€Ÿé‰´è¿™ç§â€œæ­£åˆ™åŒ– + é’ˆå¯¹æ€§ç­–ç•¥è®¾è®¡â€çš„æ€è·¯ã€‚  
2. è·¨é¢†åŸŸç»“åˆå¯å‘ï¼šå°†ERLä¸DRLç»“åˆï¼Œåˆ©ç”¨ERLçš„æ”¶æ•›æ€§æˆæœè§£å†³DRLå›æŠ¥åˆ†å¸ƒä¼°è®¡éš¾é¢˜ï¼Œä½“ç°äº†ä¸åŒRLåˆ†æ”¯é—´æŠ€æœ¯è¿ç§»ä¸èåˆçš„ä»·å€¼ï¼Œä¸ºè§£å†³å•ä¸€é¢†åŸŸç“¶é¢ˆæä¾›äº†è·¨é¢†åŸŸæ€è€ƒæ–¹å‘ã€‚  
3. æ–°Bellmanå‹æ–¹ç¨‹çš„å¯ç¤ºï¼šåœ¨å®šä¹‰å‚è€ƒæœ€ä¼˜æ€§æ—¶æå‡ºæ–°çš„ç±»Bellmanæ–¹ç¨‹ï¼Œå±•ç¤ºäº†ä»é—®é¢˜éœ€æ±‚å‡ºå‘ï¼Œæ„å»ºé€‚é…ç†è®ºå·¥å…·ï¼ˆæ–¹ç¨‹ï¼‰æ¥åˆ»ç”»æ–°æ€§è´¨ï¼ˆå‚è€ƒæœ€ä¼˜ï¼‰çš„ç ”ç©¶æ–¹æ³•ï¼Œä¸ºåç»­æ‹“å±•RLç†è®ºè¾¹ç•Œæä¾›äº†æ–¹æ³•è®ºå‚è€ƒã€‚

## enhancing-reasoning-for-diffusion-llms-via-distribution-matching-policy-optimization
### Abstract
Diffusion large language models (dLLMs) are promising alternatives to
autoregressive large language models (AR-LLMs), as they potentially allow
higher inference throughput. Reinforcement learning (RL) is a crucial component
for dLLMs to achieve comparable performance with AR-LLMs on important tasks,
such as reasoning. However, RL algorithms that are well-suited for dLLMs'
unique characteristics have yet to be developed. This paper proposes
Distribution Matching Policy Optimization (DMPO), a principled and
theoretically grounded RL fine-tuning method specifically designed to enhance
the reasoning capabilities of dLLMs by matching the dLLM policy distribution to
the optimal, reward-tilted one through cross-entropy optimization. We identify
a key challenge in the implementation with a small training batch size and
propose several effective solutions through a novel weight baseline subtraction
technique. DMPO exhibits superior performance on multiple reasoning benchmarks
without supervised fine-tuning, with an accuracy improvement of up to $42.9\%$
over previously SOTA baselines and $55.8\%$ over the base model, underscoring
the effectiveness of the distribution matching framework. Our code is available
at https://github.com/yuchen-zhu-zyc/DMPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸ºæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›èµ‹èƒ½ï¼šåˆ†å¸ƒåŒ¹é…ç­–ç•¥ä¼˜åŒ–ï¼ˆDMPOï¼‰

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ï¼ˆAR - LLMsï¼‰è™½åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†å› ç”Ÿæˆé¡ºåºå›ºå®šï¼ˆä»å·¦åˆ°å³ï¼‰ï¼Œæ¨ç†æˆæœ¬é«˜æ˜‚ï¼Œå¤§è§„æ¨¡éƒ¨ç½²å—é™ã€‚æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½ä»¥ä»»æ„é¡ºåºç”Ÿæˆåºåˆ—ï¼Œæœ‰æ›´é«˜æ¨ç†ååé‡æ½œåŠ›ï¼Œç„¶è€Œå¦‚ä½•å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨AR - LLMsä¸Šçš„æˆåŠŸç»éªŒè¿ç§»åˆ°dLLMsï¼Œæå‡å…¶æ¨ç†èƒ½åŠ›ï¼Œä»æ˜¯å¾…è§£éš¾é¢˜ã€‚ç°æœ‰RLç®—æ³•é€‚é…dLLMså­˜åœ¨æŒ‘æˆ˜ï¼šdLLMsåŒå‘ç‰¹æ€§ä½¿ç”Ÿæˆåºåˆ—å¯¹æ•°æ¦‚ç‡ä¼°è®¡æ›´æ˜‚è´µï¼Œéš¾ä»¥ç›´æ¥é€‚é…AR - LLMsçš„RLç®—æ³•ï¼›ä¸”ç°æœ‰å¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„RLæ¡†æ¶è¿‡åº¦èšç„¦å¥–åŠ±æœ€å¤§åŒ–ï¼Œæœªå……åˆ†åˆ©ç”¨dLLMséšæœºé¡ºåºç”Ÿæˆå¸¦æ¥çš„å¤šæ ·æ€§ä¼˜åŠ¿ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºé¢å‘dLLMsçš„å…¨æ–°RLå­¦ä¹ æ¡†æ¶ï¼Œä»¥åˆ†å¸ƒåŒ¹é…æ›¿ä»£å¥–åŠ±æœ€å¤§åŒ–  
ä¼ ç»ŸRLæ¡†æ¶èšç„¦å¥–åŠ±æœ€å¤§åŒ–ï¼Œè€Œè¯¥æ¡†æ¶åŸºäºéšæœºæœ€ä¼˜æ§åˆ¶ï¼ˆSOCï¼‰ç†è®ºï¼Œç›®æ ‡æ˜¯åŒ¹é…æ•´ä¸ªâ€œå¥–åŠ±å€¾æ–œâ€çš„ç­–ç•¥åˆ†å¸ƒï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒä¸­æ¢ç´¢å¤šæ ·ã€é«˜è´¨é‡çš„æ¨ç†è·¯å¾„ä¸å“åº”ï¼Œé¿å…è¿‡åº¦å…³æ³¨ç»å¯¹å¥–åŠ±å€¼å’Œæ¨¡å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡åˆ†å¸ƒåŒ¹é…ç­–ç•¥ä¼˜åŒ–ï¼ˆDMPOï¼‰æ–¹æ³•  
DMPOå€ŸåŠ©é‡è¦æ€§é‡‡æ ·å’ŒåŠ æƒå»å™ªäº¤å‰ç†µï¼ˆWDCEï¼‰æŸå¤±å®ç°ã€‚WDCEæ˜¯ä»…ä¾èµ–å¹²å‡€æ ·æœ¬å’ŒdLLMsç‰¹æœ‰çš„å»‰ä»·å‰å‘åŠ å™ªè¿‡ç¨‹çš„å‰å‘ç›®æ ‡ï¼Œä¸”è®­ç»ƒé‡‡ç”¨ç¦»ç­–ç•¥æ–¹å¼ï¼Œå¯åˆ©ç”¨é‡æ”¾ç¼“å†²åŒºæå‡æ ·æœ¬æ•ˆç‡ï¼Œè¿˜å‡å°‘äº†å¯¹rolloutè½¨è¿¹çš„ä¾èµ–ï¼Œç»“åˆå¿«é€Ÿæ¨ç†æŠ€æœ¯æ—¶æœ‰æœ›æ›´å¿«åŠ é€Ÿã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§£å†³WDCEåœ¨å°æ‰¹é‡è®­ç»ƒä¸‹çš„æŒ‘æˆ˜ï¼Œæå‡ºæƒé‡åŸºçº¿å‡æ³•ç­‰æŠ€æœ¯  
å‘ç°å°è®­ç»ƒæ‰¹é‡ä¸‹WDCEé¢ä¸´ç‰¹æ®ŠæŒ‘æˆ˜åï¼Œæå‡ºæƒé‡åŸºçº¿å‡æ³•å’ŒåŠ æƒç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ä¸¤ç§æŠ€æœ¯æ¥åº”å¯¹ï¼Œä¿éšœæ–¹æ³•åœ¨å°æ‰¹é‡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ— ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æƒ…å†µä¸‹ï¼ŒDMPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚ç›¸æ¯”ä¹‹å‰çš„SOTAåŸºçº¿ï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾42.9%ï¼›ç›¸æ¯”åŸºç¡€æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡è¾¾55.8%ï¼Œåœ¨åŒå‘dLLMsä¸­è¡¨ç°é¢†å…ˆï¼Œæœ‰åŠ›è¯æ˜äº†åˆ†å¸ƒåŒ¹é…æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šå°†åˆ†å¸ƒåŒ¹é…å¼•å…¥dLLMsçš„RLè®­ç»ƒï¼Œè·³å‡ºä¼ ç»Ÿå¥–åŠ±æœ€å¤§åŒ–æ€ç»´ï¼Œä¸ºæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›æ–°èŒƒå¼ï¼Œå¯å‘åç»­é’ˆå¯¹æ¨¡å‹ç‰¹æ€§è®¾è®¡RLç®—æ³•æ—¶å¯å…³æ³¨åˆ†å¸ƒå±‚é¢çš„ä¼˜åŒ–ã€‚
2. æŠ€æœ¯è½åœ°ï¼šé’ˆå¯¹æ–¹æ³•å®ç°ä¸­é‡åˆ°çš„å°æ‰¹é‡è®­ç»ƒæŒ‘æˆ˜ï¼Œæå‡ºå…·ä½“æŠ€æœ¯æ‰‹æ®µè§£å†³ï¼Œä¸ºç±»ä¼¼åœºæ™¯ä¸‹ç®—æ³•ä¼˜åŒ–æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼Œæ¯”å¦‚å¤„ç†è®­ç»ƒæ•°æ®é‡æœ‰é™ç­‰æƒ…å†µæ—¶çš„ä¼˜åŒ–æ€è·¯ã€‚
3. æ€§èƒ½éªŒè¯ï¼šåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸ŠéªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¯æ˜æ–°æ¡†æ¶å’Œç®—æ³•åœ¨æå‡dLLMsæ¨ç†èƒ½åŠ›ä¸Šçš„æ½œåŠ›ï¼Œä¸ºdLLMsåç»­å‘å±•å’Œåº”ç”¨æä¾›äº†æœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘æ¡ˆä¾‹ã€‚

## ttrv--test-time-reinforcement-learning-for-vision-language-models
### Abstract
Existing methods for extracting reward signals in Reinforcement Learning
typically rely on labeled data and dedicated training splits, a setup that
contrasts with how humans learn directly from their environment. In this work,
we propose TTRV to enhance vision language understanding by adapting the model
on the fly at inference time, without the need for any labeled data.
Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework
by designing rewards based on the frequency of the base model's output, while
inferring on each test sample multiple times. Further, we also propose to
control the diversity of the model's output by simultaneously rewarding the
model for obtaining low entropy of the output empirical distribution. Our
approach delivers consistent gains across both object recognition and visual
question answering (VQA), with improvements of up to 52.4% and 29.8%,
respectively, and average boosts of 24.6% and 10.0% across 16
datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B
surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining
highly competitive on VQA, demonstrating that test-time reinforcement learning
can match or exceed the strongest proprietary models. Finally, we find many
interesting properties of test-time RL for VLMs: for example, even in extremely
data-constrained scenarios, where adaptation is performed on a single randomly
chosen unlabeled test example, TTRV still yields non-trivial improvements of up
to 5.5% in recognition tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TTRVï¼šè®©è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶â€œè‡ªä¸»è¿›åŒ–â€çš„æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå–å¥–åŠ±ä¿¡å·çš„æ–¹æ³•å¾€å¾€ä¾èµ–å¸¦æ ‡ç­¾æ•°æ®å’Œä¸“é—¨çš„è®­ç»ƒé›†åˆ’åˆ†ï¼Œè¿™ä¸äººç±»ä»ç¯å¢ƒä¸­ç›´æ¥å­¦ä¹ çš„æ¨¡å¼ç›¸æ‚–ã€‚è€Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è®­ç»ƒåå¤§å¤šä¿æŒé™æ€ï¼Œé€‚åº”æ–°åœºæ™¯éœ€å¤§é‡æ ‡æ³¨æ•°æ®å’Œæ˜‚è´µå¾®è°ƒï¼Œé™åˆ¶äº†åº”å¯¹æ–°é¢†åŸŸæˆ–æœªçŸ¥ä»»åŠ¡çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç°æœ‰RLåœ¨VLMsä¸­å¸¸ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®çš„å¥–åŠ±ä¿¡å·ï¼Œä¸æ— å¤©ç„¶è®­ç»ƒ - æµ‹è¯•åŒºåˆ†çš„çœŸå®åœºæ™¯ä¸åŒ¹é…ã€‚äºæ˜¯ï¼Œæœ¬æ–‡æå‡ºTTRVæ¡†æ¶ï¼Œæ—¨åœ¨è®©VLMsåœ¨æ¨ç†æ—¶æ— éœ€æ ‡æ³¨æ•°æ®å°±èƒ½åŠ¨æ€è‡ªé€‚åº”ï¼Œè®©RLæ›´è´´è¿‘äººç±»ä»åŸå§‹ç»éªŒå­¦ä¹ çš„èŒƒå¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶TTRV
TTRVé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ¨ç†é˜¶æ®µç›´æ¥ä»æ— æ ‡ç­¾æµ‹è¯•æ•°æ®ä¸­æå–å¥–åŠ±ä¿¡å·ï¼Œä¸ºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶æœåŠ¡ã€‚å®ƒèƒ½è®©é¢„è®­ç»ƒçš„é™æ€VLMsåœ¨æ¨ç†æ—¶å˜æˆå¯è‡ªæˆ‘æ”¹è¿›çš„åŠ¨æ€ç³»ç»Ÿï¼Œæ— éœ€æœ‰ç›‘ç£æ•°æ®å°±èƒ½å®ç°æ¨¡å‹å®æ—¶è‡ªé€‚åº”ï¼Œè·µè¡Œäº†RLä»çœŸå®æ— æ ‡ç­¾ç»éªŒä¸­å­¦ä¹ çš„ç†å¿µã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºé¢‘ç‡å’Œå¤šæ ·æ€§æ§åˆ¶çš„å¥–åŠ±è®¾è®¡
å¥–åŠ±å…¬å¼åŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€æ˜¯åŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºé¢‘ç‡ï¼Œé¼“åŠ±æ¨¡å‹å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬é¢‘ç¹äº§ç”Ÿç›¸ä¼¼è¾“å‡ºï¼Œå¥–åŠ±æ›´é¢‘ç¹å‡ºç°çš„é¢„æµ‹ï¼›äºŒæ˜¯å¤šæ ·æ€§æ§åˆ¶ï¼Œé€šè¿‡å¥–åŠ±è¾“å‡ºç»éªŒåˆ†å¸ƒçš„ä½ç†µå€¼æ¥æ§åˆ¶æ¨¡å‹è¾“å‡ºå¤šæ ·æ€§ã€‚å€ŸåŠ©è¿™ä¸¤éƒ¨åˆ†å¥–åŠ±ï¼Œå¼•å¯¼æ¨¡å‹åœ¨æ¨ç†æ—¶ä¼˜åŒ–è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. ä»»åŠ¡è¡¨ç°æå‡æ˜¾è‘—ï¼šåœ¨16ä¸ªæ¶µç›–å›¾åƒè¯†åˆ«å’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„æ•°æ®é›†ä¸ŠæŒç»­æå‡æ€§èƒ½ã€‚å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­æ”¹è¿›æœ€é«˜è¾¾52.4%ï¼ŒVQAä»»åŠ¡æœ€é«˜è¾¾29.8%ï¼›16ä¸ªæ•°æ®é›†å¹³å‡æå‡åˆ†åˆ«ä¸º24.6%å’Œ10.0%ã€‚
2. è¶…è¶Šå¼ºä¸“æœ‰æ¨¡å‹ï¼šåœ¨å›¾åƒè¯†åˆ«ä¸Šï¼ŒTTRVåº”ç”¨äºInternVL 8Båœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡è¶…è¿‡GPT - 4o 2.3%ï¼ŒVQAä»»åŠ¡ä¸Šä¹Ÿä¿æŒé«˜ç«äº‰åŠ›ã€‚
3. æ•°æ®é«˜æ•ˆæ€§çªå‡ºï¼šå³ä½¿åœ¨æ•°æ®æå°‘çš„åœºæ™¯ï¼Œå¦‚ä»…ç”¨å•ä¸ªéšæœºé€‰çš„æ— æ ‡ç­¾æµ‹è¯•æ ·æœ¬é€‚é…ï¼Œåœ¨è¯†åˆ«ä»»åŠ¡ä¸­ä»èƒ½å¸¦æ¥é«˜è¾¾5.5%çš„æå‡ï¼›ç”¨20ä¸ªéšæœºæµ‹è¯•å›¾åƒå¯¹InternVL3å¾®è°ƒï¼ŒGRPOåœ¨å¤§è§„æ¨¡ImageNetä¸Šèƒ½æœ‰42.3%æå‡ï¼ŒVQAçš„AI2DåŸºå‡†ä¸Šæœ€å¤šæå‡28.0%ã€‚
4. æ­ç¤ºGRPOæ–°ç‰¹æ€§ï¼šGRPOèƒ½æå‡è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸€ä¸ªæ•°æ®é›†è®­ç»ƒèƒ½è®©å®Œå…¨æ— å…³æ•°æ®é›†æ”¶ç›Šï¼›åœ¨æä½æ•°æ®é‡ä¸‹ä¹Ÿæœ‰æ•ˆï¼Œå±•ç°å‡ºæ¿€æ´»é¢„è®­ç»ƒä¸­æ½œåœ¨èƒ½åŠ›è€Œéç®€å•é€‚é…æ•°æ®é›†åˆ†å¸ƒçš„ç‰¹ç‚¹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶åˆ›æ–°æ€§ï¼šæå‡ºé¦–ä¸ªé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸ºVLMsåœ¨æ¨ç†é˜¶æ®µçš„è‡ªé€‚åº”æä¾›æ–°èŒƒå¼ï¼Œä»»ä½•é¢„è®­ç»ƒVLMéƒ½å¯å€Ÿé‰´è¯¥æ¡†æ¶æ€è·¯å®ç°æ— ç›‘ç£å®æ—¶é€‚é…ã€‚
2. å¥–åŠ±è®¾è®¡æ€è·¯ï¼šåŸºäºé¢‘ç‡å’Œç†µçš„å¥–åŠ±è®¾è®¡ä¸ºä»æ— æ ‡ç­¾æ•°æ®ä¸­æå–æœ‰æ•ˆRLå¥–åŠ±ä¿¡å·æä¾›äº†èŒƒä¾‹ï¼Œå¯å¯å‘åç»­åœ¨æ— ç›‘ç£æˆ–å¼±ç›‘ç£åœºæ™¯ä¸‹çš„RLå¥–åŠ±æœºåˆ¶è®¾è®¡ã€‚
3. å®éªŒæŒ–æ˜æ–¹å‘ï¼šé€šè¿‡å®éªŒæ­ç¤ºçš„GRPOåœ¨æä½æ•°æ®å’Œè·¨æ•°æ®é›†æ³›åŒ–ç­‰ç‰¹æ€§ï¼Œä¸ºæœªæ¥æµ‹è¯•æ—¶è‡ªé€‚åº”ä¸å¥–åŠ±é©±åŠ¨å­¦ä¹ çš„ç ”ç©¶å¼€è¾Ÿæ–°æ–¹å‘ï¼Œæ¯”å¦‚æ¢ç´¢å¦‚ä½•åˆ©ç”¨è¿™äº›ç‰¹æ€§è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹åœ¨æ›´å¤æ‚åœºæ™¯çš„è¡¨ç°ã€‚

## beneficial-reasoning-behaviors-in-agentic-search-and-effective-post-training-to-obtain-them
### Abstract
Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ™ºèƒ½ä½“æœç´¢ä¸­çš„æœ‰ç›Šæ¨ç†è¡Œä¸ºä¸é«˜æ•ˆåè®­ç»ƒæ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ™ºèƒ½ä½“æœç´¢ï¼ˆAgentic searchï¼‰å€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£è¯»å¤æ‚ç”¨æˆ·ä¿¡æ¯éœ€æ±‚ï¼Œæ‰§è¡Œè§„åˆ’ã€æœç´¢ã€ä¿¡æ¯åˆæˆç­‰å¤šæ­¥éª¤æµç¨‹æ¥æä¾›ç­”æ¡ˆã€‚ä½†è¯¥èŒƒå¼ä¸‹ï¼ŒLLMsä¸æ£€ç´¢ç³»ç»Ÿå’Œç½‘ç»œäº¤äº’æ—¶ï¼Œå…¶æ¨ç†ä¸æ™ºèƒ½ä½“èƒ½åŠ›é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæ¯”å¦‚å¤„ç†å™ªå£°æœç´¢ç»“æœã€ä¾æ®å†²çªä¿¡æ¯è°ƒæ•´ç­–ç•¥ç­‰åœºæ™¯ä¸‹ï¼Œå“ªäº›æ¨ç†è¡Œä¸ºå¯¹è§£å†³æ™ºèƒ½ä½“æœç´¢ç‰¹æœ‰éš¾é¢˜æœ‰ç›Šå°šä¸æ˜ç¡®ã€‚åŒæ—¶ï¼Œç°æœ‰ç ”ç©¶ä¸­é’ˆå¯¹æ™ºèƒ½ä½“æœç´¢è®­ç»ƒï¼Œè™½æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨ï¼Œä½†ç¼ºä¹å¯¹æ”¯æ’‘æœ‰æ•ˆRLè®­ç»ƒçš„å…³é”®æ¨ç†è¡Œä¸ºçš„æ·±å…¥æ¢ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯†åˆ«æ™ºèƒ½ä½“æœç´¢å…³é”®æ¨ç†è¡Œä¸º  
è®¾è®¡åŸºäºLLMçš„è‡ªåŠ¨åŒ– pipeline æ¥ç ”ç©¶æ™ºèƒ½ä½“æœç´¢ä¸­çš„æœ‰æ•ˆæ¨ç†è¡Œä¸ºï¼Œæ”¶é›†å¤šLLMçš„æ™ºèƒ½ä½“æœç´¢è½¨è¿¹ï¼Œç”¨æ¨ç†LLMåˆ†æè½¨è¿¹ï¼Œä»æˆåŠŸè½¨è¿¹ä¸­æç‚¼å‡ºå››ç§å…³é”®æœ‰ç›Šæ¨ç†è¡Œä¸ºï¼šä¿¡æ¯éªŒè¯ï¼ˆè·¨æºéªŒè¯ç»“æœï¼‰ã€æƒå¨è¯„ä¼°ï¼ˆè¯„ä¼°å¯é æ€§ä¸è§£å†³å†²çªï¼‰ã€è‡ªé€‚åº”æœç´¢ï¼ˆåŠ¨æ€ä¿®æ”¹ç­–ç•¥ï¼‰ã€é”™è¯¯æ¢å¤ï¼ˆæ£€æµ‹ä¸çº æ­£é”™è¯¯ï¼‰ã€‚å‰ä¸¤ç§åº”å¯¹ä¿¡æ¯æ£€ç´¢ç‰¹æœ‰æŒ‘æˆ˜ï¼Œåä¸¤ç§æ˜¯å¤šæ­¥éª¤è§„åˆ’åŸºç¡€èƒ½åŠ›ï¼Œä¸”è¿™äº›è¡Œä¸ºå‡ºç°é¢‘ç‡ä¸æ¨¡å‹åœ¨æ™ºèƒ½ä½“æœç´¢ä»»åŠ¡è¡¨ç°å¼ºç›¸å…³ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºBehavior Primingè®­ç»ƒæŠ€æœ¯  
åŸºäºä¸Šè¿°å‘ç°ï¼Œæå‡ºBehavior Primingæ–¹æ³•å°†æœ‰ç›Šæ¨ç†è¡Œä¸ºæ³¨å…¥æ™ºèƒ½ä½“æœç´¢æ¨¡å‹ã€‚å…ˆä»å¤§é‡LLMç”Ÿæˆçš„æ™ºèƒ½ä½“æœç´¢è½¨è¿¹ä¸­ç­›é€‰å±•ç¤ºå››ç§è¡Œä¸ºçš„è½¨è¿¹ä½œä¸ºç›‘ç£æ•°æ®ï¼Œé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®©æ¨¡å‹æ˜¾å¼å­¦ä¹ è¿™äº›è¡Œä¸ºï¼Œä¹‹åç”¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚è¯¥æ–¹æ³•æ—¨åœ¨ä¸ºåç»­RLè®­ç»ƒç­‘ç‰¢åŸºç¡€ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GAIAã€WebWalkerã€HLEä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹æ¯”ç›´æ¥ç”¨RLè®­ç»ƒæ™ºèƒ½ä½“æœç´¢æ¨¡å‹ï¼ŒBehavior Primingä½¿Llama3.2 - 3Bå’ŒQwen3 - 1.7Bæ€§èƒ½æå‡è¶…35%ï¼›å¯¹æ¯”å…¶ä»–å¸¸è§â€œå…ˆSFTåRLâ€æ–¹æ³•ï¼ˆå¦‚åŸºäºå¼ºæ¨¡å‹è’¸é¦è½¨è¿¹å¾®è°ƒã€åŸºäºæ­£ç¡®æœ€ç»ˆç­”æ¡ˆè½¨è¿¹å¾®è°ƒï¼‰ï¼ŒBehavior Primingè¡¨ç°æ›´ä¼˜ã€‚  
æ¶ˆèå®éªŒåˆ†ç¦»æ¨ç†è¡Œä¸ºä¸ç»“æœæ­£ç¡®æ€§å½±å“ï¼šå¯¹å±•ç¤ºç†æƒ³æ¨ç†è¡Œä¸ºä½†æœ€ç»ˆç­”æ¡ˆé”™è¯¯çš„è½¨è¿¹å¾®è°ƒï¼Œå†ç»RLè®­ç»ƒåï¼Œæ€§èƒ½ä¸åŸºäºæ­£ç¡®ç­”æ¡ˆè½¨è¿¹å¾®è°ƒå†RLè®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼Œå‡¸æ˜¾æ¨ç†è¡Œä¸ºåœ¨RLè§£é”æ¨¡å‹æ½œåŠ›æ—¶æ¯”ç»“æœæ­£ç¡®æ€§æ›´å…³é”®ã€‚  
æœºåˆ¶åˆ†ææ˜¾ç¤ºï¼ŒBehavior Primingçš„SFTé˜¶æ®µæå‡äº†å››ç§è¡Œä¸ºé¢‘ç‡ã€pass@kå‡†ç¡®ç‡ä¸è½¨è¿¹å¹³å‡æ­¥æ•°ï¼Œä¸ºRLçš„æ¢ç´¢å’Œæµ‹è¯•æ—¶æ‰©å±•èƒ½åŠ›æ‰“åŸºç¡€ï¼›RLé˜¶æ®µï¼Œç»è¡Œä¸ºé¢„è®­ç»ƒçš„æ¨¡å‹ä¿æŒé«˜ç­–ç•¥ç†µï¼Œæ— é¢„è®­ç»ƒæ¨¡å‹ç†µä½ä¸”å¿«é€Ÿä¸‹é™è‡´ç­–ç•¥æ—©ç†Ÿæ”¶æ•›ï¼Œä¸”æ— æ³•å†…ç”ŸåŸ¹å…»å…³é”®è¡Œä¸ºã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¡Œä¸ºåˆ†æè§†è§’ï¼šé€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹è½¨è¿¹æŒ–æ˜å…³é”®æ¨ç†è¡Œä¸ºï¼Œä¸ºç†è§£æ™ºèƒ½ä½“ä»»åŠ¡ä¸­æ¨¡å‹èƒ½åŠ›æ„æˆæä¾›æ–°è§†è§’ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§ä»æˆåŠŸæ¡ˆä¾‹ä¸­æç‚¼è¡Œä¸ºæ¨¡å¼çš„æ€è·¯ï¼Œåˆ†æå…¶ä»–AIä»»åŠ¡ï¼ˆå¦‚å¤šæ™ºèƒ½ä½“åä½œã€é•¿æ–‡æœ¬ç†è§£ï¼‰çš„å…³é”®èƒ½åŠ›è¦ç´ ã€‚  
2. è®­ç»ƒæ–¹æ³•åˆ›æ–°ï¼šBehavior Primingå°†è¡Œä¸ºçº§ç›‘ç£èå…¥â€œå…ˆSFTåRLâ€æµç¨‹ï¼Œå¼ºè°ƒè¡Œä¸ºè€Œéä»…ç»“æœçš„é‡è¦æ€§ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ä»»åŠ¡ï¼ˆéœ€å¤šæ­¥éª¤æ¨ç†ã€å·¥å…·äº¤äº’ï¼‰ä¸­é«˜æ•ˆè®­ç»ƒæ¨¡å‹æä¾›æ–°èŒƒå¼ï¼Œå¯å¯å‘ç±»ä¼¼éœ€å¤šé˜¶æ®µå†³ç­–ã€å·¥å…·ä½¿ç”¨åœºæ™¯çš„AIæ¨¡å‹è®­ç»ƒï¼ˆå¦‚æ™ºèƒ½ä½“å¯¼èˆªã€ä»£ç ç”Ÿæˆè¾…åŠ©ï¼‰ã€‚  
3. å®éªŒè®¾è®¡æ€è·¯ï¼šé€šè¿‡æ¶ˆèå®éªŒæ¸…æ™°åˆ†ç¦»è¡Œä¸ºä¸ç»“æœå½±å“ï¼Œä¸¥è°¨éªŒè¯æ ¸å¿ƒå‡è®¾ï¼Œè¿™ç§å®éªŒè®¾è®¡é€»è¾‘åœ¨è®ºè¯å› æœå…³ç³»ã€æœºåˆ¶è§£é‡Šç±»ç ”ç©¶ä¸­å€¼å¾—å‚è€ƒï¼Œå¸®åŠ©æ›´ç²¾å‡†å‰–ææ¨¡å‹æ€§èƒ½æå‡æ ¹æºã€‚

## marco--a-cooperative-knowledge-transfer-framework-for-personalized-cross-domain-recommendations
### Abstract
Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MARCOï¼šå¤šæ™ºèƒ½ä½“åä½œä¸‹çš„è·¨åŸŸæ¨èæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ¨èç³»ç»Ÿåœ¨é¢å¯¹æ–°ç”¨æˆ·æˆ–æ–°ç‰©å“çš„å†·å¯åŠ¨åœºæ™¯æ—¶ï¼Œå¸¸å—æ•°æ®ç¨€ç–æ€§å›°æ‰°ã€‚å¤šæºè·¨åŸŸæ¨èï¼ˆCDRï¼‰é€šè¿‡ä»å¤šä¸ªæºåŸŸè½¬ç§»æœ‰ä»·å€¼çŸ¥è¯†æ¥å¢å¼ºç›®æ ‡åŸŸæ¨èæ•ˆæœï¼Œä½†ç°æœ‰åŸºäºå•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„CDRæ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šæºåŸŸè´¡çŒ®ä¸ä¸€è‡´ä¸æ•°æ®åˆ†å¸ƒå·®å¼‚æ˜“å¼•å‘è´Ÿè¿ç§»é—®é¢˜ï¼Œå•æ™ºèƒ½ä½“éš¾ä»¥ç²¾å‡†åˆ†é…å„æºåŸŸä¿¡ç”¨ã€å¹³è¡¡è´¡çŒ®ã€‚å› æ­¤ï¼ŒäºŸéœ€æ›´é«˜æ•ˆçš„æ¡†æ¶è§£å†³è¿™äº›éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰é©±åŠ¨çš„è·¨åŸŸæ¨èèŒƒå¼  
MARCOå°†å¤šæºCDRä»»åŠ¡å»ºæ¨¡ä¸ºåä½œå¼MARLé—®é¢˜ï¼Œä¸ºæ¯ä¸ªæºåŸŸåˆ†é…ä¸“å±æ™ºèƒ½ä½“ï¼Œè®©å…¶è´Ÿè´£ä¼°è®¡å¯¹åº”æºåŸŸå¯¹ç›®æ ‡åŸŸæ¨èæ€§èƒ½çš„è´¡çŒ®ã€‚å€ŸåŠ©è·¨åŸŸå…¨å±€ç”¨æˆ·ä¸ç‰©å“ç‰¹å¾ï¼Œå®ç°æºåŸŸé—´ä¿¡ç”¨åˆ†é…çš„æœ‰æ•ˆç®¡ç†ï¼Œå¤§å¹…ç¼“è§£è´Ÿè¿ç§»ã€‚æ¯”å¦‚å›¾1å¯¹æ¯”æ‰€ç¤ºï¼Œå•æ™ºèƒ½ä½“ä¸‹å„åŸŸè´¡çŒ®ä¼°è®¡æ˜“åå·®ï¼Œè€Œå¤šæ™ºèƒ½ä½“åä½œèƒ½åŸºäºå…¨å±€çŸ¥è¯†ä¿®æ­£åå·®ï¼Œè®©å„åŸŸè´¡çŒ®æ›´å‡è¡¡é€‚é…ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºç†µçš„åŠ¨ä½œå¤šæ ·æ€§æƒ©ç½šæœºåˆ¶  
åœ¨MARLæ¡†æ¶å†…å¼•å…¥è¯¥æœºåˆ¶ï¼Œé€šè¿‡é¼“åŠ±æ™ºèƒ½ä½“è”åˆåŠ¨ä½œçš„å¤šæ ·æ€§ï¼Œç¨³å¥åº”å¯¹æºåŸŸæ•°æ®åˆ†å¸ƒå·®å¼‚ã€‚è¿™ä¸ä»…æå‡äº†æ¨èç­–ç•¥çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè¿˜è®©è®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šï¼Œé¿å…å› åˆ†å¸ƒå·®å¼‚å¯¼è‡´çš„è®­ç»ƒæ³¢åŠ¨æˆ–ç­–ç•¥åƒµåŒ–ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¼€å±•å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜MARCOåœ¨æ¨èæ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œå……åˆ†å±•ç°å‡ºå…¶é²æ£’æ€§ä¸å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨ä¸åŒåœºæ™¯ä¸‹ç¨³å®šè¾“å‡ºä¼˜è´¨æ¨èç»“æœã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»»åŠ¡å»ºæ¨¡æ€è·¯ï¼šæŠŠè·¨åŸŸæ¨èè¿™ç±»å¤šæºçŸ¥è¯†è¿ç§»ä»»åŠ¡ä¸å¤šæ™ºèƒ½ä½“åä½œå¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œä¸ºè§£å†³â€œå¤šæºè´¡çŒ®åˆ†é…ã€è´Ÿè¿ç§»â€ç­‰å¤æ‚é—®é¢˜æä¾›äº†æ–°è§†è§’ï¼Œå¯å‘åç»­åœ¨å¤šæºåœºæ™¯ä¸‹çš„æ¨èã€è¿ç§»å­¦ä¹ ç±»ä»»åŠ¡å°è¯•ç±»ä¼¼çš„å¤šæ™ºèƒ½ä½“èŒƒå¼ã€‚  
2. è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–ï¼šåŸºäºç†µçš„åŠ¨ä½œå¤šæ ·æ€§æƒ©ç½šä¸ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­åº”å¯¹åˆ†å¸ƒå·®å¼‚ã€å¢å¼ºç­–ç•¥çµæ´»æ€§æä¾›äº†æœ‰æ•ˆæ‰‹æ®µï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–éœ€å…¼é¡¾â€œç­–ç•¥è¡¨è¾¾åŠ›+è®­ç»ƒç¨³å®šæ€§â€çš„RLåº”ç”¨åœºæ™¯ï¼ˆå¦‚å¤šæ™ºèƒ½ä½“åšå¼ˆã€å¤æ‚ç¯å¢ƒå†³ç­–ç­‰ï¼‰ã€‚  
3. å®éªŒéªŒè¯ç»´åº¦ï¼šé€šè¿‡å¤šæ•°æ®é›†éªŒè¯é²æ£’æ€§ä¸æ³›åŒ–æ€§çš„æ€è·¯ï¼Œèƒ½æŒ‡å¯¼ç ”ç©¶è€…åœ¨æ–°æ–¹æ³•éªŒè¯æ—¶æ›´å…¨é¢åœ°è€ƒé‡ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¢å¼ºæˆæœè¯´æœåŠ›ã€‚

## mitigating-forgetting-between-supervised-and-reinforcement-learning-yields-stronger-reasoners
### Abstract
Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç¼“è§£ç›‘ç£ä¸å¼ºåŒ–å­¦ä¹ é—´é—å¿˜ï¼Œæ‰“é€ æ›´å¼ºæ¨ç†å¤§æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¸¸èƒ½æ”¾å¤§è¿™ä¸€èƒ½åŠ›ã€‚ä½†RLå­˜åœ¨å±€é™ï¼šå®ƒä»æ¨¡å‹è‡ªèº«æ¨ç†è½¨è¿¹å­¦ä¹ ï¼Œéš¾æ‹“å±•æ¨ç†è¾¹ç•Œï¼›ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è™½èƒ½è¡¥å……å¤–éƒ¨çŸ¥è¯†ï¼Œå´éœ€å¤§è§„æ¨¡æ•°æ®ä¸”æ˜“è¿‡æ‹Ÿåˆã€‚è¿‘å¹´ç»“åˆSFTä¸RLçš„å°è¯•é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ•°æ®ä½æ•ˆã€ç®—æ³•ç‰¹å®šè®¾è®¡ã€ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºMIFOæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œé«˜æ•ˆç»“åˆSFTä¸RLæå‡æ¨ç†åè®­ç»ƒæ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€é›†æˆSFTä¸RLçš„å³æ’å³ç”¨æ¡†æ¶  
ä¸ºè§£å†³å¤§é‡SFTæ•°æ®ä¾èµ–ä¸ç®—æ³•ç‰¹å®šè®¾è®¡é—®é¢˜ï¼ŒMIFOåŠ¨æ€åœ°å°†SFT interleavingï¼ˆ interleaving äº¤é”™ã€ç©¿æ’ï¼‰åˆ°RLè¿‡ç¨‹ä¸­ï¼Œä¾æ®rolloutå‡†ç¡®ç‡é€‰æ‹©SFTç”¨ä¾‹ï¼ŒæŒ‰ç†µé€‰SFTæŸå¤±è®¡ç®—çš„tokenã€‚è¿™æ ·è®©æ¨¡å‹ä»…ç”¨æœ€å°‘å¿…è¦SFTæ•°æ®è·å–åˆ†å¸ƒå¤–æ¨ç†çŸ¥è¯†ï¼Œä¸”å› æœªæŠŠSFTå’ŒRLåˆå¹¶æˆå•ä¸€ä¼˜åŒ–ç›®æ ‡ï¼Œèƒ½æ— ç¼é€‚é…æ–°çš„RLæˆ–SFTç®—æ³•ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç¼“è§£ç¾éš¾æ€§é—å¿˜çš„åŒæœºåˆ¶  
é’ˆå¯¹SFTä¸RLé—´ç¾éš¾æ€§é—å¿˜ï¼ŒMIFOè®¾è®¡ä¸¤å¤§äº’è¡¥æœºåˆ¶ã€‚ä¸€æ˜¯æ•°æ®ä¸tokené€‰æ‹©ç­–ç•¥ï¼Œæ—¢å‡å°‘æ•°æ®ä½¿ç”¨å®ç°å³æ’å³ç”¨ï¼Œåˆé™åˆ¶SFTæ›´æ–°å¹…åº¦ï¼Œé™ä½å¯¹RLä¹ å¾—çŸ¥è¯†çš„é—å¿˜ï¼›äºŒæ˜¯åŸºäºSFTå’ŒRLå‚æ•°æ›´æ–°ä¸å¯¹ç§°æ€§ï¼ˆSFTæ›´æ–°å†—ä½™ã€RLæ›´æ–°æ›´ç²¾ç®€ï¼‰ï¼ŒåŠ¨æ€è¯†åˆ«RLå…³é”®å‚æ•°ï¼ŒSFTæ—¶å†»ç»“ã€åç»­RLæ­¥éª¤è§£å†»ï¼Œä¿æŠ¤RLé‡è¦å‚æ•°æ›´æ–°ä¸è¢«SFTè¦†ç›–ï¼Œæœ‰æ•ˆç¼“è§£é—å¿˜ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
MIFOåœ¨ä»…ç”¨å…ˆå‰SOTAæ–¹æ³•1.5%çš„SFTæ•°æ®ä¸20.4%çš„RLæ•°æ®æƒ…å†µä¸‹ï¼Œå®ç°äº†æ¨ç†æ€§èƒ½çš„SOTAï¼ˆ state-of-the-art æœ€å…ˆè¿›ï¼‰ã€‚åŒæ—¶èƒ½é€‚é…ä¸åŒç®—æ³•çš„æ–°RL - SFTç»„åˆï¼Œæ¨ç†æ•ˆç‡é«˜ï¼Œå¹³å‡å“åº”é•¿åº¦ä¸å¼ºåŸºçº¿ç›¸å½“ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šMIFOçš„å³æ’å³ç”¨è®¾è®¡æ€è·¯ï¼Œä¸ºä¸åŒRLã€SFTç®—æ³•ç»“åˆæä¾›äº†é«˜æ•ˆèŒƒå¼ï¼Œåœ¨å¤„ç†å¤šç»„ä»¶ååŒè®­ç»ƒæ—¶ï¼Œå¯å‚è€ƒè¿™ç§è§£è€¦ä¼˜åŒ–ç›®æ ‡ã€åŠ¨æ€é€‚é…çš„æ€è·¯ã€‚  
2. ç¾éš¾æ€§é—å¿˜å¤„ç†ï¼šä»æ•°æ®é€‰æ‹©é™åˆ¶æ›´æ–°å¹…åº¦ã€å‚æ•°å…³é”®åº¦è¯†åˆ«å†»ç»“è§£å†»ä¸¤æ–¹é¢åº”å¯¹é—å¿˜ï¼Œä¸ºå¤šé˜¶æ®µè®­ç»ƒä¸­çŸ¥è¯†ä¿ç•™ä¸æ›´æ–°å¹³è¡¡æä¾›äº†å®è·µæ–¹æ³•ï¼Œåœ¨æœ‰å…ˆåè®­ç»ƒé˜¶æ®µã€çŸ¥è¯†æ˜“å†²çªåœºæ™¯ï¼ˆå¦‚å¤šä»»åŠ¡å­¦ä¹ ç­‰ï¼‰å¯å€Ÿé‰´å‚æ•°é‡è¦æ€§åˆ†æä¸åŠ¨æ€å†»ç»“ç­–ç•¥ã€‚  
3. æ•°æ®é«˜æ•ˆåˆ©ç”¨ï¼šé€šè¿‡åŸºäºrolloutå‡†ç¡®ç‡ç­‰é€‰æ‹©SFTæ•°æ®ï¼Œå±•ç¤ºäº†ç²¾å‡†é€‰ä¾‹å¯¹é™ä½æ•°æ®é‡åŒæ—¶ä¿æ€§èƒ½çš„ä»·å€¼ï¼Œåœ¨æ•°æ®ç¨€ç¼ºæˆ–æ ‡æ³¨æˆæœ¬é«˜çš„ä»»åŠ¡ä¸­ï¼Œè¿™ç§æ•°æ®é€‰æ‹©ç­–ç•¥å€¼å¾—å‚è€ƒã€‚

## a-kl-regularization-framework-for-learning-to-plan-with-adaptive-priors
### Abstract
Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºKLæ­£åˆ™åŒ–æ¡†æ¶ï¼Œç»Ÿä¸€MPPIå‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰ä¸­ï¼Œé«˜æ•ˆæ¢ç´¢ä¸€ç›´æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡é‡Œï¼Œæ ·æœ¬æ•ˆç‡è‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œä¸å°‘å·¥ä½œåˆ©ç”¨å­¦ä¹ åˆ°çš„ç­–ç•¥ä½œä¸ºæ¨¡å‹é¢„æµ‹è·¯å¾„ç§¯åˆ†ï¼ˆMPPIï¼‰è§„åˆ’çš„æè®®åˆ†å¸ƒï¼Œä½†æ—©æœŸæ–¹æ³•ä¸­é‡‡æ ·ç­–ç•¥ä¸è§„åˆ’å™¨åˆ†å¸ƒç‹¬ç«‹æ›´æ–°ï¼Œæ˜“å‡ºç°åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œå½±å“ä»·å€¼ä¼°è®¡å‡†ç¡®æ€§ä¸é•¿æœŸæ€§èƒ½ã€‚è™½ç„¶åç»­æœ‰æ–¹æ³•å°è¯•é€šè¿‡æœ€å°åŒ–KLæ•£åº¦æˆ–å¼•å…¥è§„åˆ’å™¨å¼•å¯¼æ­£åˆ™åŒ–æ¥å¯¹é½ï¼Œä½†ç›¸å…³MPPIç±»æ–¹æ³•ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œå‘ˆç°ç¢ç‰‡åŒ–çŠ¶æ€ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œæ•´åˆè¿™äº›æ–¹æ³•å¹¶æ¨åŠ¨æŠ€æœ¯å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºPO - MPCç»Ÿä¸€æ¡†æ¶  
æå‡ºPolicy Optimization - Model Predictive Controlï¼ˆPO - MPCï¼‰è¿™ä¸€é€šç”¨MBRLæ¡†æ¶ï¼Œç”¨äºåŸºäºMPPIçš„æ–¹æ³•ã€‚å®ƒå°†é‡‡æ ·ç­–ç•¥å­¦ä¹ æ­¥éª¤è½¬åŒ–ä¸ºKLæ­£åˆ™åŒ–çš„å¼ºåŒ–å­¦ä¹ å®ä¾‹ï¼Œè®©å­¦ä¹ åˆ°çš„é‡‡æ ·ç­–ç•¥$\pi_{\theta}(s)$é’ˆå¯¹MPPIè¯±å¯¼çš„å…ˆéªŒ$\pi_p$è¿›è¡Œæ­£åˆ™åŒ–ï¼Œæ­£åˆ™åŒ–å¼ºåº¦ç”±è¶…å‚æ•°$\lambda$å†³å®šï¼ŒæŠŠå„ç±»åŸºäºMPPIçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»Ÿä¸€åˆ°è¯¥æ¡†æ¶ä¸‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¢ç´¢æ–°é¢–é…ç½®ä¸ä¸­é—´å…ˆéªŒç­‰  
é€šè¿‡è°ƒèŠ‚KLæ­£åˆ™åŒ–å¼ºåº¦$\lambda$æ¢ç´¢æ–°çš„ç®—æ³•å˜ä½“ï¼›å¼•å…¥å­¦ä¹ åˆ°çš„å…ˆéªŒï¼Œä½¿$\pi_{\theta}(s)$å…å—é‡æ”¾ç¼“å†²åŒºä¸­è¿‡æ—¶è§„åˆ’æ ·æœ¬çš„å½±å“ï¼›å±•ç¤ºäº†è®­ç»ƒMPPIè¯±å¯¼å…ˆéªŒçš„æ›¿ä»£æŸå¤±å¦‚ä½•åœ¨$\pi_{\theta}(s)$ä¸­åµŒå…¥ä¸åŒå±æ€§ä»¥è·å¾—æ›´ä¼˜æ€§èƒ½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆ©ç”¨è§„åˆ’å™¨å…¨åŠ¨ä½œåˆ†å¸ƒ  
åŒºåˆ«äºä»¥å¾€ä»…åˆ©ç”¨è§„åˆ’å™¨çš„å•ä¸ªæœ€ä½³åŠ¨ä½œæˆ–è½¨è¿¹ï¼ŒPO - MPCæè®®åˆ©ç”¨è§„åˆ’å™¨ç”Ÿæˆçš„æ•´ä¸ªåŠ¨ä½œåˆ†å¸ƒä½œä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å¼•å¯¼å…ˆéªŒï¼ŒæŒ–æ˜å¼ºåŒ–å­¦ä¹ ç­–ç•¥åˆæˆä¸åŸºäºè§„åˆ’çš„åŠ¨ä½œæ”¹è¿›ä¹‹é—´çš„ååŒä½œç”¨ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é«˜ç»´è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­éªŒè¯PO - MPCï¼Œç»“æœæ˜¾ç¤ºï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œåœ¨æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å¯¹åŸºäºMPPIçš„æ–¹æ³•è¿›è¡Œæœ‰åŸåˆ™çš„ç»Ÿä¸€ï¼Œä¸ä»…èƒ½æ˜ç¡®å…¶è®¾è®¡ç©ºé—´ï¼Œè¿˜èƒ½åœ¨å®è·µä¸­å¸¦æ¥åˆ‡å®æ”¹è¿›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•æ•´åˆè§’åº¦ï¼Œå±•ç¤ºäº†å°†ç¢ç‰‡åŒ–çš„åŒç±»æ–¹æ³•ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶çš„æ€è·¯ï¼Œæœ‰åŠ©äºåç»­ç ”ç©¶æ¢³ç†è®¾è®¡ç©ºé—´ï¼›åœ¨æŠ€æœ¯å®ç°ä¸Šï¼ŒKLæ­£åˆ™åŒ–ç»“åˆå…ˆéªŒçš„æ–¹å¼ã€å¯¹è§„åˆ’å™¨å…¨åŠ¨ä½œåˆ†å¸ƒçš„åˆ©ç”¨ç­‰ï¼Œä¸ºæå‡åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ åœ¨é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡çš„æ€§èƒ½æä¾›äº†æ–°æ–¹å‘ï¼›å®éªŒå±‚é¢ï¼Œåœ¨é«˜ç»´è¿ç»­æ§åˆ¶åŸºå‡†ä¸Šçš„éªŒè¯æ€è·¯å’Œç»“æœå‘ˆç°ï¼Œä¹Ÿä¸ºç›¸å…³é¢†åŸŸç®—æ³•è¯„ä¼°æä¾›äº†å‚è€ƒèŒƒå¼ï¼Œèƒ½å¯å‘ç ”ç©¶è€…åœ¨ç±»ä¼¼å¤æ‚ä»»åŠ¡åœºæ™¯ä¸‹è®¾è®¡å’ŒéªŒè¯ç®—æ³•ã€‚

## global-convergence-of-policy-gradient-for-entropy-regularized-linear-quadratic-control-with-multiplicative-noise
### Abstract
Reinforcement Learning (RL) has emerged as a powerful framework for
sequential decision-making in dynamic environments, particularly when system
parameters are unknown. This paper investigates RL-based control for
entropy-regularized Linear Quadratic control (LQC) problems with multiplicative
noises over an infinite time horizon. First, we adapt the Regularized Policy
Gradient (RPG) algorithm to stochastic optimal control settings, proving that
despite the non-convexity of the problem, RPG converges globally under
conditions of gradient domination and near-smoothness. Second, based on
zero-order optimization approach, we introduce a novel model free RL algorithm:
Sample-Based Regularized Policy Gradient (SB-RPG). SB-RPG operates without
knowledge of system parameters yet still retains strong theoretical guarantees
of global convergence. Our model leverages entropy regularization to accelerate
convergence and address the exploration versus exploitation trade-off inherent
in RL. Numerical simulations validate the theoretical results and demonstrate
the efficacy of SB-RPG in unknown-parameters environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¸¦ä¹˜æ€§å™ªå£°çš„ç†µæ­£åˆ™çº¿æ€§äºŒæ¬¡æ§åˆ¶ä¸­ç­–ç•¥æ¢¯åº¦çš„å…¨å±€æ”¶æ•›æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŠ¨æ€ç¯å¢ƒçš„åºè´¯å†³ç­–ä¸­è¡¨ç°å¼ºå¤§ï¼Œå°¤å…¶å½“ç³»ç»Ÿå‚æ•°æœªçŸ¥æ—¶ã€‚æœ€ä¼˜æ§åˆ¶ç†è®ºéœ€ç¯å¢ƒå‚æ•°å…¨çŸ¥ï¼Œç°å®éš¾å®ç°ï¼Œè€ŒåŸºäºRLçš„æ§åˆ¶åœ¨å‚æ•°æœªçŸ¥åœºæ™¯è¿‘å¹´æˆæœæ˜¾è‘—ã€‚çº¿æ€§äºŒæ¬¡ï¼ˆLQï¼‰æ§åˆ¶æ˜¯æ§åˆ¶ç†è®ºåŸºç¡€é—®é¢˜ï¼Œå—RLé¢†åŸŸå…³æ³¨ã€‚ç­–ç•¥æ¢¯åº¦ç®—æ³•æ˜“å®ç°ä½†é¢ä¸´éå‡¸ä¼˜åŒ–å›°å¢ƒï¼Œæ”¶æ•›æ€§æ˜¯ç ”ç©¶çƒ­ç‚¹ã€‚RLä¸­æ¢ç´¢ - åˆ©ç”¨æƒè¡¡å…³é”®ï¼Œç†µæ­£åˆ™åŒ–æ˜¯è§£å†³æ€è·¯ä¹‹ä¸€ï¼Œä½†é’ˆå¯¹å¸¦ä¹˜æ€§å™ªå£°çš„ç†µæ­£åˆ™LQæ§åˆ¶ï¼ŒåŸºäºç­–ç•¥æ¢¯åº¦ä¸”åœ¨æ¨¡å‹å·²çŸ¥å’ŒæœªçŸ¥åœºæ™¯ä¸‹çš„ç ”ç©¶ä»æœ‰ç©ºç™½ï¼Œæœ¬æ–‡å°±æ­¤å±•å¼€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ‰©å±•æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦ï¼ˆRPGï¼‰åˆ°éšæœºæœ€ä¼˜æ§åˆ¶åœºæ™¯  
å°†Regularized Policy Gradientï¼ˆRPGï¼‰ç®—æ³•é€‚é…åˆ°éšæœºæœ€ä¼˜æ§åˆ¶è®¾ç½®ä¸­ã€‚å°½ç®¡å¸¦ä¹˜æ€§å™ªå£°çš„éšæœºLQæ§åˆ¶é—®é¢˜å­˜åœ¨éå‡¸æ€§ï¼Œä½†åˆ©ç”¨æ¢¯åº¦ä¸»å¯¼å’Œè¿‘ä¼¼å…‰æ»‘æ€§ç­‰æ€§è´¨ï¼Œè¯æ˜äº†RPGèƒ½å…¨å±€æ”¶æ•›ï¼Œæå‡äº†ç³»ç»Ÿé²æ£’æ€§å¹¶æ‹“å®½åº”ç”¨åœºæ™¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºåŸºäºé‡‡æ ·çš„æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦ï¼ˆSB - RPGï¼‰ç®—æ³•  
åŸºäºé›¶é˜¶ä¼˜åŒ–æ–¹æ³•ï¼Œæå‡ºå…¨æ–°çš„æ— æ¨¡å‹RLç®—æ³•Sample - Based Regularized Policy Gradientï¼ˆSB - RPGï¼‰ã€‚è¯¥ç®—æ³•æ— éœ€ç³»ç»Ÿå‚æ•°çŸ¥è¯†ï¼Œå´èƒ½ä¿è¯å…¨å±€æ”¶æ•›ã€‚å€ŸåŠ©ç†µæ­£åˆ™åŒ–åŠ é€Ÿæ”¶æ•›ï¼ŒåŒæ—¶åº”å¯¹RLä¸­å›ºæœ‰æ¢ç´¢ - åˆ©ç”¨æƒè¡¡é—®é¢˜ï¼Œåœ¨å‚æ•°æœªçŸ¥ç¯å¢ƒä¸­æœ‰æ•ˆè¿ä½œï¼Œå¼¥è¡¥äº†RPGä¾èµ–æ¨¡å‹å‚æ•°çš„å±€é™ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­é€šè¿‡æ•°å€¼æ¨¡æ‹ŸéªŒè¯ç†è®ºç»“æœï¼Œå±•ç¤ºäº†SB - RPGåœ¨æœªçŸ¥å‚æ•°ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½åœ¨æ— ç³»ç»Ÿå‚æ•°çŸ¥è¯†æ—¶å®ç°è‰¯å¥½æ§åˆ¶æ•ˆæœï¼Œæ”¯æ’‘äº†æ‰€æç®—æ³•å…¨å±€æ”¶æ•›ç­‰ç†è®ºç»“è®ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç®—æ³•æ‰©å±•ä¸æ”¶æ•›æ€§è¯æ˜æ€è·¯ï¼šåœ¨éå‡¸ä¼˜åŒ–åœºæ™¯ä¸‹ï¼Œé€šè¿‡åˆ†æé—®é¢˜ç‰¹æ€§ï¼ˆå¦‚æ¢¯åº¦ä¸»å¯¼ã€è¿‘ä¼¼å…‰æ»‘æ€§ï¼‰æ¥è¯æ˜ç®—æ³•å…¨å±€æ”¶æ•›ï¼Œä¸ºå¤„ç†ç±»ä¼¼å¸¦å™ªå£°çš„æ§åˆ¶æˆ–RLéå‡¸é—®é¢˜æä¾›äº†æ–¹æ³•è®ºå‚è€ƒã€‚  
2. æ— æ¨¡å‹ç®—æ³•è®¾è®¡ï¼šSB - RPGåŸºäºé›¶é˜¶ä¼˜åŒ–è®¾è®¡æ— æ¨¡å‹ç®—æ³•ï¼Œä¸ºå‚æ•°æœªçŸ¥çš„å¤æ‚åŠ¨æ€ç³»ç»Ÿæ§åˆ¶æä¾›äº†æ–°çš„æœ‰æ•ˆå·¥å…·ï¼Œåœ¨å®é™…å·¥ä¸šã€æœºå™¨äººç­‰å‚æ•°éš¾è·å–çš„åœºæ™¯æœ‰åº”ç”¨å€Ÿé‰´ä»·å€¼ã€‚  
3. ç†µæ­£åˆ™åŒ–è¿ç”¨ï¼šåˆ©ç”¨ç†µæ­£åˆ™åŒ–å¹³è¡¡æ¢ç´¢ - åˆ©ç”¨å¹¶åŠ é€Ÿæ”¶æ•›ï¼Œåœ¨è®¾è®¡RLç®—æ³•æ—¶ï¼Œå¯å‚è€ƒè¿™ç§å°†æ¢ç´¢æ˜¾å¼èå…¥ä¼˜åŒ–ç›®æ ‡çš„æ€è·¯æ¥æå‡ç®—æ³•æ€§èƒ½ã€‚

## exgrpo--learning-to-reason-from-experience
### Abstract
Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ExGRPOï¼šä»ç»éªŒä¸­å­¦ä¹ æ¨ç†ï¼Œé©æ–°å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ•ˆç‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°å…´èŒƒå¼ï¼Œä½†æ ‡å‡†çš„**åœ¨çº¿ç­–ç•¥ï¼ˆon - policyï¼‰è®­ç»ƒ**å­˜åœ¨ç¼ºé™·ï¼šå•æ¬¡æ¢¯åº¦æ›´æ–°åå°±ä¸¢å¼ƒrolloutï¼ˆæ¨ç†è½¨è¿¹ï¼‰ç»éªŒï¼Œæ—¢é€ æˆè®¡ç®—èµ„æºæµªè´¹ï¼Œåˆè®©æ¨¡å‹å¤±å»ä»è¿‡å¾€æˆåŠŸæ¢ç´¢ä¸­å­¦ä¹ çš„æœºä¼šï¼Œé™åˆ¶äº†å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è§„æ¨¡åŒ–åº”ç”¨ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ é¢†åŸŸå·²æœ‰å¤ç”¨ç»éªŒèƒ½å¸¦æ¥å¥½å¤„çš„è®¤çŸ¥ï¼Œä½†åœ¨å¤§æ¨¡å‹æ¨ç†åœºæ™¯ä¸‹ï¼Œç»éªŒç‰¹å¾å¦‚ä½•å½±å“å­¦ä¹ åŠ¨æ€ä»æœªè¢«å……åˆ†ç ”ç©¶ã€‚æ‰€ä»¥ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç©¶ä»€ä¹ˆæ ·çš„æ¨ç†ç»éªŒæ›´æœ‰ä»·å€¼ï¼Œå¹¶è®¾è®¡ç›¸åº”æ¡†æ¶æ¥é«˜æ•ˆåˆ©ç”¨è¿™äº›ç»éªŒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ˜ç¡®æœ‰ä»·å€¼æ¨ç†ç»éªŒçš„è¡¡é‡æŒ‡æ ‡
é¦–æ¬¡æ¢ç©¶æ¨ç†ç»éªŒä»·å€¼çš„å†³å®šå› ç´ ï¼Œé€šè¿‡ç³»ç»Ÿåˆ†æï¼Œç¡®å®š**rolloutæ­£ç¡®æ€§ï¼ˆé’ˆå¯¹é—®é¢˜ç»´åº¦ï¼‰**å’Œ**è½¨è¿¹ç†µï¼ˆé’ˆå¯¹è½¨è¿¹ç»´åº¦ï¼‰**ä½œä¸ºè¡¡é‡ç»éªŒè´¨é‡çš„æœ‰æ•ˆåœ¨çº¿ä»£ç†æŒ‡æ ‡ã€‚å³ä¸­ç­‰éš¾åº¦ä»»åŠ¡åŠå…³è”çš„ä½ç†µè½¨è¿¹ï¼Œå¯¹RLVRä¼˜åŒ–æ›´æœ‰ç›Šã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºExGRPOæ¡†æ¶
 - ç»éªŒç®¡ç†ï¼šç»´æŠ¤ä¸€ä¸ªé‡æ”¾ç¼“å†²åŒºå­˜å‚¨éƒ¨åˆ†æ­£ç¡®rolloutçš„æ¨ç†è½¨è¿¹ï¼Œå¹¶æŒ‰æ­£ç¡®æ€§æ°´å¹³åˆ†ç»„ã€‚é‡‡æ ·æ—¶ä¼˜å…ˆé€‰æ‹©æœ€æœ‰ç›Šåˆ†ç»„çš„ç»éªŒï¼Œä»¥åŠå¯¹åº”ç†µæœ€ä½çš„è½¨è¿¹ï¼Œè®©æ¨¡å‹èƒ½ä»ä¸å½“å‰èƒ½åŠ›åŒ¹é…çš„è¿‡å¾€ç»éªŒä¸­é«˜æ•ˆå­¦ä¹ ã€‚
 - ä¼˜åŒ–ç›®æ ‡ï¼šåœ¨å°æ‰¹é‡ä¼˜åŒ–é˜¶æ®µé‡‡ç”¨**æ··åˆç­–ç•¥ä¼˜åŒ–ç›®æ ‡**ï¼Œå¹³è¡¡åˆ©ç”¨æ–°æ¢ç´¢ï¼ˆfresh explorationï¼‰å’Œå¤ç”¨ç­–ç•¥æ€§é€‰æ‹©çš„è¿‡å¾€ç»éªŒï¼Œæå‡æ ·æœ¬æ•ˆç‡ä¸è®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨5ä¸ªä¸åŒå‚æ•°è§„æ¨¡ï¼ˆ1.5B - 8Bï¼‰çš„éª¨å¹²æ¨¡å‹ï¼ˆæ¶µç›–Qwenã€Llamaç­‰ç³»åˆ—ï¼‰ä¸Šï¼Œåœ¨æ•°å­¦æ¨ç†ï¼ˆå¦‚AIME24/25ã€AMCç­‰ï¼‰å’Œåˆ†å¸ƒå¤–æ¨ç†ï¼ˆå¦‚ARC - cã€GPQAç­‰ï¼‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒExGRPOå¯¹æ¯”åœ¨çº¿ç­–ç•¥RLVRåŸºçº¿å‡æœ‰æå‡ï¼šåœ¨åˆ†å¸ƒå†…ã€åˆ†å¸ƒå¤–åŸºå‡†ä¸Šå¹³å‡åˆ†åˆ«æå‡3.5ã€7.6ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œåœ¨åœ¨çº¿ç­–ç•¥æ–¹æ³•å¤±æ•ˆçš„åœºæ™¯ï¼ˆå¦‚è¾ƒå¼±çš„Llama - 3.1 8Bæ¨¡å‹è®­ç»ƒç¨³å®šåŒ–ã€è¾ƒå¼ºçš„LUFFYæ¨¡å‹æŒç»­å­¦ä¹ ï¼‰ä¸­ï¼ŒExGRPOä¹Ÿèƒ½ç¨³å®šè®­ç»ƒã€‚æ¶ˆèç­‰åˆ†æä¹ŸéªŒè¯äº†æ€§èƒ½æå‡æºäºå…¶ç»éªŒç®¡ç†ä¸ä¼˜åŒ–æœºåˆ¶å¯¹è¿‡å¾€æ¢ç´¢æ•ˆç”¨çš„æ”¾å¤§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - ç»éªŒä»·å€¼åˆ†ææ€è·¯ï¼šåœ¨å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸‹ï¼Œä»ç»éªŒç»„æˆçš„ä¸åŒç»„ä»¶ï¼ˆé—®é¢˜ã€è½¨è¿¹ï¼‰å‡ºå‘åˆ†æä»·å€¼æŒ‡æ ‡ï¼Œä¸ºåç»­ç ”ç©¶ç»éªŒç­›é€‰æä¾›äº†æ–¹æ³•è®ºå‚è€ƒã€‚
 - ç»éªŒç®¡ç†ä¸æ··åˆä¼˜åŒ–æ¡†æ¶ï¼šExGRPOä¸­æŒ‰ä»·å€¼åˆ†ç»„ç®¡ç†ç»éªŒã€æ··åˆç­–ç•¥ä¼˜åŒ–çš„è®¾è®¡ï¼Œå±•ç¤ºäº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸å¤§æ¨¡å‹ç»“åˆæ—¶ï¼Œé€šè¿‡ç²¾ç»†åŒ–ç»éªŒåˆ©ç”¨æ¥æå‡æ•ˆç‡ä¸ç¨³å®šæ€§çš„å¯è¡Œæ€§ï¼Œå¯¹æ„å»ºæ›´é«˜æ•ˆçš„å¤§æ¨¡å‹æ¨ç†å¼ºåŒ–å­¦ä¹ èŒƒå¼æœ‰å¯å‘æ„ä¹‰ã€‚
 - å¤šæ¨¡å‹å¤šä»»åŠ¡éªŒè¯ï¼šåœ¨ä¸åŒå‚æ•°è§„æ¨¡æ¨¡å‹ã€ä¸åŒæ¨ç†ä»»åŠ¡ä¸Šçš„å…¨é¢å®éªŒï¼Œè¯æ˜äº†æ–¹æ³•çš„æ³›åŒ–æ€§ï¼Œä¸ºå·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œåœ¨ç±»ä¼¼åœºæ™¯ä¸‹éªŒè¯æ–°æ–¹æ³•æä¾›äº†å®éªŒè®¾è®¡å‚è€ƒã€‚

## asymmetric-proximal-policy-optimization--mini-critics-boost-llm-reasoning
### Abstract
Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§æ¨¡å‹æ¨ç†æ–°çªç ´ï¼šAsyPPOç”¨è½»é‡â€œå°è¯„è®ºå®¶â€é‡æŒ¯RL4LLM

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRL4LLMï¼‰é¢†åŸŸï¼Œè¿‘æœŸå¤šæ•°æ–¹æ³•ä¸ºäº†è§„é¿ä¼ ç»Ÿâ€œè¯„è®ºå®¶ï¼ˆcriticï¼‰â€ä»·å€¼å‡½æ•°çš„ç¼ºé™·ï¼Œé€‰æ‹©ç”¨å¹³å‡ä¼˜åŠ¿åŸºçº¿æ›¿ä»£æ˜¾å¼è¯„è®ºå®¶ã€‚ä¼ ç»Ÿä»·å€¼å‡½æ•°åœ¨LLMè§„æ¨¡ä¸‹è®­ç»ƒè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”åœ¨ç¨€ç–å¥–åŠ±ã€é•¿æ¨ç†åºåˆ—åœºæ™¯ä¸­è¡¨ç°æ‹‰èƒ¯ã€‚æœ¬æ–‡ä»æ¶æ„è§†è§’é‡æ–°å®¡è§†è¿™ä¸€ç—›ç‚¹ï¼Œå¸Œæœ›åœ¨å¤§æ¨¡å‹åœºæ™¯ä¸‹æ—¢æ¢å¤è¯„è®ºå®¶çš„ä½œç”¨ï¼Œåˆèƒ½ä¿æŒç®—æ³•é«˜æ•ˆæ€§ï¼Œç”±æ­¤æå‡ºAsymmetric Proximal Policy Optimizationï¼ˆAsyPPOï¼‰æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè½»é‡å¤šâ€œå°è¯„è®ºå®¶â€è®¾è®¡  
AsyPPOå¼•å…¥ä¸€ç»„è½»é‡çº§çš„â€œè¿·ä½ è¯„è®ºå®¶ï¼ˆmini - criticsï¼‰â€ï¼Œæ¯ä¸ªè¯„è®ºå®¶åœ¨äº’ä¸é‡å çš„æç¤ºåˆ†ç‰‡ï¼ˆprompt shardsï¼‰ä¸Šè®­ç»ƒã€‚è¿™ç§è®¾è®¡åœ¨ä¿ç•™ä»·å€¼ä¼°è®¡æ ¡å‡†èƒ½åŠ›çš„åŒæ—¶ä¿ƒè¿›äº†å¤šæ ·æ€§ï¼Œé™ä½äº†ä»·å€¼ä¼°è®¡åå·®ã€‚ä¸åŒäºä¼ ç»Ÿå•ä¸€æˆ– heavy - weight çš„è¯„è®ºå®¶ï¼Œå¤šä¸ªå°è¯„è®ºå®¶åˆ†æ•£è®­ç»ƒï¼Œæ—¢æ§åˆ¶äº†è®¡ç®—æˆæœ¬ï¼Œåˆèƒ½ä»ä¸åŒæ•°æ®è§†è§’å­¦ä¹ ä»·å€¼ï¼Œè®©ä»·å€¼ä¼°è®¡æ›´é²æ£’ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨è¯„è®ºå®¶é—´ä¸ç¡®å®šæ€§ä¼˜åŒ–ç­–ç•¥æ›´æ–°  
ä¸€æ–¹é¢ï¼Œåœ¨è¯„è®ºå®¶æ„è§ä¸€è‡´ã€æ¢¯åº¦å­¦ä¹ ä¿¡å·å¼±çš„çŠ¶æ€ä¸‹ï¼Œå¯¹ä¼˜åŠ¿ï¼ˆadvantageï¼‰è¿›è¡Œæ©ç å¤„ç†ï¼Œé¿å…æ— æ•ˆå­¦ä¹ ï¼›å¦ä¸€æ–¹é¢ï¼Œåœ¨ç†µæ­£åˆ™åŒ–è¿‡ç¨‹ä¸­è¿‡æ»¤é«˜åˆ†æ­§çŠ¶æ€ï¼ŒæŠ‘åˆ¶æ— æ„ä¹‰çš„æ¢ç´¢è¡Œä¸ºã€‚é€šè¿‡è¿™ä¸¤æ–¹é¢åˆ©ç”¨è¯„è®ºå®¶ä¹‹é—´çš„ä¸ç¡®å®šæ€§ï¼Œè®©ç­–ç•¥æ›´æ–°æ›´ç²¾å‡†é«˜æ•ˆï¼Œæ—¢å‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—æ¶ˆè€—ï¼Œåˆæå‡äº†ç­–ç•¥å­¦ä¹ çš„è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä»…ç”¨5000æ¡å¼€æºæ•°æ®è®­ç»ƒåï¼ŒAsyPPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­è¶…è¶Šå¼ºåŸºçº¿ï¼ˆå¦‚GRPOï¼‰ã€‚åœ¨Qwen3 - 4b - Baseä¸Šæ¯”ç»å…¸PPOæ€§èƒ½æå‡è¶…6%ï¼›åœ¨Qwen3 - 8b - Baseå’ŒQwen3 - 14b - Baseä¸Šä¹Ÿåˆ†åˆ«å®ç°çº¦3%çš„æ€§èƒ½å¢ç›Šï¼Œä¸”æ— é¢å¤–æŠ€å·§åŠ æŒã€‚åŒæ—¶ï¼Œè®­ç»ƒçš„å¹³å‡æ—¶é’Ÿæ—¶é—´ä¸å³°å€¼GPUå†…å­˜ä½¿ç”¨é‡æ˜¾è‘—ä½äºç»å…¸PPOï¼Œä¿æŒåœ¨GRPOçš„æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨æ•ˆç‡ä¸æ€§èƒ½ä¸Šçš„åŒé‡ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ¶æ„åˆ›æ–°è§’åº¦ä¸ºå¤§è§„æ¨¡é«˜æ•ˆç®—æ³•è®¾è®¡æä¾›äº†æ–°æ€è·¯ï¼šå½“ä¼ ç»Ÿç»„ä»¶åœ¨å¤§æ¨¡å‹åœºæ™¯é‡é˜»æ—¶ï¼Œå¯é€šè¿‡è½»é‡åŒ–ã€åˆ†å¸ƒå¼ï¼ˆåˆ†ç‰‡è®­ç»ƒï¼‰çš„ç»„ä»¶è®¾è®¡æ¥å¤å…´å…¶ä½œç”¨ï¼›åˆ©ç”¨ç»„ä»¶é—´çš„ä¸ç¡®å®šæ€§æ¥ç²¾ç»†åŒ–ä¼˜åŒ–æ ¸å¿ƒæ›´æ–°è¿‡ç¨‹ä¹Ÿæ˜¯å€¼å¾—å€Ÿé‰´çš„æ€è·¯ï¼Œä¸ºåç»­RL4LLMç”šè‡³æ›´å¹¿æ³›çš„å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–æä¾›äº†â€œæ¶æ„é©æ–° + ä¸ç¡®å®šæ€§åˆ©ç”¨â€çš„åŒç»´åº¦å‚è€ƒèŒƒå¼ã€‚

## vogue--guiding-exploration-with-visual-uncertainty-improves-multimodal-reasoning
### Abstract
Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VOGUEï¼šå€Ÿè§†è§‰ä¸ç¡®å®šæ€§å¼•å¯¼æ¢ç´¢ï¼Œæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è™½èƒ½æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ¢ç´¢ï¼ˆexplorationï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¯¥é—®é¢˜åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­åŒæ ·çªå‡ºã€‚ç°æœ‰æ–¹æ³•æŠŠè§†è§‰è¾“å…¥å½“ä½œå›ºå®šã€ç¡®å®šæ€§æ¡ä»¶ï¼Œå¿½ç•¥äº†è§†è§‰æ¨¡æ€æœ¬èº«å­˜åœ¨çš„æ¨¡ç³Šæ€§ï¼ˆæ¯”å¦‚å›¾åƒä¸­å¯¹è±¡æœ‰æ­§ä¹‰ã€å¯è¢«å¤šç§åˆç†è§£è¯»ã€å…³é”®ç»†èŠ‚æ˜“å—åˆç†æ‰°åŠ¨æ”¹å˜ç­‰ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å½¢æˆå¯¹è§†è§‰å˜åŒ–é²æ£’çš„ç­–ç•¥ï¼Œå¯èƒ½å­¦åˆ°è™šå‡çš„è§†è§‰ - æ–‡æœ¬å…³è”è€Œéæ·±åº¦å¯æ³›åŒ–çš„æ¨ç†èƒ½åŠ›ã€‚æ‰€ä»¥ï¼Œå¦‚ä½•åˆ©ç”¨è§†è§‰ä¸ç¡®å®šæ€§æ¥é©±åŠ¨æ›´æœ‰æ•ˆçš„æ¢ç´¢ï¼Œæˆä¸ºå¾…è§£å†³çš„å…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºVOGUEæ–¹æ³•ï¼Œå°†æ¢ç´¢ä»è¾“å‡ºï¼ˆæ–‡æœ¬ï¼‰ç©ºé—´è½¬ç§»åˆ°è¾“å…¥ï¼ˆè§†è§‰ï¼‰ç©ºé—´  
VOGUEæŠŠå›¾åƒè§†ä¸ºéšæœºä¸Šä¸‹æ–‡ï¼Œå¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬æ‰§è¡ŒåŒåˆ†æ”¯å‰å‘ä¼ æ’­ï¼šâ€œåŸå§‹åˆ†æ”¯â€å¤„ç†åŸå§‹å›¾åƒï¼Œâ€œå™ªå£°åˆ†æ”¯â€å¤„ç†ç»è¯­ä¹‰ä¿ç•™æ‰°åŠ¨åçš„å›¾åƒã€‚é€šè¿‡è®¡ç®—è¿™ä¸¤ä¸ªåˆ†æ”¯è¯±å¯¼çš„ç­–ç•¥åˆ†å¸ƒé—´çš„å¯¹ç§°KLæ•£åº¦æ¥é‡åŒ–è§†è§‰ä¸ç¡®å®šæ€§ï¼Œä»¥æ­¤è¯†åˆ«æ¨¡å‹é¢„æµ‹å¯¹åˆç†è§†è§‰æ‰°åŠ¨è„†å¼±çš„çŠ¶æ€ï¼Œè¿™äº›çŠ¶æ€æ˜¯å€¼å¾—æ¢ç´¢çš„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºè§†è§‰ä¸ç¡®å®šæ€§å¡‘é€ ä¼˜åŠ¿å¹¶å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨  
ä¸€æ–¹é¢ï¼Œåœ¨å™ªå£°åˆ†æ”¯å¼•å…¥ä¸ä¸ç¡®å®šæ€§æˆæ¯”ä¾‹çš„æœ‰ä¸Šé™è§†è§‰ä¸ç¡®å®šæ€§å¥–åŠ±ï¼Œè®©æ¢ç´¢èšç„¦äºè§†è§‰æ¨¡ç³Šçš„è¾“å…¥ï¼›å¦ä¸€æ–¹é¢ï¼Œåœ¨ä¸¤ä¸ªåˆ†æ”¯éƒ½åŠ å…¥token - ç†µå¥–åŠ±ä»¥ä¿æŒç­–ç•¥çš„éšæœºæ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨é€€ç«åˆ†æ”¯é‡‡æ ·è°ƒåº¦ç­–ç•¥ï¼Œè®­ç»ƒåˆæœŸä¼˜å…ˆåŸºäºä¸ç¡®å®šæ€§é©±åŠ¨æ¢ç´¢ï¼Œè®­ç»ƒç¨³å®šåå°†ç„¦ç‚¹è½¬å‘åŸå§‹è§†å›¾ï¼Œæœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GRPOæ¡†æ¶ä¸‹ï¼ŒåŸºäºQwen2.5 - VL - 3B/7Bä¸¤ç§æ¨¡å‹è§„æ¨¡ï¼Œåœ¨6ä¸ªæ•°å­¦å’Œé€šç”¨é¢†åŸŸæ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆMathVerseã€MathVistaã€WeMathã€HallusionBenchã€ChartQAã€LogicVistaï¼‰ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒVOGUEåœ¨è§†è§‰æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡pass@1å‡†ç¡®ç‡2.6%ï¼Œåœ¨é€šç”¨é¢†åŸŸæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡3.7%ï¼›åŒæ—¶æå‡äº†pass@4æ€§èƒ½ï¼Œç¼“è§£äº†RLå¾®è°ƒä¸­å¸¸è§çš„æ¢ç´¢è¡°å‡é—®é¢˜ï¼Œä¸”è¡¨ç°ä¼˜äºä¸»è¦åœ¨çº¯æ–‡æœ¬åœºæ™¯æœ‰æ•ˆçš„Pass@k Trainingæ–¹æ³•ï¼Œåœ¨pass@1å’Œpass@kæŒ‡æ ‡ä¸Šéƒ½æœ‰æ›´ä¼˜è¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¯†åˆ«åˆ°è§†è§‰ä¸ç¡®å®šæ€§æ˜¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æ¢ç´¢çš„å…³é”®å´è¢«å¿½è§†çš„æœºåˆ¶ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†é¢†åŸŸå¼€æ‹“äº†åˆ©ç”¨è§†è§‰ä¸ç¡®å®šæ€§æå‡æ€§èƒ½çš„æ–°æ€è·¯ï¼Œåç»­ç ”ç©¶å¯å›´ç»•è§†è§‰ä¸ç¡®å®šæ€§çš„æ›´ç²¾ç»†åˆ©ç”¨å±•å¼€ã€‚
2. VOGUEçš„åŒåˆ†æ”¯æ¶æ„ã€åŸºäºä¸ç¡®å®šæ€§çš„å¥–åŠ±è®¾è®¡ä»¥åŠé€€ç«é‡‡æ ·è°ƒåº¦ç­‰å…·ä½“å®ç°æ–¹å¼ï¼Œä¸ºè§£å†³å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢ - åˆ©ç”¨å¹³è¡¡é—®é¢˜æä¾›äº†å¯å‚è€ƒçš„å·¥ç¨‹åŒ–æ–¹æ³•ï¼Œåœ¨æ”¹è¿›å¤šæ¨¡æ€æ¨¡å‹é²æ£’æ€§å’Œæ¨ç†èƒ½åŠ›çš„å®è·µä¸­å…·æœ‰å€Ÿé‰´ä»·å€¼ã€‚

## clip-low-increases-entropy-and-clip-high-decreases-entropy-in-reinforcement-learning-of-large-language-models
### Abstract
Reinforcement learning with verifiable rewards (RLVR) has recently emerged as
the leading approach for enhancing the reasoning capabilities of large language
models (LLMs). However, RLVR is prone to entropy collapse, where the LLM
quickly converges to a near-deterministic form, hindering exploration and
progress during prolonged RL training. In this work, we reveal that the
clipping mechanism in PPO and GRPO induces biases on entropy. Through
theoretical and empirical analyses, we show that clip-low increases entropy,
while clip-high decreases it. Further, under standard clipping parameters, the
effect of clip-high dominates, resulting in an overall entropy reduction even
when purely random rewards are provided to the RL algorithm. Our findings
highlight an overlooked confounding factor in RLVR: independent of the reward
signal, the clipping mechanism influences entropy, which in turn affects the
reasoning behavior. Furthermore, our analysis demonstrates that clipping can be
deliberately used to control entropy. Specifically, with a more aggressive
clip-low value, one can increase entropy, promote exploration, and ultimately
prevent entropy collapse in RLVR training.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­Clip - Lowæå‡ç†µã€Clip - Highé™ä½ç†µ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»æµæ–¹æ³•ï¼Œä½†RLVRæ˜“å‡ºç°ç†µåç¼©é—®é¢˜ï¼Œå³LLMå¿«é€Ÿæ”¶æ•›åˆ°è¿‘ä¹ç¡®å®šçš„å½¢å¼ï¼Œé˜»ç¢é•¿æ—¶é—´å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„æ¢ç´¢ä¸è¿›å±•ã€‚è€ŒPPOå’ŒGRPOä¸­çš„è£å‰ªæœºåˆ¶ä¼šå¯¹ç†µäº§ç”Ÿåå·®å½±å“ï¼Œè¿™æ˜¯æ­¤å‰è¢«å¿½è§†çš„æ··æ·†å› ç´ ï¼Œæœ¬æ–‡æ—¨åœ¨æ­ç¤ºè¯¥æœºåˆ¶å¯¹ç†µçš„å½±å“è§„å¾‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºè£å‰ªæœºåˆ¶å¯¹ç†µçš„å½±å“è§„å¾‹
é€šè¿‡ç†è®ºä¸å®è¯åˆ†æï¼Œå‘ç°clip - lowä¼šå¢åŠ ç†µï¼Œclip - highä¼šé™ä½ç†µã€‚åœ¨æ ‡å‡†è£å‰ªå‚æ•°ä¸‹ï¼Œclip - highçš„å½±å“å ä¸»å¯¼ï¼Œå³ä¾¿ç»™RLç®—æ³•çº¯éšæœºå¥–åŠ±ï¼Œæ•´ä½“ä¹Ÿä¼šå‡ºç°ç†µå‡å°‘çš„æƒ…å†µã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨è£å‰ªæ§åˆ¶ç†µä»¥é¿å…ç†µåç¼©
è¡¨æ˜è£å‰ªæœºåˆ¶å¯è¢«æœ‰æ„ç”¨äºæ§åˆ¶ç†µã€‚ä½¿ç”¨æ›´æ¿€è¿›çš„clip - lowå€¼æ—¶ï¼Œèƒ½å¤Ÿå¢åŠ ç†µã€ä¿ƒè¿›æ¢ç´¢ï¼Œæœ€ç»ˆåœ¨RLVRè®­ç»ƒä¸­é˜²æ­¢ç†µåç¼©ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­é€šè¿‡ç†è®ºåˆ†æä¸å®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†clip - lowå¢åŠ ç†µã€clip - highé™ä½ç†µè¿™ä¸€ç»“è®ºï¼Œè¿˜éªŒè¯äº†åœ¨æ ‡å‡†è£å‰ªå‚æ•°ä¸‹clip - highä¸»å¯¼å¯¼è‡´æ•´ä½“ç†µå‡ï¼Œä»¥åŠé€šè¿‡è°ƒæ•´clip - lowå¯æ§åˆ¶ç†µé˜²æ­¢ç†µåç¼©ç­‰ç›¸å…³ç»“è®ºï¼ˆæ–‡ä¸­æœªè¯¦ç»†å±•å¼€å®éªŒæ•°æ®å‘ˆç°ï¼Œä½†ä»æ‘˜è¦åŠç ”ç©¶é€»è¾‘å¯æ¨æ–­å®éªŒæ”¯æ’‘äº†ä¸Šè¿°ç†è®ºå‘ç°ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨å¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼Œå½“é¢ä¸´ç†µåç¼©é—®é¢˜æ—¶ï¼Œå¯å…³æ³¨è£å‰ªæœºåˆ¶å°¤å…¶æ˜¯clip - lowå’Œclip - highå¯¹ç†µçš„å½±å“ã€‚å¼€å‘è€…å¯ä»¥åˆ©ç”¨è£å‰ªæœºåˆ¶æ¥ä¸»åŠ¨æ§åˆ¶ç†µï¼Œæ¯”å¦‚è°ƒæ•´clip - lowå€¼æ¥å¢åŠ ç†µä»¥ä¿ƒè¿›æ¨¡å‹æ¢ç´¢ï¼Œä¸ºä¼˜åŒ–RLVRè®­ç»ƒè¿‡ç¨‹ã€æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯ä¸è°ƒæ§æ–¹å‘ï¼›åŒæ—¶è®©ç ”ç©¶è€…æ„è¯†åˆ°åœ¨RLVRä¸­ï¼Œé™¤å¥–åŠ±ä¿¡å·å¤–ï¼Œè£å‰ªæœºåˆ¶è¿™ä¸€æ­¤å‰è¢«å¿½è§†çš„å› ç´ å¯¹æ¨¡å‹æ¨ç†è¡Œä¸ºçš„å½±å“ï¼Œä¸ºåç»­ç›¸å…³ç ”ç©¶æä¾›äº†æ–°çš„æ€è€ƒè§’åº¦ã€‚

## machine-learning-algorithms-for-improving-black-box-optimization-solvers
### Abstract
Black-box optimization (BBO) addresses problems where objectives are
accessible only through costly queries without gradients or explicit structure.
Classical derivative-free methods -- line search, direct search, and
model-based solvers such as Bayesian optimization -- form the backbone of BBO,
yet often struggle in high-dimensional, noisy, or mixed-integer settings.
  Recent advances use machine learning (ML) and reinforcement learning (RL) to
enhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning
portfolios, and generative models, while RL enables dynamic operator
configuration, robustness, and meta-optimization across tasks.
  This paper surveys these developments, covering representative algorithms
such as NNs with the modular model-based optimization framework (mlrMBO),
zeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO),
distributed block-wise optimization (DiBB), partition-based Bayesian
optimization (SPBOpt), the transformer-based optimizer (B2Opt),
diffusion-model-based BBO, surrogate-assisted RL for differential evolution
(Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with
relative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD),
policy improvement with black-box (PIBB), and offline Q-learning with Mamba
backbones (Q-Mamba).
  We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and
the MetaBox framework. Overall, we highlight how ML and RL transform classical
inexact solvers into more scalable, robust, and adaptive frameworks for
real-world optimization.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨æœºå™¨å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ èµ‹èƒ½é»‘ç®±ä¼˜åŒ–ï¼šå‰æ²¿æ–¹æ³•ä¸å®è·µå…¨æ™¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é»‘ç®±ä¼˜åŒ–ï¼ˆBBOï¼‰èšç„¦äºç›®æ ‡å‡½æ•°ä»…èƒ½é€šè¿‡æ˜‚è´µæŸ¥è¯¢è·å–ã€æ— æ¢¯åº¦æˆ–æ˜¾å¼ç»“æ„çš„ä¼˜åŒ–åœºæ™¯ï¼Œåƒç§‘å­¦æ¨¡æ‹Ÿã€å·¥ç¨‹å®éªŒè¿™ç±»éœ€é«˜æˆæœ¬è¯„ä¼°çš„ä»»åŠ¡éƒ½å±äºæ­¤èŒƒç•´ã€‚ä¼ ç»Ÿæ— å¯¼æ•°æ–¹æ³•ï¼ˆå¦‚çº¿æœç´¢ã€ç›´æ¥æœç´¢ã€è´å¶æ–¯ä¼˜åŒ–ç­‰ï¼‰è™½ä¸ºBBOæ ¸å¿ƒï¼Œä½†åœ¨é«˜ç»´ã€å«å™ªã€æ··åˆæ•´æ•°ç­‰å¤æ‚åœºæ™¯ä¸‹è¡¨ç°ä¹åŠ›ã€‚è€Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å…´èµ·ï¼Œä¸ºçªç ´è¿™äº›ç“¶é¢ˆæä¾›äº†æ–°è·¯å¾„â€”â€”MLèƒ½æ„å»ºæ›´å…·è¡¨è¾¾åŠ›çš„ä»£ç†æ¨¡å‹ã€å®ç°è‡ªé€‚åº”æ›´æ–°ä¸å…ƒå­¦ä¹ ï¼ŒRLå¯åŠ¨æ€é…ç½®ç®—å­ã€æå‡é²æ£’æ€§å¹¶è·¨ä»»åŠ¡å…ƒä¼˜åŒ–ã€‚æœ¬æ–‡æ­£æ˜¯ç„å‡†â€œå¦‚ä½•ç”¨ML/RLå¢å¼ºç»å…¸BBOæ±‚è§£å™¨â€è¿™ä¸€æ–¹å‘ï¼Œç³»ç»Ÿæ€§æ¢³ç†å‰æ²¿è¿›å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€æ¢³ç†BBOæ–¹æ³•è°±ç³»ä¸ML/RLå¢å¼ºé€»è¾‘  
æ–‡ç« å…ˆå¯¹ç»å…¸BBOæ–¹æ³•ï¼ˆ polling - åŸºäºè½®è¯¢ã€surrogate - åŸºäºä»£ç†ã€local - approximation - åŸºäºå±€éƒ¨è¿‘ä¼¼ç­‰ ï¼‰åšäº†ç»Ÿä¸€åˆ†ç±»ï¼Œæ˜ç¡®å®ƒä»¬çš„ç®—æ³•åŸºçŸ³è§’è‰²ï¼›å†å°†ML/RLçš„å‰æ²¿è¿›å±•å®šä½ä¸ºâ€œå¯¹ç»å…¸æ–¹æ³•çš„å¢å¼ºå»¶ä¼¸â€ï¼Œæ¯”å¦‚MLé€šè¿‡ä»£ç†æ¨¡å‹åŠ é€Ÿæœç´¢ã€RLé€šè¿‡å­¦ä¹ ç­–ç•¥åŠ¨æ€åˆ†é…èµ„æºç­‰ï¼Œæ­å»ºèµ·ä»ä¼ ç»Ÿåˆ°ç°ä»£çš„æŠ€æœ¯ä¼ æ‰¿ä¸åˆ›æ–°è„‰ç»œã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢è¦†ç›–MLå¢å¼ºBBOçš„ä»£è¡¨æ€§æ¡†æ¶  
è¯¦ç»†ç›˜ç‚¹äº†è¯¸å¤šMLé©±åŠ¨çš„BBOæ–¹æ¡ˆï¼š  
- **æ¨¡å—åŒ–æ¨¡å‹ä¼˜åŒ–ï¼ˆmlrMBOï¼‰**ï¼šå€ŸåŠ©ç¥ç»ç½‘ç»œç­‰æ„å»ºæ¨¡å—åŒ–ä»£ç†ï¼Œä¸ºæ¨¡å‹åŸºä¼˜åŒ–æä¾›çµæ´»æ¡†æ¶ï¼›  
- **é›¶é˜¶è‡ªé€‚åº”åŠ¨é‡æ³•ï¼ˆZO - AdaMMï¼‰**ï¼šå¯å‘è‡ªä¼˜åŒ–å™¨æ€è·¯ï¼Œæ— æ¢¯åº¦åœºæ™¯ä¸‹å®ç°ç±»åŠ¨é‡çš„è‡ªé€‚åº”æ›´æ–°ï¼›  
- **è‡ªåŠ¨åŒ–BBOï¼ˆABBOï¼‰ä¸åˆ†å¸ƒå¼å—ä¼˜åŒ–ï¼ˆDiBBï¼‰**ï¼šå‰è€…æ˜¯å…ƒå­¦ä¹ ç»„åˆç­–ç•¥é€‰ä¼˜ï¼Œåè€…æ˜¯åˆ†å¸ƒå¼åœºæ™¯ä¸‹çš„åˆ†å—ä¼˜åŒ–ï¼Œéƒ½ç„å‡†æ›´é«˜æ•ˆçš„æœç´¢é€»è¾‘ï¼›  
- **ç”Ÿæˆå¼ä¼˜åŒ–ï¼ˆB2Optã€DiffBBOï¼‰**ï¼šç”¨Transformerã€æ‰©æ•£æ¨¡å‹è¿™ç±»ç”Ÿæˆæ¨¡å‹èµ‹èƒ½BBOï¼Œä»ç”Ÿæˆè§’åº¦æ‹“å±•æœç´¢ç©ºé—´æ¢ç´¢æ–¹å¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ·±å…¥å‰–æRLå¢å¼ºBBOçš„å…³é”®æ–¹å‘  
å›´ç»•é²æ£’æ€§ã€åŠ¨æ€é…ç½®ã€ç­–ç•¥æœç´¢ç­‰ç»´åº¦ï¼Œè§£æRLåœ¨BBOé‡Œçš„åˆ›æ–°ï¼š  
- **é²æ£’ä¼˜åŒ–ï¼ˆRBOã€LB - SGDã€CAS - MOREï¼‰**ï¼šé’ˆå¯¹å«å™ªã€çº¦æŸç­‰å¤æ‚åœºæ™¯ï¼Œç”¨RLä¿éšœä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ä¸å®‰å…¨æ€§ï¼›  
- **åŠ¨æ€ç®—å­é…ç½®ï¼ˆSurr - RLDEï¼‰**ï¼šä»¥å¼ºåŒ–å­¦ä¹ ä¸ºå·®åˆ†è¿›åŒ–é…ç½®ç®—å­ï¼Œè®©ç®—æ³•æ›´é€‚é…ä»»åŠ¡ï¼›  
- **ç­–ç•¥æ”¹è¿›ä¸ç¦»çº¿å…ƒä¼˜åŒ–ï¼ˆPIBBã€Q - Mambaï¼‰**ï¼šå‰è€…æ‰“é€šBBOä¸RLçš„ç­–ç•¥æ›´æ–°é“¾è·¯ï¼Œåè€…åŸºäºMambaæ¶æ„åšç¦»çº¿Qå­¦ä¹ å®ç°å…ƒçº§BBOï¼Œä»ç­–ç•¥å­¦ä¹ å±‚é¢é©æ–°ä¼˜åŒ–èŒƒå¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šé‡è§†åŸºå‡†æµ‹è¯•ä¸å¯å¤ç°æ€§ç”Ÿæ€  
ä¸“é—¨æ¢³ç†äº†NeurIPS 2020 BBOæŒ‘æˆ˜èµ›ã€MetaBoxæ¡†æ¶ç­‰ benchmark å·¥ä½œï¼Œä¸ºä¸åŒBBOæ–¹æ³•çš„å…¬å¹³å¯¹æ¯”å»ºç«‹æ ‡å‡†åŒ–åè®®ï¼Œæ¨åŠ¨é¢†åŸŸå†…æ–¹æ³•çš„å¯å¤ç°ä¸è¿­ä»£ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­è™½æœªå±•å¼€å•ä¸€å®éªŒçš„å®šé‡å¯¹æ¯”è¡¨ï¼Œä½†é€šè¿‡å¯¹å„ç±»æ–¹æ³•â€œè®¾è®¡ç†å¿µ - é€‚ç”¨åœºæ™¯ - åˆ›æ–°é€»è¾‘â€çš„å‰–æï¼Œé—´æ¥å±•ç°äº†ML/RLå¢å¼ºåçš„BBOæ–¹æ¡ˆåœ¨**é«˜ç»´åœºæ™¯é€‚åº”æ€§ã€å«å™ªç¯å¢ƒé²æ£’æ€§ã€è·¨ä»»åŠ¡æ³›åŒ–æ€§**ç­‰ç»´åº¦ç›¸è¾ƒç»å…¸æ–¹æ³•çš„ä¼˜åŠ¿ï¼›åŒæ—¶ï¼ŒåŸºå‡†æµ‹è¯•ç”Ÿæ€ï¼ˆå¦‚BBO Challengeã€MetaBoxï¼‰çš„ä»‹ç»ï¼Œä¹Ÿä¸ºåç»­ç ”ç©¶è€…å¤ç°ã€å¯¹æ¯”ä¸åŒæ–¹æ³•æ€§èƒ½æä¾›äº†å‚ç…§æ ‡å°ºï¼Œä¾§é¢éªŒè¯äº†æ–°æ–¹æ³•åœ¨çœŸå®ä»»åŠ¡é‡Œæ¥å—æ£€éªŒçš„å¯è¡Œæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æŠ€æœ¯èåˆè§†è§’**ï¼šæ¸…æ™°å±•ç¤ºäº†MLï¼ˆä»£ç†ã€ç”Ÿæˆã€å…ƒå­¦ä¹ ï¼‰ä¸RLï¼ˆç­–ç•¥ã€é²æ£’ã€å…ƒä¼˜åŒ–ï¼‰å¦‚ä½•å’ŒBBOâ€œå¯¹ç—‡ä¸‹è¯â€ç»“åˆï¼Œä¸ºå…¶ä»–é¢†åŸŸé‡Œâ€œä¼ ç»Ÿæ–¹æ³• + æ–°å…´æ™ºèƒ½æŠ€æœ¯â€çš„èåˆåˆ›æ–°æä¾›æ€è·¯èŒƒå¼ï¼›  
2. **æ–¹æ³•æ¢³ç†ä»·å€¼**ï¼šå¯¹æ•°åç§å‰æ²¿BBOå¢å¼ºç®—æ³•çš„åˆ†ç±»ã€åŸç†è§£è¯»ï¼Œæ˜¯é¢†åŸŸæ–°äººå¿«é€Ÿå»ºç«‹çŸ¥è¯†ä½“ç³»ã€ç ”ç©¶è€…è¿½è¸ªå‰æ²¿çš„ä¼˜è´¨å‚è€ƒï¼›  
3. **åŸºå‡†ç”Ÿæ€å»ºè®¾**ï¼šå¼ºè°ƒ benchmark ä¸å¯å¤ç°æ€§ï¼Œæé†’ä»ä¸šè€…åœ¨ç®—æ³•åˆ›æ–°æ—¶å…³æ³¨â€œå…¬å¹³å¯¹æ¯” - ç»“æœå¯å¤ç° - ç”Ÿæ€å…±å»ºâ€ï¼ŒåŠ©åŠ›é¢†åŸŸè‰¯æ€§å‘å±•ï¼›  
4. **åœºæ™¯å¯¼å‘æ€ç»´**ï¼šå§‹ç»ˆå›´ç»•â€œé«˜ç»´ã€å«å™ªã€å¤æ‚çº¦æŸâ€ç­‰çœŸå®BBOç—›ç‚¹å±•å¼€æ–¹æ³•è®¾è®¡ä¸åˆ†æï¼Œå¼•å¯¼æŠ€æœ¯è½åœ°è¦é”šå®šå®é™…åœºæ™¯éœ€æ±‚ã€‚  


è¿™ç¯‡ç»¼è¿°çŠ¹å¦‚ä¸€å¹…â€œé»‘ç®±ä¼˜åŒ– + æœºå™¨å­¦ä¹ /å¼ºåŒ–å­¦ä¹ â€çš„å…¨æ™¯å¯¼èˆªå›¾ï¼Œæ—¢å¸®è¯»è€…ç†æ¸…ç»å…¸åˆ°å‰æ²¿çš„æŠ€æœ¯è„‰ç»œï¼Œåˆä¸ºåç»­ç ”ç©¶æŒ‡æ˜äº†â€œèåˆåˆ›æ–° + åœºæ™¯è½åœ° + ç”Ÿæ€å…±å»ºâ€çš„æ–¹å‘~ 

## rethinking-entropy-regularization-in-large-reasoning-models
### Abstract
Reinforcement learning with verifiable rewards (RLVR) has shown great promise
in enhancing the reasoning abilities of large reasoning models (LRMs). However,
it suffers from a critical issue: entropy collapse and premature convergence.
Naive entropy regularization, a common approach for encouraging exploration in
the traditional RL literature, fails to address this problem in the context of
LRM. Our analysis reveals that this failure stems from the vast action space
and long trajectories in LRMs, which easily trigger a global entropy explosion
as the model indiscriminately explores all possible actions and states. To
address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method
that confines exploration to a meaningful subset of actions and states. SIREN
achieves this through a two-step entropy masking mechanism, consisting of a
top-p mask and a peak-entropy mask. In addition, regularization is transformed
into a self-anchored form to stabilize training. Across five mathematical
benchmarks, SIREN attains superior average performance over previous
entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on
AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes
greater response diversity and maintains entropy at an appropriate level, which
helps to preserve the validation pass@k throughout training. This effectively
mitigates the premature convergence problem common in RLVR for LRM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é‡æ–°æ€è€ƒå¤§æ¨ç†æ¨¡å‹ä¸­çš„ç†µæ­£åˆ™åŒ–ï¼šSIRENæ–¹æ³•ç ´å±€ç†µåç¼©ä¸æ—©ç†Ÿæ”¶æ•›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¢å¼ºå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œå¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒé¢ä¸´ç€ç†µåç¼©å’Œæ—©ç†Ÿæ”¶æ•›è¿™ä¸€å…³é”®é—®é¢˜ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸­å¸¸ç”¨çš„æœ´ç´ ç†µæ­£åˆ™åŒ–æ–¹æ³•ï¼Œåœ¨LRMsåœºæ™¯ä¸‹æ— æ³•è§£å†³è¯¥é—®é¢˜ã€‚ç»åˆ†æï¼Œè¿™æ˜¯å› ä¸ºLRMså­˜åœ¨å·¨å¤§çš„åŠ¨ä½œç©ºé—´å’Œé•¿è½¨è¿¹ï¼Œæ¨¡å‹æ— å·®åˆ«æ¢ç´¢æ‰€æœ‰å¯èƒ½åŠ¨ä½œå’ŒçŠ¶æ€æ—¶æ˜“å¼•å‘å…¨å±€ç†µçˆ†ç‚¸ã€‚æ‰€ä»¥ï¼Œéœ€è¦ä¸€ç§æ–°æ–¹æ³•æ¥é™åˆ¶æ¢ç´¢åœ¨æœ‰æ„ä¹‰çš„åŠ¨ä½œå’ŒçŠ¶æ€å­é›†å†…ï¼Œä»¥æ­¤è§£å†³ç†µç›¸å…³é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSIRENï¼ˆSelectIve entRopy rEgularizatioNï¼‰æ–¹æ³• 
SIRENé€šè¿‡ä¸¤æ­¥ç†µæ©ç æœºåˆ¶æ¥é™åˆ¶æ¢ç´¢èŒƒå›´ï¼Œè¯¥æœºåˆ¶ç”±top - pæ©ç å’Œå³°å€¼ç†µæ©ç ç»„æˆï¼Œå°†æ¢ç´¢é™åˆ¶åœ¨æœ‰æ„ä¹‰çš„åŠ¨ä½œå’ŒçŠ¶æ€å­é›†é‡Œï¼Œé¿å…æ— å·®åˆ«æ¢ç´¢å¯¼è‡´çš„é—®é¢˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ­£åˆ™åŒ–å½¢å¼è½¬æ¢ 
æŠŠæ­£åˆ™åŒ–è½¬æ¢ä¸ºè‡ªé”šå®šå½¢å¼ï¼Œè¿™ç§å½¢å¼èƒ½å¤Ÿç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ›´ç¨³å®šåœ°å­¦ä¹ ï¼Œæœ‰åŠ©äºè§£å†³ä¹‹å‰å­˜åœ¨çš„è®­ç»ƒä¸ç¨³å®šç­‰é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSIRENç›¸è¾ƒäºä¹‹å‰ä¸ç†µç›¸å…³çš„RLVRæ–¹æ³•å–å¾—äº†æ›´ä¼˜çš„å¹³å‡æ€§èƒ½ã€‚ä¾‹å¦‚åœ¨AIME24/25æ•°æ®é›†ä¸Šä½¿ç”¨Qwen2.5 - Math - 7Bæ¨¡å‹æ—¶ï¼Œå®ç°äº†+6.6 maj@kçš„æ€§èƒ½æå‡ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒSIRENèƒ½ä¿ƒè¿›å“åº”å¤šæ ·æ€§æå‡ï¼ŒåŒæ—¶å°†ç†µç»´æŒåœ¨åˆé€‚æ°´å¹³ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰åŠ©äºä¿æŒéªŒè¯pass@kï¼Œæœ‰æ•ˆç¼“è§£äº†LRMsçš„RLVRä¸­å¸¸è§çš„æ—©ç†Ÿæ”¶æ•›é—®é¢˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºç ”ç©¶å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€è§£å†³æ¨¡å‹è®­ç»ƒä¸­ç†µç›¸å…³é—®é¢˜ï¼ˆå¦‚ç†µåç¼©ã€æ—©ç†Ÿæ”¶æ•›ï¼‰çš„ç ”ç©¶è€…æ¥è¯´ï¼ŒSIRENçš„ä¸¤æ­¥ç†µæ©ç æœºåˆ¶æä¾›äº†ä¸€ç§é™åˆ¶æœ‰æ•ˆæ¢ç´¢èŒƒå›´çš„æ–°æ€è·¯ï¼Œè‡ªé”šå®šå½¢å¼çš„æ­£åˆ™åŒ–ä¹Ÿä¸ºç¨³å®šè®­ç»ƒè¿‡ç¨‹æä¾›äº†å‚è€ƒæ–¹å‘ï¼›åœ¨å®é™…å¤§æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯æ¨ç†ç±»å¤§æ¨¡å‹ï¼‰çš„è®­ç»ƒä¼˜åŒ–å·¥ä½œä¸­ï¼Œå¯å€Ÿé‰´è¯¥æ–¹æ³•æ¥æå‡æ¨¡å‹æ€§èƒ½ä¸è®­ç»ƒç¨³å®šæ€§ï¼Œè§£å†³æ¢ç´¢è¿‡ç¨‹ä¸­çš„ä¸åˆç†ç†µå˜åŒ–é—®é¢˜ã€‚

## adanav--adaptive-reasoning-with-uncertainty-for-vision-language-navigation
### Abstract
Vision Language Navigation (VLN) requires agents to follow natural language
instructions by grounding them in sequential visual observations over long
horizons. Explicit reasoning could enhance temporal consistency and perception
action alignment, but reasoning at fixed steps often leads to suboptimal
performance and unnecessary computation. To address this, we propose AdaNav, an
uncertainty-based adaptive reasoning framework for VLN. At its core is the
Uncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that
dynamically triggers reasoning. We introduce Action Entropy as a policy prior
for UAR and progressively refine it through a Heuristics to RL training method,
enabling agents to learn difficulty aware reasoning policies under the strict
data limitations of embodied tasks. Results show that with only 6K training
samples, AdaNav achieves substantial gains over closed source models trained on
million scale data, improving success rate by 20% on R2R val-unseen, 11.7% on
RxR-CE, and 11.4% in real world scenes. The code is available at
https://github.com/xinding-sys/AdaNav.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdaNavï¼šåŸºäºä¸ç¡®å®šæ€§è‡ªé€‚åº”æ¨ç†çš„è§†è§‰è¯­è¨€å¯¼èˆªæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰ - è¯­è¨€å¯¼èˆªï¼ˆVision - Language Navigationï¼ŒVLNï¼‰ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“ä¾æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œåœ¨é•¿æ—¶é—´åºåˆ—çš„è§†è§‰è§‚æµ‹ä¸­å®ŒæˆæŒ‡ä»¤è½åœ°æ‰§è¡Œã€‚æ˜¾å¼æ¨ç†è™½èƒ½å¢å¼ºæ—¶é—´ä¸€è‡´æ€§ä¸æ„ŸçŸ¥ - åŠ¨ä½œå¯¹é½æ€§ï¼Œä½†å›ºå®šæ­¥éª¤çš„æ¨ç†å¾€å¾€å¯¼è‡´æ€§èƒ½æ¬ ä½³ä¸è®¡ç®—èµ„æºæµªè´¹ã€‚å¹¶ä¸”åœ¨å…·èº«ä»»åŠ¡ä¸¥æ ¼çš„æ•°æ®é™åˆ¶ä¸‹ï¼Œå¦‚ä½•è®©æ™ºèƒ½ä½“å­¦ä¹ åˆ°éš¾åº¦æ„ŸçŸ¥çš„æ¨ç†ç­–ç•¥ä¹Ÿæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºAdaNavæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAdaNavæ¡†æ¶ä¸ä¸ç¡®å®šæ€§è‡ªé€‚åº”æ¨ç†å—ï¼ˆUARï¼‰
AdaNavæ˜¯ä¸€ä¸ªåŸºäºä¸ç¡®å®šæ€§çš„è§†è§‰ - è¯­è¨€å¯¼èˆªè‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯è½»é‡çº§æ’ä»¶UARï¼Œè¯¥æ’ä»¶èƒ½å¤ŸåŠ¨æ€è§¦å‘æ¨ç†è¿‡ç¨‹ï¼Œä¸å†é‡‡ç”¨å›ºå®šæ­¥éª¤æ¨ç†ï¼Œä»è€Œé¿å…ä¸å¿…è¦è®¡ç®—ä¸æ€§èƒ½çŸ­æ¿ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥Action Entropyä½œä¸ºUARçš„ç­–ç•¥å…ˆéªŒå¹¶é‡‡ç”¨Heuristics to RLè®­ç»ƒæ–¹æ³•
å°†Action Entropyä½œä¸ºUARçš„ç­–ç•¥å…ˆéªŒï¼Œå¹¶ä¸”é€šè¿‡Heuristics to RLè®­ç»ƒæ–¹æ³•å¯¹å…¶è¿›è¡Œé€æ­¥ä¼˜åŒ–ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å…·èº«ä»»åŠ¡çš„æ•°æ®é™åˆ¶ä¸‹ï¼Œå­¦ä¹ åˆ°éš¾åº¦æ„ŸçŸ¥çš„æ¨ç†ç­–ç•¥ï¼Œè®©æ™ºèƒ½ä½“å¯ä»¥æ ¹æ®ä»»åŠ¡éš¾åº¦ç­‰æƒ…å†µè‡ªé€‚åº”åœ°è¿›è¡Œæ¨ç†å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°æ®é‡ä»…æœ‰6Kè®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒAdaNavç›¸æ¯”åœ¨ç™¾ä¸‡çº§æ•°æ®ä¸Šè®­ç»ƒçš„é—­æºæ¨¡å‹å–å¾—äº†æ˜¾è‘—æå‡ï¼šåœ¨R2R val - unseenæ•°æ®é›†ä¸ŠæˆåŠŸç‡æå‡20%ï¼›åœ¨RxR - CEæ•°æ®é›†ä¸Šæå‡11.7%ï¼›åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ä¹Ÿæå‡äº†11.4%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªé€‚åº”æ¨ç†ç†å¿µï¼šæ‰“ç ´å›ºå®šæ­¥éª¤æ¨ç†çš„å±€é™ï¼Œé‡‡ç”¨åŸºäºä¸ç¡®å®šæ€§çš„åŠ¨æ€æ¨ç†è§¦å‘æœºåˆ¶ï¼Œä¸ºè§£å†³åºåˆ—ä»»åŠ¡ä¸­æ¨ç†æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–éœ€è¦åºåˆ—æ¨ç†çš„ä»»åŠ¡åœºæ™¯ï¼Œå¦‚æœºå™¨äººæ“ä½œåºåˆ—å†³ç­–ç­‰ã€‚
2. ç­–ç•¥å…ˆéªŒä¸è®­ç»ƒæ–¹æ³•ï¼šå¼•å…¥Action Entropyä½œä¸ºç­–ç•¥å…ˆéªŒå¹¶ç»“åˆHeuristics to RLè®­ç»ƒæ–¹æ³•ï¼Œåœ¨æ•°æ®æœ‰é™åœºæ™¯ä¸‹æœ‰æ•ˆå­¦ä¹ éš¾åº¦æ„ŸçŸ¥ç­–ç•¥ï¼Œè¿™ç§åœ¨æœ‰é™æ•°æ®ä¸‹çš„ç­–ç•¥å­¦ä¹ æ–¹å¼å¯ä¸ºå…·èº«æ™ºèƒ½ç­‰æ•°æ®ç¨€ç¼ºé¢†åŸŸçš„ç®—æ³•è®¾è®¡æä¾›å‚è€ƒã€‚
3. è½»é‡çº§æ’ä»¶è®¾è®¡ï¼šUARä½œä¸ºè½»é‡çº§æ’ä»¶çš„è®¾è®¡æ€è·¯ï¼Œä¾¿äºåœ¨ç°æœ‰ç³»ç»Ÿä¸­é›†æˆå¤ç”¨ï¼Œä¸ºæå‡ç°æœ‰è§†è§‰ - è¯­è¨€æˆ–å…¶ä»–å¤šæ¨¡æ€ç³»ç»Ÿæ€§èƒ½æä¾›äº†è½»é‡é«˜æ•ˆçš„æ”¹é€ æ–¹å‘ã€‚

## efficient-multi-turn-rl-for-gui-agents-via-decoupled-training-and-adaptive-data-curation
### Abstract
Vision-language model (VLM) based GUI agents show promise for automating
complex desktop and mobile tasks, but face significant challenges in applying
reinforcement learning (RL): (1) slow multi-turn interactions with GUI
environments for policy rollout, and (2) insufficient high-quality
agent-environment interactions for policy learning. To address these
challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI
agents, which coordinates heterogeneous modules in a highly decoupled manner.
DART separates the training system into four asynchronous modules: environment
cluster, rollout service, data manager, and trainer. This design enables
non-blocking communication, asynchronous training, rollout-wise trajectory
sampling, and per-worker model synchronization, significantly improving the
system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,
and 5.5* environment utilization. To facilitate effective learning from
abundant samples, we introduce an adaptive data curation scheme: (1)
pre-collecting successful trajectories for challenging tasks to supplement
sparse success in online sampling; (2) dynamically adjusting rollout numbers
and trajectory lengths based on task difficulty; (3) training selectively on
high-entropy steps to prioritize critical decisions; (4) stabilizing learning
via truncated importance sampling for policy mismatch between policy rollout
and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task
success rate, a 14.61% absolute gain over the base model, and 7.34% higher than
open-source SOTA. We will fully open-source our training framework, data, and
model checkpoints via computer-use-agents.github.io/dart-gui, which we believe
is a timely contribution to the open-source community of agentic RL training.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜æ•ˆå¤šè½®GUIæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼šDARTæ¡†æ¶çš„çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºè§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„GUIæ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–å¤æ‚æ¡Œé¢å’Œç§»åŠ¨ä»»åŠ¡æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ä¸GUIç¯å¢ƒè¿›è¡Œå¤šè½®äº¤äº’ä»¥è¿›è¡Œç­–ç•¥æ¨æ¼”æ—¶é€Ÿåº¦ç¼“æ…¢ï¼›äºŒæ˜¯ç”¨äºç­–ç•¥å­¦ä¹ çš„é«˜è´¨é‡æ™ºèƒ½ä½“ - ç¯å¢ƒäº¤äº’æ•°æ®ä¸è¶³ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†GUIæ™ºèƒ½ä½“åœ¨å¼ºåŒ–å­¦ä¹ æ–¹å‘çš„é«˜æ•ˆè®­ç»ƒä¸æ€§èƒ½æå‡ï¼Œå› æ­¤éœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDARTæ¡†æ¶ï¼Œè§£è€¦è®­ç»ƒç³»ç»Ÿæ¨¡å—
DARTï¼ˆDecoupled Agentic RL Training frameworkï¼‰å°†è®­ç»ƒç³»ç»Ÿè§£è€¦ä¸ºç¯å¢ƒé›†ç¾¤ã€æ¨æ¼”æœåŠ¡ã€æ•°æ®ç®¡ç†å™¨å’Œè®­ç»ƒå™¨è¿™å››ä¸ªå¼‚æ­¥æ¨¡å—ã€‚è¿™ç§è®¾è®¡å®ç°äº†éé˜»å¡é€šä¿¡ã€å¼‚æ­¥è®­ç»ƒã€æŒ‰æ¨æ¼”é‡‡æ ·è½¨è¿¹ä»¥åŠæ¯ä¸ªå·¥ä½œå™¨çš„æ¨¡å‹åŒæ­¥ï¼Œå¤§å¹…æå‡äº†ç³»ç»Ÿæ•ˆç‡ï¼Œå¦‚æ¨æ¼”çš„GPUåˆ©ç”¨ç‡æå‡1.6å€ã€è®­ç»ƒååé‡æå‡1.9å€ã€ç¯å¢ƒåˆ©ç”¨ç‡æå‡5.5å€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªé€‚åº”æ•°æ®ç­–é€‰æ–¹æ¡ˆ
ä¸ºäº†ä»å¤§é‡æ ·æœ¬ä¸­æœ‰æ•ˆå­¦ä¹ ï¼Œå¼•å…¥äº†è‡ªé€‚åº”æ•°æ®ç­–é€‰æ–¹æ¡ˆã€‚åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡é¢„å…ˆæ”¶é›†æˆåŠŸè½¨è¿¹ï¼Œè¡¥å……åœ¨çº¿é‡‡æ ·ä¸­ç¨€ç–çš„æˆåŠŸæ¡ˆä¾‹ï¼›ï¼ˆ2ï¼‰æ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´æ¨æ¼”æ¬¡æ•°å’Œè½¨è¿¹é•¿åº¦ï¼›ï¼ˆ3ï¼‰æœ‰é€‰æ‹©åœ°åœ¨é«˜ç†µæ­¥éª¤ä¸Šè®­ç»ƒï¼Œä¼˜å…ˆå¤„ç†å…³é”®å†³ç­–ï¼›ï¼ˆ4ï¼‰é€šè¿‡æˆªæ–­é‡è¦æ€§é‡‡æ ·ç¨³å®šå­¦ä¹ ï¼Œåº”å¯¹ç­–ç•¥æ¨æ¼”å’Œæ›´æ–°ä¹‹é—´çš„ç­–ç•¥ä¸åŒ¹é…é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDART - GUI - 7Bå®ç°äº†42.13%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ¯”åŸºç¡€æ¨¡å‹ç»å¯¹æå‡14.61%ï¼Œæ¯”å¼€æºSOTAï¼ˆæœ€å…ˆè¿›ï¼‰æ¨¡å‹é«˜å‡º7.34%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç³»ç»Ÿè®¾è®¡è§’åº¦ï¼Œè§£è€¦å¼çš„æ¨¡å—è®¾è®¡æ€è·¯ä¸ºæå‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿæ•ˆç‡æä¾›äº†å‚è€ƒï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦å¤šæ¨¡å—åä½œçš„æ™ºèƒ½ä½“è®­ç»ƒåœºæ™¯ï¼›åœ¨æ•°æ®å¤„ç†æ–¹é¢ï¼Œè‡ªé€‚åº”æ•°æ®ç­–é€‰çš„å¤šç§æ‰‹æ®µï¼Œå¦‚é¢„æ”¶é›†æˆåŠŸè½¨è¿¹ã€åŠ¨æ€è°ƒæ•´æ¨æ¼”å‚æ•°ã€åŸºäºç†µçš„è®­ç»ƒé€‰æ‹©ä»¥åŠæˆªæ–­é‡è¦æ€§é‡‡æ ·ç­‰ï¼Œä¸ºè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æ•°æ®è´¨é‡å’Œåˆ©ç”¨æ•ˆç‡é—®é¢˜æä¾›äº†å…¨é¢çš„æ–¹æ³•å€Ÿé‰´ï¼Œæœ‰åŠ©äºå…¶ä»–ç ”ç©¶è€…åœ¨å¤„ç†ç±»ä¼¼æ•°æ®ç¨€ç–ã€ç­–ç•¥ä¸ç¨³å®šç­‰é—®é¢˜æ—¶è·å¾—å¯å‘ã€‚åŒæ—¶ï¼Œè¯¥å›¢é˜Ÿè¿˜è®¡åˆ’å¼€æºè®­ç»ƒæ¡†æ¶ã€æ•°æ®å’Œæ¨¡å‹ checkpointï¼Œè¿™å¯¹å¼€æºç¤¾åŒºåœ¨æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹å‘çš„å‘å±•ä¹Ÿå…·æœ‰æ¨åŠ¨ä½œç”¨ã€‚

## c$^2$gspg--confidence-calibrated-group-sequence-policy-gradient-towards-self-aware-reasoning
### Abstract
Reinforcement Learning (RL) methods, exemplified by Group Relative Policy
Optimization (GRPO) and its variants, play a central role in developing
reasoning models. However, these methods often suffer from a critical
overconfidence issue, which prevents them from achieving self-aware reasoning
models. In this study, we propose a simple yet effective confidence-calibration
group sequence policy gradient method, called C$^2$GSPG, which simultaneously
enhances reasoning performance while suppressing overconfidence. In principle,
we propose a Group Sequence Policy Gradient (GSPG) framework for learning
reasoning models, which eliminates the token-level bias commonly appearing in
GRPO and its variants. In this framework, we define the model confidence for
each reasoning problem using the normalized sequence-level probability, and
then apply a cross-entropy regularizer to calibrate the model confidence to the
sequence's reward. We demonstrate that the confidence calibration regularizer
and GSPG are collaborative for binary rewards, as their objectives always share
the same gradient direction. For non-binary rewards, we apply nonlinear reward
normalization and adaptive regularizer clipping, mitigating the potential
conflict between the two objectives. Applying C$^2$GSPG to post-train large
language models in logical and mathematical reasoning tasks, we show its
superiority over state-of-the-art methods in both reasoning accuracy and
confidence calibration. The code of C$^2$GSPG is available at
https://github.com/HaotianLiu123/CCGSPG.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CÂ²GSPGï¼šé¢å‘è‡ªæ„ŸçŸ¥æ¨ç†çš„ç½®ä¿¡åº¦æ ¡å‡†åˆ†ç»„åºåˆ—ç­–ç•¥æ¢¯åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼ˆå¦‚ Group Relative Policy Optimization (GRPO) åŠå…¶å˜ä½“ï¼‰åœ¨æ¨ç†æ¨¡å‹å¼€å‘ä¸­è‡³å…³é‡è¦ï¼Œä½†è¿™äº›æ–¹æ³•å¸¸å­˜åœ¨è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œå³ç”Ÿæˆåºåˆ—çš„æ¦‚ç‡ä¸åºåˆ—å¥–åŠ±ä¸åŒ¹é…ï¼Œé«˜æ¦‚ç‡åºåˆ—å¯èƒ½å¯¹åº”ä½å¥–åŠ±ï¼Œé˜»ç¢äº†è‡ªæ„ŸçŸ¥æ¨ç†æ¨¡å‹çš„å®ç°ã€‚ç°æœ‰ç¼“è§£è¿‡åº¦è‡ªä¿¡çš„æ–¹æ³•ï¼Œå› å¿½è§†ç­–ç•¥ä¼˜åŒ–ä¸ç½®ä¿¡åº¦æ ¡å‡†é—´çš„æ½œåœ¨å†²çªï¼Œå¸¸å¯¼è‡´æ¨ç†ç²¾åº¦ä¸‹é™æˆ–æ ¡å‡†ä¸è¶³ã€‚å¦‚ä½•åœ¨ä¸å½±å“ç­–ç•¥ä¼˜åŒ–çš„å‰æä¸‹å®ç°ç½®ä¿¡åº¦æ ¡å‡†ï¼Œå°¤å…¶æ˜¯éäºŒå…ƒå¥–åŠ±åœºæ™¯ä¸‹ï¼Œä»æ˜¯å¼€æ”¾é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º Group Sequence Policy Gradientï¼ˆGSPGï¼‰æ¡†æ¶  
ä¸ºå­¦ä¹ æ¨ç†æ¨¡å‹ï¼ŒGSPG æ¡†æ¶æ¶ˆé™¤äº† GRPO åŠå…¶å˜ä½“ä¸­å¸¸è§çš„ token çº§åå·®ã€‚è¯¥æ¡†æ¶é’ˆå¯¹æ¯ä¸ªæ¨ç†é—®é¢˜ï¼Œåˆ©ç”¨å½’ä¸€åŒ–çš„åºåˆ—çº§æ¦‚ç‡å®šä¹‰æ¨¡å‹ç½®ä¿¡åº¦ï¼Œå†é€šè¿‡äº¤å‰ç†µæ­£åˆ™åŒ–å™¨å°†æ¨¡å‹ç½®ä¿¡åº¦æ ¡å‡†åˆ°åºåˆ—å¥–åŠ±ä¸Šã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç½®ä¿¡åº¦æ ¡å‡†ä¸ç­–ç•¥ä¼˜åŒ–çš„ååŒè®¾è®¡  
å¯¹äºäºŒå…ƒå¥–åŠ±åœºæ™¯ï¼Œç†è®ºè¯æ˜ç½®ä¿¡åº¦æ ¡å‡†æ­£åˆ™åŒ–å™¨ä¸ GSPG çš„ç›®æ ‡æ¢¯åº¦æ–¹å‘å§‹ç»ˆä¸€è‡´ï¼ŒäºŒè€…ååŒå·¥ä½œã€‚é’ˆå¯¹éäºŒå…ƒå¥–åŠ±ï¼Œé‡‡ç”¨éçº¿æ€§å¥–åŠ±å½’ä¸€åŒ–å’Œè‡ªé€‚åº”æ­£åˆ™åŒ–å™¨è£å‰ªï¼Œç¼“è§£ä¸¤ä¸ªç›®æ ‡é—´çš„æ½œåœ¨å†²çªï¼Œæå‡º Confidence - calibrated Group Sequence Policy Gradientï¼ˆCÂ²GSPGï¼‰æ–¹æ³•ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨é€»è¾‘å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œå¯¹æ¯” GRPO åŠå…¶å˜ä½“ï¼ŒCÂ²GSPG å±•ç°å‡ºä¼˜åŠ¿ï¼š  
- æœ‰æ•ˆç¼“è§£æ¨¡å‹è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œå¦‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡çš„å¯é æ€§å›¾ä¸­ï¼ŒCÂ²GSPG å½¢æˆäº†åŒå³°ç½®ä¿¡åˆ†å¸ƒï¼ˆæ­£ç¡®ç­”æ¡ˆé«˜ç½®ä¿¡ã€é”™è¯¯ç­”æ¡ˆä½ç½®ä¿¡ï¼‰ï¼Œè€ŒåŸºçº¿æ–¹æ³•åˆ†å¸ƒæ··ä¹±ï¼›  
- æ¨ç†ç²¾åº¦æå‡ï¼Œåœ¨ â€œKnights and Knavesâ€ é€»è¾‘è°œé¢˜æ•°æ®é›†ä¸Šï¼ŒCÂ²GSPG åœ¨éªŒè¯é›†å‡†ç¡®ç‡æ›´é«˜ï¼ŒåŒæ—¶ Expected Calibration Errorï¼ˆECEï¼‰æ›´ä½ï¼Œè¿›å…¥é«˜å‡†ç¡®ç‡ã€ä½ ECE çš„æœ€ä½³æ€§èƒ½è±¡é™ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- é’ˆå¯¹åºåˆ—ç”Ÿæˆç±»ä»»åŠ¡ä¸­å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„åå·®ä¸è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œæä¾›äº†ä»æ¡†æ¶è®¾è®¡ï¼ˆGSPG æ¶ˆé™¤ token çº§åå·®ï¼‰åˆ°æ­£åˆ™åŒ–æ‰‹æ®µï¼ˆç½®ä¿¡åº¦æ ¡å‡†æ­£åˆ™åŒ–ï¼‰çš„å®Œæ•´è§£å†³æ€è·¯ï¼Œä¸ºåç»­æ¨ç†æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›äº†æ–°èŒƒå¼ï¼›  
- å¯¹äºŒå…ƒå’ŒéäºŒå…ƒå¥–åŠ±åœºæ™¯åˆ†åˆ«è®¾è®¡é€‚é…ç­–ç•¥ï¼Œè¿™ç§åˆ†åœºæ™¯å¤„ç†å†²çªçš„æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å­˜åœ¨å¤šç›®æ ‡ä¼˜åŒ–å†²çªçš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼›  
- å¼€æºä»£ç ä¾¿äºç¤¾åŒºå¤ç°ä¸è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸå‘å±•ã€‚

## quantile-advantage-estimation-for-entropy-safe-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM
reasoning, but training often oscillates between {entropy collapse} and
{entropy explosion}. We trace both hazards to the mean baseline used in
value-free RL (e.g., GRPO and DAPO), which improperly penalizes
negative-advantage samples under reward outliers. We propose {Quantile
Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile
baseline. QAE induces a response-level, two-regime gate: on hard queries (p <=
1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it
targets remaining failures. Under first-order softmax updates, we prove
{two-sided entropy safety}, giving lower and upper bounds on one-step entropy
change that curb explosion and prevent collapse. Empirically, this minimal
modification stabilizes entropy, sparsifies credit assignment (with tuned K,
roughly 80% of responses receive zero advantage), and yields sustained pass@1
gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results
identify {baseline design} -- rather than token-level heuristics -- as the
primary mechanism for scaling RLVR.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ†ä½æ•°ä¼˜åŠ¿ä¼°è®¡ï¼šä¸ºç†µå®‰å…¨æ¨ç†ä¿é©¾æŠ¤èˆª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ç”¨äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹å¸¸é¢ä¸´â€œç†µåç¼©ï¼ˆentropy collapseï¼‰â€ä¸â€œç†µçˆ†ç‚¸ï¼ˆentropy explosionï¼‰â€çš„é—®é¢˜ã€‚è¿™ç±»é—®é¢˜æ ¹æºåœ¨äºæ— ä»·å€¼å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚GRPOã€DAPOï¼‰ä¸­é‡‡ç”¨çš„å‡å€¼åŸºçº¿ï¼Œå½“å­˜åœ¨å¥–åŠ±å¼‚å¸¸å€¼æ—¶ï¼Œå®ƒä¼šå¯¹è´Ÿä¼˜åŠ¿æ ·æœ¬è¿›è¡Œä¸æ°å½“çš„æƒ©ç½šã€‚æ‰€ä»¥éœ€è¦ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³å‡å€¼åŸºçº¿å¸¦æ¥çš„ç¼ºé™·ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç†µå¹¶æå‡æ¨¡å‹æ¨ç†è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåˆ†ä½æ•°ä¼˜åŠ¿ä¼°è®¡ï¼ˆQuantile Advantage Estimationï¼ŒQAEï¼‰
ç”¨åˆ†ç»„å¼çš„Kåˆ†ä½æ•°åŸºçº¿æ›¿ä»£åŸæœ‰çš„å‡å€¼åŸºçº¿ã€‚QAEèƒ½åœ¨å“åº”çº§åˆ«å½¢æˆåŒæœºåˆ¶é—¨æ§ï¼šé¢å¯¹å›°éš¾æŸ¥è¯¢ï¼ˆæ¦‚ç‡p â‰¤ 1 - Kï¼‰æ—¶ï¼Œå¼ºåŒ–ç½•è§çš„æˆåŠŸæ¡ˆä¾‹ï¼›é¢å¯¹ç®€å•æŸ¥è¯¢ï¼ˆæ¦‚ç‡p > 1 - Kï¼‰æ—¶ï¼Œé’ˆå¯¹å‰©ä½™çš„å¤±è´¥æƒ…å†µè¿›è¡Œä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºè¯æ˜åŒä¾§ç†µå®‰å…¨
åœ¨ä¸€é˜¶softmaxæ›´æ–°ä¸‹ï¼Œè¯æ˜äº†QAEå…·å¤‡â€œåŒä¾§ç†µå®‰å…¨â€æ€§è´¨ï¼Œå³å¯¹å•æ­¥ç†µå˜åŒ–ç»™å‡ºäº†ä¸‹ç•Œä¸ä¸Šç•Œï¼Œèƒ½æŠ‘åˆ¶ç†µçˆ†ç‚¸ã€é˜²æ­¢ç†µåç¼©ï¼Œä»ç†è®ºå±‚é¢ä¿éšœè®­ç»ƒè¿‡ç¨‹ä¸­ç†µçš„ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒä¸­ï¼Œè¿™ä¸€â€œå¾®å°ä¿®æ”¹â€å±•ç°å‡ºè‰¯å¥½æ•ˆæœï¼šç¨³å®šäº†ç†µï¼›å®ç°äº†ä¿¡ç”¨åˆ†é…çš„ç¨€ç–åŒ–ï¼ˆè°ƒä¼˜Kåï¼Œçº¦80%çš„å“åº”ä¼˜åŠ¿ä¸º0ï¼‰ï¼›åœ¨Qwen3 - 8B/14B - Baseæ¨¡å‹ä¸Šï¼Œé’ˆå¯¹AIME 2024/2025å’ŒAMC 2023ç­‰ä»»åŠ¡ï¼ŒæŒç»­æå‡äº†pass@1æŒ‡æ ‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æŒ‡å‡ºåŸºçº¿è®¾è®¡ï¼ˆè€Œétokençº§å¯å‘å¼æ–¹æ³•ï¼‰æ‰æ˜¯RLVR scalingçš„ä¸»è¦æœºåˆ¶ï¼Œè¿™ä¸ºåç»­RLVRæ–¹å‘ç ”ç©¶æä¾›äº†æ–°çš„æ€è€ƒè§’åº¦â€”â€”é‡è§†åŸºçº¿ç¯èŠ‚çš„è®¾è®¡ä¼˜åŒ–ï¼›QAEæ–¹æ³•åœ¨è§£å†³ç†µä¸ç¨³å®šä¸æå‡ä»»åŠ¡è¡¨ç°ä¸Šçš„æ€è·¯ï¼Œä¹Ÿä¸ºå¤„ç†å¼ºåŒ–å­¦ä¹ ä¸­åŸºçº¿ä¸åˆç†å¯¼è‡´çš„è®­ç»ƒé—®é¢˜æä¾›äº†å¯å‚è€ƒçš„èŒƒå¼ï¼Œå°¤å…¶æ˜¯åˆ†ä½æ•°åŸºçº¿ç»“åˆåŒæœºåˆ¶é—¨æ§ä¸ç†è®ºä¿éšœçš„è®¾è®¡é€»è¾‘ï¼Œå€¼å¾—åœ¨ç±»ä¼¼éœ€ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€ä¼˜åŒ–ä¼˜åŠ¿ä¼°è®¡çš„åœºæ™¯ä¸­å€Ÿé‰´ã€‚

## learn-the-ropes--then-trust-the-wins--self-imitation-with-progressive-exploration-for-agentic-reinforcement-learning
### Abstract
Reinforcement learning (RL) is the dominant paradigm for sharpening strategic
tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,
yet it faces a fundamental challenge of exploration-exploitation trade-off.
Existing studies stimulate exploration through the lens of policy entropy, but
such mechanical entropy maximization is prone to RL training instability due to
the multi-turn distribution shifting. In this paper, we target the progressive
exploration-exploitation balance under the guidance of the agent own
experiences without succumbing to either entropy collapsing or runaway
divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)
recipe for training agentic LLMs. It extends the vanilla SIL framework, where a
replay buffer stores self-generated promising trajectories for off-policy
update, by gradually steering the policy evolution within a well-balanced range
of entropy across stages. Specifically, our approach incorporates a curriculum
to manage the exploration process, utilizing intrinsic rewards to foster
skill-level exploration and facilitating action-level exploration through SIL.
At first, the auxiliary tool call reward plays a critical role in the
accumulation of tool-use skills, enabling broad exposure to the unfamiliar
distributions of the environment feedback with an upward entropy trend. As
training progresses, self-imitation gets strengthened to exploit existing
successful patterns from replayed experiences for comparative action-level
exploration, accelerating solution iteration without unbounded entropy growth.
To further stabilize training, we recalibrate the advantages of experiences in
the replay buffer to address the potential policy drift. Reugularizations such
as the clipping of tokens with high covariance between probability and
advantage are introduced to the trajectory-level entropy control to curb
over-confidence.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡æ–°èŒƒå¼ï¼šSPEARåŠ©åŠ›æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿å‘¨æœŸã€ç¨€ç–å¥–åŠ±æ™ºèƒ½ä½“ä»»åŠ¡ä¸­ç­–ç•¥æ€§å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„ä¸»æµèŒƒå¼ï¼Œä½†é¢ä¸´æ¢ç´¢ - åˆ©ç”¨æƒè¡¡è¿™ä¸€æ ¹æœ¬æŒ‘æˆ˜ã€‚ç°æœ‰é€šè¿‡ç­–ç•¥ç†µåˆºæ¿€æ¢ç´¢çš„ç ”ç©¶ï¼Œå› å¤šè½®åˆ†å¸ƒè½¬ç§»æ˜“å¯¼è‡´RLè®­ç»ƒä¸ç¨³å®šï¼Œå­˜åœ¨ç†µåç¼©æˆ–å¤±æ§å‘æ•£é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨å€ŸåŠ©æ™ºèƒ½ä½“è‡ªèº«ç»éªŒå¼•å¯¼ï¼Œå®ç°æ¢ç´¢ - åˆ©ç”¨çš„æ¸è¿›å¹³è¡¡ï¼Œé¿å…ä¸Šè¿°é—®é¢˜ï¼Œä¸ºè®­ç»ƒæ™ºèƒ½ä½“LLMsæä¾›æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSPEARæ¡†æ¶
SPEARæ˜¯åŸºäºè¯¾ç¨‹å­¦ä¹ çš„è‡ªæ¨¡ä»¿å­¦ä¹ ï¼ˆSILï¼‰æ–¹æ¡ˆï¼Œæ‰©å±•äº†åŸå§‹SILæ¡†æ¶ã€‚åŸå§‹SILç”¨å›æ”¾ç¼“å†²åŒºå­˜å‚¨è‡ªç”Ÿæˆæœ‰å‰æ™¯è½¨è¿¹ç”¨äºç¦»ç­–ç•¥æ›´æ–°ï¼ŒSPEARåˆ™é€šè¿‡åœ¨ä¸åŒé˜¶æ®µå¹³è¡¡ç†µçš„èŒƒå›´æ¥å¼•å¯¼ç­–ç•¥è¿›åŒ–ã€‚å¼•å…¥è¯¾ç¨‹ç®¡ç†æ¢ç´¢è¿‡ç¨‹ï¼Œåˆ©ç”¨å†…åœ¨å¥–åŠ±ä¿ƒè¿›æŠ€èƒ½å±‚é¢æ¢ç´¢ï¼Œé€šè¿‡SILæ¨åŠ¨åŠ¨ä½œå±‚é¢æ¢ç´¢ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†é˜¶æ®µå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
è®­ç»ƒåˆæœŸï¼Œè¾…åŠ©å·¥å…·è°ƒç”¨å¥–åŠ±åœ¨å·¥å…·ä½¿ç”¨æŠ€èƒ½ç§¯ç´¯ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå€ŸåŠ©ç†µä¸Šå‡è¶‹åŠ¿å¹¿æ³›æ¥è§¦ç¯å¢ƒåé¦ˆçš„æœªçŸ¥åˆ†å¸ƒï¼›è®­ç»ƒæ¨è¿›åï¼Œå¼ºåŒ–è‡ªæ¨¡ä»¿ï¼Œä»å›æ”¾ç»éªŒä¸­åˆ©ç”¨ç°æœ‰æˆåŠŸæ¨¡å¼è¿›è¡ŒåŠ¨ä½œå±‚é¢çš„æ¯”è¾ƒæ€§æ¢ç´¢ï¼ŒåŠ é€Ÿè§£å†³æ–¹æ¡ˆè¿­ä»£åŒæ—¶é¿å…ç†µæ— ç•Œå¢é•¿ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–æªæ–½
é‡æ–°æ ¡å‡†å›æ”¾ç¼“å†²åŒºä¸­ç»éªŒçš„ä¼˜åŠ¿ä»¥åº”å¯¹æ½œåœ¨ç­–ç•¥æ¼‚ç§»ï¼›åœ¨è½¨è¿¹çº§ç†µæ§åˆ¶ä¸­å¼•å…¥å¯¹æ¦‚ç‡å’Œä¼˜åŠ¿é—´é«˜åæ–¹å·® tokens çš„è£å‰ªç­‰æ­£åˆ™åŒ–æ–¹æ³•ï¼ŒæŠ‘åˆ¶è¿‡åº¦è‡ªä¿¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡æœªæä¾›å…·ä½“å®éªŒç»“æœç›¸å…³å†…å®¹ï¼Œæš‚æ— æ³•é˜è¿°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä½“LLMsåœºæ™¯ä¸‹ï¼Œæä¾›äº†æ¢ç´¢ - åˆ©ç”¨å¹³è¡¡çš„æ–°æ€è·¯ï¼Œåˆ†é˜¶æ®µå¼•å¯¼ç­–ç•¥è¿›åŒ–çš„æ–¹å¼ä¸ºå¤„ç†é•¿å‘¨æœŸä»»åŠ¡æä¾›äº†è¯¾ç¨‹å¼å­¦ä¹ å‚è€ƒï¼›é’ˆå¯¹è®­ç»ƒç¨³å®šæ€§ï¼Œé‡æ–°æ ¡å‡†ç»éªŒä¼˜åŠ¿å’Œè½¨è¿¹çº§ç†µæ§åˆ¶çš„æ­£åˆ™åŒ–æ‰‹æ®µï¼Œä¸ºè§£å†³ç­–ç•¥æ¼‚ç§»ã€è¿‡åº¦è‡ªä¿¡ç­‰å¸¸è§è®­ç»ƒé—®é¢˜æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯æ€è·¯ï¼›è‡ªæ¨¡ä»¿å­¦ä¹ ç»“åˆå†…åœ¨å¥–åŠ±ç­‰æœºåˆ¶ï¼Œåœ¨æŠ€èƒ½ç§¯ç´¯ä¸åŠ¨ä½œæ¢ç´¢å±‚é¢çš„è®¾è®¡ï¼Œå¯¹å…¶ä»–éœ€è¦å¹³è¡¡æ¢ç´¢åˆ©ç”¨çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¹Ÿæœ‰å¯å‘æ„ä¹‰ã€‚

## epo--entropy-regularized-policy-optimization-for-llm-agents-reinforcement-learning
### Abstract
Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLMæ™ºèƒ½ä½“å¤šè½®ç¨€ç–å¥–åŠ±è®­ç»ƒæ–°çªç ´ï¼šEPOæ¡†æ¶ç ´è§£æ¢ç´¢åˆ©ç”¨çº§è”å¤±æ•ˆéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šè½®äº¤äº’ï¼ˆå•ä»»åŠ¡éœ€30 + è½®äº¤äº’ï¼‰ä¸”å¥–åŠ±ç¨€ç–çš„ç¯å¢ƒä¸­è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“ï¼Œå¼ºåŒ–å­¦ä¹ é¢ä¸´ç€æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°è¯¥åœºæ™¯ä¸‹å­˜åœ¨ç‹¬ç‰¹çš„â€œæ¢ç´¢ - åˆ©ç”¨çº§è”å¤±æ•ˆâ€é—®é¢˜ï¼šé¦–å…ˆæ˜¯æ—©æœŸç­–ç•¥è¿‡æ—©æ”¶æ•›ï¼Œç¨€ç–åé¦ˆè®©æ™ºèƒ½ä½“é™·å…¥æœ‰ç¼ºé™·ã€ä½ç†µçš„ç­–ç•¥ï¼›æ¥ç€è¿›å…¥åæœŸç­–ç•¥å´©æºƒï¼Œä¼ ç»Ÿç†µæ­£åˆ™åŒ–èµ·åˆ°åä½œç”¨ï¼Œå¼•å‘æ··ä¹±æ¢ç´¢ä½¿è®­ç»ƒä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™ä¸€åœ¨LLMæ™ºèƒ½ä½“å¤šè½®ç¨€ç–å¥–åŠ±è®­ç»ƒåœºæ™¯ä¸‹çš„å…³é”®é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†Entropy - regularized Policy Optimizationï¼ˆEPOï¼‰æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šè½®åœºæ™¯ä¸‹é‡‡ç”¨ç†µæ­£åˆ™åŒ–å¢å¼ºæ¢ç´¢  
åœ¨LLMæ™ºèƒ½ä½“å¤šè½®äº¤äº’çš„ç¯å¢ƒä¸­ï¼Œå¼•å…¥ç†µæ­£åˆ™åŒ–æœºåˆ¶ï¼Œä»¥æ­¤æ¥æå‡æ™ºèƒ½ä½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢ç´¢èƒ½åŠ›ï¼Œé¿å…å› ç¨€ç–å¥–åŠ±å¯¼è‡´æ—©æœŸå°±é™·å…¥ä½ç†µçš„ä¸è‰¯ç­–ç•¥ï¼Œä¸ºåç»­æœ‰æ•ˆå­¦ä¹ å¥ å®šåŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†µå¹³æ»‘æ­£åˆ™åŒ–é™åˆ¶ç­–ç•¥ç†µæ³¢åŠ¨  
è®¾è®¡äº†ç†µå¹³æ»‘æ­£åˆ™åŒ–å™¨ï¼Œå°†ç­–ç•¥ç†µé™åˆ¶åœ¨å†å²å¹³å‡å€¼èŒƒå›´å†…ï¼Œé˜²æ­¢ç­–ç•¥ç†µå‡ºç°çªç„¶çš„æ³¢åŠ¨æƒ…å†µã€‚è¿™æ ·èƒ½é¿å…å› ç†µçš„æ€¥å‰§å˜åŒ–è€Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œä¿éšœè®­ç»ƒè¿‡ç¨‹çš„å¹³ç¨³æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºé˜¶æ®µçš„è‡ªé€‚åº”åŠ æƒå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨  
æå‡ºè‡ªé€‚åº”çš„åŸºäºé˜¶æ®µçš„åŠ æƒæ–¹å¼ï¼Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨è¿™ä¸¤ä¸ªå…³é”®ç¯èŠ‚ã€‚æ ¹æ®è®­ç»ƒçš„ä¸åŒé˜¶æ®µç‰¹ç‚¹ï¼Œåˆç†åˆ†é…æƒé‡ï¼Œè®©æ™ºèƒ½ä½“åœ¨è®­ç»ƒå‰æœŸå……åˆ†æ¢ç´¢ï¼ŒåæœŸåˆèƒ½æœ‰æ•ˆåˆ©ç”¨å­¦åˆ°çš„çŸ¥è¯†ï¼Œæ‰“ç ´â€œæ¢ç´¢ - åˆ©ç”¨çº§è”å¤±æ•ˆâ€çš„å¾ªç¯ã€‚å¹¶ä¸”ç†è®ºåˆ†æè¯æ˜EPOèƒ½ä¿è¯ç†µæ–¹å·®å•è°ƒé€’å‡åŒæ—¶ç»´æŒæ”¶æ•›æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒæ–¹é¢ï¼ŒEPOåœ¨ScienceWorldæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾152%çš„æ€§èƒ½æå‡ï¼Œåœ¨ALFWorldæ•°æ®é›†ä¸Šä¹Ÿæœ‰é«˜è¾¾19.8%çš„æ€§èƒ½æå‡ã€‚è¿™å……åˆ†éªŒè¯äº†EPOæ¡†æ¶åœ¨LLMæ™ºèƒ½ä½“å¤šè½®ç¨€ç–å¥–åŠ±è®­ç»ƒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢æ¥çœ‹ï¼ŒEPOæ¡†æ¶é’ˆå¯¹å¤šè½®ç¨€ç–å¥–åŠ±åœºæ™¯ä¸‹LLMæ™ºèƒ½ä½“è®­ç»ƒçš„ç‰¹æ®Šé—®é¢˜ï¼Œåˆ›æ–°æ€§åœ°æ•´åˆäº†å¤šæœºåˆ¶æ¥è§£å†³æ¢ç´¢åˆ©ç”¨å¤±è¡¡éš¾é¢˜ï¼Œä¸ºåŒç±»åœºæ™¯ä¸‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ï¼Œå³è¦è€ƒè™‘åœºæ™¯ç‰¹æ®Šæ€§å»å®šåˆ¶ç†µæ§åˆ¶ç­‰æœºåˆ¶ï¼Œè€Œéç›´æ¥å¥—ç”¨ä¼ ç»ŸRLæ–¹æ³•ï¼›ä»åº”ç”¨å±‚é¢ï¼Œå®éªŒåœ¨ScienceWorldå’ŒALFWorldç­‰å…¸å‹çš„å¤šè½®äº¤äº’ä»»åŠ¡ç¯å¢ƒä¸­éªŒè¯æœ‰æ•ˆï¼Œè¯´æ˜è¯¥æ–¹æ³•å¯è¿ç§»åˆ°ç±»ä¼¼çš„LLMæ™ºèƒ½ä½“è®­ç»ƒä»»åŠ¡ä¸­ï¼Œä¸ºç›¸å…³é¢†åŸŸå¦‚æ™ºèƒ½åŠ©æ‰‹ã€è™šæ‹Ÿç¯å¢ƒäº¤äº’ç­‰åœºæ™¯ä¸‹çš„LLMæ™ºèƒ½ä½“è®­ç»ƒæä¾›äº†æŠ€æœ¯å‚è€ƒï¼›ä»ç†è®ºå±‚é¢ï¼Œå¯¹EPOä¿è¯ç†µæ–¹å·®å•è°ƒé€’å‡å’Œæ”¶æ•›æ€§çš„åˆ†æï¼Œä¹Ÿä¸ºåç»­ç›¸å…³ç®—æ³•çš„ç†è®ºæ¨å¯¼å’Œæ€§èƒ½ä¿éšœæä¾›äº†å¯å€Ÿé‰´çš„åˆ†æèŒƒå¼ã€‚

## learning-more-with-less--a-dynamic-dual-level-down-sampling-framework-for-efficient-policy-optimization
### Abstract
Critic-free methods like GRPO reduce memory demands by estimating advantages
from multiple rollouts but tend to converge slowly, as critical learning
signals are diluted by an abundance of uninformative samples and tokens. To
tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling
(D$^3$S)} framework that prioritizes the most informative samples and tokens
across groups to improve the efficient of policy optimization. D$^3$S operates
along two levels: (1) the sample-level, which selects a subset of rollouts to
maximize advantage variance ($\text{Var}(A)$). We theoretically proven that
this selection is positively correlated with the upper bound of the policy
gradient norms, yielding higher policy gradients. (2) the token-level, which
prioritizes tokens with a high product of advantage magnitude and policy
entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the
policy is both uncertain and impactful. Moreover, to prevent overfitting to
high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by
curriculum learning. This schedule starts with aggressive down-sampling to
accelerate early learning and gradually relaxes to promote robust
generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that
integrating D$^3$S into advanced RL algorithms achieves state-of-the-art
performance and generalization while requiring \textit{fewer} samples and
tokens across diverse reasoning benchmarks. Our code is added in the
supplementary materials and will be made publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ›´å°‘æ•°æ®å­¦æ›´å¤šï¼šåŠ¨æ€åŒçº§ä¸‹é‡‡æ ·åŠ©åŠ›é«˜æ•ˆç­–ç•¥ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼ŒåƒGRPOè¿™ç±»æ— Criticçš„æ–¹æ³•è™½é€šè¿‡å¤šè½®æ¬¡ï¼ˆrolloutsï¼‰ä¼°è®¡ä¼˜åŠ¿æ¥é™ä½å†…å­˜éœ€æ±‚ï¼Œä½†å­˜åœ¨æ”¶æ•›æ…¢çš„é—®é¢˜ã€‚åŸå› åœ¨äºå¤§é‡æ— ä¿¡æ¯çš„æ ·æœ¬å’Œtokenç¨€é‡Šäº†å…³é”®å­¦ä¹ ä¿¡å·ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºDynamic Dual - Level Down - Samplingï¼ˆDÂ³Sï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜å…ˆé€‰æ‹©æœ€å…·ä¿¡æ¯æ€§çš„æ ·æœ¬å’Œtokenæ¥æå‡ç­–ç•¥ä¼˜åŒ–æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ·æœ¬çº§ä¸‹é‡‡æ ·
åœ¨æ ·æœ¬çº§ï¼ŒDÂ³Sé€‰æ‹©ä¸€éƒ¨åˆ†rolloutsä»¥æœ€å¤§åŒ–ä¼˜åŠ¿æ–¹å·®ï¼ˆVar(A)ï¼‰ã€‚ä»ç†è®ºä¸Šè¯æ˜äº†è¿™ç§é€‰æ‹©ä¸ç­–ç•¥æ¢¯åº¦èŒƒæ•°çš„ä¸Šç•Œæ­£ç›¸å…³ï¼Œèƒ½äº§ç”Ÿæ›´å¤§çš„ç­–ç•¥æ¢¯åº¦ï¼Œè¿›è€Œæå‡ç­–ç•¥ä¼˜åŒ–æ•ˆæœã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼štokençº§ä¸‹é‡‡æ ·
åœ¨tokençº§ï¼Œä¼˜å…ˆå¤„ç†ä¼˜åŠ¿å¹…åº¦ä¸ç­–ç•¥ç†µä¹˜ç§¯ï¼ˆ|A_{i,t}|Ã—H_{i,t}ï¼‰é«˜çš„tokenï¼Œå°†æ›´æ–°èšç„¦åœ¨ç­–ç•¥æ—¢ä¸ç¡®å®šåˆæœ‰å½±å“çš„tokenä¸Šã€‚æ­¤å¤–ï¼Œä¸ºé˜²æ­¢å¯¹é«˜ä¿¡å·æ•°æ®è¿‡æ‹Ÿåˆï¼ŒDÂ³Sé‡‡ç”¨å—è¯¾ç¨‹å­¦ä¹ å¯å‘çš„åŠ¨æ€ä¸‹é‡‡æ ·è°ƒåº¦ã€‚è¯¥è°ƒåº¦åˆæœŸé‡‡ç”¨æ¿€è¿›ä¸‹é‡‡æ ·åŠ é€Ÿæ—©æœŸå­¦ä¹ ï¼Œä¹‹åé€æ¸æ”¾æ¾ä»¥ä¿ƒè¿›é²æ£’æ³›åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Qwen2.5å’ŒLlama3.1ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå°†DÂ³Sé›†æˆåˆ°å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œåœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½åœ¨ä½¿ç”¨æ›´å°‘æ ·æœ¬å’Œtokençš„æƒ…å†µä¸‹ï¼Œå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼ŒåŒçº§ä¸‹é‡‡æ ·ï¼ˆæ ·æœ¬çº§å’Œtokençº§ï¼‰çš„æ€è·¯ä¸ºå¤„ç†æ•°æ®å†—ä½™ã€æå‡å­¦ä¹ æ•ˆç‡æä¾›äº†æ–°æ–¹å‘ï¼Œå¯å€Ÿé‰´è¿™ç§åˆ†å±‚å¤„ç†ä¸åŒç²’åº¦æ•°æ®çš„æ–¹å¼ï¼›åŠ¨æ€è°ƒåº¦æœºåˆ¶å—è¯¾ç¨‹å­¦ä¹ å¯å‘ï¼Œè¿™ç§æ ¹æ®å­¦ä¹ é˜¶æ®µè°ƒæ•´ç­–ç•¥çš„æ€æƒ³åœ¨å…¶ä»–éœ€è¦é€æ­¥ä¼˜åŒ–ã€é¿å…è¿‡æ‹Ÿåˆçš„ä»»åŠ¡ä¸­ä¹Ÿæœ‰å‚è€ƒä»·å€¼ï¼›ä»å®éªŒéªŒè¯æ¥çœ‹ï¼Œåœ¨å¤§æ¨¡å‹ï¼ˆå¦‚Qwen2.5ã€Llama3.1ï¼‰ä¸Šçš„åº”ç”¨éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§æ¨¡å‹ç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ—¶æå‡æ•ˆç‡æä¾›äº†å®è·µèŒƒä¾‹ï¼Œç›¸å…³ä»£ç åç»­å…¬å¼€ä¹Ÿä¸ºå¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶æä¾›äº†ä¾¿åˆ©ã€‚

## structural-information-based-hierarchical-diffusion-for-offline-reinforcement-learning
### Abstract
Diffusion-based generative methods have shown promising potential for
modeling trajectories from offline reinforcement learning (RL) datasets, and
hierarchical diffusion has been introduced to mitigate variance accumulation
and computational challenges in long-horizon planning tasks. However, existing
approaches typically assume a fixed two-layer diffusion hierarchy with a single
predefined temporal scale, which limits adaptability to diverse downstream
tasks and reduces flexibility in decision making. In this work, we propose
SIHD, a novel Structural Information-based Hierarchical Diffusion framework for
effective and stable offline policy learning in long-horizon environments with
sparse rewards. Specifically, we analyze structural information embedded in
offline trajectories to construct the diffusion hierarchy adaptively, enabling
flexible trajectory modeling across multiple temporal scales. Rather than
relying on reward predictions from localized sub-trajectories, we quantify the
structural information gain of each state community and use it as a
conditioning signal within the corresponding diffusion layer. To reduce
overreliance on offline datasets, we introduce a structural entropy regularizer
that encourages exploration of underrepresented states while avoiding
extrapolation errors from distributional shifts. Extensive evaluations on
challenging offline RL tasks show that SIHD significantly outperforms
state-of-the-art baselines in decision-making performance and demonstrates
superior generalization across diverse scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºç»“æ„ä¿¡æ¯çš„åˆ†å±‚æ‰©æ•£ï¼šé•¿ horizon ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢†åŸŸï¼ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ–¹æ³•åœ¨å¯¹ç¦»çº¿è½¨è¿¹å»ºæ¨¡ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œåˆ†å±‚æ‰©æ•£ä¹Ÿè¢«ç”¨äºç¼“è§£é•¿ horizon è§„åˆ’ä»»åŠ¡ä¸­çš„æ–¹å·®ç´¯ç§¯ä¸è®¡ç®—æŒ‘æˆ˜ã€‚ä½†ç°æœ‰æ–¹æ³•å¸¸å‡è®¾å›ºå®šçš„ä¸¤å±‚æ‰©æ•£å±‚çº§ä¸”ä»…å•ä¸€é¢„å®šä¹‰æ—¶é—´å°ºåº¦ï¼Œè¿™é™åˆ¶äº†å¯¹å¤šæ ·ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ä¸å†³ç­–çµæ´»æ€§ã€‚åŒæ—¶ï¼Œé•¿ horizon ä¸”ç¨€ç–å¥–åŠ±çš„ç¯å¢ƒä¸‹ï¼Œç¦»çº¿ç­–ç•¥å­¦ä¹ çš„æœ‰æ•ˆæ€§ä¸ç¨³å®šæ€§ä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œæ¯”å¦‚å¯¹ç¦»çº¿æ•°æ®é›†çš„è¿‡åº¦ä¾èµ–æ˜“å¼•å‘åˆ†å¸ƒåç§»ä¸‹çš„å¤–æ¨é”™è¯¯ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½è‡ªé€‚åº”æ„å»ºæ‰©æ•£å±‚çº§ã€çµæ´»å»ºæ¨¡å¤šæ—¶é—´å°ºåº¦è½¨è¿¹å¹¶ç¼“è§£æ•°æ®é›†ä¾èµ–é—®é¢˜çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªé€‚åº”æ„å»ºæ‰©æ•£å±‚çº§  
åˆ†æç¦»çº¿è½¨è¿¹ä¸­åµŒå…¥çš„ç»“æ„ä¿¡æ¯ï¼Œä»¥æ­¤è‡ªé€‚åº”åœ°æ„å»ºæ‰©æ•£å±‚çº§ï¼Œå®ç°è·¨å¤šä¸ªæ—¶é—´å°ºåº¦çš„çµæ´»è½¨è¿¹å»ºæ¨¡ã€‚ä¸å†å±€é™äºå›ºå®šå±‚çº§ä¸å•ä¸€æ—¶é—´å°ºåº¦ï¼Œè®©æ¨¡å‹èƒ½æ›´å¥½é€‚é…ä¸åŒä»»åŠ¡åœºæ™¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“æ„ä¿¡æ¯å¢ç›Šä½œä¸ºæ¡ä»¶ä¿¡å·  
ä¸ä¾èµ–å±€éƒ¨å­è½¨è¿¹çš„å¥–åŠ±é¢„æµ‹ï¼Œè€Œæ˜¯é‡åŒ–æ¯ä¸ªçŠ¶æ€ç¾¤è½çš„ç»“æ„ä¿¡æ¯å¢ç›Šï¼Œå¹¶å°†å…¶ä½œä¸ºå¯¹åº”æ‰©æ•£å±‚å†…çš„æ¡ä»¶ä¿¡å·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œåˆ©ç”¨ç»“æ„ä¿¡æ¯å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œæå‡è½¨è¿¹å»ºæ¨¡ä¸å†³ç­–çš„åˆç†æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»“æ„ç†µæ­£åˆ™åŒ–å™¨  
ä¸ºå‡å°‘å¯¹ç¦»çº¿æ•°æ®é›†çš„è¿‡åº¦ä¾èµ–ï¼Œå¼•å…¥ç»“æ„ç†µæ­£åˆ™åŒ–å™¨ã€‚è¯¥æ­£åˆ™åŒ–å™¨é¼“åŠ±æ¢ç´¢ä»£è¡¨æ€§ä¸è¶³çš„çŠ¶æ€ï¼ŒåŒæ—¶é¿å…åˆ†å¸ƒåç§»å¸¦æ¥çš„å¤–æ¨é”™è¯¯ï¼Œåœ¨åˆ©ç”¨ç¦»çº¿æ•°æ®çš„åŒæ—¶å¢å¼ºæ¨¡å‹æ³›åŒ–æ€§ä¸é²æ£’æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¦»çº¿ RL ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º SIHD åœ¨å†³ç­–æ€§èƒ½æ–¹é¢æ˜¾è‘—è¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¸åŒåœºæ™¯ä¸‹å±•ç°å‡ºæ›´ä¼˜çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨é•¿ horizonã€ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹ç¦»çº¿ç­–ç•¥å­¦ä¹ çš„æœ‰æ•ˆæ€§ä¸ç¨³å®šæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œè‡ªé€‚åº”åˆ©ç”¨æ•°æ®ä¸­ç»“æ„ä¿¡æ¯æ¥æ„å»ºæ¨¡å‹å±‚çº§çš„æ€è·¯ï¼Œä¸ºå¤„ç†éœ€å¤šå°ºåº¦å»ºæ¨¡çš„ä»»åŠ¡æä¾›äº†å‚è€ƒï¼›å°†ç»“æ„ä¿¡æ¯å¢ç›Šä½œä¸ºæ¡ä»¶ä¿¡å·çš„æ–¹å¼ï¼Œå¯å‘äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¦‚ä½•æŒ–æ˜éå¥–åŠ±ç±»ä¿¡æ¯è¾…åŠ©å†³ç­–ï¼›ç»“æ„ç†µæ­£åˆ™åŒ–å™¨åˆ™ç»™å‡ºäº†ç¼“è§£ç¦»çº¿æ•°æ®ä¾èµ–ä¸åˆ†å¸ƒåç§»é—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ­£åˆ™åŒ–æ‰‹æ®µï¼Œè¿™äº›è®¾è®¡æ€è·¯åœ¨å…¶ä»–éœ€å¤„ç†å±‚çº§ç»“æ„ã€æ•°æ®ä¾èµ–ä¸æ³›åŒ–æ€§çš„ä»»åŠ¡æˆ–é¢†åŸŸä¸­éƒ½æœ‰å€Ÿé‰´ä»·å€¼ã€‚

## rest--reshaping-token-level-policy-gradients-for-tool-use-large-language-models
### Abstract
Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ResTï¼šé‡å¡‘å·¥å…·ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹çš„Tokençº§ç­–ç•¥æ¢¯åº¦

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·ä»è¢«åŠ¨ç”Ÿæˆè½¬å˜ä¸ºç›®æ ‡å¯¼å‘çš„æ™ºèƒ½ä½“ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºä¼˜åŒ–è¿™äº›å·¥å…·ä½¿ç”¨ç­–ç•¥æä¾›äº†ç†è®ºæ¡†æ¶ã€‚ç„¶è€Œå½“å‰ä¸»æµèŒƒå¼ä»…ä¾èµ–ç¨€ç–çš„ç»“æœå¥–åŠ±ï¼Œä¸”æœªè€ƒè™‘å·¥å…·ä½¿ç”¨ä»»åŠ¡çš„ç‰¹æ®Šæ€§ï¼Œè¿™ä¼šå¢å¤§ç­–ç•¥æ¢¯åº¦æ–¹å·®ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè®ºæ–‡å…ˆå»ºç«‹äº†å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸­ç­–ç•¥ç†µä¸è®­ç»ƒç¨³å®šæ€§çš„ç†è®ºè”ç³»ï¼Œå‘ç°ç»“æ„åŒ–ã€ä½ç†µçš„tokensæ˜¯å¥–åŠ±çš„ä¸»è¦å†³å®šå› ç´ ï¼ŒåŸºäºæ­¤æå‡ºResTæ–¹æ³•æ¥ä¼˜åŒ–å·¥å…·ä½¿ç”¨ä»»åŠ¡çš„è®­ç»ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç†è®ºå…³è”æ„å»º  
å»ºç«‹èµ·å·¥å…·ä½¿ç”¨ä»»åŠ¡é‡Œç­–ç•¥ç†µå’Œè®­ç»ƒç¨³å®šæ€§ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œæ­ç¤ºå‡ºç»“æ„åŒ–ã€ä½ç†µçš„tokensæ˜¯å¥–åŠ±çš„ä¸»è¦å†³å®šå› ç´ ï¼Œä¸ºåç»­æ–¹æ³•è®¾è®¡æä¾›ç†è®ºä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šResTæ–¹æ³•æå‡º  
æå‡ºReshaped Token - level policy gradientsï¼ˆResTï¼‰ç”¨äºå·¥å…·ä½¿ç”¨ä»»åŠ¡ã€‚ResTé€šè¿‡ç†µæ„ŸçŸ¥çš„tokené‡åŠ æƒæ¥é‡å¡‘ç­–ç•¥æ¢¯åº¦ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ æ¨ç†tokensçš„æƒé‡ã€‚è¿™ç§ç†µæ„ŸçŸ¥çš„æ–¹æ¡ˆèƒ½å¤Ÿå®ç°ä»ç»“æ„æ­£ç¡®æ€§åˆ°è¯­ä¹‰æ¨ç†çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå¹¶ä¸”åœ¨å¤šè½®å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸­ç¨³å®šæ”¶æ•›è¿‡ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨BFCLå’ŒAPI - Bankæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒResTå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ¯”ä¹‹å‰çš„æ–¹æ³•æ€§èƒ½æå‡é«˜è¾¾8.76%ã€‚åœ¨4Bè§„æ¨¡çš„åŸºç¡€å¤§è¯­è¨€æ¨¡å‹ä¸Šå¾®è°ƒæ—¶ï¼ŒResTåœ¨å•è½®ä»»åŠ¡ä¸Šæ¯”GPT - 4oè¶…å‡º4.11%ï¼Œåœ¨å¤šè½®åŸºç¡€ä»»åŠ¡ä¸Šè¶…å‡º1.50%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œé€šè¿‡ç†è®ºåˆ†ææ‰¾åˆ°ä»»åŠ¡å…³é”®å½±å“å› ç´ ï¼ˆå¦‚è¿™é‡Œçš„ç­–ç•¥ç†µä¸ä½ç†µtokensä½œç”¨ï¼‰æ¥æŒ‡å¯¼æ–¹æ³•åˆ›æ–°æ˜¯å¾ˆå€¼å¾—å€Ÿé‰´çš„æ€è·¯ï¼›åœ¨å¤„ç†åºåˆ—å†³ç­–ç±»ä»»åŠ¡ï¼ˆå¦‚å·¥å…·ä½¿ç”¨è¿™ç§å¤šè½®äº¤äº’åœºæ™¯ï¼‰æ—¶ï¼ŒResTè¿™ç§åŸºäºtokençº§åˆ«çš„ã€ç»“åˆä»»åŠ¡ç‰¹æ€§çš„æ¢¯åº¦é‡å¡‘ä¸æƒé‡è°ƒæ•´æ–¹å¼ï¼Œä¸ºä¼˜åŒ–ç­–ç•¥è®­ç»ƒè¿‡ç¨‹æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘åç»­é’ˆå¯¹ç‰¹å®šä»»åŠ¡åœºæ™¯ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹ä¸­åº”ç”¨çš„ç ”ç©¶ï¼›å®éªŒå±‚é¢ï¼Œå…¶åœ¨ä¸åŒè§„æ¨¡æ¨¡å‹å’Œä»»åŠ¡ç±»å‹ä¸Šçš„æµ‹è¯•ä¸å¯¹æ¯”ï¼Œä¹Ÿä¸ºç›¸å…³ç ”ç©¶éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§æä¾›äº†å‚è€ƒèŒƒå¼ï¼Œå³å…¨é¢è¦†ç›–ä¸åŒä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹è§„æ¨¡æ¥å±•ç°æ–¹æ³•æ™®é€‚æ€§ä¸ä¼˜åŠ¿ã€‚

## learning-to-reason-with-mixture-of-tokens
### Abstract
Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨æ··åˆtokenå­¦ä¹ æ¨ç†ï¼šçªç ´å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»æµæ–¹æ³•ã€‚å½“å‰å¤šæ•°æ–¹æ³•éµå¾ªGroup Relative Policy Optimizationçš„å˜ä½“ï¼Œåœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤é‡‡æ ·ç¦»æ•£tokenï¼Œä¸¢å¼ƒäº†æ¨¡å‹å€™é€‰tokenæ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸°å¯Œåˆ†å¸ƒä¿¡æ¯ã€‚åœ¨éå¼ºåŒ–å­¦ä¹ åœºæ™¯åˆ©ç”¨è¿™äº›åˆ†å¸ƒä¿¡æ¯å·²è¢«è¯æ˜æœ‰æ•ˆï¼Œä½†ç°æœ‰RLVRæ–¹æ³•å› æœªåˆ©ç”¨è¯¥ä¿¡æ¯ï¼Œä¸å¿…è¦åœ°é™åˆ¶äº†æ¨ç†æœç´¢ç©ºé—´ã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œæœ¬æ–‡æ¢ç´¢RLVRä¸­çš„æ··åˆtokenç”Ÿæˆï¼ˆMoT - Gï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºç»Ÿä¸€æ¡†æ¶æ³›åŒ–ç°æœ‰MoT - Gæ–¹æ³• 
æ„å»ºäº†èƒ½æ³›åŒ–ç°æœ‰MoT - Gæ–¹æ³•çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–ç°æœ‰æ— è®­ç»ƒæ–¹æ³•ï¼ˆè¿™ç±»æ–¹æ³•å°†æ··åˆåµŒå…¥æ„å»ºä¸ºtokenåµŒå…¥çš„åŠ æƒå’Œï¼‰ï¼ŒæŠŠRLVRæ‰©å±•åˆ°åœ¨è¿ç»­æ··åˆç©ºé—´ç›´æ¥ç”Ÿæˆæ€ç»´é“¾ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¢ç´¢æ··åˆtokenç”Ÿæˆåœ¨RLVRä¸­åº”ç”¨ 
é’ˆå¯¹RLVRåœºæ™¯æ·±å…¥ç ”ç©¶æ··åˆtokenç”Ÿæˆï¼ˆMoT - Gï¼‰ï¼Œè®©RLVRèƒ½åœ¨è¿ç»­çš„æ··åˆtokenç©ºé—´ä¸­è¿ä½œæ¥ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼Œçªç ´äº†ä»¥å¾€ç¦»æ•£tokené‡‡æ ·å¯¹æ¨ç†æœç´¢ç©ºé—´çš„é™åˆ¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¨ç†å¯†é›†å‹è¯­è¨€ä»»åŠ¡å¥—ä»¶Reasoning - Gymä¸Šè¯„ä¼°ä¸¤ç§MoT - Gå˜ä½“ï¼Œä½¿ç”¨Qwen2.5 - 1.5Bæ¨¡å‹æ—¶ï¼ŒMoT - Gæ–¹æ³•ç›¸æ¯”æ ‡å‡†è§£ç æœ‰æ˜¾è‘—æå‡ï¼ˆ10ä¸ªä»»åŠ¡ä¸­7ä¸ªä»»åŠ¡æœ‰5 - 35%çš„æ€§èƒ½æå‡ï¼‰ï¼›åŒæ—¶åœ¨è¾¾åˆ°å¯æ¯”ç²¾åº¦æ—¶ï¼Œæ‰€éœ€è½¨è¿¹æ•°ä»…ä¸ºä¸€åŠï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ã€‚é€šè¿‡éšè—çŠ¶æ€å’Œtokençº§çš„ç»¼åˆåˆ†æï¼Œå‘ç°MoT - Gçš„ä¼˜åŠ¿å¯èƒ½æºäºåœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ç»´æŒæ›´é«˜çš„éšè—çŠ¶æ€ç†µï¼Œä»¥åŠä¿ƒè¿›tokenç©ºé—´çš„æ¢ç´¢ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å·¥ä½œä¸­ï¼Œå¯è€ƒè™‘å¦‚ä½•åˆ©ç”¨æ¨¡å‹å€™é€‰tokençš„åˆ†å¸ƒä¿¡æ¯ï¼Œçªç ´ç¦»æ•£é‡‡æ ·çš„é™åˆ¶ï¼›å¯¹äºå¼ºåŒ–å­¦ä¹ ç»“åˆè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡çš„ç ”ç©¶ï¼Œæä¾›äº†åœ¨è¿ç»­æ··åˆç©ºé—´æ“ä½œçš„æ–°æ€è·¯ï¼Œä¸ºåç»­ä¼˜åŒ–æ¨ç†æœç´¢ç©ºé—´ã€æå‡è®­ç»ƒæ•ˆç‡æä¾›äº†æ–¹å‘ï¼›å…¶å¯¹éšè—çŠ¶æ€ç†µå’Œtokenç©ºé—´æ¢ç´¢çš„åˆ†æè§’åº¦ï¼Œä¹Ÿä¸ºç ”ç©¶æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å†…åœ¨æœºåˆ¶æä¾›äº†å¯å€Ÿé‰´çš„åˆ†ææ€è·¯ã€‚

## expanding-reasoning-potential-in-foundation-model-by-learning-diverse-chains-of-thought-patterns
### Abstract
Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¤šæ ·åŒ–æ€ç»´é“¾æ¨¡å¼æ‹“å±•å¤§æ¨¡å‹æ¨ç†æ½œåŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨é¢å‘å¤æ‚æ•°å­¦æ¨ç†çš„å¤§æ¨¡å‹ç ”ç©¶ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨åŠ¨äº†ä¸å°‘è¿›å±•ï¼Œè®­ç»ƒä¸­æœŸèå…¥é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ä¹Ÿè¢«è¯å®èƒ½æå‡æ¨ç†æ·±åº¦ã€‚ä½†å½“å‰æ–¹æ³•å­˜åœ¨å¯¹CoTæ•°æ®ä¸åŠ åŒºåˆ†ä½¿ç”¨çš„é—®é¢˜ï¼Œâ€œå“ªç§æ•°æ®ç±»å‹æœ€èƒ½æœ‰æ•ˆå¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›â€æˆäº†å¾…è§£çš„å…³é”®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡é¦–æ¬¡å®šä¹‰å¤§æ¨¡å‹çš„â€œæ¨ç†æ½œåŠ›â€ï¼ˆå³æ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€ç‹¬ç«‹å°è¯•æ¬¡æ•°çš„å€’æ•°ï¼Œä¸æœ€ç»ˆæ¨¡å‹æ€§èƒ½å¼ºç›¸å…³ï¼‰ï¼Œå¹¶æ¢ç´¢ç”¨å¯Œå«é«˜ä»·å€¼æ¨ç†æ¨¡å¼çš„å¤šæ ·åŒ–æ•°æ®æ¥æ‹“å±•è¿™ä¸€æ½œåŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰æ¨ç†æ½œåŠ›å¹¶æŠ½è±¡åŸå­æ¨ç†æ¨¡å¼  
é¦–æ¬¡æå‡ºâ€œæ¨ç†æ½œåŠ›â€æ¦‚å¿µï¼Œå°†å…¶å®šä¹‰ä¸ºæ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€ç‹¬ç«‹å°è¯•æ¬¡æ•°çš„å€’æ•°ï¼Œä¸”è¯¥æŒ‡æ ‡å’Œæ¨¡å‹æœ€ç»ˆæ€§èƒ½å¼ºå…³è”ã€‚åŒæ—¶ä»æ€ç»´é“¾åºåˆ—é‡ŒæŠ½è±¡å‡ºâ€œåŸå­æ¨ç†æ¨¡å¼â€â€”â€”è¿™ç±»æ¨¡å¼å…·å¤‡é€šç”¨æ€§ä¸å½’çº³èƒ½åŠ›ï¼Œå¹¶ç”¨å®ƒä»¬æ„å»ºå¯Œå«é«˜ä»·å€¼æ¨ç†æ¨¡å¼çš„æ ¸å¿ƒå‚è€ƒé›†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒç²’åº¦æ•°æ®é€‰æ‹©ç®—æ³•  
æå‡ºç»“åˆæ¨ç†æ¨¡å¼é“¾ä¸tokenç†µçš„åŒç²’åº¦ç®—æ³•ï¼Œä»æ•°æ®æ± ä¸­é«˜æ•ˆç­›é€‰ä¸æ ¸å¿ƒé›†åŒ¹é…çš„é«˜ä»·å€¼æ€ç»´é“¾æ•°æ®ï¼ˆCoTPï¼‰ï¼Œè®©æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°å­¦ä¹ æŒæ¡æ¨ç†èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
ä»…ç”¨10B - tokençš„CoTPæ•°æ®ï¼Œå°±è®©85A6Bçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹åœ¨é«˜éš¾åº¦çš„AIME 2024å’Œ2025èµ›äº‹ç›¸å…³ä»»åŠ¡ä¸Šæå‡äº†9.58%ï¼›åŒæ—¶ï¼Œä¸‹æ¸¸å¼ºåŒ–å­¦ä¹ æ€§èƒ½çš„ä¸Šé™ä¹Ÿè¢«æå‡äº†7.81%ï¼Œæœ‰åŠ›éªŒè¯äº†æ–¹æ³•åœ¨å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹â€œæ¨ç†æ½œåŠ›â€çš„å®šä¹‰ä¸ºè¡¡é‡æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†æ–°è§†è§’ï¼Œåç»­ç ”ç©¶å¯å‚è€ƒè¯¥æ€è·¯æ„å»ºæ›´ç²¾å‡†çš„èƒ½åŠ›è¯„ä¼°æŒ‡æ ‡ï¼›  
2. ä»æ€ç»´é“¾ä¸­æŠ½è±¡åŸå­æ¨ç†æ¨¡å¼å¹¶æ„å»ºæ ¸å¿ƒé›†çš„æ–¹å¼ï¼Œä¸ºæŒ–æ˜é«˜ä»·å€¼è®­ç»ƒæ•°æ®æä¾›äº†å¯å¤ç”¨çš„æ–¹æ³•è®ºï¼Œå¯ç”¨äºå…¶ä»–éœ€å¤æ‚æ¨ç†çš„ä»»åŠ¡åœºæ™¯æ•°æ®å¤„ç†ï¼›  
3. åŒç²’åº¦çš„æ•°æ®é€‰æ‹©ç®—æ³•å…¼é¡¾äº†æ¨ç†æ¨¡å¼å±‚é¢ä¸tokenå±‚é¢çš„ä¿¡æ¯ï¼Œè¿™ç§å¤šç»´åº¦ç­›é€‰é«˜ä»·å€¼æ•°æ®çš„æ€è·¯ï¼Œèƒ½ä¸ºå¤§æ¨¡å‹è®­ç»ƒä¸­æ•°æ®é«˜æ•ˆåˆ©ç”¨æä¾›å¯å‘ã€‚

## reinforcement-learning-fine-tuning-enhances-activation-intensity-and-diversity-in-the-internal-circuitry-of-llms
### Abstract
Large language models (LLMs) acquire extensive prior knowledge through
large-scale pretraining and can be further enhanced via supervised fine-tuning
(SFT) or reinforcement learning (RL)-based post-training. A growing body of
evidence has shown that RL fine-tuning improves the capability of LLMs beyond
what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning
is able to enhance the capability of various LLMs with distinct intrinsic
characteristics remain underexplored. In this study, we draw inspiration from
prior work on edge attribution patching (EAP) to investigate the internal
differences of LLMs before and after RL fine-tuning. Our analysis across
multiple model families shows two robust effects of online RL post-training:
(i) an overall increase in activation intensity, indicating that more internal
pathways are engaged and their signals become stronger, and (ii) greater
diversity in activation patterns, reflected by higher entropy and less
concentrated edge distributions. These changes suggest that RL reshapes
information flow to be both more redundant and more flexible, which may explain
its advantage in generalization. Notably, models fine-tuned with Direct
Preference Optimization (DPO) deviate from these trends, exhibiting
substantially weaker or inconsistent internal changes compared to PPO- and
GRPO-based training. Together, our findings provide a unified view of how RL
fine-tuning systematically alters the internal circuitry of LLMs and highlight
the methodological distinctions between online RL and preference-based
approaches. Our code is open source at
https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RLå¾®è°ƒå¦‚ä½•é‡å¡‘å¤§è¯­è¨€æ¨¡å‹å†…éƒ¨ç”µè·¯ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒç§¯ç´¯å¤§é‡å…ˆéªŒçŸ¥è¯†ï¼Œè¿˜èƒ½ç»æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒè¿›ä¸€æ­¥å¢å¼ºèƒ½åŠ›ã€‚å·²æœ‰ä¸å°‘è¯æ®è¡¨æ˜RLå¾®è°ƒèƒ½è®©LLMsèƒ½åŠ›è¶…è¶Šä»…SFTçš„æƒ…å†µï¼Œä½†RLå¾®è°ƒä¸ºä½•èƒ½å¢å¼ºä¸åŒå›ºæœ‰ç‰¹æ€§LLMsèƒ½åŠ›çš„åº•å±‚æœºåˆ¶å´ç ”ç©¶ä¸è¶³ã€‚æ‰€ä»¥æœ¬æ–‡å—è¾¹ç¼˜å½’å› ä¿®è¡¥ï¼ˆEAPï¼‰ç›¸å…³å·¥ä½œå¯å‘ï¼Œæ¢ç©¶LLMsåœ¨RLå¾®è°ƒå‰åçš„å†…éƒ¨å·®å¼‚ï¼Œä»¥æ­ç¤ºRLå¾®è°ƒæå‡æ¨¡å‹èƒ½åŠ›çš„å†…åœ¨åŸå› ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå€Ÿé‰´è¾¹ç¼˜å½’å› ä¿®è¡¥ï¼ˆEAPï¼‰æ–¹æ³•æ¢ç©¶å†…éƒ¨å·®å¼‚
å€ŸåŠ©EAPè¿™ä¸€å·¥å…·ï¼Œå¯¹å¤šä¸ªæ¨¡å‹å®¶æ—åœ¨RLå¾®è°ƒå‰åçš„å†…éƒ¨æƒ…å†µå±•å¼€åˆ†æï¼Œä»¥æ­¤æ¥æŒ–æ˜RLå¾®è°ƒç»™æ¨¡å‹å†…éƒ¨ç”µè·¯å¸¦æ¥çš„å˜åŒ–è§„å¾‹ï¼Œä¸ºç†è§£RLå¾®è°ƒä½œç”¨æœºåˆ¶æä¾›æŠ€æœ¯æ‰‹æ®µæ”¯æ’‘ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèšç„¦åœ¨çº¿RLåè®­ç»ƒçš„å½±å“åˆ†æç»´åº¦
ä»æ¿€æ´»å¼ºåº¦å’Œæ¿€æ´»æ¨¡å¼å¤šæ ·æ€§è¿™ä¸¤ä¸ªå…³é”®ç»´åº¦ï¼Œåˆ†æåœ¨çº¿RLåè®­ç»ƒå¯¹æ¨¡å‹å†…éƒ¨çš„å½±å“ï¼Œæ¸…æ™°ç•Œå®šå‡ºRLå¾®è°ƒå¸¦æ¥çš„å¦‚æ¿€æ´»å¼ºåº¦æ•´ä½“æå‡ã€æ¿€æ´»æ¨¡å¼æ›´å…·å¤šæ ·æ€§ç­‰æ ¸å¿ƒå˜åŒ–è¡¨ç°ï¼Œä»è€Œå‰–æRLå¦‚ä½•é‡å¡‘æ¨¡å‹ä¿¡æ¯æµã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¯¹å¤šä¸ªæ¨¡å‹å®¶æ—åˆ†æå‘ç°åœ¨çº¿RLåè®­ç»ƒæœ‰ä¸¤ä¸ªç¨³å¥æ•ˆåº”ï¼šä¸€æ˜¯æ¿€æ´»å¼ºåº¦æ•´ä½“æå‡ï¼Œæ„å‘³ç€æ›´å¤šå†…éƒ¨è·¯å¾„è¢«æ¿€æ´»ä¸”ä¿¡å·æ›´å¼ºï¼›äºŒæ˜¯æ¿€æ´»æ¨¡å¼å¤šæ ·æ€§å¢åŠ ï¼Œä½“ç°ä¸ºæ›´é«˜çš„ç†µå’Œæ›´ä¸é›†ä¸­çš„è¾¹ç¼˜åˆ†å¸ƒã€‚è¿™äº›å˜åŒ–è¡¨æ˜RLå°†ä¿¡æ¯æµé‡å¡‘å¾—æ›´å…·å†—ä½™æ€§ä¸çµæ´»æ€§ï¼Œæˆ–èƒ½è§£é‡Šå…¶æ³›åŒ–ä¼˜åŠ¿ã€‚è€Œç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¾®è°ƒçš„æ¨¡å‹åç¦»è¿™äº›è¶‹åŠ¿ï¼Œä¸åŸºäºPPOå’ŒGRPOè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œå†…éƒ¨å˜åŒ–æ˜æ˜¾æ›´å¼±æˆ–ä¸ä¸€è‡´ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä¸€æ–¹é¢ï¼Œæä¾›äº†ç†è§£RLå¾®è°ƒå¦‚ä½•ç³»ç»Ÿæ€§æ”¹å˜LLMså†…éƒ¨ç”µè·¯çš„ç»Ÿä¸€è§†è§’ï¼Œè®©ç ”ç©¶è€…èƒ½ä»æ¨¡å‹å†…éƒ¨æœºåˆ¶å±‚é¢æ›´æ·±å…¥è®¤è¯†RLå¾®è°ƒçš„ä»·å€¼ï¼Œä¸ºåç»­ä¼˜åŒ–RLå¾®è°ƒç­–ç•¥ç­‰æä¾›ç†è®ºå‚è€ƒï¼›å¦ä¸€æ–¹é¢ï¼Œå‡¸æ˜¾äº†åœ¨çº¿RLä¸åŸºäºåå¥½æ–¹æ³•ï¼ˆå¦‚DPOï¼‰çš„æ–¹æ³•å­¦å·®å¼‚ï¼Œèƒ½æŒ‡å¯¼ç ”ç©¶è€…åœ¨é€‰æ‹©æ¨¡å‹å¾®è°ƒæ–¹æ³•æ—¶ç»“åˆå¯¹å†…éƒ¨æœºåˆ¶å˜åŒ–çš„éœ€æ±‚æ¥è€ƒé‡ï¼›æ­¤å¤–ï¼Œå¼€æºä»£ç ä¹Ÿä¸ºç›¸å…³é¢†åŸŸç ”ç©¶è€…å¤ç°å®éªŒã€æ‹“å±•ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œåˆ©äºæ¨åŠ¨è¯¥æ–¹å‘ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚ 

## exmolrl--phenotype-target-joint-generation-of-de-novo-molecules-via-multi-objective-reinforcement-learning
### Abstract
The generation of high-quality candidate molecules remains a central
challenge in AI-driven drug design. Current phenotype-based and target-based
strategies each suffer limitations, either incurring high experimental costs or
overlook system-level cellular responses. To bridge this gap, we propose
ExMoIRL, a novel generative framework that synergistically integrates
phenotypic and target-specific cues for de novo molecular generation. The
phenotype-guided generator is first pretrained on expansive drug-induced
transcriptional profiles and subsequently fine-tuned via multi-objective
reinforcement learning (RL). Crucially, the reward function fuses docking
affinity and drug-likeness scores, augmented with ranking loss,
prior-likelihood regularization, and entropy maximization. The multi-objective
RL steers the model toward chemotypes that are simultaneously potent, diverse,
and aligned with the specified phenotypic effects. Extensive experiments
demonstrate ExMoIRL's superior performance over state-of-the-art
phenotype-based and target-based models across multiple well-characterized
targets. Our generated molecules exhibit favorable drug-like properties, high
target affinity, and inhibitory potency (IC50) against cancer cells. This
unified framework showcases the synergistic potential of combining
phenotype-guided and target-aware strategies, offering a more effective
solution for de novo drug discovery.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ExMolRLï¼šå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è¡¨å‹-é¶ç‚¹è”åˆä»å¤´åˆ†å­ç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„è¯ç‰©è®¾è®¡é¢†åŸŸï¼Œç”Ÿæˆé«˜è´¨é‡å€™é€‰åˆ†å­ä¸€ç›´æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚å½“å‰åŸºäºè¡¨å‹å’ŒåŸºäºé¶ç‚¹çš„ç­–ç•¥å„æœ‰å±€é™ï¼šåŸºäºè¡¨å‹çš„ç­–ç•¥å®éªŒæˆæœ¬é«˜ï¼Œè€ŒåŸºäºé¶ç‚¹çš„ç­–ç•¥æ˜“å¿½è§†ç³»ç»Ÿçº§çš„ç»†èƒå“åº”ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºExMolRLæ¡†æ¶ï¼Œæ—¨åœ¨ååŒæ•´åˆè¡¨å‹å’Œé¶ç‚¹ç‰¹å¼‚æ€§çº¿ç´¢æ¥è¿›è¡Œä»å¤´åˆ†å­ç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¡¨å‹å¼•å¯¼çš„ç”Ÿæˆå™¨é¢„è®­ç»ƒä¸å¾®è°ƒ  
é¦–å…ˆåœ¨å¤§é‡è¯ç‰©è¯±å¯¼çš„è½¬å½•ç»„è°±ä¸Šå¯¹è¡¨å‹å¼•å¯¼çš„ç”Ÿæˆå™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œä¹‹åé€šè¿‡å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œè®©æ¨¡å‹èƒ½å……åˆ†åˆ©ç”¨è¡¨å‹å±‚é¢çš„ä¿¡æ¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°è®¾è®¡  
å¥–åŠ±å‡½æ•°èåˆäº†å¯¹æ¥äº²å’ŒåŠ›å’Œç±»è¯æ€§åˆ†æ•°ï¼ŒåŒæ—¶åŠ å…¥æ’åºæŸå¤±ã€å…ˆéªŒä¼¼ç„¶æ­£åˆ™åŒ–å’Œç†µæœ€å¤§åŒ–ã€‚å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼æ¨¡å‹ç”ŸæˆåŒæ—¶å…·å¤‡é«˜æ•ˆåŠ›ã€å¤šæ ·æ€§ä¸”ä¸æŒ‡å®šè¡¨å‹æ•ˆåº”ä¸€è‡´çš„åŒ–å­¦å‹åˆ†å­ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªç‰¹å¾æ˜ç¡®çš„é¶ç‚¹ä¸Šï¼Œå¤§é‡å®éªŒè¡¨æ˜ExMolRLçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºäºè¡¨å‹å’ŒåŸºäºé¶ç‚¹çš„æ¨¡å‹ã€‚ç”Ÿæˆçš„åˆ†å­è¡¨ç°å‡ºè‰¯å¥½çš„ç±»è¯å±æ€§ã€é«˜é¶ç‚¹äº²å’ŒåŠ›ä»¥åŠå¯¹ç™Œç»†èƒçš„æŠ‘åˆ¶æ•ˆåŠ›ï¼ˆIC50ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è¯¥ç»Ÿä¸€æ¡†æ¶å±•ç¤ºäº†ç»“åˆè¡¨å‹å¼•å¯¼å’Œé¶ç‚¹æ„ŸçŸ¥ç­–ç•¥çš„ååŒæ½œåŠ›ï¼Œä¸ºä»å¤´è¯ç‰©å‘ç°æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å…¶å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°çš„è®¾è®¡æ€è·¯ã€è¡¨å‹ä¸é¶ç‚¹ä¿¡æ¯ååŒæ•´åˆçš„æ–¹å¼ç­‰ï¼Œå¯ä¸ºåç»­AIé©±åŠ¨è¯ç‰©è®¾è®¡é¢†åŸŸçš„æ¨¡å‹æ„å»ºæä¾›å‚è€ƒï¼Œå°¤å…¶æ˜¯åœ¨å¤šç»´åº¦ä¿¡æ¯èåˆä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹å‘ä¸Šå…·æœ‰å€Ÿé‰´ä»·å€¼ã€‚

## ce-gppo--coordinating-entropy-via-gradient-preserving-clipping-policy-optimization-in-reinforcement-learning
### Abstract
Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}oordinating \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | CE - GPPOï¼šå¼ºåŒ–å­¦ä¹ ä¸­è°ƒæ§ç†µçš„æ–°åˆ©å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„å¼ºå¤§èŒƒå¼ã€‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼Œæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºç®¡ç†ç­–ç•¥ç†µï¼Œå®ƒåæ˜ äº†è®­ç»ƒæœŸé—´æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚ç°æœ‰çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åŠå…¶å˜ä½“ç­‰æ–¹æ³•ï¼Œç”±äºè£å‰ªæœºåˆ¶ä¼šä¸¢å¼ƒæ¥è‡ªä½æ¦‚ç‡æ ‡è®°çš„æœ‰ä»·å€¼æ¢¯åº¦ä¿¡å·ã€‚æ·±å…¥åˆ†æå‘ç°ï¼Œè¿™äº›è¢«è£å‰ªçš„æ ‡è®°åœ¨è°ƒèŠ‚ç†µçš„æ¼”åŒ–ä¸­èµ·ç€å…³é”®ä½†è¢«å¿½è§†çš„ä½œç”¨ã€‚è‹¥å¿½è§†è¢«è£å‰ªçš„ä½æ¦‚ç‡æ ‡è®°ï¼ˆNA&LPå’ŒPA&LPæ ‡è®°ï¼‰ï¼Œä¼šå¯¼è‡´ç†µåå¡Œï¼ˆå› ç¼ºå°‘PA&LPæ ‡è®°ï¼‰å’Œç†µçˆ†ç‚¸ï¼ˆå› ç¼ºå°‘NA&LPæ ‡è®°ï¼‰ç­‰é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºç†µåŠ¨æ€çš„å†…åœ¨æœºåˆ¶
ç³»ç»Ÿåœ°åˆ†æäº†å¼ºåŒ–å­¦ä¹ ä¸­ç†µçš„åŠ¨æ€è¡Œä¸ºï¼Œæ­ç¤ºäº†å…¶ä¸ä¼˜åŠ¿å‡½æ•°å’Œæ ‡è®°æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å†…åœ¨ç›¸äº’ä½œç”¨ï¼Œå°†æ¢¯åº¦æ›´æ–°åˆ†ä¸ºå››ç§å…¸å‹æ¨¡å¼ï¼Œæ‰¾åˆ°äº†æ§åˆ¶ç†µæ¼”åŒ–çš„æ–°è§†è§’ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºCE - GPPOç®—æ³•
å°†ç†µåŠ¨æ€çš„æ§åˆ¶é‡æ–°æ„å»ºä¸ºå¯¹è£å‰ªåŒºé—´å¤–æ ‡è®°æ¢¯åº¦çš„ç®¡ç†ã€‚é€šè¿‡åœæ­¢æ¢¯åº¦æ“ä½œçº³å…¥è£å‰ªåŒºé—´å¤–æ ‡è®°çš„æ¢¯åº¦ï¼Œå¹¶è°ƒæ•´å…¶å¤§å°ï¼Œä»¥å°†ç­–ç•¥ç†µç»´æŒåœ¨è¾ƒé«˜ä¸”ç¨³å®šçš„æ°´å¹³ï¼Œå®ç°äº†å¯¹ç­–ç•¥ç†µçš„ç»†ç²’åº¦æ§åˆ¶å’Œæ›´æ–°ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜CE - GPPOåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç†µçš„ä¸ç¨³å®šæ€§ï¼Œå¹¶ä¸”è¡¨ç°å‡ºå¾ˆå¼ºçš„è¶…å‚æ•°é²æ£’æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç ”ç©¶æ€è·¯**ï¼šå¯¹äºå¤æ‚é—®é¢˜ï¼Œæ·±å…¥åˆ†æå…¶å†…åœ¨æœºåˆ¶ï¼Œä»çœ‹ä¼¼å¸¸è§„çš„æ–¹æ³•ä¸­æŒ–æ˜è¢«å¿½è§†çš„å…³é”®å› ç´ ï¼Œä¸ºè§£å†³é—®é¢˜æä¾›æ–°çš„è§†è§’å’Œæ€è·¯ã€‚
2. **ç®—æ³•æ”¹è¿›**ï¼šåœ¨ç°æœ‰ç®—æ³•åŸºç¡€ä¸Šï¼Œé€šè¿‡å·§å¦™åœ°è°ƒæ•´å’Œæ”¹è¿›ï¼Œåœ¨ä¸ç ´ååŸæœ‰ç®—æ³•ç¨³å®šæ€§çš„å‰æä¸‹ï¼Œæå‡ç®—æ³•æ€§èƒ½ï¼Œè¿™ç§æ”¹è¿›æ–¹å¼å¯¹äºå…¶ä»–ç®—æ³•çš„ä¼˜åŒ–å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
3. **å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå®ç°æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡è‡³å…³é‡è¦ï¼ŒCE - GPPOé€šè¿‡å¯¹ä¸åŒæ ‡è®°æ¢¯åº¦çš„å¤„ç†æ¥åè°ƒç­–ç•¥ç†µï¼Œä»è€Œè¾¾åˆ°è¿™ä¸€å¹³è¡¡ï¼Œä¸ºåœ¨ç±»ä¼¼åœºæ™¯ä¸­è§£å†³è¯¥é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚
``` 

## failure-modes-of-maximum-entropy-rlhf
### Abstract
In this paper, we show that Simple Preference Optimization (SimPO) can be
derived as Maximum Entropy Reinforcement Learning with length-normalized
temperature, providing a theoretical foundation for this reference-free method.
Motivated by SimPO's strong performance in offline preference optimization, we
investigate whether Maximum Entropy RL can achieve similar results in online
RLHF settings. Our experiments find that Maximum Entropy RL consistently
exhibits overoptimization and unstable KL dynamics, even at very low learning
rates. Unlike KL-constrained methods that maintain stable training, entropy
regularization fails to prevent reward hacking and appears to correlate with
overoptimization. Lastly, we discuss possible explanations for why SimPO
succeeds in offline settings while Maximum Entropy RL struggles in online
scenarios. Our findings suggest that reference-free approaches may face
distinct challenges when applied to online or offline preference learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜æœ€å¤§ç†µRLHFçš„å¤±æ•ˆæ¨¡å¼ï¼šSimPOç†è®ºæº¯æºä¸åœ¨çº¿RLHFæŒ‘æˆ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨AIç³»ç»Ÿä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„ç ”ç©¶ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸»æµæ–¹æ³•ï¼Œä½†ä¼ ç»ŸRLHF pipelineï¼ˆç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼‰å­˜åœ¨è®¡ç®—é‡å¤§ã€æ“ä½œå¤æ‚ç­‰é—®é¢˜ï¼Œæ¨åŠ¨äº†ç›´æ¥å¯¹é½ç®—æ³•ï¼ˆå¦‚DPOã€SimPOï¼‰çš„æ¢ç´¢ã€‚SimPOä½œä¸ºæ— å‚è€ƒæ¨¡å‹çš„æ–¹æ³•åœ¨ç¦»çº¿åå¥½ä¼˜åŒ–ä¸­è¡¨ç°å‡ºè‰²ï¼Œå´ç¼ºä¹ç†è®ºåŸºç¡€ï¼›åŒæ—¶ï¼ŒåŸºäºæœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ åœ¨ç¦»çº¿åœºæ™¯åŠ©åŠ›SimPOçš„è¡¨ç°ï¼Œå¼•å‘äº†å…¶åœ¨åœ¨çº¿RLHFåœºæ™¯æ˜¯å¦æœ‰æ•ˆçš„ç–‘é—®ï¼Œè¿™æ„æˆäº†æœ¬æ–‡ç ”ç©¶çš„åŠ¨æœºâ€”â€”æ¢ç©¶SimPOçš„ç†è®ºæ ¹åŸºåŠæœ€å¤§ç†µRLåœ¨åœ¨çº¿RLHFçš„è¡¨ç°ä¸æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSimPOçš„ç†è®ºæº¯æºâ€”â€”æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ è§†è§’  
æœ¬æ–‡è¯æ˜Simple Preference Optimizationï¼ˆSimPOï¼‰å¯è¢«æ¨å¯¼ä¸º**å¸¦é•¿åº¦å½’ä¸€åŒ–æ¸©åº¦çš„æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ **å½¢å¼ï¼Œä¸ºè¿™ä¸€æ— å‚è€ƒæ¨¡å‹çš„æ–¹æ³•æä¾›äº†ç†è®ºåŸºç¡€ã€‚å…·ä½“è€Œè¨€ï¼ŒSimPOå¯è§£é‡Šä¸ºæœ€å¤§ç†µRLç›®æ ‡åœ¨é•¿åº¦å½’ä¸€åŒ–æ¸©åº¦ç¼©æ”¾ä¸‹çš„é—­å¼è§£ï¼Œå¡«è¡¥äº†SimPOç¼ºä¹ç†è®ºæ”¯æ’‘çš„ç©ºç™½ï¼Œä½¿å…¶ä¸DPOä¾æ‰˜KLçº¦æŸRLçš„ç†è®ºåœ°ä½ç›¸å‘¼åº”ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨çº¿RLHFä¸­æœ€å¤§ç†µRLä¸KLçº¦æŸRLçš„å¯¹æ¯”å®éªŒè®¾è®¡  
å—SimPOåœ¨ç¦»çº¿åœºæ™¯è¡¨ç°çš„å¯å‘ï¼Œæœ¬æ–‡æ¢ç©¶æœ€å¤§ç†µRLåœ¨**åœ¨çº¿RLHFåœºæ™¯**æ˜¯å¦èƒ½åª²ç¾KLçº¦æŸæ–¹æ³•ã€‚é€šè¿‡åœ¨TL;DRæ‘˜è¦åŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨Pythiaç³»åˆ—æ¨¡å‹å¯¹æ¯”æœ€å¤§ç†µRLä¸KLçº¦æŸRLçš„å®éªŒï¼Œæ·±å…¥åˆ†æä¸¤è€…åœ¨è®­ç»ƒç¨³å®šæ€§ã€ä¼˜åŒ–ç¨‹åº¦ç­‰ç»´åº¦çš„è¡¨ç°å·®å¼‚ï¼ŒæŒ–æ˜æœ€å¤§ç†µRLåœ¨åœ¨çº¿åœºæ™¯çš„å¤±æ•ˆæ¨¡å¼ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒå‘ç°æœ€å¤§ç†µRLåœ¨åœ¨çº¿RLHFåœºæ™¯å­˜åœ¨æ˜¾è‘—é—®é¢˜ï¼šå³ä½¿é‡‡ç”¨æä½å­¦ä¹ ç‡ï¼Œä¹Ÿ**æŒç»­è¡¨ç°å‡ºè¿‡åº¦ä¼˜åŒ–ä¸ä¸ç¨³å®šçš„KLåŠ¨æ€**ï¼›ä¸ç»´æŒç¨³å®šè®­ç»ƒçš„KLçº¦æŸæ–¹æ³•ä¸åŒï¼Œç†µæ­£åˆ™åŒ–æ— æ³•é˜»æ­¢â€œå¥–åŠ±é»‘å®¢ï¼ˆreward hackingï¼‰â€ç°è±¡ï¼Œä¸”ç†µçš„å¢åŠ ä¸è¿‡åº¦ä¼˜åŒ–å­˜åœ¨ç›¸å…³æ€§ã€‚è€ŒSimPOåœ¨ç¦»çº¿åœºæ™¯æˆåŠŸï¼Œåœ¨çº¿æœ€å¤§ç†µRLå´é‡é˜»ï¼ŒèƒŒåå¯èƒ½æ˜¯SimPOå—ç›Šäºæ•°æ®é›†çº¦æŸã€ç›®æ ‡è¾¹é™…ç­‰éšæ€§ç¨³å®šå› ç´ ï¼Œåœ¨çº¿åœºæ™¯ä¸­è¿™äº›ä¿æŠ¤æœºåˆ¶ç¼ºå¤±ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºå±‚é¢**ï¼šä¸ºæ— å‚è€ƒæ¨¡å‹çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚SimPOï¼‰æä¾›äº†ä¸ç»å…¸å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆæœ€å¤§ç†µRLï¼‰çš„è¿æ¥ï¼Œå¸®åŠ©ç†è§£è¿™ç±»æ–¹æ³•çš„æœ¬è´¨ï¼Œåç»­ç ”ç©¶å¯åŸºäºæ­¤æ‹“å±•æ— å‚è€ƒæ¨¡å‹æ–¹æ³•çš„ç†è®ºæ·±åº¦ã€‚  
2. **å®è·µå±‚é¢**ï¼šæ­ç¤ºäº†ç†µæ­£åˆ™åŒ–åœ¨åœ¨çº¿RLHFåœºæ™¯çš„å±€é™æ€§ï¼Œè­¦ç¤ºç ”ç©¶è€…åœ¨è®¾è®¡åœ¨çº¿å¯¹é½ç®—æ³•æ—¶éœ€å…³æ³¨ç¨³å®šè®­ç»ƒçš„çº¦æŸæœºåˆ¶ï¼ˆå¦‚å‚è€ƒæ¨¡å‹ã€æ•°æ®é›†éšå¼çº¦æŸç­‰ï¼‰ï¼›åŒæ—¶ï¼ŒSimPOåœ¨ç¦»çº¿åœºæ™¯åˆ©ç”¨æ•°æ®é›†ä¸ç›®æ ‡è¾¹é™…ç¨³å®šä¼˜åŒ–çš„æ€è·¯ï¼Œä¸ºç¦»çº¿åå¥½å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›äº†â€œéšæ€§æ­£åˆ™åŒ–â€çš„å€Ÿé‰´æ–¹å‘ã€‚  
3. **ç ”ç©¶è§†è§’**ï¼šæå‡ºæ— å‚è€ƒæ¨¡å‹æ–¹æ³•åœ¨åœ¨çº¿ä¸ç¦»çº¿åå¥½å­¦ä¹ ä¸­å¯èƒ½é¢ä¸´ä¸åŒæŒ‘æˆ˜ï¼Œä¸ºåç»­åŒºåˆ†åœºæ™¯è®¾è®¡ç®—æ³•æä¾›äº†æ€è€ƒç»´åº¦ï¼Œæ¨åŠ¨å­¦ç•Œæ›´ç»†è‡´åœ°æ¢ç´¢ä¸åŒåœºæ™¯ä¸‹çš„å¯¹é½ç­–ç•¥ã€‚  

## mental-accounts-for-actions--ewa-inspired-attention-in-decision-transformers
### Abstract
Transformers have emerged as a compelling architecture for sequential
decision-making by modeling trajectories via self-attention. In reinforcement
learning (RL), they enable return-conditioned control without relying on value
function approximation. Decision Transformers (DTs) exploit this by casting RL
as supervised sequence modeling, but they are restricted to offline data and
lack exploration. Online Decision Transformers (ODTs) address this limitation
through entropy-regularized training on on-policy rollouts, offering a stable
alternative to traditional RL methods like Soft Actor-Critic, which depend on
bootstrapped targets and reward shaping. Despite these advantages, ODTs use
standard attention, which lacks explicit memory of action-specific outcomes.
This leads to inefficiencies in learning long-term action effectiveness.
Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we
propose Experience-Weighted Attraction with Vector Quantization for Online
Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains
per-action mental accounts summarizing recent successes and failures.
Continuous actions are routed via direct grid lookup to a compact
vector-quantized codebook, where each code stores a scalar attraction updated
online through decay and reward-based reinforcement. These attractions modulate
attention by biasing the columns associated with action tokens, requiring no
change to the backbone or training objective. On standard continuous-control
benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,
particularly in early training. The module is computationally efficient,
interpretable via per-code traces, and supported by theoretical guarantees that
bound the attraction dynamics and its impact on attention drift.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸ºåœ¨çº¿å†³ç­–Transformeræ³¨å…¥è®¤çŸ¥çµæ„Ÿï¼šEWA-VQ-ODTæå‡é•¿æœŸå†³ç­–æ•ˆç‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢†åŸŸï¼ŒTransformeræ¶æ„å› èƒ½é€šè¿‡è‡ªæ³¨æ„åŠ›å»ºæ¨¡è½¨è¿¹ï¼Œä¸ºåºåˆ—å†³ç­–æä¾›äº†æ–°æ€è·¯ã€‚å†³ç­–Transformerï¼ˆDTï¼‰å°†RLè½¬åŒ–ä¸ºç›‘ç£åºåˆ—å»ºæ¨¡ï¼Œä½†å—é™äºç¦»çº¿æ•°æ®ä¸”ç¼ºä¹æ¢ç´¢ï¼›åœ¨çº¿å†³ç­–Transformerï¼ˆODTï¼‰é€šè¿‡å¯¹ç­–ç•¥è½¨è¿¹çš„ç†µæ­£åˆ™åŒ–è®­ç»ƒè§£å†³äº†éƒ¨åˆ†é—®é¢˜ï¼Œæˆä¸ºSoft Actor - Criticç­‰ä¼ ç»Ÿä¾èµ–è‡ªä¸¾ç›®æ ‡å’Œå¥–åŠ±å¡‘é€ æ–¹æ³•çš„ç¨³å®šæ›¿ä»£ã€‚ç„¶è€Œï¼ŒODTé‡‡ç”¨çš„æ ‡å‡†æ³¨æ„åŠ›ç¼ºä¹å¯¹ç‰¹å®šåŠ¨ä½œç»“æœçš„æ˜¾å¼è®°å¿†ï¼Œåœ¨å­¦ä¹ é•¿æœŸåŠ¨ä½œæœ‰æ•ˆæ€§æ—¶æ•ˆç‡ä½ä¸‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡å—è®¤çŸ¥æ¨¡å‹ï¼ˆå¦‚Experience - Weighted Attractionï¼ŒEWAï¼‰å¯å‘ï¼Œæ—¨åœ¨æå‡ODTå¯¹åŠ¨ä½œé•¿æœŸæ•ˆæœçš„å­¦ä¹ èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEWA - VQ - ODTæ¨¡å—
å—EWAè®¤çŸ¥æ¨¡å‹å¯å‘ï¼Œè®¾è®¡äº†Experience - Weighted Attraction with Vector Quantization for Online Decision Transformersï¼ˆEWA - VQ - ODTï¼‰è¿™ä¸€è½»é‡çº§æ¨¡å—ã€‚è¯¥æ¨¡å—ä¸ºæ¯ä¸ªåŠ¨ä½œç»´æŠ¤â€œå¿ƒç†è´¦æˆ·â€ï¼Œæ€»ç»“è¿‘æœŸåŠ¨ä½œçš„æˆåŠŸä¸å¤±è´¥æƒ…å†µã€‚å¯¹äºè¿ç»­åŠ¨ä½œï¼Œé€šè¿‡ç›´æ¥ç½‘æ ¼æŸ¥æ‰¾è·¯ç”±åˆ°ç´§å‡‘çš„å‘é‡é‡åŒ–ç æœ¬ï¼Œæ¯ä¸ªç å­˜å‚¨ä¸€ä¸ªæ ‡é‡å¸å¼•åŠ›ï¼Œè¯¥å¸å¼•åŠ›é€šè¿‡è¡°å‡å’ŒåŸºäºå¥–åŠ±çš„å¼ºåŒ–åœ¨çº¿æ›´æ–°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¸å¼•åŠ›è°ƒåˆ¶æ³¨æ„åŠ›æœºåˆ¶
åˆ©ç”¨ä¸Šè¿°ç æœ¬ä¸­å­˜å‚¨çš„å¸å¼•åŠ›æ¥è°ƒåˆ¶æ³¨æ„åŠ›ï¼Œå…·ä½“æ˜¯å¯¹ä¸åŠ¨ä½œtokenç›¸å…³çš„åˆ—è¿›è¡Œåç½®ï¼Œä¸”æ— éœ€æ”¹å˜Transformerçš„ backboneï¼ˆä¸»å¹²ç»“æ„ï¼‰å’Œè®­ç»ƒç›®æ ‡ï¼Œä»¥ä¸€ç§è½»é‡ä¸”å…¼å®¹çš„æ–¹å¼å¢å¼ºäº†ODTå¯¹åŠ¨ä½œé•¿æœŸæ•ˆæœçš„å­¦ä¹ èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ ‡å‡†è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEWA - VQ - ODTç›¸è¾ƒäºODTæå‡äº†æ ·æœ¬æ•ˆç‡å’Œå¹³å‡å›æŠ¥ï¼Œå°¤å…¶åœ¨è®­ç»ƒæ—©æœŸè¡¨ç°çªå‡ºï¼Œè¯æ˜äº†è¯¥æ¨¡å—åœ¨æå‡åœ¨çº¿å†³ç­–Transformeræ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å—è®¡ç®—é«˜æ•ˆï¼Œè¿˜èƒ½é€šè¿‡æ¯ä¸ªç çš„è½¨è¿¹å®ç°å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”æœ‰ç†è®ºä¿è¯æ¥çº¦æŸå¸å¼•åŠ›åŠ¨æ€åŠå…¶å¯¹æ³¨æ„åŠ›æ¼‚ç§»çš„å½±å“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è®¤çŸ¥æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼šå°†è®¤çŸ¥é¢†åŸŸçš„EWAæ¨¡å‹å¼•å…¥å¼ºåŒ–å­¦ä¹ çš„Transformeræ¶æ„æ”¹è¿›ï¼Œä¸ºè·¨é¢†åŸŸçµæ„Ÿèåˆæä¾›äº†èŒƒä¾‹ï¼Œå¯ç¤ºç ”ç©¶è€…å¯å…³æ³¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚å¿ƒç†å­¦ã€ç¥ç»ç§‘å­¦ï¼‰çš„æ¨¡å‹å¯¹AIæŠ€æœ¯æ”¹è¿›çš„æ½œåœ¨ä»·å€¼ã€‚
2. è½»é‡çº§æ¨¡å—è®¾è®¡ï¼šEWA - VQ - ODTä½œä¸ºè½»é‡çº§æ¨¡å—ï¼Œæ— éœ€å¯¹ä¸»å¹²ç»“æ„å’Œè®­ç»ƒç›®æ ‡åšå¤§æ”¹å°±èƒ½æå‡æ€§èƒ½ï¼Œè¿™ç§â€œå°è€Œç¾â€çš„æ”¹è¿›æ€è·¯åœ¨å®é™…å·¥ç¨‹å’Œç ”ç©¶ä¸­å…·æœ‰å¯å€Ÿé‰´æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å·²æœ‰æˆç†Ÿæ¶æ„åŸºç¡€ä¸Šåšä¼˜åŒ–æ—¶ï¼Œè½»é‡çº§æ¨¡å—æ›´å®¹æ˜“é›†æˆå’Œæ¨å¹¿ã€‚
3. å¯è§£é‡Šæ€§ä¸ç†è®ºä¿éšœï¼šæ¨¡å—å…¼å…·å¯è§£é‡Šæ€§ï¼ˆé€šè¿‡per - code tracesï¼‰å’Œç†è®ºä¿è¯ï¼Œåœ¨è¿½æ±‚æ¨¡å‹æ€§èƒ½çš„åŒæ—¶å…³æ³¨å¯è§£é‡Šæ€§å’Œç†è®ºæ”¯æ’‘ï¼Œä¸ºæ„å»ºæ›´å¯é çš„AIç³»ç»Ÿæä¾›äº†æ–¹å‘ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§åœ¨æ–¹æ³•åˆ›æ–°æ—¶å…¼é¡¾è§£é‡Šæ€§ä¸ç†è®ºæ€§çš„æ€è·¯ã€‚

## evolving-language-models-without-labels--majority-drives-selection--novelty-promotes-variation
### Abstract
Large language models (LLMs) are increasingly trained with reinforcement
learning from verifiable rewards (RLVR), yet real-world deployment demands
models that can self-improve without labels or external judges. Existing
self-improvement approaches primarily rely on self-confirmation signals (e.g.,
confidence, entropy, or consistency) to generate rewards. This reliance drives
models toward over-confident, majority-favored solutions, causing an entropy
collapse that degrades pass@n and reasoning complexity. To address this, we
propose EVOL-RL, a label-free framework that mirrors the evolutionary principle
of balancing selection with variation. Concretely, EVOL-RL retains the
majority-voted answer as an anchor for stability, but adds a novelty-aware
reward that scores each sampled solution by how different its reasoning is from
other concurrently generated responses. This majority-for-stability +
novelty-for-exploration rule mirrors the variation-selection principle:
selection prevents drift, while novelty prevents collapse. Evaluation results
show that EVOL-RL consistently outperforms the majority-only baseline; e.g.,
training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's
4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents
in-domain diversity collapse but also improves out-of-domain generalization
(from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The
code is available at: https://github.com/YujunZhou/EVOL-RL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— æ ‡ç­¾è¿›åŒ–è¯­è¨€æ¨¡å‹ï¼šå¤šæ•°é©±åŠ¨é€‰æ‹©ï¼Œæ–°é¢–æ€§ä¿ƒè¿›å˜å¼‚

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸‹è®­ç»ƒå–å¾—è¿›å±•ï¼Œä½†ç°å®éƒ¨ç½²éœ€è¦æ¨¡å‹åœ¨æ— æ ‡ç­¾æˆ–æ— å¤–éƒ¨è¯„åˆ¤ä¸‹è‡ªæˆ‘æ”¹è¿›ã€‚ç°æœ‰è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ä¾èµ–è‡ªæˆ‘ç¡®è®¤ä¿¡å·ï¼ˆå¦‚ç½®ä¿¡åº¦ã€ç†µã€ä¸€è‡´æ€§ï¼‰ç”Ÿæˆå¥–åŠ±ï¼Œè¿™ä¼šä½¿æ¨¡å‹è¶‹å‘è¿‡åº¦è‡ªä¿¡ã€å¤šæ•°åå¥½çš„è§£å†³æ–¹æ¡ˆï¼Œå¼•å‘â€œç†µåç¼©â€ï¼Œé™ä½pass@nå’Œæ¨ç†å¤æ‚åº¦ã€‚åŒæ—¶ï¼Œåœ¨æ— æ ‡ç­¾è®¾ç½®ä¸­å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„å›°å¢ƒä¸¥å³»ï¼Œä¾èµ–å†…éƒ¨ä¿¡å·çš„å­¦ä¹ è¿‡ç¨‹ä¼šä¸»åŠ¨é™ä½å¥–åŠ±ä¿¡å·è´¨é‡ï¼Œå½¢æˆé€€åŒ–åé¦ˆå¾ªç¯ï¼Œå¯¼è‡´ç­–ç•¥åç¼©åˆ°ä½ç†µçŠ¶æ€ï¼Œæ¨ç†å¤šæ ·æ€§ä¸‹é™ç­‰é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEVOL - RLæ¡†æ¶ï¼Œå€Ÿé‰´è¿›åŒ–ä¸­é€‰æ‹©ä¸å˜å¼‚å¹³è¡¡çš„åŸç†ã€‚å°†æ— æ ‡ç­¾å­¦ä¹ æ„å»ºä¸ºè¿›åŒ–ç³»ç»Ÿï¼ŒæŠŠå¤šæ ·æ€§åç¼©è¯Šæ–­ä¸ºè¿‡æ—©æ”¶æ•›é—®é¢˜ï¼Œç”¨é€‰æ‹©ä¸å˜å¼‚å¹³è¡¡çš„æ ¸å¿ƒè¿›åŒ–åŸç†è§£å†³ã€‚EVOL - RLä¿ç•™å¤šæ•°æŠ•ç¥¨ç­”æ¡ˆä½œä¸ºç¨³å®šæ€§é”šç‚¹ï¼ŒåŒæ—¶æ·»åŠ æ–°é¢–æ€§æ„ŸçŸ¥å¥–åŠ±ï¼Œä¾æ®æ¯ä¸ªé‡‡æ ·è§£å†³æ–¹æ¡ˆçš„æ¨ç†ä¸å…¶ä»–åŒæ—¶ç”Ÿæˆå“åº”çš„å·®å¼‚ç¨‹åº¦ï¼ˆæ¨ç†è½¨è¿¹çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰æ¥è¯„åˆ†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡å®ç”¨çš„æ–°é¢–æ€§æ„ŸçŸ¥å¥–åŠ±ï¼Œè¡¥å……å¤šæ•°é€‰æ‹©ï¼Œå®ç°ç¨³å®šçš„æ— æ ‡ç­¾æ”¹è¿›ã€‚è¯¥æ¡†æ¶éµå¾ªâ€œå¤šæ•°ä¿ç¨³å®š + æ–°é¢–ä¿ƒæ¢ç´¢â€è§„åˆ™ï¼Œæ¨¡æ‹Ÿå˜å¼‚ - é€‰æ‹©åŸç†ï¼šé€‰æ‹©é˜²æ­¢æ¼‚ç§»ï¼Œæ–°é¢–æ€§é˜²æ­¢åç¼©ï¼Œåœ¨ä¼˜åŒ–å·²çŸ¥è§£å†³æ–¹æ¡ˆå’Œå‘ç°æ–°æ–¹æ¡ˆé—´å»ºç«‹å¥åº·å¹³è¡¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ— æ ‡ç­¾AIME24ä¸Šè®­ç»ƒæ—¶ï¼ŒEVOL - RLå°†Qwen3 - 4B - Baseåœ¨AIME25çš„pass@1ä»åŸºçº¿çš„4.6%æå‡åˆ°16.4%ï¼Œpass@16ä»18.5%æå‡åˆ°37.9%ï¼›åœ¨MATH - 500æ— æ ‡ç­¾è®­ç»ƒçš„å®éªŒä¸­ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„Test - Time Reinforcement Learningï¼ˆTTRLï¼‰ï¼ŒEVOL - RLæ”¹å–„äº†å‡†ç¡®æ€§ï¼Œç»´æŒäº†æ¨ç†å¤šæ ·æ€§ï¼Œé¿å…äº†pass@nä¸‹é™ã€æ¨ç†é•¿åº¦ç¼©çŸ­å’Œç†µåç¼©ç­‰é—®é¢˜ï¼›æ­¤å¤–ï¼ŒEVOL - RLä¸ä»…é˜²æ­¢äº†åŸŸå†…å¤šæ ·æ€§åç¼©ï¼Œè¿˜æå‡äº†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ï¼ˆä»æ•°å­¦æ¨ç†åˆ°GPQAã€MMLU - Proã€BBEHç­‰æ›´å¹¿æ³›ä»»åŠ¡ï¼‰ï¼Œåœ¨æ— ç›‘ç£RLè®­ç»ƒä¸­å–å¾—äº†å½“å‰æœ€ä¼˜ç»“æœï¼Œåœ¨AIME25åŸºå‡†æµ‹è¯•ä¸­pass@1å‡†ç¡®ç‡æå‡è¶…3å€ï¼Œpass@16å‡†ç¡®ç‡æå‡1å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç ”ç©¶è§†è§’çœ‹ï¼Œå°†æ— æ ‡ç­¾å­¦ä¹ è§†ä¸ºè¿›åŒ–ç³»ç»Ÿçš„æ–°è§†è§’ï¼Œä¸ºè¯Šæ–­å’Œè§£å†³æ¨¡å‹è®­ç»ƒä¸­çš„é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå¯å¯å‘åç»­åœ¨æ— ç›‘ç£æˆ–è‡ªç›‘ç£å­¦ä¹ æ–¹å‘å¯¹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„åˆ†æï¼›ä»æ–¹æ³•è®¾è®¡çœ‹ï¼ŒEVOL - RLä¸­å¹³è¡¡é€‰æ‹©ä¸å˜å¼‚çš„æ€è·¯ï¼Œä»¥åŠæ–°é¢–æ€§æ„ŸçŸ¥å¥–åŠ±çš„è®¾è®¡ï¼Œä¸ºæ‰“é€ æ›´ç¨³å®šã€æ›´å…·æ¢ç´¢æ€§çš„æ— æ ‡ç­¾å­¦ä¹ æ¡†æ¶æä¾›äº†å®è·µå‚è€ƒï¼Œå¯ç”¨äºæ”¹è¿›å…¶ä»–éœ€è¦å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„å¼ºåŒ–å­¦ä¹ æˆ–è‡ªæ”¹è¿›æ¨¡å‹åœºæ™¯ï¼›ä»å®éªŒéªŒè¯è§’åº¦ï¼Œå…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æœ‰æ•ˆéªŒè¯ï¼Œä¸ºç›¸å…³æ–¹æ³•çš„æ•ˆæœè¯„ä¼°æä¾›äº†å¯å‚è€ƒçš„å®éªŒèŒƒå¼ï¼Œæœ‰åŠ©äºåç»­ç±»ä¼¼ç ”ç©¶çš„å®éªŒè®¾è®¡ä¸å¯¹æ¯”åˆ†æã€‚

## gta--supervised-guided-reinforcement-learning-for-text-classification-with-large-language-models
### Abstract
In natural language processing tasks, pure reinforcement learning (RL)
fine-tuning methods often suffer from inefficient exploration and slow
convergence; while supervised fine-tuning (SFT) methods, although efficient in
training, have limited performance ceiling and less solid theoretical
foundation compared to RL. To address efficiency-capability trade-off, we
propose the Guess-Think-Answer (GTA) framework that combines the efficiency of
SFT with the capability gains of RL in a unified training paradigm. GTA works
by having the model first produce a provisional guess (optimized via
cross-entropy loss), then reflect on this guess before generating the final
answer, with RL rewards shaping both the final output and the format of the
entire GTA structure. This hybrid approach achieves both faster convergence
than pure RL and higher performance ceiling than pure SFT. To mitigate gradient
conflicts between the two training signals, we employ loss masking and gradient
constraints. Empirical results on four text classification benchmarks
demonstrate that GTA substantially accelerates convergence while outperforming
both standalone SFT and RL baselines.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹æ–‡æœ¬åˆ†ç±»æ–°èŒƒå¼ï¼šGTAæ¡†æ¶èåˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŠ¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œçº¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæ–¹æ³•å­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ã€æ”¶æ•›æ…¢çš„é—®é¢˜ï¼›è€Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è™½è®­ç»ƒé«˜æ•ˆï¼Œä½†æ€§èƒ½ä¸Šé™æœ‰é™ä¸”ç†è®ºåŸºç¡€ä¸å¦‚RLæ‰å®ã€‚åŒæ—¶ï¼Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæŠ€æœ¯è™½èƒ½æå‡æ¨ç†ä»»åŠ¡è¡¨ç°ï¼Œå´éœ€å¤§é‡äººå·¥æ ‡æ³¨æ¨ç†é“¾ï¼Œæˆæœ¬é«˜ä¸”æ˜“å—åå·®å½±å“ã€‚ä¸ºå¹³è¡¡æ•ˆç‡ä¸èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºGuess - Think - Answerï¼ˆGTAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€è®­ç»ƒèŒƒå¼ä¸­ç»“åˆSFTçš„æ•ˆç‡ä¸RLçš„èƒ½åŠ›å¢ç›Šã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºGTAæ¡†æ¶é‡æ„æ¨ç†æµç¨‹  
å°†æ¨ç†è¿‡ç¨‹ç»“æ„åŒ–åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œé¦–å…ˆç”Ÿæˆç›´è§‚çš„åˆæ­¥çŒœæµ‹ï¼ˆGuessï¼‰ï¼Œè¯¥é˜¶æ®µé€šè¿‡äº¤å‰ç†µæŸå¤±ç›‘ç£ä¼˜åŒ–ï¼›æ¥ç€åŸºäºåˆæ­¥çŒœæµ‹å’Œè¾“å…¥é—®é¢˜è¿›è¡Œæ˜¾å¼æ¨ç†ï¼ˆThinkï¼‰ï¼›æœ€åæ•´åˆæ¨ç†ç»“æœç”Ÿæˆç²¾ç‚¼çš„æœ€ç»ˆç­”æ¡ˆï¼ˆAnswerï¼‰ï¼ŒRLå¥–åŠ±åŒæ—¶å¡‘é€ æœ€ç»ˆè¾“å‡ºä¸æ•´ä¸ªGTAç»“æ„çš„æ ¼å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå•é˜¶æ®µç»Ÿä¸€SFTä¸RLè®­ç»ƒ  
åœ¨åŒä¸€è®­ç»ƒæµç¨‹ä¸­æ— ç¼æ•´åˆSFTä¸RLã€‚å¯¹Guessç»„ä»¶ç”¨äº¤å‰ç†µæŸå¤±ç›‘ç£ï¼Œå¯¹æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆåˆ™é€šè¿‡åŸºäºRLçš„å¥–åŠ±ä¼˜åŒ–ã€‚ä¸ºç¼“è§£ä¸¤ç§è®­ç»ƒä¿¡å·é—´çš„æ¢¯åº¦å†²çªï¼Œå¼•å…¥ä¸“é—¨çš„æŸå¤±æ©ç ç­–ç•¥å’Œæ¢¯åº¦ä½™å¼¦è°ƒæ•´æŠ€æœ¯ï¼Œç¡®ä¿ä¸åŒå­¦ä¹ èŒƒå¼æœ‰æ•ˆåä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ— éœ€äººå·¥æ ‡æ³¨æ¨ç†é“¾  
è®©æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªå‘å­¦ä¹ æœ‰æ•ˆæ¨ç†æ¨¡å¼ï¼Œæ‘†è„±äº†å¯¹äººå·¥æ ‡æ³¨æ¨ç†é“¾çš„ä¾èµ–ï¼Œé™ä½æˆæœ¬ä¸”é¿å…æ ‡æ³¨åå·®ç­‰é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªæ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTAæ¡†æ¶å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼šç›¸æ¯”çº¯RLæ–¹æ³•æ”¶æ•›é€Ÿåº¦å¤§å¹…æå‡ï¼Œç›¸æ¯”çº¯SFTæ–¹æ³•æ€§èƒ½ä¸Šé™æ›´é«˜ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Šäº†å•ç‹¬çš„SFTå’Œå…ˆè¿›RLåŸºçº¿æ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¶æ„è®¾è®¡å±‚é¢ï¼šGTAæ¡†æ¶å¯¹æ¨ç†æµç¨‹çš„é˜¶æ®µåŒ–æ‹†åˆ†æ€è·¯ï¼Œä¸ºç»“åˆä¸åŒè®­ç»ƒèŒƒå¼è§£å†³ä»»åŠ¡æä¾›äº†ç»“æ„åŒ–å‚è€ƒï¼Œå¯å¯å‘å…¶ä»–éœ€å¹³è¡¡æ•ˆç‡ä¸èƒ½åŠ›çš„NLPä»»åŠ¡æ¶æ„è®¾è®¡ã€‚  
2. è®­ç»ƒä¼˜åŒ–å±‚é¢ï¼šé’ˆå¯¹å¤šè®­ç»ƒä¿¡å·æ¢¯åº¦å†²çªé—®é¢˜ï¼Œé‡‡ç”¨çš„æŸå¤±æ©ç å’Œæ¢¯åº¦çº¦æŸç­‰æŠ€æœ¯æ‰‹æ®µï¼Œä¸ºå¤šç›®æ ‡è”åˆè®­ç»ƒæ—¶çš„å†²çªç¼“è§£æä¾›äº†å®ç”¨æ–¹æ³•å€Ÿé‰´ã€‚  
3. æ•°æ®åˆ©ç”¨å±‚é¢ï¼šæ‘†è„±äººå·¥æ ‡æ³¨æ¨ç†é“¾ã€è®©æ¨¡å‹è‡ªä¸»å­¦ä¹ æ¨ç†æ¨¡å¼çš„æ€è·¯ï¼Œä¸ºé™ä½NLPä»»åŠ¡å¯¹æ˜‚è´µäººå·¥æ ‡æ³¨çš„ä¾èµ–æä¾›äº†æ–°æ–¹å‘ã€‚
```

## mutual-information-tracks-policy-coherence-in-reinforcement-learning
### Abstract
Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨äº’ä¿¡æ¯è¿½è¸ªå¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥ä¸€è‡´æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ç¯å¢ƒä¸­éƒ¨ç½²å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“æ—¶ï¼Œä¼šé¢ä¸´ä¼ æ„Ÿå™¨æ•…éšœã€æ‰§è¡Œå™¨ç£¨æŸå’Œç¯å¢ƒå˜åŒ–ç­‰å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œç„¶è€ŒRLæ™ºèƒ½ä½“æœ¬èº«ç¼ºä¹æ£€æµ‹å’Œè¯Šæ–­è¿™äº›æ•…éšœçš„å†…åœ¨æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¿¡æ¯è®ºæ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºRLçš„åŸºæœ¬åŠ¨æ€å¹¶æä¾›éƒ¨ç½²æ—¶å¼‚å¸¸è¯Šæ–­çš„å®ç”¨æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºä¿¡æ¯è®ºåˆ†æRLå­¦ä¹ è¿‡ç¨‹ç‰¹å¾  
é€šè¿‡å¯¹æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­çŠ¶æ€ - åŠ¨ä½œäº’ä¿¡æ¯æ¨¡å¼çš„åˆ†æï¼Œå‘ç°æˆåŠŸå­¦ä¹ å­˜åœ¨ç‰¹å¾æ€§ä¿¡æ¯ç‰¹å¾ã€‚åœ¨çŠ¶æ€ç†µå¢é•¿çš„æƒ…å†µä¸‹ï¼ŒçŠ¶æ€å’ŒåŠ¨ä½œä¹‹é—´çš„äº’ä¿¡æ¯ä»0.84ç¨³å®šå¢é•¿åˆ°2.83æ¯”ç‰¹ï¼ˆå¢é•¿238%ï¼‰ï¼Œè¿™è¡¨æ˜æ™ºèƒ½ä½“å¯¹ä»»åŠ¡ç›¸å…³æ¨¡å¼å‘å±•å‡ºäº†è¶Šæ¥è¶Šæœ‰é€‰æ‹©æ€§çš„æ³¨æ„åŠ›ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ­ç¤ºè”åˆäº’ä¿¡æ¯çš„å˜åŒ–è§„å¾‹  
å‘ç°çŠ¶æ€ã€åŠ¨ä½œå’Œä¸‹ä¸€ä¸ªçŠ¶æ€çš„è”åˆäº’ä¿¡æ¯MI(S,A;S')éµå¾ªå€’Uå‹æ›²çº¿ï¼Œåœ¨å­¦ä¹ æ—©æœŸè¾¾åˆ°å³°å€¼ï¼Œä¹‹åéšç€æ™ºèƒ½ä½“ä¸“ä¸šåŒ–ï¼ˆä»å¹¿æ³›æ¢ç´¢è¿‡æ¸¡åˆ°é«˜æ•ˆåˆ©ç”¨ï¼‰è€Œä¸‹é™ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå·®å¼‚åŒ–è¯Šæ–­ç³»ç»Ÿæ•…éšœ  
ä¿¡æ¯åº¦é‡å¯ä»¥å·®å¼‚åŒ–è¯Šæ–­ç³»ç»Ÿæ•…éšœï¼šè§‚æµ‹ç©ºé—´ï¼ˆå³çŠ¶æ€å™ªå£°ï¼Œä¼ æ„Ÿå™¨æ•…éšœï¼‰ä¼šåœ¨æ‰€æœ‰ä¿¡æ¯é€šé“äº§ç”Ÿå¹¿æ³›å´©æºƒï¼ŒçŠ¶æ€ - åŠ¨ä½œè€¦åˆæ˜¾è‘—ä¸‹é™ï¼›è€ŒåŠ¨ä½œç©ºé—´å™ªå£°ï¼ˆæ‰§è¡Œå™¨æ•…éšœï¼‰ä¼šé€‰æ‹©æ€§åœ°ç ´ååŠ¨ä½œ - ç»“æœçš„å¯é¢„æµ‹æ€§ï¼ŒåŒæ—¶ä¿ç•™çŠ¶æ€ - åŠ¨ä½œå…³ç³»ã€‚è¿™ç§å·®å¼‚åŒ–è¯Šæ–­èƒ½åŠ›èƒ½åœ¨æ— éœ€æ¶æ„ä¿®æ”¹å’Œæ€§èƒ½ä¸‹é™çš„æƒ…å†µä¸‹å®ç°ç²¾ç¡®æ•…éšœå®šä½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡çš„åˆ†æä¸­ï¼ŒæˆåŠŸå­¦ä¹ å±•ç°å‡ºçŠ¶æ€ - åŠ¨ä½œäº’ä¿¡æ¯ç¨³å®šå¢é•¿ç­‰ç‰¹å¾ä¿¡æ¯ç­¾åï¼›MI(S,A;S')å‘ˆç°å€’Uå‹æ›²çº¿å˜åŒ–ï¼›åœ¨å—æ§æ‰°åŠ¨å®éªŒä¸­ï¼Œè§‚æµ‹ç©ºé—´å™ªå£°å’ŒåŠ¨ä½œç©ºé—´å™ªå£°ä¸‹ä¿¡æ¯åº¦é‡å‘ˆç°ä¸åŒå˜åŒ–æ¨¡å¼ï¼ŒéªŒè¯äº†ä¿¡æ¯åº¦é‡å¯¹ä¸åŒç³»ç»Ÿæ•…éšœçš„å·®å¼‚åŒ–è¯Šæ–­èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡å»ºç«‹äº†ä¿¡æ¯æ¨¡å¼ä½œä¸ºå­¦ä¹ çš„ç‰¹å¾å’Œç³»ç»Ÿå¥åº·è¯Šæ–­çš„ä¾æ®ï¼Œä¸ºåŸºäºä¿¡æ¯è®ºåŸç†çš„ã€èƒ½å¤Ÿè‡ªä¸»æ•…éšœæ£€æµ‹å’Œç­–ç•¥è°ƒæ•´çš„è‡ªé€‚åº”RLç³»ç»Ÿæä¾›äº†åŸºç¡€ã€‚åœ¨å®é™…RLç³»ç»Ÿéƒ¨ç½²æ—¶ï¼Œå¯å€Ÿé‰´å…¶ä¿¡æ¯è®ºåˆ†ææ–¹æ³•æ¥ç›‘æµ‹ç³»ç»ŸçŠ¶æ€ï¼ŒåŠæ—¶å‘ç°æ•…éšœå¹¶å®šä½æ•…éšœç±»å‹ï¼Œä¸ºRLç³»ç»Ÿçš„é²æ£’æ€§æå‡å’Œæ•…éšœå¤„ç†æä¾›äº†æ–°çš„æ€è·¯ä¸æ–¹æ³•ã€‚

## inpainting-guided-policy-optimization-for-diffusion-large-language-models
### Abstract
Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨ä¿®å¤å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œè§£é”æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹æ–°å¯èƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ©ç æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹çš„æœ‰å‰æ™¯æ›¿ä»£æ–¹æ¡ˆï¼Œæ€§èƒ½æœ‰ç«äº‰åŠ›ä¸”æ”¯æŒå¦‚ä¿®å¤ï¼ˆinpaintingï¼‰è¿™ç±»ç‹¬ç‰¹ç”Ÿæˆèƒ½åŠ›ã€‚ä½†å¤§è¯­è¨€æ¨¡å‹ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ—¶é¢ä¸´æ¢ç´¢éš¾é¢˜ï¼šå¥–åŠ±ä¿¡å·ç¨€ç–ï¼Œä¸”æ¨¡å‹æ²¡æ‰¾åˆ°æ­£ç¡®è§£æ—¶ä¼šé€ æˆæ ·æœ¬æµªè´¹ã€‚è™½è¿™ä½æ•ˆæ€§æ™®éå½±å“å¤§è¯­è¨€æ¨¡å‹ï¼Œå¯dLLMsçš„ä¿®å¤èƒ½åŠ›æä¾›äº†ç‹¬ç‰¹æœºä¼šâ€”â€”å…¶ä¿®å¤èƒ½åŠ›èƒ½å¼•å¯¼æ¢ç´¢ã€‚æ‰€ä»¥æœ¬æ–‡æ¢ç´¢å¦‚ä½•è®©ä¿®å¤ä¸ºdLLMsçš„RLç®—æ³•è®¾è®¡æä¾›æ€è·¯ï¼Œè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢ä¸è¶³ç­‰é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºIGPOï¼ˆInpainting Guided Policy Optimizationï¼‰æ¡†æ¶ã€‚è¯¥RLæ¡†æ¶åœ¨åœ¨çº¿é‡‡æ ·æœŸé—´æˆ˜ç•¥æ€§æ’å…¥éƒ¨åˆ†çœŸå®æ¨ç†è½¨è¿¹ï¼Œä¸åŒäºç›´æ¥ç»™å…¨è§£ï¼Œä¿®å¤åœ¨ä¿ç•™è‡ªç”Ÿæˆæ¨ç†çš„åŒæ—¶ï¼Œå°†æ¢ç´¢å¼•å‘æœ‰å‰æ™¯çš„è½¨è¿¹ç©ºé—´ï¼Œæ­å»ºèµ·ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ é—´çš„æ¡¥æ¢ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé’ˆå¯¹åŸºäºåˆ†ç»„çš„ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ï¼ŒIGPOèƒ½æ¢å¤æœ‰æ„ä¹‰æ¢¯åº¦å¹¶æå‡æ ·æœ¬æ•ˆç‡ã€‚å› ä¸ºåœ¨è¿™äº›æ–¹æ³•ä¸­æ¢ç´¢å¤±è´¥ä¼šå¯¼è‡´ä¼˜åŠ¿å’Œæ¢¯åº¦ä¸ºé›¶ï¼ŒIGPOè§£å†³äº†è¯¥é—®é¢˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºåœ¨åˆæˆé‡å†™çš„ç®€æ´è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œè¿™äº›è½¨è¿¹æ›´è´´åˆdLLMç”Ÿæˆæ¨¡å¼ã€‚åŒæ—¶ç»“åˆåŸºäºç†µçš„è¿‡æ»¤ç­‰é¢å¤–æŠ€æœ¯ï¼Œå½¢æˆè®­ç»ƒæ–¹æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GSM8Kã€Math500å’ŒAMCè¿™ä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œé‡‡ç”¨è¯¥è®­ç»ƒæ–¹æ¡ˆï¼ˆIGPOåŠç›¸å…³æŠ€æœ¯ï¼‰çš„æ¨¡å‹å–å¾—äº†æ˜¾è‘—æå‡ï¼Œä¸ºå…¨æ³¨æ„åŠ›æ©ç dLLMså®ç°äº†æ–°çš„æœ€å…ˆè¿›æˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«ç‰¹æœ‰èƒ½åŠ›ï¼ˆå¦‚dLLMsçš„ä¿®å¤èƒ½åŠ›ï¼‰æ¥è¾…åŠ©å¼ºåŒ–å­¦ä¹ æ¢ç´¢è¿‡ç¨‹ï¼Œä¸ºè§£å†³å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢éš¾é¢˜æä¾›äº†æ–°è§†è§’ï¼›åœ¨æŠ€æœ¯ç»“åˆä¸Šï¼Œå°†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ é€šè¿‡å·§å¦™æ–¹å¼è¡”æ¥ï¼ˆå¦‚éƒ¨åˆ†çœŸå®è½¨è¿¹æ’å…¥å¼•å¯¼ï¼‰ï¼Œè¿™ç§æ··åˆè®­ç»ƒæ€è·¯å€¼å¾—å‚è€ƒï¼›é’ˆå¯¹ç‰¹å®šä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚åˆ†ç»„ä¼˜åŒ–ï¼‰çš„é—®é¢˜é’ˆå¯¹æ€§è®¾è®¡è§£å†³æ–¹æ¡ˆï¼Œä»¥åŠåœ¨åˆæˆæ•°æ®ä¸Šåšç›‘ç£å¾®è°ƒé€‚é…æ¨¡å‹ç”Ÿæˆæ¨¡å¼ç­‰ï¼Œéƒ½æ˜¯åœ¨æ¨¡å‹è®­ç»ƒä¼˜åŒ–ä¸­å¯å€Ÿé‰´çš„æŠ€å·§ï¼Œä¸ºæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡è¡¨ç°æä¾›äº†è·¯å¾„ã€‚ 

## cde--curiosity-driven-exploration-for-efficient-reinforcement-learning-in-large-language-models
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨â€œå¥½å¥‡å¿ƒâ€é©±åŠ¨å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ¢ç´¢ï¼šCDEæ¡†æ¶ç ´è§£RLVRæ¢ç´¢ä¸è¶³éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦ã€ç¼–ç ç­‰é¢†åŸŸæ¨ç†èƒ½åŠ›è¿›æ­¥æ˜¾è‘—ï¼Œä½†å¦‚ä½•é«˜æ•ˆå¼•å¯¼é«˜è´¨é‡æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä»æ˜¯æŒ‘æˆ˜ã€‚åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ˜¯æå‡LLMsæ¨ç†èƒ½åŠ›çš„æœ‰åŠ›èŒƒå¼ï¼Œç„¶è€Œç°æœ‰RLVRæ–¹æ³•å­˜åœ¨æ¢ç´¢ä¸è¶³é—®é¢˜ï¼Œæ˜“å¯¼è‡´è¿‡æ—©æ”¶æ•›ä¸ç†µåç¼©ï¼ˆæ¨¡å‹è¿‡åº¦åå‘â€œåˆ©ç”¨â€å·²çŸ¥ç­–ç•¥ï¼Œè€Œéå……åˆ†æ¢ç´¢ç¯å¢ƒæ‰¾æ›´ä¼˜è§£ï¼‰ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ¢ç´¢ç­–ç•¥ï¼ˆå¦‚ç†µå¥–åŠ±ã€Ïµ - greedyï¼‰åœ¨LLMsåœºæ™¯ä¸‹è¦ä¹ˆç†è®ºæ¬¡ä¼˜ã€è¦ä¹ˆæ•ˆæœå­˜ç–‘ï¼›åŸºäºè®¡æ•°çš„æ¢ç´¢æ–¹æ³•ï¼ˆå¦‚UCBç±»ï¼‰å› è®¡ç®—é‡å¤§ã€ä¾èµ–å¤æ‚çŠ¶æ€ - åŠ¨ä½œè¡¨ç¤ºï¼Œåœ¨é•¿æ€ç»´é“¾LLMsæ¨ç†ä¸­ä¹Ÿä¸å®ç”¨ã€‚æ­¤å¤–ï¼Œç›´æ¥å°†è®¡æ•°æ¢ç´¢æ–¹æ³•ç”¨äºRLVRæ—¶ï¼Œå› æ€ç»´é“¾è½¨è¿¹éš¾ç”¨å›ºå®šåµŒå…¥åˆ»ç”»ï¼Œå¤šæ•°å“åº”ä¼šåç¼©åˆ°ç›¸åŒå“ˆå¸Œç½‘æ ¼ï¼Œå‰Šå¼±æ¢ç´¢æ•ˆæœã€‚å› æ­¤ï¼Œéœ€ä¸ºLLMsè®¾è®¡é«˜æ•ˆå¯æ‰©å±•çš„æ¢ç´¢æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢ï¼ˆCDEï¼‰æ¡†æ¶  
åˆ©ç”¨æ¨¡å‹å†…åœ¨â€œå¥½å¥‡å¿ƒâ€å¼•å¯¼æ¢ç´¢ï¼Œå°†actorå’Œcriticçš„ä¿¡å·ç»“åˆæ¥å½¢å¼åŒ–å¥½å¥‡å¿ƒã€‚actorä¾§ç”¨ç”Ÿæˆå“åº”çš„å›°æƒ‘åº¦ï¼ˆperplexityï¼ŒPPLï¼‰è¡¡é‡å¥½å¥‡å¿ƒï¼šLLMåœ¨æµ·é‡æ¨ç†è¯­æ–™ä¸Šè®­ç»ƒåï¼Œèƒ½æ„ŸçŸ¥æ¨ç†æ¨¡å¼çš„ç†Ÿæ‚‰åº¦ä¸æ–°é¢–åº¦ï¼ŒPPLåæ˜ ç”Ÿæˆå†…å®¹çš„ä¸ç¡®å®šæ€§ï¼Œå¯ä½œä¸ºæ¢ç´¢å¥–åŠ±ã€‚criticä¾§å€ŸåŠ©å¤šå¤´æ¶æ„ä»·å€¼ä¼°è®¡çš„æ–¹å·®è¡¡é‡å¥½å¥‡å¿ƒï¼šæ‰©å±•PPOæ¡†æ¶ä¸ºå¤šå¤´è‡ªä¸¾ç»“æ„ï¼Œç”¨åéªŒä»·å€¼åˆ†å¸ƒæ–¹å·®è¿‘ä¼¼å¥½å¥‡å¿ƒï¼Œå°†è¿™ä¸¤ç§ä¿¡å·ä½œä¸ºæ¢ç´¢å¥–åŠ±æ³¨å…¥RLVRæ¡†æ¶ï¼Œå¡‘é€ å¥–åŠ±å’Œä¼˜åŠ¿å‡½æ•°å¼•å¯¼æ¢ç´¢ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºåˆ†ææ”¯æ’‘æ–¹æ³•åˆç†æ€§  
ä»ç†è®ºå±‚é¢å‰–æCDEä¼˜åŠ¿ï¼šå¯¹äºactorä¾§åŸºäºPPLçš„å¥–åŠ±ï¼Œå®šç†è¡¨æ˜å…¶èƒ½å†…åœ¨æƒ©ç½šè¿‡åº¦è‡ªä¿¡é”™è¯¯ï¼ŒåŒæ—¶ä¿ƒè¿›æ­£ç¡®å“åº”çš„å¤šæ ·æ€§ï¼›å¯¹äºcriticä¾§å¥–åŠ±ï¼Œåœ¨çº¿æ€§MDPåœºæ™¯ä¸‹ï¼Œè¯æ˜å…¶ä¸ç»å…¸åŸºäºè®¡æ•°çš„æ¢ç´¢å¥–åŠ±ç†è®ºç­‰ä»·ï¼Œè®©æ–¹æ³•æ‰æ ¹äºæˆç†Ÿæ¢ç´¢åŸç†ï¼Œå¢å¼ºå¯é æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››ä¸ªå¸¸ç”¨æ•°å­¦åŸºå‡†ï¼ˆAIME25ã€AIME24ã€AMC23ã€MATHï¼‰ä¸Šè¯„ä¼°ï¼ŒCDEåœ¨å…·æŒ‘æˆ˜æ€§çš„AIMEåŸºå‡†ä¸Šï¼Œç›¸è¾ƒæ ‡å‡†RLVRï¼ˆç”¨GRPO/PPOï¼‰å®ç°çº¦3ä¸ªç‚¹çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè®­ç»ƒè¿‡ç¨‹åˆ†ææ­ç¤ºâ€œæ ¡å‡†åç¼©â€ç°è±¡ï¼š naive GRPOç­–ç•¥ä¸‹ï¼Œæ¨¡å‹ç½®ä¿¡åº¦ä¸æ­£ç¡®æ€§é€æ¸è„±èŠ‚ï¼Œè€ŒåŠ å…¥PPLå¥–åŠ±èƒ½ç¼“è§£è¿™ç§è¯¯æ ¡å‡†ï¼ŒéªŒè¯äº†æ–¹æ³•å¯¹è®­ç»ƒç¨³å®šæ€§çš„æ”¹å–„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¢ç´¢æ€è·¯åˆ›æ–°ï¼šè·³å‡ºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ¢ç´¢çš„å±€é™ï¼Œåˆ©ç”¨æ¨¡å‹å†…åœ¨è®¤çŸ¥ï¼ˆå¦‚å›°æƒ‘åº¦åæ˜ çš„æ¨ç†æ¨¡å¼ç†Ÿæ‚‰åº¦ã€ä»·å€¼æ–¹å·®åæ˜ çš„ä¸ç¡®å®šæ€§ï¼‰å¼•å¯¼æ¢ç´¢ï¼Œä¸ºLLMså¼ºåŒ–å­¦ä¹ æ¢ç´¢æä¾›æ–°èŒƒå¼ï¼Œå¯å‘åç»­ä»æ¨¡å‹è‡ªèº«å±æ€§æŒ–æ˜æ¢ç´¢ä¿¡å·ã€‚  
2. ç†è®ºå®è·µç»“åˆï¼šé€šè¿‡ç†è®ºåˆ†æè®ºè¯æ–¹æ³•åˆç†æ€§ï¼Œå†ç”¨å®éªŒéªŒè¯æ€§èƒ½æå‡ä¸ç°è±¡è§£é‡Šï¼ˆå¦‚æ ¡å‡†åç¼©ï¼‰ï¼Œè¿™ç§ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´é“¾è·¯ï¼Œä¸ºç®—æ³•ç±»ç ”ç©¶æä¾›ç¤ºèŒƒï¼Œè®©æ–¹æ³•ä¸ä»…â€œæœ‰æ•ˆâ€è¿˜â€œæœ‰ç†â€ã€‚  
3. é—®é¢˜æ´å¯Ÿæ·±å…¥ï¼šå‘ç°RLVRä¸­çš„æ ¡å‡†åç¼©æœºåˆ¶ï¼Œä¸ºç†è§£å¤§æ¨¡å‹å¸¸è§å¤±æ•ˆæ¨¡å¼æä¾›æ–°è§†è§’ï¼Œæœ‰åŠ©äºåç»­é’ˆå¯¹æ€§ä¼˜åŒ–è®­ç»ƒæµç¨‹ä¸ç­–ç•¥ã€‚

