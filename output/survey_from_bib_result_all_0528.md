# Paper List from BIB File: tmpxv1owcwg.bib
- [25/03] **A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond**  
[[Paper](http://arxiv.org/pdf/2503.21614v1)] [[Code/Page]()] [[TLDR/Notes](#a-survey-of-efficient-reasoning-for-large-reasoning-models--language--multimodality--and-beyond)]

- [25/05] **Model Merging in Pre-training of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2505.12082v3)] [[Code/Page]()] [[TLDR/Notes](#model-merging-in-pre-training-of-large-language-models)]

- [25/04] **Improved Visual-Spatial Reasoning via R1-Zero-Like Training**  
[[Paper](http://arxiv.org/pdf/2504.00883v2)] [[Code/Page]()] [[TLDR/Notes](#improved-visual-spatial-reasoning-via-r1-zero-like-training)]

- [25/05] **Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL**  
[[Paper](http://arxiv.org/pdf/2505.10832v1)] [[Code/Page]()] [[TLDR/Notes](#learning-when-to-think--shaping-adaptive-reasoning-in-r1-style-models-via-multi-stage-rl)]

- [25/04] **Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods**  
[[Paper](http://arxiv.org/pdf/2504.14047v1)] [[Code/Page]()] [[TLDR/Notes](#think-deep--think-fast--investigating-efficiency-of-verifier-free-inference-time-scaling-methods)]

- [25/04] **TTRL: Test-Time Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.16084v2)] [[Code/Page](https://github.com/PRIME-RL/TTRL)] [[TLDR/Notes](#ttrl--test-time-reinforcement-learning)]

- [25/05] **Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner**  
[[Paper](http://arxiv.org/pdf/2505.11404v1)] [[Code/Page](https://github.com/Wenchuan-Zhang/Patho-R1.)] [[TLDR/Notes](#patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner)]

- [25/04] **Efficient Reasoning Models: A Survey**  
[[Paper](http://arxiv.org/pdf/2504.10903v1)] [[Code/Page]()] [[TLDR/Notes](#efficient-reasoning-models--a-survey)]

- [25/05] **Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL**  
[[Paper](http://arxiv.org/pdf/2505.02391v1)] [[Code/Page](https://github.com/RLHFlow/GVM.)] [[TLDR/Notes](#optimizing-chain-of-thought-reasoners-via-gradient-variance-minimization-in-rejection-sampling-and-rl)]

- [25/03] **PharMolixFM: All-Atom Foundation Models for Molecular Modeling and Generation**  
[[Paper](http://arxiv.org/pdf/2503.21788v3)] [[Code/Page](https://github.com/PharMolix/OpenBioMed.)] [[TLDR/Notes](#pharmolixfm--all-atom-foundation-models-for-molecular-modeling-and-generation)]

- [25/05] **R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO**  
[[Paper](http://arxiv.org/pdf/2505.16673v1)] [[Code/Page](https://github.com/HJYao00/R1-ShareVL.)] [[TLDR/Notes](#r1-sharevl--incentivizing-reasoning-capability-of-multimodal-large-language-models-via-share-grpo)]

- [25/05] **RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.15034v1)] [[Code/Page](https://github.com/kaiwenzha/rl-tango.)] [[TLDR/Notes](#rl-tango--reinforcing-generator-and-verifier-together-for-language-reasoning)]

- [25/05] **Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs**  
[[Paper](http://arxiv.org/pdf/2505.12929v1)] [[Code/Page](https://github.com/zhyang2226/AR-Lopti.)] [[TLDR/Notes](#do-not-let-low-probability-tokens-over-dominate-in-rl-for-llms)]

- [25/04] **NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation**  
[[Paper](http://arxiv.org/pdf/2504.13055v3)] [[Code/Page]()] [[TLDR/Notes](#noisyrollout--reinforcing-visual-reasoning-with-data-augmentation)]

- [25/04] **Reinforcement Learning from Human Feedback**  
[[Paper](http://arxiv.org/pdf/2504.12501v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-from-human-feedback)]

- [25/03] **Reasoning Beyond Limits: Advances and Open Problems for LLMs**  
[[Paper](http://arxiv.org/pdf/2503.22732v1)] [[Code/Page]()] [[TLDR/Notes](#reasoning-beyond-limits--advances-and-open-problems-for-llms)]

- [25/03] **Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains**  
[[Paper](http://arxiv.org/pdf/2503.23829v2)] [[Code/Page]()] [[TLDR/Notes](#crossing-the-reward-bridge--expanding-rl-with-verifiable-rewards-across-diverse-domains)]

- [25/04] **Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?**  
[[Paper](http://arxiv.org/pdf/2504.13837v2)] [[Code/Page]()] [[TLDR/Notes](#does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model-)]

- [25/05] **RLVR-World: Training World Models with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.13934v1)] [[Code/Page]()] [[TLDR/Notes](#rlvr-world--training-world-models-with-reinforcement-learning)]

- [25/02] **Atom of Thoughts for Markov LLM Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2502.12018v2)] [[Code/Page](https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.)] [[TLDR/Notes](#atom-of-thoughts-for-markov-llm-test-time-scaling)]

- [25/05] **Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO**  
[[Paper](http://arxiv.org/pdf/2505.17017v1)] [[Code/Page](https://github.com/ZiyuGuo99/Image-Generation-CoT)] [[TLDR/Notes](#delving-into-rl-for-image-generation-with-cot--a-study-on-dpo-vs--grpo)]

- [25/04] **Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization**  
[[Paper](http://arxiv.org/pdf/2504.05812v3)] [[Code/Page](https://github.com/QingyangZhang/EMPO.)] [[TLDR/Notes](#right-question-is-already-half-the-answer--fully-unsupervised-llm-reasoning-incentivization)]

- [25/04] **ToolRL: Reward is All Tool Learning Needs**  
[[Paper](http://arxiv.org/pdf/2504.13958v1)] [[Code/Page]()] [[TLDR/Notes](#toolrl--reward-is-all-tool-learning-needs)]

- [25/04] **SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM**  
[[Paper](http://arxiv.org/pdf/2504.14286v2)] [[Code/Page]()] [[TLDR/Notes](#srpo--a-cross-domain-implementation-of-large-scale-reinforcement-learning-on-llm)]

- [25/05] **Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities**  
[[Paper](http://arxiv.org/pdf/2505.15692v2)] [[Code/Page]()] [[TLDR/Notes](#thought-augmented-policy-optimization--bridging-external-guidance-and-internal-capabilities)]

- [25/04] **Think When You Need: Self-Adaptive Chain-of-Thought Learning**  
[[Paper](http://arxiv.org/pdf/2504.03234v2)] [[Code/Page]()] [[TLDR/Notes](#think-when-you-need--self-adaptive-chain-of-thought-learning)]

- [25/04] **Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models**  
[[Paper](http://arxiv.org/pdf/2504.21277v2)] [[Code/Page]()] [[TLDR/Notes](#reinforced-mllm--a-survey-on-rl-based-reasoning-in-multimodal-large-language-models)]

- [25/05] **EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.04623v1)] [[Code/Page]()] [[TLDR/Notes](#echoink-r1--exploring-audio-visual-reasoning-in-multimodal-llms-via-reinforcement-learning)]

- [25/05] **Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.16854v2)] [[Code/Page](https://github.com/kokolerk/TON.)] [[TLDR/Notes](#think-or-not--selective-reasoning-via-reinforcement-learning-for-vision-language-models)]

- [25/05] **Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.13261v1)] [[Code/Page]()] [[TLDR/Notes](#unlocking-the-potential-of-difficulty-prior-in-rl-based-multimodal-reasoning)]

- [25/05] **Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.11827v2)] [[Code/Page](https://github.com/usail-hkust/LongShort.)] [[TLDR/Notes](#not-all-thoughts-are-generated-equal--efficient-llm-reasoning-via-multi-turn-reinforcement-learning)]

- [25/05] **RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs**  
[[Paper](http://arxiv.org/pdf/2505.13697v2)] [[Code/Page]()] [[TLDR/Notes](#rl-in-name-only--analyzing-the-structural-assumptions-in-rl-post-training-for-llms)]

- [25/05] **CEC-Zero: Chinese Error Correction Solution Based on LLM**  
[[Paper](http://arxiv.org/pdf/2505.09082v1)] [[Code/Page]()] [[TLDR/Notes](#cec-zero--chinese-error-correction-solution-based-on-llm)]

- [25/05] **DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization**  
[[Paper](http://arxiv.org/pdf/2505.12366v1)] [[Code/Page]()] [[TLDR/Notes](#disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization)]

- [25/05] **KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.16826v1)] [[Code/Page]()] [[TLDR/Notes](#ktae--a-model-free-algorithm-to-key-tokens-advantage-estimation-in-mathematical-reasoning)]

- [25/05] **S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.07686v2)] [[Code/Page]()] [[TLDR/Notes](#s-grpo--early-exit-via-reinforcement-learning-in-reasoning-models)]

- [25/05] **MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining**  
[[Paper](http://arxiv.org/pdf/2505.07608v1)] [[Code/Page](https://github.com/xiaomimimo/MiMo.)] [[TLDR/Notes](#mimo--unlocking-the-reasoning-potential-of-language-model----from-pretraining-to-posttraining)]

- [25/05] **Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning**  
[[Paper](http://arxiv.org/pdf/2505.14684v2)] [[Code/Page]()] [[TLDR/Notes](#mind-the-gap--bridging-thought-leap-for-improved-chain-of-thought-tuning)]

- [25/05] **AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners**  
[[Paper](http://arxiv.org/pdf/2505.16322v1)] [[Code/Page]()] [[TLDR/Notes](#adastar--adaptive-data-sampling-for-training-self-taught-reasoners)]

- [25/03] **A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?**  
[[Paper](http://arxiv.org/pdf/2503.24235v3)] [[Code/Page](https://github.com/testtimescaling/testtimescaling.github.io/)] [[TLDR/Notes](#a-survey-on-test-time-scaling-in-large-language-models--what--how--where--and-how-well-)]

- [25/05] **ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay**  
[[Paper](http://arxiv.org/pdf/2505.16282v1)] [[Code/Page](https://github.com/dvlab-research/ARPO.git.)] [[TLDR/Notes](#arpo-end-to-end-policy-optimization-for-gui-agents-with-experience-replay)]

- [25/05] **Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions**  
[[Paper](http://arxiv.org/pdf/2505.11614v1)] [[Code/Page]()] [[TLDR/Notes](#using-reinforcement-learning-to-train-large-language-models-to-explain-human-decisions)]

- [25/05] **UFT: Unifying Supervised and Reinforcement Fine-Tuning**  
[[Paper](http://arxiv.org/pdf/2505.16984v1)] [[Code/Page]()] [[TLDR/Notes](#uft--unifying-supervised-and-reinforcement-fine-tuning)]

- [25/04] **VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.08837v3)] [[Code/Page]()] [[TLDR/Notes](#vl-rethinker--incentivizing-self-reflection-of-vision-language-models-with-reinforcement-learning)]

- [25/05] **A New DAPO Algorithm for Stock Trading**  
[[Paper](http://arxiv.org/pdf/2505.06408v2)] [[Code/Page](https://github.com/Ruijian-Zha/FinRL-DAPO-SR/)] [[TLDR/Notes](#a-new-dapo-algorithm-for-stock-trading)]

- [25/04] **A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility**  
[[Paper](http://arxiv.org/pdf/2504.07086v1)] [[Code/Page]()] [[TLDR/Notes](#a-sober-look-at-progress-in-language-model-reasoning--pitfalls-and-paths-to-reproducibility)]

- [25/05] **Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2505.02686v1)] [[Code/Page](https://github.com/bobxwu/learning-from-rewards-llm-papers.)] [[TLDR/Notes](#sailing-ai-by-the-stars--a-survey-of-learning-from-rewards-in-post-training-and-test-time-scaling-of-large-language-models)]

- [25/04] **Heimdall: test-time scaling on the generative verification**  
[[Paper](http://arxiv.org/pdf/2504.10337v2)] [[Code/Page]()] [[TLDR/Notes](#heimdall--test-time-scaling-on-the-generative-verification)]

- [25/05] **Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning**  
[[Paper](http://arxiv.org/pdf/2505.16761v1)] [[Code/Page](https://hitcslj.github.io/mesh-rft/}{this)] [[TLDR/Notes](#mesh-rft--enhancing-mesh-generation-via-fine-grained-reinforcement-fine-tuning)]

- [25/05] **ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving**  
[[Paper](http://arxiv.org/pdf/2505.12717v1)] [[Code/Page]()] [[TLDR/Notes](#totrl--unlock-llm-tree-of-thoughts-reasoning-potential-through-puzzles-solving)]

- [25/05] **Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.14810v2)] [[Code/Page](https://github.com/TingchenFu/MathIF.)] [[TLDR/Notes](#scaling-reasoning--losing-control--evaluating-instruction-following-in-large-reasoning-models)]

- [25/05] **SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward**  
[[Paper](http://arxiv.org/pdf/2505.17018v1)] [[Code/Page](https://github.com/kxfan2002/SophiaVL-R1.)] [[TLDR/Notes](#sophiavl-r1--reinforcing-mllms-reasoning-with-thinking-reward)]

- [25/04] **Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time**  
[[Paper](http://arxiv.org/pdf/2504.12329v1)] [[Code/Page]()] [[TLDR/Notes](#speculative-thinking--enhancing-small-model-reasoning-with-large-model-guidance-at-inference-time)]

- [25/05] **OViP: Online Vision-Language Preference Learning**  
[[Paper](http://arxiv.org/pdf/2505.15963v1)] [[Code/Page]()] [[TLDR/Notes](#ovip--online-vision-language-preference-learning)]

- [25/05] **AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16400v1)] [[Code/Page]()] [[TLDR/Notes](#acereason-nemotron--advancing-math-and-code-reasoning-through-reinforcement-learning)]

- [25/05] **Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.12432v1)] [[Code/Page](https://github.com/zrguo/Observe-R1.)] [[TLDR/Notes](#observe-r1--unlocking-reasoning-abilities-of-mllms-with-dynamic-progressive-reinforcement-learning)]

- [25/05] **Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling**  
[[Paper](http://arxiv.org/pdf/2505.11792v1)] [[Code/Page]()] [[TLDR/Notes](#solver-informed-rl--grounding-large-language-models-for-authentic-optimization-modeling)]

- [25/04] **Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.13818v1)] [[Code/Page]()] [[TLDR/Notes](#not-all-rollouts-are-useful--down-sampling-rollouts-in-llm-reinforcement-learning)]

- [25/05] **Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.15311v1)] [[Code/Page]()] [[TLDR/Notes](#trajectory-bellman-residual-minimization--a-simple-value-based-method-for-llm-reasoning)]

- [25/05] **T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT**  
[[Paper](http://arxiv.org/pdf/2505.00703v1)] [[Code/Page](https://github.com/CaraJ7/T2I-R1)] [[TLDR/Notes](#t2i-r1--reinforcing-image-generation-with-collaborative-semantic-level-and-token-level-cot)]

- [25/04] **Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining**  
[[Paper](http://arxiv.org/pdf/2504.07912v1)] [[Code/Page]()] [[TLDR/Notes](#echo-chamber--rl-post-training-amplifies-behaviors-learned-in-pretraining)]

- [25/05] **SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.12346v1)] [[Code/Page]()] [[TLDR/Notes](#seed-grpo--semantic-entropy-enhanced-grpo-for-uncertainty-aware-policy-optimization)]

- [25/05] **lmgame-Bench: How Good are LLMs at Playing Games?**  
[[Paper](http://arxiv.org/pdf/2505.15146v1)] [[Code/Page](https://github.com/lmgame-org/GamingAgent/lmgame-bench.)] [[TLDR/Notes](#lmgame-bench--how-good-are-llms-at-playing-games-)]

- [25/05] **RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation**  
[[Paper](http://arxiv.org/pdf/2505.03344v1)] [[Code/Page]()] [[TLDR/Notes](#rift--closed-loop-rl-fine-tuning-for-realistic-and-controllable-traffic-simulation)]

- [25/03] **Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models**  
[[Paper](http://arxiv.org/pdf/2503.09567v3)] [[Code/Page]()] [[TLDR/Notes](#towards-reasoning-era--a-survey-of-long-chain-of-thought-for-reasoning-large-language-models)]

- [25/04] **TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.09641v1)] [[Code/Page](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.)] [[TLDR/Notes](#tinyllava-video-r1--towards-smaller-lmms-for-video-reasoning)]

- [25/05] **O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering**  
[[Paper](http://arxiv.org/pdf/2505.16582v2)] [[Code/Page]()] [[TLDR/Notes](#o$^2$-searcher--a-searching-based-agent-model-for-open-domain-open-ended-question-answering)]

- [25/05] **Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO**  
[[Paper](http://arxiv.org/pdf/2505.11595v1)] [[Code/Page]()] [[TLDR/Notes](#spectral-policy-optimization--coloring-your-incorrect-reasoning-in-grpo)]

- [25/05] **Group-in-Group Policy Optimization for LLM Agent Training**  
[[Paper](http://arxiv.org/pdf/2505.10978v1)] [[Code/Page]()] [[TLDR/Notes](#group-in-group-policy-optimization-for-llm-agent-training)]

- [25/05] **Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens**  
[[Paper](http://arxiv.org/pdf/2505.13775v2)] [[Code/Page]()] [[TLDR/Notes](#beyond-semantics--the-unreasonable-effectiveness-of-reasonless-intermediate-tokens)]

- [25/04] **Deep Reasoning Translation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.10187v1)] [[Code/Page]()] [[TLDR/Notes](#deep-reasoning-translation-via-reinforcement-learning)]

- [25/05] **DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management**  
[[Paper](http://arxiv.org/pdf/2505.12951v1)] [[Code/Page]()] [[TLDR/Notes](#dgro--enhancing-llm-reasoning-via-exploration-exploitation-control-and-reward-variance-management)]

- [25/05] **J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.13346v2)] [[Code/Page]()] [[TLDR/Notes](#j4r--learning-to-judge-with-equivalent-initial-state-group-relative-policy-optimization)]

- [25/05] **An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents**  
[[Paper](http://arxiv.org/pdf/2505.15117v1)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents)]

- [25/04] **GVPO: Group Variance Policy Optimization for Large Language Model Post-Training**  
[[Paper](http://arxiv.org/pdf/2504.19599v2)] [[Code/Page]()] [[TLDR/Notes](#gvpo--group-variance-policy-optimization-for-large-language-model-post-training)]

- [25/05] **Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16410v1)] [[Code/Page](https://github.com/dongguanting/Tool-Star.)] [[TLDR/Notes](#tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning)]

- [25/05] **Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16142v1)] [[Code/Page]()] [[TLDR/Notes](#distilling-the-implicit-multi-branch-structure-in-llms--reasoning-via-reinforcement-learning)]

- [25/04] **StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation**  
[[Paper](http://arxiv.org/pdf/2504.15930v1)] [[Code/Page]()] [[TLDR/Notes](#streamrl--scalable--heterogeneous--and-elastic-rl-for-llms-with-disaggregated-stream-generation)]

- [25/05] **UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection**  
[[Paper](http://arxiv.org/pdf/2505.12457v1)] [[Code/Page]()] [[TLDR/Notes](#ufo-rl--uncertainty-focused-optimization-for-efficient-reinforcement-learning-data-selection)]



# TLDR/Notes
## a-survey-of-efficient-reasoning-for-large-reasoning-models--language--multimodality--and-beyond
### Abstract
Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have
demonstrated strong performance gains by scaling up the length of
Chain-of-Thought (CoT) reasoning during inference. However, a growing concern
lies in their tendency to produce excessively long reasoning traces, which are
often filled with redundant content (e.g., repeated definitions), over-analysis
of simple problems, and superficial exploration of multiple reasoning paths for
harder tasks. This inefficiency introduces significant challenges for training,
inference, and real-world deployment (e.g., in agent-based systems), where
token economy is critical. In this survey, we provide a comprehensive overview
of recent efforts aimed at improving reasoning efficiency in LRMs, with a
particular focus on the unique challenges that arise in this new paradigm. We
identify common patterns of inefficiency, examine methods proposed across the
LRM lifecycle, i.e., from pretraining to inference, and discuss promising
future directions for research. To support ongoing development, we also
maintain a real-time GitHub repository tracking recent progress in the field.
We hope this survey serves as a foundation for further exploration and inspires
innovation in this rapidly evolving area.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹æ¨ç†æ¨¡å‹çš„é«˜æ•ˆæ¨ç†ï¼šè¯­è¨€ã€å¤šæ¨¡æ€ä¸æœªæ¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚DeepSeek-R1å’ŒOpenAI o1åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡å¢åŠ é“¾å¼æ€ç»´ï¼ˆCoTï¼‰çš„é•¿åº¦ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆæ¨ç†è½¨è¿¹æ—¶å¾€å¾€è¿‡äºå†—é•¿ï¼ŒåŒ…å«é‡å¤å†…å®¹ã€å¯¹ç®€å•é—®é¢˜è¿‡åº¦åˆ†æï¼Œä»¥åŠå¯¹äºæ›´éš¾ä»»åŠ¡è¡¨é¢åŒ–çš„æ¢ç´¢ã€‚è¿™ç§ä½æ•ˆæ€§ç»™è®­ç»ƒã€æ¨ç†ä»¥åŠå®é™…éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ³¨é‡tokenç»æµçš„ç³»ç»Ÿä¸­ã€‚æœ¬æ–‡æ—¨åœ¨æä¾›å¯¹æé«˜LRMsæ¨ç†æ•ˆç‡çš„æœ€æ–°åŠªåŠ›çš„å…¨é¢æ¦‚è¿°ï¼Œå¹¶æ¢è®¨è¿™ä¸€æ–°èŒƒå¼ä¸‹å‡ºç°çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆå®šä¹‰äº†æ¨ç†æ•ˆç‡ï¼Œå¹¶è¯†åˆ«äº†LRMsä¸­å¸¸è§çš„æ¨ç†ä½æ•ˆæ¨¡å¼ï¼Œå¦‚é‡å¤å®šä¹‰ã€å¯¹ç®€å•é—®é¢˜è¿‡åº¦åˆ†æå’Œæ¨ç†è·¯å¾„çš„æµ…å±‚æ¢ç´¢ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾äº†ä»é¢„è®­ç»ƒåˆ°ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å†åˆ°æ¨ç†é˜¶æ®µï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡çš„å„ç§æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…³äºé«˜æ•ˆæ¨ç†æ–¹æ³•çš„åˆ†ç±»æ¡†æ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
ä½œè€…è¿˜ç»´æŠ¤äº†ä¸€ä¸ªå®æ—¶GitHubä»“åº“ï¼Œè·Ÿè¸ªè¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå®è´µçš„ä¿¡æ¯æ¥æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰å…·ä½“å±•ç¤ºå®éªŒç»“æœï¼Œè€Œæ˜¯æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ–‡çŒ®ç»¼è¿°ï¼Œæ¶µç›–äº†æé«˜LRMsæ¨ç†æ•ˆç‡çš„å„ç§æ–¹æ³•å’ŒæŠ€æœ¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡å¯¹äºç†è§£å’Œæé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ•ˆç‡æä¾›äº†å®è´µçš„æ´è§ã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªå¯å€Ÿé‰´ä¹‹å¤„ï¼š
- å®šä¹‰äº†æ¨ç†æ•ˆç‡çš„æ¦‚å¿µï¼Œå¹¶æå‡ºäº†è¡¡é‡æ•ˆç‡çš„æ–¹æ³•ã€‚
- è¯†åˆ«äº†LRMsä¸­å¸¸è§çš„ä½æ•ˆæ¨ç†æ¨¡å¼ï¼Œä¸ºæ”¹è¿›æ¨ç†è¿‡ç¨‹æä¾›äº†å…·ä½“çš„ç›®æ ‡ã€‚
- æä¾›äº†ä¸€ä¸ªå…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†ä»æ¨¡å‹é¢„è®­ç»ƒåˆ°æ¨ç†çš„å„ä¸ªé˜¶æ®µï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜å¿«é€Ÿäº†è§£è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚
- é€šè¿‡GitHubä»“åº“å®æ—¶è·Ÿè¸ªé¢†åŸŸè¿›å±•ï¼Œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªäº’åŠ¨å’Œæ›´æ–°çš„å¹³å°ã€‚

## model-merging-in-pre-training-of-large-language-models
### Abstract
Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„æ¨¡å‹åˆå¹¶æŠ€æœ¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç°ä»£å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå®ƒä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´ç€ä¸€ç³»åˆ—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜æ˜‚çš„é¢„è®­ç»ƒæˆæœ¬ã€é¢†åŸŸç‰¹å®šåè®­ç»ƒæ•ˆæœæŠ˜æ‰£ã€æ€§èƒ½æ‰©å±•é¢„æµ‹ä¸ç²¾ç¡®ä»¥åŠå¤§è§„æ¨¡è®­ç»ƒçš„ä¸ç¨³å®šæ€§ã€‚æ¨¡å‹åˆå¹¶ä½œä¸ºä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œæœ‰æ½œåŠ›ç¼“è§£è¿™äº›æŒ‘æˆ˜ï¼Œä½†ç›®å‰åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­çš„åº”ç”¨è¿˜ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶æ¨¡å‹åˆå¹¶æŠ€æœ¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆå¹¶ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†é¢„è®­ç»ƒæ¨¡å‹å¹³å‡ï¼ˆPMAï¼‰ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæ¨¡å‹çº§æƒé‡åˆå¹¶çš„æ–°æ¡†æ¶ã€‚é€šè¿‡åœ¨ç¨³å®šè®­ç»ƒé˜¶æ®µåˆå¹¶æ£€æŸ¥ç‚¹ï¼ŒPMAèƒ½å¤Ÿäº§ç”Ÿä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä½œè€…è¯¦ç»†ç ”ç©¶äº†å„ç§æ¨¡å‹åˆå¹¶æŠ€æœ¯åŠå…¶ç›¸å…³è¶…å‚æ•°ï¼Œæä¾›äº†å®ç”¨çš„é¢„è®­ç»ƒæŒ‡å—ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹åˆå¹¶çš„æ–°åº”ç”¨ï¼Œä¾‹å¦‚ç”¨äºæƒé‡åˆå§‹åŒ–çš„PMA-initï¼Œå®ƒæœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒåŠ¨æ€å‡ºç°ä¸¥é‡ä¸å¯æ¢å¤çš„æŸå¤±å³°å€¼æ—¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä»æ•°ç™¾ä¸‡åˆ°è¶…è¿‡1000äº¿å‚æ•°çš„ä¸åŒå¤§å°å’Œæ¶æ„çš„å¯†é›†æ¨¡å‹å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç¨³å®šè®­ç»ƒé˜¶æ®µåº”ç”¨PMAä¸ä»…èƒ½å¤Ÿæé«˜æ€§èƒ½ï¼Œè¿˜èƒ½å‡†ç¡®é¢„æµ‹é€€ç«è¡Œä¸ºï¼Œä»è€ŒåŠ å¿«éªŒè¯å‘¨æœŸå¹¶æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒPMA-initåœ¨è¿ç»­ç»§ç»­è®­ç»ƒï¼ˆCTï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µçš„åº”ç”¨ä¹Ÿæ˜¾ç¤ºå‡ºç¨³å®šè®­ç»ƒåŠ¨æ€çš„æ•ˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæä¾›äº†ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- æ¨¡å‹åˆå¹¶æŠ€æœ¯å¯ä»¥åœ¨é¢„è®­ç»ƒé˜¶æ®µæœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘è®­ç»ƒæˆæœ¬ã€‚
- PMAç­–ç•¥åœ¨ç¨³å®šè®­ç»ƒé˜¶æ®µçš„åº”ç”¨èƒ½å¤Ÿé¢„æµ‹é€€ç«åçš„æ¨¡å‹æ€§èƒ½ï¼Œæœ‰åŠ©äºå¿«é€ŸéªŒè¯å’Œæˆæœ¬èŠ‚çº¦ã€‚
- PMA-initæ–¹æ³•ä¸ºè®­ç»ƒä¸ç¨³å®šæ—¶æä¾›äº†å¯é çš„æ¢å¤æ‰‹æ®µï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
- å¯¹æ¨¡å‹åˆå¹¶æŠ€æœ¯å’Œè¶…å‚æ•°çš„è¯¦ç»†æ¶ˆèç ”ç©¶ä¸ºç¤¾åŒºæä¾›äº†å®ç”¨çš„é¢„è®­ç»ƒæŒ‡å—ã€‚

## improved-visual-spatial-reasoning-via-r1-zero-like-training
### Abstract
Increasing attention has been placed on improving the reasoning capacities of
multi-modal large language models (MLLMs). As the cornerstone for AI agents
that function in the physical realm, video-based visual-spatial intelligence
(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This
work conducts a first, in-depth study on improving the visual-spatial reasoning
of MLLMs via R1-Zero-like training. Technically, we first identify that the
visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models
cannot be activated via Chain of Thought (CoT) prompts. We then incorporate
GRPO training for improved visual-spatial reasoning, using the carefully
curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,
we identify the necessity to keep the KL penalty (even with a small value) in
GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from
Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.
Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves
performance comparable to that of the best open-source model
LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning
and direct preference optimization baselines and observe strong performance
superiority. The code and dataset will be available soon.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡R1-Zero-likeè®­ç»ƒæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å…´èµ·ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘è¾“å…¥æ—¶å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç‰©ç†ä¸–ç•Œä¸­çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ä»ç„¶ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘åŸºç¡€ä¸Šçš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡R1-Zero-likeè®­ç»ƒæ–¹æ³•ï¼Œæ·±å…¥ç ”ç©¶å¦‚ä½•æå‡MLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆå‘ç°ï¼Œå¯¹äºå°å‹åˆ°ä¸­å‹Qwen2-VLæ¨¡å‹ï¼Œç®€å•çš„æ¨ç†å¯¼å‘æç¤ºï¼ˆCoTï¼‰æ— æ³•æ¿€æ´»å…¶è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒçš„æç¤ºç­–ç•¥ï¼Œå‘ç°éCoTæç¤ºåœ¨å°åˆ°ä¸­å‹Qwen2-VLæ¨¡å‹ä¸Šè¡¨ç°æœ€ä½³ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†æå‡è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡é‡‡ç”¨äº†åŸºäºR1-Zero-likeçš„GRPOï¼ˆGeneralized Reward Propagation Optimizationï¼‰è®­ç»ƒæ–¹æ³•ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡10ä¸‡æ ·æœ¬çš„è§†é¢‘é—®ç­”æ•°æ®é›†VSI-100kã€‚è¯¥æ•°æ®é›†åŸºäºScanNetè·å–çš„é«˜ä¿çœŸè§†é¢‘æ‰«æå’Œè¯¦ç»†çš„å¯¹è±¡çº§3Dæ³¨é‡Šï¼Œä»è€Œå¯ä»¥è½»æ¾æ„å»ºä¸ç©ºé—´ä¿¡æ¯ç›¸å…³çš„ï¼ˆé—®é¢˜ï¼Œç­”æ¡ˆï¼‰å¯¹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡120 GPUå°æ—¶çš„è®­ç»ƒï¼Œæœ¬æ–‡çš„vsGRPO-2Bæ¨¡å‹ä»Qwen2-VL-2Bæ¨¡å‹å¾®è°ƒè€Œæ¥ï¼Œæ€§èƒ½æå‡äº†12.1%ï¼Œè¶…è¿‡äº†GPT-4oã€‚åŒæ—¶ï¼ŒvsGRPO-7Bæ¨¡å‹ä»Qwen2-VL-7Bæ¨¡å‹å¾®è°ƒè€Œæ¥ï¼Œæ€§èƒ½ä¸æœ€ä½³çš„å¼€æ”¾æºä»£ç æ¨¡å‹LLaVA-NeXT-Video-72Bç›¸å½“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¯”è¾ƒäº†GRPOä¸ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿ï¼Œå‘ç°GRPOåœ¨æå‡Qwen2-VLæ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒæ–¹æ³•å’Œæ•°æ®é›†ï¼Œå¯ä»¥æ˜¾è‘—æå‡MLLMsçš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°äº†åœ¨GRPOè®­ç»ƒä¸­ä¿æŒKLæƒ©ç½šï¼ˆå³ä½¿å€¼å¾ˆå°ï¼‰çš„å¿…è¦æ€§ï¼Œå¹¶è§‚å¯Ÿåˆ°äº†å¥–åŠ±é»‘å®¢ç°è±¡ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶å’Œæ¨¡å‹è®­ç»ƒæä¾›äº†å®è´µçš„å‚è€ƒã€‚

## learning-when-to-think--shaping-adaptive-reasoning-in-r1-style-models-via-multi-stage-rl
### Abstract
Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis ("...") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å­¦ä¹ ä½•æ—¶æ€è€ƒï¼šé€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ å¡‘é€ R1é£æ ¼æ¨¡å‹çš„è‡ªé€‚åº”æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ“…é•¿åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ç”Ÿæˆè¯¦ç»†çš„ã€é€æ­¥æ¨ç†åºåˆ—ã€‚ç„¶è€Œï¼Œå¯¹äºç®€å•é—®é¢˜ï¼Œè¿™ç§è¯¦ç»†çš„æ¨ç†å¯èƒ½ä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—å¼€é”€å’Œå»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™ç§è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œæœ¬æ–‡æ¢è®¨äº†å¦‚ä½•è®©LRMså…·å¤‡è‡ªé€‚åº”æ€è€ƒèƒ½åŠ›ï¼Œå³æ ¹æ®é—®é¢˜çš„å¤æ‚åº¦åŠ¨æ€å†³å®šæ˜¯å¦è¿›è¡Œæ˜¾å¼æ¨ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å‘ç°ï¼Œåœ¨R1é£æ ¼æ¨¡å‹çš„æç¤ºä¸­æ’å…¥ä¸€ä¸ªç®€å•çš„çœç•¥å·ï¼ˆ"..."ï¼‰å¯ä»¥éšæœºè§¦å‘æ€è€ƒæ¨¡å¼æˆ–éæ€è€ƒæ¨¡å¼ï¼Œæ­ç¤ºäº†æ¨ç†è¡Œä¸ºæ½œåœ¨çš„å¯æ§æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åŸºäºè¿™ä¸€å‘ç°ï¼Œæœ¬æ–‡æå‡ºäº†AutoThinkï¼Œä¸€ä¸ªå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é˜¶æ®µæ€§çš„å¥–åŠ±å¡‘é€ é€æ­¥ä¼˜åŒ–æ¨ç†ç­–ç•¥ã€‚AutoThinkåªåœ¨å¿…è¦æ—¶è°ƒç”¨æ˜¾å¼æ¨ç†ï¼Œè€Œå¯¹äºç®€å•ä»»åŠ¡åˆ™é»˜è®¤ç»™å‡ºç®€æ´çš„å“åº”ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªä¸»æµæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAutoThinkä¸æœ€è¿‘çš„æç¤ºå’ŒåŸºäºRLçš„ä¿®å‰ªæ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´æœ‰åˆ©çš„å‡†ç¡®æ€§ä¸æ•ˆç‡æƒè¡¡ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹ä¸Šï¼ŒAutoThinkå°†ç›¸å¯¹å‡†ç¡®æ€§æé«˜äº†6.4%ï¼ŒåŒæ—¶å‡å°‘äº†52%çš„tokenä½¿ç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºå¤§å‹æ¨ç†æ¨¡å‹æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”è‡ªé€‚åº”çš„æ¨ç†èŒƒå¼ï¼Œä¸ä»…èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ä»»ä½•R1é£æ ¼æ¨¡å‹ä¸­ï¼Œè¿˜æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®éªŒéªŒè¯äº†çœç•¥å·æç¤ºçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨ä¼˜åŒ–æ¨ç†è¡Œä¸ºæ–¹é¢çš„æ½œåŠ›ã€‚

## think-deep--think-fast--investigating-efficiency-of-verifier-free-inference-time-scaling-methods
### Abstract
There is intense interest in investigating how inference time compute (ITC)
(e.g. repeated sampling, refinements, etc) can improve large language model
(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,
such as Deepseek-R1, unlock the opportunity for reinforcement learning to
improve LLM reasoning skills. An in-depth understanding of how ITC interacts
with reasoning across different models could provide important guidance on how
to further advance the LLM frontier. This work conducts a comprehensive
analysis of inference-time scaling methods for both reasoning and non-reasoning
models on challenging reasoning tasks. Specifically, we focus our research on
verifier-free inference time-scaling methods due to its generalizability
without needing a reward model. We construct the Pareto frontier of quality and
efficiency. We find that non-reasoning models, even with an extremely high
inference budget, still fall substantially behind reasoning models. For
reasoning models, majority voting proves to be a robust inference strategy,
generally competitive or outperforming other more sophisticated ITC methods
like best-of-N and sequential revisions, while the additional inference compute
offers minimal improvements. We further perform in-depth analyses of the
association of key response features (length and linguistic markers) with
response quality, with which we can improve the existing ITC methods. We find
that correct responses from reasoning models are typically shorter and have
fewer hedging and thinking markers (but more discourse markers) than the
incorrect responses.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢æ— éªŒè¯å™¨æ¨ç†æ—¶æ‰©å±•æ–¹æ³•çš„é«˜æ•ˆæ€§ï¼šæ·±å…¥æ€è€ƒï¼Œå¿«é€Ÿè¡ŒåŠ¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè¶Šæ¥è¶Šå¼ºçš„èƒ½åŠ›ï¼Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬å¼€å§‹æ¢ç´¢åœ¨æ¨ç†æ—¶å¢åŠ è®¡ç®—èµ„æºï¼ˆæ¨ç†æ—¶è®¡ç®—ï¼ŒITCï¼‰çš„æ–¹æ³•ã€‚åŒæ—¶ï¼Œä¸€äº›æ–°çš„æ¨ç†æ¨¡å‹å¦‚Deepseek-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜äº†LLMçš„æ¨ç†æŠ€èƒ½ã€‚æœ¬æ–‡æ—¨åœ¨æ·±å…¥åˆ†æITCæ–¹æ³•å¦‚ä½•ä¸ä¸åŒç±»å‹çš„æ¨¡å‹ï¼ˆæ¨ç†æ¨¡å‹å’Œéæ¨ç†æ¨¡å‹ï¼‰çš„æ¨ç†èƒ½åŠ›ç›¸äº’ä½œç”¨ï¼Œä»¥æŒ‡å¯¼LLMé¢†åŸŸçš„å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡ä¸“æ³¨äºæ— éªŒè¯å™¨æ¨ç†æ—¶æ‰©å±•æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¸éœ€è¦å¥–åŠ±æ¨¡å‹ï¼Œå› æ­¤å…·æœ‰æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚ä½œè€…å¯¹æ¨ç†æ¨¡å‹å’Œéæ¨ç†æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢çš„ITCæ–¹æ³•åˆ†æã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä½œè€…æ„å»ºäº†è´¨é‡ä¸æ•ˆç‡çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œå‘ç°å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¤šæ•°æŠ•ç¥¨ç­–ç•¥é€šå¸¸æ¯”å…¶ä»–æ›´å¤æ‚çš„æ–¹æ³•ï¼ˆå¦‚æœ€ä½³Næ³•å’Œé¡ºåºä¿®è®¢ï¼‰æ›´å…·ç«äº‰åŠ›æˆ–æ›´ä¼˜ï¼Œè€Œé¢å¤–çš„æ¨ç†è®¡ç®—å¸¦æ¥çš„æ”¹è¿›å¾®ä¹å…¶å¾®ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜åˆ†æäº†å“åº”è¯­è¨€ç‰¹å¾ï¼ˆé•¿åº¦å’Œè¯­è¨€æ ‡è®°ï¼‰ä¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä¸ºæ”¹è¿›ç°æœ‰ITCæ–¹æ³•æä¾›äº†å®é™…æŒ‡å¯¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿éæ¨ç†æ¨¡å‹æ‹¥æœ‰æé«˜çš„æ¨ç†æ—¶è®¡ç®—é¢„ç®—ï¼Œå®ƒä»¬çš„è¡¨ç°ä»ç„¶è¿œè¿œè½åäºæ¨ç†æ¨¡å‹ã€‚å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¤šæ•°æŠ•ç¥¨è¯æ˜æ˜¯ä¸€ç§ç¨³å¥çš„æ¨ç†ç­–ç•¥ï¼Œé€šå¸¸æ¯”å…¶ä»–æ›´å¤æ‚çš„ITCæ–¹æ³•æ›´å…·ç«äº‰åŠ›æˆ–æ›´ä¼˜ã€‚æ­¤å¤–ï¼Œæ­£ç¡®çš„å“åº”é€šå¸¸æ¯”é”™è¯¯çš„å“åº”æ›´çŸ­ï¼Œä¸”å…·æœ‰æ›´å°‘çš„çŠ¹è±«å’Œæ€è€ƒæ ‡è®°ï¼Œä½†æ›´å¤šçš„è®ºè¿°æ ‡è®°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºå¹³è¡¡æ¨ç†è´¨é‡å’Œè®¡ç®—æ•ˆç‡æä¾›äº†å®ç”¨çš„æŒ‡å¯¼ï¼Œè¡¨æ˜æ— éœ€å¢åŠ è®¡ç®—æˆæœ¬å³å¯é€šè¿‡åˆ†æå“åº”çš„è¯­è¨€ç‰¹å¾æ¥æ”¹è¿›ç°æœ‰çš„ITCæ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼ºè°ƒäº†ä¸“é—¨ä¸ºæ¨ç†è®¾è®¡çš„æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„å†…åœ¨ä»·å€¼ã€‚

## ttrl--test-time-reinforcement-learning
### Abstract
This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the maj@n metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model maj@n,
and approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢æ— æ ‡ç­¾æ•°æ®ä¸Šçš„å¼ºåŒ–å­¦ä¹ ï¼šTTRLæ–¹æ³•è§£æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„è¿›æ­¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºé•¿é“¾æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºäº†å…¶é‡è¦æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¤§å¤šä¾èµ–äºæ ‡æ³¨æ•°æ®ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ç§åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåä¸ºTest-Time Reinforcement Learningï¼ˆTTRLï¼‰ï¼Œä»¥æ¨åŠ¨AIç³»ç»Ÿåœ¨æ— ç›‘ç£ç¯å¢ƒä¸‹çš„è‡ªæˆ‘è¿›åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
TTRLæ–¹æ³•é€šè¿‡åœ¨æµ‹è¯•æ—¶å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”æœªè§è¿‡çš„æ•°æ®ã€‚å®ƒé€šè¿‡é‡å¤é‡‡æ ·ç­–ç•¥åœ¨å±•å¼€é˜¶æ®µå‡†ç¡®ä¼°è®¡æ ‡ç­¾ï¼Œå¹¶è®¡ç®—åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œä»è€Œå®ç°åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„å¼ºåŒ–å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å¼•å…¥äº†å¤šæ•°æŠ•ç¥¨å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨æ²¡æœ‰åœ°é¢çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æä¾›äº†æœ‰æ•ˆçš„å¥–åŠ±ä¼°è®¡ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆä¸”ç¨³å®šçš„å¼ºåŒ–å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œå°†TTRLåº”ç”¨äºQwen2.5-Math-7Bæ¨¡å‹åï¼Œåœ¨AIME 2024ä»»åŠ¡ä¸Šçš„é€šè¿‡ç‡æå‡äº†211%ï¼ˆä»12.9æå‡åˆ°40.2ï¼‰ï¼Œå¹¶ä¸”åœ¨AIME 2024ã€AMCã€MATH-500å’ŒGPQAä»»åŠ¡ä¸Šçš„å¹³å‡æå‡è¾¾åˆ°äº†76%ã€‚è¿™äº›æ”¹è¿›æ˜¯åœ¨æ²¡æœ‰ä»»ä½•æ ‡æ³¨è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹é€šè¿‡è‡ªæˆ‘è¿›åŒ–å®ç°çš„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TTRLæ–¹æ³•ä¸ä»…æé«˜äº†é€šè¿‡ç‡ï¼Œè¿˜æ”¹å–„äº†æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰çš„æ•ˆæœã€‚æ­¤å¤–ï¼ŒTTRLåœ¨ä¸åŒè§„æ¨¡å’Œç±»å‹çš„æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•é›†æˆã€‚è¿™è¡¨æ˜TTRLæœ‰æ½œåŠ›æ˜¾è‘—å‡å°‘å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ï¼Œæ¨åŠ¨è¿ç»­å­¦ä¹ å’Œå¤§è§„æ¨¡æ— ç›‘ç£è®­ç»ƒçš„æ‰©å±•ã€‚

## patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner
### Abstract
Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose PathoCLIP, trained on the same figure-caption corpus used for continued
pretraining. Comprehensive experimental results demonstrate that both PathoCLIP
and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Patho-R1ï¼šå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ åŠ©åŠ›ç—…ç†å­¦è¯Šæ–­

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç—…ç†å­¦è¿™ä¸€å­é¢†åŸŸä»ç„¶é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„ç—…ç†å­¦ç‰¹å®šVLMsåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†å¯ä¿¡åº¦æ–¹é¢å­˜åœ¨å±€é™ï¼Œè¿™ä¸»è¦å½’å› äºå½“å‰ç—…ç†å­¦æ•°æ®é›†çš„æ€§è´¨ï¼Œè¿™äº›æ•°æ®é›†ä¸»è¦ç”±ç¼ºä¹æ·±åº¦å’Œç»“æ„åŒ–è¯Šæ–­èŒƒå¼çš„å›¾åƒæè¿°å¯¹ç»„æˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡åˆ©ç”¨ç—…ç†å­¦æ•™æå’ŒçœŸå®ä¸–ç•Œç—…ç†å­¦ä¸“å®¶çš„çŸ¥è¯†ï¼Œæ„å»ºäº†é«˜è´¨é‡ã€æ¨ç†å¯¼å‘çš„æ•°æ®é›†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®ç­›é€‰ç®¡é“ï¼Œè¯¥ç®¡é“åœ¨æœ€å°åŒ–äººå·¥åŠªåŠ›çš„åŒæ—¶ï¼Œç¡®ä¿äº†é«˜è´¨é‡æ¨ç†å¯¼å‘çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®çš„å¯æ‰©å±•ç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡ä»‹ç»äº†PathoCLIPï¼Œä¸€ä¸ªå¼€æºçš„ç—…ç†å­¦é€‚åº”CLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æœ¬æ–‡æ¢ç´¢äº†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¢†åŸŸé€‚åº”ä¸­çš„ç«¯åˆ°ç«¯è®­ç»ƒè¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼šGroup Relative Policy Optimizationï¼ˆGRPOï¼‰å’ŒDecoupled Clip and Dynamic sAmpling Policy Optimizationï¼ˆDAPOï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4
æœ¬æ–‡å‘å¸ƒäº†è§†è§‰è¯­è¨€ç—…ç†å­¦æ¨ç†æ¨¡å‹Patho-R1çš„æ¨¡å‹æƒé‡ï¼Œè¯¥æ¨¡å‹åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
PathoCLIPå’ŒPatho-R1åœ¨ä¸€ç³»åˆ—ç—…ç†å­¦ç›¸å…³ä»»åŠ¡ä¸­å‡å–å¾—äº†ç¨³å¥çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå¤šé¡¹é€‰æ‹©é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºç—…ç†å­¦é¢†åŸŸçš„æ•°æ®é›†æ„å»ºå’Œæ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯ä»¥ä¸‹æ–¹é¢å€¼å¾—å€Ÿé‰´ï¼š
- åˆ©ç”¨ç—…ç†å­¦æ•™æå’Œä¸“å®¶çŸ¥è¯†æ„å»ºé«˜è´¨é‡æ•°æ®é›†çš„æ–¹æ³•ã€‚
- ç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨¡å‹æ¨ç†è´¨é‡ä¼˜åŒ–çš„ç­–ç•¥ã€‚
- PathoCLIPå’ŒPatho-R1æ¨¡å‹çš„å®ç°å’Œå¼€æºï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚

## efficient-reasoning-models--a-survey
### Abstract
Reasoning models have demonstrated remarkable progress in solving complex and
logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to
arriving at a final answer. Yet, the emergence of this "slow-thinking"
paradigm, with numerous tokens generated in sequence, inevitably introduces
substantial computational overhead. To this end, it highlights an urgent need
for effective acceleration. This survey aims to provide a comprehensive
overview of recent advances in efficient reasoning. It categorizes existing
works into three key directions: (1) shorter - compressing lengthy CoTs into
concise yet effective reasoning chains; (2) smaller - developing compact
language models with strong reasoning capabilities through techniques such as
knowledge distillation, other model compression techniques, and reinforcement
learning; and (3) faster - designing efficient decoding strategies to
accelerate inference. A curated collection of papers discussed in this survey
is available in our GitHub repository.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢é«˜æ•ˆæ¨ç†æ¨¡å‹ï¼šå…¨é¢ç»¼è¿°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œæ¨ç†æ¨¡å‹åœ¨è§£å†³å¤æ‚é€»è¾‘å¯†é›†å‹ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡ç”Ÿæˆæ‰©å±•çš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtsï¼ŒCoTsï¼‰æ¥è¾¾åˆ°æœ€ç»ˆç­”æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ç§â€œæ…¢æ€è€ƒâ€èŒƒå¼åœ¨ç”Ÿæˆå¤§é‡è¿ç»­çš„æ ‡è®°æ—¶ï¼Œä¸å¯é¿å…åœ°å¼•å…¥äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ä»½å…³äºé«˜æ•ˆæ¨ç†æ¨¡å‹çš„å…¨é¢ç»¼è¿°ï¼Œæ¢è®¨å¦‚ä½•é€šè¿‡ä¸åŒæ–¹æ³•æé«˜æ¨ç†æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå‹ç¼©é•¿CoTs
æœ¬æ–‡å°†ç°æœ‰å·¥ä½œåˆ†ä¸ºä¸‰ç±»ä¸»è¦æ–¹å‘ä¹‹ä¸€æ˜¯â€œæ›´çŸ­â€ï¼Œå³é€šè¿‡å¤šç§æ–¹æ³•ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ ã€ç›‘ç£å¾®è°ƒã€æç¤ºé©±åŠ¨ç­–ç•¥å’Œæ½œåœ¨æ¨ç†ï¼‰å‹ç¼©å†—é•¿çš„CoTsï¼Œç”Ÿæˆç®€æ´è€Œæœ‰æ•ˆçš„æ¨ç†é“¾ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå°å‹å¼ºæ¨ç†èƒ½åŠ›è¯­è¨€æ¨¡å‹
ç¬¬äºŒä¸ªæ–¹å‘æ˜¯â€œæ›´å°â€ï¼Œå³é€šè¿‡çŸ¥è¯†è’¸é¦ã€æ¨¡å‹å‹ç¼©æŠ€æœ¯å’Œå¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼Œå¼€å‘å…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å°å‹è¯­è¨€æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæé«˜è§£ç æ•ˆç‡
ç¬¬ä¸‰ä¸ªæ–¹å‘æ˜¯â€œæ›´å¿«â€ï¼Œå³é€šè¿‡è®¾è®¡æ›´é«˜æ•ˆçš„è§£ç ç­–ç•¥ï¼ˆå¦‚æµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥ã€å¹¶è¡Œè§£ç å’Œé—®é¢˜åˆ†è§£ç­‰ï¼‰æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ç»¼è¿°äº†å„ç§é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„å®éªŒç»“æœã€‚è¿™äº›æ–¹æ³•åœ¨å‹ç¼©CoTsé•¿åº¦ã€å‡å°æ¨¡å‹å¤§å°å’Œæé«˜è§£ç æ•ˆç‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºæ¨ç†æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç»¼è¿°ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- é€šè¿‡ä¸åŒæ–¹æ³•å‹ç¼©CoTsé•¿åº¦ï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€ï¼›
- å¼€å‘å°å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥é™ä½èµ„æºéœ€æ±‚ï¼›
- è®¾è®¡é«˜æ•ˆçš„è§£ç ç­–ç•¥ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦ï¼›
- ç»¼åˆè€ƒè™‘æ¨¡å‹å¤§å°ã€æ¨ç†é“¾é•¿åº¦å’Œè§£ç æ•ˆç‡ï¼Œä»¥å®ç°å…¨é¢çš„æ¨ç†æ€§èƒ½ä¼˜åŒ–ã€‚

## optimizing-chain-of-thought-reasoners-via-gradient-variance-minimization-in-rejection-sampling-and-rl
### Abstract
Chain-of-thought (CoT) reasoning in large language models (LLMs) can be
formalized as a latent variable problem, where the model needs to generate
intermediate reasoning steps. While prior approaches such as iterative
reward-ranked fine-tuning (RAFT) have relied on such formulations, they
typically apply uniform inference budgets across prompts, which fails to
account for variability in difficulty and convergence behavior. This work
identifies the main bottleneck in CoT training as inefficient stochastic
gradient estimation due to static sampling strategies. We propose GVM-RAFT, a
prompt-specific Dynamic Sample Allocation Strategy designed to minimize
stochastic gradient variance under a computational budget constraint. The
method dynamically allocates computational resources by monitoring prompt
acceptance rates and stochastic gradient norms, ensuring that the resulting
gradient variance is minimized. Our theoretical analysis shows that the
proposed dynamic sampling strategy leads to accelerated convergence guarantees
under suitable conditions. Experiments on mathematical reasoning show that
GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over
vanilla RAFT. The proposed dynamic sampling strategy is general and can be
incorporated into other reinforcement learning algorithms, such as GRPO,
leading to similar improvements in convergence and test accuracy. Our code is
available at https://github.com/RLHFlow/GVM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¼˜åŒ–é“¾å¼æ€ç»´æ¨ç†ï¼šé€šè¿‡æ¢¯åº¦æ–¹å·®æœ€å°åŒ–æå‡é‡‡æ ·æ•ˆç‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ—¶ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚è™½ç„¶ä¹‹å‰çš„è¿­ä»£å¥–åŠ±æ’åºå¾®è°ƒï¼ˆRAFTï¼‰ç­‰æ–¹æ³•ä¾èµ–è¿™ç§æ¨ç†æ¡†æ¶ï¼Œä½†å®ƒä»¬é€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†é¢„ç®—ï¼Œæœªèƒ½è€ƒè™‘åˆ°ä¸åŒæç¤ºçš„éš¾åº¦å’Œæ”¶æ•›è¡Œä¸ºçš„å·®å¼‚æ€§ã€‚æœ¬æ–‡æŒ‡å‡ºï¼ŒCoTè®­ç»ƒä¸­çš„ä¸»è¦ç“¶é¢ˆåœ¨äºç”±äºé™æ€é‡‡æ ·ç­–ç•¥å¯¼è‡´çš„ä¸é«˜æ•ˆéšæœºæ¢¯åº¦ä¼°è®¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥GVM-RAFTï¼Œæ—¨åœ¨åœ¨è®¡ç®—é¢„ç®—çº¦æŸä¸‹æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›‘æ§æç¤ºæ¥å—ç‡å’Œéšæœºæ¢¯åº¦èŒƒæ•°ï¼ŒåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œç¡®ä¿æ¢¯åº¦æ–¹å·®æœ€å°åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ç†è®ºåˆ†æè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥åœ¨åˆé€‚çš„æ¡ä»¶ä¸‹å¯ä»¥åŠ é€Ÿæ”¶æ•›ã€‚åœ¨æ•°å­¦æ¨ç†çš„å®éªŒä¸­ï¼ŒGVM-RAFTå®ç°äº†2-4å€çš„æ”¶æ•›é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”æ˜¾è‘—æé«˜äº†æœ€ç»ˆæµ‹è¯•çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæå‡ºçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥æ˜¯é€šç”¨çš„ï¼Œå¯ä»¥æ•´åˆåˆ°å…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œå¦‚GRPOï¼Œå¸¦æ¥ç±»ä¼¼çš„æ”¶æ•›é€Ÿåº¦å’Œæµ‹è¯•å‡†ç¡®æ€§çš„æå‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜GVM-RAFTæ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿçš„RAFTæ–¹æ³•ï¼Œä¸ä»…æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œè€Œä¸”æµ‹è¯•å‡†ç¡®æ€§ä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚å…·ä½“æ¥è¯´ï¼ŒGVM-RAFTå®ç°äº†2-4å€çš„æ”¶æ•›é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”æé«˜äº†æœ€ç»ˆæµ‹è¯•çš„å‡†ç¡®æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºä¼˜åŒ–CoTæ¨ç†æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå³é€šè¿‡åŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥æ¥æé«˜æ¢¯åº¦ä¼°è®¡çš„æ•ˆç‡ã€‚è¿™ç§æ–¹æ³•ä¸ä»…é€‚ç”¨äºRAFTç®—æ³•ï¼Œè¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä¸­æå‡ºçš„ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯éƒ½ä¸ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§æä¾›äº†æ”¯æŒã€‚

## pharmolixfm--all-atom-foundation-models-for-molecular-modeling-and-generation
### Abstract
Structural biology relies on accurate three-dimensional biomolecular
structures to advance our understanding of biological functions, disease
mechanisms, and therapeutics. While recent advances in deep learning have
enabled the development of all-atom foundation models for molecular modeling
and generation, existing approaches face challenges in generalization due to
the multi-modal nature of atomic data and the lack of comprehensive analysis of
training and sampling strategies. To address these limitations, we propose
PharMolixFM, a unified framework for constructing all-atom foundation models
based on multi-modal generative techniques. Our framework includes three
variants using state-of-the-art multi-modal generative models. By formulating
molecular tasks as a generalized denoising process with task-specific priors,
PharMolixFM achieves robust performance across various structural biology
applications. Experimental results demonstrate that PharMolixFM-Diff achieves
competitive prediction accuracy in protein-small-molecule docking (83.9% vs.
90.2% RMSD < 2{\AA}, given pocket) with significantly improved inference speed.
Moreover, we explore the empirical inference scaling law by introducing more
sampling repeats or steps. Our code and model are available at
https://github.com/PharMolix/OpenBioMed.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "PharMolixFMï¼šåˆ†å­å»ºæ¨¡ä¸ç”Ÿæˆçš„å…¨åŸå­åŸºç¡€æ¨¡å‹æ–°æ¡†æ¶"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç»“æ„ç”Ÿç‰©å­¦ä¾èµ–äºç²¾ç¡®çš„ä¸‰ç»´ç”Ÿç‰©åˆ†å­ç»“æ„ï¼Œä»¥æ¨åŠ¨æˆ‘ä»¬å¯¹ç”Ÿç‰©åŠŸèƒ½ã€ç–¾ç—…æœºåˆ¶å’Œç–—æ³•çš„ç†è§£ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•å·²ç»ä½¿å¾—å…¨åŸå­åŸºç¡€æ¨¡å‹åœ¨åˆ†å­å»ºæ¨¡å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†å‘å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ³›åŒ–æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜ä¸»è¦æºäºåŸå­æ•°æ®çš„å¤šå…ƒæ¨¡æ€ç‰¹æ€§ä»¥åŠè®­ç»ƒå’Œé‡‡æ ·ç­–ç•¥çš„å…¨é¢åˆ†æä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†PharMolixFMï¼Œä¸€ä¸ªåŸºäºå¤šå…ƒæ¨¡æ€ç”ŸæˆæŠ€æœ¯çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºæ„å»ºå…¨åŸå­åŸºç¡€æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
PharMolixFMæ¡†æ¶é‡‡ç”¨äº†ä¸‰ç§æœ€æ–°çš„å¤šå…ƒæ¨¡æ€ç”Ÿæˆæ¨¡å‹å˜ä½“ï¼ŒåŒ…æ‹¬å¤šå…ƒæ¨¡æ€æ‰©æ•£ï¼ˆPharMolixFM-Diffusionï¼‰ã€å¤šå…ƒæ¨¡æ€æµåŒ¹é…ï¼ˆPharMolixFM-Flowï¼‰å’Œè´å¶æ–¯æµç½‘ç»œï¼ˆPharMolixFM-BFNï¼‰ï¼Œä»¥å…±åŒæ•æ‰ä¸åŒç±»å‹ç”Ÿç‰©åˆ†å­ä¸­çš„åŸå­ç±»å‹å’ŒåŸå­åæ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å°†ä¸åŒçš„åˆ†å­ä»»åŠ¡å…¬å¼åŒ–ä¸ºä¸€ä¸ªå¸¦æœ‰ä»»åŠ¡ç‰¹å®šå…ˆéªŒçš„å¹¿ä¹‰å»å™ªè¿‡ç¨‹ï¼Œè¿™ä½¿å¾—PharMolixFMåœ¨å¤šç§ç»“æ„ç”Ÿç‰©å­¦åº”ç”¨ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚é€šè¿‡å€Ÿé‰´PocketXMolçš„æ–¹æ³•ï¼ŒPharMolixFMå°†ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ç»Ÿä¸€åœ¨åŒä¸€ä¸ªç”Ÿæˆå»å™ªè¿‡ç¨‹ä¸­ï¼Œä½†ä½¿ç”¨ä¸åŒçš„å…ˆéªŒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è›‹ç™½è´¨-å°åˆ†å­å¯¹æ¥ä»»åŠ¡ä¸­ï¼ŒPharMolixFM-Diffusionå®ç°äº†ä¸AlphaFold3ç›¸å½“çš„é¢„æµ‹ç²¾åº¦ï¼ˆ83.9% vs. 90.2% RMSD < 2Ã…ï¼Œç»™å®šå£è¢‹ï¼‰ï¼Œå¹¶ä¸”å…·æœ‰æ˜¾è‘—æé«˜çš„æ¨ç†é€Ÿåº¦ï¼ˆå•ä¸ªA800 GPUä¸Šå¤§çº¦4.6ç§’ vs. çº¦249.0ç§’ï¼‰ã€‚åœ¨åŸºäºç»“æ„çš„è¯ç‰©è®¾è®¡ä»»åŠ¡ä¸­ï¼Œæ‰€æœ‰PharMolixFMæ¨¡å‹ç”Ÿæˆçš„åˆ†å­çš„è¯ç‰©æ€§å‡æ˜¾ç¤ºå‡ºä¸€è‡´çš„æ”¹å–„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†PharMolixFMçš„å®è¯æ¨ç†è§„æ¨¡æ³•åˆ™ï¼Œå¹¶å¯¹ä¸åŒè®­ç»ƒä»»åŠ¡çš„å½±å“ªäº›è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PharMolixFMæ¡†æ¶ä¸ºåˆ†å­å»ºæ¨¡å’Œç”Ÿæˆæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯å…¶å¤šå…ƒæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„æ•´åˆå’Œä»»åŠ¡ç‰¹å®šçš„å»å™ªè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹è®­ç»ƒå’Œé‡‡æ ·ç­–ç•¥çš„æ·±å…¥åˆ†æä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†å®è´µçš„å‚è€ƒã€‚ä»£ç å’Œæ¨¡å‹å·²ç»åœ¨GitHubä¸Šå…¬å¼€ï¼Œå¯ä¾›ç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶å’Œä½¿ç”¨ã€‚

## r1-sharevl--incentivizing-reasoning-capability-of-multimodal-large-language-models-via-share-grpo
### Abstract
In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œè§£é”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼šShare-GRPOæ–¹æ³•ç»¼è¿°â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå¦‚Kimi-K1.5å’ŒDeepSeek-R1ï¼Œæ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡å¦‚æ•°å­¦å’Œç§‘å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å­˜åœ¨ç¨€ç–å¥–åŠ±å’Œä¼˜åŠ¿æ¶ˆå¤±çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æå‡å—é™ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºShare-GRPOçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æ¿€åŠ±MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
Share-GRPOé€šè¿‡æ•°æ®è½¬æ¢æŠ€æœ¯æ‰©å±•é—®é¢˜ç©ºé—´ï¼Œé¼“åŠ±MLLMsåœ¨æ‰©å±•çš„é—®é¢˜ç©ºé—´ä¸­æœ‰æ•ˆåœ°æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å…±äº«è¿™äº›å‘ç°çš„æ¨ç†è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¯ä¸ªæ‰©å±•çš„é—®é¢˜å˜ä½“éƒ½èƒ½ä»å…¶ä»–é—®é¢˜å˜ä½“ä¸­ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ä¸­å—ç›Šï¼Œå…±åŒæ¢ç´¢å’Œå­¦ä¹ å…±äº«çš„è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
Share-GRPOåœ¨ä¼˜åŠ¿è®¡ç®—è¿‡ç¨‹ä¸­å…±äº«å¥–åŠ±ä¿¡æ¯ï¼Œé‡‡ç”¨åˆ†å±‚ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æœ¬åœ°çº§åˆ«å’Œå…¨å±€çº§åˆ«ä¼°è®¡ä¼˜åŠ¿ï¼Œæœ¬åœ°çº§åˆ«æ•è·æ¯ä¸ªé—®é¢˜å˜ä½“çš„å†…éƒ¨ç»“æ„å’Œå˜åŒ–ï¼Œè€Œå…¨å±€çº§åˆ«åˆ©ç”¨ä¸åŒå˜ä½“ä¹‹é—´çš„å¤šæ ·æ€§å’Œäº’è¡¥æ€§ï¼Œä»è€Œç¨³å®šå¥–åŠ±ä¿¡å·ï¼Œæé«˜ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœæ˜¾ç¤ºShare-GRPOæ–¹æ³•åœ¨æ•°å­¦å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šå‡ä¼˜äºåŸºçº¿å’Œå…¶ä»–æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ¨ç†MLLMsã€‚å®éªŒè¯æ˜äº†Share-GRPOåœ¨ç¼“è§£ç¨€ç–å¥–åŠ±å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Share-GRPOæ–¹æ³•ä¸ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æå‡æ–¹é¢æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ‰©å±•é—®é¢˜ç©ºé—´å’Œå…±äº«æ¨ç†è½¨è¿¹æ¥å¢å¼ºæ¨¡å‹çš„æ¢ç´¢å’Œå­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåˆ†å±‚ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•ä¸ºç¨³å®šå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹æä¾›äº†æ–°çš„æ€è·¯ã€‚è¿™äº›æ–¹æ³•å’ŒæŠ€æœ¯å¯¹äºæœªæ¥å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## rl-tango--reinforcing-generator-and-verifier-together-for-language-reasoning
### Abstract
Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œRL Tangoï¼šååŒå¼ºåŒ–å­¦ä¹ æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°æ—¥ç›Šå‡ºè‰²ï¼Œå®ƒä»¬åœ¨å¤„ç†éœ€è¦å¤šæ­¥éª¤æ€è€ƒå’Œè®¡åˆ’çš„å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†å¢å¼ºè¿™äº›æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé€šå¸¸é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œåè®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLåè®­ç»ƒæ–¹æ³•é€šå¸¸ä½¿ç”¨å›ºå®šçš„éªŒè¯å™¨ï¼ˆåŸºäºè§„åˆ™æˆ–é¢„è®­ç»ƒçš„ï¼‰æˆ–é€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒçš„åˆ¤åˆ«å¼éªŒè¯å™¨ï¼Œè¿™äº›è®¾è®¡å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œä¸”åœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–æ³›åŒ–èƒ½åŠ›å·®ã€‚æœ¬æ–‡æ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶TANGOï¼Œé€šè¿‡RLåŒæ—¶ååŒè®­ç»ƒLLMç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
TANGOæ¡†æ¶çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ä¸€ä¸ªç”Ÿæˆæ€§çš„ã€è¿‡ç¨‹çº§çš„LLMéªŒè¯å™¨ï¼Œè¯¥éªŒè¯å™¨é€šè¿‡RLè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸ç”Ÿæˆå™¨å…±åŒè¿›åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒTANGOçš„éªŒè¯å™¨ä»…åŸºäºç»“æœçº§çš„éªŒè¯æ­£ç¡®æ€§å¥–åŠ±è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ˜¾å¼çš„è¿‡ç¨‹çº§æ³¨é‡Šã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
TANGOé€šè¿‡RLå®ç°äº†ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨çš„äº¤é”™è®­ç»ƒï¼Œä½¿å¾—ä¸¤è€…èƒ½å¤Ÿç›¸äº’å¼ºåŒ–ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚ç”Ÿæˆå™¨é€šè¿‡ç»“åˆç»“æœçº§æ­£ç¡®æ€§ä¿¡å·å’ŒéªŒè¯å™¨æä¾›çš„è¯¦ç»†æ­¥éª¤çº§å¥–åŠ±ï¼ŒæŒ‡å¯¼ç”Ÿæˆå™¨å‘æ›´ç¨³å¥çš„æ¨ç†ç­–ç•¥å‘å±•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒæ¥è¯„ä¼°TANGOçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºï¼ŒTANGOåœ¨7B/8Bè§„æ¨¡æ¨¡å‹ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ç”Ÿæˆå™¨åœ¨äº”ä¸ªç«èµ›çº§åˆ«çš„æ•°å­¦åŸºå‡†æµ‹è¯•å’Œå››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸŸå¤–æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè€ŒéªŒè¯å™¨åœ¨ProcessBenchæ•°æ®é›†ä¸Šé¢†å…ˆã€‚ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTANGOåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†é—®é¢˜ä¸Šå±•ç°å‡ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TANGOæ¡†æ¶æä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ååŒè¿›åŒ–ç³»ç»Ÿè®¾è®¡ï¼Œé€šè¿‡RLè®­ç»ƒéªŒè¯å™¨ï¼Œå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒTANGOçš„ç”Ÿæˆæ€§å’ŒæŠ½æ ·æ€§éªŒè¯å™¨å¢åŠ äº†å¥–åŠ±ä¿¡å·çš„éšæœºæ€§ï¼Œæé«˜äº†å…¶ç¨³å¥æ€§ã€‚è¿™äº›è®¾è®¡ç†å¿µå¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## do-not-let-low-probability-tokens-over-dominate-in-rl-for-llms
### Abstract
Reinforcement learning (RL) has become a cornerstone for enhancing the
reasoning capabilities of large language models (LLMs), with recent innovations
such as Group Relative Policy Optimization (GRPO) demonstrating exceptional
effectiveness. In this study, we identify a critical yet underexplored issue in
RL training: low-probability tokens disproportionately influence model updates
due to their large gradient magnitudes. This dominance hinders the effective
learning of high-probability tokens, whose gradients are essential for LLMs'
performance but are substantially suppressed. To mitigate this interference, we
propose two novel methods: Advantage Reweighting and Low-Probability Token
Isolation (Lopti), both of which effectively attenuate gradients from
low-probability tokens while emphasizing parameter updates driven by
high-probability tokens. Our approaches promote balanced updates across tokens
with varying probabilities, thereby enhancing the efficiency of RL training.
Experimental results demonstrate that they substantially improve the
performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K
Logic Puzzle reasoning tasks. Our implementation is available at
https://github.com/zhyang2226/AR-Lopti.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼šæŠ‘åˆ¶ä½æ¦‚ç‡è¯æ±‡çš„è¿‡åº¦å½±å“â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æå‡ä¸­çš„åº”ç”¨æ—¥ç›Šæˆç†Ÿï¼Œè¿‘æœŸçš„ç ”ç©¶å¦‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰å·²ç»æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœã€‚ç„¶è€Œï¼Œæœ¬æ–‡æŒ‡å‡ºä¸€ä¸ªåœ¨RLè®­ç»ƒä¸­å°šæœªè¢«å……åˆ†æ¢ç´¢çš„å…³é”®é—®é¢˜ï¼šä½æ¦‚ç‡è¯æ±‡ç”±äºæ¢¯åº¦å¹…å€¼è¾ƒå¤§ï¼Œä¸æˆæ¯”ä¾‹åœ°å½±å“äº†æ¨¡å‹æ›´æ–°ã€‚è¿™ç§ç°è±¡é˜»ç¢äº†é«˜æ¦‚ç‡è¯æ±‡çš„æœ‰æ•ˆå­¦ä¹ ï¼Œè€Œé«˜æ¦‚ç‡è¯æ±‡çš„æ¢¯åº¦å¯¹äºLLMçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
ä¸ºäº†ç¼“è§£ä½æ¦‚ç‡è¯æ±‡å¯¹æ¨¡å‹æ›´æ–°çš„è¿‡åº¦å½±å“ï¼Œæœ¬æ–‡æå‡ºäº†â€œä¼˜åŠ¿é‡åŠ æƒâ€æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡é™ä½ä½æ¦‚ç‡è¯æ±‡çš„æƒé‡ï¼Œä»è€Œå‡å°‘å®ƒä»¬å¯¹æ¢¯åº¦æ›´æ–°çš„è´¡çŒ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡è¿˜æå‡ºäº†â€œä½æ¦‚ç‡è¯æ±‡éš”ç¦»â€ï¼ˆLoptiï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä½æ¦‚ç‡è¯æ±‡ä¸é«˜æ¦‚ç‡è¯æ±‡åˆ†å¼€å¤„ç†ï¼Œå…ˆæ›´æ–°ä½æ¦‚ç‡è¯æ±‡ï¼Œå†æ›´æ–°é«˜æ¦‚ç‡è¯æ±‡ï¼Œä»è€Œç¡®ä¿é«˜æ¦‚ç‡è¯æ±‡çš„æ¢¯åº¦æ›´æ–°ä¸ä¼šå—åˆ°ä½æ¦‚ç‡è¯æ±‡çš„å¹²æ‰°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½æ˜¾è‘—æé«˜GRPOè®­ç»ƒçš„LLMæ€§èƒ½ã€‚åœ¨K&K Logic Puzzleæ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸åŸå§‹GRPOç›¸æ¯”ï¼Œä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•å¯ä»¥åˆ†åˆ«æé«˜35.9%å’Œ38.5%ï¼ŒåŒæ—¶ä½¿ç”¨æ—¶å¯ä»¥æé«˜46.2%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†æ–°çš„è§†è§’ï¼Œæ­ç¤ºäº†ä½æ¦‚ç‡è¯æ±‡åœ¨æ¢¯åº¦æ›´æ–°ä¸­çš„è¿‡åº¦å½±å“é—®é¢˜ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè€Œä¸”ä¸ºæœªæ¥RLåœ¨LLMä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ä¸­çš„ä¸€ç§ï¼ˆä¼˜åŠ¿é‡åŠ æƒï¼‰å‡ ä¹ä¸å¢åŠ é¢å¤–çš„è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­æ›´å…·å¸å¼•åŠ›ã€‚

## noisyrollout--reinforcing-visual-reasoning-with-data-augmentation
### Abstract
Recent advances in reinforcement learning (RL) have strengthened the
reasoning capabilities of vision-language models (VLMs). However, enhancing
policy exploration to better scale test-time compute remains largely
underexplored. In addition, VLMs continue to struggle with imperfect visual
perception, which in turn affects the subsequent reasoning process. To this
end, we propose NoisyRollout, a simple yet effective data augmentation method
that mixes trajectories from both clean and moderately distorted images during
RL training. By injecting targeted diversity in visual perception and the
resulting reasoning patterns, NoisyRollout promotes better policy exploration
through vision-oriented inductive biases, ultimately leading to more robust
reasoning behaviors. We further adopt a noise annealing schedule that gradually
reduces distortion strength over training, leveraging noisy signals early on
while ensuring training stability in later stages. Crucially, our method is
easy-to-adopt--requiring no additional training cost and no modifications to
the RL objective. Extensive experiments on $2$ distinct training datasets
demonstrate that NoisyRollout achieves state-of-the-art performance among
open-source RL-tuned models across $5$ out-of-domain reasoning and perception
benchmarks. Furthermore, we validate the effectiveness of NoisyRollout across
model sizes ($7$B and $32$B) and data scales (from $1$K to $6$K), highlighting
its generalizability and scalability.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œæ³¨å…¥å™ªå£°ä»¥å¼ºåŒ–è§†è§‰æ¨ç†ï¼šNoisyRolloutæ–¹æ³•è§£æâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„åº”ç”¨ï¼Œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œå¦‚ä½•é€šè¿‡å¢å¼ºç­–ç•¥æ¢ç´¢æ¥æ›´å¥½åœ°æ‰©å±•æµ‹è¯•æ—¶çš„è®¡ç®—èƒ½åŠ›ï¼Œä»¥åŠå¦‚ä½•è§£å†³VLMåœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„ä¸è¶³ï¼Œè¿™ä¸¤ä¸ªé—®é¢˜åœ¨ç°æœ‰ç ”ç©¶ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æœ¬æ–‡é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†NoisyRolloutæ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ··åˆä½¿ç”¨å¹²å‡€å’Œé€‚åº¦æ‰­æ›²çš„å›¾åƒè½¨è¿¹ï¼Œä»¥å¢å¼ºè§†è§‰æ¨ç†çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
NoisyRolloutå¼•å…¥äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ°´å°å¢å¼ºç­–ç•¥ï¼Œå³åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå°†åŸºäºåŸå§‹å¹²å‡€å›¾åƒå’Œé€‚åº¦æ‰­æ›²å›¾åƒçš„è½¨è¿¹æ··åˆèµ·æ¥ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ³¨å…¥é’ˆå¯¹æ€§çš„è§†è§‰æ„ŸçŸ¥å¤šæ ·æ€§å’Œæ¨ç†æ¨¡å¼ï¼Œä¿ƒè¿›äº†ç­–ç•¥æ¢ç´¢ï¼Œæœ€ç»ˆå¯¼è‡´äº†æ›´é²æ£’çš„æ¨ç†è¡Œä¸ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†ä¸€ä¸ªå™ªå£°é€€ç«è®¡åˆ’ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸å‡å°‘å›¾åƒæ‰­æ›²çš„å¼ºåº¦ã€‚è¿™ç§ç­–ç•¥åœ¨è®­ç»ƒæ—©æœŸåˆ©ç”¨äº†å™ªå£°ä¿¡å·ï¼ŒåŒæ—¶åœ¨è®­ç»ƒåæœŸä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œé¿å…äº†ç­–ç•¥æ¢¯åº¦ä¼°è®¡çš„ä¸ç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸¤ä¸ªä¸åŒçš„è®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜NoisyRolloutåœ¨äº”ä¸ªè¶…å‡ºè®­ç»ƒåŸŸçš„æ¨ç†å’Œæ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾¾åˆ°äº†å¼€æºRLè°ƒä¼˜æ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å¤§å°ï¼ˆ7Bå’Œ32Bï¼‰å’Œæ•°æ®è§„æ¨¡ï¼ˆä»1Kåˆ°6Kï¼‰ä¸Šéƒ½è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
NoisyRolloutæ–¹æ³•æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå³é€šè¿‡æ•°æ®å¢å¼ºæ¥æ”¹å–„VLMçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºæ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬ï¼Œä¹Ÿä¸éœ€è¦å¯¹RLç›®æ ‡è¿›è¡Œä¿®æ”¹ï¼Œå³å¯è½»æ¾é›†æˆåˆ°ç°æœ‰çš„GRPOå®ç°ä¸­ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”ç”±äºå…¶ç®€å•æ€§å’Œè½»é‡çº§ç‰¹æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## reinforcement-learning-from-human-feedback
### Abstract
Reinforcement learning from human feedback (RLHF) has become an important
technical and storytelling tool to deploy the latest machine learning systems.
In this book, we hope to give a gentle introduction to the core methods for
people with some level of quantitative background. The book starts with the
origins of RLHF -- both in recent literature and in a convergence of disparate
fields of science in economics, philosophy, and optimal control. We then set
the stage with definitions, problem formulation, data collection, and other
common math used in the literature. The core of the book details every
optimization stage in using RLHF, from starting with instruction tuning to
training a reward model and finally all of rejection sampling, reinforcement
learning, and direct alignment algorithms. The book concludes with advanced
topics -- understudied research questions in synthetic data and evaluation --
and open questions for the field.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "è¿ˆå‘æ›´æ™ºèƒ½çš„æœºå™¨å­¦ä¹ ï¼šåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ å…¨è§£è¯»"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ä¸æ–­å‘å±•ï¼Œå¦‚ä½•å°†è¿™äº›ç³»ç»Ÿä¸äººç±»åå¥½ç›¸ç»“åˆï¼Œä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´ç¬¦åˆäººç±»æœŸæœ›çš„è¡Œä¸ºï¼Œæˆä¸ºäº†ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºå…·æœ‰ä¸€å®šå®šé‡èƒŒæ™¯çš„è¯»è€…æä¾›ä¸€ä¸ªå…³äºåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„å…¨é¢ä»‹ç»ï¼Œä»èµ·æºåˆ°æœ€æ–°çš„ç ”ç©¶è¿›å±•ï¼Œä»¥åŠç›¸å…³çš„æ•°å­¦å®šä¹‰å’Œä¼˜åŒ–æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†RLHFçš„æ ¸å¿ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ä»äººç±»åå¥½ä¸­å­¦ä¹ ã€æŒ‡ä»¤å¾®è°ƒã€å¥–åŠ±æ¨¡å‹è®­ç»ƒã€æ‹’ç»é‡‡æ ·ã€ç­–ç•¥æ¢¯åº¦ç®—æ³•å’Œç›´æ¥å¯¹é½ç®—æ³•ç­‰ã€‚è¿™äº›æ–¹æ³•å…±åŒæ„æˆäº†RLHFçš„å®Œæ•´æµç¨‹ï¼Œä½¿å¾—æœºå™¨å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œäººç±»çš„æŒ‡ä»¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è®ºæ–‡æ·±å…¥æ¢è®¨äº†åå¥½æ•°æ®çš„æ”¶é›†å’Œå¤„ç†ï¼ŒåŒ…æ‹¬å¦‚ä½•é€šè¿‡ç•Œé¢è®¾è®¡ã€æ’åä¸è¯„åˆ†ã€ç»“æ„åŒ–åå¥½æ•°æ®ä»¥åŠæ•°æ®æ¥æºå’ŒåˆåŒç­‰æ–¹é¢æ¥ä¼˜åŒ–æ•°æ®è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†å¥–åŠ±æ¨¡å‹çš„æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œå˜ä½“ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡æ­£åˆ™åŒ–æŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è™½ç„¶æœ¬æ–‡ä¸»è¦æä¾›ç†è®ºå’Œæ–¹æ³•è®ºä¸Šçš„ä»‹ç»ï¼Œä½†ä½œè€…æŒ‡å‡ºï¼ŒRLHFåœ¨å®é™…åº”ç”¨ä¸­å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€æ¨¡å‹é¢†åŸŸã€‚é€šè¿‡å®éªŒéªŒè¯ï¼ŒRLHFèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶ç”Ÿæˆçš„å†…å®¹æ›´ç¬¦åˆäººç±»çš„æœŸæœ›å’Œåå¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆæä¾›äº†ä¸€ä¸ªå®è´µçš„èµ„æºï¼Œè¯¦ç»†ä»‹ç»äº†RLHFçš„ç†è®ºåŸºç¡€å’Œå®è·µæ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- å¦‚ä½•ä»äººç±»åå¥½ä¸­å­¦ä¹ å¹¶ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
- æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ¨¡å‹è®­ç»ƒçš„å…·ä½“æ­¥éª¤å’Œæœ€ä½³å®è·µã€‚
- å¦‚ä½•é€šè¿‡æ‹’ç»é‡‡æ ·å’Œç­–ç•¥æ¢¯åº¦ç®—æ³•æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚
- ç›´æ¥å¯¹é½ç®—æ³•çš„åº”ç”¨å’Œå®ç°ç»†èŠ‚ã€‚
- åå¥½æ•°æ®çš„æ”¶é›†å’Œå¤„ç†æŠ€å·§ï¼Œä»¥åŠå¦‚ä½•é¿å…æ•°æ®æ±¡æŸ“å’Œè¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚
- å¦‚ä½•åœ¨äº§å“è®¾è®¡å’Œç”¨æˆ·ä½“éªŒä¸­èå…¥RLHFï¼Œä»¥åˆ›å»ºæ›´å…·ä¸ªæ€§å’Œå¸å¼•åŠ›çš„AIäº§å“ã€‚

## reasoning-beyond-limits--advances-and-open-problems-for-llms
### Abstract
Recent generative reasoning breakthroughs have transformed how large language
models (LLMs) tackle complex problems by dynamically retrieving and refining
information while generating coherent, multi-step thought processes. Techniques
such as inference-time scaling, reinforcement learning, supervised fine-tuning,
and distillation have been successfully applied to models like DeepSeek-R1,
OpenAI's o1 & o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in
enhanced reasoning capabilities. In this paper, we provide a comprehensive
analysis of the top 27 LLM models released between 2023 and 2025 (including
models such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and
phi-4). Then, we present an extensive overview of training methodologies that
spans general training approaches, mixture-of-experts (MoE) and architectural
innovations, retrieval-augmented generation (RAG), chain-of-thought and
self-improvement techniques, as well as test-time compute scaling,
distillation, and reinforcement learning (RL) methods. Finally, we discuss the
key challenges in advancing LLM capabilities, including improving multi-step
reasoning without human supervision, overcoming limitations in chained tasks,
balancing structured prompts with flexibility, and enhancing long-context
retrieval and external tool integration.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æé™ï¼šæœ€æ–°è¿›å±•ä¸æœªè§£éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚é—®é¢˜æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬æ–‡çš„åŠ¨æœºåœ¨äºï¼Œå°½ç®¡LLMåœ¨æ¨ç†èƒ½åŠ›ä¸Šæœ‰äº†çªç ´ï¼Œä½†åœ¨å¤šæ­¥éª¤æ¨ç†å’Œè§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨åˆ†æ2023è‡³2025å¹´é—´å‘å¸ƒçš„27ä¸ªé¡¶çº§LLMæ¨¡å‹ï¼Œå¹¶æ¢è®¨è¿™äº›æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„å¢å¼ºåŠå…¶èƒŒåçš„è®­ç»ƒæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆå¯¹åŒ…æ‹¬Mistral AI Small 3 24Bã€DeepSeek-R1ã€Search-o1ã€QwQ-32Bå’Œphi-4åœ¨å†…çš„27ä¸ªLLMæ¨¡å‹è¿›è¡Œäº†å…¨é¢åˆ†æã€‚è¿™äº›æ¨¡å‹é€šè¿‡æ¨ç†æ—¶çš„æ‰©å±•ã€å¼ºåŒ–å­¦ä¹ ã€ç›‘ç£å¾®è°ƒä»¥åŠè’¸é¦ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æ–‡ç« è¯¦ç»†ä»‹ç»äº†å¤šç§è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬é€šç”¨è®­ç»ƒæ–¹æ³•ã€æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å’Œæ¶æ„åˆ›æ–°ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€é“¾å¼æ€ç»´å’Œè‡ªæˆ‘æå‡æŠ€æœ¯ï¼Œä»¥åŠæµ‹è¯•æ—¶çš„è®¡ç®—æ‰©å±•ã€è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è®¨è®ºäº†LLMæ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚åŒæ—¶ï¼Œæ–‡ç« æŒ‡å‡ºäº†è¿™äº›æ¨¡å‹åœ¨å¤šæ­¥éª¤æ¨ç†å’Œè§£å†³å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æä¾›äº†ä»¥ä¸‹å‡ ä¸ªæ–¹é¢çš„å€Ÿé‰´æ„ä¹‰ï¼š
1. å¦‚ä½•é€šè¿‡æ¨ç†æ—¶çš„åŠ¨æ€ä¿¡æ¯æ£€ç´¢å’Œä¼˜åŒ–æ¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚
2. å¦‚ä½•åˆ©ç”¨æ··åˆä¸“å®¶ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé“¾å¼æ€ç»´ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚
3. å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œè’¸é¦æ–¹æ³•æ¥ä¼˜åŒ–LLMçš„è®­ç»ƒè¿‡ç¨‹ã€‚
4. é’ˆå¯¹LLMæ¨ç†èƒ½åŠ›çš„æå‡ï¼Œæœ¬æ–‡æå‡ºäº†å…³é”®çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æ— ç›‘ç£æ¨ç†èƒ½åŠ›çš„æå‡ã€é“¾å¼ä»»åŠ¡é™åˆ¶çš„å…‹æœã€ç»“æ„åŒ–æç¤ºä¸çµæ´»æ€§çš„å¹³è¡¡ä»¥åŠé•¿ä¸Šä¸‹æ–‡æ£€ç´¢å’Œå¤–éƒ¨å·¥å…·é›†æˆçš„å¢å¼ºã€‚

## crossing-the-reward-bridge--expanding-rl-with-verifiable-rewards-across-diverse-domains
### Abstract
Reinforcement learning with verifiable rewards (RLVR) has demonstrated
significant success in enhancing mathematical reasoning and coding performance
of large language models (LLMs), especially when structured reference answers
are accessible for verification. However, its extension to broader, less
structured domains remains unexplored. In this work, we investigate the
effectiveness and scalability of RLVR across diverse real-world domains
including medicine, chemistry, psychology, economics, and education, where
structured reference answers are typically unavailable. We reveal that binary
verification judgments on broad-domain tasks exhibit high consistency across
various LLMs provided expert-written reference answers exist. Motivated by this
finding, we utilize a generative scoring technique that yields soft,
model-based reward signals to overcome limitations posed by binary
verifications, especially in free-form, unstructured answer scenarios. We
further demonstrate the feasibility of training cross-domain generative reward
models using relatively small (7B) LLMs without the need for extensive
domain-specific annotation. Through comprehensive experiments, our RLVR
framework establishes clear performance gains, significantly outperforming
state-of-the-art open-source aligned models such as Qwen2.5-72B and
DeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach
notably enhances the robustness, flexibility, and scalability of RLVR,
representing a substantial step towards practical reinforcement learning
applications in complex, noisy-label scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è·¨è¶Šå¥–åŠ±æ¡¥æ¢ï¼šåœ¨å¤šæ ·åŒ–é¢†åŸŸä¸­æ‰©å±•å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†å’Œç¼–ç æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨æœ‰ç»“æ„åŒ–å‚è€ƒç­”æ¡ˆå¯ä¾›éªŒè¯çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå°†RLVRæ‰©å±•åˆ°æ›´å¹¿æ³›ã€ç»“æ„åŒ–ç¨‹åº¦è¾ƒä½çš„åº”ç”¨é¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªæœªæ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶RLVRåœ¨åŒ»å­¦ã€åŒ–å­¦ã€å¿ƒç†å­¦ã€ç»æµå­¦å’Œæ•™è‚²ç­‰å¤šæ ·åŒ–ç°å®ä¸–ç•Œé¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¿™äº›é¢†åŸŸé€šå¸¸ç¼ºä¹ç»“æ„åŒ–çš„å‚è€ƒç­”æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å‘ç°ï¼Œåœ¨æä¾›ä¸“å®¶ç¼–å†™çš„å‚è€ƒç­”æ¡ˆçš„æƒ…å†µä¸‹ï¼Œå³ä½¿æ˜¯å¹¿æ³›é¢†åŸŸçš„ä»»åŠ¡ï¼Œä¸åŒLLMä¹‹é—´çš„äºŒå…ƒéªŒè¯åˆ¤æ–­ä¹Ÿè¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºé‡æ–°æ€è€ƒå¤šé¢†åŸŸåœºæ™¯ä¸­å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„ä¼ ç»Ÿåšæ³•æä¾›äº†ä¾æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†å…‹æœäºŒå…ƒéªŒè¯åœ¨è‡ªç”±å½¢å¼ã€éç»“æ„åŒ–ç­”æ¡ˆåœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆè¯„åˆ†æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯äº§ç”Ÿè½¯çš„ã€åŸºäºæ¨¡å‹çš„å¥–åŠ±ä¿¡å·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯æ˜äº†ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„ï¼ˆ7Bè§„æ¨¡ï¼‰LLMè®­ç»ƒè·¨é¢†åŸŸç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œæ— éœ€è¿›è¡Œå¤§é‡é¢†åŸŸç‰¹å®šæ³¨é‡Šã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡ç»¼åˆå®éªŒï¼Œæœ¬æ–‡çš„RLVRæ¡†æ¶åœ¨å¤šæ ·åŒ–ã€è‡ªç”±å½¢å¼çš„æ¨ç†ä»»åŠ¡ä¸­ç¡®ç«‹äº†æ˜ç¡®çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—è¶…è¿‡äº†æœ€æ–°çš„å¼€æºå¯¹é½æ¨¡å‹ï¼Œå¦‚Qwen2.5-72Bå’ŒDeepSeek-R1-Distill-Qwen-32Bã€‚ç‰¹åˆ«åœ°ï¼ŒåŸºäºæ¨¡å‹çš„è½¯å¥–åŠ±åœ¨éç»“æ„åŒ–ç­”æ¡ˆåœºæ™¯å’Œè¾ƒå¤§è®­ç»ƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„å¯æ‰©å±•æ€§å’Œæ›´ç¨³å¥çš„æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†RLVRçš„é²æ£’æ€§ã€çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºåœ¨å¤æ‚ã€å™ªå£°æ ‡ç­¾åœºæ™¯ä¸­å®é™…åº”ç”¨å¼ºåŒ–å­¦ä¹ è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªåŒ…å«57ä¸‡ä¸ªå¤šé¢†åŸŸè‡ªç”±å½¢å¼æ•°æ®å’Œç›¸åº”è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ•°æ®åº“ï¼Œä»¥ä¿ƒè¿›è¿™ä¸€æœ‰å‰æ™¯æ–¹å‘çš„ç ”ç©¶ã€‚

## does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model-
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has recently
demonstrated notable success in enhancing the reasoning performance of large
language models (LLMs), particularly on mathematics and programming tasks.
Similar to how traditional RL helps agents explore and learn new strategies,
RLVR is believed to enable LLMs to continuously self-improve, thus acquiring
novel reasoning abilities beyond those of the corresponding base models. In
this study we critically examine the current state of RLVR by systematically
probing the reasoning capability boundaries of RLVR-trained LLMs across various
model families, RL algorithms, and math, coding, and visual reasoning
benchmarks, using pass@k at large k values as the evaluation metric.
Surprisingly, we find that the current training setup does not elicit
fundamentally new reasoning patterns. While RLVR-trained models outperform
their base models at small k (e.g., k = 1), the base models achieve a higher
pass@k score when k is large. Coverage and perplexity analyses show that the
observed reasoning abilities originate from and are bounded by the base model.
Treating the base model as an upper bound, our quantitative analysis shows that
six popular RLVR algorithms perform similarly and remain far from optimal in
leveraging the potential of the base model. By contrast, we find that
distillation can introduce new reasoning patterns from the teacher and
genuinely expand the model's reasoning capabilities. Overall, our findings
suggest that current RLVR methods have not yet realized the potential of RL to
elicit truly novel reasoning abilities in LLMs. This highlights the need for
improved RL paradigms, such as continual scaling and multi-turn
agent-environment interaction, to unlock this potential.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç©¶å¼ºåŒ–å­¦ä¹ æ˜¯å¦çœŸæ­£æå‡äº†LLMçš„æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œè¿™ä¸»è¦å½’åŠŸäºå¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿå¸®åŠ©æ™ºèƒ½ä½“æ¢ç´¢å’Œå­¦ä¹ æ–°ç­–ç•¥ï¼Œäººä»¬æ™®éè®¤ä¸ºRLVRä¹Ÿèƒ½è®©LLMæŒç»­è‡ªæˆ‘æ”¹è¿›ï¼Œä»è€Œè·å¾—è¶…è¶ŠåŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰RLVRæ–¹æ³•æ˜¯å¦çœŸçš„å®ç°äº†è¿™ä¸€ç›®æ ‡ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæœªç»éªŒè¯çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶ä¸åŒæ¨¡å‹å®¶æ—ã€RLç®—æ³•ä»¥åŠæ•°å­¦ã€ç¼–ç¨‹å’Œè§†è§‰æ¨ç†åŸºå‡†ï¼Œä½¿ç”¨å¤§kå€¼çš„pass@kä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œæ¥æ¢æŸ¥RLVRè®­ç»ƒçš„LLMçš„æ¨ç†èƒ½åŠ›è¾¹ç•Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ç ”ç©¶å‘ç°ï¼Œå°½ç®¡RLVRè®­ç»ƒçš„æ¨¡å‹åœ¨å°kå€¼ï¼ˆå¦‚k=1ï¼‰æ—¶è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨kå€¼è¾ƒå¤§æ—¶ï¼ŒåŸºç¡€æ¨¡å‹å´èƒ½è¾¾åˆ°æ›´é«˜çš„pass@kåˆ†æ•°ã€‚è¿™è¡¨æ˜å½“å‰çš„è®­ç»ƒå¹¶æ²¡æœ‰æ¿€å‘å‡ºæœ¬è´¨ä¸Šæ–°çš„æ¨ç†æ¨¡å¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„RLVRæ¨¡å‹åœ¨æ¨ç†è¦†ç›–é¢ä¸Šæ¯”åŸºç¡€æ¨¡å‹æ›´çª„ã€‚å°½ç®¡RLVRæ¨¡å‹åœ¨æé«˜é‡‡æ ·æ•ˆç‡æ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼Œä½†å¹¶æ²¡æœ‰è®©æ¨¡å‹è§£å†³æ–°çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹æ¨ç†è·¯å¾„çš„å¤æ‚åº¦åˆ†æï¼Œå‘ç°RLVRæ¨¡å‹çš„æ¨ç†è·¯å¾„å·²ç»å­˜åœ¨äºåŸºç¡€æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒä¸­ï¼Œè¯´æ˜RLVRå¹¶æ²¡æœ‰å¼•å…¥æœ¬è´¨ä¸Šæ–°çš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„RLVRæ–¹æ³•å¹¶æ²¡æœ‰å®Œå…¨å®ç°é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€å‘LLMæ–°å‹æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚è¿™æç¤ºæˆ‘ä»¬éœ€è¦æ”¹è¿›RLèŒƒå¼ï¼Œå¦‚æŒç»­æ‰©å±•å’Œå¤šæ–¹äº¤äº’ï¼Œä»¥è§£é”è¿™ä¸€æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°è’¸é¦æ–¹æ³•èƒ½å¤Ÿä»æ•™å¸ˆæ¨¡å‹ä¸­å¼•å…¥æ–°çš„æ¨ç†æ¨¡å¼ï¼Œå¹¶çœŸæ­£æ‰©å±•æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æ–¹å‘ã€‚

## rlvr-world--training-world-models-with-reinforcement-learning
### Abstract
World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œRLVR-Worldï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡ä¸–ç•Œæ¨¡å‹æ€§èƒ½â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ä¸–ç•Œæ¨¡å‹åœ¨å„ç§æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€è§†é¢‘å’Œæ„Ÿå®˜æ•°æ®ï¼‰ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œè¿™äº›æ¨¡å‹åœ¨é¢„æµ‹çŠ¶æ€è½¬æ¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è®­ç»ƒç›®æ ‡ï¼ˆå¦‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡MLEï¼‰å¾€å¾€ä¸ä»»åŠ¡ç‰¹å®šçš„ç›®æ ‡ï¼ˆå¦‚é¢„æµ‹å‡†ç¡®æ€§æˆ–æ„ŸçŸ¥è´¨é‡ï¼‰ä¸ä¸€è‡´ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRLVR-Worldçš„ç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç›´æ¥ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹çš„ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å°†ä¸–ç•Œæ¨¡å‹ç»Ÿä¸€ä¸ºä¸€ä¸ªé€šç”¨çš„è‡ªå›å½’ç”Ÿæˆæ¡†æ¶ï¼Œå°†å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œç¼–ç ä¸ºé—®é¢˜ä»¤ç‰Œï¼Œå°†ä¸‹ä¸€ä¸ªçŠ¶æ€ç¼–ç ä¸ºå“åº”ä»¤ç‰Œï¼Œç±»ä¼¼äºè¯­è¨€æ¨¡å‹çš„æ„å»ºæ–¹å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹ï¼Œæœ¬æ–‡ç»¼åˆæ¢è®¨äº†RLVRåœ¨ä¸¤ç§ä»£è¡¨æ€§æ¨¡æ€ï¼ˆè¯­è¨€å’Œè§†é¢‘ï¼‰çš„ä¸–ç•Œæ¨¡å‹ä¸­çš„åº”ç”¨ã€‚å¯¹äºè¯­è¨€ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨é¢„æµ‹å‡†ç¡®æ€§ä½œä¸ºå¯éªŒè¯å¥–åŠ±ï¼Œæœ‰æ•ˆå¾®è°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨æ–‡æœ¬æ¸¸æˆçŠ¶æ€é¢„æµ‹å’Œç½‘é¡µçŠ¶æ€é¢„æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å¯¹äºè§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡ç›´æ¥æµ‹é‡å’Œä¼˜åŒ–è§£ç é¢„æµ‹å¸§çš„æ„ŸçŸ¥æŒ‡æ ‡ï¼Œå®ç°äº†åœ¨æœºå™¨äººæ“ä½œè½¨è¿¹é¢„æµ‹æ–¹é¢çš„æ˜¾è‘—å¢ç›Šã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLVR-Worldåœ¨è¯­è¨€å’Œè§†é¢‘ä¸–ç•Œæ¨¡å‹ä¸­å‡å–å¾—äº†å®è´¨æ€§æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ–‡æœ¬æ¸¸æˆçŠ¶æ€é¢„æµ‹ä¸­ï¼Œå‡†ç¡®åº¦æé«˜äº†30.7%ï¼›åœ¨ç½‘é¡µçŠ¶æ€é¢„æµ‹ä¸­ï¼ŒF1åˆ†æ•°æé«˜äº†15.1%ã€‚åœ¨è§†é¢‘æ–¹é¢ï¼Œé€šè¿‡ä»…å‡ ç™¾æ¬¡çš„RLVRæ¢¯åº¦æ›´æ–°ï¼Œå®ç°äº†LPIPSæŒ‡æ ‡ä¸Šçš„9.2%ç›¸å¯¹æ”¹è¿›ï¼Œè€ŒMLEè®­ç»ƒéœ€è¦æ•°ä¸‡æ¬¡è¿­ä»£æ‰èƒ½è¾¾åˆ°ç›¸ä¼¼çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•å’Œå®éªŒåˆ†æè¡¨æ˜ï¼ŒRLVRä½œä¸ºä¸€ç§é€šç”¨çš„åè®­ç»ƒèŒƒå¼ï¼Œå¯ä»¥æ˜¾è‘—æå‡ä¸–ç•Œæ¨¡å‹çš„æœ‰ç”¨æ€§ï¼Œå¹¶ä¸”æœ‰æœ›æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„ç”Ÿæˆæ¨¡å‹ã€‚å¯¹äºå¸Œæœ›ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡çš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒRLVR-Worldæä¾›äº†ä¸€ç§æ–°é¢–ä¸”æœ‰æ•ˆçš„é€”å¾„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„å®éªŒç»“æœä¹Ÿè¯æ˜äº†RLVRåœ¨å‡å°‘é‡å¤æ€§é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¿™å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹å°¤å…¶é‡è¦ã€‚

## atom-of-thoughts-for-markov-llm-test-time-scaling
### Abstract
Large Language Models (LLMs) achieve superior performance through
training-time scaling, and test-time scaling further enhances their
capabilities by conducting effective reasoning during inference. However, as
the scale of reasoning increases, existing test-time scaling methods suffer
from accumulated historical information, which not only wastes computational
resources but also interferes with effective reasoning. To address this issue,
we observe that complex reasoning can be achieved by solving a series of
independent and self-contained subquestions. These subquestions are essentially
\textit{atomic questions}, exhibiting the memoryless property similar to Markov
processes. Based on this observation, we propose Atom of Thoughts (\our), where
each state transition consists of decomposing the current question into a
dependency-based directed acyclic graph and contracting its subquestions,
forming a simplified question that maintains answer equivalence with the
original problem. This answer preservation enables the iterative
\textit{decomposition-contraction} process to naturally form a meaningful
Markov reasoning process. Furthermore, these atomic states can be seamlessly
integrated into existing test-time scaling methods, enabling \our to serve as a
plug-in enhancement for improving reasoning capabilities. Experiments across
six benchmarks demonstrate the effectiveness of \our both as a standalone
framework and a plug-in enhancement. Notably, on HotpotQA, when applied to
gpt-4o-mini, \our achieves an \textbf{80.6\%} F1 score, surpassing o3-mini by
\textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}. The code is available at
\href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œåŸå­æ€ç»´â€åŠ©åŠ›å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡æå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡è®­ç»ƒæ—¶é—´çš„æ‰©å±•å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè€Œæµ‹è¯•æ—¶é—´çš„æ‰©å±•åˆ™é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†è¿›ä¸€æ­¥å¢å¼ºäº†å…¶èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€æ¨ç†è§„æ¨¡çš„å¢åŠ ï¼Œç°æœ‰çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿‡åº¦ç»´æŠ¤å†å²ä¿¡æ¯ï¼Œè¿™ä¸ä»…æµªè´¹äº†è®¡ç®—èµ„æºï¼Œè¿˜å¹²æ‰°äº†æœ‰æ•ˆçš„æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†â€œåŸå­æ€ç»´â€ï¼ˆAtom of Thoughtsï¼Œç®€ç§°AOTï¼‰æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
è§‚å¯Ÿåˆ°å¤æ‚æ¨ç†å¯ä»¥é€šè¿‡è§£å†³ä¸€ç³»åˆ—ç‹¬ç«‹ä¸”è‡ªåŒ…å«çš„å­é—®é¢˜æ¥å®ç°ã€‚è¿™äº›å­é—®é¢˜æœ¬è´¨ä¸Šæ˜¯å…·æœ‰é©¬å°”å¯å¤«æ€§è´¨çš„åŸå­é—®é¢˜ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼ŒAOTæ¡†æ¶é€šè¿‡å°†å½“å‰é—®é¢˜åˆ†è§£ä¸ºä¾èµ–å…³ç³»çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œç„¶åå°†å…¶å­é—®é¢˜æ”¶ç¼©å½¢æˆä¸€ä¸ªæ–°çš„ç®€åŒ–é—®é¢˜ï¼Œè¿™ä¸ªç®€åŒ–é—®é¢˜ä¸åŸå§‹é—®é¢˜å…·æœ‰ç›¸åŒçš„ç­”æ¡ˆç­‰ä»·æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
AOTæ¡†æ¶ä¸­çš„æ¯ä¸ªçŠ¶æ€è½¬æ¢éƒ½åŒ…æ‹¬é—®é¢˜çš„åˆ†è§£å’Œæ”¶ç¼©è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°å¯ä»¥ç›´æ¥è§£å†³çš„åŸå­é—®é¢˜ã€‚è¿™ç§è¿­ä»£çš„è¿‡ç¨‹ç¡®ä¿äº†æ¯ä¸ªçŠ¶æ€è½¬æ¢ä»…ä¾èµ–äºå½“å‰çŠ¶æ€ï¼ŒåŒæ—¶é€æ­¥é™ä½é—®é¢˜çš„å¤æ‚æ€§ã€‚è¿™äº›åŸå­çŠ¶æ€å¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ç°æœ‰çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ä¸­ï¼Œä½¿AOTå¯ä»¥ä½œä¸ºæ’ä»¶å¢å¼ºæ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAOTæ¡†æ¶ä½œä¸ºç‹¬ç«‹æ¡†æ¶å’Œæ’ä»¶å¢å¼ºéƒ½æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨HotpotQAæ•°æ®é›†ä¸Šï¼Œå½“åº”ç”¨äºgpt-4o-miniæ—¶ï¼ŒAOTå®ç°äº†80.6%çš„F1åˆ†æ•°ï¼Œè¶…è¿‡äº†o3-miniçš„3.4%å’ŒDeepSeek-R1çš„10.6%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„AOTæ¡†æ¶æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºåŸå­é—®é¢˜ï¼Œå‡å°‘äº†åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹å†å²ä¿¡æ¯çš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†è®¡ç®—æ•ˆç‡å’Œæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒAOTæ¡†æ¶çš„è®¾è®¡å…è®¸å…¶ä½œä¸ºæ’ä»¶è½»æ¾é›†æˆåˆ°ç°æœ‰çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ä¸­ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚è®ºæ–‡çš„ä»£ç å·²ç»åœ¨GitHubä¸Šå…¬å¼€ï¼Œä¾¿äºç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚

## delving-into-rl-for-image-generation-with-cot--a-study-on-dpo-vs--grpo
### Abstract
Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±å…¥ç ”ç©¶åŸºäºå¼ºåŒ–å­¦ä¹ çš„å›¾åƒç”Ÿæˆï¼šDPOä¸GRPOçš„æ¯”è¾ƒåˆ†æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆå°±ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºLLMçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢æ‰®æ¼”äº†é‡è¦è§’è‰²ã€‚æœ¬æ–‡å…³æ³¨ä¸¤ç§çªå‡ºçš„RLç®—æ³•ï¼šç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå®ƒä»¬åœ¨æå‡LLMæ€§èƒ½æ–¹é¢å„æœ‰ä¼˜åŠ£ã€‚ç„¶è€Œï¼Œåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œå°¤å…¶æ˜¯è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­ï¼Œè¿™äº›ç®—æ³•çš„åº”ç”¨å’ŒæŒ‘æˆ˜å°šæœªå¾—åˆ°æ·±å…¥åˆ†æã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ¢è®¨DPOå’ŒGRPOåœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–æ¬¡å¯¹DPOå’ŒGRPOç®—æ³•åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼Œè¯„ä¼°äº†å®ƒä»¬åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„è¡¨ç°ï¼Œå¹¶æ·±å…¥åˆ†æäº†ä¸åŒå¥–åŠ±æ¨¡å‹å¯¹å®ƒä»¬æ€§èƒ½çš„å½±å“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸‰ç§å¸¸è§çš„æ‰©å±•ç­–ç•¥ï¼Œä»¥æå‡DPOå’ŒGRPOåœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„æ€§èƒ½ï¼Œä¸ºæ¯ç§ç®—æ³•çš„æ•ˆç‡æ‰©å±•æä¾›äº†ç‹¬ç‰¹çš„è§è§£ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- **é¢†åŸŸå†…ä¸é¢†åŸŸå¤–è¡¨ç°**ï¼šDPOåœ¨é¢†åŸŸå†…ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€ŒGRPOåœ¨é¢†åŸŸå¤–ä»»åŠ¡ä¸Šå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚
- **ä¸åŒå¥–åŠ±æ¨¡å‹çš„å½±å“**ï¼šDPOå¯¹å¥–åŠ±æ¨¡å‹çš„å˜åŠ¨æ›´ä¸ºæ•æ„Ÿï¼Œè€Œä¸€ä¸ªå…·æœ‰æ›´å¼ºæ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹å¯èƒ½æå‡RLç®—æ³•çš„æ³›åŒ–æ€§èƒ½ã€‚
- **æ‰©å±•ç­–ç•¥çš„å½±å“**ï¼šå¯¹äºGRPOï¼Œé€šè¿‡æ‰©å±•é‡‡æ ·å›¾åƒå¯ä»¥æ›´é«˜æ•ˆåœ°æå‡é¢†åŸŸå†…æ€§èƒ½ï¼›è€Œå¯¹äºDPOï¼Œè¿­ä»£è®­ç»ƒå¯ä»¥æœ€å¤§åŒ–é¢†åŸŸå†…æ€§èƒ½ï¼Œä½†è¿‡å¤šçš„è¿­ä»£å¯èƒ½ä¼šæŸå®³æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºç†è§£DPOå’ŒGRPOåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜æä¾›äº†å®è´µçš„æ•°æ®å’Œè§è§£ã€‚ç ”ç©¶è€…å¯ä»¥å€Ÿé‰´æœ¬æ–‡çš„æ–¹æ³•æ¥è®¾è®¡æ›´æœ‰æ•ˆçš„RLç®—æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±æ¨¡å‹å’Œæ‰©å±•ç­–ç•¥çš„é€‰æ‹©ä¸Šï¼Œä»¥å®ç°æ›´ç¨³å¥çš„CoTæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ä»£ç å’Œæ•°æ®é›†å·²ç»å…¬å¼€ï¼Œä¾¿äºç¤¾åŒºè¿›ä¸€æ­¥éªŒè¯å’Œæ‰©å±•ç ”ç©¶ã€‚

## right-question-is-already-half-the-answer--fully-unsupervised-llm-reasoning-incentivization
### Abstract
Existing methods to enhance the reasoning capability of large language models
predominantly rely on supervised fine-tuning (SFT) followed by reinforcement
learning (RL) on reasoning-specific data. These approaches critically depend on
external supervisions--such as labeled reasoning traces, verified golden
answers, or pre-trained reward models. In this work, we propose Entropy
Minimized Policy Optimization (\ours), which makes an early attempt at fully
unsupervised LLM reasoning incentivization. By continuously minimizing the
predictive entropy of LLMs on unlabeled questions in a latent semantic space,
\ours achieves competitive performance compared to supervised counterparts on
both mathematical and free-form natural reasoning tasks. Specifically, without
any supervised signals, \ours boosts the accuracy of Qwen2.5-Math-7B Base from
30.7\% to 48.1\% on mathematical benchmarks and improves the accuracy of
Qwen2.5-7B Base from 32.1\% to 50.1\% on MMLU-Pro. Primary experiments and
analysis are also provided to interpret the effectiveness of \ours. Code is
available at https://github.com/QingyangZhang/EMPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œè§£é”LLMæ¨ç†èƒ½åŠ›ï¼šæ— éœ€ç›‘ç£çš„å…¨è‡ªåŠ¨è·¯å¾„æ¢ç´¢â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›é€šå¸¸ä¾èµ–äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºæ¨ç†ç‰¹å®šæ•°æ®çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚è¿™äº›æ–¹æ³•é«˜åº¦ä¾èµ–å¤–éƒ¨ç›‘ç£ä¿¡å·ï¼Œä¾‹å¦‚æ ‡è®°çš„æ¨ç†è½¨è¿¹ã€éªŒè¯çš„é‡‘æ ‡å‡†ç­”æ¡ˆæˆ–é¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚è¿™ç§ä¾èµ–æ€§ä½¿å¾—æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŸ¹å…»å˜å¾—è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†æ¨ç†æ¨¡å‹çš„æ‰©å±•æ€§å’Œå¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§å®Œå…¨æ— ç›‘ç£çš„æ–¹å¼ï¼Œæ¿€åŠ±LLMçš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ç†µæœ€å°åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆEMPOï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ—©æœŸå°è¯•å®Œå…¨æ— ç›‘ç£çš„LLMæ¨ç†æ¿€åŠ±æ–¹æ³•ã€‚EMPOé€šè¿‡åœ¨æ½œåœ¨è¯­ä¹‰ç©ºé—´ä¸­æŒç»­æœ€å°åŒ–LLMå¯¹æœªæ ‡è®°é—®é¢˜çš„é¢„æµ‹ç†µï¼Œå®ç°äº†ä¸ç›‘ç£æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
EMPOæ–¹æ³•å°†è¯­ä¹‰ç†µä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„å†…åœ¨å¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼LLMçš„æ¨ç†ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œæœ¬æ–‡ç¡®è®¤äº†è¯­ä¹‰ç†µä¸æ¨¡å‹å‡†ç¡®æ€§ä¹‹é—´çš„å¼ºçƒˆè´Ÿç›¸å…³æ€§ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºç¨³å¥çš„æ— ç›‘ç£ä¼˜åŒ–ç›®æ ‡çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡å’Œè‡ªç”±å½¢å¼çš„è‡ªç„¶æ¨ç†ä»»åŠ¡ä¸­ï¼ŒEMPOæ–¹æ³•å‡æ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ²¡æœ‰ç›‘ç£ä¿¡å·çš„æƒ…å†µä¸‹ï¼ŒEMPOå°†Qwen2.5-Math-7B Baseåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®åº¦ä»30.7%æå‡åˆ°48.1%ï¼Œå°†Qwen2.5-7B Baseåœ¨MMLU-Proä¸Šçš„å‡†ç¡®åº¦ä»32.1%æå‡åˆ°50.1%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºLLMæ¨ç†èƒ½åŠ›çš„åŸ¹å…»æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå³é€šè¿‡ç†µæœ€å°åŒ–æ¥æ¿€åŠ±æ¨¡å‹å†…åœ¨çš„æ¨ç†è·¯å¾„ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å‡å°‘äº†å¯¹å¤–éƒ¨ç›‘ç£ä¿¡å·çš„ä¾èµ–ï¼Œè€Œä¸”èƒ½å¤Ÿæœ‰æ•ˆæŒ–æ˜å’Œä¼˜åŒ–æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒEMPOæ–¹æ³•åœ¨å®éªŒä¸­è¡¨ç°å‡ºçš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥æ— ç›‘ç£å­¦ä¹ çš„ç ”ç©¶æä¾›äº†å®è´µçš„å‚è€ƒã€‚

## toolrl--reward-is-all-tool-learning-needs
### Abstract
Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å·¥å…·å­¦ä¹ ï¼Œå¥–åŠ±æœºåˆ¶è‡³å…³é‡è¦ï¼šæ¢ç´¢LLMå·¥å…·ä½¿ç”¨çš„æ–°è·¯å¾„

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è·å–å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSFTåœ¨æ³›åŒ–åˆ°ä¸ç†Ÿæ‚‰æˆ–å¤æ‚çš„å·¥å…·ä½¿ç”¨åœºæ™¯ä¸Šå­˜åœ¨å›°éš¾ã€‚å°½ç®¡æœ€è¿‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯R1-likeæ¨¡å‹ï¼Œå±•ç¤ºäº†ä»¤äººé¼“èˆçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å·¥å…·ä½¿ç”¨çš„å¥–åŠ±è®¾è®¡å´é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹å·¥å…·é€‰æ‹©å’Œåº”ç”¨ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡ï¼Œä»¥æå‡LLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æ˜¯é¦–ä¸ªé’ˆå¯¹å·¥å…·é€‰æ‹©å’Œåº”ç”¨ä»»åŠ¡åœ¨RLæ¡†æ¶ä¸‹çš„å¥–åŠ±è®¾è®¡è¿›è¡Œå…¨é¢ç ”ç©¶çš„è®ºæ–‡ã€‚ä½œè€…ç³»ç»Ÿåœ°æ¢ç´¢äº†å¤šç§å¥–åŠ±ç­–ç•¥ï¼ŒåŒ…æ‹¬å¥–åŠ±ç±»å‹ã€å°ºåº¦ã€ç²’åº¦å’Œæ—¶é—´åŠ¨æ€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åŸºäºè¿™äº›æ¢ç´¢ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸåˆ™æ€§çš„å¥–åŠ±è®¾è®¡ï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMçš„è®­ç»ƒä¸­ï¼Œä½¿ç”¨äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•ã€‚è¿™ç§è®¾è®¡è€ƒè™‘äº†å·¥å…·ä½¿ç”¨çš„å¤æ‚æ€§ï¼Œæä¾›äº†ç»†ç²’åº¦çš„åé¦ˆï¼Œæœ‰åŠ©äºæ¨¡å‹çš„æœ‰æ•ˆå­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½œè€…çš„æ–¹æ³•å±•ç¤ºäº†ç¨³å¥ã€å¯æ‰©å±•å’Œç¨³å®šçš„è®­ç»ƒæ•ˆæœï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æé«˜äº†17%ï¼Œç›¸è¾ƒäºSFTæ¨¡å‹æé«˜äº†15%ã€‚è¿™äº›ç»“æœçªå‡ºäº†ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±æœºåˆ¶åœ¨å¢å¼ºLLMå·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ä¸­çš„å…³é”®ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æä¾›äº†ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- é•¿çš„æ¨ç†è½¨è¿¹å¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼Œé•¿åº¦å¥–åŠ±å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚
- åŠ¨æ€çš„å¥–åŠ±å°ºåº¦æœ‰åŠ©äºæ¨¡å‹å¹³æ»‘åœ°ä»ç®€å•è¡Œä¸ºè¿‡æ¸¡åˆ°å¤æ‚è¡Œä¸ºã€‚
- ç»†ç²’åº¦çš„å¥–åŠ±åˆ†è§£å¯ä»¥å¸¦æ¥æ›´ç¨³å®šå’Œæœ‰æ•ˆçš„å­¦ä¹ ã€‚
æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼€æºäº†æ‰€æœ‰ä»£ç ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¾¿åˆ©ã€‚

## srpo--a-cross-domain-implementation-of-large-scale-reinforcement-learning-on-llm
### Abstract
Recent advances of reasoning models, exemplified by OpenAI's o1 and
DeepSeek's R1, highlight the significant potential of Reinforcement Learning
(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).
However, replicating these advancements across diverse domains remains
challenging due to limited methodological transparency. In this work, we
present two-Staged history-Resampling Policy Optimization (SRPO), which
surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and
LiveCodeBench benchmarks. SRPO achieves this using the same base model as
DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps
required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building
upon Group Relative Policy Optimization (GRPO), we introduce two key
methodological innovations: (1) a two-stage cross-domain training paradigm
designed to balance the development of mathematical reasoning and coding
proficiency, and (2) History Resampling (HR), a technique to address
ineffective samples. Our comprehensive experiments validate the effectiveness
of our approach, offering valuable insights into scaling LLM reasoning
capabilities across diverse tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œSRPOï¼šè·¨åŸŸå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„åº”ç”¨â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œä»¥OpenAIçš„o1å’ŒDeepSeekçš„R1ä¸ºä»£è¡¨çš„æ¨ç†æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•åº”ç”¨åˆ°ä¸åŒé¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹æ–¹æ³•è®ºé€æ˜åº¦ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSRPOï¼ˆä¸¤é˜¶æ®µå†å²é‡é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼‰çš„ç®—æ³•ï¼Œä»¥å®ç°è·¨åŸŸæ¨ç†èƒ½åŠ›çš„æå‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸¤é˜¶æ®µè·¨åŸŸè®­ç»ƒèŒƒå¼
æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨å¹³è¡¡æ•°å­¦æ¨ç†å’Œç¼–ç èƒ½åŠ›çš„åŸ¹å…»ã€‚ç¬¬ä¸€é˜¶æ®µä¸»è¦è®­ç»ƒæ•°å­¦æ•°æ®ï¼Œä»¥ä¿ƒè¿›åæ€æ€§æ€è€ƒå’Œé€æ­¥é—®é¢˜è§£å†³èƒ½åŠ›çš„å½¢æˆï¼›ç¬¬äºŒé˜¶æ®µåˆ™åŠ å…¥ç¼–ç æ•°æ®ï¼Œå»ºç«‹åœ¨ç¬¬ä¸€é˜¶æ®µæ¨ç†æŠ€èƒ½çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å‘å±•ç¼–ç èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå†å²é‡é‡‡æ ·æŠ€æœ¯
ä¸ºäº†è§£å†³Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä¸­çš„é›¶ä¼˜åŠ¿ç°è±¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å†å²é‡é‡‡æ ·æŠ€æœ¯ã€‚é€šè¿‡è¿‡æ»¤æ‰å§‹ç»ˆæ­£ç¡®çš„ç­”æ¡ˆï¼Œç¡®ä¿äº†æœ‰æ„ä¹‰çš„æ¢¯åº¦æ›´æ–°ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶åŠ é€Ÿäº†æ”¶æ•›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SRPOåœ¨AIME24å’ŒLiveCodeBenchåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†DeepSeek-R1-Zero-32Bçš„æ€§èƒ½ï¼Œä½¿ç”¨ä¸DeepSeekç›¸åŒçš„åŸºæ¨¡å‹ï¼ˆQwen2.5-32Bï¼‰ï¼Œä½†ä»…éœ€è¦DeepSeek-R1-Zero-32Bè®­ç»ƒæ­¥æ•°çš„1/10ï¼Œå±•ç¤ºäº†å“è¶Šçš„æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†SRPOç®—æ³•çš„è®¾è®¡å’Œå®ç°ï¼Œä¸ºå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ åœ¨LLMä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªå€¼å¾—å€Ÿé‰´çš„æ–¹é¢ï¼š
- ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ä¸ºLLMåœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†æœ‰æ•ˆçš„è·¯å¾„ã€‚
- å†å²é‡é‡‡æ ·æŠ€æœ¯æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¯¹äºå¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢¯åº¦æ›´æ–°å’Œæ”¶æ•›å…·æœ‰é‡è¦ä½œç”¨ã€‚
- å¯¹æ•°æ®æ¸…æ´—å’Œéš¾åº¦ç­‰çº§åˆ†ç±»çš„è¯¦ç»†æè¿°ï¼Œä¸ºæ„å»ºé«˜è´¨é‡è®­ç»ƒæ•°æ®é›†æä¾›äº†å‚è€ƒã€‚
- å¯¹å¤§è§„æ¨¡RLè®­ç»ƒä¸­é‡åˆ°çš„æŒ‘æˆ˜å’Œæ€è€ƒè¡Œä¸ºçš„æ·±å…¥åˆ†æï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è¯è§è§£ã€‚

## thought-augmented-policy-optimization--bridging-external-guidance-and-internal-capabilities
### Abstract
Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
("thought patterns"). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œæ€ç»´å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼šèåˆå¤–éƒ¨æŒ‡å¯¼ä¸å†…éƒ¨èƒ½åŠ›â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒæ¨ç†æ¨¡å‹çš„æ–¹æ³•ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¾€å¾€å€¾å‘äºå°†æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒåå‘äºå¥–åŠ±æœ€å¤§åŒ–çš„è·¯å¾„ï¼Œè€Œæ²¡æœ‰å¼•å…¥å¤–éƒ¨çŸ¥è¯†ã€‚è¿™é™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œå¹¶å¯¼è‡´å…¶æ¨ç†èƒ½åŠ›è¾¹ç•Œç›¸æ¯”åŸºç¡€æ¨¡å‹æ›´çª„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†TAPOï¼ˆæ€ç»´å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¤–éƒ¨é«˜çº§æŒ‡å¯¼ï¼ˆâ€œæ€ç»´æ¨¡å¼â€ï¼‰æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
TAPOæ¡†æ¶é€šè¿‡å¼•å…¥å¤–éƒ¨é«˜çº§æ€ç»´æ¨¡å¼ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨¡å‹å†…éƒ¨çš„æ¢ç´¢å’Œå¤–éƒ¨æŒ‡å¯¼çš„åˆ©ç”¨ã€‚è¿™äº›æ€ç»´æ¨¡å¼æ˜¯ä»å…ˆå‰çš„500ä¸ªæ ·æœ¬ä¸­æŠ½è±¡å‡ºæ¥çš„ï¼Œæ¯ä¸ªæ¨¡æ¿ä»£è¡¨äº†ä¸€ç±»é—®é¢˜çš„æŠ½è±¡é—®é¢˜è§£å†³ç­–ç•¥ï¼Œä½œä¸ºæ¨ç†æŒ‡å¯¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
TAPOè®¾è®¡äº†ä¸€ä¸ªâ€œæ€ç»´åº“â€ï¼Œè¿™æ˜¯ä¸€ä¸ªå­˜å‚¨é«˜çº§æ€ç»´æ¨¡æ¿çš„é€šç”¨ä»“åº“ã€‚åœ¨GRPOé‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ªæ–°é—®é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªé€‚åº”åœ°è¯†åˆ«å¹¶åº”ç”¨æ€ç»´åº“ä¸­ç›¸å…³çš„æ€ç»´æ¨¡æ¿ï¼Œä»¥å¢å¼ºæ¨ç†è¿‡ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡çš„TAPOæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºGRPOï¼Œå¹³å‡æé«˜äº†12.0ä¸ªç‚¹ï¼ŒåŒ…æ‹¬åœ¨AIMEä¸Šæé«˜äº†99%ï¼Œåœ¨AMCä¸Šæé«˜äº†41%ï¼Œåœ¨Minerva Mathä¸Šæé«˜äº†17%ã€‚TAPOæ–¹æ³•åœ¨å„ç§æ¨¡å‹è§„æ¨¡å’Œæ¶æ„ä¸Šéƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°åˆ†å¸ƒå¤–æ¨ç†ä»»åŠ¡ã€‚è¿›ä¸€æ­¥çš„åˆ†æç¡®è®¤ï¼Œå¼•å…¥å¤–éƒ¨æŒ‡å¯¼ä¸ä»…å¢å¼ºäº†æ¨¡å‹è¾“å‡ºçš„å¯è§£é‡Šæ€§ï¼Œä¹Ÿæé«˜äº†è¾“å‡ºçš„å¯è¯»æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„TAPOæ¡†æ¶ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†æ¨¡å‹ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ç»“åˆå¤–éƒ¨é«˜çº§æŒ‡å¯¼å’Œå†…éƒ¨æ¨ç†èƒ½åŠ›ã€‚TAPOçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†å…¶æ³›åŒ–èƒ½åŠ›å’Œè¾“å‡ºè´¨é‡ã€‚æ­¤å¤–ï¼ŒTAPOæ¡†æ¶çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯¹äºå…¶ä»–éœ€è¦æ¨ç†èƒ½åŠ›çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¹Ÿå…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## think-when-you-need--self-adaptive-chain-of-thought-learning
### Abstract
Chain of Thought (CoT) reasoning enhances language models' performance but
often leads to inefficient "overthinking" on simple problems. We identify that
existing approaches directly penalizing reasoning length fail to account for
varying problem complexity. Our approach constructs rewards through length and
quality comparisons, guided by theoretical assumptions that jointly enhance
solution correctness with conciseness. Moreover, we further demonstrate our
method to fuzzy tasks where ground truth is unavailable. Experiments across
multiple reasoning benchmarks demonstrate that our method maintains accuracy
while generating significantly more concise explanations, effectively teaching
models to "think when needed."
### ğŸŒŸ è®ºæ–‡è§£è¯» | "æ·±åº¦å­¦ä¹ æ–°ç­–ç•¥ï¼šæŒ‰éœ€æ€è€ƒçš„é“¾å¼æ¨ç†å­¦ä¹ "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ¨ç†æ¨¡å‹å¦‚OpenAI o1å’ŒDeepseek-R1çš„å¹¿æ³›åº”ç”¨ï¼Œé“¾å¼æ¨ç†ï¼ˆChain of Thought, CoTï¼‰è¢«è¯æ˜å¯ä»¥æ˜¾è‘—æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†æ–¹å¼åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶å¾€å¾€ä¼šå‡ºç°è¿‡åº¦æ¨ç†çš„æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡ç›´æ¥å¯¹æ¨ç†é•¿åº¦è¿›è¡Œæƒ©ç½šæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™ç§æ–¹æ³•å¯¹è¶…å‚æ•°æ•æ„Ÿï¼Œä¸”ç¼ºä¹æ™®éé€‚ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ˜ç¡®ç­”æ¡ˆçš„æ¨¡ç³Šä»»åŠ¡ä¸­ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸æ˜¯ç›´æ¥å¯¹å“åº”é•¿åº¦è¿›è¡Œæƒ©ç½šï¼Œè€Œæ˜¯é€šè¿‡æ ·æœ¬é—´çš„é•¿åº¦å’Œè´¨é‡æ¯”è¾ƒæ¥æ„å»ºå¥–åŠ±ã€‚è¿™ç§æ–¹æ³•åŸºäºç†è®ºå‡è®¾ï¼Œæ—¨åœ¨åŒæ—¶æå‡è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§å’Œç®€æ´æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è¯¥æ–¹æ³•è‡ªç„¶åœ°æ‰©å±•åˆ°äº†å…·æœ‰æ˜ç¡®ç­”æ¡ˆçš„å¯éªŒè¯ä»»åŠ¡å’Œéœ€è¦ä¸»è§‚è¯„ä»·çš„æ¨¡ç³Šä»»åŠ¡ã€‚é€šè¿‡æˆå¯¹æ¯”è¾ƒæ‰€æœ‰å¯èƒ½çš„æ ·æœ¬ç»„åˆï¼Œå¹¶ä¸ºæ¯å¯¹æ ·æœ¬è®¡ç®—æˆå¯¹å¥–åŠ±ï¼Œæœ€ç»ˆé€šè¿‡èšåˆæ¯ä¸ªæ ·æœ¬æ”¶åˆ°çš„æ‰€æœ‰æˆå¯¹å¥–åŠ±æ¥ç¡®å®šæ¯ä¸ªæ ·æœ¬çš„æœ€ç»ˆå¥–åŠ±ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´åŠ ç®€æ´çš„è§£é‡Šï¼Œæœ‰æ•ˆåœ°æ•™ä¼šæ¨¡å‹åœ¨éœ€è¦æ—¶æ‰è¿›è¡Œæ€è€ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•è®ºä¸ºå¤„ç†æ¨ç†æ¨¡å‹ä¸­çš„æ•ˆç‡é—®é¢˜æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¨¡ç³Šä»»åŠ¡æ—¶ï¼Œæä¾›äº†ä¸€ç§åœ¨æ²¡æœ‰æ˜ç¡®çœŸç›¸çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆå­¦ä¹ çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„è®¾è®¡åŸºäºæ¸…æ™°çš„ç†è®ºå‡è®¾ï¼Œæ˜“äºæ³›åŒ–å’Œä¸å…¶ä»–å¥–åŠ±ç»“æ„å…¼å®¹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ¢ç´¢ç©ºé—´ã€‚

## reinforced-mllm--a-survey-on-rl-based-reasoning-in-multimodal-large-language-models
### Abstract
The application of reinforcement learning (RL) to enhance the reasoning
capabilities of Multimodal Large Language Models (MLLMs) constitutes a rapidly
advancing research area. While MLLMs extend Large Language Models (LLMs) to
handle diverse modalities such as vision, audio, and video, enabling robust
reasoning across multimodal inputs remains challenging. This paper provides a
systematic review of recent advances in RL-based reasoning for MLLMs, covering
key algorithmic designs, reward mechanism innovations, and practical
applications. We highlight two main RL paradigms, value-model-free and
value-model-based methods, and analyze how RL enhances reasoning abilities by
optimizing reasoning trajectories and aligning multimodal information.
Additionally, we provide an extensive overview of benchmark datasets,
evaluation protocols, and current limitations, and propose future research
directions to address challenges such as sparse rewards, inefficient
cross-modal reasoning, and real-world deployment constraints. Our goal is to
provide a comprehensive and structured guide to RL-based multimodal reasoning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "å¼ºåŒ–å­¦ä¹ èµ‹èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼šæ¨ç†èƒ½åŠ›æå‡ç»¼è¿°"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸè¿æ¥äº†å‰æ‰€æœªæœ‰çš„æ–°æ—¶ä»£ï¼ŒLLM å±•ç°å‡ºäº†å“è¶Šçš„æŒ‡ä»¤éµå¾ªå’Œå°‘é‡æ ·æœ¬å­¦ä¹ çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¦å®ç°äººç±»çº§åˆ«çš„æ™ºèƒ½ï¼Œä¸ä»…éœ€è¦è¶…è¶ŠåŸºæœ¬çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿˜éœ€è¦å‘å±•å¤æ‚çš„è®¤çŸ¥æ¨ç†æŠ€èƒ½ï¼Œè¿™äº›æŠ€èƒ½èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡ç†è§£å’Œè‡ªæˆ‘ä¿®æ­£è¿›è¡Œè¿­ä»£æ¨ç†ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†å¤šç§æ¨¡æ€è¾“å…¥æ—¶çš„ç¨³å¥æ¨ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæä¾›äº†ä¸€ç¯‡å…³äºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ MLLM æ¨ç†èƒ½åŠ›çš„ç»¼è¿°ï¼Œç³»ç»Ÿåœ°å›é¡¾äº†æœ€æ–°çš„ç ”ç©¶è¿›å±•ã€å…³é”®ç®—æ³•è®¾è®¡ã€å¥–åŠ±æœºåˆ¶åˆ›æ–°ä»¥åŠå®é™…åº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆä»‹ç»äº†ä¸¤ç§ä¸»è¦çš„ RL æ–¹æ³•ï¼šä»·å€¼æ¨¡å‹æ— å…³ï¼ˆvalue-model-freeï¼‰å’Œä»·å€¼æ¨¡å‹åŸºäºï¼ˆvalue-model-basedï¼‰æ–¹æ³•ï¼Œå¹¶åˆ†æäº†å®ƒä»¬å¦‚ä½•é€šè¿‡ä¼˜åŒ–æ¨ç†è½¨è¿¹å’Œå¯¹é½å¤šæ¨¡æ€ä¿¡æ¯æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡è¯¦ç»†è°ƒç ”äº†åŸºäº RL çš„ MLLM æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬ç®—æ³•æ¡†æ¶ã€å¥–åŠ±å‡½æ•°è®¾è®¡ï¼ˆåŒ…æ‹¬å‡†ç¡®æ€§å¯¼å‘å’Œç»“æ„å¯¼å‘çš„æ–¹æ³•ï¼‰ï¼Œä»¥åŠå¤šæ¨¡æ€ä¿¡æ¯èåˆç­–ç•¥ã€‚è¿™äº›ç»„ä»¶å¦‚ä½•å…±åŒè§£å†³å¤šæ¨¡æ€æ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ä¹Ÿå¾—åˆ°äº†å¼ºè°ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æä¾›äº†å¯¹åŸºå‡†æ•°æ®é›†ã€è¯„ä¼°åè®®ä»¥åŠæ¶µç›–æ•°å­¦ã€ç§‘å­¦ã€ç©ºé—´å’ŒåŸºäºäº¤äº’çš„é¢†åŸŸçš„åŸºå‡†çš„å¹¿æ³›æ¦‚è¿°ï¼Œä½†å…·ä½“çš„å®éªŒç»“æœåœ¨æ‘˜è¦ä¸­æœªè¯¦ç»†è¯´æ˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå…¨é¢è€Œç³»ç»Ÿçš„æŒ‡å—ï¼Œä»¥è¯†åˆ«åœ¨ MLLM æ¨ç†é¢†åŸŸå¿«é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹åˆé€‚çš„æ–¹æ³•ï¼Œä»è€Œä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥åˆ›æ–°å’Œè¿›æ­¥ã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªå¯å€Ÿé‰´ä¹‹å¤„ï¼š
- å¯¹äº RL åœ¨ LLM å’Œ MLLM ä¸­çš„æ ¸å¿ƒè®¾è®¡è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œè®¨è®ºäº†è®­ç»ƒæ•ˆç‡ã€ç¨³å®šæ€§å’Œæ€§èƒ½çš„åˆ›æ–°ï¼Œä»¥åŠä¼˜åŒ–æœºä¼šã€‚
- å¯¹åŸºäº RL çš„æ¨ç†æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬ç®—æ³•æ¡†æ¶ã€å¥–åŠ±å‡½æ•°è®¾è®¡å’Œå¤šæ¨¡æ€ä¿¡æ¯èåˆç­–ç•¥ã€‚
- æä¾›äº†å…³äºæ¨ç†ä»»åŠ¡çš„æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•çš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬æ•°æ®æ¥æºå’Œæ³¨é‡Šæ„å»ºæµç¨‹ã€‚
- æŒ‡å‡ºäº†å½“å‰æœªè§£å†³çš„æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–å¥–åŠ±ã€ä½æ•ˆçš„è½¨è¿¹å’Œè·¨æ¨¡æ€åè°ƒï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬åˆ†å±‚å¥–åŠ±å»ºæ¨¡ã€è§†è§‰å¼•å¯¼çš„ CoT å’Œé€‚ç”¨äºå®é™…åº”ç”¨çš„è½»é‡çº§ RL æ¡†æ¶ã€‚

## echoink-r1--exploring-audio-visual-reasoning-in-multimodal-llms-via-reinforcement-learning
### Abstract
Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EchoInk-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„éŸ³è§†è§‰æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ„ŸçŸ¥æ–¹é¢çš„è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–è·¨æ¨¡æ€æ¨ç†æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•´åˆéŸ³é¢‘å’Œè§†è§‰ä¿¡å·æ—¶ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºEchoInk-R1çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥å¢å¼ºMLLMsä¸­çš„éŸ³è§†è§‰æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
EchoInk-R1æ¡†æ¶å»ºç«‹åœ¨Qwen2.5-Omni-7Bæ¨¡å‹ä¹‹ä¸Šï¼Œå¹¶é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¯¥æ¡†æ¶ä¸“é—¨é’ˆå¯¹åŒæ­¥éŸ³é¢‘-å›¾åƒå¯¹çš„å¤šé¡¹é€‰æ‹©é¢˜å›ç­”ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†æ”¯æŒè¿™ä¸€æ¡†æ¶ï¼Œæœ¬æ–‡æ„å»ºäº†AVQA-R1-6Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å°†éŸ³é¢‘-å›¾åƒè¾“å…¥ä¸OmniInstruct-v1æ•°æ®é›†ä¸­çš„å¤šé¡¹é€‰æ‹©é¢˜ç›¸ç»“åˆã€‚è¿™ä¸€æ•°æ®é›†ä¸“é—¨ä¸ºéŸ³è§†è§‰æ¨ç†ä»»åŠ¡è®¾è®¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEchoInk-R1-7Båœ¨AVQA-R1-6KéªŒè¯é›†ä¸Šå–å¾—äº†85.77%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹Qwen2.5-Omni-7Bçš„80.53%ï¼Œä¸”ä»…ä½¿ç”¨äº†562æ­¥å¼ºåŒ–å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒEchoInk-R1è¿˜å±•ç¤ºäº†åœ¨é¢ä¸´æ¨¡ç³Šå¤šæ¨¡æ€è¾“å…¥æ—¶ï¼Œé‡æ–°å®¡è§†åˆå§‹è§£é‡Šå¹¶ refined å“åº”çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯è½»é‡çº§çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¹Ÿèƒ½æ˜¾è‘—å¢å¼ºMLLMsçš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚EchoInk-R1æ˜¯ç¬¬ä¸€ä¸ªå°†éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ç»Ÿä¸€äºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œç”¨äºé€šç”¨å¼€æ”¾ä¸–ç•Œæ¨ç†çš„æ¡†æ¶ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä¸­æå‡ºçš„â€œaha momentsâ€ç°è±¡ï¼Œå³æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªæˆ‘ä¿®æ­£æ¨ç†çš„èƒ½åŠ›ï¼Œä¸ºç†è§£æ¨¡å‹å¦‚ä½•è¿›è¡Œè·¨æ¨¡æ€æ¨ç†æä¾›äº†æ–°çš„è§†è§’ã€‚è®ºæ–‡çš„ä»£ç å’Œæ•°æ®å·²å…¬å¼€ï¼Œæœ‰åŠ©äºè¿›ä¸€æ­¥çš„ç ”ç©¶å’ŒéªŒè¯ã€‚

## think-or-not--selective-reasoning-via-reinforcement-learning-for-vision-language-models
### Abstract
Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across diverse vision-language tasks-covering
a range of reasoning difficulties under both 3B and 7B models-consistently
reveal that the model progressively learns to bypass unnecessary reasoning
steps as training advances. These findings shed light on the path toward
human-like reasoning patterns in reinforcement learning approaches. Our code is
available at https://github.com/kokolerk/TON.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "æ™ºèƒ½æ¨ç†æ–°ç­–ç•¥ï¼šå­¦ä¼šé€‰æ‹©ä½•æ—¶æ€è€ƒ"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„åè®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¦‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¾€å¾€å¯¼è‡´ä¸å¿…è¦çš„é•¿æ¨ç†è¿‡ç¨‹ï¼Œå¢åŠ äº†tokençš„ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡çš„åŠ¨æœºåœ¨äºï¼Œæ¨¡ä»¿äººç±»çš„æ€è€ƒè¿‡ç¨‹â€”â€”å¯¹äºç®€å•é—®é¢˜è·³è¿‡æ¨ç†ï¼Œå¯¹äºå›°éš¾é—®é¢˜ä»”ç»†æ€è€ƒâ€”â€”æ¥æé«˜VLMsçš„æ¨ç†æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æå‡ºäº†TONï¼ˆThink or Notï¼‰è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå’ŒGRPOé˜¶æ®µã€‚åœ¨SFTé˜¶æ®µï¼Œå¼•å…¥äº†â€œæ€ç»´dropoutâ€æ“ä½œï¼Œéšæœºå°†æ¨ç†è½¨è¿¹æ›¿æ¢ä¸ºç©ºæ€ç»´ï¼Œä»è€Œå¼•å…¥äº†â€œæ€è€ƒä¸å¦â€æ ¼å¼ï¼Œä¸ºé€‰æ‹©æ€§æ¨ç†æä¾›äº†å†·å¯åŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åœ¨GRPOé˜¶æ®µï¼Œæ¨¡å‹å¯ä»¥è‡ªç”±æ¢ç´¢ä½•æ—¶æ€è€ƒæˆ–è·³è¿‡æ€è€ƒï¼ŒåŒæ—¶æœ€å¤§åŒ–ä»»åŠ¡æ„ŸçŸ¥çš„äº§å‡ºå¥–åŠ±ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§é€‚åº”æ€§åœ°å­¦ä¹ è·³è¿‡ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTONèƒ½å¤Ÿå°†å®Œæˆé•¿åº¦å‡å°‘å¤šè¾¾90%ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¿˜èƒ½æé«˜æ€§èƒ½ã€‚åœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„è¯„ä¼°ä¸€è‡´è¡¨æ˜ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¨¡å‹é€æ­¥å­¦ä¼šè·³è¿‡ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†è¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’ï¼Œå³â€œä½•æ—¶æ€è€ƒâ€è€Œä¸æ˜¯â€œå¦‚ä½•æ€è€ƒâ€ã€‚é€šè¿‡è®©æ¨¡å‹é¦–å…ˆå†³å®šæ˜¯å¦éœ€è¦æ¨ç†ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡å’Œå‡å°‘è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¿æŒæˆ–æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†tokençš„ä½¿ç”¨ï¼Œå¯¹äºèµ„æºæœ‰é™çš„åœºæ™¯å°¤å…¶æœ‰ä»·å€¼ã€‚è®ºæ–‡ä¸­çš„â€œæ€ç»´dropoutâ€æ“ä½œå’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä¸ºæœªæ¥è§†è§‰è¯­è¨€æ¨¡å‹çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## unlocking-the-potential-of-difficulty-prior-in-rl-based-multimodal-reasoning
### Abstract
In this work, we investigate how explicitly modeling problem's difficulty
prior information shapes the effectiveness of reinforcement learning based
fine-tuning for multimodal reasoning. Our exploration mainly comprises of
following three perspective: First, through offline data curation, we analyze
the U-shaped difficulty distribution of two given datasets using the base model
by multi-round sampling, and then filter out prompts that are either too simple
or extremely difficult to provide meaningful gradients and perform subsequent
two-stage training. Second, we implement an online advantage differentiation,
computing group-wise empirical accuracy as a difficulty proxy to adaptively
reweight advantages estimation, providing stronger learning signals for more
challenging problems. Finally, we introduce difficulty hints as explicit
prompts for more complex samples in the second training stage, encouraging the
model to calibrate its reasoning depth and perform reflective validation
checks. Our comprehensive approach demonstrates significant performances across
various multi-modal mathematical reasoning benchmarks with only 2K+0.6K
two-stage training data.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£é”å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„éš¾é¢˜ä¼˜å…ˆæ½œåŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„è¡¨ç°è¶Šæ¥è¶Šå‡ºè‰²ï¼Œå¦‚ä½•æé«˜è¿™äº›æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§æˆä¸ºç ”ç©¶çš„çƒ­ç‚¹ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€é¢ä¸´ç€æ•°æ®éš¾åº¦æ··åˆã€å¥–åŠ±æœºåˆ¶å•ä¸€å’Œå¹³æ»‘éš¾åº¦æ„è¯†ç¼ºå¤±ç­‰é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•é€šè¿‡æ˜¾å¼å»ºæ¨¡é—®é¢˜çš„éš¾åº¦å…ˆéªŒä¿¡æ¯æ¥æå‡åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆé€šè¿‡ç¦»çº¿æ•°æ®ç­›é€‰ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹å¯¹ç»™å®šæ•°æ®é›†è¿›è¡Œå¤šè½®é‡‡æ ·ï¼Œåˆ†æå‡ºUå½¢çš„éš¾åº¦åˆ†å¸ƒï¼Œå¹¶è¿‡æ»¤æ‰è¿‡äºç®€å•æˆ–è¿‡äºå›°éš¾çš„é—®é¢˜ï¼Œä»è€Œè¿›è¡Œåç»­çš„ä¸¤é˜¶æ®µè®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åœ¨åœ¨çº¿è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡å®ç°äº†ä¼˜åŠ¿å·®å¼‚åŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—ç»„å†…ç»éªŒå‡†ç¡®ç‡ä½œä¸ºéš¾åº¦ä»£ç†ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´ä¼˜åŠ¿ä¼°è®¡ï¼Œä¸ºæ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜æä¾›æ›´å¼ºçš„å­¦ä¹ ä¿¡å·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æœ¬æ–‡è¿˜å¼•å…¥äº†éš¾åº¦æç¤ºï¼Œä½œä¸ºç¬¬äºŒé˜¶æ®µè®­ç»ƒä¸­å¯¹æ›´å¤æ‚æ ·æœ¬çš„æ˜¾å¼æç¤ºï¼Œé¼“åŠ±æ¨¡å‹è°ƒæ•´å…¶æ¨ç†æ·±åº¦å¹¶è¿›è¡Œåæ€æ€§éªŒè¯æ£€æŸ¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡çš„æ–¹æ³•åœ¨å„ç§å¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ï¼Œä»…ä½¿ç”¨2K+0.6Kä¸¤é˜¶æ®µè®­ç»ƒæ•°æ®å°±å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶ï¼Œå……åˆ†è€ƒè™‘äº†é—®é¢˜çš„éš¾åº¦åˆ†å¸ƒï¼Œé€šè¿‡ç¦»çº¿ç­›é€‰å’Œåœ¨çº¿è°ƒæ•´ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå¼•å…¥éš¾åº¦æç¤ºçš„åˆ›æ–°æ€è·¯ï¼Œä¸ºæ¨¡å‹æä¾›äº†æ›´æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ï¼Œå¯¹äºæå‡å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## not-all-thoughts-are-generated-equal--efficient-llm-reasoning-via-multi-turn-reinforcement-learning
### Abstract
Compressing long chain-of-thought (CoT) from large language models (LLMs) is
an emerging strategy to improve the reasoning efficiency of LLMs. Despite its
promising benefits, existing studies equally compress all thoughts within a
long CoT, hindering more concise and effective reasoning. To this end, we first
investigate the importance of different thoughts by examining their
effectiveness and efficiency in contributing to reasoning through automatic
long CoT chunking and Monte Carlo rollouts. Building upon the insights, we
propose a theoretically bounded metric to jointly measure the effectiveness and
efficiency of different thoughts. We then propose Long$\otimes$Short, an
efficient reasoning framework that enables two LLMs to collaboratively solve
the problem: a long-thought LLM for more effectively generating important
thoughts, while a short-thought LLM for efficiently generating remaining
thoughts. Specifically, we begin by synthesizing a small amount of cold-start
data to fine-tune LLMs for long-thought and short-thought reasoning styles,
respectively. Furthermore, we propose a synergizing-oriented multi-turn
reinforcement learning, focusing on the model self-evolution and collaboration
between long-thought and short-thought LLMs. Experimental results show that our
method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance
compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while
reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and
GPQA Diamond benchmarks. Our data and code are available at
https://github.com/usail-hkust/LongShort.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œæ€ç»´ä¸ç­‰åŒï¼šé€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ¨ç†â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œé•¿é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›é€æ¸æˆä¸ºå…¶å…³é”®ç‰¹æ€§ã€‚ç„¶è€Œï¼Œé•¿CoTæ¨ç†çš„è¿‡åº¦tokené•¿åº¦é—®é¢˜é™åˆ¶äº†å…¶æ•ˆç‡å’Œå®ç”¨æ€§ã€‚ç°æœ‰ç ”ç©¶åœ¨å‹ç¼©CoTé•¿åº¦æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å¹³ç­‰åœ°å‹ç¼©æ‰€æœ‰æ€ç»´æ­¥éª¤ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä¸åŒæ€ç»´æ­¥éª¤çš„é‡è¦æ€§å·®å¼‚ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆæ¨ç†æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªæ€ç»´åˆ†ææ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨å°†é•¿CoTåˆ†è§£ä¸ºå¤šä¸ªæ€ç»´å—ï¼Œå¹¶é‡åŒ–å®ƒä»¬å¯¹æ¨ç†è¿‡ç¨‹çš„è´¡çŒ®ã€‚è¿™ä¸€æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šè‡ªåŠ¨é•¿CoTåˆ†å—ã€æ€ç»´å›æ»šå’Œè”åˆæµ‹é‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åŸºäºä¸Šè¿°åˆ†æï¼Œæœ¬æ–‡æå‡ºäº†LongâŠ—Shortæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªLLMååŒè§£å†³é—®é¢˜ï¼šä¸€ä¸ªé•¿æ€ç»´LLMç”Ÿæˆé‡è¦æ€ç»´ï¼Œè€Œä¸€ä¸ªçŸ­æ€ç»´LLMç”Ÿæˆå‰©ä½™æ€ç»´ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡åˆæˆå°‘é‡å†·å¯åŠ¨æ•°æ®æ¥å¾®è°ƒLLMï¼Œä½¿å…¶é€‚åº”é•¿æ€ç»´å’ŒçŸ­æ€ç»´æ¨ç†é£æ ¼ã€‚ç„¶åï¼Œæå‡ºäº†ä¸€ç§é¢å‘ååŒçš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œä¸“æ³¨äºæ¨¡å‹çš„è‡ªæˆ‘è¿›åŒ–å’Œé•¿æ€ç»´ä¸çŸ­æ€ç»´LLMä¹‹é—´çš„åä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLongâŠ—Shortæ¡†æ¶ä½¿Qwen2.5-7Bå’ŒLlama3.1-8Båœ¨MATH500ã€AIME24/25ã€AMC23å’ŒGPQA DiamondåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸å®ƒä»¬çš„è’¸é¦ç‰ˆæœ¬ï¼ˆDeepSeek-R1-Distill-Qwen-7Bå’ŒDeepSeek-R1-Distill-Llama-8Bï¼‰ç›¸æ¯”ï¼Œå®ç°äº†ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†tokené•¿åº¦å‡å°‘äº†è¶…è¿‡80%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ä»…ä¸ºé•¿CoTå‹ç¼©æä¾›äº†ç†è®ºåŸºç¡€ï¼Œè¿˜é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å…¶åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
- è‡ªåŠ¨åŒ–æ€ç»´åˆ†ææ¡†æ¶ï¼Œä¸ºé‡åŒ–æ€ç»´è´¡çŒ®æä¾›äº†æ–°æ–¹æ³•ã€‚
- LongâŠ—Shortæ¡†æ¶ï¼Œé€šè¿‡é•¿æ€ç»´å’ŒçŸ­æ€ç»´çš„ååŒå·¥ä½œï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚
- é¢å‘ååŒçš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œä¸ºæ¨¡å‹è‡ªæˆ‘è¿›åŒ–å’Œåä½œæä¾›äº†æ–°æ€è·¯ã€‚
è¿™äº›æˆæœå¯¹äºæå‡LLMçš„æ¨ç†æ•ˆç‡å’Œå®ç”¨æ€§å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## rl-in-name-only--analyzing-the-structural-assumptions-in-rl-post-training-for-llms
### Abstract
Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦è§£æï¼šLLMåè®­ç»ƒä¸­çš„RLæ–¹æ³•æ˜¯å¦åä¸å‰¯å®ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢ã€‚å…¶ä¸­ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæ–¹æ³•ï¼Œå¦‚ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œæœ¬æ–‡ä½œè€…å¯¹è¿™ç±»æ–¹æ³•èƒŒåçš„ç»“æ„å‡è®¾è¿›è¡Œäº†æ‰¹åˆ¤æ€§åˆ†æï¼ŒæŒ‡å‡ºè¿™äº›å‡è®¾å¯èƒ½å¯¼è‡´RLåœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœå¹¶ä¸å¦‚é¢„æœŸã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆæ­ç¤ºäº†åœ¨å°†LLMè®­ç»ƒå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ—¶æ‰€åšçš„æµè¡Œç»“æ„å‡è®¾ã€‚è¿™äº›å‡è®¾åŒ…æ‹¬ï¼šå°†MDPçŠ¶æ€è§†ä¸ºåŠ¨ä½œçš„ç®€å•è¿æ¥ï¼Œå…¶ä¸­çŠ¶æ€æˆä¸ºä¸Šä¸‹æ–‡çª—å£ï¼ŒåŠ¨ä½œæˆä¸ºLLMä¸­çš„æ ‡è®°ï¼›ä»¥åŠå°†å¥–åŠ±å‡åŒ€åˆ†é…åˆ°çŠ¶æ€-åŠ¨ä½œè½¨è¿¹ä¸Šã€‚ä½œè€…é€šè¿‡åˆ†æè¡¨æ˜ï¼Œè¿™äº›ç®€åŒ–çš„å‡è®¾å®é™…ä¸Šä½¿å¾—è¿™ç§æ–¹æ³•ç­‰æ•ˆäºç»“æœé©±åŠ¨çš„ç›‘ç£å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä½œè€…è¿›ä¸€æ­¥é€šè¿‡å®éªŒéªŒè¯äº†ï¼Œåœ¨GSM8Kå’ŒCountdownåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨Qwen-2.5åŸºæ¨¡å‹çš„è¿­ä»£ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ï¼Œç»“åˆæ­£è´Ÿæ ·æœ¬ï¼Œå¯ä»¥è¾¾åˆ°ä¸GRPOåŸºè®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æŒ‡å‡ºï¼Œè¿™äº›ç»“æ„å‡è®¾é—´æ¥æ¿€åŠ±RLç”Ÿæˆæ›´é•¿çš„ä¸­é—´æ ‡è®°åºåˆ—ï¼Œè¿™è¿›ä¸€æ­¥å¼ºåŒ–äº†â€œRLç”Ÿæˆæ›´é•¿çš„æ€è€ƒè½¨è¿¹â€çš„è¯´æ³•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå‡åŒ€åˆ†é…ç»ˆç«¯å¥–åŠ±çš„å‡è®¾ç›´æ¥å¯¼è‡´äº†è®­ç»ƒè¿‡ç¨‹ä¸­å“åº”é•¿åº¦çš„å¢åŠ ã€‚ä¸DeepSeek-R1è®ºæ–‡ä¸­æå‡ºçš„å¢åŠ å“åº”é•¿åº¦å½’å› äºè®¡ç®—è§„æ¨¡ã€è‡ªæˆ‘åæ€ã€è‡ªæˆ‘éªŒè¯å’Œå›æº¯ç­‰è§‚ç‚¹ä¸åŒï¼Œæœ¬æ–‡ä½œè€…è¯æ˜äº†å“åº”å»¶é•¿çš„ä¸»å› æ˜¯å‡åŒ€çš„ä¿¡ç”¨åˆ†é…ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡å¯¹LLMåè®­ç»ƒä¸­RLæ–¹æ³•çš„ç»“æ„å‡è®¾è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†è¿™äº›å‡è®¾å¯èƒ½å¯¼è‡´RLæ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ã€‚ä½œè€…å»ºè®®è€ƒè™‘æ›¿ä»£çš„MDPå…¬å¼åŒ–æ–¹æ³•ï¼Œå¹¶é‡æ–°å®¡è§†ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä»¥æ›´ç®€å•çš„æ–¹å¼è¾¾åˆ°ç›¸ä¼¼ç”šè‡³æ›´å¥½çš„æ•ˆç‡å’Œæ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶å¯¹äºç†è§£å’Œæ”¹è¿›LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’å’Œå¯ç¤ºã€‚

## cec-zero--chinese-error-correction-solution-based-on-llm
### Abstract
Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "CEC-Zeroï¼šå¼€å¯ä¸­æ–‡æ–‡æœ¬çº é”™æ–°ç¯‡ç« "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é£é€Ÿå‘å±•ï¼Œå…¶åœ¨ä¸­æ–‡æ–‡æœ¬å¤„ç†æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œå°½ç®¡LLMåœ¨ä¸­æ–‡æ–‡æœ¬çº é”™æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¾ç„¶å­˜åœ¨å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†CEC-Zeroï¼Œä¸€ä¸ªåŸºäºLLMå’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡å‹è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£æˆ–è¾…åŠ©æ¨¡å‹ï¼Œè‡ªä¸»å­¦ä¹ çº é”™ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºçš„CEC-Zeroæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆLLMçš„ç”Ÿæˆèƒ½åŠ›å’ŒRLçš„è‡ªä¸»å­¦ä¹ èƒ½åŠ›ï¼Œå®ç°äº†æ— éœ€æ ‡æ³¨æ•°æ®çš„è‡ªæˆ‘çº é”™ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®å’Œè¾…åŠ©æ¨¡å‹çš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
CEC-Zeroæ¡†æ¶é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡é‡å¤é‡‡æ ·ç­–ç•¥å’ŒåŸºäºå¤šæ•°æŠ•ç¥¨æœºåˆ¶çš„ä¼ªå¥–åŠ±ä¿¡å·ï¼Œè¿›è¡Œç¨³å®šçš„å¼ºåŒ–å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºæ–‡æœ¬çº é”™è¿™ç±»ç­”æ¡ˆä¸å”¯ä¸€çš„åœºæ™¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªä¸­æ–‡æ–‡æœ¬çº é”™æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œç»è¿‡RLå¢å¼ºçš„LLMåœ¨å‡†ç¡®æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šå‡å–å¾—äº†å·¥ä¸šçº§å¯ç”¨çš„æ°´å¹³ï¼Œä¸ºä¸­æ–‡NLPåº”ç”¨ä¸­çš„å¯é æ€§ä¼˜åŒ–æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
CEC-Zeroæ¡†æ¶ä¸ºä¸­æ–‡æ–‡æœ¬çº é”™é¢†åŸŸæä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œå…¶åŸºäºLLMå’ŒRLçš„ç»“åˆï¼Œä¸ºæ¨¡å‹çš„è‡ªæˆ‘å­¦ä¹ å’Œä¼˜åŒ–æä¾›äº†æ–°çš„é€”å¾„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„æ–¹æ³•ä¹Ÿä¸ºå…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›æå‡æä¾›äº†å¯ç¤ºã€‚

## disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization
### Abstract
The recent success and openness of DeepSeek-R1 have brought widespread
attention to Group Relative Policy Optimization (GRPO) as a reinforcement
learning method for large reasoning models (LRMs). In this work, we analyze the
GRPO objective under a binary reward setting and reveal an inherent limitation
of question-level difficulty bias. We also identify a connection between GRPO
and traditional discriminative methods in supervised learning. Motivated by
these insights, we introduce a new Discriminative Constrained Optimization
(DisCO) framework for reinforcing LRMs, grounded in the principle of
discriminative learning. The main differences between DisCO and GRPO and its
recent variants are: (1) it replaces the group relative objective with a
discriminative objective defined by a scoring function; (2) it abandons
clipping-based surrogates in favor of non-clipping RL surrogate objectives used
as scoring functions; (3) it employs a simple yet effective constrained
optimization approach to enforce the KL divergence constraint, ensuring stable
training. As a result, DisCO offers notable advantages over GRPO and its
variants: (i) it completely eliminates difficulty bias by adopting
discriminative objectives; (ii) it addresses the entropy instability in GRPO
and its variants through the use of non-clipping scoring functions and a
constrained optimization approach; (iii) it allows the incorporation of
advanced discriminative learning techniques to address data imbalance, where a
significant number of questions have more negative than positive generated
answers during training. Our experiments on enhancing the mathematical
reasoning capabilities of SFT-finetuned models show that DisCO significantly
outperforms GRPO and its improved variants such as DAPO, achieving average
gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B
model.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "DisCOï¼šç”¨åˆ¤åˆ«å¼ä¼˜åŒ–å¼ºåŒ–å¤§å‹æ¨ç†æ¨¡å‹"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ•°å­¦å’Œç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œç‰¹åˆ«æ˜¯DeepSeek-R1æ¨¡å‹çš„å¼€æ”¾æ€§å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä½œä¸ºä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æå‡LRMsæ€§èƒ½æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼ŒGRPOå­˜åœ¨ä¸€äº›å†…åœ¨é™åˆ¶ï¼Œå¦‚é—®é¢˜éš¾åº¦åå·®å’Œè®­ç»ƒä¸ç¨³å®šæ€§ã€‚æœ¬æ–‡æ—¨åœ¨è®¾è®¡ä¸€ç§æ–°çš„ä¼˜åŒ–æ¡†æ¶ï¼Œä»¥å…‹æœGRPOçš„è¿™äº›é™åˆ¶ï¼Œå¹¶æé«˜LRMsçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡åˆ†æäº†GRPOåœ¨äºŒå…ƒå¥–åŠ±è®¾ç½®ä¸‹çš„ç›®æ ‡å‡½æ•°ï¼Œæ­ç¤ºäº†å…¶é—®é¢˜éš¾åº¦åå·®çš„æ ¹æœ¬åŸå› ï¼Œå³å…¶ç»„ç›¸å¯¹ä¼˜åŠ¿å‡½æ•°å¯¼è‡´å¯¹è¿‡äºç®€å•æˆ–è¿‡äºå›°éš¾çš„é—®é¢˜èµ‹äºˆä¸ç›¸ç§°çš„æƒé‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°äº†GRPOä¸ä¼ ç»Ÿç›‘ç£å­¦ä¹ ä¸­çš„åˆ¤åˆ«å¼æ–¹æ³•ä¹‹é—´çš„æ¦‚å¿µè”ç³»ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åŸºäºè¿™äº›æ´å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ¤åˆ«å¼ä¼˜åŒ–æ¡†æ¶DisCOï¼Œç”¨äºå¼ºåŒ–LRMsã€‚DisCOçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
- ä½¿ç”¨åˆ¤åˆ«å¼ç›®æ ‡ä»£æ›¿ç»„ç›¸å¯¹ç›®æ ‡ï¼Œé€šè¿‡ä¸€ä¸ªè¯„åˆ†å‡½æ•°å®šä¹‰ï¼›
- æ”¾å¼ƒåŸºäºå‰ªè¾‘çš„æ›¿ä»£å‡½æ•°ï¼Œè½¬è€Œä½¿ç”¨éå‰ªè¾‘çš„å¼ºåŒ–å­¦ä¹ æ›¿ä»£ç›®æ ‡ä½œä¸ºè¯„åˆ†å‡½æ•°ï¼›
- é‡‡ç”¨ç®€å•æœ‰æ•ˆçš„çº¦æŸä¼˜åŒ–æ–¹æ³•æ¥å¼ºåˆ¶æ‰§è¡ŒKLæ•£åº¦çº¦æŸï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¢å¼ºSFTå¾®è°ƒæ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢è¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºDisCOåœ¨å…­ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºGRPOåŠå…¶æ”¹è¿›ç‰ˆæœ¬DAPOï¼Œå¯¹äºä¸€ä¸ª1.5Bæ¨¡å‹ï¼Œå¹³å‡å¢ç›Šè¶…è¿‡GRPOçš„7%å’ŒDAPOçš„6%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- DisCOæ¡†æ¶é€šè¿‡é‡‡ç”¨åˆ¤åˆ«å¼å­¦ä¹ åŸåˆ™ï¼Œå®Œå…¨æ¶ˆé™¤äº†éš¾åº¦åå·®ï¼Œå¹¶é€šè¿‡ä½¿ç”¨éå‰ªè¾‘è¯„åˆ†å‡½æ•°å’Œçº¦æŸä¼˜åŒ–æ–¹æ³•è§£å†³äº†GRPOåŠå…¶å˜ä½“çš„ç†µä¸ç¨³å®šæ€§é—®é¢˜ã€‚
- DisCOå…è®¸é›†æˆå…ˆè¿›çš„åˆ¤åˆ«å¼å­¦ä¹ æŠ€æœ¯æ¥å¤„ç†ç”Ÿæˆå›æ”¾ä¸­çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰å¾ˆé«˜çš„ä»·å€¼ã€‚
- æœ¬æ–‡çš„å®éªŒå’Œåˆ†æä¸ºå¼ºåŒ–LRMsæä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œå¯¹äºå¸Œæœ›åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­æå‡æ¨¡å‹æ€§èƒ½çš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆå…·æœ‰å‚è€ƒæ„ä¹‰ã€‚

## ktae--a-model-free-algorithm-to-key-tokens-advantage-estimation-in-mathematical-reasoning
### Abstract
Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œKTAEï¼šæ•°å­¦æ¨ç†ä¸­çš„æ— æ¨¡å‹å…³é”®ä»¤ç‰Œä¼˜åŠ¿ä¼°è®¡ç®—æ³•â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå°†å¼ºåŒ–å­¦ä¹ ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ç›¸ç»“åˆï¼Œå·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œç”šè‡³åœ¨æ— éœ€ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—æ˜¾è‘—æ•ˆæœã€‚ç„¶è€Œï¼Œæµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚GRPOåŠå…¶å˜ä½“DAPOåœ¨è®¡ç®—ä¼˜åŠ¿æ—¶å­˜åœ¨ç²’åº¦ç²—ç³™çš„é—®é¢˜ï¼Œå®ƒä»¬è®¡ç®—çš„æ˜¯å›æ»šçº§åˆ«çš„ä¼˜åŠ¿ï¼Œä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä»¤ç‰Œåˆ†é…ç›¸åŒçš„å€¼ï¼Œæ— æ³•æ•æ‰åˆ°ç‰¹å®šä»¤ç‰Œçš„è´¡çŒ®ï¼Œä»è€Œé˜»ç¢äº†æœ‰æ•ˆçš„å­¦ä¹ ã€‚é’ˆå¯¹è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†å…³é”®ä»¤ç‰Œä¼˜åŠ¿ä¼°è®¡ï¼ˆKTAEï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ä¸å¼•å…¥é¢å¤–æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡ç»†ç²’åº¦çš„ä»¤ç‰Œçº§ä¼˜åŠ¿ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
KTAEç®—æ³•åˆ©ç”¨äº†é‡‡æ ·å›æ»šçš„æ­£ç¡®æ€§ï¼Œå¹¶åº”ç”¨ç»Ÿè®¡åˆ†ææ–¹æ³•æ¥é‡åŒ–åºåˆ—ä¸­æ¯ä¸ªä»¤ç‰Œå¯¹æœ€ç»ˆç»“æœçš„é‡è¦æ€§ã€‚é€šè¿‡æ„å»ºåˆ—è”è¡¨ï¼Œå¹¶ä½¿ç”¨å¦‚Fisherç²¾ç¡®æ£€éªŒå’Œä¿¡æ¯å¢ç›Šç­‰ç»Ÿè®¡æ–¹æ³•ï¼ŒKTAEèƒ½å¤Ÿé‡åŒ–æ¯ä¸ªä»¤ç‰Œä¸æ­£ç¡®å›æ»šä¹‹é—´çš„å…³è”å¼ºåº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
KTAEç®—æ³•å°†é‡åŒ–åçš„ä»¤ç‰Œé¢‘ç‡å’Œå¯¹åº”å›æ»šçš„å¥–åŠ±ç»“åˆèµ·æ¥ï¼Œè¿›ä¸€æ­¥é‡åŒ–è¿™ç§å…³è”è´¡çŒ®çš„æ–¹å‘ã€‚æœ€ç»ˆï¼Œè¿™äº›åº¦é‡é€šè¿‡ä¹˜æ³•ç­‰æ“ä½œç»“åˆï¼Œä¸ºæ¯ä¸ªä»¤ç‰Œäº§ç”Ÿä¸€ä¸ªâ€œå…³é”®ä»¤ç‰Œå€¼â€ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æä¾›äº†æ›´ç»†ç²’åº¦çš„ä¼˜åŠ¿ä¿¡æ¯ï¼Œè€Œä¸”ç”±äºå…¶ä¸å¼•å…¥é¢å¤–çš„æ¨¡å‹ï¼Œå› æ­¤å…·æœ‰æ›´ä½çš„è®­ç»ƒæˆæœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨GRPO+KTAEå’ŒDAPO+KTAEè®­ç»ƒçš„æ¨¡å‹åœ¨äº”ä¸ªä¸»è¦çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹åœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œè¿˜èƒ½æ˜¾è‘—å‡å°‘å“åº”é•¿åº¦ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
KTAEç®—æ³•æä¾›äº†ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. KTAEåœ¨ä¸å¼•å…¥é¢å¤–æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†æ›´ç»†ç²’åº¦çš„ä¼˜åŠ¿ä¿¡æ¯ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚
2. KTAEä½¿ç”¨ç»Ÿè®¡åˆ†ææ–¹æ³•é‡åŒ–ä»¤ç‰Œä¹‹é—´çš„é‡è¦æ€§å·®å¼‚ï¼Œæä¾›äº†è¾ƒå¼ºçš„è§£é‡Šæ€§ã€‚
3. KTAEçš„å…³é”®ä»¤ç‰Œå€¼åŸºäºæœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§è®¡ç®—ï¼Œå¹¶ä¿ç•™äº†åŸå§‹çš„å›æ»šçº§åˆ«ä¼˜åŠ¿ï¼Œä½¿å…¶ä¸æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚
4. KTAEå¸®åŠ©æ¨¡å‹å…³æ³¨å…³é”®ä»¤ç‰Œï¼Œå‡å°‘äº†å¯¹æ— å…³ä»¤ç‰Œçš„å­¦ä¹ ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†å“åº”é•¿åº¦ã€‚

## s-grpo--early-exit-via-reinforcement-learning-in-reasoning-models
### Abstract
As Test-Time Scaling emerges as an active research focus in the large
language model community, advanced post-training methods increasingly emphasize
extending chain-of-thought (CoT) generation length, thereby enhancing reasoning
capabilities to approach Deepseek R1-like reasoning models. However, recent
studies reveal that reasoning models (even Qwen3) consistently exhibit
excessive thought redundancy in CoT generation. This overthinking issue arises
from the inherent limitations of conventional outcome-reward reinforcement
learning, which systematically overlooks the regulation of intermediate
reasoning processes. This paper introduces Serial-Group Decaying-Reward Policy
Optimization (S-GRPO), a novel reinforcement learning paradigm that enables
models to implicitly evaluate the sufficiency of intermediate reasoning steps,
thereby facilitating early exit in CoT generation. Unlike GRPO, which samples
multiple possible reasoning paths in parallel (parallel group), S-GRPO only
samples one reasoning path and serially selects multiple temporal positions
from the path to exit thinking and directly generate answers (serial group).
For correct answers within a serial group, rewards gradually decrease based on
the exit positions along the reasoning path from front to back. This design
encourages the model to produce more accurate and concise thoughts, while also
incentivizing early thinking termination when appropriate. Empirical
evaluations demonstrate that S-GRPO is compatible with state-of-the-art
reasoning models, including Qwen3 and Deepseek-distill. Across diverse
benchmarks such as GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond,
S-GRPO achieves a substantial reduction in sequence length (35.4% - 61.1%)
while simultaneously improving accuracy (absolute 0.72% - 6.08%).
### ğŸŒŸ è®ºæ–‡è§£è¯» | S-GRPOï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°æ¨ç†æ¨¡å‹ä¸­çš„æ—©æœŸé€€å‡ºç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æµ‹è¯•æ—¶é—´æ‰©å±•ï¼ˆTest-Time Scalingï¼‰æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç¤¾åŒºçš„ç ”ç©¶ç„¦ç‚¹ï¼Œå…ˆè¿›çš„è®­ç»ƒåæ–¹æ³•è¶Šæ¥è¶Šæ³¨é‡å»¶é•¿é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰çš„ç”Ÿæˆé•¿åº¦ï¼Œä»¥æå‡æ¨ç†èƒ½åŠ›ï¼Œæ¥è¿‘Deepseek R1ç­‰æ¨ç†æ¨¡å‹çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯Qwen3è¿™æ ·çš„æ¨ç†æ¨¡å‹ï¼Œåœ¨CoTç”Ÿæˆè¿‡ç¨‹ä¸­ä¹Ÿæ™®éå­˜åœ¨è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†é“¾è¿‡é•¿ï¼ŒåŒ…å«æ— å…³ä¿¡æ¯å’Œå¤šä½™çš„æ¨ç†æ­¥éª¤ã€‚è¿™ä¸€é—®é¢˜æºäºä¼ ç»Ÿçš„ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚GRPOï¼‰çš„å›ºæœ‰å±€é™ï¼Œå®ƒä¾èµ–äºæœ€ç»ˆç»“æœçš„å¥–åŠ±ï¼Œå¿½ç•¥äº†ä¸­é—´æ¨ç†è¿‡ç¨‹çš„è°ƒèŠ‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†åºåˆ—ç»„è¡°å‡å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆSerial-Group Decaying-Reward Policy Optimization, S-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéšå¼è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤çš„å……åˆ†æ€§ï¼Œä»è€Œåœ¨CoTç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°æ—©æœŸé€€å‡ºã€‚ä¸GRPOä¸åŒï¼ŒGRPOåœ¨å¹¶è¡Œä¸­é‡‡æ ·å¤šä¸ªå¯èƒ½çš„æ¨ç†è·¯å¾„ï¼Œè€ŒS-GRPOåªé‡‡æ ·ä¸€ä¸ªæ¨ç†è·¯å¾„ï¼Œå¹¶ä»è·¯å¾„ä¸­é€‰æ‹©å¤šä¸ªæ—¶é—´ä½ç½®æ¥æå‰ç»“æŸæ€è€ƒå’Œç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
S-GRPOé€šè¿‡åœ¨æ­£ç¡®ç­”æ¡ˆçš„æ—©æœŸé€€å‡ºä½ç½®èµ‹äºˆé€’å‡çš„å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹åœ¨CoTç”Ÿæˆçš„æ—©æœŸé˜¶æ®µå°±äº§ç”Ÿé«˜è´¨é‡çš„æ¨ç†è·¯å¾„ï¼Œå¹¶åœ¨è¾¾åˆ°å……åˆ†æ€§åç«‹å³é€€å‡ºã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„æ»šåŠ¨è¿‡ç¨‹ä¿æŒäº†åŸå§‹æ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ï¼Œç¡®ä¿æ¨¡å‹ç°æœ‰çš„æ¨ç†èƒ½åŠ›ä¸å—å½±å“ï¼Œé€‚åˆä½œä¸ºè®­ç»ƒåçš„æœ€ç»ˆé˜¶æ®µã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GSM8Kã€AIME 2024ã€AMC 2023ã€MATH-500å’ŒGPQA Diamondç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨Qwen3å’ŒDeepseekç³»åˆ—æ¨ç†æ¨¡å‹è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒS-GRPOå®ç°äº†0.72%è‡³6.08%çš„ç»å¯¹å‡†ç¡®åº¦æå‡ï¼ŒåŒæ—¶å¹³å‡å‡å°‘äº†35.4%è‡³61.1%çš„åºåˆ—é•¿åº¦ï¼Œè¯æ˜äº†æ•ˆç‡ä¸å‡†ç¡®åº¦çš„ååŒæå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„S-GRPOæ–¹æ³•ä¸ºæ¨ç†æ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡æ—©æœŸé€€å‡ºæœºåˆ¶å‡å°‘äº†è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼ŒS-GRPOçš„è®­ç»ƒæ¡†æ¶çš„å¼€æºä¹Ÿä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶è€…æä¾›äº†ä¾¿åˆ©ã€‚

## mimo--unlocking-the-reasoning-potential-of-language-model----from-pretraining-to-posttraining
### Abstract
We present MiMo-7B, a large language model born for reasoning tasks, with
optimization across both pre-training and post-training stages. During
pre-training, we enhance the data preprocessing pipeline and employ a
three-stage data mixing strategy to strengthen the base model's reasoning
potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional
Multi-Token Prediction objective for enhanced performance and accelerated
inference speed. During post-training, we curate a dataset of 130K verifiable
mathematics and programming problems for reinforcement learning, integrating a
test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and
employing strategic data resampling to stabilize training. Extensive
evaluations show that MiMo-7B-Base possesses exceptional reasoning potential,
outperforming even much larger 32B models. The final RL-tuned model,
MiMo-7B-RL, achieves superior performance on mathematics, code and general
reasoning tasks, surpassing the performance of OpenAI o1-mini. The model
checkpoints are available at https://github.com/xiaomimimo/MiMo.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£é”è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œèƒ½ï¼šä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒçš„å…¨é¢ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°æ—¥ç›Šçªå‡ºï¼Œå¦‚ä½•åœ¨å°è§„æ¨¡æ¨¡å‹ä¸­åŒæ—¶æå‡æ•°å­¦å’Œä»£ç æ¨ç†èƒ½åŠ›æˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒå’Œåè®­ç»ƒç­–ç•¥ï¼Œå…¨é¢è§£é”è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œèƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¼˜åŒ–é¢„è®­ç»ƒæ•°æ®
- å¯¹æ•°æ®é¢„å¤„ç†æµç¨‹è¿›è¡Œä¼˜åŒ–ï¼Œæé«˜æ–‡æœ¬æå–å·¥å…·çš„è´¨é‡ï¼Œå¹¶åº”ç”¨å¤šç»´æ•°æ®è¿‡æ»¤ï¼Œå¢åŠ é¢„è®­ç»ƒæ•°æ®ä¸­çš„æ¨ç†æ¨¡å¼å¯†åº¦ã€‚
- é€šè¿‡é«˜çº§æ¨ç†æ¨¡å‹ç”Ÿæˆå¤§é‡å¤šæ ·åŒ–çš„åˆæˆæ¨ç†æ•°æ®ã€‚
- é‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸçš„æ¨ç†æ½œèƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥å¤šæ ‡è®°é¢„æµ‹ç›®æ ‡
- åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å¤šæ ‡è®°é¢„æµ‹ï¼ˆMultiple-Token Predictionï¼‰ä½œä¸ºé¢å¤–è®­ç»ƒç›®æ ‡ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½å¹¶åŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåè®­ç»ƒç­–ç•¥
- ç²¾å¿ƒæ„å»ºäº†13ä¸‡ä¸ªå¯éªŒè¯çš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜ä½œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ•°æ®ï¼Œå¹¶é‡‡ç”¨åŸºäºè§„åˆ™çš„å‡†ç¡®æ€§å¥–åŠ±ï¼Œé¿å…å¥–åŠ±æ¬ºéª—ã€‚
- å¼•å…¥åŸºäºæµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±ï¼Œä»¥è§£å†³å›°éš¾ä»£ç é—®é¢˜çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚
- å®æ–½æ•°æ®é‡é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜å›æ”¾é‡‡æ ·æ•ˆç‡å’Œç¨³å®šç­–ç•¥æ›´æ–°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½
- å¼€å‘äº†æ— ç¼å›æ”¾å¼•æ“ï¼ŒåŠ é€ŸRLè®­ç»ƒå’ŒéªŒè¯ã€‚
- æ”¯æŒåœ¨vLLMä¸­å®ç°å¤šæ ‡è®°é¢„æµ‹ï¼Œå¹¶å¢å¼ºRLç³»ç»Ÿä¸­æ¨ç†å¼•æ“çš„é²æ£’æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- MiMo-7B-Baseåœ¨7Bå‚æ•°çš„å¼€æºæ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é€šç”¨çŸ¥è¯†å’Œç¼–ç ä»»åŠ¡ä¸Šã€‚
- MiMo-7B-RL-Zeroåœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šçš„RLè®­ç»ƒæ€§èƒ½è¶…è¿‡äº†32BåŸºæ¨¡å‹ã€‚
- MiMo-7B-RLåœ¨æ¨ç†æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒAIME 2025å¾—åˆ†è¶…è¿‡o1-mini 4.7åˆ†ï¼Œåœ¨ç®—æ³•ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šä¹Ÿæ˜¾è‘—ä¼˜äºOpenAI o1-miniã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æä¾›äº†å¦‚ä½•é€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒå’Œåè®­ç»ƒç­–ç•¥æ¥æå‡è¯­è¨€æ¨¡å‹æ¨ç†æ½œèƒ½çš„è¯¦ç»†æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹æ¶æ„è®¾è®¡ã€å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½ç­‰æ–¹é¢çš„åˆ›æ–°æ€è·¯ï¼Œå¯¹äºæœªæ¥è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œå¼€å‘å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚åŒæ—¶ï¼Œå¼€æºçš„MiMo-7Bç³»åˆ—æ¨¡å‹å’Œä»£ç ä¹Ÿä¸ºç¤¾åŒºæä¾›äº†å®è´µçš„èµ„æºã€‚

## mind-the-gap--bridging-thought-leap-for-improved-chain-of-thought-tuning
### Abstract
Large language models (LLMs) have achieved remarkable progress on
mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œå¡«è¡¥æ€ç»´è·³è·ƒç©ºç™½ï¼šæå‡é“¾å¼æ€ç»´è°ƒä¼˜çš„æ•ˆèƒ½â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨æ•°å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°å­¦CoTæ•°æ®é›†å¸¸å¸¸å› ä¸ºä¸“å®¶çœç•¥ä¸­é—´æ­¥éª¤è€Œå­˜åœ¨æ€ç»´è·³è·ƒï¼ˆThought Leapï¼‰ï¼Œè¿™ä¼šå¯¹æ¨¡å‹çš„å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨æ£€æµ‹æ€ç»´è·³è·ƒå¹¶ç”Ÿæˆç¼ºå¤±ä¸­é—´æ¨ç†æ­¥éª¤çš„æ–¹æ³•ï¼Œä»¥æ¢å¤CoTçš„å®Œæ•´æ€§å’Œè¿è´¯æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯†åˆ«å¹¶å½¢å¼åŒ–äº†CoTæ¨ç†ä¸­çš„æ€ç»´è·³è·ƒç°è±¡ï¼Œå¹¶å¼•å…¥äº†CoTæ€ç»´è·³è·ƒæ¡¥æ¥ä»»åŠ¡ï¼ˆCoT Thought Leap Bridge Taskï¼‰ï¼Œä»¥åŠä¸€ä¸ªç”¨äºè§£å†³è¿™ä¸€é—®é¢˜çš„è¯„ä¼°æ¡†æ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æ„å»ºäº†ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒæ•°æ®é›†ScaleQM+ï¼ŒåŸºäºç»“æ„åŒ–çš„ScaleQuestMathæ•°æ®é›†ï¼Œé€šè¿‡ç³»ç»Ÿåœ°ç§»é™¤ä¸­é—´æ­¥éª¤ï¼Œå¹¶å°†ä¸å®Œæ•´çš„æ¨ç†é“¾ä¸å®ƒä»¬çš„å®Œæ•´å¯¹åº”ç‰©é…å¯¹ã€‚ç„¶åï¼Œå¼€å‘äº†ä¸€ä¸ªåä¸ºCoT-Bridgeçš„æ¨¡å‹ï¼ŒåŸºäºQwen2.5-Math-7Bï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯†åˆ«å’Œæ¡¥æ¥æ•°å­¦æ¨ç†ä¸­çš„æ€ç»´è·³è·ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œç»¼åˆå®éªŒï¼Œæœ¬æ–‡è¯æ˜äº†åœ¨æ¡¥æ¥æ•°æ®é›†ä¸Šå¾®è°ƒçš„æ¨¡å‹å§‹ç»ˆä¼˜äºåœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼ŒNuminaMathä¸Šçš„æ”¹è¿›è¾¾åˆ°äº†+5.87%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æœ‰æ•ˆæå‡ç²¾ç‚¼æ•°æ®ï¼ˆ+3.02%ï¼‰å¹¶ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›æ›´å¥½çš„èµ·å§‹ç‚¹ï¼ˆ+3.1%ï¼‰ã€‚CoT-Bridgeè¿˜åœ¨åŸŸå¤–é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šå±•ç¤ºäº†æ”¹è¿›çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¡®è®¤äº†æå‡æ¨ç†å®Œæ•´æ€§å¸¦æ¥çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥ä½œä¸ºå³æ’å³ç”¨çš„å¢å¼ºæ¨¡å—ï¼Œä¸ç°æœ‰çš„ä¼˜åŒ–æŠ€æœ¯å…¼å®¹ï¼Œå¦‚çŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æ”¾å¤§æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºæ•°å­¦æ¨ç†ï¼Œè¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–éœ€è¦é“¾å¼æ€ç»´æ¨ç†çš„é¢†åŸŸï¼Œä¸ºæå‡LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’å’Œå·¥å…·ã€‚

## adastar--adaptive-data-sampling-for-training-self-taught-reasoners
### Abstract
Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdaSTaRï¼šè‡ªé€‚åº”æ•°æ®é‡‡æ ·æå‡è‡ªæ•™å¼æ¨ç†æ¨¡å‹è®­ç»ƒæ•ˆç‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªæ•™å¼æ¨ç†æ¨¡å‹ï¼ˆSelf-Taught Reasonersï¼Œç®€ç§°STaRï¼‰æ˜¯è‡ªæˆ‘æå‡è¯­è¨€æ¨¡å‹è®­ç»ƒæµç¨‹ä¸­çš„å…³é”®éƒ¨åˆ†ã€‚è¿™ç§æ¨¡å‹é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„æ¨ç†æ­¥éª¤ï¼ˆ Chains-of-Thoughtï¼Œç®€ç§°CoTï¼‰è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„STaRæ¡†æ¶åœ¨æ•°æ®é‡‡æ ·æ–¹é¢å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œéšæœºé‡‡æ ·å®¹æ˜“å¯¼è‡´è®­ç»ƒæ•°æ®çš„ä¸å¹³è¡¡ï¼Œæ¨¡å‹åœ¨å·²è§£å†³ç¤ºä¾‹ä¸Šè¿‡åº¦è®­ç»ƒï¼Œè€Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ç¤ºä¾‹ä¸Šè®­ç»ƒä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ•°æ®é‡‡æ ·æ–¹æ³•ï¼Œä»¥æé«˜è‡ªæ•™å¼æ¨ç†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªé€‚åº”é‡‡æ ·å¤šæ ·æ€§
æœ¬æ–‡æå‡ºçš„AdaSTaRç®—æ³•é€šè¿‡ä¼˜å…ˆè€ƒè™‘æœªå……åˆ†è®­ç»ƒçš„ç¤ºä¾‹ï¼Œç¡®ä¿è®­ç»ƒçš„å¹³è¡¡æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºé¿å…æ¨¡å‹åœ¨å·²è§£å†³ç¤ºä¾‹ä¸Šçš„è¿‡åº¦è®­ç»ƒï¼ŒåŒæ—¶ç¡®ä¿åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ç¤ºä¾‹ä¸Šè·å¾—è¶³å¤Ÿçš„è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªé€‚åº”é‡‡æ ·è¯¾ç¨‹
AdaSTaRç®—æ³•è¿˜é€šè¿‡åŠ¨æ€è°ƒæ•´æ•°æ®éš¾åº¦ï¼Œä»¥åŒ¹é…æ¨¡å‹ä¸æ–­å˜åŒ–çš„æ€§èƒ½ï¼Œä»è€Œå®ç°è¯¾ç¨‹å¼çš„è‡ªé€‚åº”é‡‡æ ·ã€‚åœ¨æ¨¡å‹è¾ƒå¼±æ—¶ï¼Œä¼˜å…ˆé‡‡æ ·è¾ƒå®¹æ˜“çš„æ•°æ®ï¼Œéšç€æ¨¡å‹æ€§èƒ½çš„æå‡ï¼Œé€æ¸å¢åŠ éš¾åº¦è¾ƒé«˜çš„æ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…­ä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdaSTaRç®—æ³•åœ¨æ‰€æœ‰å®ä¾‹ä¸­å‡å–å¾—äº†æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡ï¼ˆ6/6ï¼‰ï¼Œå¹¶ä¸”å¹³å‡å‡å°‘äº†58.6%çš„è®­ç»ƒæµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰ï¼Œç›¸æ¯”å…¶ä»–åŸºçº¿æ–¹æ³•å…·æœ‰æ˜¾è‘—æ€§èƒ½å’Œæ•ˆç‡æå‡ã€‚è¿™äº›æ”¹è¿›åœ¨ä¸åŒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œæ›´å¤§è§„æ¨¡æ¨¡å‹ä¸ŠåŒæ ·æœ‰æ•ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„è‡ªé€‚åº”æ•°æ®é‡‡æ ·æ–¹æ³•ä¸ºæå‡è‡ªæ•™å¼æ¨ç†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœæä¾›äº†æ–°çš„æ€è·¯ã€‚AdaSTaRç®—æ³•é€šè¿‡å¹³è¡¡å­¦ä¹ å¤šæ ·æ€§å’Œè¯¾ç¨‹éš¾åº¦ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸSTaRæ¡†æ¶ä¸­çš„è®­ç»ƒæ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡å’Œç±»å‹çš„è¯­è¨€æ¨¡å‹ä¸Šå‡å…·æœ‰æ™®éé€‚ç”¨æ€§ï¼Œä¸ºæœªæ¥è‡ªæ•™å¼æ¨ç†æ¨¡å‹çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## a-survey-on-test-time-scaling-in-large-language-models--what--how--where--and-how-well-
### Abstract
As enthusiasm for scaling computation (data and parameters) in the
pretraining era gradually diminished, test-time scaling (TTS), also referred to
as ``test-time computing'' has emerged as a prominent research focus. Recent
studies demonstrate that TTS can further elicit the problem-solving
capabilities of large language models (LLMs), enabling significant
breakthroughs not only in specialized reasoning tasks, such as mathematics and
coding, but also in general tasks like open-ended Q&A. However, despite the
explosion of recent efforts in this area, there remains an urgent need for a
comprehensive survey offering a systemic understanding. To fill this gap, we
propose a unified, multidimensional framework structured along four core
dimensions of TTS research: what to scale, how to scale, where to scale, and
how well to scale. Building upon this taxonomy, we conduct an extensive review
of methods, application scenarios, and assessment aspects, and present an
organized decomposition that highlights the unique functional roles of
individual techniques within the broader TTS landscape. From this analysis, we
distill the major developmental trajectories of TTS to date and offer hands-on
guidelines for practical deployment. Furthermore, we identify several open
challenges and offer insights into promising future directions, including
further scaling, clarifying the functional essence of techniques, generalizing
to more tasks, and more attributions. Our repository is available on
https://github.com/testtimescaling/testtimescaling.github.io/
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶æ‰©å±•ï¼šå…¨é¢æ¢³ç†æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€é¢„è®­ç»ƒæ—¶ä»£è®¡ç®—æ‰©å±•çš„çƒ­æƒ…é€æ¸å‡å¼±ï¼Œæµ‹è¯•æ—¶æ‰©å±•ï¼ˆTest-Time Scaling, TTSï¼‰ä½œä¸ºä¸€ç§æ–°çš„ç ”ç©¶ç„¦ç‚¹æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼ŒTTSèƒ½å¤Ÿè¿›ä¸€æ­¥æ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œä¸ä»…åœ¨æ•°å­¦å’Œç¼–ç ç­‰ä¸“é—¨æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—çªç ´ï¼Œä¹Ÿåœ¨å¼€æ”¾æ€§é—®é¢˜å›ç­”ç­‰é€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå°½ç®¡åœ¨è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶æˆæœå±‚å‡ºä¸ç©·ï¼Œä½†ä»ç„¶è¿«åˆ‡éœ€è¦ä¸€ä»½å…¨é¢çš„ç»¼è¿°ï¼Œä»¥æä¾›ç³»ç»Ÿæ€§çš„ç†è§£ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šç»´åº¦æ¡†æ¶ï¼Œå¯¹TTSç ”ç©¶è¿›è¡Œäº†å…¨é¢çš„å›é¡¾å’Œåˆ†æã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šç»´åº¦æ¡†æ¶ï¼Œå°†TTSç ”ç©¶åˆ†ä¸ºå››ä¸ªæ ¸å¿ƒç»´åº¦ï¼šæ‰©å±•ä»€ä¹ˆï¼ˆWhat to scaleï¼‰ã€å¦‚ä½•æ‰©å±•ï¼ˆHow to scaleï¼‰ã€åœ¨å“ªé‡Œæ‰©å±•ï¼ˆWhere to scaleï¼‰ä»¥åŠæ‰©å±•æ•ˆæœå¦‚ä½•ï¼ˆHow well to scaleï¼‰ã€‚è¿™ä¸€æ¡†æ¶ä¸ºåˆ†ç±»ã€æ¯”è¾ƒå’Œæ‰©å±•TTSæ–¹æ³•æä¾›äº†ç»“æ„åŒ–çš„æ”¯æŒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åŸºäºè¿™ä¸€æ¡†æ¶ï¼Œæœ¬æ–‡å¯¹TTSé¢†åŸŸçš„æ–¹æ³•ã€åº”ç”¨åœºæ™¯å’Œè¯„ä¼°æ–¹é¢è¿›è¡Œäº†å¹¿æ³›çš„å›é¡¾ï¼Œå¹¶å‘ˆç°äº†ä¸€ä¸ªæœ‰ç»„ç»‡çš„åˆ†è§£ï¼Œçªå‡ºäº†å„ç§æŠ€æœ¯åœ¨TTSæ™¯è§‚ä¸­çš„ç‹¬ç‰¹åŠŸèƒ½è§’è‰²ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æç‚¼äº†TTSå‘å±•çš„ä¸»è¦è½¨è¿¹ï¼Œå¹¶æä¾›äº†å®é™…éƒ¨ç½²çš„å®ç”¨æŒ‡å—ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰å…·ä½“ä»‹ç»å®éªŒç»“æœï¼Œè€Œæ˜¯æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ–‡çŒ®ç»¼è¿°å’Œæ¡†æ¶ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå®è·µè€…ç†è§£TTSçš„å„ç§æ–¹æ³•å’Œè¯„ä¼°æ–¹é¢ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„å¯å€Ÿé‰´ä¹‹å¤„åœ¨äºï¼š
- æä¾›äº†ä¸€ä¸ªå…¨é¢çš„å¤šç»´åº¦æ¡†æ¶ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜ç³»ç»Ÿåœ°ç†è§£å’Œåˆ†æTTSçš„å„ç§æ–¹æ³•ã€‚
- é€šè¿‡å¯¹ç°æœ‰ç ”ç©¶çš„ç»„ç»‡å’Œåˆ†æï¼Œä¸ºæœªæ¥TTSæŠ€æœ¯çš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ï¼ŒåŒ…æ‹¬è¿›ä¸€æ­¥æ‰©å±•ã€æ˜ç¡®æŠ€æœ¯çš„åŠŸèƒ½æœ¬è´¨ã€æ¨å¹¿åˆ°æ›´å¤šä»»åŠ¡ä»¥åŠä¼˜åŒ–æ•ˆç‡ç­‰æ–¹é¢ã€‚
- è¯†åˆ«äº†TTSé¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„æŒç»­å‘å±•ã€‚

## arpo-end-to-end-policy-optimization-for-gui-agents-with-experience-replay
### Abstract
Training large language models (LLMs) as interactive agents for controlling
graphical user interfaces (GUIs) presents a unique challenge to optimize
long-horizon action sequences with multimodal feedback from complex
environments. While recent works have advanced multi-turn reinforcement
learning (RL) for reasoning and tool-using capabilities in LLMs, their
application to GUI-based agents remains relatively underexplored due to the
difficulty of sparse rewards, delayed feedback, and high rollout costs. In this
paper, we investigate end-to-end policy optimization for vision-language-based
GUI agents with the aim of improving performance on complex, long-horizon
computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an
end-to-end RL approach that augments Group Relative Policy Optimization (GRPO)
with a replay buffer to reuse the successful experience across training
iterations. To further stabilize the training process, we propose a task
selection strategy that filters tasks based on baseline agent performance,
allowing the agent to focus on learning from informative interactions.
Additionally, we compare ARPO with offline preference optimization approaches,
highlighting the advantages of policy-based methods in GUI environments.
Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive
results, establishing a new performance baseline for LLM-based GUI agents
trained via reinforcement learning. Our findings underscore the effectiveness
of reinforcement learning for training multi-turn, vision-language GUI agents
capable of managing complex real-world UI interactions. Codes and
models:https://github.com/dvlab-research/ARPO.git.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ARPOï¼šé¢å‘å›¾å½¢ç•Œé¢ä»£ç†çš„ç«¯åˆ°ç«¯ç­–ç•¥ä¼˜åŒ–ä¸ç»éªŒå›æ”¾

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äº¤äº’å¼ä»£ç†ä¸­çš„åº”ç”¨ï¼Œå¦‚ä½•ä¼˜åŒ–æ§åˆ¶å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„é•¿å‘¨æœŸåŠ¨ä½œåºåˆ—æˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„ç ”ç©¶åœ¨å¤šè½®æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨GUIä»£ç†çš„åº”ç”¨ä¸Šä»ç„¶æ¢ç´¢ä¸è¶³ï¼Œä¸»è¦å› ä¸ºGUIç¯å¢ƒä¸­çš„ç¨€ç–å¥–åŠ±ã€å»¶è¿Ÿåé¦ˆå’Œé«˜æˆæœ¬å›æ”¾ç­‰é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç«¯åˆ°ç«¯çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæå‡åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„GUIä»£ç†åœ¨å¤æ‚ã€é•¿å‘¨æœŸè®¡ç®—æœºä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºäº†ä»£ç†é‡æ”¾ç­–ç•¥ä¼˜åŒ–ï¼ˆARPOï¼‰æ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œå³ARPOï¼Œè¯¥æ–¹æ³•åœ¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰çš„åŸºç¡€ä¸Šå¢åŠ äº†ç»éªŒå›æ”¾åŠŸèƒ½ï¼Œä»¥é‡ç”¨è®­ç»ƒè¿­ä»£ä¸­çš„æˆåŠŸç»éªŒï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä»»åŠ¡é€‰æ‹©ç­–ç•¥å’Œç»éªŒå›æ”¾ç¼“å†²åŒº
ä¸ºäº†è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸºçº¿ä»£ç†æ€§èƒ½è¿‡æ»¤ä»»åŠ¡çš„ä»»åŠ¡é€‰æ‹©ç­–ç•¥ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä¸“æ³¨äºä»ä¿¡æ¯ä¸°å¯Œçš„äº¤äº’ä¸­å­¦ä¹ ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ä¸ªç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œä¸“é—¨ç”¨äºå­˜å‚¨æˆåŠŸçš„è½¨è¿¹ï¼Œä»¥å¢å¼ºåœ¨ç¨€ç–å¥–åŠ±è®¾ç½®ä¸­çš„æ ·æœ¬æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒARPOæ–¹æ³•å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œä¸ºåŸºäºLLMçš„GUIä»£ç†é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè®¾å®šäº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨è®­ç»ƒå¤šè½®ã€è§†è§‰-è¯­è¨€GUIä»£ç†æ–¹é¢æ˜¯æœ‰æ•ˆçš„ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿç®¡ç†å¤æ‚çš„ç°å®ä¸–ç•ŒUIäº¤äº’ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºGUIä»£ç†çš„ç«¯åˆ°ç«¯ç­–ç•¥ä¼˜åŒ–æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥ä¸‹æ–¹é¢å€¼å¾—å€Ÿé‰´ï¼š
- å¦‚ä½•åœ¨GUIç¯å¢ƒä¸­æœ‰æ•ˆåœ°åˆ©ç”¨ç»éªŒå›æ”¾æ¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚
- ä»»åŠ¡é€‰æ‹©ç­–ç•¥åœ¨ç»´æŒå¥–åŠ±å¤šæ ·æ€§å’Œä¼˜åŒ–ç­–ç•¥å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚
- å¼ºåŒ–å­¦ä¹ åœ¨æå‡ä»£ç†åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ï¼Œä»¥åŠå¯¹è·¨é¢†åŸŸä»»åŠ¡æ€§èƒ½çš„é€‚åº¦æå‡ã€‚

## using-reinforcement-learning-to-train-large-language-models-to-explain-human-decisions
### Abstract
A central goal of cognitive modeling is to develop models that not only
predict human behavior but also provide insight into the underlying cognitive
mechanisms. While neural network models trained on large-scale behavioral data
often achieve strong predictive performance, they typically fall short in
offering interpretable explanations of the cognitive processes they capture. In
this work, we explore the potential of pretrained large language models (LLMs)
to serve as dual-purpose cognitive models--capable of both accurate prediction
and interpretable explanation in natural language. Specifically, we employ
reinforcement learning with outcome-based rewards to guide LLMs toward
generating explicit reasoning traces for explaining human risky choices. Our
findings demonstrate that this approach produces high-quality explanations
alongside strong quantitative predictions of human decisions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šäººç±»å†³ç­–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è®¤çŸ¥å»ºæ¨¡çš„æ ¸å¿ƒç›®æ ‡æ˜¯å¼€å‘ä¸ä»…èƒ½å¤Ÿé¢„æµ‹äººç±»è¡Œä¸ºï¼Œè¿˜èƒ½æä¾›å¯¹åº•å±‚è®¤çŸ¥æœºåˆ¶æ´å¯Ÿçš„æ¨¡å‹ã€‚è™½ç„¶åŸºäºå¤§è§„æ¨¡è¡Œä¸ºæ•°æ®çš„ç¥ç»ç½‘ç»œæ¨¡å‹é€šå¸¸èƒ½å¤Ÿå®ç°å¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨æä¾›å¯è§£é‡Šæ€§çš„è®¤çŸ¥è¿‡ç¨‹è§£é‡Šæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æ¢è®¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºåŒé‡ç”¨é€”è®¤çŸ¥æ¨¡å‹çš„æ½œåŠ›â€”â€”æ—¢èƒ½å®ç°å‡†ç¡®çš„é¢„æµ‹ï¼Œä¹Ÿèƒ½ä»¥è‡ªç„¶è¯­è¨€æä¾›å¯è§£é‡Šçš„è§£é‡Šã€‚å…·ä½“è€Œè¨€ï¼Œæœ¬æ–‡é‡‡ç”¨åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ æ¥å¼•å¯¼LLMç”Ÿæˆè§£é‡Šäººç±»é£é™©é€‰æ‹©çš„æ˜ç¡®æ¨ç†è½¨è¿¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ä½¿ç”¨åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒLLMï¼Œä½¿å…¶åœ¨ç”Ÿæˆäººç±»é£é™©é€‰æ‹©é¢„æµ‹çš„åŒæ—¶ï¼Œä¹Ÿèƒ½ç”Ÿæˆæ˜ç¡®çš„æ¨ç†è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•å°†LLMçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä½œä¸ºåº•å±‚è®¤çŸ¥æœºåˆ¶çš„å£å¤´æè¿°ï¼Œä»¥ä¾¿è®¤çŸ¥ç§‘å­¦å®¶èƒ½å¤Ÿåˆ†æè¿™äº›CoTï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦ä¸ºè§‚å¯Ÿæ•°æ®æä¾›äº†æœ‰æ„ä¹‰ä¸”å¯è§£é‡Šçš„è§£é‡Šã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å¯¹æ¯”äº†ä¸‰ç§ä¸åŒçš„LLMåè®­ç»ƒç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€é’ˆå¯¹è®¤çŸ¥ä»»åŠ¡è®¾è®¡çš„Centauré£æ ¼SFTï¼Œä»¥åŠåŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰ã€‚æ‰€æœ‰æ–¹æ³•éƒ½åº”ç”¨äºåœ¨æœ€å¤§å¯ç”¨çš„äººç±»é£é™©é€‰æ‹©æ•°æ®é›†choices13kä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨ç”Ÿæˆæœ‰ç”¨è®¤çŸ¥æ¨¡å‹æ–¹é¢çš„æ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•èƒ½å¤Ÿä»äººç±»è¡Œä¸ºæ•°æ®ä¸­å¼•å¯¼å‡ºåˆç†çš„CoTæ¨ç†è½¨è¿¹ï¼Œå¹¶ä¸”å…¶é¢„æµ‹å‡†ç¡®æ€§å¯ä»¥ä¸åŸºäºSFTçš„æ–¹æ³•ç›¸åª²ç¾ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„CoTèƒ½å¤Ÿæ ¹æ®è®­ç»ƒæ•°æ®çš„ç»“æ„è¿›è¡Œé€‚åº”ï¼Œå½“äººç±»è¡Œä¸ºæ•°æ®è¢«åˆæˆæ•°æ®æ›¿ä»£æ—¶ï¼ŒCoTä¹Ÿä¼šç›¸åº”åœ°è°ƒæ•´ä»¥åæ˜ åˆæˆæ•°æ®é›†çš„ç»“æ„ã€‚åŒæ—¶ï¼ŒCoTçš„è´¨é‡ä¾èµ–äºåŸºç¡€LLMçš„å¼ºåº¦ï¼Œä½¿ç”¨è¾ƒå¼±çš„æ¨¡å‹ä¼šå¯¼è‡´æ¨ç†è´¨é‡çš„æ˜æ˜¾ä¸‹é™ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æˆæœä¸ºè®¤çŸ¥å»ºæ¨¡é¢†åŸŸæä¾›äº†æ–°çš„è§†è§’ï¼Œå³åˆ©ç”¨LLMçš„æ¨ç†ç”Ÿæˆèƒ½åŠ›æ¥åˆ›å»ºæ—¢èƒ½å¤Ÿé¢„æµ‹äººç±»è¡Œä¸ºï¼Œåˆèƒ½å¤Ÿæä¾›è®¤çŸ¥æœºåˆ¶è§£é‡Šçš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æœ‰åŠ©äºæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œè¿˜èƒ½å¤Ÿä¸ºç†è§£äººç±»å†³ç­–è¿‡ç¨‹æä¾›æ›´æ·±å…¥çš„æ´å¯Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„æ–¹æ³•ä¹Ÿä¸ºè‡ªåŠ¨å‘ç°è®¤çŸ¥æ¨¡å‹çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ï¼Œå€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢å’Œåº”ç”¨ã€‚

## uft--unifying-supervised-and-reinforcement-fine-tuning
### Abstract
Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç»Ÿä¸€å¾®è°ƒï¼šèåˆç›‘ç£ä¸å¼ºåŒ–å­¦ä¹ çš„çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æå‡ï¼Œåè®­ç»ƒæ–¹æ³•åœ¨å¢å¼ºè¿™äº›æ¨¡å‹çš„èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºå…¶é‡è¦æ€§ã€‚ç›®å‰ä¸»è¦çš„åè®­ç»ƒæ–¹æ³•åˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚SFTæ–¹æ³•é«˜æ•ˆä¸”é€‚åˆå°å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå¹¶é™åˆ¶å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è€ŒRFTé€šå¸¸èƒ½è·å¾—æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä¸¥é‡ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„å¼ºåº¦ã€‚ä¸ºäº†å…‹æœSFTå’ŒRFTçš„é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°†SFTå’ŒRFTèåˆä¸ºä¸€ä½“çš„æ–°å‹åè®­ç»ƒèŒƒå¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
UFTæ–¹æ³•é€šè¿‡å°†ç›‘ç£ä¿¡å·ä¸å¥–åŠ±ä¿¡å·ç›¸ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¢ç´¢è§£å†³æ–¹æ¡ˆçš„åŒæ—¶ï¼Œåˆ©ç”¨ä¿¡æ¯ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œå¼¥è¡¥äº†ç°æœ‰æ–¹æ³•ä¸­è®°å¿†ä¸æ€è€ƒä¹‹é—´çš„é¸¿æ²Ÿã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡ç†è®ºè¯æ˜äº†UFTæ‰“ç ´äº†RFTå†…åœ¨çš„æŒ‡æ•°æ ·æœ¬å¤æ‚åº¦ç“¶é¢ˆï¼Œæ˜¾ç¤ºå‡ºç»Ÿä¸€è®­ç»ƒå¯ä»¥åœ¨é•¿è·ç¦»æ¨ç†ä»»åŠ¡ä¸ŠæŒ‡æ•°çº§åŠ é€Ÿæ”¶æ•›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Countdownã€MATHå’ŒKnights and Knavesé€»è¾‘è°œé¢˜ç­‰å¤šä¸ªä»»åŠ¡ä¸Šï¼ŒUFTæ–¹æ³•å‡ä¼˜äºSFTå’ŒRFTï¼Œæ— è®ºæ¨¡å‹å¤§å°å¦‚ä½•ã€‚å®éªŒç»“æœè¿˜è¡¨æ˜ï¼ŒUFTåœ¨æ ·æœ¬å¤æ‚åº¦ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜é•¿è·ç¦»æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„UFTæ–¹æ³•ä¸ºåè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡èåˆç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä¸ä»…æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿˜å‡å°‘äº†æ ·æœ¬å¤æ‚åº¦ã€‚è¿™ç§æ–¹æ³•å¯¹äºç†è§£å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µä»·å€¼ã€‚æ­¤å¤–ï¼ŒUFTæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ä¸Šçš„è¡¨ç°ä¸€è‡´æ€§ï¼Œä¹Ÿè¯æ˜äº†å…¶ç¨³å¥æ€§å’Œå¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## vl-rethinker--incentivizing-self-reflection-of-vision-language-models-with-reinforcement-learning
### Abstract
Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated
great potential in solving challenging problems through explicit reflection.
They significantly outperform the best fast-thinking models, such as GPT-4o, on
various math and science benchmarks. However, their multimodal reasoning
capabilities remain on par with fast-thinking models. For instance, GPT-o1's
performance on benchmarks like MathVista, MathVerse, and MathVision is similar
to fast-thinking models. In this paper, we aim to enhance the slow-thinking
capabilities of vision-language models using reinforcement learning (without
relying on distillation) to advance the state of the art. First, we adapt the
GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to
address the vanishing advantages problem. While this approach yields strong
performance, the resulting RL-trained models exhibit limited self-reflection or
self-verification. To further encourage slow-thinking, we introduce Forced
Rethinking, which appends a rethinking trigger token to the end of rollouts in
RL training, explicitly enforcing a self-reflection reasoning step. By
combining these two techniques, our model, VL-Rethinker, advances
state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%
respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary
benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the
gap with OpenAI-o1. Our empirical results show the effectiveness of our
approaches.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œVL-Rethinkerï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åæ€â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåƒGPT-o1å’ŒDeepSeek-R1è¿™æ ·çš„æ…¢æ€è€ƒç³»ç»Ÿé€šè¿‡æ˜ç¡®çš„åæ€è¿‡ç¨‹ï¼Œåœ¨è§£å†³å¤æ‚é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚å®ƒä»¬åœ¨å„ç§æ•°å­¦å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æœ€å¿«çš„æ€è€ƒæ¨¡å‹ï¼Œå¦‚GPT-4oã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ä¸Šä¸å¿«æ€è€ƒæ¨¡å‹ç›¸å½“ï¼Œç‰¹åˆ«æ˜¯åœ¨MathVistaã€MathVerseå’ŒMathVisionç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸å¿«æ€è€ƒæ¨¡å‹ç›¸ä¼¼ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆä¸ä¾èµ–æ¨¡å‹è’¸é¦ï¼‰æ¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆå¯¹GRPOç®—æ³•è¿›è¡Œäº†æ”¹è¿›ï¼Œå¼•å…¥äº†é€‰æ‹©æ€§æ ·æœ¬é‡æ”¾ï¼ˆSSRï¼‰æŠ€æœ¯ï¼Œä»¥è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚SSRé€šè¿‡ä»è¿‡å»è¿­ä»£ä¸­æŠ½å–é«˜ä»·å€¼ç»éªŒï¼Œå¢å¼ºå½“å‰è®­ç»ƒæ‰¹æ¬¡ï¼Œä»è€Œæä¾›æ›´ä¸€è‡´çš„æ¢¯åº¦ä¿¡å·ï¼Œå¹¶åŠ¨æ€è°ƒæ•´è®­ç»ƒç„¦ç‚¹ï¼Œæ¥è¿‘æ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†è¿›ä¸€æ­¥æ¿€åŠ±æ…¢æ€è€ƒï¼Œæœ¬æ–‡æå‡ºäº†å¼ºåˆ¶åæ€ç­–ç•¥ï¼Œå³åœ¨RLè®­ç»ƒä¸­åœ¨è¾“å‡ºå“åº”çš„æœ«å°¾æ·»åŠ ä¸€ä¸ªåæ€è§¦å‘ä»¤ç‰Œï¼Œæ˜ç¡®å¼ºåˆ¶æ‰§è¡Œè‡ªæˆ‘åæ€æ¨ç†æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•ä¿ƒä½¿æ¨¡å‹åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆä¹‹å‰è¿›è¡Œè‡ªæˆ‘åæ€å’Œè‡ªæˆ‘éªŒè¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡ç»“åˆè¿™ä¸¤ç§æŠ€æœ¯ï¼Œæœ¬æ–‡æå‡ºçš„VL-Rethinkeræ¨¡å‹åœ¨MathVistaå’ŒMathVerseåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†80.4%å’Œ63.5%çš„æœ€æ–°æˆç»©ã€‚æ­¤å¤–ï¼ŒVL-Rethinkeråœ¨MathVisionã€MMMU-Proã€EMMAå’ŒMEGA-Benchç­‰å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¼€æºæœ€æ–°æ°´å¹³ï¼Œä¸OpenAI-o1çš„å·®è·æ˜¾è‘—ç¼©å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ç›´æ¥çš„RLæ–¹æ³•æ¥å¢å¼ºVLMçš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤æ‚çš„ç›‘ç£å¾®è°ƒå’Œè’¸é¦ç®¡é“æä¾›äº†ä¸€ä¸ªå¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥SSRå’Œå¼ºåˆ¶åæ€ç­–ç•¥ï¼Œæœ¬æ–‡ä¸ºæ¿€åŠ±è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è‡ªæˆ‘åæ€æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚è¿™äº›æˆæœå¯¹äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚

## a-new-dapo-algorithm-for-stock-trading
### Abstract
Recent advances in reinforcement learning, such as Dynamic Sampling Policy
Optimization (DAPO), show strong performance when paired with large language
models (LLMs). Motivated by this success, we ask whether similar gains can be
realized in financial trading. We design a trading agent that combines an
improved Group Relative Policy Optimization (GRPO) algorithm, augmented with
ideas from DAPO, with LLM-based risk and sentiment signals extracted from
financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a
cumulative return of 230.49 percent and an information ratio of 0.37,
outperforming the CPPO-DeepSeek baseline. It also cuts training time from about
8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The
proposed RL-LLM framework offers a scalable path toward data-efficient trading
agents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œDAPOç®—æ³•é©æ–°ï¼šAIäº¤æ˜“å‘˜åœ¨è‚¡å¸‚çš„æ–°ç­–ç•¥â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ åœ¨ç®—æ³•äº¤æ˜“ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¦‚ä½•è‡ªåŠ¨åŒ–åœ°åœ¨ä¸ç¡®å®šæ€§ä¸‹åšå‡ºæŠ•èµ„ç»„åˆå†³ç­–æˆä¸ºç ”ç©¶çš„çƒ­ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä»ç„¶é¢ä¸´ç€è¯¸å¦‚å¤§å¹…å›æ’¤å’Œè§£é‡Šæ€§ä¸è¶³ç­‰é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥å®ç°æ›´é«˜æ•ˆã€æ›´ç¨³å¥çš„é‡‘èäº¤æ˜“ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é‡‡ç”¨äº†ä¸€ç§æ”¹è¿›çš„ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡ä»æ¯ä¸ªçŠ¶æ€ä¸­é‡‡æ ·å¤šä¸ªåŠ¨ä½œå¹¶æ ‡å‡†åŒ–å¥–åŠ±ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ä»·å€¼å‡½æ•°çš„éœ€æ±‚ï¼Œé™ä½äº†å†…å­˜ä½¿ç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å¼•å…¥äº†åŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•çš„å…ƒç´ ï¼ŒåŸæœ¬ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„åå¥½è°ƒæ•´ã€‚é€šè¿‡ä½¿ç”¨è§£è€¦å‰ªè¾‘å’ŒåŠ¨æ€é‡‡æ ·ï¼Œç®—æ³•èƒ½å¤Ÿåœ¨é«˜å›æŠ¥åœºæ™¯ä¸­å¢å¼ºæ¢ç´¢ï¼ŒåŒæ—¶é™åˆ¶é£é™©ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è°ƒæ•´çš„å¥–åŠ±å…¬å¼ï¼Œå…è®¸æ›´ç»†è‡´çš„æ§åˆ¶ï¼Œå¯ä»¥æ ¹æ®éœ€è¦å¼ºè°ƒæƒ…ç»ªæˆ–é£é™©ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨NASDAQ-100æŒ‡æ•°çš„FNSPIDæ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æå‡ºçš„æ”¹è¿›DAPOç®—æ³•å®ç°äº†230.49%çš„ç´¯ç§¯å›æŠ¥å’Œ0.37çš„ä¿¡æ¯æ¯”ç‡ï¼Œè¶…è¿‡äº†CPPO-DeepSeekåŸºçº¿ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•å°†è®­ç»ƒæ—¶é—´ä»å¤§çº¦8å°æ—¶å‡å°‘åˆ°2.5å°æ—¶ï¼Œå¹¶æ˜¾è‘—é™ä½äº†å†…å­˜ä½¿ç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºæ„å»ºæ•°æ®é«˜æ•ˆçš„é‡‘èäº¤æ˜“ä»£ç†æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼ŒåŒæ—¶å±•ç¤ºäº†ç»“åˆLLMçš„æƒ…ç»ªå’Œé£é™©ä¿¡å·åœ¨é‡‘èäº¤æ˜“ä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶ä¸­çš„å¯è°ƒæ•´å¥–åŠ±å…¬å¼å’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¸ºæé«˜äº¤æ˜“ç­–ç•¥çš„çµæ´»æ€§å’Œæ•ˆç‡æä¾›äº†æ–°çš„æ€è·¯ã€‚

## a-sober-look-at-progress-in-language-model-reasoning--pitfalls-and-paths-to-reproducibility
### Abstract
Reasoning has emerged as the next major frontier for language models (LMs),
with rapid advances from both academic and industrial labs. However, this
progress often outpaces methodological rigor, with many evaluations relying on
benchmarking practices that lack transparency, robustness, or statistical
grounding. In this work, we conduct a comprehensive empirical study and find
that current mathematical reasoning benchmarks are highly sensitive to subtle
implementation choices - including decoding parameters, random seeds, prompt
formatting, and even hardware and software-framework configurations.
Performance gains reported in recent studies frequently hinge on unclear
comparisons or unreported sources of variance. To address these issues, we
propose a standardized evaluation framework with clearly defined best practices
and reporting standards. Using this framework, we reassess recent methods and
find that reinforcement learning (RL) approaches yield only modest improvements
- far below prior claims - and are prone to overfitting, especially on
small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)
methods show consistently stronger generalization. To foster reproducibility,
we release all code, prompts, and model outputs, for reasoning benchmarks,
establishing more rigorous foundations for future work.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦è¯­è¨€æ¨¡å‹æ¨ç†è¿›å±•çš„å†·æ€è€ƒï¼šè¯¯åŒºä¸å¯å¤ç°æ€§è·¯å¾„

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ·±åº¦è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„å¿«é€Ÿè¿›æ­¥ï¼Œå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½åœ¨ç§¯ææ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›å±•å¾€å¾€ä¼´éšç€æ–¹æ³•è®ºä¸Šçš„ä¸å¤Ÿä¸¥è°¨ï¼Œè®¸å¤šè¯„ä¼°ä¾èµ–äºç¼ºä¹é€æ˜åº¦ã€é²æ£’æ€§æˆ–ç»Ÿè®¡åŸºç¡€çš„åŸºå‡†æµ‹è¯•å®è·µã€‚æœ¬æ–‡é’ˆå¯¹æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¿™äº›é—®é¢˜ï¼Œè¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œå‘ç°å½“å‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å¯¹å¾®å¦™çš„å®ç°é€‰æ‹©é«˜åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬è§£ç å‚æ•°ã€éšæœºç§å­ã€æç¤ºæ ¼å¼ï¼Œç”šè‡³ç¡¬ä»¶å’Œè½¯ä»¶æ¡†æ¶é…ç½®ã€‚è¿™å¯¼è‡´äº†è®¸å¤šç ”ç©¶ä¸­æŠ¥å‘Šçš„æ€§èƒ½æå‡å»ºç«‹åœ¨ä¸æ¸…æ™°çš„æ¯”è¾ƒæˆ–æœªæŠ¥å‘Šçš„æ–¹å·®æ¥æºä¸Šã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€å¥—æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«äº†æ˜ç¡®å®šä¹‰çš„æœ€ä½³å®è·µå’ŒæŠ¥å‘Šæ ‡å‡†ã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œä½œè€…é‡æ–°è¯„ä¼°äº†æœ€è¿‘çš„æ–¹æ³•ï¼Œå¹¶å‘ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä»…å¸¦æ¥å¾®å°çš„æ”¹è¿›ï¼Œè¿œä½äºä¹‹å‰çš„å®£ç§°ï¼Œå¹¶ä¸”å®¹æ˜“åœ¨å°è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸Šè¿‡æ‹Ÿåˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä½œè€…å¯¹æ¯”äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å’ŒRLæ–¹æ³•ï¼Œå‘ç°SFTæ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°æ›´ä¸ºç¨³å®šå’Œæ™®éã€‚ä¸ºäº†ä¿ƒè¿›å¯å¤ç°æ€§ï¼Œæœ¬æ–‡å…¬å¼€äº†æ‰€æœ‰ä»£ç ã€æç¤ºå’Œæ¨¡å‹è¾“å‡ºï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†æ›´ä¸¥è°¨çš„åŸºç¡€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¯¹æœ€è¿‘çš„æ–¹æ³•è¿›è¡Œæ ‡å‡†åŒ–è¯„ä¼°ï¼Œæœ¬æ–‡å‘ç°RLæ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡å¹¶ä¸æ˜¾è‘—ï¼Œè€Œä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSFTæ–¹æ³•åœ¨å„ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½è¡¨ç°å‡ºäº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°ï¼Œç”±äºæ ·æœ¬é‡å°ï¼ŒåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æŒ‡æ ‡éå¸¸ä¸ç¨³å®šï¼Œä¸€ä¸ªé—®é¢˜çš„å˜åŒ–å°±å¯èƒ½å¯¼è‡´æ€§èƒ½æŒ‡æ ‡å‡ºç°è¶…è¿‡3ä¸ªç™¾åˆ†ç‚¹çš„æ³¢åŠ¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æé†’äº†æˆ‘ä»¬åœ¨è¯„ä¼°æ·±åº¦è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ—¶éœ€è¦æ³¨æ„çš„å‡ ä¸ªå…³é”®ç‚¹ï¼š
- è¯„ä¼°å®è·µéœ€è¦æ›´åŠ é€æ˜ã€é²æ£’å’Œæœ‰ç»Ÿè®¡åŸºç¡€ã€‚
- å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½æå‡å¯èƒ½è¢«é«˜ä¼°ï¼Œéœ€è¦è°¨æ…å¯¹å¾…ã€‚
- ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸Šæ›´ä¸ºå¯é ã€‚
- ä¸ºäº†æé«˜ç ”ç©¶çš„å¯å¤ç°æ€§ï¼Œåº”è¯¥å…¬å¼€æ‰€æœ‰ç›¸å…³ä»£ç ã€æ•°æ®å’Œæ¨¡å‹è¾“å‡ºã€‚
è¿™äº›å‘ç°å¯¹äºæœªæ¥è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç ”ç©¶å…·æœ‰é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚

## sailing-ai-by-the-stars--a-survey-of-learning-from-rewards-in-post-training-and-test-time-scaling-of-large-language-models
### Abstract
Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities. In this survey,
we present a comprehensive overview of the paradigm of learning from rewards.
We categorize and analyze the strategies under this paradigm across training,
inference, and post-inference stages. We further discuss the benchmarks for
reward models and the primary applications. Finally we highlight the challenges
and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "å¼•é¢†å¤§å‹è¯­è¨€æ¨¡å‹è¿›åŒ–çš„æ˜Ÿè¾°å¤§æµ·ï¼šå¥–åŠ±å­¦ä¹ å…¨é¢è§£æ"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚Chat-GPTã€Claudeå’ŒLlamaç­‰å–å¾—äº†é£é€Ÿå‘å±•ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒè§„æ¨¡åŒ–ï¼ˆpre-training scalingï¼‰åœ¨å¤§é‡è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†å¹¿æ³›çš„è¯­è¨€å’ŒçŸ¥è¯†è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸€äº›æ ¹æœ¬æ€§é™åˆ¶ï¼ŒåŒ…æ‹¬ä¸äººç±»ä»·å€¼è§‚çš„é”™ä½ã€éš¾ä»¥é€‚åº”ä¸åŒä»»åŠ¡ç›®æ ‡ä»¥åŠæ·±åº¦æ¨ç†èƒ½åŠ›çš„ä¸è¶³ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œç ”ç©¶è½¬å‘äº†åè®­ç»ƒè§„æ¨¡åŒ–ï¼ˆpost-training scalingï¼‰å’Œæµ‹è¯•æ—¶è§„æ¨¡åŒ–ï¼ˆtest-time scalingï¼‰ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–LLMã€‚æœ¬æ–‡ç»¼è¿°äº†å¥–åŠ±å­¦ä¹ ï¼ˆLearning from Rewardsï¼‰è¿™ä¸€å…³é”®ç»Ÿä¸€èŒƒå¼ï¼Œå®ƒåœ¨åè®­ç»ƒå’Œæµ‹è¯•æ—¶è§„æ¨¡åŒ–ä¸­æ‰®æ¼”äº†æ ¸å¿ƒè§’è‰²ï¼Œé€šè¿‡å¥–åŠ±ä¿¡å·æ¥å¼•å¯¼æ¨¡å‹è¡Œä¸ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¦‚å¿µæ¡†æ¶ï¼Œç”¨äºç†è§£å¥–åŠ±å­¦ä¹ ç³»ç»Ÿçš„å…³é”®ç»„ä»¶å’Œäº¤äº’ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†è¯­è¨€æ¨¡å‹ã€å¥–åŠ±æ¨¡å‹å’Œå­¦ä¹ ç­–ç•¥ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼Œä¸ºç†è§£å’Œåˆ†ç±»ç°æœ‰æ–¹æ³•æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è§†è§’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æ–‡ç« è¯¦ç»†åˆ†ç±»äº†ç°æœ‰æ–¹æ³•åœ¨å¥–åŠ±æ¥æºã€å¥–åŠ±æ¨¡å‹è®¾è®¡ã€å¥–åŠ±å­¦ä¹ æ—¶æœºå’Œå­¦ä¹ ç­–ç•¥æ–¹é¢çš„å˜åŒ–ã€‚å¥–åŠ±æ¥æºåŒ…æ‹¬äººç±»åé¦ˆå’Œè‡ªåŠ¨åŒ–åé¦ˆï¼Œå¥–åŠ±æ¨¡å‹è®¾è®¡æ¶‰åŠæ¨¡å‹æ¶æ„ã€å¥–åŠ±æ ¼å¼ã€è¯„åˆ†æ¨¡å¼å’Œå¥–åŠ±ç²’åº¦ç­‰å¤šä¸ªç»´åº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ç»¼è¿°äº†åœ¨ä¸åŒé˜¶æ®µï¼ˆè®­ç»ƒã€æ¨ç†å’Œåæ¨ç†ï¼‰ä½¿ç”¨å¥–åŠ±å­¦ä¹ çš„ä»£è¡¨æ€§æŠ€æœ¯ï¼Œå¹¶æ€»ç»“äº†æœ€è¿‘å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•çš„ç»“æœã€‚è¿™äº›æŠ€æœ¯å’ŒåŸºå‡†æµ‹è¯•å±•ç¤ºäº†å¥–åŠ±å­¦ä¹ åœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šæ¨¡æ€ã€ä»£ç†å’Œå…·èº«AIç­‰å¤šä¸ªåº”ç”¨é¢†åŸŸçš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„å¥–åŠ±å­¦ä¹ ç»¼è¿°ï¼Œå¯¹äºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒå’Œæµ‹è¯•æ—¶è§„æ¨¡åŒ–å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- å¥–åŠ±å­¦ä¹ èŒƒå¼ä¸ºLLMæä¾›äº†ä»é™æ€æ•°æ®è¢«åŠ¨å­¦ä¹ åˆ°åŠ¨æ€åé¦ˆä¸»åŠ¨å­¦ä¹ çš„è½¬å˜ï¼Œä½¿å…¶å…·å¤‡äº†å¯¹é½çš„åå¥½å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚
- ç»Ÿä¸€çš„æ¦‚å¿µæ¡†æ¶æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å¥–åŠ±å­¦ä¹ ç³»ç»Ÿçš„è®¾è®¡å’Œè¿ä½œã€‚
- å¯¹å¥–åŠ±æ¨¡å‹è®¾è®¡çš„åˆ†ç±»å’Œè®¨è®ºä¸ºæœªæ¥ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ¸…æ™°çš„æŒ‡å¯¼ã€‚
- è¯†åˆ«äº†å¥–åŠ±å­¦ä¹ é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶è€…æä¾›äº†å®è´µçš„å‚è€ƒã€‚

## heimdall--test-time-scaling-on-the-generative-verification
### Abstract
An AI system can create and maintain knowledge only to the extent that it can
verify that knowledge itself. Recent work on long Chain-of-Thought reasoning
has demonstrated great potential of LLMs on solving competitive problems, but
their verification ability remains to be weak and not sufficiently
investigated. In this paper, we propose Heimdall, the long CoT verification LLM
that can accurately judge the correctness of solutions. With pure reinforcement
learning, we boost the verification accuracy from 62.5% to 94.5% on competitive
math problems. By scaling with repeated sampling, the accuracy further
increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive
generalization capabilities, successfully detecting most issues in challenging
math proofs, the type of which is not included during training. Furthermore, we
propose Pessimistic Verification to extend the functionality of Heimdall to
scaling up the problem solving. It calls Heimdall to judge the solutions from a
solver model and based on the pessimistic principle, selects the most likely
correct solution with the least uncertainty. Taking
DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification
improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute
budget and to 83.3% with more compute budget. With the stronger solver Gemini
2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge
discovery system, a ternary system where one poses questions, another provides
solutions, and the third verifies the solutions. Using the data synthesis work
NuminaMath for the first two components, Heimdall effectively identifies
problematic records within the dataset and reveals that nearly half of the data
is flawed, which interestingly aligns with the recent ablation studies from
NuminaMath.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œHeimdallï¼šåˆ©ç”¨é•¿é“¾æ¨ç†æå‡ç”ŸæˆéªŒè¯çš„å‡†ç¡®æ€§â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼ŒçŸ¥è¯†çš„åˆ›å»ºä¸ç»´æŠ¤ä¾èµ–äºç³»ç»Ÿçš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘çš„é•¿é“¾æ¨ç†ï¼ˆCoTï¼‰ç ”ç©¶å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šçš„å·¨å¤§æ½œåŠ›ï¼Œä½†è¿™äº›æ¨¡å‹çš„éªŒè¯èƒ½åŠ›ä»ç„¶è¾ƒå¼±ï¼Œä¸”æœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºHeimdallçš„é•¿é“¾æ¨ç†éªŒè¯LLMï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒäº†Heimdallæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†éªŒè¯å‡†ç¡®æ€§ã€‚åœ¨ç«äº‰æ€§æ•°å­¦é—®é¢˜ä¸Šï¼ŒéªŒè¯å‡†ç¡®æ€§ä»62.5%æé«˜åˆ°äº†94.5%ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡å¤é‡‡æ ·å’Œå¤šæ•°æŠ•ç¥¨ï¼Œå‡†ç¡®æ€§è¿›ä¸€æ­¥å¢åŠ åˆ°äº†97.5%ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡æå‡ºäº†æ‚²è§‚éªŒè¯ï¼ˆPessimistic Verificationï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡è°ƒç”¨Heimdallå¯¹æ±‚è§£å™¨æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œåˆ¤æ–­ï¼Œå¹¶æ ¹æ®æ‚²è§‚åŸåˆ™é€‰æ‹©æœ€å¯èƒ½æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æœ€å°åŒ–é€‰æ‹©é”™è¯¯è§£å†³æ–¹æ¡ˆçš„ä¸ç¡®å®šæ€§ã€‚ä½¿ç”¨DeepSeek-R1-Distill-Qwen-32Bä½œä¸ºæ±‚è§£å™¨æ¨¡å‹ï¼Œæ‚²è§‚éªŒè¯å°†AIME2025çš„è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§ä»54.2%æé«˜åˆ°äº†70.0%ï¼Œåœ¨æ›´å¤šè®¡ç®—èµ„æºä¸‹æé«˜åˆ°äº†83.3%ã€‚ä½¿ç”¨æ›´å¼ºå¤§çš„æ±‚è§£å™¨Gemini 2.5 Proï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†93.0%ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒHeimdallä¸ä»…åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”åœ¨æœªåŒ…å«åœ¨è®­ç»ƒä¸­çš„æŒ‘æˆ˜æ€§æ•°å­¦è¯æ˜é—®é¢˜ä¸Šä¹Ÿå±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸NuminaMathæ•°æ®åˆæˆå·¥ä½œçš„ç»“åˆï¼ŒHeimdallæœ‰æ•ˆåœ°è¯†åˆ«äº†æ•°æ®é›†ä¸­çš„é—®é¢˜è®°å½•ï¼Œæ­ç¤ºäº†è¿‘ä¸€åŠçš„æ•°æ®å­˜åœ¨ç¼ºé™·ï¼Œè¿™ä¸NuminaMathæœ€è¿‘çš„æ¶ˆèç ”ç©¶ç›¸ç¬¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºä»¥ä¸‹æ–¹é¢æä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒï¼š
- é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒé•¿é“¾æ¨ç†éªŒè¯æ¨¡å‹ï¼Œæé«˜äº†LLMçš„éªŒè¯èƒ½åŠ›ã€‚
- åˆ©ç”¨æ‚²è§‚éªŒè¯ç®—æ³•ï¼Œä¼˜åŒ–äº†è§£å†³æ–¹æ¡ˆçš„é€‰æ‹©è¿‡ç¨‹ï¼Œæé«˜äº†é—®é¢˜è§£å†³çš„æ•´ä½“å‡†ç¡®æ€§ã€‚
- åœ¨è‡ªåŠ¨çŸ¥è¯†å‘ç°ç³»ç»Ÿä¸­ï¼ŒHeimdallçš„åº”ç”¨å±•ç¤ºäº†å…¶åœ¨è¯†åˆ«åˆæˆæ•°æ®é›†ä¸­çš„é”™è¯¯æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ•°æ®è´¨é‡è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’ã€‚

## mesh-rft--enhancing-mesh-generation-via-fine-grained-reinforcement-fine-tuning
### Abstract
Existing pretrained models for 3D mesh generation often suffer from data
biases and produce low-quality results, while global reinforcement learning
(RL) methods rely on object-level rewards that struggle to capture local
structure details. To address these challenges, we present \textbf{Mesh-RFT}, a
novel fine-grained reinforcement fine-tuning framework that employs Masked
Direct Preference Optimization (M-DPO) to enable localized refinement via
quality-aware face masking. To facilitate efficient quality evaluation, we
introduce an objective topology-aware scoring system to evaluate geometric
integrity and topological regularity at both object and face levels through two
metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating
these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first
method to optimize mesh quality at the granularity of individual faces,
resolving localized errors while preserving global coherence. Experiment
results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\%
and improves Topology Score (TS) by 3.8\% over pre-trained models, while
outperforming global DPO methods with a 17.4\% HD reduction and 4.9\% TS gain.
These results demonstrate Mesh-RFT's ability to improve geometric integrity and
topological regularity, achieving new state-of-the-art performance in
production-ready mesh generation. Project Page:
\href{https://hitcslj.github.io/mesh-rft/}{this https URL}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç²¾ç»†åŒ–å¼ºåŒ–å­¦ä¹ æå‡ç½‘æ ¼ç”Ÿæˆè´¨é‡ï¼šMesh-RFTæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰çš„3Dç½‘æ ¼ç”Ÿæˆæ¨¡å‹å¾€å¾€å­˜åœ¨æ•°æ®åå·®é—®é¢˜ï¼Œç”Ÿæˆçš„ç½‘æ ¼è´¨é‡è¾ƒä½ã€‚è€Œå…¨å±€å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºå¯¹è±¡çº§åˆ«çš„å¥–åŠ±ä¿¡å·ï¼Œéš¾ä»¥æ•æ‰åˆ°å±€éƒ¨ç»“æ„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Mesh-RFTï¼Œä¸€ä¸ªæ–°é¢–çš„ç»†ç²’åº¦å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡è´¨é‡æ„ŸçŸ¥çš„é¢é®è”½å’ŒMasked Direct Preference Optimization (M-DPO)æŠ€æœ¯ï¼Œå®ç°å¯¹ç½‘æ ¼è´¨é‡çš„å±€éƒ¨åŒ–ä¼˜åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
å¼•å…¥äº†å®¢è§‚çš„æ‹“æ‰‘æ„ŸçŸ¥è¯„åˆ†ç³»ç»Ÿï¼Œé€šè¿‡è¾¹ç•Œè¾¹æ¯”ï¼ˆBERï¼‰å’Œæ‹“æ‰‘å¾—åˆ†ï¼ˆTSï¼‰ä¸¤ä¸ªæŒ‡æ ‡ï¼Œå®¢è§‚è¯„ä¼°ç½‘æ ¼çš„å‡ ä½•å®Œæ•´æ€§å’Œæ‹“æ‰‘è§„åˆ™æ€§ï¼Œæ¶ˆé™¤äº†å¯¹äººå·¥æ³¨é‡Šçš„ä¾èµ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
é‡‡ç”¨M-DPOå’Œè´¨é‡æ„ŸçŸ¥é®è”½çš„å±€éƒ¨ä¼˜åŒ–æœºåˆ¶ï¼Œä¸“é—¨é’ˆå¯¹æœ‰ç¼ºé™·çš„åŒºåŸŸè¿›è¡Œå‡ ä½•å’Œæ‹“æ‰‘ä¼˜åŒ–ï¼Œè§£å†³äº†å…¨å±€å¥–åŠ±ä¿¡å·ç²—ç³™ç›‘ç£çš„é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡çš„M-DPOæ–¹æ³•ç›¸æ¯”é¢„è®­ç»ƒæ¨¡å‹ï¼ŒHausdorffè·ç¦»ï¼ˆHDï¼‰å‡å°‘äº†24.6%ï¼Œæ‹“æ‰‘å¾—åˆ†ï¼ˆTSï¼‰æé«˜äº†3.8%ã€‚è€Œä¸å…¨å±€DPOæ–¹æ³•ç›¸æ¯”ï¼ŒHausdorffè·ç¦»å‡å°‘äº†17.4%ï¼Œæ‹“æ‰‘å¾—åˆ†æé«˜äº†4.9%ã€‚è¿™äº›ç»“æœè¯æ˜äº†Mesh-RFTåœ¨æé«˜å‡ ä½•å®Œæ•´æ€§å’Œæ‹“æ‰‘è§„åˆ™æ€§æ–¹é¢çš„èƒ½åŠ›ï¼Œå®ç°äº†ç”Ÿæˆå°±ç»ªç½‘æ ¼çš„æ–°ä¸€ä»£æœ€ä½³æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Mesh-RFTæ¡†æ¶ä¸º3Dç½‘æ ¼ç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„ç»†ç²’åº¦ä¼˜åŒ–æ–¹æ³•ï¼Œä¸ä»…æé«˜äº†ç½‘æ ¼çš„è´¨é‡ï¼Œè¿˜å‡å°‘äº†äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚å…¶åˆ›æ–°çš„æ‹“æ‰‘æ„ŸçŸ¥è¯„åˆ†ç³»ç»Ÿå’Œå±€éƒ¨ä¼˜åŒ–æœºåˆ¶å¯¹äºå…¶ä»–3Dç”Ÿæˆä»»åŠ¡ä¹Ÿå…·æœ‰å€Ÿé‰´æ„ä¹‰ï¼Œæœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨å‘å±•ã€‚

## totrl--unlock-llm-tree-of-thoughts-reasoning-potential-through-puzzles-solving
### Abstract
Large language models (LLMs) demonstrate significant reasoning capabilities,
particularly through long chain-of-thought (CoT) processes, which can be
elicited by reinforcement learning (RL). However, prolonged CoT reasoning
presents limitations, primarily verbose outputs due to excessive introspection.
The reasoning process in these LLMs often appears to follow a trial-and-error
methodology rather than a systematic, logical deduction. In contrast,
tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling
reasoning as an exploration within a tree structure. This reasoning structure
facilitates the parallel generation and evaluation of multiple reasoning
branches, allowing for the active identification, assessment, and pruning of
unproductive paths. This process can potentially lead to improved performance
and reduced token costs. Building upon the long CoT capability of LLMs, we
introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a
rule-based reward. ToTRL is designed to guide LLMs in developing the parallel
ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs
as players in a puzzle game during the ToTRL training process. Solving puzzle
games inherently necessitates exploring interdependent choices and managing
multiple constraints, which requires the construction and exploration of a
thought tree, providing challenging tasks for cultivating the ToT reasoning
capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,
trained with our ToTRL, achieves significant improvement in performance and
reasoning efficiency on complex reasoning tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£é”å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ€ç»´æ ‘æ¨ç†æ½œåŠ›ï¼šé€šè¿‡è§£è°œæ¸¸æˆæå‡æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯é€šè¿‡é•¿é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ç§é•¿æ—¶é—´çš„CoTæ¨ç†å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦è¡¨ç°ä¸ºç”±äºè¿‡åº¦å†…çœå¯¼è‡´çš„å†—é•¿è¾“å‡ºã€‚æ­¤å¤–ï¼ŒLLMçš„æ¨ç†è¿‡ç¨‹å¾€å¾€éµå¾ªè¯•é”™æ–¹æ³•ï¼Œè€Œéç³»ç»Ÿæ€§çš„é€»è¾‘æ¼”ç»ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæ€ç»´æ ‘ï¼ˆToTï¼‰çš„æ¨ç†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨æ ‘ç»“æ„ä¸­æ¢ç´¢æ½œåœ¨æ€ç»´æˆ–çŠ¶æ€ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å¼•å…¥äº†æ ‘çŠ¶æ€ç»´å¼ºåŒ–å­¦ä¹ ï¼ˆToTRLï¼‰ï¼Œä¸€ç§æ–°é¢–çš„on-policy RLæ¡†æ¶ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ã€‚ToTRLæ—¨åœ¨æŒ‡å¯¼LLMåŸºäºé¡ºåºCoTç­–ç•¥å‘å±•å¹¶è¡Œçš„ToTç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†åŸ¹å…»LLMçš„ToTæ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡å°†LLMä½œä¸ºè§£è°œæ¸¸æˆä¸­çš„ç©å®¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§£å†³è°œé¢˜ã€‚è§£è°œæ¸¸æˆéœ€è¦æ¢ç´¢ç›¸äº’ä¾èµ–çš„é€‰æ‹©å’Œç®¡ç†å¤šä¸ªçº¦æŸï¼Œè¿™ä¸ºåŸ¹å…»ToTæ¨ç†èƒ½åŠ›æä¾›äº†æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å®éªŒè¯„ä¼°ï¼Œæœ¬æ–‡è®­ç»ƒçš„ToTQwen3-8Bæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å’Œæ¨ç†æ•ˆç‡æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ToTRLæ¡†æ¶å’Œè®­ç»ƒç­–ç•¥ä¸ºLLMæä¾›äº†æ–°çš„æ¨ç†æ–¹æ³•ï¼Œæœ‰åŠ©äºè§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- åˆ©ç”¨è§„åˆ™å¥–åŠ±çš„on-policy RLç®—æ³•ï¼Œæœ‰æ•ˆæŒ‡å¯¼LLMå‘å±•ToTæ¨ç†ç­–ç•¥ã€‚
- é€šè¿‡è§£è°œæ¸¸æˆè®­ç»ƒLLMï¼ŒåŸ¹å…»å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ToTæ¨ç†èƒ½åŠ›ã€‚
- å®éªŒè¯æ˜äº†ToTæ¨ç†åœ¨æé«˜æ¨ç†æ•ˆç‡å’Œå‡å°‘å†—ä½™æ¢ç´¢æ–¹é¢çš„æ½œåŠ›ã€‚

## scaling-reasoning--losing-control--evaluating-instruction-following-in-large-reasoning-models
### Abstract
Instruction-following is essential for aligning large language models (LLMs)
with user intent. While recent reasoning-oriented models exhibit impressive
performance on complex mathematical problems, their ability to adhere to
natural language instructions remains underexplored. In this work, we introduce
MathIF, a dedicated benchmark for evaluating instruction-following in
mathematical reasoning tasks. Our empirical analysis reveals a consistent
tension between scaling up reasoning capacity and maintaining controllability,
as models that reason more effectively often struggle to comply with user
directives. We find that models tuned on distilled long chains-of-thought or
trained with reasoning-oriented reinforcement learning often degrade in
instruction adherence, especially when generation length increases.
Furthermore, we show that even simple interventions can partially recover
obedience, though at the cost of reasoning performance. These findings
highlight a fundamental tension in current LLM training paradigms and motivate
the need for more instruction-aware reasoning models. We release the code and
data at https://github.com/TingchenFu/MathIF.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ï¼Œå´ä¸¢äº†æ§åˆ¶ï¼Ÿæ¢ç©¶å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„æŒ‡ä»¤éµå¾ªé—®é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ•°å­¦æ¨ç†é¢†åŸŸçš„çªç ´æ€§è¿›å±•ï¼Œå®ƒä»¬åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜ä¸Šçš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢çš„è¡¨ç°å´é²œå°‘è¢«ç ”ç©¶ã€‚æœ¬æ–‡çš„åŠ¨æœºåœ¨äºæ¢ç©¶éšç€æ¨ç†èƒ½åŠ›çš„å¢å¼ºï¼Œæ¨¡å‹æ˜¯å¦å˜å¾—æ›´åŠ æ™ºèƒ½ä½†åŒæ—¶ä¹Ÿæ›´éš¾ä»¥æ§åˆ¶ï¼Œä»¥åŠè¿™ä¸€ç°è±¡èƒŒåçš„åŸå› ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å¼•å…¥äº†MathIFï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°æ•°å­¦é¢†åŸŸä¸­å¤§å‹æ¨ç†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›è€Œè®¾è®¡çš„åŸºå‡†ã€‚MathIFé€šè¿‡ç¨‹åºåŒ–ç»„åˆ15ç§Pythonå¯éªŒè¯çš„çº¦æŸæ¡ä»¶ï¼Œåˆ›å»ºäº†30ä¸ªåŒçº¦æŸå’Œ15ä¸ªä¸‰çº¦æŸçš„æç¤ºï¼Œåº”ç”¨äºä¸åŒéš¾åº¦çš„æ•°å­¦é—®é¢˜ï¼Œå½¢æˆäº†æ€»å…±420ä¸ªé«˜è´¨é‡çš„è¯„ä¼°æ ·æœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
é€šè¿‡å¯¹23ä¸ªä¸åŒè§„æ¨¡å’Œæ¶æ„çš„LRMsè¿›è¡Œè¯„ä¼°ï¼Œæœ¬æ–‡å‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢å­˜åœ¨æ™®éé—®é¢˜ï¼Œä¸”æ€§èƒ½å¹¶ä¸éšæ¨¡å‹è§„æ¨¡çš„å¢å¤§è€Œä¸€è‡´æå‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ï¼Œè¿™ç§å¹²æ‰°åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½æœ‰æ‰€ä½“ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹Qwen3-14Bï¼Œåœ¨ä¸¥æ ¼çš„æŒ‡ä»¤éµå¾ªæ–¹é¢çš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰50.71%ã€‚éšç€ä»»åŠ¡éš¾åº¦å’Œçº¦æŸå¤æ‚æ€§çš„å¢åŠ ï¼Œæ€§èƒ½è¿›ä¸€æ­¥ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºæœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°ï¼Œå¸¸è§çš„æ¨ç†å¯¼å‘çš„è®­ç»ƒç­–ç•¥ï¼ˆå¦‚ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼‰è™½ç„¶å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†ä¼šé™ä½æŒ‡ä»¤éµå¾ªåº¦ï¼Œå°¤å…¶æ˜¯å½“æ¨ç†é“¾é•¿åº¦å¢åŠ æ—¶ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤§å‹æ¨ç†æ¨¡å‹è®­ç»ƒèŒƒå¼ä¸­çš„åŸºæœ¬çŸ›ç›¾ï¼Œå³æ¨ç†èƒ½åŠ›çš„æå‡å¾€å¾€ä»¥æŒ‡ä»¤éµå¾ªåº¦çš„é™ä½ä¸ºä»£ä»·ã€‚è¿™ä¸€å‘ç°å¯¹äºæœªæ¥å¤§å‹æ¨¡å‹çš„å‘å±•å…·æœ‰é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ï¼Œæç¤ºæˆ‘ä»¬éœ€è¦å¼€å‘æ›´åŠ æ³¨é‡æŒ‡ä»¤éµå¾ªçš„æ¨ç†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒMathIFåŸºå‡†çš„æå‡ºä¸ºè¯„ä¼°å’Œæ”¹è¿›å¤§å‹æ¨ç†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„å·¥å…·ï¼Œå¯¹äºç›¸å…³é¢†åŸŸçš„ç ”ç©¶è€…å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## sophiavl-r1--reinforcing-mllms-reasoning-with-thinking-reward
### Abstract
Recent advances have shown success in eliciting strong reasoning abilities in
multimodal large language models (MLLMs) through rule-based reinforcement
learning (RL) with outcome rewards. However, this paradigm typically lacks
supervision over the thinking process leading to the final outcome.As a result,
the model may learn sub-optimal reasoning strategies, which can hinder its
generalization ability. In light of this, we propose SophiaVL-R1, as an attempt
to add reward signals for the thinking process in this paradigm. To achieve
this, we first train a thinking reward model that evaluates the quality of the
entire thinking process. Given that the thinking reward may be unreliable for
certain samples due to reward hacking, we propose the Trust-GRPO method, which
assigns a trustworthiness weight to the thinking reward during training. This
weight is computed based on the thinking reward comparison of responses leading
to correct answers versus incorrect answers, helping to mitigate the impact of
potentially unreliable thinking rewards. Moreover, we design an annealing
training strategy that gradually reduces the thinking reward over time,
allowing the model to rely more on the accurate rule-based outcome reward in
later training stages. Experiments show that our SophiaVL-R1 surpasses a series
of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),
demonstrating strong reasoning and generalization capabilities. Notably, our
SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite
the latter having 10 times more parameters. All code, models, and datasets are
made publicly available at https://github.com/kxfan2002/SophiaVL-R1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SophiaVL-R1ï¼šé€šè¿‡æ€è€ƒå¥–åŠ±å¼ºåŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠç»“æœå¥–åŠ±ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼é€šå¸¸ç¼ºä¹å¯¹æ€è€ƒè¿‡ç¨‹çš„ç›‘ç£ï¼Œå¯¼è‡´æ¨¡å‹å¯èƒ½å­¦ä¹ åˆ°æ¬¡ä¼˜çš„æ¨ç†ç­–ç•¥ï¼Œä»è€Œå½±å“å…¶æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SophiaVL-R1æ¨¡å‹ï¼Œè¯•å›¾åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥å¯¹æ€è€ƒè¿‡ç¨‹çš„å¥–åŠ±ä¿¡å·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªæ€è€ƒå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»æ•´ä½“ä¸Šè¯„ä¼°æ•´ä¸ªæ€è€ƒè¿‡ç¨‹çš„è´¨é‡ã€‚ä¸ä¼ ç»Ÿçš„é€æ­¥éª¤æä¾›åé¦ˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä¸åŒï¼ŒSophiaVL-R1çš„æ€è€ƒå¥–åŠ±æ¨¡å‹ä»é€»è¾‘åˆç†æ€§ã€æ­¥éª¤ä¸€è‡´æ€§ä»¥åŠæ€è€ƒè¿‡ç¨‹ä¸­çš„å†—ä½™ç­‰å¤šä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è€ƒè™‘åˆ°æ¨¡å‹ç”Ÿæˆçš„æ€è€ƒå¥–åŠ±å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹ä¸å¯é ï¼Œæœ¬æ–‡æå‡ºäº†Trust-GRPOæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºæ€è€ƒå¥–åŠ±åˆ†é…ä¸€ä¸ªä¿¡ä»»åº¦æƒé‡ï¼Œæ¥è¯„ä¼°å¥–åŠ±çš„å¯é æ€§ã€‚è¿™ä¸ªæƒé‡æ˜¯é€šè¿‡æ¯”è¾ƒæ­£ç¡®ç­”æ¡ˆå’Œé”™è¯¯ç­”æ¡ˆçš„æ€è€ƒå¥–åŠ±æ¥ç¡®å®šçš„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªé€€ç«è®­ç»ƒç­–ç•¥ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸å‡å°‘æ€è€ƒå¥–åŠ±çš„å½±å“ï¼Œä½¿æ¨¡å‹åœ¨åæœŸæ›´å¤šåœ°ä¾èµ–å‡†ç¡®çš„ç»“æœå¥–åŠ±ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒSophiaVL-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVisitaã€MMMUï¼‰ä¸Šè¶…è¿‡äº†å…¶ä»–ä¸€ç³»åˆ—æ¨ç†MLLMsï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒSophiaVL-R1-7Bç”šè‡³åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šè¶…è¿‡äº†å‚æ•°æ•°é‡æ˜¯å…¶10å€çš„LLaVA-OneVision-72Bã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„SophiaVL-R1æ¨¡å‹ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå³é€šè¿‡åŠ å…¥å¯¹æ€è€ƒè¿‡ç¨‹çš„å¥–åŠ±ä¿¡å·æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´åˆç†çš„æ¨ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒTrust-GRPOæ–¹æ³•ä¸ºå¤„ç†æ¨¡å‹ç”Ÿæˆå¥–åŠ±çš„ä¸å¯é æ€§æä¾›äº†ä¸€ç§æœ‰æ•ˆæ‰‹æ®µï¼Œè¿™äº›æ–¹æ³•å¯¹äºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†è´¨é‡å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## speculative-thinking--enhancing-small-model-reasoning-with-large-model-guidance-at-inference-time
### Abstract
Recent advances leverage post-training to enhance model reasoning
performance, which typically requires costly training pipelines and still
suffers from inefficient, overly lengthy outputs. We introduce Speculative
Thinking, a training-free framework that enables large reasoning models to
guide smaller ones during inference at the reasoning level, distinct from
speculative decoding, which operates at the token level. Our approach is based
on two observations: (1) reasoning-supportive tokens such as "wait" frequently
appear after structural delimiters like "\n\n", serving as signals for
reflection or continuation; and (2) larger models exhibit stronger control over
reflective behavior, reducing unnecessary backtracking while improving
reasoning quality. By strategically delegating reflective steps to a more
capable model, our method significantly boosts the reasoning accuracy of
reasoning models while shortening their output. With the assistance of the 32B
reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to
89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average
output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%
decrease. Moreover, when applied to a non-reasoning model
(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%
on the same benchmark, achieving a relative improvement of 7.8%.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œä»¥å¤§å¸¦å°â€ï¼šæ¨ç†æ¨¡å‹æ¨ç†èƒ½åŠ›å¢å¼ºæ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å½“å‰çš„AIåº”ç”¨ä¸­ï¼Œå°å‹è¯­è¨€æ¨¡å‹å› è®¡ç®—å’Œå†…å­˜éœ€æ±‚è¾ƒä½è€Œè¢«å¹¿æ³›ä½¿ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ã€‚è™½ç„¶å¯ä»¥é€šè¿‡åè®­ç»ƒï¼ˆå¦‚ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼‰æ¥æå‡å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™äº›æ–¹æ³•æˆæœ¬é«˜æ˜‚ã€æ•°æ®å¯†é›†ä¸”éš¾ä»¥æ‰©å±•ã€‚ä¸ºäº†åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æå‡å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ¨ç†æ—¶å¢å¼ºæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†â€œSpeculative Thinkingâ€æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå®ƒå…è®¸å¤§å‹æ¨ç†æ¨¡å‹åœ¨æ¨ç†æ—¶æŒ‡å¯¼å°å‹æ¨¡å‹ï¼Œä»è€Œæå‡å…¶æ¨ç†æ€§èƒ½ã€‚ä¸åœ¨ä»¤ç‰Œçº§åˆ«æ“ä½œçš„æŠ•æœºè§£ç ä¸åŒï¼Œæœ¬æ–‡çš„æ–¹æ³•å…³æ³¨äºæ¨ç†çº§åˆ«ï¼Œå³å¤§å‹æ¨¡å‹åœ¨å…³é”®æ¨ç†æ­¥éª¤ä¸ŠæŒ‡å¯¼å°å‹æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è¯¥æ–¹æ³•åŸºäºä¸¤ä¸ªè§‚å¯Ÿï¼šé¦–å…ˆï¼Œæ¨ç†æ”¯æŒçš„ä»¤ç‰Œï¼ˆå¦‚â€œwaitâ€ï¼‰ç»å¸¸åœ¨ç»“æ„åˆ†éš”ç¬¦ï¼ˆå¦‚â€œ\n\nâ€ï¼‰ä¹‹åå‡ºç°ï¼Œä½œä¸ºåæ€æˆ–ç»§ç»­çš„ä¿¡å·ï¼›å…¶æ¬¡ï¼Œå¤§å‹æ¨¡å‹åœ¨æ§åˆ¶åæ€è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„å›æº¯ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†è´¨é‡ã€‚é€šè¿‡åŠ¨æ€æ£€æµ‹è¿™äº›ç‚¹å¹¶å°†åæ€æ­¥éª¤å§”æ‰˜ç»™èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ï¼ŒSpeculative Thinkingæ—¢ä¿æŒäº†å°å‹æ¨¡å‹çš„æ•ˆç‡ï¼Œåˆåœ¨å…¶æœ€éœ€è¦çš„åœ°æ–¹åˆ©ç”¨äº†å¤§å‹æ¨¡å‹çš„æ¨ç†ä¼˜åŠ¿ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨32Bæ¨ç†æ¨¡å‹çš„è¾…åŠ©ä¸‹ï¼Œ1.5Bæ¨¡å‹çš„MATH500å‡†ç¡®åº¦ä»83.2%æå‡è‡³89.4%ï¼Œæé«˜äº†6.2%ã€‚åŒæ—¶ï¼Œå¹³å‡è¾“å‡ºé•¿åº¦ä»5439ä¸ªä»¤ç‰Œå‡å°‘åˆ°4583ä¸ªä»¤ç‰Œï¼Œå‡å°‘äº†15.7%ã€‚æ­¤å¤–ï¼Œå½“è¯¥æ–¹æ³•åº”ç”¨äºéæ¨ç†æ¨¡å‹ï¼ˆQwen-2.5-7B-Instructï¼‰æ—¶ï¼Œå…¶åœ¨MATH500ä¸Šçš„å‡†ç¡®åº¦ä»74.0%æå‡è‡³81.8%ï¼Œç›¸å¯¹æé«˜äº†7.8%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Speculative Thinkingæ¡†æ¶ä¸ºå°å‹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¢å¼ºæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå®ƒé€šè¿‡ç»“åˆå°å‹æ¨¡å‹çš„æ•ˆç‡å’Œå¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„æ¨ç†å¢å¼ºæä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä¸ä»…é€‚ç”¨äºä¸“é—¨è®­ç»ƒçš„æ¨ç†æ¨¡å‹ï¼Œä¹Ÿé€‚ç”¨äºæœªç»è¿‡æ¨ç†è®­ç»ƒçš„æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚

## ovip--online-vision-language-preference-learning
### Abstract
Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œOViPï¼šå®æ—¶è§†è§‰-è¯­è¨€åå¥½å­¦ä¹ æ¡†æ¶ï¼Œå‡å°‘AIå¹»è§‰ç°è±¡â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†ä»ç„¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ä¸è§†è§‰è¾“å…¥ä¸åŒ¹é…ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶é€šè¿‡å¤šæ¨¡æ€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•æ¥å‡è½»å¹»è§‰é—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰æˆ–éšæœºç¼–è¾‘çš„è´Ÿæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å¹¶ä¸èƒ½çœŸå®åæ˜ æ¨¡å‹çš„é”™è¯¯ï¼Œä»è€Œé™åˆ¶äº†è®­ç»ƒæ•ˆæœã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨çº¿è§†è§‰-è¯­è¨€åå¥½å­¦ä¹ ï¼ˆOViPï¼‰æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
OViPæ¡†æ¶åŠ¨æ€æ„å»ºåŸºäºæ¨¡å‹è‡ªèº«å¹»è§‰è¾“å‡ºçš„å¯¹æ¯”è®­ç»ƒæ•°æ®ã€‚é€šè¿‡è¯†åˆ«é‡‡æ ·å“åº”å¯¹ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆæˆè´Ÿå›¾åƒï¼ŒOViPå®æ—¶ç”Ÿæˆæ›´ç›¸å…³çš„ç›‘ç£ä¿¡å·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
OViPæ¡†æ¶ç»“åˆäº†åœ¨çº¿åå¥½å­¦ä¹ å’Œå›¾åƒæ„ŸçŸ¥è®­ç»ƒï¼Œé€šè¿‡å®æ—¶é‡‡æ ·å’Œæ•´åˆæ–°çš„åå¥½å¯¹ï¼Œæ ¹æ®æ¨¡å‹å‡ºç°çš„æ–°çš„å¤±è´¥æ¨¡å¼è¿›è¡Œè‡ªé€‚åº”å­¦ä¹ ï¼Œä»è€Œå‡å°‘å¯¹é™æ€æ•°æ®é›†çš„ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†OViPæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸“é—¨é’ˆå¯¹å¹»è§‰å’Œä¸€èˆ¬æ€§çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOViPæ¡†æ¶åœ¨å‡å°‘å¹»è§‰çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„æ ¸å¿ƒå¤šæ¨¡æ€èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ”¹è¿›äº†ç°æœ‰çš„è¯„ä¼°åè®®ï¼Œå¼•å…¥äº†ä¸€ç§æ›´ç¨³å¥çš„è¯„ä¼°ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŒæ—¶è¯„ä¼°å¹»è§‰å’Œè§†è§‰-è¯­è¨€èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„OViPæ¡†æ¶ä¸ºå‡å°‘å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç°è±¡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å…¶åŠ¨æ€æ„å»ºå¯¹æ¯”è®­ç»ƒæ•°æ®å’Œå®æ—¶æ›´æ–°æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¯¹äºæé«˜æ¨¡å‹çš„è§†è§‰æ¥åœ°æ€§å’Œè¾“å‡ºè´¨é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹è¯„ä¼°åè®®çš„æ”¹è¿›ä¹Ÿä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†æ›´åŠ å…¨é¢å’Œå‡†ç¡®çš„è¯„ä¼°æ‰‹æ®µã€‚

## acereason-nemotron--advancing-math-and-code-reasoning-through-reinforcement-learning
### Abstract
Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ æå‡æ•°å­¦ä¸ä»£ç æ¨ç†èƒ½åŠ›ï¼šAceReason-Nemotron

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡è¿‘å¹´æ¥åœ¨æ¨ç†é¢†åŸŸçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ„å»ºé«˜æ€§èƒ½æ¨ç†æ¨¡å‹çš„è®­ç»ƒé…æ–¹ä»ç„¶ä¸æ˜ç¡®ã€‚å‰æ²¿æ¨¡å‹çš„å…³é”®å®ç°ç»†èŠ‚ï¼Œå¦‚æ•°æ®ç­›é€‰ç­–ç•¥å’ŒRLè®­ç»ƒé…æ–¹ï¼Œå¸¸å¸¸è¢«çœç•¥ã€‚æ­¤å¤–ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå°å‹æ¨¡å‹ï¼ŒçŸ¥è¯†è’¸é¦é€šå¸¸æ¯”RLæ›´æœ‰æ•ˆã€‚æœ¬æ–‡æ—¨åœ¨å±•ç¤ºå¤§è§„æ¨¡RLå¯ä»¥æ˜¾è‘—æå‡ä¸­å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°ç”šè‡³è¶…è¿‡æœ€å…ˆè¿›çš„åŸºäºè’¸é¦çš„æ¨¡å‹çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼šå…ˆå¯¹æ•°å­¦é—®é¢˜è¿›è¡ŒRLè®­ç»ƒï¼Œå†å¯¹ä»£ç é—®é¢˜è¿›è¡ŒRLè®­ç»ƒã€‚è¿™ä¸€æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº†æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼ˆä¾‹å¦‚ï¼Œ7Bæ¨¡å‹åœ¨AIME 2025ä¸Šçš„è¡¨ç°æå‡äº†14.6%ï¼Œ14Bæ¨¡å‹æå‡äº†17.2%ï¼‰ï¼Œè€Œä¸”åœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æå‡ï¼ˆä¾‹å¦‚ï¼Œ7Bæ¨¡å‹åœ¨LiveCodeBenchä¸Šçš„è¡¨ç°æå‡äº†6.8%ï¼Œ14Bæ¨¡å‹æå‡äº†5.8%ï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªç¨³å¥çš„æ•°æ®ç­›é€‰ç®¡é“ï¼Œç”¨äºæ”¶é›†å…·æœ‰é«˜è´¨é‡ã€å¯éªŒè¯ç­”æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œä»è€Œæ”¯æŒæ•°å­¦å’Œä»£ç é¢†åŸŸçš„éªŒè¯å¼RLã€‚æ­¤å¤–ï¼Œé€šè¿‡è¯¦ç»†çš„æ¶ˆèç ”ç©¶å’Œåˆ†æï¼Œæœ¬æ–‡æ­ç¤ºäº†å‡ ä¸ªå…³é”®å®éªŒè§è§£ï¼ŒåŒ…æ‹¬é€æ­¥å¢åŠ å“åº”é•¿åº¦çš„è¯¾ç¨‹å­¦ä¹ ä»¥åŠç­–ç•¥å‚æ•°æ›´æ–°çš„ç¨³å®šæ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒRLä¸ä»…æ¿€å‘äº†æ¨¡å‹åœ¨é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆå¦‚è’¸é¦ï¼‰æœŸé—´è·å¾—çš„åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ¨åŠ¨äº†æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æé™ï¼Œä½¿å…¶èƒ½å¤Ÿè§£å†³ä¹‹å‰æ— æ³•è§£å†³çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæ•°å­¦é—®é¢˜ä¸Šçš„RLè®­ç»ƒæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè€Œä»£ç é—®é¢˜ä¸Šçš„RLè®­ç»ƒè¿›ä¸€æ­¥æé«˜äº†ä»£ç ä»»åŠ¡çš„è¡¨ç°ï¼ŒåŒæ—¶å¯¹æ•°å­¦ä»»åŠ¡çš„æ€§èƒ½å½±å“æœ€å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºä¸­å°å‹æ¨¡å‹ï¼Œé€šè¿‡é€‚å½“çš„æ•°æ®ç­›é€‰å’Œè®­ç»ƒç­–ç•¥ï¼Œå¤§è§„æ¨¡RLå¯ä»¥æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å’Œç­–ç•¥å‚æ•°æ›´æ–°æ–¹æ³•ä¸ºç¨³å®šå’Œé«˜æ•ˆçš„RLè®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ã€‚æœ€åï¼Œæœ¬æ–‡çš„å¼€æºæ•°æ®é›†å’Œæ¨¡å‹å°†æœ‰åŠ©äºç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›æ¨ç†æ¨¡å‹ã€‚

## observe-r1--unlocking-reasoning-abilities-of-mllms-with-dynamic-progressive-reinforcement-learning
### Abstract
Reinforcement Learning (RL) has shown promise in improving the reasoning
abilities of Large Language Models (LLMs). However, the specific challenges of
adapting RL to multimodal data and formats remain relatively unexplored. In
this work, we present Observe-R1, a novel framework aimed at enhancing the
reasoning capabilities of multimodal large language models (MLLMs). We draw
inspirations from human learning progression--from simple to complex and easy
to difficult, and propose a gradual learning paradigm for MLLMs. To this end,
we construct the NeuraLadder dataset, which is organized and sampled according
to the difficulty and complexity of data samples for RL training. To tackle
multimodal tasks, we introduce a multimodal format constraint that encourages
careful observation of images, resulting in enhanced visual abilities and
clearer and more structured responses. Additionally, we implement a bonus
reward system that favors concise, correct answers within a length constraint,
alongside a dynamic weighting mechanism that prioritizes uncertain and
medium-difficulty problems, ensuring that more informative samples have a
greater impact on training. Our experiments with the Qwen2.5-VL-3B and
Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that
Observe-R1 outperforms a series of larger reasoning models on both reasoning
and general benchmarks, achieving superior clarity and conciseness in reasoning
chains. Ablation studies validate the effectiveness of our strategies,
highlighting the robustness and generalization of our approach. The dataset and
code will be released at https://github.com/zrguo/Observe-R1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£é”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼šåŠ¨æ€æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶Observe-R1

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æˆåŠŸï¼Œå¦‚ä½•å°†è¿™ç§æŠ€æœ¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æˆä¸ºä¸€ä¸ªæ–°çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„ç ”ç©¶è™½ç„¶è¯æ˜äº†RLå¯ä»¥å¢å¼ºMLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å°šæœªé’ˆå¯¹å¤šæ¨¡æ€æ•°æ®å’Œæ¨¡å‹çš„ç‰¹æ€§è¿›è¡Œæ·±å…¥ä¼˜åŒ–å’Œæ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºObserve-R1çš„æ¡†æ¶ï¼Œä»¥åŠ¨æ€æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼ï¼Œæå‡MLLMçš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å—åˆ°äººç±»å­¦ä¹ è¿›é˜¶çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªæ¸è¿›å¼å­¦ä¹ èŒƒå¼ï¼Œå³ä»ç®€å•åˆ°å¤æ‚ã€ä»å®¹æ˜“åˆ°å›°éš¾çš„å­¦ä¹ è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºNeuraLadderçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ ¹æ®é—®é¢˜æ ·æœ¬çš„éš¾åº¦å’Œå¤æ‚æ€§è¿›è¡Œç»„ç»‡å’Œé‡‡æ ·ï¼Œä»¥é€‚åº”RLè®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
é’ˆå¯¹å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€æ ¼å¼çº¦æŸï¼Œé¼“åŠ±æ¨¡å‹ä»”ç»†è§‚å¯Ÿå›¾åƒï¼Œä»è€Œæé«˜è§†è§‰èƒ½åŠ›ï¼Œå¹¶äº§ç”Ÿæ›´æ¸…æ™°ã€ç»“æ„åŒ–çš„å“åº”ã€‚åŒæ—¶ï¼Œå®æ–½äº†ä¸€ä¸ªå¥–åŠ±ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåå¥½ç®€æ´æ­£ç¡®çš„ç­”æ¡ˆï¼Œå¹¶è®¾ç½®äº†é•¿åº¦é™åˆ¶ï¼Œä»¥åŠä¸€ä¸ªåŸºäºæ¨¡å‹ä¸ç¡®å®šæ€§çš„åŠ¨æ€æƒé‡æœºåˆ¶ï¼Œç¡®ä¿æ›´ä¿¡æ¯ä¸°å¯Œçš„æ ·æœ¬åœ¨è®­ç»ƒä¸­äº§ç”Ÿæ›´å¤§çš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½¿ç”¨Qwen2.5-VL-3Bå’ŒQwen2.5-VL-7Bä½œä¸ºåŸºæ¨¡å‹ï¼Œåœ¨NeuraLadderæ•°æ®é›†çš„20kä¸ªæ ·æœ¬ä¸Šè¿›è¡Œå®éªŒï¼Œå¾—åˆ°çš„Observe-R1-3Bå’Œ7Bæ¨¡å‹åœ¨æ•°å­¦å’Œç§‘å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¶…è¿‡äº†7-11Bå‚æ•°çš„æ¨ç†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒObserve-R1åœ¨æ¨ç†é“¾ä¸Šè¡¨ç°å‡ºæ›´æ¸…æ™°ã€æ›´ç®€æ´çš„ç‰¹ç‚¹ã€‚æ¶ˆèç ”ç©¶ä¹ŸéªŒè¯äº†æœ¬æ–‡ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŠ¨æ€æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸ºæå‡MLLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡æ„å»ºä¸“é—¨çš„æ•°æ®é›†ã€å¼•å…¥å¤šæ¨¡æ€æ ¼å¼çº¦æŸå’ŒåŠ¨æ€æƒé‡æœºåˆ¶ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å®éªŒä¸­å–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„å¼€æºæ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ä¹Ÿä¸ºåç»­çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†ä¾¿åˆ©ã€‚

## solver-informed-rl--grounding-large-language-models-for-authentic-optimization-modeling
### Abstract
Optimization modeling is fundamental to decision-making across diverse
domains.Despite progress in automating optimization formulation from natural
language descriptions, Large Language Models (LLMs) often struggle to generate
formally correct and usable models due to hallucinations, posing a challenge
for reliable automation. Inspired by the success of Reinforcement Learning (RL)
in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement
Learning (SIRL).This novel framework leverages external optimization solvers as
verifiable reward mechanisms to significantly improve the authenticity of LLMs
for optimization modeling.Acting as precise verifiers, these solvers
automatically assess the executable code and the instance-level mathematical
model represented by the associated LP file, yielding precise and comprehensive
feedback signals -- including syntax, feasibility, and solution quality that
directly inform the RL process. This automated verification process, powered by
classic optimization solvers, also underpins our instance-enhanced
self-consistency method to synthesize high-quality training data. Extensive
experiments on diverse public benchmarks demonstrate that SIRL achieves
state-of-the-art performance, substantially outperforming existing methods in
generating accurate and executable optimization models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨æ±‚è§£å™¨æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼šä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æ‰“é€ çœŸå®ä¼˜åŒ–å»ºæ¨¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼˜åŒ–å»ºæ¨¡æ˜¯å†³ç­–è¿‡ç¨‹ä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ï¼Œå¹¿æ³›åº”ç”¨äºç‰©æµã€é‡‘èã€å·¥ç¨‹å’Œæœºå™¨å­¦ä¹ ç­‰å¤šä¸ªé¢†åŸŸã€‚å°½ç®¡ç°ä»£ä¼˜åŒ–æ±‚è§£å™¨å¦‚Gurobiã€COPTå’ŒCPLEXç­‰åŠŸèƒ½å¼ºå¤§ï¼Œä½†å°†å¤æ‚ç°å®é—®é¢˜è½¬åŒ–ä¸ºç²¾ç¡®çš„æ•°å­¦æ¨¡å‹å’Œå¯æ‰§è¡Œä¼˜åŒ–ä»£ç ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„é¢†åŸŸçŸ¥è¯†å’Œæ‰‹åŠ¨å·¥ä½œã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºè‡ªåŠ¨åŒ–æˆ–è¾…åŠ©è¿™ä¸€å¤æ‚çš„æ•°å­¦å»ºæ¨¡å’Œä»£ç ç”Ÿæˆè¿‡ç¨‹æä¾›äº†å¸Œæœ›ï¼Œä½†ç¡®ä¿LLMç”Ÿæˆçš„ä¼˜åŒ–æ¨¡å‹çš„æ­£ç¡®æ€§ã€å¯è¡Œæ€§å’Œæ±‚è§£å™¨å…¼å®¹æ€§ä»ç„¶æ˜¯ä¸€ä¸ªç ”ç©¶éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œæ±‚è§£å™¨æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆSIRLï¼‰â€çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤–éƒ¨ä¼˜åŒ–æ±‚è§£å™¨ä½œä¸ºå¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†LLMåœ¨ä¼˜åŒ–å»ºæ¨¡ä¸­çš„çœŸå®æ€§ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„ä¼˜åŠ¿ï¼Œç›´æ¥æ ¹æ®éªŒè¯ç»“æœä¼˜åŒ–LLMçš„ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
SIRLæ¡†æ¶é‡‡ç”¨äº†ä¸€ç§å®ä¾‹å¢å¼ºçš„è‡ªä¸€è‡´æ€§æ–¹æ³•æ¥åˆæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚é€šè¿‡æ‰§è¡Œç”Ÿæˆçš„ä»£ç ï¼Œåˆ©ç”¨ç»å…¸ä¼˜åŒ–æ±‚è§£å™¨æä¾›çš„ä¸°å¯Œå¥–åŠ±ä¿¡å·ï¼Œå¦‚å¯è¡Œæ€§çŠ¶æ€ã€ç›®æ ‡å‡½æ•°å€¼å’Œæ•°å­¦æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯ï¼Œæ¥è¯„ä¼°LLMçš„æ€§èƒ½å’Œæœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSIRLå®ç°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–ç¦»çº¿å­¦ä¹ å’ŒåŸºäºä»£ç†çš„æ–¹æ³•åœ¨ç”Ÿæˆæ­£ç¡®å’Œå¯é æ¨¡å‹æ–¹é¢çš„æ€§èƒ½ã€‚è®­ç»ƒå‡ºçš„7Bå‚æ•°æ¨¡å‹åœ¨SIRLæ¡†æ¶ä¸‹å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„å·¥ä½œä¸ºLLMåœ¨ä¼˜åŒ–å»ºæ¨¡é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åˆ©ç”¨å¤–éƒ¨ä¼˜åŒ–æ±‚è§£å™¨ä½œä¸ºéªŒè¯å·¥å…·å’Œå¥–åŠ±ä¿¡å·æ¥æºï¼Œä¸ºæé«˜LLMç”Ÿæˆæ¨¡å‹çš„çœŸå®æ€§å’Œå¯é æ€§æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚æ­¤å¤–ï¼Œå®ä¾‹å¢å¼ºçš„è‡ªä¸€è‡´æ€§æ–¹æ³•ä¸ºåˆæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹äºå…¶ä»–éœ€è¦ç²¾ç¡®æ•°æ®åˆæˆçš„ä»»åŠ¡ä¹Ÿå…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## not-all-rollouts-are-useful--down-sampling-rollouts-in-llm-reinforcement-learning
### Abstract
Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing
reasoning capabilities in large language models, but faces a fundamental
asymmetry in computation and memory requirements: inference is embarrassingly
parallel with a minimal memory footprint, while policy updates require
extensive synchronization and are memory-intensive. To address this asymmetry,
we introduce PODS (Policy Optimization with Down-Sampling), a framework that
strategically decouples these phases by generating numerous rollouts in
parallel but updating only on an informative subset. Within this framework, we
develop max-variance down-sampling, a theoretically motivated method that
selects rollouts with maximally diverse reward signals. We prove that this
approach has an efficient algorithmic solution, and empirically demonstrate
that GRPO with PODS using max-variance down-sampling achieves superior
performance over standard GRPO on the GSM8K benchmark.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œä¼˜åŒ–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼šPODSæ¡†æ¶ä¸‹çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ç­–ç•¥â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶åœ¨æ•°å­¦ã€ç¼–ç¨‹å’Œé€šç”¨é—®é¢˜è§£å†³ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ åœ¨è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸Šå­˜åœ¨æ ¹æœ¬æ€§çš„ä¸å¯¹ç§°æ€§ï¼šæ¨ç†é˜¶æ®µå¯ä»¥è½»æ¾å¹¶è¡Œå¤„ç†ï¼Œè€Œç­–ç•¥æ›´æ–°é˜¶æ®µåˆ™éœ€è¦å¤§é‡çš„åŒæ­¥å’Œå†…å­˜èµ„æºã€‚è¿™ç§ä¸å¯¹ç§°æ€§é™åˆ¶äº†å¼ºåŒ–å­¦ä¹ åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨ï¼Œå¹¶å½±å“äº†å…¶åœ¨èµ„æºä¸°å¯Œç¯å¢ƒä¸‹çš„å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ™ºèƒ½é€‰æ‹©ç­–ç•¥æ›´æ–°ä¸­ä½¿ç”¨çš„æ¨ç†ç»“æœï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†PODSï¼ˆPolicy Optimization with Down-Samplingï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨æ¨ç†é˜¶æ®µç”Ÿæˆå¤§é‡æ¨ç†ç»“æœï¼ˆrolloutsï¼‰ï¼Œä½†åœ¨ç­–ç•¥æ›´æ–°é˜¶æ®µåªé€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„å­é›†è¿›è¡Œæ›´æ–°ï¼Œä»è€Œæœ‰æ•ˆåœ°è§£è€¦äº†æ¨ç†å’Œè®­ç»ƒé˜¶æ®µï¼Œä¼˜åŒ–äº†èµ„æºåˆ©ç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åœ¨PODSæ¡†æ¶ä¸‹ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§ç†è®ºé©±åŠ¨çš„æœ€å¤§æ–¹å·®æŠ½æ ·æ–¹æ³•ï¼ˆmax-variance down-samplingï¼‰ï¼Œè¯¥æ–¹æ³•é€‰æ‹©å…·æœ‰æœ€å¤§å¤šæ ·åŒ–å¥–åŠ±ä¿¡å·çš„æ¨ç†ç»“æœï¼Œä»¥ä¿ƒè¿›å¯¹æ¯”ä¿¡å·çš„è·å–ã€‚ä½œè€…è¯æ˜äº†è¿™ç§æ–¹æ³•å…·æœ‰é«˜æ•ˆçš„ç®—æ³•è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨GSM8KåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡åœ¨GSM8KåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œæœ¬æ–‡è¯æ˜äº†ä½¿ç”¨æœ€å¤§æ–¹å·®æŠ½æ ·çš„GRPO-PODSæ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºæ ‡å‡†GRPOã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRPO-PODSåœ¨ä¿æŒæˆ–æé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œå†…å­˜éœ€æ±‚ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼˜åŒ–èµ„æºåˆ©ç”¨å’Œæé«˜è®­ç»ƒæ•ˆç‡æ–¹é¢ã€‚PODSæ¡†æ¶å’Œæœ€å¤§æ–¹å·®æŠ½æ ·æ–¹æ³•ä¸ä»…é€‚ç”¨äºè¯­è¨€æ¨¡å‹ï¼Œä¹Ÿå¯èƒ½ä¸ºå…¶ä»–å¼ºåŒ–å­¦ä¹ åº”ç”¨æä¾›å¯å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„å®éªŒè®¾è®¡å’Œç»“æœåˆ†æä¸ºç†è§£å’Œè¯„ä¼°å¼ºåŒ–å­¦ä¹ ç­–ç•¥æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## trajectory-bellman-residual-minimization--a-simple-value-based-method-for-llm-reasoning
### Abstract
Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œè§£é”LLMæ¨ç†æ–°æ–¹æ³•ï¼šè½¨è¿¹è´å°”æ›¼æ®‹å·®æœ€å°åŒ–â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„åº”ç”¨ä¸»è¦é‡‡ç”¨åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚è¿™äº›æ–¹æ³•è™½ç„¶å®è¯æ•ˆæœæ˜¾è‘—ï¼Œä½†å­˜åœ¨ä¸€äº›å®é™…æŒ‘æˆ˜ï¼Œä¾‹å¦‚éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºè¿›è¡Œæ–°é²œçš„å¯¹ç­–ç•¥æ»šåŠ¨ï¼Œä¾èµ–é¢å¤–çš„ç»„ä»¶å¦‚è¯„ä¼°æ¨¡å‹ã€ä¼˜åŠ¿å½’ä¸€åŒ–å’Œå‰ªè¾‘æœºåˆ¶ï¼Œä»¥åŠå¯¹äºæ¯ä¸ªæ ‡è®°çš„å†³ç­–è¿‡äºç®€åŒ–ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é‡æ–°å®¡è§†äº†ç»å…¸çš„è´å°”æ›¼æ®‹å·®æœ€å°åŒ–ï¼ˆBRMï¼‰èŒƒå¼ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åä¸ºè½¨è¿¹è´å°”æ›¼æ®‹å·®æœ€å°åŒ–ï¼ˆTBRMï¼‰çš„ç®—æ³•ã€‚TBRMæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç¦»ç­–ç•¥ç®—æ³•ï¼Œå®ƒä½¿ç”¨æ¨¡å‹è‡ªèº«çš„æ—¥å¿—æ¦‚ç‡ä½œä¸ºQå€¼ï¼Œä¼˜åŒ–å•ä¸ªè½¨è¿¹çº§åˆ«çš„è´å°”æ›¼ç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
TBRMç®—æ³•æ¶ˆé™¤äº†å¯¹è¯„ä¼°æ¨¡å‹ã€é‡è¦æ€§é‡‡æ ·æ¯”æˆ–å‰ªè¾‘çš„éœ€æ±‚ï¼Œå¹¶ä¸”åªéœ€è¦æ¯ä¸ªæç¤ºè¿›è¡Œä¸€æ¬¡æ»šåŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¯æ˜äº†åœ¨æ ‡å‡†å¯å®ç°æ€§å‡è®¾ä¸‹ï¼Œå³ä½¿è®­ç»ƒæ•°æ®ç”±ä»»æ„è¡Œä¸ºç­–ç•¥ç”Ÿæˆï¼Œç®—æ³•ä¹Ÿèƒ½æ”¶æ•›åˆ°æœ€ä¼˜çš„KLæ­£åˆ™åŒ–ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTBRMç®—æ³•ä¸€è‡´åœ°ä¼˜äºPPOå’ŒGRPOåŸºçº¿ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨AIME24æµ‹è¯•ä¸­ï¼ŒTBRMä½¿ç”¨Qwen2.5-Math-7Bæ¨¡å‹è¾¾åˆ°äº†30.5%çš„å‡†ç¡®ç‡ã€‚ä¸GRPOç›¸æ¯”ï¼ŒTBRMå¹³å‡åŸºå‡†æµ‹è¯•å¾—åˆ†æé«˜äº†1.3%ï¼Œåœ¨ç›¸åŒçš„æ¡ä»¶ä¸‹ï¼Œä¸PPOç›¸æ¯”ï¼ŒTBRMåœ¨æ›´å°‘çš„è®­ç»ƒæ—¶é—´å’Œæ›´ä½çš„GPUå†…å­˜æ¶ˆè€—ä¸‹å®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸ºå¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰åŸåˆ™ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å°†ä»·å€¼å­¦ä¹ é‡æ–°å®šä½åœ¨è½¨è¿¹çº§åˆ«ï¼ŒTBRMæä¾›äº†ä¸€ç§åŸç†æ¸…æ™°ã€æ•ˆç‡é«˜ä¸”ç†è®ºä¸Šæœ‰æ®å¯ä¾çš„æ–¹æ³•ï¼Œç”¨äºæé«˜æ•°å­¦æ¨ç†ä»»åŠ¡çš„è¡¨ç°ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºå¸Œæœ›æé«˜LLMæ¨ç†èƒ½åŠ›çš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## t2i-r1--reinforcing-image-generation-with-collaborative-semantic-level-and-token-level-cot
### Abstract
Recent advancements in large language models have demonstrated how
chain-of-thought (CoT) and reinforcement learning (RL) can improve performance.
However, applying such reasoning strategies to the visual generation domain
remains largely unexplored. In this paper, we present T2I-R1, a novel
reasoning-enhanced text-to-image generation model, powered by RL with a
bi-level CoT reasoning process. Specifically, we identify two levels of CoT
that can be utilized to enhance different stages of generation: (1) the
semantic-level CoT for high-level planning of the prompt and (2) the
token-level CoT for low-level pixel processing during patch-by-patch
generation. To better coordinate these two levels of CoT, we introduce
BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes
both generation CoTs within the same training step. By applying our reasoning
strategies to the baseline model, Janus-Pro, we achieve superior performance
with 13% improvement on T2I-CompBench and 19% improvement on the WISE
benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available
at: https://github.com/CaraJ7/T2I-R1
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œT2I-R1ï¼šååŒè¯­ä¹‰çº§ä¸æ ‡è®°çº§é“¾å¼æ€ç»´å¢å¼ºçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰å¤šä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç»“åˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ¨ç†ç­–ç•¥åº”ç”¨äºè§†è§‰ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶è¿˜ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•å°†CoTå’ŒRLåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ï¼Œä»¥æå‡å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†T2I-R1æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¨ç†å¢å¼ºå‹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡RLå’ŒåŒçº§åˆ«CoTæ¨ç†è¿‡ç¨‹å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯†åˆ«äº†ä¸¤ä¸ªçº§åˆ«çš„CoTï¼Œåˆ†åˆ«ç”¨äºå¢å¼ºç”Ÿæˆè¿‡ç¨‹çš„ä¸åŒé˜¶æ®µï¼šï¼ˆ1ï¼‰è¯­ä¹‰çº§CoTç”¨äºç”Ÿæˆå‰çš„é«˜çº§åˆ«è§„åˆ’ï¼›ï¼ˆ2ï¼‰æ ‡è®°çº§CoTç”¨äºç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½çº§åˆ«åƒç´ å¤„ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†æ›´å¥½åœ°åè°ƒè¿™ä¸¤ä¸ªçº§åˆ«çš„CoTï¼Œæœ¬æ–‡å¼•å…¥äº†BiCoT-GRPOæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å¤šç§ç”Ÿæˆå¥–åŠ±çš„RLæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨åŒä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ— ç¼ä¼˜åŒ–ä¸¤ä¸ªç”ŸæˆCoTã€‚é€šè¿‡å°†æ¨ç†ç­–ç•¥åº”ç”¨äºåŸºçº¿æ¨¡å‹Janus-Proï¼ŒT2I-R1åœ¨T2I-CompBenchä¸Šå®ç°äº†13%çš„æ€§èƒ½æå‡ï¼Œåœ¨WISEåŸºå‡†ä¸Šå®ç°äº†19%çš„æ€§èƒ½æå‡ï¼Œç”šè‡³è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹FLUXã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å®éªŒéªŒè¯ï¼ŒT2I-R1æ¨¡å‹åœ¨å¤„ç†éœ€è¦æ¨ç†æˆ–åŒ…å«éå¸¸è§„åœºæ™¯çš„æç¤ºæ—¶ï¼Œèƒ½å¤ŸæˆåŠŸæ¨å¯¼å‡ºæç¤ºèƒŒåçš„çœŸå®æ„å›¾ï¼Œæˆ–ä¸ºéå¸¸è§„åœºæ™¯æä¾›åˆç†çš„æƒ³è±¡ï¼Œä»è€Œç”Ÿæˆæ»¡æ„çš„ç»“æœã€‚ä¸åŸºçº¿æ¨¡å‹Janus-Proç›¸æ¯”ï¼ŒT2I-R1åœ¨å›¾åƒè´¨é‡å’Œä¸æç¤ºçš„å¯¹åº”å…³ç³»ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºè§†è§‰ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ¨ç†å¢å¼ºæ¡†æ¶ï¼Œä¸ä»…èƒ½å¤Ÿæå‡å›¾åƒç”Ÿæˆçš„è´¨é‡ï¼Œè¿˜èƒ½å¤Ÿä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸æç¤ºä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨å¤šç§è§†è§‰ä¸“å®¶ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±è®¾è®¡ç­–ç•¥ï¼Œè¿™å¯¹äºå…¶ä»–è§†è§‰ç”Ÿæˆä»»åŠ¡ä¹Ÿå…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## echo-chamber--rl-post-training-amplifies-behaviors-learned-in-pretraining
### Abstract
Reinforcement learning (RL)-based fine-tuning has become a crucial step in
post-training language models for advanced mathematical reasoning and coding.
Following the success of frontier reasoning models, recent work has
demonstrated that RL fine-tuning consistently improves performance, even in
smaller-scale models; however, the underlying mechanisms driving these
improvements are not well-understood. Understanding the effects of RL
fine-tuning requires disentangling its interaction with pretraining data
composition, hyperparameters, and model scale, but such problems are
exacerbated by the lack of transparency regarding the training data used in
many existing models. In this work, we present a systematic end-to-end study of
RL fine-tuning for mathematical reasoning by training models entirely from
scratch on different mixtures of fully open datasets. We investigate the
effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration)
across models of different scales. Our study reveals that RL algorithms
consistently converge towards a dominant output distribution, amplifying
patterns in the pretraining data. We also find that models of different scales
trained on the same data mixture will converge to distinct output
distributions, suggesting that there are scale-dependent biases in model
generalization. Moreover, we find that RL post-training on simpler questions
can lead to performance gains on harder ones, indicating that certain reasoning
capabilities generalize across tasks. Our findings show that small-scale
proxies in controlled settings can elicit interesting insights regarding the
role of RL in shaping language model behavior.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±å…¥æ¢ç©¶å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¯¹æ•°å­¦æ¨ç†æ¨¡å‹çš„å½±å“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæˆä¸ºæå‡è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†å’Œç¼–ç èƒ½åŠ›çš„å…³é”®æ­¥éª¤ï¼Œç ”ç©¶è€…ä»¬å‘ç°å³ä½¿æ˜¯å°è§„æ¨¡æ¨¡å‹ï¼ŒRLå¾®è°ƒä¹Ÿèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¹è¿›èƒŒåçš„æœºåˆ¶å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡çš„åŠ¨æœºåœ¨äºæ·±å…¥ç†è§£RLå¾®è°ƒå¦‚ä½•ä¸é¢„è®­ç»ƒæ•°æ®ç»„æˆã€è¶…å‚æ•°è®¾ç½®å’Œæ¨¡å‹è§„æ¨¡ç›¸äº’ä½œç”¨ï¼Œä»è€Œå½±å“æ¨¡å‹çš„è¡Œä¸ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é€šè¿‡ä»é›¶å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨ä¸åŒç»„åˆçš„å®Œå…¨å¼€æ”¾æ•°æ®é›†ï¼Œè¿›è¡Œäº†ä¸€é¡¹ç³»ç»Ÿçš„ç«¯åˆ°ç«¯ç ”ç©¶ï¼Œä»¥æ¢ç©¶RLå¾®è°ƒå¯¹æ•°å­¦æ¨ç†çš„å½±å“ã€‚è¿™ç§æ–¹æ³•å…è®¸ç ”ç©¶è€…å®Œå…¨æ§åˆ¶æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µæ¥è§¦åˆ°çš„æ•°æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡ç ”ç©¶äº†ä¸åŒè§„æ¨¡çš„æ¨¡å‹åœ¨ä¸åŒæ•°æ®æ··åˆè®­ç»ƒä¸‹çš„è¡Œä¸ºï¼Œæ­ç¤ºäº†RLç®—æ³•å€¾å‘äºæ”¶æ•›åˆ°ä¸»å¯¼è¾“å‡ºåˆ†å¸ƒï¼Œæ”¾å¤§äº†é¢„è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°äº†æ¨¡å‹åœ¨ä¸åŒè§„æ¨¡ä¸‹ä¼šæ”¶æ•›åˆ°ä¸åŒçš„è¾“å‡ºåˆ†å¸ƒï¼Œè¡¨æ˜æ¨¡å‹æ³›åŒ–å­˜åœ¨è§„æ¨¡ä¾èµ–æ€§åå·®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒRLå¾®è°ƒä¸ä»…æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œè¿˜å‡å°‘äº†è¾“å‡ºçš„å¤šæ ·æ€§ã€‚å°½ç®¡å¶å°”ä¼šå‡ºç°å¤±è´¥æ¡ˆä¾‹ï¼Œä½†æ¨¡å‹é€šå¸¸å€¾å‘äºç”Ÿæˆæ€§èƒ½æœ€ä¼˜çš„åˆ†å¸ƒæ ¼å¼ã€‚æ­¤å¤–ï¼ŒRLå¾®è°ƒè¿˜èƒ½åœ¨ç®€å•é—®é¢˜ä¸Šçš„è®­ç»ƒå¸¦æ¥åœ¨æ›´éš¾é—®é¢˜ä¸Šæ€§èƒ½æå‡çš„ç§¯æè¿ç§»æ•ˆåº”ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æ­ç¤ºäº†ä»¥ä¸‹å‡ ä¸ªå¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. RLå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ•°å­¦æ¨ç†æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å…¶æ•ˆæœä¸é¢„è®­ç»ƒæ•°æ®çš„ç»„æˆå¯†åˆ‡ç›¸å…³ã€‚
2. æ¨¡å‹çš„è§„æ¨¡ä¼šå½±å“å…¶è¾“å‡ºçš„åˆ†å¸ƒï¼Œå°è§„æ¨¡æ¨¡å‹æ›´å€¾å‘äºç”Ÿæˆä»£ç å¼çš„ç®€å•è¾“å‡ºï¼Œè€Œå¤§è§„æ¨¡æ¨¡å‹åˆ™å€¾å‘äºç”Ÿæˆè‡ªç„¶è¯­è¨€è¾“å‡ºã€‚
3. RLå¾®è°ƒåœ¨ç®€å•é—®é¢˜ä¸Šçš„è®­ç»ƒå¯ä»¥ä¿ƒè¿›æ¨¡å‹åœ¨æ›´éš¾é—®é¢˜ä¸Šçš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜æŸäº›æ¨ç†èƒ½åŠ›å¯ä»¥åœ¨ä»»åŠ¡ä¹‹é—´è¿ç§»ã€‚
4. ä¸ºäº†æ›´å¥½åœ°ç†è§£RLå¾®è°ƒçš„å½±å“ï¼Œéœ€è¦å¯¹é¢„è®­ç»ƒæ•°æ®æœ‰æ›´é«˜çš„é€æ˜åº¦å’Œæ§åˆ¶åŠ›ã€‚

## seed-grpo--semantic-entropy-enhanced-grpo-for-uncertainty-aware-policy-optimization
### Abstract
Large language models (LLMs) exhibit varying levels of confidence across
input prompts (questions): some lead to consistent, semantically similar
answers, while others yield diverse or contradictory outputs. This variation
reflects LLM's uncertainty about the input prompt, a signal of how confidently
the model understands a given problem. However, vanilla Group Relative Policy
Optimization (GRPO) treats all prompts equally during policy updates, ignoring
this important information about the model's knowledge boundaries. To address
this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which
explicitly measures LLMs' uncertainty of the input prompts semantic entropy.
Semantic entropy measures the diversity of meaning in multiple generated
answers given a prompt and uses this to modulate the magnitude of policy
updates. This uncertainty-aware training mechanism enables dynamic adjustment
of policy update magnitudes based on question uncertainty. It allows more
conservative updates on high-uncertainty questions while maintaining the
original learning signal on confident ones. Experimental results on five
mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva
34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new
state-of-the-art performance in average accuracy, validating the effectiveness
of uncertainty-aware policy optimization.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨è¯­ä¹‰ç†µå¢å¼ºGRPOå®ç°ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ç­–ç•¥ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„åº”ç”¨ï¼Œæ¨¡å‹å¯¹äºä¸åŒè¾“å…¥æç¤ºï¼ˆé—®é¢˜ï¼‰çš„å“åº”è¡¨ç°å‡ºä¸åŒçš„ç½®ä¿¡åº¦æ°´å¹³ã€‚ä¸€äº›é—®é¢˜ä¼šå¾—åˆ°ä¸€è‡´ã€è¯­ä¹‰ç›¸ä¼¼çš„ç­”æ¡ˆï¼Œè€Œå¦ä¸€äº›åˆ™äº§ç”Ÿå¤šæ ·ç”šè‡³çŸ›ç›¾çš„è¾“å‡ºã€‚è¿™ç§å˜åŒ–åæ˜ äº†æ¨¡å‹å¯¹è¾“å…¥æç¤ºçš„ä¸ç¡®å®šæ€§ï¼Œè¿™æ˜¯æ¨¡å‹å¯¹é—®é¢˜ç†è§£è‡ªä¿¡ç¨‹åº¦çš„ä¸€ä¸ªä¿¡å·ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨ç­–ç•¥æ›´æ–°æ—¶å¯¹æ‰€æœ‰æç¤ºä¸€è§†åŒä»ï¼Œå¿½ç•¥äº†è¿™ä¸€å…³äºæ¨¡å‹çŸ¥è¯†è¾¹ç•Œçš„é‡è¦ä¿¡æ¯ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†SEED-GRPOï¼ˆSemantic Entropy EnhanceD GRPOï¼‰ï¼Œä¸€ç§æ˜¾å¼æµ‹é‡LLMå¯¹è¾“å…¥æç¤ºè¯­ä¹‰ç†µçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ç®—æ³•ã€‚è¯­ä¹‰ç†µæ˜¯ä¸€ç§åŸºäºç†µçš„åº¦é‡ï¼Œå®ƒæ•æ‰äº†ç»™å®šæç¤ºçš„å¤šä¸ªç”Ÿæˆç­”æ¡ˆä¹‹é—´æ„ä¹‰çš„å¤šæ ·æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
SEED-GRPOåˆ©ç”¨è¯­ä¹‰ç†µæ¥è°ƒæ•´ç­–ç•¥æ›´æ–°çš„å¹…åº¦ï¼Œä½¿å¾—åœ¨é—®é¢˜ä¸ç¡®å®šæ€§é«˜æ—¶è¿›è¡Œæ›´ä¿å®ˆçš„æ›´æ–°ï¼Œè€Œåœ¨æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜è‡ªä¿¡çš„é—®é¢˜ä¸Šä¿æŒåŸå§‹çš„å­¦ä¹ ä¿¡å·ã€‚è¿™ç§æ–¹æ³•å®ç°äº†ä¸€ç§åŠ¨æ€å­¦ä¹ ç‡æœºåˆ¶ï¼Œæ ¹æ®æ¨¡å‹å½“å‰çš„èƒ½åŠ›å’Œé—®é¢˜éš¾åº¦è‡ªåŠ¨æ ¡å‡†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ï¼ˆAIME24ã€AMCã€MATHã€Minervaå’ŒOlympiadBenchï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSEED-GRPOåœ¨å¹³å‡å‡†ç¡®åº¦ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼ŒéªŒè¯äº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ºLLMçš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå³é€šè¿‡æµ‹é‡å’Œåˆ©ç”¨æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ¥ä¼˜åŒ–ç­–ç•¥æ›´æ–°ã€‚SEED-GRPOçš„æˆåŠŸè¡¨æ˜ï¼Œåœ¨ç­–ç•¥å­¦ä¹ ä¸­è€ƒè™‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§å¯ä»¥å¸¦æ¥æ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ï¼Œè¿™å¯¹äºæœªæ¥çš„æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–å…·æœ‰å¯å‘æ„ä¹‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## lmgame-bench--how-good-are-llms-at-playing-games-
### Abstract
Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "lmgame-Benchï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆä¸­çš„è¡¨ç°å¦‚ä½•ï¼Ÿ"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°å˜å¾—å°¤ä¸ºé‡è¦ã€‚è§†é¢‘æ¸¸æˆå› å…¶éœ€è¦æ„ŸçŸ¥ã€è®°å¿†å’Œè§„åˆ’èƒ½åŠ›ï¼Œæˆä¸ºäº†è¯„ä¼°LLMçš„ç†æƒ³å¹³å°ã€‚ç„¶è€Œï¼Œç›´æ¥å°†LLMæ”¾å…¥æ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è§†è§‰æ„ŸçŸ¥è„†å¼±ã€æç¤ºæ•æ„Ÿæ€§å’Œæ•°æ®æ±¡æŸ“ç­‰é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºlmgame-Benchçš„è¯„ä¼°æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–æ¬¡æå‡ºäº†lmgame-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†é¢‘æ¸¸æˆçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥æ„ŸçŸ¥å’Œè®°å¿†æ¨¡å—æ¥å…‹æœLLMåœ¨æ¸¸æˆä¸­çš„å¸¸è§æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å—å¸®åŠ©æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥å’Œé•¿æœŸè§„åˆ’æ–¹é¢è¿›è¡Œä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
lmgame-Benché€šè¿‡ä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡çš„æ¸¸æˆï¼ˆåŒ…æ‹¬å¹³å°è·³è·ƒã€è§£è°œå’Œå™äº‹é©±åŠ¨çš„ä¾¦æ¢æ¸¸æˆï¼‰æ¥è¯„ä¼°LLMçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨äº†ä¸€ç§æ ‡å‡†åŒ–çš„æç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥å‡å°‘æç¤ºæ•æ„Ÿæ€§ï¼Œå¹¶é€‚åº”æ¸¸æˆè®¾ç½®ä»¥å‡è½»æ•°æ®æ±¡æŸ“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨13ä¸ªé¢†å…ˆæ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºlmgame-Benchèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒè¿˜å‘ç°ï¼Œæ¯ä¸ªæ¸¸æˆéƒ½æ¢æµ‹åˆ°äº†æ¨¡å‹èƒ½åŠ›çš„ç‹¬ç‰¹ç»„åˆï¼Œè¿™äº›èƒ½åŠ›åœ¨å…¶ä»–å­¤ç«‹çš„ç¯å¢ƒä¸­é€šå¸¸ä¸ä¼šä¸€èµ·æµ‹è¯•ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œå¯¹lmgame-Benchä¸­çš„å•ä¸ªæ¸¸æˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä¸ä»…èƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨è¯¥æ¸¸æˆä¸Šçš„è¡¨ç°ï¼Œè¿˜èƒ½å¤Ÿè½¬ç§»åˆ°æœªè§è¿‡çš„æ¸¸æˆå’Œå¤–éƒ¨è§„åˆ’ä»»åŠ¡ä¸Šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
lmgame-Benchä¸ºè¯„ä¼°LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚å®ƒä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¿˜èƒ½å¤Ÿä½œä¸ºæ”¹è¿›LLMèƒ½åŠ›çš„è®­ç»ƒç¯å¢ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æä¾›çš„å®šé‡åˆ†ææ–¹æ³•å’Œå®éªŒç»“æœï¼Œä¸ºç†è§£å’Œæ”¹è¿›LLMçš„æ¸¸æˆè¡¨ç°æä¾›äº†å®è´µçš„å‚è€ƒã€‚

## rift--closed-loop-rl-fine-tuning-for-realistic-and-controllable-traffic-simulation
### Abstract
Achieving both realism and controllability in interactive closed-loop traffic
simulation remains a key challenge in autonomous driving. Data-driven
simulation methods reproduce realistic trajectories but suffer from covariate
shift in closed-loop deployment, compounded by simplified dynamics models that
further reduce reliability. Conversely, physics-based simulation methods
enhance reliable and controllable closed-loop interactions but often lack
expert demonstrations, compromising realism. To address these challenges, we
introduce a dual-stage AV-centered simulation framework that conducts open-loop
imitation learning pre-training in a data-driven simulator to capture
trajectory-level realism and multimodality, followed by closed-loop
reinforcement learning fine-tuning in a physics-based simulator to enhance
controllability and mitigate covariate shift. In the fine-tuning stage, we
propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that
preserves the trajectory-level multimodality through a GRPO-style
group-relative advantage formulation, while enhancing controllability and
training stability by replacing KL regularization with the dual-clip mechanism.
Extensive experiments demonstrate that RIFT significantly improves the realism
and controllability of generated traffic scenarios, providing a robust platform
for evaluating autonomous vehicle performance in diverse and interactive
scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œRIFTï¼šå®ç°äº¤äº’å¼é—­ç¯äº¤é€šæ¨¡æ‹Ÿçš„çœŸå®æ€§ä¸å¯æ§æ€§â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¼€å‘ä¸­ï¼Œå¯é çš„é—­ç¯äº¤é€šæ¨¡æ‹Ÿè‡³å…³é‡è¦ï¼Œå®ƒæ”¯æŒè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚ç†æƒ³çš„äº¤é€šæ¨¡æ‹Ÿåº”å…·å¤‡çœŸå®æ€§å’Œå¯æ§æ€§ä¸¤å¤§ç‰¹æ€§ï¼šçœŸå®æ€§åæ˜ ç°å®ä¸–ç•Œçš„é©¾é©¶è¡Œä¸ºï¼Œè€Œå¯æ§æ€§å…è®¸å®šåˆ¶åŒ–çš„äº¤äº’é£æ ¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¾€å¾€åªå…³æ³¨ç”ŸæˆçœŸå®çš„äº¤é€šåœºæ™¯æˆ–æ„å»ºå¯æ§çš„äº¤äº’ï¼ŒåŒæ—¶ç¡®ä¿çœŸå®æ€§å’Œå¯æ§æ€§åœ¨äº¤äº’å¼é—­ç¯åœºæ™¯ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé˜¶æ®µè‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVï¼‰ä¸­å¿ƒåŒ–æ¨¡æ‹Ÿæ¡†æ¶ã€‚é¦–å…ˆï¼Œåœ¨æ•°æ®é©±åŠ¨æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œå¼€ç¯æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒï¼Œä»¥æ•è·è½¨è¿¹çº§åˆ«çš„çœŸå®æ€§å’Œå¤šæ¨¡æ€æ€§ï¼›ç„¶åï¼Œåœ¨ç‰©ç†åŸºç¡€æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œé—­ç¯å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œä»¥å¢å¼ºå¯æ§æ€§å’Œå‡è½»åå˜é‡åç§»ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
åœ¨å¾®è°ƒé˜¶æ®µï¼Œæœ¬æ–‡æå‡ºäº†RIFTï¼ˆé—­ç¯RLå¾®è°ƒç­–ç•¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡GRPOé£æ ¼çš„ç»„ç›¸å¯¹ä¼˜åŠ¿å…¬å¼ä¿æŒè½¨è¿¹çº§åˆ«çš„å¤šæ¨¡æ€æ€§ï¼ŒåŒæ—¶é€šè¿‡æ›¿æ¢KLæ­£åˆ™åŒ–ä¸ºåŒå‰ªè¾‘æœºåˆ¶æ¥å¢å¼ºå¯æ§æ€§å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒRIFTæ˜¾è‘—æé«˜äº†ç”Ÿæˆäº¤é€šåœºæ™¯çš„çœŸå®æ€§å’Œå¯æ§æ€§ï¼Œä¸ºè¯„ä¼°è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å¤šæ ·åŒ–å’Œäº¤äº’å¼åœºæ™¯ä¸­çš„æ€§èƒ½æä¾›äº†ä¸€ä¸ªç¨³å¥çš„å¹³å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„åŒé˜¶æ®µæ¨¡æ‹Ÿæ¡†æ¶æœ‰æ•ˆåœ°ç»“åˆäº†æ•°æ®é©±åŠ¨å’Œç‰©ç†åŸºç¡€æ¨¡æ‹Ÿå™¨çš„ä¼˜åŠ¿ï¼Œä¸ºè§£å†³çœŸå®æ€§å’Œå¯æ§æ€§ä¹‹é—´çš„æ ¹æœ¬æ€§æƒè¡¡æä¾›äº†æ–°çš„æ€è·¯ã€‚RIFTç­–ç•¥åœ¨ä¿æŒè½¨è¿¹å¤šæ¨¡æ€æ€§çš„åŒæ—¶ï¼Œé€šè¿‡æ–°é¢–çš„ä¼˜åŒ–æœºåˆ¶æé«˜äº†é—­ç¯åœºæ™¯ä¸­çš„å¯æ§æ€§å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œå¯¹äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ¨¡æ‹Ÿå’Œè¯„ä¼°å…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚

## towards-reasoning-era--a-survey-of-long-chain-of-thought-for-reasoning-large-language-models
### Abstract
Recent advancements in reasoning with large language models (RLLMs), such as
OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in
complex domains like mathematics and coding. A central factor in their success
lies in the application of long chain-of-thought (Long CoT) characteristics,
which enhance reasoning abilities and enable the solution of intricate
problems. However, despite these developments, a comprehensive survey on Long
CoT is still lacking, limiting our understanding of its distinctions from
traditional short chain-of-thought (Short CoT) and complicating ongoing debates
on issues like "overthinking" and "test-time scaling." This survey seeks to
fill this gap by offering a unified perspective on Long CoT. (1) We first
distinguish Long CoT from Short CoT and introduce a novel taxonomy to
categorize current reasoning paradigms. (2) Next, we explore the key
characteristics of Long CoT: deep reasoning, extensive exploration, and
feasible reflection, which enable models to handle more complex tasks and
produce more efficient, coherent outcomes compared to the shallower Short CoT.
(3) We then investigate key phenomena such as the emergence of Long CoT with
these characteristics, including overthinking, and test-time scaling, offering
insights into how these processes manifest in practice. (4) Finally, we
identify significant research gaps and highlight promising future directions,
including the integration of multi-modal reasoning, efficiency improvements,
and enhanced knowledge frameworks. By providing a structured overview, this
survey aims to inspire future research and further the development of logical
reasoning in artificial intelligence.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢é•¿é“¾æ¨ç†æ—¶ä»£ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿é“¾æ¨ç†ç ”ç©¶ç»¼è¿°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰å¤æ‚é¢†åŸŸå±•ç°å‡ºä»¤äººç©ç›®çš„æ¨ç†èƒ½åŠ›ï¼Œé•¿é“¾æ¨ç†ï¼ˆLong CoTï¼‰ç‰¹æ€§çš„åº”ç”¨æˆä¸ºå…¶æˆåŠŸçš„å…³é”®ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œå…³äºé•¿é“¾æ¨ç†çš„å…¨é¢ç»¼è¿°ä»ç„¶ç¼ºå¤±ï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬å¯¹é•¿é“¾æ¨ç†ä¸ä¼ ç»ŸçŸ­é“¾æ¨ç†ï¼ˆShort CoTï¼‰å·®å¼‚çš„ç†è§£ï¼Œå¹¶ä½¿å¾—å…³äºâ€œè¿‡åº¦æ€è€ƒâ€å’Œâ€œæµ‹è¯•æ—¶æ‰©å±•â€ç­‰é—®é¢˜çš„è®¨è®ºå˜å¾—å¤æ‚ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæä¾›ä¸€ä¸ªå…³äºé•¿é“¾æ¨ç†çš„ç»Ÿä¸€è§†è§’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é¦–å…ˆåŒºåˆ†äº†é•¿é“¾æ¨ç†ï¼ˆLong CoTï¼‰å’Œä¼ ç»ŸçŸ­é“¾æ¨ç†ï¼ˆShort CoTï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åˆ†ç±»æ³•æ¥å½’ç±»å½“å‰çš„æ¨ç†èŒƒå¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡æ¢è®¨äº†é•¿é“¾æ¨ç†çš„ä¸‰ä¸ªå…³é”®ç‰¹æ€§ï¼šæ·±åº¦æ¨ç†ã€å¹¿æ³›æ¢ç´¢å’Œå¯è¡Œåæ€ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶äº§ç”Ÿæ›´é«˜æ•ˆã€ä¸€è‡´çš„ç»“æœã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æœ¬æ–‡ç ”ç©¶äº†ä¸é•¿é“¾æ¨ç†ç‰¹æ€§ç›¸å…³çš„ä¸€äº›å…³é”®ç°è±¡ï¼Œå¦‚é•¿é“¾æ¨ç†çš„å‡ºç°ã€è¿‡åº¦æ€è€ƒå’Œæµ‹è¯•æ—¶æ‰©å±•ç­‰ï¼Œæä¾›äº†è¿™äº›è¿‡ç¨‹åœ¨å®é™…ä¸­å¦‚ä½•ä½“ç°çš„è§è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4
æœ¬æ–‡è¯†åˆ«äº†é•¿é“¾æ¨ç†é¢†åŸŸçš„é‡è¦ç ”ç©¶ç©ºç™½ï¼Œå¹¶çªå‡ºäº†æœ‰å‰æ™¯çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ¨ç†çš„æ•´åˆã€æ•ˆç‡æå‡å’Œå¢å¼ºçŸ¥è¯†æ¡†æ¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡æä¾›ä¸€ä¸ªç»“æ„åŒ–çš„æ¦‚è¿°ï¼Œæ—¨åœ¨æ¿€å‘æœªæ¥çš„ç ”ç©¶ï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½ä¸­é€»è¾‘æ¨ç†çš„å‘å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºç†è§£é•¿é“¾æ¨ç†æä¾›äº†æ¸…æ™°çš„æ¡†æ¶ï¼Œå¹¶å¯¹å…¶å…³é”®ç‰¹æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç ”ç©¶è€…å¯ä»¥ä»æœ¬æ–‡ä¸­è·å¾—å…³äºé•¿é“¾æ¨ç†çš„æœ€æ–°è¿›å±•å’Œæœªæ¥ç ”ç©¶æ–¹å‘çš„å®è´µä¿¡æ¯ï¼Œä»¥åŠå¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­é¿å…è¿‡åº¦æ€è€ƒå’Œæ— æ•ˆæ¢ç´¢çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸°å¯Œçš„æ–‡çŒ®èµ„æºå’Œå¼€æ”¾èµ„æºæ¡†æ¶ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

## tinyllava-video-r1--towards-smaller-lmms-for-video-reasoning
### Abstract
Recently, improving the reasoning ability of large multimodal models (LMMs)
through reinforcement learning has made great progress. However, most existing
works are based on highly reasoning-intensive datasets such as mathematics and
code, and researchers generally choose large-scale models as the foundation. We
argue that exploring small-scale models' reasoning capabilities remains
valuable for researchers with limited computational resources. Moreover,
enabling models to explain their reasoning processes on general
question-answering datasets is equally meaningful. Therefore, we present the
small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,
a traceably trained video understanding model with no more than 4B parameters,
it not only demonstrates significantly improved reasoning and thinking
capabilities after using reinforcement learning on general Video-QA datasets,
but also exhibits the emergent characteristic of "aha moments". Furthermore, we
share a series of experimental findings, aiming to provide practical insights
for future exploration of video reasoning (thinking) abilities in small-scale
models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å°å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†é¢†åŸŸçš„æ½œåŠ›ï¼šTinyLLaVA-Video-R1

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ¨ç†èƒ½åŠ›çš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œéƒ½åŸºäºé«˜åº¦æ¨ç†å¯†é›†å‹çš„æ•°æ®é›†ï¼Œå¦‚æ•°å­¦å’Œä»£ç ï¼Œå¹¶ä¸”ç ”ç©¶è€…é€šå¸¸é€‰æ‹©å¤§å‹æ¨¡å‹ä½œä¸ºåŸºç¡€ã€‚æœ¬æ–‡ä½œè€…è®¤ä¸ºï¼Œå¯¹äºè®¡ç®—èµ„æºæœ‰é™çš„ç ”ç©¶è€…æ¥è¯´ï¼Œæ¢ç´¢å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»ç„¶å…·æœ‰ä»·å€¼ã€‚æ­¤å¤–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£é‡Šå…¶åœ¨ä¸€èˆ¬æ€§é—®é¢˜å›ç­”æ•°æ®é›†ä¸Šçš„æ¨ç†è¿‡ç¨‹ä¹Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å°å‹è§†é¢‘æ¨ç†æ¨¡å‹TinyLLaVA-Video-R1ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡åŸºäºTinyLLaVA-Videoæ¨¡å‹ï¼Œä¸€ä¸ªå‚æ•°ä¸è¶…è¿‡40äº¿çš„è§†é¢‘ç†è§£æ¨¡å‹ï¼Œé€šè¿‡åœ¨ä¸€èˆ¬è§†é¢‘é—®ç­”ï¼ˆVideo-QAï¼‰æ•°æ®é›†ä¸Šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†å’Œæ€è€ƒèƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
TinyLLaVA-Video-R1æ¨¡å‹å±•ç°å‡ºäº†ä¸€ç§â€œçµæ„Ÿçªç°â€çš„æ¶Œç°ç‰¹æ€§ï¼Œè¿™æ„å‘³ç€æ¨¡å‹ä¸ä»…èƒ½å¤Ÿé€šè¿‡æ„ŸçŸ¥ç”Ÿæˆç­”æ¡ˆï¼Œè¿˜èƒ½å¤Ÿç»å†æ¨ç†è¿‡ç¨‹ä¸­çš„â€œaha momentsâ€ï¼Œå³çªç„¶ç†è§£é—®é¢˜å…³é”®çš„æ—¶åˆ»ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å¹¿æ³›çš„å®éªŒé…ç½®ï¼Œè·å¾—äº†ä¸€ç³»åˆ—æœ‰æ´å¯ŸåŠ›çš„å‘ç°ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒTinyLLaVA-Video-R1æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ä»…æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œè¿˜èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œä¸ºæœªæ¥å°å‹æ¨¡å‹åœ¨è§†é¢‘æ¨ç†é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å®ç”¨è§è§£ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯å°å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡é€‚å½“çš„æ–¹æ³•å’Œè®­ç»ƒç­–ç•¥ï¼Œä¹Ÿèƒ½å¤Ÿåœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ã€‚è¿™å¯¹äºè®¡ç®—èµ„æºæœ‰é™çš„ç ”ç©¶è€…æ¥è¯´ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„ç ”ç©¶æ–¹å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æä¾›çš„å®éªŒå‘ç°å’Œæ¨¡å‹è®¾è®¡æ€è·¯ï¼Œå¯¹äºæœªæ¥æ¢ç´¢å°å‹æ¨¡å‹çš„è§†é¢‘æ¨ç†èƒ½åŠ›å…·æœ‰å‚è€ƒä»·å€¼ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ï¼Œå¯ä¾›ç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¤ç°ã€‚

## o$^2$-searcher--a-searching-based-agent-model-for-open-domain-open-ended-question-answering
### Abstract
Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œO2-Searcherï¼šå¼€å¯å¼€æ”¾å¼é—®é¢˜è§£ç­”æ–°ç¯‡ç« â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºå…¶é™æ€çš„å‚æ•°åŒ–çŸ¥è¯†é™åˆ¶ï¼Œéš¾ä»¥åº”å¯¹éœ€è¦å®æ—¶æ›´æ–°çš„å¼€æ”¾åŸŸä»»åŠ¡ã€‚å½“å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨å°é—­å¼é—®é¢˜ï¼Œè€Œå¯¹äºç¼ºä¹æ ‡å‡†ç­”æ¡ˆæˆ–æä¾›éå”¯ä¸€ã€å¤šæ ·åŒ–ç­”æ¡ˆçš„å¼€æ”¾å¼é—®é¢˜ï¼Œç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†O2-Searcherï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æœç´¢ä»£ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¼€æ”¾åŸŸä¸­çš„å¼€æ”¾å¼å’Œå°é—­å¼é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
O2-Searcheré‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆã€æœ¬åœ°æ¨¡æ‹Ÿçš„æœç´¢ç¯å¢ƒï¼Œç”¨äºåŠ¨æ€çŸ¥è¯†è·å–ï¼Œæœ‰æ•ˆåœ°å°†å¤–éƒ¨ä¸–ç•ŒçŸ¥è¯†ä¸æ¨¡å‹çš„å¤æ‚æ¨ç†è¿‡ç¨‹åˆ†ç¦»ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿã€æˆæœ¬æ•ˆç›Šåœ°è·å–å¤–éƒ¨ä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è®­ç»ƒæœºåˆ¶ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¯†åˆ«é—®é¢˜ç±»å‹å¹¶é€‚åº”æ€§åœ°è°ƒæ•´ç­”æ¡ˆç”Ÿæˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºO2-QAçš„é«˜è´¨é‡å¼€æ”¾åŸŸé—®ç­”åŸºå‡†ï¼ŒåŒ…å«300ä¸ªæ¥è‡ªä¸åŒé¢†åŸŸçš„ä¸“å®¶ç­–åˆ’çš„å¼€æ”¾å¼é—®é¢˜åŠå…¶ç›¸å…³çš„ç½‘é¡µç¼“å­˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒO2-Searcherå³ä½¿ä»…ä½¿ç”¨3Bå‚æ•°çš„æ¨¡å‹ï¼Œä¹Ÿæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„LLMä»£ç†åœ¨O2-QAåŸºå‡†ä¸Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œåœ¨å¤šä¸ªå°é—­å¼é—®ç­”åŸºå‡†ä¸Šï¼ŒO2-Searcherä¸ä»…ä¸å‚æ•°å¤§å°ç›¸å½“çš„å…¶ä»–æ¨¡å‹ç›¸æ¯”å–å¾—äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œè€Œä¸”ä¸å‚æ•°æ›´å¤§çš„æ¨¡å‹ç›¸æ¯”ä¹Ÿè¡¨ç°å‡ºäº†ç›¸å½“çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„O2-Searcheræ¨¡å‹ä¸ºå¤„ç†å¼€æ”¾åŸŸä¸­çš„å¼€æ”¾å¼é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶é«˜æ•ˆçš„çŸ¥è¯†è·å–å’Œçµæ´»çš„åº”ç”¨ç­–ç•¥å¯¹äºæå‡LLMåœ¨å¼€æ”¾åŸŸä»»åŠ¡ä¸Šçš„è¡¨ç°å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œæ„å»ºçš„O2-QAåŸºå‡†ä¸ºè¯„ä¼°LLMåœ¨å¼€æ”¾å¼é—®é¢˜ä¸Šçš„æ€§èƒ½æä¾›äº†ä¸€ä¸ªé«˜è´¨é‡çš„æ ‡å‡†ï¼Œæœ‰åŠ©äºæ¨åŠ¨ç›¸å…³ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## spectral-policy-optimization--coloring-your-incorrect-reasoning-in-grpo
### Abstract
Reinforcement learning (RL) has demonstrated significant success in enhancing
reasoning capabilities in large language models (LLMs). One of the most widely
used RL methods is Group Relative Policy Optimization
(GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and
success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls
when all sampled responses in a group are incorrect -- referred to as an
\emph{all-negative-sample} group -- as it fails to update the policy, hindering
learning progress. The contributions of this paper are two-fold. First, we
propose a simple yet effective framework that introduces response diversity
within all-negative-sample groups in GRPO using AI feedback. We also provide a
theoretical analysis, via a stylized model, showing how this diversification
improves learning dynamics. Second, we empirically validate our approach,
showing the improved performance across various model sizes (7B, 14B, 32B) in
both offline and online learning settings with 10 benchmarks, including base
and distilled variants. Our findings highlight that learning from
all-negative-sample groups is not only feasible but beneficial, advancing
recent insights from \citet{Xiong-2025-Minimalist}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨AIåé¦ˆä¼˜åŒ–æ¨ç†èƒ½åŠ›ï¼šSpectral Policy Optimizationçš„æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„æå‡ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨æ¨¡å‹è®­ç»ƒä¸­æ‰®æ¼”äº†é‡è¦è§’è‰²ã€‚å…¶ä¸­ï¼ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰å› å…¶å†…å­˜æ•ˆç‡é«˜å’ŒæˆåŠŸè®­ç»ƒDeepSeek-R1æ¨¡å‹è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼ŒGRPOåœ¨å¤„ç†æ‰€æœ‰æ ·æœ¬å‡é”™è¯¯çš„ç»„ï¼ˆç§°ä¸ºå…¨è´Ÿæ ·æœ¬ç»„ï¼‰æ—¶é‡åˆ°äº†ç“¶é¢ˆï¼Œå› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ”¿ç­–æ— æ³•æ¥æ”¶åˆ°ä»»ä½•å­¦ä¹ ä¿¡å·ï¼Œå¯¼è‡´å­¦ä¹ è¿›ç¨‹åœæ»ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡AIåé¦ˆå¼•å…¥å…¨è´Ÿæ ·æœ¬ç»„å†…çš„å“åº”å¤šæ ·æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ¡†æ¶ï¼Œç§°ä¸ºSpectral Policy Optimizationï¼ˆSPOï¼‰ï¼Œå®ƒé€šè¿‡AIåé¦ˆåœ¨å…¨è´Ÿæ ·æœ¬ç»„å†…å¼•å…¥å“åº”å¤šæ ·æ€§ã€‚è¿™ç§æ–¹æ³•å°†åŸæœ¬çš„â€œé»‘ç™½â€ç»“æœå¥–åŠ±è½¬å˜ä¸ºå¤šæ ·åŒ–çš„â€œå…‰è°±â€å¥–åŠ±ï¼Œä»è€Œæœ‰æ•ˆåˆ©ç”¨å…¨è´Ÿæ ·æœ¬ç»„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
SPOæ¡†æ¶é€šè¿‡ä½¿ç”¨é«˜çº§LLMï¼ˆå¦‚o4-miniå’ŒClaude 3.74ï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¹å¤šæ­¥éª¤æ¨ç†é“¾è¿›è¡Œæ•´ä½“è¯„ä¼°ï¼Œä»è€Œä¸ºè´Ÿæ ·æœ¬æä¾›ç²¾ç»†åŒ–çš„å¥–åŠ±ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»Ÿè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„å†…å­˜è´Ÿæ‹…ï¼Œä¹Ÿæ— éœ€æ˜‚è´µçš„æ­¥éª¤çº§äººå·¥æ ‡æ³¨ï¼Œç®€åŒ–å¹¶åŠ é€Ÿäº†è®­ç»ƒæµç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šç§æ¨¡å‹å¤§å°ï¼ˆ7Bã€14Bã€32Bï¼‰çš„ç¦»çº¿å’Œåœ¨çº¿å­¦ä¹ è®¾ç½®ä¸‹ï¼Œå¯¹10ä¸ªæœ€å…ˆè¿›çš„åŸºå‡†è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬åŸºç¡€å’Œç²¾ç®€å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPOæ–¹æ³•åœ¨æé«˜LLMæ¨ç†èƒ½åŠ›æ–¹é¢æ˜¯æœ‰æ•ˆçš„ï¼Œè§£å†³äº†å½“å‰GRPOç®¡é“çš„å±€é™æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æˆæœè¡¨æ˜ï¼Œé€šè¿‡AIåé¦ˆå¼•å…¥å“åº”å¤šæ ·æ€§ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å…¨è´Ÿæ ·æœ¬ç»„è¿›è¡Œå­¦ä¹ ï¼Œè¿™ä¸ä»…å¯è¡Œï¼Œè€Œä¸”æœ‰ç›Šã€‚è¿™ç§æ–¹æ³•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’å’Œå·¥å…·ï¼Œå¯¹äºæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè§£å†³å¤æ‚ä»»åŠ¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼ŒSPOæ¡†æ¶çš„è®¾è®¡å’Œå®éªŒéªŒè¯ä¹Ÿä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å¯å€Ÿé‰´çš„èŒƒä¾‹ã€‚

## group-in-group-policy-optimization-for-llm-agent-training
### Abstract
Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield sparse or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œè¿ˆå‘ç²¾ç»†åŒ–çš„LLMæ™ºèƒ½ä½“è®­ç»ƒï¼šGiGPOç®—æ³•çš„é©æ–°ä¹‹è·¯â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å†³ç­–åˆ¶å®šé¢†åŸŸçš„åº”ç”¨ä¸æ–­æ‰©å±•ï¼Œå®ƒä»¬å·²ç»ä»é™æ€çš„é—®ç­”ç³»ç»Ÿè½¬å˜ä¸ºèƒ½å¤Ÿåœ¨å¼€æ”¾ç¯å¢ƒä¸­æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨çš„æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•åœ¨å¤„ç†é•¿å‘¨æœŸä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¥–åŠ±ç¨€ç–æˆ–å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œå¯¹å•ä¸ªæ­¥éª¤çš„ä¿¡ç”¨åˆ†é…å˜å¾—æå…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„RLç®—æ³•â€”â€”GiGPOï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¿æŒç¾¤ä½“RLç®—æ³•ä¼˜åŠ¿çš„åŒæ—¶ï¼Œä¸ºLLMæ™ºèƒ½ä½“æä¾›æ›´ç²¾ç»†çš„ä¿¡ç”¨åˆ†é…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
GiGPOå¼•å…¥äº†ä¸€ç§åŒå±‚ç»“æ„æ¥ä¼°è®¡ç›¸å¯¹ä¼˜åŠ¿ã€‚åœ¨å‰§é›†çº§åˆ«ï¼Œå®ƒåŸºäºä¸€ç»„å®Œæ•´çš„è½¨è¿¹è®¡ç®—å®è§‚ç›¸å¯¹ä¼˜åŠ¿ï¼›åœ¨æ­¥éª¤çº§åˆ«ï¼Œå®ƒé€šè¿‡è¯†åˆ«è½¨è¿¹é—´é‡å¤çš„ç¯å¢ƒçŠ¶æ€ï¼Œå¼•å…¥äº†ä¸€ç§é”šç‚¹çŠ¶æ€åˆ†ç»„æœºåˆ¶ï¼Œä»è€Œå…è®¸å¯¹å±€éƒ¨æ­¥éª¤çš„æœ‰æ•ˆæ€§è¿›è¡Œæ›´ç²¾ç»†çš„ä¼°è®¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
GiGPOåˆ©ç”¨åœ¨ç›¸åŒä»»åŠ¡å’Œåˆå§‹ç¯å¢ƒæ¡ä»¶ä¸‹ï¼Œè®¸å¤šè½¨è¿¹å¤šæ¬¡é‡åˆ°ç›¸åŒçŠ¶æ€çš„è‡ªç„¶ç‰¹æ€§ï¼Œé€šè¿‡è¿™äº›å…±äº«çŠ¶æ€æ„å»ºæ­¥éª¤çº§åˆ«çš„åˆ†ç»„ï¼Œä»è€Œå®ç°å¯¹åŠ¨ä½œçš„å±€éƒ¨åŒ–ä¿¡ç”¨åˆ†é…ï¼Œè€Œæ— éœ€é¢å¤–çš„æ¨¡æ‹Ÿæˆ–è¾…åŠ©ä»·å€¼æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•â€”â€”ALFWorldå’ŒWebShopä¸Šè¯„ä¼°äº†GiGPOç®—æ³•ï¼Œä½¿ç”¨Qwen2.5-1.5B-Instructå’ŒQwen2.5-7B-Instructæ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGiGPOç®—æ³•åœ¨ä¿æŒä¸ç¾¤ä½“RLç›¸åŒçš„GPUå†…å­˜å¼€é”€å’ŒLLMæ¨¡æ‹Ÿçš„åŒæ—¶ï¼Œæä¾›äº†æ›´ç²¾ç»†çš„æ¯æ­¥ä¿¡ç”¨ä¿¡å·ï¼Œå¹¶åœ¨ALFWorldä¸Šå®ç°äº†è¶…è¿‡12%çš„æ€§èƒ½æå‡ï¼Œåœ¨WebShopä¸Šå®ç°äº†è¶…è¿‡9%çš„æ€§èƒ½æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GiGPOç®—æ³•åœ¨ä¿æŒç¾¤ä½“RLç®—æ³•çš„æ— ä»·å€¼å‡½æ•°ã€ä½å†…å­˜å’Œç¨³å®šæ”¶æ•›ç‰¹æ€§çš„åŒæ—¶ï¼Œå¼•å…¥äº†æ›´ç²¾ç»†çš„ä¿¡ç”¨åˆ†é…æœºåˆ¶ï¼Œè¿™å¯¹äºé•¿å‘¨æœŸä»»åŠ¡ä¸­çš„LLMæ™ºèƒ½ä½“è®­ç»ƒå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚æ­¤å¤–ï¼Œå…¶æå‡ºçš„é”šç‚¹çŠ¶æ€åˆ†ç»„æœºåˆ¶ä¸ºè§£å†³é•¿å‘¨æœŸä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## beyond-semantics--the-unreasonable-effectiveness-of-reasonless-intermediate-tokens
### Abstract
Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ­ç§˜è¯­è¨€æ¨¡å‹ä¸­çš„â€œæ— æ•ˆæ€è€ƒâ€ï¼šæ— æ„ä¹‰ä¸­é—´ç¬¦å·çš„éå‡¡æœ‰æ•ˆæ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹åœ¨Chain of Thoughtï¼ˆCoTï¼‰ç­–ç•¥çš„æ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå°¤å…¶æ˜¯é€šè¿‡åœ¨åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šé‡‡æ ·CoTè¿›è¡Œè®­ç»ƒï¼Œä»¥å¸®åŠ©å‘ç°æ–°çš„æ¨ç†æ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™ç§è§£é‡Šæ˜¯å¦åˆç†ï¼Ÿæœ¬æ–‡é€šè¿‡ç ”ç©¶ä¸­é—´ç¬¦å·çš„è¯­ä¹‰å¦‚ä½•å®é™…å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå¯¹è¿™ä¸€è§£é‡Šè¿›è¡Œäº†æ‰¹åˆ¤æ€§è€ƒå¯Ÿã€‚ä½œè€…ä»¬å…³æ³¨çš„æ˜¯ï¼Œè¿™äº›å¸¸è¢«æ‹ŸäººåŒ–ä¸ºâ€œæ€è€ƒâ€æˆ–æ¨ç†è½¨è¿¹çš„ä¸­é—´ç¬¦å·ï¼Œæ˜¯å¦çœŸçš„ä»¥å¯é¢„æµ‹çš„æ–¹å¼å½±å“æ¨¡å‹çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
ä½œè€…ä»¬è®­ç»ƒäº†Transformeræ¨¡å‹ï¼Œä½¿å…¶åœ¨å½¢å¼ä¸Šå¯éªŒè¯çš„æ¨ç†è½¨è¿¹å’Œè§£å†³æ–¹æ¡ˆä¸Šè¿›è¡Œå­¦ä¹ ï¼ŒåŒæ—¶çº¦æŸä¸­é—´æ­¥éª¤å’Œæœ€ç»ˆè¾“å‡ºä¸å½¢å¼æ±‚è§£å™¨ï¼ˆå¦‚A*æœç´¢ï¼‰ä¿æŒä¸€è‡´ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå½¢å¼è§£é‡Šå™¨æ¥è¯„ä¼°é—®é¢˜è¯­ä¹‰å’Œé¢„æœŸç®—æ³•ï¼Œä»–ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§ä»¥åŠä¸­é—´è½¨è¿¹çš„æ­£ç¡®æ€§ï¼Œä»è€Œèƒ½å¤Ÿåˆ¤æ–­åè€…æ˜¯å¦å› æœå½±å“äº†å‰è€…ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†è¿›ä¸€æ­¥è¯æ˜è½¨è¿¹å‡†ç¡®æ€§ä¸è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§ä¹‹é—´çš„è”ç³»æ¾æ•£ï¼Œä½œè€…ä»¬è¿˜åœ¨å™ªå£°å¹²æ‰°ã€ä¸ç‰¹å®šé—®é¢˜æ— å…³çš„è½¨è¿¹ä¸Šè®­ç»ƒäº†æ¨¡å‹ã€‚ç»“æœå‘ç°ï¼Œè¿™äº›æ¨¡å‹çš„æ€§èƒ½ä¸ä»…ä¸åœ¨æ­£ç¡®æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¿æŒä¸€è‡´ï¼Œæœ‰æ—¶ç”šè‡³åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šè¡¨ç°å¾—æ›´åŠ ç¨³å¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å®Œå…¨æ­£ç¡®çš„è½¨è¿¹ä¸Šè®­ç»ƒï¼Œæ¨¡å‹åœ¨å¾—åˆ°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶ä»ç„¶ä¼šäº§ç”Ÿæ— æ•ˆçš„æ¨ç†è½¨è¿¹ã€‚æ­¤å¤–ï¼Œåœ¨å™ªå£°å¹²æ‰°çš„è½¨è¿¹ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸åœ¨æ­£ç¡®æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å¾—æ›´å¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æŒ‘æˆ˜äº†ä¸­é—´ç¬¦å·æˆ–â€œé“¾å¼æ€è€ƒâ€èƒ½å¤Ÿè¯±å¯¼å¯é¢„æµ‹æ¨ç†è¡Œä¸ºçš„å‡è®¾ï¼Œå¹¶è­¦å‘Šä¸è¦è¿‡åº¦æ‹ŸäººåŒ–è¿™äº›è¾“å‡ºæˆ–å°†å®ƒä»¬è¿‡åº¦è§£é‡Šä¸ºäººç±»æˆ–ç®—æ³•è¡Œä¸ºçš„è¯æ®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ€§èƒ½æå‡ï¼Œå‡è®¾äººç±»èˆ¬çš„æˆ–ç®—æ³•å¯è§£é‡Šçš„è½¨è¿¹è¯­ä¹‰ä¸ä»…æ˜¯ä¸å¿…è¦çš„ï¼Œç”šè‡³å¯èƒ½æ˜¯è¯¯å¯¼æ€§çš„ã€‚è¿™å¯¹äºç†è§£å’Œè®¾è®¡æ›´æœ‰æ•ˆçš„æ¨ç†æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚

## deep-reasoning-translation-via-reinforcement-learning
### Abstract
Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown
promising performance in various complex tasks. Free translation is an
important and interesting task in the multilingual world, which requires going
beyond word-for-word translation and taking cultural differences into account.
This task is still under-explored in deep reasoning LLMs. In this paper, we
introduce DeepTrans, a deep reasoning translation model that learns free
translation via reinforcement learning. Specifically, we carefully build a
reward model with pre-defined scoring criteria on both the translation results
and the thought process. Given the source sentences, the reward model teaches
the deep translation model how to think and free-translate them during
reinforcement learning. In this way, training DeepTrans does not need any
labeled translations, avoiding the human-intensive annotation or
resource-intensive data synthesis. Experimental results show the effectiveness
of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance
by 16.3% in literature translation, and outperforms strong deep reasoning
baselines as well as baselines that are fine-tuned with synthesized data.
Moreover, we summarize the failures and interesting findings during our RL
exploration. We hope this work could inspire other researchers in free
translation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦æ¨ç†ç¿»è¯‘çš„å¼ºåŒ–å­¦ä¹ ä¹‹è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œæ·±åº¦æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚OpenAIçš„o1/o3å’ŒDeepSeek-R1ï¼‰åœ¨å„ç§å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººç©ç›®çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨å¤šè¯­è¨€ä¸–ç•Œä¸­ï¼Œè‡ªç”±ç¿»è¯‘æ˜¯ä¸€é¡¹é‡è¦ä¸”æœ‰è¶£çš„ä»»åŠ¡ï¼Œå®ƒè¦æ±‚ç¿»è¯‘ä¸ä»…ä»…åœç•™åœ¨é€å­—é€å¥çš„å±‚é¢ï¼Œè¿˜éœ€è¦è€ƒè™‘åˆ°æ–‡åŒ–å·®å¼‚ã€‚è¿™é¡¹ä»»åŠ¡åœ¨æ·±åº¦æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä»ç„¶é²œæœ‰äººæ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡æ·±åº¦æ¨ç†æ¨¡å‹çš„è‡ªç”±ç¿»è¯‘èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDeepTransçš„æ·±åº¦æ¨ç†ç¿»è¯‘æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒDeepTransæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è‡ªç”±ç¿»è¯‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ„å»ºäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…å«é¢„å®šä¹‰çš„è¯„åˆ†æ ‡å‡†ï¼Œæ—¢é’ˆå¯¹ç¿»è¯‘ç»“æœï¼Œä¹Ÿé’ˆå¯¹æ€è€ƒè¿‡ç¨‹ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹æ•™ä¼šæ·±åº¦ç¿»è¯‘æ¨¡å‹å¦‚ä½•æ€è€ƒå’Œç¿»è¯‘æºå¥å­ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
DeepTransæ¨¡å‹çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•æ ‡è®°çš„ç¿»è¯‘ï¼Œè¿™é¿å…äº†è€—æ—¶çš„æ³¨é‡Šæˆ–èµ„æºå¯†é›†å‹æ•°æ®åˆæˆã€‚é€šè¿‡ä½¿ç”¨å…ˆè¿›çš„æŒ‡ä»¤å‹å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç¡®ä¿å¥–åŠ±æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½¿ç”¨Qwen2.5-7Bä½œä¸ºåŸºç¡€æ¨¡å‹ï¼ŒDeepTransåœ¨æ–‡å­¦ç¿»è¯‘ä¸­æé«˜äº†16.3%çš„æ€§èƒ½ï¼Œå¹¶ä¸”è¶…è¿‡äº†å¼ºå¤§çš„æ·±åº¦æ¨ç†åŸºçº¿ä»¥åŠä½¿ç”¨åˆæˆæ•°æ®å¾®è°ƒçš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ€»ç»“äº†åœ¨å¼ºåŒ–å­¦ä¹ æ¢ç´¢è¿‡ç¨‹ä¸­çš„å¤±è´¥æ¡ˆä¾‹å’Œæœ‰è¶£å‘ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æˆæœä¸ºè‡ªç”±ç¿»è¯‘ä»»åŠ¡æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡æ·±åº¦æ¨ç†æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¯¦ç»†æ€»ç»“äº†å¥–åŠ±æ¨¡å‹çš„å…³é”®ç»„æˆéƒ¨åˆ†å’Œå¤±è´¥æ¡ˆä¾‹ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å®è´µçš„ç»éªŒæ•™è®­ã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿæ¿€å‘å…¶ä»–ç ”ç©¶è€…åœ¨è‡ªç”±ç¿»è¯‘é¢†åŸŸçš„ç ”ç©¶ã€‚

## dgro--enhancing-llm-reasoning-via-exploration-exploitation-control-and-reward-variance-management
### Abstract
Inference scaling further accelerates Large Language Models (LLMs) toward
Artificial General Intelligence (AGI), with large-scale Reinforcement Learning
(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning
approaches usually rely on handcrafted rule-based reward functions. However,
the tarde-offs of exploration and exploitation in RL algorithms involves
multiple complex considerations, and the theoretical and empirical impacts of
manually designed reward functions remain insufficiently explored. In this
paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL
algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional
regularization coefficient into two independent hyperparameters: one scales the
policy gradient term, and the other regulates the distance from the sampling
policy. This decoupling not only enables precise control over balancing
exploration and exploitation, but also can be seamlessly extended to Online
Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward
Optimization. On the other hand, we observe that reward variance significantly
affects both convergence speed and final model performance. We conduct both
theoretical analysis and extensive empirical validation to assess DGRO,
including a detailed ablation study that investigates its performance and
optimization dynamics. Experimental results show that DGRO achieves
state-of-the-art performance on the Logic dataset with an average accuracy of
96.9\%, and demonstrates strong generalization across mathematical benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DGROï¼šé€šè¿‡æ¢ç´¢-åˆ©ç”¨æ§åˆ¶å’Œå¥–åŠ±æ–¹å·®ç®¡ç†æå‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ¨ç†è§„æ¨¡åŒ–çš„æ¨è¿›ï¼Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åŠ é€Ÿè¿ˆå‘äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰ã€‚å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨é‡Šæ”¾é•¿é“¾æ¨ç†èƒ½åŠ›æ–¹é¢æ‰®æ¼”äº†å…³é”®è§’è‰²ã€‚ç„¶è€Œï¼Œç°ä»£æ¨ç†æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œè¿™äº›æ–¹æ³•åœ¨æ¢ç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„æƒè¡¡æ¶‰åŠå¤šä¸ªå¤æ‚çš„è€ƒé‡ï¼Œè€Œæ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°çš„ç†è®ºå’Œå®è¯å½±å“ä»ç„¶æ¢è®¨ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„RLç®—æ³•ï¼Œä»¥ä¼˜åŒ–LLMçš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†åˆ†ç¦»ç»„å¥–åŠ±ä¼˜åŒ–ï¼ˆDGROï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„RLç®—æ³•ï¼Œç”¨äºLLMæ¨ç†ä»»åŠ¡ã€‚DGROå°†ä¼ ç»Ÿçš„æ­£åˆ™åŒ–ç³»æ•°åˆ†è§£ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„è¶…å‚æ•°ï¼šä¸€ä¸ªç”¨äºç¼©æ”¾ç­–ç•¥æ¢¯åº¦é¡¹ï¼Œå¦ä¸€ä¸ªç”¨äºè°ƒèŠ‚é‡‡æ ·ç­–ç•¥çš„è·ç¦»ã€‚è¿™ç§åˆ†ç¦»ä¸ä»…èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶æ¢ç´¢å’Œåˆ©ç”¨çš„å¹³è¡¡ï¼Œè¿˜å¯ä»¥æ— ç¼æ‰©å±•åˆ°Kimi k1.5ä¸­çš„åœ¨çº¿ç­–ç•¥é•œåƒä¸‹é™ï¼ˆOPMDï¼‰ç®—æ³•å’Œç›´æ¥å¥–åŠ±ä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è§‚å¯Ÿåˆ°å¥–åŠ±æ–¹å·®æ˜¾è‘—å½±å“æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ¨¡å‹æ€§èƒ½ï¼Œæœ¬æ–‡è¿›è¡Œäº†ç†è®ºåˆ†æå’Œå¹¿æ³›çš„å®è¯éªŒè¯ï¼ŒåŒ…æ‹¬è¯¦ç»†çš„æ¶ˆèç ”ç©¶ï¼Œä»¥è¯„ä¼°DGROçš„æ€§èƒ½å’Œä¼˜åŒ–åŠ¨æ€ã€‚ç ”ç©¶å‘ç°ï¼Œé€‚å½“çš„è¶…å‚æ•°ç»„åˆå’Œè¾ƒé«˜çš„å¥–åŠ±æ–¹å·®å¯ä»¥ä¿ƒè¿›æ›´å¿«æ”¶æ•›å’Œæ›´å¥½çš„æœ€ç»ˆå‡†ç¡®åº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒDGROåœ¨é€»è¾‘æ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡96.9%çš„å‡†ç¡®åº¦ï¼Œå¹¶åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¯æ˜äº†DGROåœ¨LLMæ¨ç†ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„DGROç®—æ³•ä¸ºLLMæ¨ç†æä¾›äº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå…¶ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ä¸ºæ¢ç´¢-åˆ©ç”¨æ§åˆ¶å’Œå¥–åŠ±æ–¹å·®ç®¡ç†æä¾›äº†å®è·µæŒ‡å¯¼ã€‚DGROçš„æˆåŠŸè¡¨æ˜ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´å¥–åŠ±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡LLMçš„æ¨ç†èƒ½åŠ›å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç†è®ºåˆ†æä¸ºç†è§£å’Œè®¾è®¡æ›´æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æä¾›äº†é‡è¦è§è§£ã€‚

## j4r--learning-to-judge-with-equivalent-initial-state-group-relative-policy-optimization
### Abstract
To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œJ4Rï¼šåˆ©ç”¨ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å­¦ä¹ è¯„åˆ¤â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•é€Ÿåº¦ä¸æ–­åŠ å¿«ï¼Œæ¨¡å‹è¾“å‡ºè¯„ä»·å·²ç»ä»è€—æ—¶çš„äººå·¥è¯„ä»·è½¬å‘è‡ªåŠ¨è¯„ä»·ã€‚åœ¨è‡ªåŠ¨è¯„ä»·ä¸­ï¼ŒLLMè‡ªèº«è¢«ç”¨äºè¯„ä¼°å’Œè¯„åˆ¤å…¶ä»–æ¨¡å‹çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMä½œä¸ºè¯„åˆ¤è€…ï¼ˆLLM-as-judgeï¼‰æ¨¡å‹åœ¨å¤„ç†ç›¸å¯¹ç®€å•çš„é¢†åŸŸï¼ˆå¦‚èŠå¤©è´¨é‡ï¼‰æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦æ›´å¤šå®è´¨æ€§å†…å®¹å’ŒæŒ‘æˆ˜æ€§å†…å®¹çš„æ¨ç†å¯†é›†å‹é¢†åŸŸä¸­å´é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³ç°æœ‰è¯„åˆ¤è€…çš„ä¸è¶³ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒè¯„åˆ¤è€…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å…è®¸è®­ç»ƒè¯„åˆ¤è€…åœ¨é¢å¯¹æ›´å¤æ‚çš„è¯„ä»·è®¾ç½®æ—¶å¯¹ä½ç½®åå·®å…·æœ‰é²æ£’æ€§ã€‚é€šè¿‡å°†è¾“å…¥ä¸Šä¸‹æ–‡çš„ä¸åŒè½¬æ¢è§†ä¸ºç­‰æ•ˆï¼Œè¯¥ç®—æ³•åœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒè´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†è¯„åˆ¤è€…çš„ä¸€è‡´æ€§å’Œé²æ£’æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å¼•å…¥äº†ReasoningJudgeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„åˆ¤è€…åœ¨å¤šæ ·åŒ–æ¨ç†è®¾ç½®ä¸­çš„è¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–äº†ä¹‹å‰å·¥ä½œä¸­æœªæ¶‰åŠåˆ°çš„æ¨ç†åœºæ™¯ã€‚è¿™ä¸ªåŸºå‡†åŒ…å«äº†1483ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æˆå¯¹æ ·æœ¬ï¼Œä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°æä¾›äº†æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æœ¬æ–‡è®­ç»ƒäº†Judge for Reasoningï¼ˆJ4Rï¼‰ï¼Œä¸€ä¸ªä½¿ç”¨EIS-GRPOç®—æ³•è®­ç»ƒçš„7Bè¯„åˆ¤è€…æ¨¡å‹ã€‚J4Råœ¨æ¨ç†è¯„ä»·æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†GPT-4oå’Œä¸‹ä¸€ä¸ªæœ€ä½³å°å‹è¯„åˆ¤è€…ï¼Œæ€§èƒ½åˆ†åˆ«æé«˜äº†6.7%å’Œ9%ï¼Œåœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šéƒ½åŒ¹é…æˆ–è¶…è¿‡äº†æ›´å¤§GRPOè®­ç»ƒè¯„åˆ¤è€…çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨EIS-GRPOç®—æ³•è®­ç»ƒçš„J4Ræ¨¡å‹åœ¨æ¨ç†è¯„ä»·æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ä»…è¶…è¿‡äº†GPT-4oï¼Œè€Œä¸”åœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šçš„è¡¨ç°ä¸æ›´å¤§çš„GRPOè®­ç»ƒè¯„åˆ¤è€…ç›¸å½“æˆ–æ›´ä¼˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºè‡ªåŠ¨è¯„ä»·é¢†åŸŸæä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­ã€‚EIS-GRPOç®—æ³•ä¸ºè®­ç»ƒé²æ£’æ€§å¼ºçš„è¯„åˆ¤è€…æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œè€ŒReasoningJudgeBenchåˆ™ä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°æä¾›äº†æ–°çš„æŒ‘æˆ˜å’ŒåŸºå‡†ã€‚æ­¤å¤–ï¼ŒJ4Ræ¨¡å‹çš„æˆåŠŸè¡¨æ˜ï¼Œå³ä½¿æ˜¯å°å‹æ¨¡å‹ä¹Ÿå¯ä»¥é€šè¿‡é€‚å½“çš„æ–¹æ³•åœ¨æ¨ç†è¯„ä»·ä¸­å–å¾—æ˜¾è‘—æ•ˆæœã€‚è¿™äº›å‘ç°å¯¹äºæœªæ¥çš„è‡ªåŠ¨è¯„ä»·ç³»ç»Ÿå’ŒLLMçš„å‘å±•éƒ½å…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚

## an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents
### Abstract
Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†æœç´¢æ··åˆå‹LLMä»£ç†ä¸­çš„åº”ç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„è¡¨ç°è¶Šæ¥è¶Šå‡ºè‰²ï¼Œå¦‚ä½•å°†è¿™äº›æ¨¡å‹è®­ç»ƒæˆèƒ½å¤Ÿè¿›è¡Œå¤æ‚æ¨ç†å¹¶è§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„æ™ºèƒ½ä»£ç†æˆä¸ºç ”ç©¶çš„çƒ­ç‚¹ã€‚è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«ç”¨äºè®­ç»ƒLLMï¼Œä½¿å…¶èƒ½å¤Ÿç»“åˆæ¨ç†å’Œæœç´¢å¼•æ“ä½¿ç”¨ï¼Œå½¢æˆæ‰€è°“çš„LLM-basedæœç´¢ä»£ç†ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•ä¼˜åŒ–è¿™äº›ä»£ç†çš„è®¾è®¡ï¼Œå°¤å…¶æ˜¯å…³äºå¥–åŠ±è®¾è®¡ã€LLMçš„é€‰æ‹©å’Œæœç´¢å¼•æ“åœ¨RLè¿‡ç¨‹ä¸­çš„ä½œç”¨ï¼Œç›®å‰ä»ç¼ºä¹æ·±å…¥çš„ç ”ç©¶ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å®è¯ç ”ç©¶ï¼Œæä¾›å…³äºè¿™äº›å…³é”®å› ç´ çš„è§è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¥–åŠ±è®¾è®¡
æœ¬æ–‡å‘ç°ï¼Œæ ¼å¼å¥–åŠ±ï¼ˆformat rewardsï¼‰å¯¹äºæé«˜æœ€ç»ˆæ€§èƒ½éå¸¸æœ‰æ•ˆï¼Œè€Œä¸­é—´æ£€ç´¢å¥–åŠ±ï¼ˆintermediate retrieval rewardsï¼‰çš„å½±å“åˆ™æœ‰é™ã€‚æ ¼å¼å¥–åŠ±æœ‰åŠ©äºæ¨¡å‹éµå¾ªä»£ç†è¡ŒåŠ¨çš„æ ¼å¼ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMçš„é€‰æ‹©å’Œåˆå§‹åŒ–
ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMçš„è§„æ¨¡å’Œåˆå§‹åŒ–ï¼ˆé€šç”¨å‹ä¸æ¨ç†ä¸“ç”¨å‹ï¼‰å¯¹RLçš„ç»“æœæœ‰æ˜¾è‘—å½±å“ã€‚é€šç”¨å‹LLMåœ¨RLè®¾ç½®ä¸­è¡¨ç°ä¼˜äºæ¨ç†ä¸“ç”¨å‹LLMï¼Œå¯èƒ½æ˜¯å› ä¸ºåè€…åœ¨è®­ç»ƒåˆæœŸé˜¶æ®µçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›è¾ƒå¼±ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæœç´¢å¼•æ“çš„é€‰æ‹©
æœç´¢å¼•æ“çš„è´¨é‡å¯¹RLçš„è®­ç»ƒåŠ¨æ€æœ‰å¼ºçƒˆå½±å“ã€‚ä½¿ç”¨éä¿¡æ¯æ€§æœç´¢å¼•æ“ï¼ˆå¦‚éšæœºå™ªå£°ï¼‰ä¼šå¯¼è‡´ä»£ç†å®Œå…¨é¿å…æ£€ç´¢ï¼Œè€Œä½¿ç”¨å¼±æœç´¢å¼•æ“ï¼ˆå¦‚BM25ï¼‰ä¼šå¯¼è‡´é¢‘ç¹ä½†æ•ˆç‡ä½ä¸‹çš„æœç´¢è°ƒç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºå¤§çš„æœç´¢å¼•æ“ï¼ˆå¦‚å¯†é›†æ£€ç´¢å™¨ï¼‰èƒ½å¤Ÿäº§ç”Ÿæ›´ç¨³å®šçš„è®­ç»ƒæ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ ¼å¼å¥–åŠ±çš„ä»£ç†åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œè€Œä¸­é—´æ£€ç´¢å¥–åŠ±çš„åŠ å…¥å¹¶æ²¡æœ‰å¸¦æ¥ä¸€è‡´çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œé€šç”¨å‹LLMåœ¨RLè®¾ç½®ä¸­è¡¨ç°æ›´å¥½ï¼Œä¸”æ¨¡å‹è§„æ¨¡çš„å¢åŠ é€šå¸¸èƒ½æé«˜æœ€ç»ˆæ€§èƒ½ï¼Œä½†å›æŠ¥é€’å‡ã€‚æœç´¢å¼•æ“çš„é€‰æ‹©å¯¹è®­ç»ƒåŠ¨æ€å’Œä»£ç†çš„é²æ£’æ€§æœ‰é‡è¦å½±å“ï¼Œè€Œåœ¨æ¨ç†æ—¶ï¼Œä»£ç†å¯¹ä¸åŒçš„æ£€ç´¢ç³»ç»Ÿè¡¨ç°å‡ºä¸€å®šçš„é²æ£’æ€§ï¼Œå¼ºå¤§çš„æœç´¢å¼•æ“èƒ½å¤Ÿå¸¦æ¥æ›´å¥½çš„ä¸‹æ¸¸æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºæ„å»ºå’Œéƒ¨ç½²é¢å‘ç°å®ä¸–ç•Œåº”ç”¨çš„LLM-basedæœç´¢ä»£ç†æä¾›äº†é‡è¦çš„æŒ‡å¯¼åŸåˆ™ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆç†çš„å¥–åŠ±è®¾è®¡ã€LLMçš„é€‰æ‹©å’Œæœç´¢å¼•æ“çš„ä¼˜åŒ–æ˜¯æˆåŠŸè®­ç»ƒæœç´¢ä»£ç†çš„å…³é”®å› ç´ ã€‚è¿™äº›å‘ç°å¯¹äºæœªæ¥çš„ç ”ç©¶å’Œå®è·µéƒ½å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ï¼Œä¾¿äºç¤¾åŒºè¿›ä¸€æ­¥éªŒè¯å’Œæ‰©å±•è¿™äº›ç ”ç©¶æˆæœã€‚

## gvpo--group-variance-policy-optimization-for-large-language-model-post-training
### Abstract
Post-training plays a crucial role in refining and aligning large language
models to meet specific tasks and human preferences. While recent advancements
in post-training techniques, such as Group Relative Policy Optimization (GRPO),
leverage increased sampling with relative reward scoring to achieve superior
performance, these methods often suffer from training instability that limits
their practical adoption. To address this challenge, we present Group Variance
Policy Optimization (GVPO). GVPO incorporates the analytical solution to
KL-constrained reward maximization directly into its gradient weights, ensuring
alignment with the optimal policy. The method provides intuitive physical
interpretations: its gradient mirrors the mean squared error between the
central distance of implicit rewards and that of actual rewards. GVPO offers
two key advantages: (1) it guarantees a unique optimal solution, exactly the
KL-constrained reward maximization objective, (2) it supports flexible sampling
distributions that avoids on-policy and importance sampling limitations. By
unifying theoretical guarantees with practical adaptability, GVPO establishes a
new paradigm for reliable and versatile LLM post-training.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GVPOï¼šå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åè®­ç»ƒçš„æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡å¤§é‡æ•°æ®å­¦ä¹ åˆ°äº†å¹¿æ³›çš„é€šç”¨è¯­è¨€æ¨¡å¼ï¼Œä½†å…¶å®é™…åº”ç”¨å’Œä¸äººç±»ä»·å€¼è§‚çš„å¥‘åˆåº¦ä¾èµ–äºåè®­ç»ƒçš„ç²¾ç»†åŒ–è°ƒæ•´ã€‚ç°æœ‰çš„åè®­ç»ƒæŠ€æœ¯ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œå¯¹äºå°†æ¨¡å‹é€‚åº”ç‰¹å®šåº”ç”¨å’Œç¡®ä¿è¾“å‡ºç¬¦åˆä¼¦ç†ã€å®‰å…¨åŠç”¨æˆ·ä¸­å¿ƒæ ‡å‡†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ä¸€äº›åè®­ç»ƒæ–¹æ³•ï¼Œå¦‚ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè™½ç„¶é€šè¿‡å¢åŠ é‡‡æ ·å’Œç›¸å¯¹å¥–åŠ±è¯„åˆ†æ¥æå‡æ€§èƒ½ï¼Œä½†å¾€å¾€å­˜åœ¨è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ç¾¤æ–¹å·®ç­–ç•¥ä¼˜åŒ–ï¼ˆGVPOï¼‰æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
GVPOæ–¹æ³•å°†KLçº¦æŸä¸‹çš„å¥–åŠ±æœ€å¤§åŒ–é—®é¢˜çš„è§£æè§£ç›´æ¥æ•´åˆåˆ°æ¢¯åº¦æƒé‡ä¸­ï¼Œç¡®ä¿äº†ä¸æœ€ä¼˜ç­–ç•¥çš„å¯¹é½ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ä¸ªç›´è§‚çš„ç‰©ç†è§£é‡Šï¼šå…¶æ¢¯åº¦åæ˜ äº†éšå«å¥–åŠ±çš„ä¸­å¿ƒè·ç¦»ä¸å®é™…å¥–åŠ±çš„ä¸­å¿ƒè·ç¦»ä¹‹é—´çš„å‡æ–¹è¯¯å·®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
GVPOå…·æœ‰ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å®ƒä¿è¯äº†å”¯ä¸€çš„æœ€ä¼˜è§£ï¼Œå³KLçº¦æŸä¸‹çš„å¥–åŠ±æœ€å¤§åŒ–ç›®æ ‡ï¼›ï¼ˆ2ï¼‰å®ƒæ”¯æŒçµæ´»çš„é‡‡æ ·åˆ†å¸ƒï¼Œé¿å…äº†ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­çš„on-policyå’Œé‡è¦æ€§é‡‡æ ·çš„é™åˆ¶ã€‚è¿™ä½¿å¾—GVPOæˆä¸ºä¸€ä¸ªæ—¢å¯é åˆçµæ´»çš„LLMåè®­ç»ƒæ–°èŒƒå¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯¦ç»†åˆ†æäº†GVPOçš„ç»“æ„ä¸ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†GVPOåœ¨ä¿è¯ç†è®ºæœ€ä¼˜è§£çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ”¯æŒçµæ´»çš„æ•°æ®é‡‡æ ·ï¼Œé¿å…äº†æ¢¯åº¦çˆ†ç‚¸é£é™©ï¼Œä¸”ä¸å¼•å…¥é€šè¿‡å‰ªè¾‘æŠ€æœ¯äº§ç”Ÿçš„åå·®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GVPOæ–¹æ³•ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è®­ç»ƒä¸ç¨³å®šæ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å…¶ç†è®ºä¸Šçš„ä¿è¯å’Œå®é™…åº”ç”¨çš„çµæ´»æ€§ï¼Œä¸ºåç»­çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å®è´µçš„å‚è€ƒã€‚æ­¤å¤–ï¼ŒGVPOå¯¹äºç†è§£å¥–åŠ±æ¨¡å‹ä¸ç­–ç•¥ä¹‹é—´çš„å…³ç³»ä¹Ÿæä¾›äº†æ–°çš„è§†è§’ã€‚

## tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning
### Abstract
Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Tool-Starï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹èƒ½LLMçš„å¤šå·¥å…·ååŒæ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å±•ç°å‡ºæ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¦‚ä½•åˆ©ç”¨RLç®—æ³•æœ‰æ•ˆèµ‹èƒ½LLMè¿›è¡Œå¤šå·¥å…·ååŒæ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨å®é™…ä¸–ç•Œä¸­ï¼Œæ¨ç†åœºæ™¯å¾€å¾€éœ€è¦æ¨¡å‹æ•´åˆå¤šç§èƒ½åŠ›ï¼Œä¾‹å¦‚æ·±åº¦ä¿¡æ¯æœç´¢ã€é•¿æœŸçŸ¥è¯†è®°å¿†å’Œç²¾ç¡®è®¡ç®—ï¼Œè€Œç°æœ‰çš„å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€å·¥å…·çš„ä½¿ç”¨ï¼Œå¯¹å¤šå·¥å…·ååŒæ¨ç†çš„ç ”ç©¶è¿˜ç›¸å¯¹è¾ƒå°‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†Tool-Starï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMèƒ½å¤Ÿåœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤šä¸ªå¤–éƒ¨å·¥å…·ã€‚Tool-Staré›†æˆäº†å…­ç§ç±»å‹çš„å·¥å…·ï¼Œå¹¶åœ¨æ•°æ®åˆæˆå’Œè®­ç»ƒç®—æ³•ä¸­é‡‡ç”¨äº†ç³»ç»ŸåŒ–è®¾è®¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†è§£å†³å·¥å…·ä½¿ç”¨æ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„TIRæ•°æ®åˆæˆç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†å·¥å…·é›†æˆæç¤ºå’ŒåŸºäºæç¤ºçš„é‡‡æ ·ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡çš„å·¥å…·ä½¿ç”¨è½¨è¿¹ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è´¨é‡å½’ä¸€åŒ–å’Œéš¾åº¦æ„ŸçŸ¥åˆ†ç±»è¿‡ç¨‹ï¼Œä»¥è¿‡æ»¤æ‰ä¸åˆç†çš„å·¥å…·ä½¿ç”¨æ ·æœ¬ï¼Œå¹¶å°†æ•°æ®é›†ä»æ˜“åˆ°éš¾ç»„ç»‡èµ·æ¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„TIRè®­ç»ƒæ¡†æ¶ï¼Œä»¥å¢å¼ºLLMçš„å¤šå·¥å…·ååŒæ¨ç†èƒ½åŠ›ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯å†·å¯åŠ¨å¾®è°ƒï¼Œå¼•å¯¼LLMé€šè¿‡å·¥å…·è°ƒç”¨åé¦ˆæ¢ç´¢æ¨ç†æ¨¡å¼ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯å¤šå±‚æ¬¡å¥–åŠ±è®¾è®¡çš„å¤šå·¥å…·è‡ªæˆ‘æ‰¹è¯„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¼ºåŒ–LLMå¯¹å¥–åŠ±åŸåˆ™çš„ç†è§£å¹¶ä¿ƒè¿›æœ‰æ•ˆçš„å·¥å…·åä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è¶…è¿‡10ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTool-Starå±•ç°å‡ºäº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTool-Staråœ¨ä¿è¯å·¥å…·ä½¿ç”¨æ•ˆç‡çš„åŒæ—¶ï¼Œè¿˜èƒ½ç¡®ä¿æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Tool-Staræ¡†æ¶ä¸ºLLMçš„å¤šå·¥å…·ååŒæ¨ç†æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå…¶åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
- è®¾è®¡äº†ä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡å·¥å…·ä½¿ç”¨è½¨è¿¹çš„æ•°æ®åˆæˆç®¡é“ã€‚
- æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œæœ‰æ•ˆæå‡äº†å¤šå·¥å…·ååŒæ¨ç†çš„èƒ½åŠ›ã€‚
- å¼•å…¥äº†å¤šå±‚æ¬¡å¥–åŠ±æœºåˆ¶ï¼Œå¼ºåŒ–äº†æ¨¡å‹å¯¹å¥–åŠ±åŸåˆ™çš„ç†è§£ï¼Œä¿ƒè¿›äº†å·¥å…·ä¹‹é—´çš„æœ‰æ•ˆåä½œã€‚
è¿™äº›æ–¹æ³•å¯¹äºç†è§£å’Œä¼˜åŒ–LLMçš„æ¨ç†è¿‡ç¨‹å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## distilling-the-implicit-multi-branch-structure-in-llms--reasoning-via-reinforcement-learning
### Abstract
Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±åº¦è§£æï¼šå¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æç‚¼å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„éšå¼å¤šåˆ†æ”¯æ¨ç†ç»“æ„

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚ä½•å°†è¿™äº›æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¼ é€’ç»™è¾ƒå°çš„LLMæˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•è™½ç„¶èƒ½å¤Ÿæé«˜å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒå¾€å¾€åªèƒ½å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„è¡¨é¢æ¨ç†è·¯å¾„ï¼Œæ— æ³•æ•æ‰åˆ°å…¶èƒŒåçš„çœŸå®æ¨ç†è¿‡ç¨‹ã€‚çœŸå®æ¨ç†æ¶‰åŠåˆ°åœ¨å¤šä¸ªå€™é€‰å­é—®é¢˜ä¸­é€‰æ‹©é€‚å½“çš„é—®é¢˜ï¼ˆå…ƒæ¨ç†ï¼‰å’Œè§£å†³è¯¥é—®é¢˜ï¼ˆè§£å†³é—®é¢˜ï¼‰ï¼Œå½¢æˆäº†ä¸€ä¸ªéšå¼çš„å¤šåˆ†æ”¯ç»“æ„ã€‚SFTæ–¹æ³•æ— æ³•æç‚¼è¿™ç§ç»“æ„ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹æ— æ³•è¿›è¡ŒçœŸæ­£çš„è‡ªä¸»æ¨ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRLKDçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åŸºäºçš„çŸ¥è¯†æç‚¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³SFTæ–¹æ³•æ— æ³•æç‚¼LLMéšå¼å¤šåˆ†æ”¯æ¨ç†ç»“æ„çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥äººç±»è®¤çŸ¥ç¥ç»ç§‘å­¦çš„è§‚ç‚¹ï¼Œå°†çœŸå®æ¨ç†åˆ†ä¸ºå…ƒæ¨ç†å’Œè§£å†³é—®é¢˜ä¸¤ä¸ªé˜¶æ®µã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆç»“æ„å¥–åŠ±æ¨¡å‹ï¼ˆGSRMï¼‰ï¼Œå®ƒå¯ä»¥å°†æ¨ç†è·¯å¾„è½¬æ¢ä¸ºå¤šä¸ªå…ƒæ¨ç†-è§£å†³é—®é¢˜æ­¥éª¤çš„åºåˆ—ï¼Œå¹¶è®¡ç®—å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹æ¨ç†ç»“æ„ä¹‹é—´çš„åŒ¹é…åº¦ä½œä¸ºå¥–åŠ±ã€‚ç»“åˆRLï¼ŒRLKDèƒ½å¤ŸæŒ‡å¯¼å­¦ç”ŸLLMåœ¨æ­¥éª¤çº§åˆ«ä¸Šè¿›è¡Œæ›´å¥½çš„é‡‡æ ·ï¼Œä»è€Œé€‰æ‹©æœ€åˆé€‚çš„å­é—®é¢˜å¹¶è§£å†³å®ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨åªæœ‰0.1%è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒRLKDä¹Ÿèƒ½åœ¨Qwen2.5-Mathä»»åŠ¡ä¸­è¶…è¶Šä¼ ç»Ÿçš„SFT-RLæµç¨‹ã€‚æ­¤å¤–ï¼ŒRLKDè¿˜èƒ½å¤Ÿè¿›ä¸€æ­¥é‡Šæ”¾å­¦ç”ŸLLMçš„æ¨ç†æ½œåŠ›ï¼Œæ¯”åŸºäºSFTçš„æç‚¼æ–¹æ³•è¡¨ç°å¾—æ›´å¥½ï¼Œå¹¶ä¸”è¶…è¿‡äº†ç°æœ‰çš„RLåŸºçº¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯ä»¥æ›´æœ‰æ•ˆåœ°æç‚¼å’Œä¼ é€’LLMçš„æ¨ç†ç»“æ„ï¼Œè¿™å¯¹äºå¼€å‘èµ„æºå—é™çš„å›¢é˜Ÿæ¥è¯´æ˜¯ä¸€ä¸ªé‡è¦çš„å¯ç¤ºã€‚æ­¤å¤–ï¼ŒGSRMçš„è®¾è®¡ä¸ºå¦‚ä½•è¯„ä¼°å’Œä¼˜åŒ–æ¨ç†è·¯å¾„çš„ç»“æ„æä¾›äº†æ–°çš„æ€è·¯ï¼Œè¿™å¯¹äºæœªæ¥çš„LLMç ”ç©¶å’Œåº”ç”¨å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## streamrl--scalable--heterogeneous--and-elastic-rl-for-llms-with-disaggregated-stream-generation
### Abstract
Reinforcement learning (RL) has become the core post-training technique for
large language models (LLMs). RL for LLMs involves two stages: generation and
training. The LLM first generates samples online, which are then used to derive
rewards for training. The conventional view holds that the colocated
architecture, where the two stages share resources via temporal multiplexing,
outperforms the disaggregated architecture, in which dedicated resources are
assigned to each stage. However, in real-world deployments, we observe that the
colocated architecture suffers from resource coupling, where the two stages are
constrained to use the same resources. This coupling compromises the
scalability and cost-efficiency of colocated RL in large-scale training. In
contrast, the disaggregated architecture allows for flexible resource
allocation, supports heterogeneous training setups, and facilitates
cross-datacenter deployment.
  StreamRL is designed with disaggregation from first principles and fully
unlocks its potential by addressing two types of performance bottlenecks in
existing disaggregated RL frameworks: pipeline bubbles, caused by stage
dependencies, and skewness bubbles, resulting from long-tail output length
distributions. To address pipeline bubbles, StreamRL breaks the traditional
stage boundary in synchronous RL algorithms through stream generation and
achieves full overlapping in asynchronous RL. To address skewness bubbles,
StreamRL employs an output-length ranker model to identify long-tail samples
and reduces generation time via skewness-aware dispatching and scheduling.
Experiments show that StreamRL improves throughput by up to 2.66x compared to
existing state-of-the-art systems, and improves cost-effectiveness by up to
1.33x in a heterogeneous, cross-datacenter setting.
### ğŸŒŸ è®ºæ–‡è§£è¯» | StreamRLï¼šé¢å‘å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¼¹æ€§å¼ºåŒ–å­¦ä¹ æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å…³é”®åè®­ç»ƒæŠ€æœ¯ï¼Œä¼ ç»Ÿçš„RLè®­ç»ƒæ¡†æ¶ä¸»è¦é‡‡ç”¨ä¸¤ç§æ¶æ„ï¼šé›†ä¸­å¼ï¼ˆcolocatedï¼‰å’Œåˆ†å¸ƒå¼ï¼ˆdisaggregatedï¼‰ã€‚é›†ä¸­å¼æ¶æ„é€šè¿‡æ—¶é—´å¤ç”¨å…±äº«èµ„æºï¼Œè€Œåˆ†å¸ƒå¼æ¶æ„ä¸ºæ¯ä¸ªé˜¶æ®µåˆ†é…ä¸“ç”¨çš„èµ„æºã€‚å°½ç®¡åˆ†å¸ƒå¼æ¶æ„åœ¨ç†è®ºä¸Šå…·æœ‰èµ„æºåˆ†é…çš„çµæ´»æ€§ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œä¼ ç»Ÿçš„åˆ†å¸ƒå¼æ¡†æ¶å­˜åœ¨èµ„æºé—²ç½®å’Œé•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨ä¸å……åˆ†ã€‚

æœ¬æ–‡æå‡ºäº†StreamRLï¼Œä¸€ç§é’ˆå¯¹LLMçš„åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¡†æ¶ä¸­çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œæé«˜èµ„æºåˆ©ç”¨ç‡å’Œè®­ç»ƒæ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
StreamRLä»åŸç†ä¸Šé‡‡ç”¨åˆ†å¸ƒå¼æ¶æ„ï¼Œé€šè¿‡æŠ½è±¡å‡ºç”ŸæˆæœåŠ¡ï¼ˆSGSï¼‰å’Œè®­ç»ƒå™¨ï¼ˆTrainerï¼‰ä¸¤ä¸ªé˜¶æ®µï¼Œä½¿å¾—Trainerå¯ä»¥æµå¼åœ°æ¥æ”¶SGSç”Ÿæˆçš„æ ·æœ¬ï¼Œä»è€Œå‡å°‘èµ„æºé—²ç½®ï¼Œæé«˜å¹¶å‘æ‰§è¡Œæ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†è§£å†³é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼ŒStreamRLå¼•å…¥äº†ä¸€ä¸ªè¾“å‡ºé•¿åº¦æ’åºæ¨¡å‹æ¥è¯†åˆ«é•¿å°¾æ ·æœ¬ï¼Œå¹¶é€šè¿‡åæ–œæ„ŸçŸ¥çš„è°ƒåº¦æœºåˆ¶ï¼Œæœ‰é€‰æ‹©åœ°ä¸ºé•¿å°¾æ ·æœ¬åˆ†é…èµ„æºï¼Œä»è€Œå‡å°‘ç”Ÿæˆæ—¶é—´å¹¶æé«˜æ•´ä½“æ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒStreamRLåœ¨ååé‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›çš„ç³»ç»Ÿæé«˜äº†2.66å€ï¼Œåœ¨å¼‚æ„ã€è·¨æ•°æ®ä¸­å¿ƒçš„è®¾ç½®ä¸­ï¼Œæˆæœ¬æ•ˆç›Šæé«˜äº†1.33å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
StreamRLçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒå¼æ¶æ„ä¸‹å¦‚ä½•æœ‰æ•ˆç®¡ç†èµ„æºåˆ†é…å’Œè°ƒåº¦ã€‚æ­¤å¤–ï¼Œå…¶æå‡ºçš„åŠ¨æ€èµ„æºè°ƒæ•´æœºåˆ¶ä¸ºä¿æŒè®­ç»ƒè¿‡ç¨‹ä¸­å„é˜¶æ®µå¹³è¡¡æ‰§è¡Œæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç¯‡è®ºæ–‡å¯¹äºå¸Œæœ›åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸè¿›è¡Œæ·±å…¥ç ”ç©¶çš„å­¦è€…å’Œå·¥ç¨‹å¸ˆæ¥è¯´ï¼Œå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## ufo-rl--uncertainty-focused-optimization-for-efficient-reinforcement-learning-data-selection
### Abstract
Scaling RL for LLMs is computationally expensive, largely due to
multi-sampling for policy optimization and evaluation, making efficient data
selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,
we hypothesize LLMs learn best from data within their potential comprehension
zone. Addressing the limitation of conventional, computationally intensive
multi-sampling methods for data assessment, we introduce UFO-RL. This novel
framework uses a computationally efficient single-pass uncertainty estimation
to identify informative data instances, achieving up to 185x faster data
evaluation. UFO-RL leverages this metric to select data within the estimated
ZPD for training. Experiments show that training with just 10% of data selected
by UFO-RL yields performance comparable to or surpassing full-data training,
reducing overall training time by up to 16x while enhancing stability and
generalization. UFO-RL offers a practical and highly efficient strategy for
scaling RL fine-tuning of LLMs by focusing learning on valuable data.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "UFO-RLï¼šèšç„¦ä¸ç¡®å®šæ€§çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ•°æ®é€‰æ‹©ç­–ç•¥"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„åº”ç”¨ï¼Œä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜æ˜¯é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨ç­–ç•¥ä¼˜åŒ–å’Œè¯„ä¼°ä¸­éœ€è¦å¤šæ¬¡é‡‡æ ·ã€‚è¿™ç§è®¡ç®—ä¸Šçš„è´Ÿæ‹…ä½¿å¾—é«˜æ•ˆçš„æ•°æ®é€‰æ‹©å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡çš„åŠ¨æœºåœ¨äºæå‡ºä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡å—åˆ°â€œæœ€è¿‘å‘å±•åŒºâ€ï¼ˆZone of Proximal Development, ZPDï¼‰ç†è®ºçš„å¯å‘ï¼Œå‡è®¾LLMä»å®ƒä»¬å°šæœªå®Œå…¨æŒæ¡ä½†å…·æœ‰æ½œåœ¨ç†è§£èƒ½åŠ›çš„æ•°æ®ä¸­å­¦ä¹ æ•ˆæœæœ€ä½³ã€‚è¿™ä¸€å‡è®¾ä¸ºé«˜æ•ˆçš„æ•°æ®é€‰æ‹©æä¾›äº†ç†è®ºåŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†å…‹æœä¼ ç»Ÿæ–¹æ³•ä¸­è®¡ç®—æˆæœ¬é«˜çš„ç¼ºç‚¹ï¼Œæœ¬æ–‡æå‡ºäº†UFO-RLï¼ˆUncertainty-Focused Optimization for Reinforcement Learningï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„å•æ¬¡éå†ä¸ç¡®å®šæ€§ä¼°è®¡æŠ€æœ¯ï¼Œä»¥è¯†åˆ«è®­ç»ƒå®ä¾‹ä¸­çš„ä¿¡æ¯æ€§æ•°æ®ã€‚è¿™ç§æ–¹æ³•ä»…éœ€ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œé¿å…äº†è¿­ä»£è®¡ç®—ä¸‹ä¸€ä¸ªæ ‡è®°çš„éœ€è¦ï¼Œä»è€Œå°†æ•°æ®è¯„ä¼°çš„é€Ÿåº¦æé«˜äº†é«˜è¾¾185å€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†å’Œä¸åŒè§„æ¨¡çš„è¯­è¨€æ¨¡å‹æ¶æ„ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®è¯éªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ç²¾å¿ƒé€‰æ‹©ä»…10%çš„æ•°æ®ï¼ŒUFO-RLæ‰€éœ€çš„è®¡ç®—èµ„æºä¸åˆ°å…¨æ•°æ®è®­ç»ƒçš„1/16ï¼Œå´å®ç°äº†ä¸å…¨æ•°æ®åŸºçº¿ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒUFO-RLè¿˜å±•ç¤ºäº†å¢å¼ºçš„è®­ç»ƒç¨³å®šæ€§å’Œæ”¹è¿›çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºå¤§è§„æ¨¡LLMçš„RLå¾®è°ƒæä¾›äº†ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡å…³æ³¨æœ€æœ‰ä»·å€¼å’Œä¿¡æ¯æ€§çš„æ•°æ®ï¼Œå‡è½»äº†ä¼ ç»ŸRLè®­ç»ƒä¸­çš„è®¡ç®—ç“¶é¢ˆã€‚UFO-RLä¸ä»…æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œè¿˜é€šè¿‡èšç„¦äºæ¨¡å‹ä¸ç¡®å®šæ€§è¾ƒé«˜çš„æ•°æ®ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™å¯¹äºæœªæ¥çš„RLç ”ç©¶å’Œåº”ç”¨å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

