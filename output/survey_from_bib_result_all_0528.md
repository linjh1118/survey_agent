# Paper List from BIB File: tmpxv1owcwg.bib
- [25/03] **A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond**  
[[Paper](http://arxiv.org/pdf/2503.21614v1)] [[Code/Page]()] [[TLDR/Notes](#a-survey-of-efficient-reasoning-for-large-reasoning-models--language--multimodality--and-beyond)]

- [25/05] **Model Merging in Pre-training of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2505.12082v3)] [[Code/Page]()] [[TLDR/Notes](#model-merging-in-pre-training-of-large-language-models)]

- [25/04] **Improved Visual-Spatial Reasoning via R1-Zero-Like Training**  
[[Paper](http://arxiv.org/pdf/2504.00883v2)] [[Code/Page]()] [[TLDR/Notes](#improved-visual-spatial-reasoning-via-r1-zero-like-training)]

- [25/05] **Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL**  
[[Paper](http://arxiv.org/pdf/2505.10832v1)] [[Code/Page]()] [[TLDR/Notes](#learning-when-to-think--shaping-adaptive-reasoning-in-r1-style-models-via-multi-stage-rl)]

- [25/04] **Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods**  
[[Paper](http://arxiv.org/pdf/2504.14047v1)] [[Code/Page]()] [[TLDR/Notes](#think-deep--think-fast--investigating-efficiency-of-verifier-free-inference-time-scaling-methods)]

- [25/04] **TTRL: Test-Time Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.16084v2)] [[Code/Page](https://github.com/PRIME-RL/TTRL)] [[TLDR/Notes](#ttrl--test-time-reinforcement-learning)]

- [25/05] **Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner**  
[[Paper](http://arxiv.org/pdf/2505.11404v1)] [[Code/Page](https://github.com/Wenchuan-Zhang/Patho-R1.)] [[TLDR/Notes](#patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner)]

- [25/04] **Efficient Reasoning Models: A Survey**  
[[Paper](http://arxiv.org/pdf/2504.10903v1)] [[Code/Page]()] [[TLDR/Notes](#efficient-reasoning-models--a-survey)]

- [25/05] **Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL**  
[[Paper](http://arxiv.org/pdf/2505.02391v1)] [[Code/Page](https://github.com/RLHFlow/GVM.)] [[TLDR/Notes](#optimizing-chain-of-thought-reasoners-via-gradient-variance-minimization-in-rejection-sampling-and-rl)]

- [25/03] **PharMolixFM: All-Atom Foundation Models for Molecular Modeling and Generation**  
[[Paper](http://arxiv.org/pdf/2503.21788v3)] [[Code/Page](https://github.com/PharMolix/OpenBioMed.)] [[TLDR/Notes](#pharmolixfm--all-atom-foundation-models-for-molecular-modeling-and-generation)]

- [25/05] **R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO**  
[[Paper](http://arxiv.org/pdf/2505.16673v1)] [[Code/Page](https://github.com/HJYao00/R1-ShareVL.)] [[TLDR/Notes](#r1-sharevl--incentivizing-reasoning-capability-of-multimodal-large-language-models-via-share-grpo)]

- [25/05] **RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.15034v1)] [[Code/Page](https://github.com/kaiwenzha/rl-tango.)] [[TLDR/Notes](#rl-tango--reinforcing-generator-and-verifier-together-for-language-reasoning)]

- [25/05] **Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs**  
[[Paper](http://arxiv.org/pdf/2505.12929v1)] [[Code/Page](https://github.com/zhyang2226/AR-Lopti.)] [[TLDR/Notes](#do-not-let-low-probability-tokens-over-dominate-in-rl-for-llms)]

- [25/04] **NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation**  
[[Paper](http://arxiv.org/pdf/2504.13055v3)] [[Code/Page]()] [[TLDR/Notes](#noisyrollout--reinforcing-visual-reasoning-with-data-augmentation)]

- [25/04] **Reinforcement Learning from Human Feedback**  
[[Paper](http://arxiv.org/pdf/2504.12501v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-from-human-feedback)]

- [25/03] **Reasoning Beyond Limits: Advances and Open Problems for LLMs**  
[[Paper](http://arxiv.org/pdf/2503.22732v1)] [[Code/Page]()] [[TLDR/Notes](#reasoning-beyond-limits--advances-and-open-problems-for-llms)]

- [25/03] **Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains**  
[[Paper](http://arxiv.org/pdf/2503.23829v2)] [[Code/Page]()] [[TLDR/Notes](#crossing-the-reward-bridge--expanding-rl-with-verifiable-rewards-across-diverse-domains)]

- [25/04] **Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?**  
[[Paper](http://arxiv.org/pdf/2504.13837v2)] [[Code/Page]()] [[TLDR/Notes](#does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model-)]

- [25/05] **RLVR-World: Training World Models with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.13934v1)] [[Code/Page]()] [[TLDR/Notes](#rlvr-world--training-world-models-with-reinforcement-learning)]

- [25/02] **Atom of Thoughts for Markov LLM Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2502.12018v2)] [[Code/Page](https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.)] [[TLDR/Notes](#atom-of-thoughts-for-markov-llm-test-time-scaling)]

- [25/05] **Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO**  
[[Paper](http://arxiv.org/pdf/2505.17017v1)] [[Code/Page](https://github.com/ZiyuGuo99/Image-Generation-CoT)] [[TLDR/Notes](#delving-into-rl-for-image-generation-with-cot--a-study-on-dpo-vs--grpo)]

- [25/04] **Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization**  
[[Paper](http://arxiv.org/pdf/2504.05812v3)] [[Code/Page](https://github.com/QingyangZhang/EMPO.)] [[TLDR/Notes](#right-question-is-already-half-the-answer--fully-unsupervised-llm-reasoning-incentivization)]

- [25/04] **ToolRL: Reward is All Tool Learning Needs**  
[[Paper](http://arxiv.org/pdf/2504.13958v1)] [[Code/Page]()] [[TLDR/Notes](#toolrl--reward-is-all-tool-learning-needs)]

- [25/04] **SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM**  
[[Paper](http://arxiv.org/pdf/2504.14286v2)] [[Code/Page]()] [[TLDR/Notes](#srpo--a-cross-domain-implementation-of-large-scale-reinforcement-learning-on-llm)]

- [25/05] **Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities**  
[[Paper](http://arxiv.org/pdf/2505.15692v2)] [[Code/Page]()] [[TLDR/Notes](#thought-augmented-policy-optimization--bridging-external-guidance-and-internal-capabilities)]

- [25/04] **Think When You Need: Self-Adaptive Chain-of-Thought Learning**  
[[Paper](http://arxiv.org/pdf/2504.03234v2)] [[Code/Page]()] [[TLDR/Notes](#think-when-you-need--self-adaptive-chain-of-thought-learning)]

- [25/04] **Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models**  
[[Paper](http://arxiv.org/pdf/2504.21277v2)] [[Code/Page]()] [[TLDR/Notes](#reinforced-mllm--a-survey-on-rl-based-reasoning-in-multimodal-large-language-models)]

- [25/05] **EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.04623v1)] [[Code/Page]()] [[TLDR/Notes](#echoink-r1--exploring-audio-visual-reasoning-in-multimodal-llms-via-reinforcement-learning)]

- [25/05] **Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.16854v2)] [[Code/Page](https://github.com/kokolerk/TON.)] [[TLDR/Notes](#think-or-not--selective-reasoning-via-reinforcement-learning-for-vision-language-models)]

- [25/05] **Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.13261v1)] [[Code/Page]()] [[TLDR/Notes](#unlocking-the-potential-of-difficulty-prior-in-rl-based-multimodal-reasoning)]

- [25/05] **Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.11827v2)] [[Code/Page](https://github.com/usail-hkust/LongShort.)] [[TLDR/Notes](#not-all-thoughts-are-generated-equal--efficient-llm-reasoning-via-multi-turn-reinforcement-learning)]

- [25/05] **RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs**  
[[Paper](http://arxiv.org/pdf/2505.13697v2)] [[Code/Page]()] [[TLDR/Notes](#rl-in-name-only--analyzing-the-structural-assumptions-in-rl-post-training-for-llms)]

- [25/05] **CEC-Zero: Chinese Error Correction Solution Based on LLM**  
[[Paper](http://arxiv.org/pdf/2505.09082v1)] [[Code/Page]()] [[TLDR/Notes](#cec-zero--chinese-error-correction-solution-based-on-llm)]

- [25/05] **DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization**  
[[Paper](http://arxiv.org/pdf/2505.12366v1)] [[Code/Page]()] [[TLDR/Notes](#disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization)]

- [25/05] **KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.16826v1)] [[Code/Page]()] [[TLDR/Notes](#ktae--a-model-free-algorithm-to-key-tokens-advantage-estimation-in-mathematical-reasoning)]

- [25/05] **S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.07686v2)] [[Code/Page]()] [[TLDR/Notes](#s-grpo--early-exit-via-reinforcement-learning-in-reasoning-models)]

- [25/05] **MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining**  
[[Paper](http://arxiv.org/pdf/2505.07608v1)] [[Code/Page](https://github.com/xiaomimimo/MiMo.)] [[TLDR/Notes](#mimo--unlocking-the-reasoning-potential-of-language-model----from-pretraining-to-posttraining)]

- [25/05] **Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning**  
[[Paper](http://arxiv.org/pdf/2505.14684v2)] [[Code/Page]()] [[TLDR/Notes](#mind-the-gap--bridging-thought-leap-for-improved-chain-of-thought-tuning)]

- [25/05] **AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners**  
[[Paper](http://arxiv.org/pdf/2505.16322v1)] [[Code/Page]()] [[TLDR/Notes](#adastar--adaptive-data-sampling-for-training-self-taught-reasoners)]

- [25/03] **A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?**  
[[Paper](http://arxiv.org/pdf/2503.24235v3)] [[Code/Page](https://github.com/testtimescaling/testtimescaling.github.io/)] [[TLDR/Notes](#a-survey-on-test-time-scaling-in-large-language-models--what--how--where--and-how-well-)]

- [25/05] **ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay**  
[[Paper](http://arxiv.org/pdf/2505.16282v1)] [[Code/Page](https://github.com/dvlab-research/ARPO.git.)] [[TLDR/Notes](#arpo-end-to-end-policy-optimization-for-gui-agents-with-experience-replay)]

- [25/05] **Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions**  
[[Paper](http://arxiv.org/pdf/2505.11614v1)] [[Code/Page]()] [[TLDR/Notes](#using-reinforcement-learning-to-train-large-language-models-to-explain-human-decisions)]

- [25/05] **UFT: Unifying Supervised and Reinforcement Fine-Tuning**  
[[Paper](http://arxiv.org/pdf/2505.16984v1)] [[Code/Page]()] [[TLDR/Notes](#uft--unifying-supervised-and-reinforcement-fine-tuning)]

- [25/04] **VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.08837v3)] [[Code/Page]()] [[TLDR/Notes](#vl-rethinker--incentivizing-self-reflection-of-vision-language-models-with-reinforcement-learning)]

- [25/05] **A New DAPO Algorithm for Stock Trading**  
[[Paper](http://arxiv.org/pdf/2505.06408v2)] [[Code/Page](https://github.com/Ruijian-Zha/FinRL-DAPO-SR/)] [[TLDR/Notes](#a-new-dapo-algorithm-for-stock-trading)]

- [25/04] **A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility**  
[[Paper](http://arxiv.org/pdf/2504.07086v1)] [[Code/Page]()] [[TLDR/Notes](#a-sober-look-at-progress-in-language-model-reasoning--pitfalls-and-paths-to-reproducibility)]

- [25/05] **Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2505.02686v1)] [[Code/Page](https://github.com/bobxwu/learning-from-rewards-llm-papers.)] [[TLDR/Notes](#sailing-ai-by-the-stars--a-survey-of-learning-from-rewards-in-post-training-and-test-time-scaling-of-large-language-models)]

- [25/04] **Heimdall: test-time scaling on the generative verification**  
[[Paper](http://arxiv.org/pdf/2504.10337v2)] [[Code/Page]()] [[TLDR/Notes](#heimdall--test-time-scaling-on-the-generative-verification)]

- [25/05] **Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning**  
[[Paper](http://arxiv.org/pdf/2505.16761v1)] [[Code/Page](https://hitcslj.github.io/mesh-rft/}{this)] [[TLDR/Notes](#mesh-rft--enhancing-mesh-generation-via-fine-grained-reinforcement-fine-tuning)]

- [25/05] **ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving**  
[[Paper](http://arxiv.org/pdf/2505.12717v1)] [[Code/Page]()] [[TLDR/Notes](#totrl--unlock-llm-tree-of-thoughts-reasoning-potential-through-puzzles-solving)]

- [25/05] **Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.14810v2)] [[Code/Page](https://github.com/TingchenFu/MathIF.)] [[TLDR/Notes](#scaling-reasoning--losing-control--evaluating-instruction-following-in-large-reasoning-models)]

- [25/05] **SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward**  
[[Paper](http://arxiv.org/pdf/2505.17018v1)] [[Code/Page](https://github.com/kxfan2002/SophiaVL-R1.)] [[TLDR/Notes](#sophiavl-r1--reinforcing-mllms-reasoning-with-thinking-reward)]

- [25/04] **Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time**  
[[Paper](http://arxiv.org/pdf/2504.12329v1)] [[Code/Page]()] [[TLDR/Notes](#speculative-thinking--enhancing-small-model-reasoning-with-large-model-guidance-at-inference-time)]

- [25/05] **OViP: Online Vision-Language Preference Learning**  
[[Paper](http://arxiv.org/pdf/2505.15963v1)] [[Code/Page]()] [[TLDR/Notes](#ovip--online-vision-language-preference-learning)]

- [25/05] **AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16400v1)] [[Code/Page]()] [[TLDR/Notes](#acereason-nemotron--advancing-math-and-code-reasoning-through-reinforcement-learning)]

- [25/05] **Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.12432v1)] [[Code/Page](https://github.com/zrguo/Observe-R1.)] [[TLDR/Notes](#observe-r1--unlocking-reasoning-abilities-of-mllms-with-dynamic-progressive-reinforcement-learning)]

- [25/05] **Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling**  
[[Paper](http://arxiv.org/pdf/2505.11792v1)] [[Code/Page]()] [[TLDR/Notes](#solver-informed-rl--grounding-large-language-models-for-authentic-optimization-modeling)]

- [25/04] **Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.13818v1)] [[Code/Page]()] [[TLDR/Notes](#not-all-rollouts-are-useful--down-sampling-rollouts-in-llm-reinforcement-learning)]

- [25/05] **Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.15311v1)] [[Code/Page]()] [[TLDR/Notes](#trajectory-bellman-residual-minimization--a-simple-value-based-method-for-llm-reasoning)]

- [25/05] **T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT**  
[[Paper](http://arxiv.org/pdf/2505.00703v1)] [[Code/Page](https://github.com/CaraJ7/T2I-R1)] [[TLDR/Notes](#t2i-r1--reinforcing-image-generation-with-collaborative-semantic-level-and-token-level-cot)]

- [25/04] **Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining**  
[[Paper](http://arxiv.org/pdf/2504.07912v1)] [[Code/Page]()] [[TLDR/Notes](#echo-chamber--rl-post-training-amplifies-behaviors-learned-in-pretraining)]

- [25/05] **SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.12346v1)] [[Code/Page]()] [[TLDR/Notes](#seed-grpo--semantic-entropy-enhanced-grpo-for-uncertainty-aware-policy-optimization)]

- [25/05] **lmgame-Bench: How Good are LLMs at Playing Games?**  
[[Paper](http://arxiv.org/pdf/2505.15146v1)] [[Code/Page](https://github.com/lmgame-org/GamingAgent/lmgame-bench.)] [[TLDR/Notes](#lmgame-bench--how-good-are-llms-at-playing-games-)]

- [25/05] **RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation**  
[[Paper](http://arxiv.org/pdf/2505.03344v1)] [[Code/Page]()] [[TLDR/Notes](#rift--closed-loop-rl-fine-tuning-for-realistic-and-controllable-traffic-simulation)]

- [25/03] **Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models**  
[[Paper](http://arxiv.org/pdf/2503.09567v3)] [[Code/Page]()] [[TLDR/Notes](#towards-reasoning-era--a-survey-of-long-chain-of-thought-for-reasoning-large-language-models)]

- [25/04] **TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.09641v1)] [[Code/Page](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.)] [[TLDR/Notes](#tinyllava-video-r1--towards-smaller-lmms-for-video-reasoning)]

- [25/05] **O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering**  
[[Paper](http://arxiv.org/pdf/2505.16582v2)] [[Code/Page]()] [[TLDR/Notes](#o$^2$-searcher--a-searching-based-agent-model-for-open-domain-open-ended-question-answering)]

- [25/05] **Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO**  
[[Paper](http://arxiv.org/pdf/2505.11595v1)] [[Code/Page]()] [[TLDR/Notes](#spectral-policy-optimization--coloring-your-incorrect-reasoning-in-grpo)]

- [25/05] **Group-in-Group Policy Optimization for LLM Agent Training**  
[[Paper](http://arxiv.org/pdf/2505.10978v1)] [[Code/Page]()] [[TLDR/Notes](#group-in-group-policy-optimization-for-llm-agent-training)]

- [25/05] **Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens**  
[[Paper](http://arxiv.org/pdf/2505.13775v2)] [[Code/Page]()] [[TLDR/Notes](#beyond-semantics--the-unreasonable-effectiveness-of-reasonless-intermediate-tokens)]

- [25/04] **Deep Reasoning Translation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2504.10187v1)] [[Code/Page]()] [[TLDR/Notes](#deep-reasoning-translation-via-reinforcement-learning)]

- [25/05] **DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management**  
[[Paper](http://arxiv.org/pdf/2505.12951v1)] [[Code/Page]()] [[TLDR/Notes](#dgro--enhancing-llm-reasoning-via-exploration-exploitation-control-and-reward-variance-management)]

- [25/05] **J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.13346v2)] [[Code/Page]()] [[TLDR/Notes](#j4r--learning-to-judge-with-equivalent-initial-state-group-relative-policy-optimization)]

- [25/05] **An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents**  
[[Paper](http://arxiv.org/pdf/2505.15117v1)] [[Code/Page](https://github.com/PeterGriffinJin/Search-R1.)] [[TLDR/Notes](#an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents)]

- [25/04] **GVPO: Group Variance Policy Optimization for Large Language Model Post-Training**  
[[Paper](http://arxiv.org/pdf/2504.19599v2)] [[Code/Page]()] [[TLDR/Notes](#gvpo--group-variance-policy-optimization-for-large-language-model-post-training)]

- [25/05] **Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16410v1)] [[Code/Page](https://github.com/dongguanting/Tool-Star.)] [[TLDR/Notes](#tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning)]

- [25/05] **Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.16142v1)] [[Code/Page]()] [[TLDR/Notes](#distilling-the-implicit-multi-branch-structure-in-llms--reasoning-via-reinforcement-learning)]

- [25/04] **StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation**  
[[Paper](http://arxiv.org/pdf/2504.15930v1)] [[Code/Page]()] [[TLDR/Notes](#streamrl--scalable--heterogeneous--and-elastic-rl-for-llms-with-disaggregated-stream-generation)]

- [25/05] **UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection**  
[[Paper](http://arxiv.org/pdf/2505.12457v1)] [[Code/Page]()] [[TLDR/Notes](#ufo-rl--uncertainty-focused-optimization-for-efficient-reinforcement-learning-data-selection)]



# TLDR/Notes
## a-survey-of-efficient-reasoning-for-large-reasoning-models--language--multimodality--and-beyond
### Abstract
Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have
demonstrated strong performance gains by scaling up the length of
Chain-of-Thought (CoT) reasoning during inference. However, a growing concern
lies in their tendency to produce excessively long reasoning traces, which are
often filled with redundant content (e.g., repeated definitions), over-analysis
of simple problems, and superficial exploration of multiple reasoning paths for
harder tasks. This inefficiency introduces significant challenges for training,
inference, and real-world deployment (e.g., in agent-based systems), where
token economy is critical. In this survey, we provide a comprehensive overview
of recent efforts aimed at improving reasoning efficiency in LRMs, with a
particular focus on the unique challenges that arise in this new paradigm. We
identify common patterns of inefficiency, examine methods proposed across the
LRM lifecycle, i.e., from pretraining to inference, and discuss promising
future directions for research. To support ongoing development, we also
maintain a real-time GitHub repository tracking recent progress in the field.
We hope this survey serves as a foundation for further exploration and inspires
innovation in this rapidly evolving area.
### 🌟 论文解读 | 探索大型推理模型的高效推理：语言、多模态与未来

### 📌 背景痛点/本文动机
近年来，大型推理模型（LRMs）如DeepSeek-R1和OpenAI o1在推理过程中通过增加链式思维（CoT）的长度，展示了显著的性能提升。然而，这些模型在生成推理轨迹时往往过于冗长，包含重复内容、对简单问题过度分析，以及对于更难任务表面化的探索。这种低效性给训练、推理以及实际部署带来了重大挑战，特别是在需要注重token经济的系统中。本文旨在提供对提高LRMs推理效率的最新努力的全面概述，并探讨这一新范式下出现的独特挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首先定义了推理效率，并识别了LRMs中常见的推理低效模式，如重复定义、对简单问题过度分析和推理路径的浅层探索。

💡 创新点2
本文系统地回顾了从预训练到监督微调、强化学习再到推理阶段，旨在提高推理效率的各种方法，并提出了一个关于高效推理方法的分类框架。

💡 创新点3
作者还维护了一个实时GitHub仓库，跟踪该领域的最新进展，为研究人员提供了一个宝贵的信息来源。

### 📈 实验结果
本文没有具体展示实验结果，而是提供了一个全面的文献综述，涵盖了提高LRMs推理效率的各种方法和技术。

### 💬 可借鉴之处
本文对于理解和提高大型推理模型的推理效率提供了宝贵的洞见。以下是几个可借鉴之处：
- 定义了推理效率的概念，并提出了衡量效率的方法。
- 识别了LRMs中常见的低效推理模式，为改进推理过程提供了具体的目标。
- 提供了一个全面的综述，涵盖了从模型预训练到推理的各个阶段，有助于研究人员快速了解该领域的最新进展。
- 通过GitHub仓库实时跟踪领域进展，为社区提供了一个互动和更新的平台。

## model-merging-in-pre-training-of-large-language-models
### Abstract
Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.
### 🌟 论文解读 | 探索大规模语言模型预训练中的模型合并技术

### 📌 背景痛点/本文动机
随着现代大规模语言模型（LLM）在各种任务中展现出卓越的能力，它们在预训练过程中面临着一系列挑战，包括高昂的预训练成本、领域特定后训练效果折扣、性能扩展预测不精确以及大规模训练的不稳定性。模型合并作为一种新兴技术，有潜力缓解这些挑战，但目前在大规模预训练中的应用还相对较少。本文旨在探究模型合并技术在大型语言模型预训练中的应用，并提出了一个新的合并策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了预训练模型平均（PMA）策略，这是一种在大型语言模型预训练过程中进行模型级权重合并的新框架。通过在稳定训练阶段合并检查点，PMA能够产生一致且显著的性能提升。

💡 创新点2
作者详细研究了各种模型合并技术及其相关超参数，提供了实用的预训练指南，并揭示了模型合并的新应用，例如用于权重初始化的PMA-init，它有助于稳定训练过程，尤其是在训练动态出现严重不可恢复的损失峰值时。

### 📈 实验结果
本文在从数百万到超过1000亿参数的不同大小和架构的密集模型和混合专家（MoE）架构上进行了广泛实验。结果表明，在稳定训练阶段应用PMA不仅能够提高性能，还能准确预测退火行为，从而加快验证周期并显著降低训练成本。此外，PMA-init在连续继续训练（CT）和监督微调（SFT）阶段的应用也显示出稳定训练动态的效果。

### 💬 可借鉴之处
本文的研究为大规模语言模型的预训练提供了以下可借鉴之处：
- 模型合并技术可以在预训练阶段有效提高模型性能，减少训练成本。
- PMA策略在稳定训练阶段的应用能够预测退火后的模型性能，有助于快速验证和成本节约。
- PMA-init方法为训练不稳定时提供了可靠的恢复手段，有助于稳定训练过程。
- 对模型合并技术和超参数的详细消融研究为社区提供了实用的预训练指南。

## improved-visual-spatial-reasoning-via-r1-zero-like-training
### Abstract
Increasing attention has been placed on improving the reasoning capacities of
multi-modal large language models (MLLMs). As the cornerstone for AI agents
that function in the physical realm, video-based visual-spatial intelligence
(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This
work conducts a first, in-depth study on improving the visual-spatial reasoning
of MLLMs via R1-Zero-like training. Technically, we first identify that the
visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models
cannot be activated via Chain of Thought (CoT) prompts. We then incorporate
GRPO training for improved visual-spatial reasoning, using the carefully
curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,
we identify the necessity to keep the KL penalty (even with a small value) in
GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from
Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.
Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves
performance comparable to that of the best open-source model
LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning
and direct preference optimization baselines and observe strong performance
superiority. The code and dataset will be available soon.
### 🌟 论文解读 | 通过R1-Zero-like训练提升多模态大语言模型的视觉空间推理能力

### 📌 背景痛点/本文动机
随着多模态大语言模型（MLLMs）在人工智能领域的兴起，这些模型在处理文本、图像和视频输入时展现出强大的能力。然而，这些模型在物理世界中的视觉空间推理能力仍然不足，尤其是在视频基础上的视觉空间智能（VSI）。本文旨在通过R1-Zero-like训练方法，深入研究如何提升MLLMs的视觉空间推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首先发现，对于小型到中型Qwen2-VL模型，简单的推理导向提示（CoT）无法激活其视觉空间推理能力。通过对比不同的提示策略，发现非CoT提示在小到中型Qwen2-VL模型上表现最佳。

💡 创新点2
为了提升视觉空间推理能力，本文采用了基于R1-Zero-like的GRPO（Generalized Reward Propagation Optimization）训练方法，并构建了一个包含超过10万样本的视频问答数据集VSI-100k。该数据集基于ScanNet获取的高保真视频扫描和详细的对象级3D注释，从而可以轻松构建与空间信息相关的（问题，答案）对。

### 📈 实验结果
通过120 GPU小时的训练，本文的vsGRPO-2B模型从Qwen2-VL-2B模型微调而来，性能提升了12.1%，超过了GPT-4o。同时，vsGRPO-7B模型从Qwen2-VL-7B模型微调而来，性能与最佳的开放源代码模型LLaVA-NeXT-Video-72B相当。此外，本文还比较了GRPO与监督微调和直接偏好优化基线，发现GRPO在提升Qwen2-VL模型的视觉空间推理能力方面具有显著优势。

### 💬 可借鉴之处
本文的研究表明，通过精心设计的训练方法和数据集，可以显著提升MLLMs的视觉空间推理能力。此外，本文还发现了在GRPO训练中保持KL惩罚（即使值很小）的必要性，并观察到了奖励黑客现象。这些发现为未来的研究和模型训练提供了宝贵的参考。

## learning-when-to-think--shaping-adaptive-reasoning-in-r1-style-models-via-multi-stage-rl
### Abstract
Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis ("...") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs.
### 🌟 论文解读 | 学习何时思考：通过多阶段强化学习塑造R1风格模型的自适应推理

### 📌 背景痛点/本文动机
大型推理模型（LRMs）擅长在给出最终答案之前生成详细的、逐步推理序列。然而，对于简单问题，这种详细的推理可能会带来巨大的计算开销和延迟。为了解决这种过度思考的问题，本文探讨了如何让LRMs具备自适应思考能力，即根据问题的复杂度动态决定是否进行显式推理。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文发现，在R1风格模型的提示中插入一个简单的省略号（"..."）可以随机触发思考模式或非思考模式，揭示了推理行为潜在的可控性。

💡 创新点2
基于这一发现，本文提出了AutoThink，一个多阶段强化学习框架，通过阶段性的奖励塑造逐步优化推理策略。AutoThink只在必要时调用显式推理，而对于简单任务则默认给出简洁的响应。

### 📈 实验结果
在五个主流数学基准测试中，AutoThink与最近的提示和基于RL的修剪方法相比，实现了更有利的准确性与效率权衡。特别地，在DeepSeek-R1-Distill-Qwen-1.5B模型上，AutoThink将相对准确性提高了6.4%，同时减少了52%的token使用。

### 💬 可借鉴之处
本文提出的方法为大型推理模型提供了一种可扩展且自适应的推理范式，不仅能够无缝集成到任何R1风格模型中，还显著提高了模型的效率，同时保持了性能。此外，通过实验验证了省略号提示的有效性，以及多阶段强化学习框架在优化推理行为方面的潜力。

## think-deep--think-fast--investigating-efficiency-of-verifier-free-inference-time-scaling-methods
### Abstract
There is intense interest in investigating how inference time compute (ITC)
(e.g. repeated sampling, refinements, etc) can improve large language model
(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,
such as Deepseek-R1, unlock the opportunity for reinforcement learning to
improve LLM reasoning skills. An in-depth understanding of how ITC interacts
with reasoning across different models could provide important guidance on how
to further advance the LLM frontier. This work conducts a comprehensive
analysis of inference-time scaling methods for both reasoning and non-reasoning
models on challenging reasoning tasks. Specifically, we focus our research on
verifier-free inference time-scaling methods due to its generalizability
without needing a reward model. We construct the Pareto frontier of quality and
efficiency. We find that non-reasoning models, even with an extremely high
inference budget, still fall substantially behind reasoning models. For
reasoning models, majority voting proves to be a robust inference strategy,
generally competitive or outperforming other more sophisticated ITC methods
like best-of-N and sequential revisions, while the additional inference compute
offers minimal improvements. We further perform in-depth analyses of the
association of key response features (length and linguistic markers) with
response quality, with which we can improve the existing ITC methods. We find
that correct responses from reasoning models are typically shorter and have
fewer hedging and thinking markers (but more discourse markers) than the
incorrect responses.
### 🌟 论文解读 | 探索无验证器推理时扩展方法的高效性：深入思考，快速行动

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在各个任务中表现出越来越强的能力，复杂的推理任务仍然是一个挑战。为了提升LLM的推理能力，研究者们开始探索在推理时增加计算资源（推理时计算，ITC）的方法。同时，一些新的推理模型如Deepseek-R1通过强化学习进一步提高了LLM的推理技能。本文旨在深入分析ITC方法如何与不同类型的模型（推理模型和非推理模型）的推理能力相互作用，以指导LLM领域的发展。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文专注于无验证器推理时扩展方法，这些方法不需要奖励模型，因此具有更广泛的适用性。作者对推理模型和非推理模型在具有挑战性的推理任务上进行了全面的ITC方法分析。

💡 创新点2
作者构建了质量与效率的帕累托前沿，发现对于推理模型，多数投票策略通常比其他更复杂的方法（如最佳N法和顺序修订）更具竞争力或更优，而额外的推理计算带来的改进微乎其微。此外，作者还分析了响应语言特征（长度和语言标记）与任务性能之间的相关性，为改进现有ITC方法提供了实际指导。

### 📈 实验结果
实验结果表明，即使非推理模型拥有极高的推理时计算预算，它们的表现仍然远远落后于推理模型。对于推理模型，多数投票证明是一种稳健的推理策略，通常比其他更复杂的ITC方法更具竞争力或更优。此外，正确的响应通常比错误的响应更短，且具有更少的犹豫和思考标记，但更多的论述标记。

### 💬 可借鉴之处
本文的研究为平衡推理质量和计算效率提供了实用的指导，表明无需增加计算成本即可通过分析响应的语言特征来改进现有的ITC方法。此外，研究强调了专门为推理设计的模型在处理复杂推理任务时的内在价值。

## ttrl--test-time-reinforcement-learning
### Abstract
This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the maj@n metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model maj@n,
and approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
### 🌟 论文解读 | 探索无标签数据上的强化学习：TTRL方法解析

### 📌 背景痛点/本文动机
随着大型推理模型（LRMs）的进步，强化学习（RL）在增强长链推理能力方面显示出了其重要性。然而，现有的强化学习方法大多依赖于标注数据，这在实际应用中限制了其可扩展性。本文旨在解决这一痛点，提出了一种在无标签数据上应用强化学习的方法，名为Test-Time Reinforcement Learning（TTRL），以推动AI系统在无监督环境下的自我进化。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
TTRL方法通过在测试时对预训练模型进行强化学习，使其能够适应未见过的数据。它通过重复采样策略在展开阶段准确估计标签，并计算基于规则的奖励，从而实现在无标签数据上的强化学习。

💡 创新点2
本文引入了多数投票奖励函数，该函数在没有地面真实标签的情况下，为强化学习提供了有效的奖励估计。这种方法使得模型能够在没有明确监督的情况下进行高效且稳定的强化学习。

### 📈 实验结果
实验表明，将TTRL应用于Qwen2.5-Math-7B模型后，在AIME 2024任务上的通过率提升了211%（从12.9提升到40.2），并且在AIME 2024、AMC、MATH-500和GPQA任务上的平均提升达到了76%。这些改进是在没有任何标注训练数据的情况下通过自我进化实现的。

### 💬 可借鉴之处
TTRL方法不仅提高了通过率，还改善了测试时间缩放（TTS）的效果。此外，TTRL在不同规模和类型的模型上均表现出有效性，并且可以与现有的强化学习算法集成。这表明TTRL有潜力显著减少对人类标注的依赖，推动连续学习和大规模无监督训练的扩展。

## patho-r1--a-multimodal-reinforcement-learning-based-pathology-expert-reasoner
### Abstract
Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose PathoCLIP, trained on the same figure-caption corpus used for continued
pretraining. Comprehensive experimental results demonstrate that both PathoCLIP
and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.
### 🌟 论文解读 | Patho-R1：多模态强化学习助力病理学诊断

### 📌 背景痛点/本文动机
随着视觉语言模型（VLMs）在医学领域的广泛应用，病理学这一子领域仍然面临着巨大的挑战。现有的病理学特定VLMs在诊断准确性和推理可信度方面存在局限，这主要归因于当前病理学数据集的性质，这些数据集主要由缺乏深度和结构化诊断范式的图像描述对组成。为了解决这一问题，本文利用病理学教材和真实世界病理学专家的知识，构建了高质量、推理导向的数据集。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种全面的数据筛选管道，该管道在最小化人工努力的同时，确保了高质量推理导向的监督微调（SFT）数据的可扩展生成。

💡 创新点2
本文介绍了PathoCLIP，一个开源的病理学适应CLIP模型，该模型在分类和检索任务上的表现超过了现有最佳模型。

💡 创新点3
本文探索了预训练视觉语言模型在领域适应中的端到端训练过程，特别是最新的强化学习方法：Group Relative Policy Optimization（GRPO）和Decoupled Clip and Dynamic sAmpling Policy Optimization（DAPO）。

💡 创新点4
本文发布了视觉语言病理学推理模型Patho-R1的模型权重，该模型在各种基准测试中展示了卓越的性能。

### 📈 实验结果
PathoCLIP和Patho-R1在一系列病理学相关任务中均取得了稳健的性能，包括零样本分类、跨模态检索、视觉问答和多项选择题。实验结果表明，这两种模型在诊断准确性和推理能力方面均优于现有模型。

### 💬 可借鉴之处
本文的方法为病理学领域的数据集构建和模型训练提供了新的视角，特别是以下方面值得借鉴：
- 利用病理学教材和专家知识构建高质量数据集的方法。
- 结合强化学习进行模型推理质量优化的策略。
- PathoCLIP和Patho-R1模型的实现和开源，为后续研究提供了基础。

## efficient-reasoning-models--a-survey
### Abstract
Reasoning models have demonstrated remarkable progress in solving complex and
logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to
arriving at a final answer. Yet, the emergence of this "slow-thinking"
paradigm, with numerous tokens generated in sequence, inevitably introduces
substantial computational overhead. To this end, it highlights an urgent need
for effective acceleration. This survey aims to provide a comprehensive
overview of recent advances in efficient reasoning. It categorizes existing
works into three key directions: (1) shorter - compressing lengthy CoTs into
concise yet effective reasoning chains; (2) smaller - developing compact
language models with strong reasoning capabilities through techniques such as
knowledge distillation, other model compression techniques, and reinforcement
learning; and (3) faster - designing efficient decoding strategies to
accelerate inference. A curated collection of papers discussed in this survey
is available in our GitHub repository.
### 🌟 论文解读 | 探索高效推理模型：全面综述

### 📌 背景痛点/本文动机
近年来，推理模型在解决复杂逻辑密集型任务方面取得了显著进展，通过生成扩展的链式思维（Chain-of-Thoughts，CoTs）来达到最终答案。然而，这种“慢思考”范式在生成大量连续的标记时，不可避免地引入了巨大的计算开销。为了解决这一问题，本文旨在提供一份关于高效推理模型的全面综述，探讨如何通过不同方法提高推理效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：压缩长CoTs
本文将现有工作分为三类主要方向之一是“更短”，即通过多种方法（如强化学习、监督微调、提示驱动策略和潜在推理）压缩冗长的CoTs，生成简洁而有效的推理链。

💡 创新点2：构建小型强推理能力语言模型
第二个方向是“更小”，即通过知识蒸馏、模型压缩技术和强化学习等方法，开发具有强大推理能力的小型语言模型。

💡 创新点3：提高解码效率
第三个方向是“更快”，即通过设计更高效的解码策略（如测试时间缩放策略、并行解码和问题分解等）来加速推理过程。

### 📈 实验结果
本文综述了各种高效推理方法，并提供了相应的实验结果。这些方法在压缩CoTs长度、减小模型大小和提高解码效率方面都取得了显著进展，为推理模型的实际应用提供了有效的优化策略。

### 💬 可借鉴之处
本文的综述为研究人员提供了以下可借鉴之处：
- 通过不同方法压缩CoTs长度，以减少计算开销；
- 开发小型语言模型，以降低资源需求；
- 设计高效的解码策略，以提高推理速度；
- 综合考虑模型大小、推理链长度和解码效率，以实现全面的推理性能优化。

## optimizing-chain-of-thought-reasoners-via-gradient-variance-minimization-in-rejection-sampling-and-rl
### Abstract
Chain-of-thought (CoT) reasoning in large language models (LLMs) can be
formalized as a latent variable problem, where the model needs to generate
intermediate reasoning steps. While prior approaches such as iterative
reward-ranked fine-tuning (RAFT) have relied on such formulations, they
typically apply uniform inference budgets across prompts, which fails to
account for variability in difficulty and convergence behavior. This work
identifies the main bottleneck in CoT training as inefficient stochastic
gradient estimation due to static sampling strategies. We propose GVM-RAFT, a
prompt-specific Dynamic Sample Allocation Strategy designed to minimize
stochastic gradient variance under a computational budget constraint. The
method dynamically allocates computational resources by monitoring prompt
acceptance rates and stochastic gradient norms, ensuring that the resulting
gradient variance is minimized. Our theoretical analysis shows that the
proposed dynamic sampling strategy leads to accelerated convergence guarantees
under suitable conditions. Experiments on mathematical reasoning show that
GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over
vanilla RAFT. The proposed dynamic sampling strategy is general and can be
incorporated into other reinforcement learning algorithms, such as GRPO,
leading to similar improvements in convergence and test accuracy. Our code is
available at https://github.com/RLHFlow/GVM.
### 🌟 论文解读 | 优化链式思维推理：通过梯度方差最小化提升采样效率

### 📌 背景痛点/本文动机
在大型语言模型（LLM）中进行链式思维（CoT）推理时，模型需要生成中间推理步骤。虽然之前的迭代奖励排序微调（RAFT）等方法依赖这种推理框架，但它们通常采用统一的推理预算，未能考虑到不同提示的难度和收敛行为的差异性。本文指出，CoT训练中的主要瓶颈在于由于静态采样策略导致的不高效随机梯度估计。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种新的动态样本分配策略GVM-RAFT，旨在在计算预算约束下最小化随机梯度方差。该方法通过监控提示接受率和随机梯度范数，动态分配计算资源，确保梯度方差最小化。

💡 创新点2
理论分析表明，所提出的动态采样策略在合适的条件下可以加速收敛。在数学推理的实验中，GVM-RAFT实现了2-4倍的收敛速度提升，并且显著提高了最终测试的准确性。此外，提出的动态采样策略是通用的，可以整合到其他强化学习算法中，如GRPO，带来类似的收敛速度和测试准确性的提升。

### 📈 实验结果
本文在数学推理任务上进行了实验，结果表明GVM-RAFT方法相较于传统的RAFT方法，不仅收敛速度更快，而且测试准确性也有显著提升。具体来说，GVM-RAFT实现了2-4倍的收敛速度提升，并且提高了最终测试的准确性。

### 💬 可借鉴之处
本文的方法为优化CoT推理提供了一种新的视角，即通过动态调整采样策略来提高梯度估计的效率。这种方法不仅适用于RAFT算法，还可以推广到其他强化学习算法中，为相关领域的研究提供了新的思路和工具。此外，论文中提出的理论分析和实验验证都为该方法的有效性提供了支持。

## pharmolixfm--all-atom-foundation-models-for-molecular-modeling-and-generation
### Abstract
Structural biology relies on accurate three-dimensional biomolecular
structures to advance our understanding of biological functions, disease
mechanisms, and therapeutics. While recent advances in deep learning have
enabled the development of all-atom foundation models for molecular modeling
and generation, existing approaches face challenges in generalization due to
the multi-modal nature of atomic data and the lack of comprehensive analysis of
training and sampling strategies. To address these limitations, we propose
PharMolixFM, a unified framework for constructing all-atom foundation models
based on multi-modal generative techniques. Our framework includes three
variants using state-of-the-art multi-modal generative models. By formulating
molecular tasks as a generalized denoising process with task-specific priors,
PharMolixFM achieves robust performance across various structural biology
applications. Experimental results demonstrate that PharMolixFM-Diff achieves
competitive prediction accuracy in protein-small-molecule docking (83.9% vs.
90.2% RMSD < 2{\AA}, given pocket) with significantly improved inference speed.
Moreover, we explore the empirical inference scaling law by introducing more
sampling repeats or steps. Our code and model are available at
https://github.com/PharMolix/OpenBioMed.
### 🌟 论文解读 | "PharMolixFM：分子建模与生成的全原子基础模型新框架"

### 📌 背景痛点/本文动机
结构生物学依赖于精确的三维生物分子结构，以推动我们对生物功能、疾病机制和疗法的理解。尽管深度学习的最新进展已经使得全原子基础模型在分子建模和生成方面取得了发展，但现有方法在泛化方面仍面临挑战。这些挑战主要源于原子数据的多元模态特性以及训练和采样策略的全面分析不足。为了解决这些限制，本文提出了PharMolixFM，一个基于多元模态生成技术的统一框架，用于构建全原子基础模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
PharMolixFM框架采用了三种最新的多元模态生成模型变体，包括多元模态扩散（PharMolixFM-Diffusion）、多元模态流匹配（PharMolixFM-Flow）和贝叶斯流网络（PharMolixFM-BFN），以共同捕捉不同类型生物分子中的原子类型和原子坐标。

💡 创新点2
本文将不同的分子任务公式化为一个带有任务特定先验的广义去噪过程，这使得PharMolixFM在多种结构生物学应用中表现出稳健的性能。通过借鉴PocketXMol的方法，PharMolixFM将不同的下游任务统一在同一个生成去噪过程中，但使用不同的先验。

### 📈 实验结果
在蛋白质-小分子对接任务中，PharMolixFM-Diffusion实现了与AlphaFold3相当的预测精度（83.9% vs. 90.2% RMSD < 2Å，给定口袋），并且具有显著提高的推理速度（单个A800 GPU上大约4.6秒 vs. 约249.0秒）。在基于结构的药物设计任务中，所有PharMolixFM模型生成的分子的药物性均显示出一致的改善。此外，本文还探索了PharMolixFM的实证推理规模法则，并对不同训练任务的影哪些进行了深入分析。

### 💬 可借鉴之处
PharMolixFM框架为分子建模和生成提供了一个新的视角，特别是其多元模态生成模型的整合和任务特定的去噪过程。此外，本文对训练和采样策略的深入分析为未来相关研究提供了宝贵的参考。代码和模型已经在GitHub上公开，可供社区进一步研究和使用。

## r1-sharevl--incentivizing-reasoning-capability-of-multimodal-large-language-models-via-share-grpo
### Abstract
In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.
### 🌟 论文解读 | “解锁多模态大语言模型的推理能力：Share-GRPO方法综述”

### 📌 背景痛点/本文动机
随着强化学习在大型语言模型中的应用，如Kimi-K1.5和DeepSeek-R1，模型在处理复杂任务如数学和科学推理方面的能力得到了显著提升。然而，现有的强化学习方法在多模态大型语言模型（MLLMs）中存在稀疏奖励和优势消失的问题，导致模型推理能力的提升受限。本文旨在解决这一问题，提出了一种名为Share-GRPO的强化学习方法，以激励MLLMs的推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
Share-GRPO通过数据转换技术扩展问题空间，鼓励MLLMs在扩展的问题空间中有效地探索多样化的推理轨迹，并在强化学习过程中共享这些发现的推理轨迹。这种方法使得每个扩展的问题变体都能从其他问题变体中生成的推理轨迹中受益，共同探索和学习共享的解决方案空间。

💡 创新点2
Share-GRPO在优势计算过程中共享奖励信息，采用分层优势估计方法。该方法在本地级别和全局级别估计优势，本地级别捕获每个问题变体的内部结构和变化，而全局级别利用不同变体之间的多样性和互补性，从而稳定奖励信号，提高相对优势估计的准确性。

### 📈 实验结果
本文在六个广泛使用的推理基准上进行了大量实验，结果显示Share-GRPO方法在数学和一般推理任务上均优于基线和其他最先进的强化学习推理MLLMs。实验证明了Share-GRPO在缓解稀疏奖励和优势消失问题方面的有效性。

### 💬 可借鉴之处
本文提出的Share-GRPO方法为多模态大型语言模型在推理能力提升方面提供了新的视角，特别是通过扩展问题空间和共享推理轨迹来增强模型的探索和学习能力。此外，分层优势估计方法为稳定强化学习过程提供了新的思路。这些方法和技术对于未来大型语言模型的研究和应用具有很高的参考价值。

## rl-tango--reinforcing-generator-and-verifier-together-for-language-reasoning
### Abstract
Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.
### 🌟 论文解读 | “RL Tango：协同强化学习提升大型语言模型推理能力”

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在自然语言处理（NLP）任务中的表现日益出色，它们在处理需要多步骤思考和计划的复杂推理任务时仍然面临挑战。为了增强这些模型的推理能力，通常采用监督微调（SFT）或强化学习（RL）进行后训练。然而，现有的RL后训练方法通常使用固定的验证器（基于规则或预训练的）或通过监督微调训练的判别式验证器，这些设计容易受到奖励黑客攻击，且在训练分布之外泛化能力差。本文旨在克服这些限制，提出了一种新的框架TANGO，通过RL同时协同训练LLM生成器和验证器。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
TANGO框架的核心是引入了一个生成性的、过程级的LLM验证器，该验证器通过RL进行训练，并与生成器共同进化。与现有方法不同，TANGO的验证器仅基于结果级的验证正确性奖励进行训练，无需显式的过程级注释。

💡 创新点2
TANGO通过RL实现了生成器和验证器的交错训练，使得两者能够相互强化，提高了训练效率和最终性能。生成器通过结合结果级正确性信号和验证器提供的详细步骤级奖励，指导生成器向更稳健的推理策略发展。

### 📈 实验结果
本文进行了广泛的实验来评估TANGO的有效性，结果显示，TANGO在7B/8B规模模型中取得了最先进的结果。生成器在五个竞赛级别的数学基准测试和四个具有挑战性的域外推理任务中取得了最佳性能，而验证器在ProcessBench数据集上领先。特别值得注意的是，TANGO在最具挑战性的数学推理问题上展现出了显著的改进。

### 💬 可借鉴之处
TANGO框架提供了一种更有效的生成器和验证器协同进化系统设计，通过RL训练验证器，增强了其推理能力和泛化能力。此外，TANGO的生成性和抽样性验证器增加了奖励信号的随机性，提高了其稳健性。这些设计理念对于提升大型语言模型的推理能力具有很高的参考价值。

## do-not-let-low-probability-tokens-over-dominate-in-rl-for-llms
### Abstract
Reinforcement learning (RL) has become a cornerstone for enhancing the
reasoning capabilities of large language models (LLMs), with recent innovations
such as Group Relative Policy Optimization (GRPO) demonstrating exceptional
effectiveness. In this study, we identify a critical yet underexplored issue in
RL training: low-probability tokens disproportionately influence model updates
due to their large gradient magnitudes. This dominance hinders the effective
learning of high-probability tokens, whose gradients are essential for LLMs'
performance but are substantially suppressed. To mitigate this interference, we
propose two novel methods: Advantage Reweighting and Low-Probability Token
Isolation (Lopti), both of which effectively attenuate gradients from
low-probability tokens while emphasizing parameter updates driven by
high-probability tokens. Our approaches promote balanced updates across tokens
with varying probabilities, thereby enhancing the efficiency of RL training.
Experimental results demonstrate that they substantially improve the
performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K
Logic Puzzle reasoning tasks. Our implementation is available at
https://github.com/zhyang2226/AR-Lopti.
### 🌟 论文解读 | “优化大型语言模型强化学习：抑制低概率词汇的过度影响”

### 📌 背景痛点/本文动机
随着强化学习（RL）在大型语言模型（LLM）推理能力提升中的应用日益成熟，近期的研究如Group Relative Policy Optimization（GRPO）已经显示出显著的效果。然而，本文指出一个在RL训练中尚未被充分探索的关键问题：低概率词汇由于梯度幅值较大，不成比例地影响了模型更新。这种现象阻碍了高概率词汇的有效学习，而高概率词汇的梯度对于LLM的性能至关重要。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
为了缓解低概率词汇对模型更新的过度影响，本文提出了“优势重加权”方法。该方法通过降低低概率词汇的权重，从而减少它们对梯度更新的贡献。

💡 创新点2
本文还提出了“低概率词汇隔离”（Lopti）方法，该方法将低概率词汇与高概率词汇分开处理，先更新低概率词汇，再更新高概率词汇，从而确保高概率词汇的梯度更新不会受到低概率词汇的干扰。

### 📈 实验结果
实验结果表明，这两种方法都能显著提高GRPO训练的LLM性能。在K&K Logic Puzzle推理任务中，与原始GRPO相比，使用这两种方法可以分别提高35.9%和38.5%，同时使用时可以提高46.2%。

### 💬 可借鉴之处
本文的研究为大型语言模型的强化学习提供了新的视角，揭示了低概率词汇在梯度更新中的过度影响问题，并提出了有效的解决方案。这些方法不仅提高了模型在特定任务上的性能，而且为未来RL在LLM中的应用提供了新的思路和工具。此外，这些方法中的一种（优势重加权）几乎不增加额外的计算成本，使得它们在实际应用中更具吸引力。

## noisyrollout--reinforcing-visual-reasoning-with-data-augmentation
### Abstract
Recent advances in reinforcement learning (RL) have strengthened the
reasoning capabilities of vision-language models (VLMs). However, enhancing
policy exploration to better scale test-time compute remains largely
underexplored. In addition, VLMs continue to struggle with imperfect visual
perception, which in turn affects the subsequent reasoning process. To this
end, we propose NoisyRollout, a simple yet effective data augmentation method
that mixes trajectories from both clean and moderately distorted images during
RL training. By injecting targeted diversity in visual perception and the
resulting reasoning patterns, NoisyRollout promotes better policy exploration
through vision-oriented inductive biases, ultimately leading to more robust
reasoning behaviors. We further adopt a noise annealing schedule that gradually
reduces distortion strength over training, leveraging noisy signals early on
while ensuring training stability in later stages. Crucially, our method is
easy-to-adopt--requiring no additional training cost and no modifications to
the RL objective. Extensive experiments on $2$ distinct training datasets
demonstrate that NoisyRollout achieves state-of-the-art performance among
open-source RL-tuned models across $5$ out-of-domain reasoning and perception
benchmarks. Furthermore, we validate the effectiveness of NoisyRollout across
model sizes ($7$B and $32$B) and data scales (from $1$K to $6$K), highlighting
its generalizability and scalability.
### 🌟 论文解读 | “注入噪声以强化视觉推理：NoisyRollout方法解析”

### 📌 背景痛点/本文动机
随着强化学习（RL）在视觉语言模型（VLM）中的应用，模型的推理能力得到了显著提升。然而，如何通过增强策略探索来更好地扩展测试时的计算能力，以及如何解决VLM在视觉感知方面的不足，这两个问题在现有研究中尚未得到充分探讨。本文针对这些问题，提出了NoisyRollout方法，通过在训练过程中混合使用干净和适度扭曲的图像轨迹，以增强视觉推理的鲁棒性和泛化能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
NoisyRollout引入了一种简单而有效的水印增强策略，即在强化学习训练过程中，将基于原始干净图像和适度扭曲图像的轨迹混合起来。这种方法通过注入针对性的视觉感知多样性和推理模式，促进了策略探索，最终导致了更鲁棒的推理行为。

💡 创新点2
该方法还采用了一个噪声退火计划，随着训练的进行，逐渐减少图像扭曲的强度。这种策略在训练早期利用了噪声信号，同时在训练后期保证了训练的稳定性，避免了策略梯度估计的不稳定性。

### 📈 实验结果
本文在两个不同的训练数据集上进行了大量实验，结果表明NoisyRollout在五个超出训练域的推理和感知基准测试中，达到了开源RL调优模型中的最佳性能。此外，该方法在不同模型大小（7B和32B）和数据规模（从1K到6K）上都表现出了有效性和可扩展性。

### 💬 可借鉴之处
NoisyRollout方法提供了一种新的视角，即通过数据增强来改善VLM的视觉推理能力。其创新之处在于无需额外训练成本，也不需要对RL目标进行修改，即可轻松集成到现有的GRPO实现中。这种方法不仅提高了模型的性能，而且由于其简单性和轻量级特性，为未来的研究和应用提供了有价值的参考。

## reinforcement-learning-from-human-feedback
### Abstract
Reinforcement learning from human feedback (RLHF) has become an important
technical and storytelling tool to deploy the latest machine learning systems.
In this book, we hope to give a gentle introduction to the core methods for
people with some level of quantitative background. The book starts with the
origins of RLHF -- both in recent literature and in a convergence of disparate
fields of science in economics, philosophy, and optimal control. We then set
the stage with definitions, problem formulation, data collection, and other
common math used in the literature. The core of the book details every
optimization stage in using RLHF, from starting with instruction tuning to
training a reward model and finally all of rejection sampling, reinforcement
learning, and direct alignment algorithms. The book concludes with advanced
topics -- understudied research questions in synthetic data and evaluation --
and open questions for the field.
### 🌟 论文解读 | "迈向更智能的机器学习：基于人类反馈的强化学习全解读"

### 📌 背景痛点/本文动机
随着机器学习系统的不断发展，如何将这些系统与人类偏好相结合，以实现更智能、更符合人类期望的行为，成为了一个关键问题。本文旨在为具有一定定量背景的读者提供一个关于基于人类反馈的强化学习（RLHF）的全面介绍，从起源到最新的研究进展，以及相关的数学定义和优化方法。

### 🚀 核心方法
💡 创新点1
本文详细介绍了RLHF的核心方法，包括从人类偏好中学习、指令微调、奖励模型训练、拒绝采样、策略梯度算法和直接对齐算法等。这些方法共同构成了RLHF的完整流程，使得机器学习模型能够更好地理解和执行人类的指令。

💡 创新点2
论文深入探讨了偏好数据的收集和处理，包括如何通过界面设计、排名与评分、结构化偏好数据以及数据来源和合同等方面来优化数据质量。此外，还介绍了奖励模型的架构、训练方法和变体，以及如何通过正则化技术来提高模型的性能。

### 📈 实验结果
虽然本文主要提供理论和方法论上的介绍，但作者指出，RLHF在实际应用中已经取得了显著的成功，特别是在语言模型领域。通过实验验证，RLHF能够有效提高模型的性能，使其生成的内容更符合人类的期望和偏好。

### 💬 可借鉴之处
本文为机器学习领域的研究者和工程师提供了一个宝贵的资源，详细介绍了RLHF的理论基础和实践方法。以下是一些可借鉴之处：
- 如何从人类偏好中学习并优化机器学习模型。
- 指令微调和奖励模型训练的具体步骤和最佳实践。
- 如何通过拒绝采样和策略梯度算法来提高模型的性能。
- 直接对齐算法的应用和实现细节。
- 偏好数据的收集和处理技巧，以及如何避免数据污染和过度优化问题。
- 如何在产品设计和用户体验中融入RLHF，以创建更具个性和吸引力的AI产品。

## reasoning-beyond-limits--advances-and-open-problems-for-llms
### Abstract
Recent generative reasoning breakthroughs have transformed how large language
models (LLMs) tackle complex problems by dynamically retrieving and refining
information while generating coherent, multi-step thought processes. Techniques
such as inference-time scaling, reinforcement learning, supervised fine-tuning,
and distillation have been successfully applied to models like DeepSeek-R1,
OpenAI's o1 & o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in
enhanced reasoning capabilities. In this paper, we provide a comprehensive
analysis of the top 27 LLM models released between 2023 and 2025 (including
models such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and
phi-4). Then, we present an extensive overview of training methodologies that
spans general training approaches, mixture-of-experts (MoE) and architectural
innovations, retrieval-augmented generation (RAG), chain-of-thought and
self-improvement techniques, as well as test-time compute scaling,
distillation, and reinforcement learning (RL) methods. Finally, we discuss the
key challenges in advancing LLM capabilities, including improving multi-step
reasoning without human supervision, overcoming limitations in chained tasks,
balancing structured prompts with flexibility, and enhancing long-context
retrieval and external tool integration.
### 🌟 论文解读 | 探索大型语言模型推理极限：最新进展与未解难题

### 📌 背景痛点/本文动机
随着人工智能技术的快速发展，大型语言模型（LLM）在处理复杂问题方面取得了显著进展。本文的动机在于，尽管LLM在推理能力上有了突破，但在多步骤推理和解决复杂任务方面仍存在挑战。本文旨在分析2023至2025年间发布的27个顶级LLM模型，并探讨这些模型在推理能力上的增强及其背后的训练方法。

### 🚀 核心方法
💡 创新点1
本文首先对包括Mistral AI Small 3 24B、DeepSeek-R1、Search-o1、QwQ-32B和phi-4在内的27个LLM模型进行了全面分析。这些模型通过推理时的扩展、强化学习、监督微调以及蒸馏等技术，显著提高了推理能力。

💡 创新点2
文章详细介绍了多种训练方法，包括通用训练方法、混合专家（MoE）和架构创新、检索增强生成（RAG）、链式思维和自我提升技术，以及测试时的计算扩展、蒸馏和强化学习方法。

### 📈 实验结果
本文讨论了LLM模型在推理能力上的提升，特别是在数学推理、代码生成和特定领域任务上的表现。同时，文章指出了这些模型在多步骤推理和解决复杂任务时面临的挑战。

### 💬 可借鉴之处
本文提供了以下几个方面的借鉴意义：
1. 如何通过推理时的动态信息检索和优化来增强LLM的推理能力。
2. 如何利用混合专家、检索增强生成和链式思维等技术来提升模型的推理质量。
3. 如何通过强化学习和蒸馏方法来优化LLM的训练过程。
4. 针对LLM推理能力的提升，本文提出了关键的挑战和未来研究方向，包括无监督推理能力的提升、链式任务限制的克服、结构化提示与灵活性的平衡以及长上下文检索和外部工具集成的增强。

## crossing-the-reward-bridge--expanding-rl-with-verifiable-rewards-across-diverse-domains
### Abstract
Reinforcement learning with verifiable rewards (RLVR) has demonstrated
significant success in enhancing mathematical reasoning and coding performance
of large language models (LLMs), especially when structured reference answers
are accessible for verification. However, its extension to broader, less
structured domains remains unexplored. In this work, we investigate the
effectiveness and scalability of RLVR across diverse real-world domains
including medicine, chemistry, psychology, economics, and education, where
structured reference answers are typically unavailable. We reveal that binary
verification judgments on broad-domain tasks exhibit high consistency across
various LLMs provided expert-written reference answers exist. Motivated by this
finding, we utilize a generative scoring technique that yields soft,
model-based reward signals to overcome limitations posed by binary
verifications, especially in free-form, unstructured answer scenarios. We
further demonstrate the feasibility of training cross-domain generative reward
models using relatively small (7B) LLMs without the need for extensive
domain-specific annotation. Through comprehensive experiments, our RLVR
framework establishes clear performance gains, significantly outperforming
state-of-the-art open-source aligned models such as Qwen2.5-72B and
DeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach
notably enhances the robustness, flexibility, and scalability of RLVR,
representing a substantial step towards practical reinforcement learning
applications in complex, noisy-label scenarios.
### 🌟 论文解读 | 跨越奖励桥梁：在多样化领域中扩展可验证奖励的强化学习

### 📌 背景痛点/本文动机
强化学习与可验证奖励（RLVR）在增强大型语言模型（LLM）的数学推理和编码性能方面取得了显著成功，尤其是在有结构化参考答案可供验证的情况下。然而，将RLVR扩展到更广泛、结构化程度较低的应用领域仍然是一个未探索的领域。本文旨在研究RLVR在医学、化学、心理学、经济学和教育等多样化现实世界领域的有效性和可扩展性，这些领域通常缺乏结构化的参考答案。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文发现，在提供专家编写的参考答案的情况下，即使是广泛领域的任务，不同LLM之间的二元验证判断也表现出高度一致性。这一发现为重新思考多领域场景中奖励模型训练的传统做法提供了依据。

💡 创新点2
为了克服二元验证在自由形式、非结构化答案场景中的局限性，本文提出了一种生成评分技术，该技术产生软的、基于模型的奖励信号。此外，本文还证明了使用相对较小的（7B规模）LLM训练跨领域生成奖励模型的可行性，无需进行大量领域特定注释。

### 📈 实验结果
通过综合实验，本文的RLVR框架在多样化、自由形式的推理任务中确立了明确的性能提升，显著超过了最新的开源对齐模型，如Qwen2.5-72B和DeepSeek-R1-Distill-Qwen-32B。特别地，基于模型的软奖励在非结构化答案场景和较大训练数据集上表现出更好的可扩展性和更稳健的模型。

### 💬 可借鉴之处
本文的方法显著提高了RLVR的鲁棒性、灵活性和可扩展性，为在复杂、噪声标签场景中实际应用强化学习迈出了重要一步。此外，本文还提供了一个包含57万个多领域自由形式数据和相应训练奖励模型的数据库，以促进这一有前景方向的研究。

## does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model-
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has recently
demonstrated notable success in enhancing the reasoning performance of large
language models (LLMs), particularly on mathematics and programming tasks.
Similar to how traditional RL helps agents explore and learn new strategies,
RLVR is believed to enable LLMs to continuously self-improve, thus acquiring
novel reasoning abilities beyond those of the corresponding base models. In
this study we critically examine the current state of RLVR by systematically
probing the reasoning capability boundaries of RLVR-trained LLMs across various
model families, RL algorithms, and math, coding, and visual reasoning
benchmarks, using pass@k at large k values as the evaluation metric.
Surprisingly, we find that the current training setup does not elicit
fundamentally new reasoning patterns. While RLVR-trained models outperform
their base models at small k (e.g., k = 1), the base models achieve a higher
pass@k score when k is large. Coverage and perplexity analyses show that the
observed reasoning abilities originate from and are bounded by the base model.
Treating the base model as an upper bound, our quantitative analysis shows that
six popular RLVR algorithms perform similarly and remain far from optimal in
leveraging the potential of the base model. By contrast, we find that
distillation can introduce new reasoning patterns from the teacher and
genuinely expand the model's reasoning capabilities. Overall, our findings
suggest that current RLVR methods have not yet realized the potential of RL to
elicit truly novel reasoning abilities in LLMs. This highlights the need for
improved RL paradigms, such as continual scaling and multi-turn
agent-environment interaction, to unlock this potential.
### 🌟 论文解读 | 探究强化学习是否真正提升了LLM的推理能力

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLM）在解决数学和编程任务方面的能力得到了显著提升，这主要归功于大规模的强化学习与可验证奖励（RLVR）方法。传统强化学习能够帮助智能体探索和学习新策略，人们普遍认为RLVR也能让LLM持续自我改进，从而获得超越基础模型的推理能力。然而，当前RLVR方法是否真的实现了这一目标，仍然是一个未经验证的问题。

### 🚀 核心方法
💡 创新点1
本文通过系统地研究不同模型家族、RL算法以及数学、编程和视觉推理基准，使用大k值的pass@k作为评估指标，来探查RLVR训练的LLM的推理能力边界。

💡 创新点2
研究发现，尽管RLVR训练的模型在小k值（如k=1）时表现优于基础模型，但在k值较大时，基础模型却能达到更高的pass@k分数。这表明当前的训练并没有激发出本质上新的推理模式。

### 📈 实验结果
实验结果显示，当前的RLVR模型在推理覆盖面上比基础模型更窄。尽管RLVR模型在提高采样效率方面有所改进，但并没有让模型解决新的问题。此外，通过对推理路径的复杂度分析，发现RLVR模型的推理路径已经存在于基础模型的输出分布中，说明RLVR并没有引入本质上新的推理能力。

### 💬 可借鉴之处
本文的研究表明，当前的RLVR方法并没有完全实现通过强化学习激发LLM新型推理能力的潜力。这提示我们需要改进RL范式，如持续扩展和多方交互，以解锁这一潜力。此外，本文还发现蒸馏方法能够从教师模型中引入新的推理模式，并真正扩展模型的推理能力，这为未来的研究提供了一个有价值的方向。

## rlvr-world--training-world-models-with-reinforcement-learning
### Abstract
World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.
### 🌟 论文解读 | “RLVR-World：利用强化学习提升世界模型性能”

### 📌 背景痛点/本文动机
随着世界模型在各种模态（如文本、视频和感官数据）中的广泛应用，这些模型在预测状态转换方面取得了显著进展。然而，传统的训练目标（如最大似然估计MLE）往往与任务特定的目标（如预测准确性或感知质量）不一致。本文旨在解决这一问题，提出了一种名为RLVR-World的统一框架，利用强化学习与可验证奖励（RLVR）直接优化世界模型的任务特定指标。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文将世界模型统一为一个通用的自回归生成框架，将当前状态和动作编码为问题令牌，将下一个状态编码为响应令牌，类似于语言模型的构建方式。

💡 创新点2
在统一框架下，本文综合探讨了RLVR在两种代表性模态（语言和视频）的世界模型中的应用。对于语言世界模型，通过使用预测准确性作为可验证奖励，有效微调了大型语言模型（LLM），在文本游戏状态预测和网页状态预测方面取得了显著改进。对于视频世界模型，通过直接测量和优化解码预测帧的感知指标，实现了在机器人操作轨迹预测方面的显著增益。

### 📈 实验结果
实验结果显示，RLVR-World在语言和视频世界模型中均取得了实质性性能提升。例如，在文本游戏状态预测中，准确度提高了30.7%；在网页状态预测中，F1分数提高了15.1%。在视频方面，通过仅几百次的RLVR梯度更新，实现了LPIPS指标上的9.2%相对改进，而MLE训练需要数万次迭代才能达到相似的性能。

### 💬 可借鉴之处
本文提出的方法和实验分析表明，RLVR作为一种通用的后训练范式，可以显著提升世界模型的有用性，并且有望推广到更广泛的生成模型。对于希望优化生成模型任务特定指标的研究者和工程师来说，RLVR-World提供了一种新颖且有效的途径。此外，本文的实验结果也证明了RLVR在减少重复性问题方面的有效性，这对于大型语言模型尤其重要。

## atom-of-thoughts-for-markov-llm-test-time-scaling
### Abstract
Large Language Models (LLMs) achieve superior performance through
training-time scaling, and test-time scaling further enhances their
capabilities by conducting effective reasoning during inference. However, as
the scale of reasoning increases, existing test-time scaling methods suffer
from accumulated historical information, which not only wastes computational
resources but also interferes with effective reasoning. To address this issue,
we observe that complex reasoning can be achieved by solving a series of
independent and self-contained subquestions. These subquestions are essentially
\textit{atomic questions}, exhibiting the memoryless property similar to Markov
processes. Based on this observation, we propose Atom of Thoughts (\our), where
each state transition consists of decomposing the current question into a
dependency-based directed acyclic graph and contracting its subquestions,
forming a simplified question that maintains answer equivalence with the
original problem. This answer preservation enables the iterative
\textit{decomposition-contraction} process to naturally form a meaningful
Markov reasoning process. Furthermore, these atomic states can be seamlessly
integrated into existing test-time scaling methods, enabling \our to serve as a
plug-in enhancement for improving reasoning capabilities. Experiments across
six benchmarks demonstrate the effectiveness of \our both as a standalone
framework and a plug-in enhancement. Notably, on HotpotQA, when applied to
gpt-4o-mini, \our achieves an \textbf{80.6\%} F1 score, surpassing o3-mini by
\textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}. The code is available at
\href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.
### 🌟 论文解读 | “原子思维”助力大规模语言模型推理效率提升

### 📌 背景痛点/本文动机
大规模语言模型（LLMs）通过训练时间的扩展实现了卓越的性能，而测试时间的扩展则通过在推理过程中进行有效的推理进一步增强了其能力。然而，随着推理规模的增加，现有的测试时间扩展方法在推理过程中过度维护历史信息，这不仅浪费了计算资源，还干扰了有效的推理。为了解决这一问题，本文提出了“原子思维”（Atom of Thoughts，简称AOT）框架。

### 🚀 核心方法
💡 创新点1
观察到复杂推理可以通过解决一系列独立且自包含的子问题来实现。这些子问题本质上是具有马尔可夫性质的原子问题。基于这一观察，AOT框架通过将当前问题分解为依赖关系的有向无环图（DAG），然后将其子问题收缩形成一个新的简化问题，这个简化问题与原始问题具有相同的答案等价性。

💡 创新点2
AOT框架中的每个状态转换都包括问题的分解和收缩过程，直到达到可以直接解决的原子问题。这种迭代的过程确保了每个状态转换仅依赖于当前状态，同时逐步降低问题的复杂性。这些原子状态可以无缝地集成到现有的测试时间扩展方法中，使AOT可以作为插件增强来提高推理能力。

### 📈 实验结果
在六个基准测试上的实验表明，AOT框架作为独立框架和插件增强都显示出有效性。特别地，在HotpotQA数据集上，当应用于gpt-4o-mini时，AOT实现了80.6%的F1分数，超过了o3-mini的3.4%和DeepSeek-R1的10.6%。

### 💬 可借鉴之处
本文提出的AOT框架提供了一种新的思路，通过将复杂问题分解为原子问题，减少了在推理过程中对历史信息的依赖，从而提高了计算效率和推理能力。此外，AOT框架的设计允许其作为插件轻松集成到现有的测试时间扩展方法中，为大规模语言模型的推理能力提升提供了灵活的解决方案。论文的代码已经在GitHub上公开，便于社区进一步研究和应用。

## delving-into-rl-for-image-generation-with-cot--a-study-on-dpo-vs--grpo
### Abstract
Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT
### 🌟 论文解读 | 深入研究基于强化学习的图像生成：DPO与GRPO的比较分析

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在多种挑战性任务中取得显著成就，强化学习（RL）在增强LLM的链式思维（CoT）推理能力方面扮演了重要角色。本文关注两种突出的RL算法：直接偏好优化（DPO）和组相对策略优化（GRPO），它们在提升LLM性能方面各有优劣。然而，在图像生成领域，尤其是自回归图像生成中，这些算法的应用和挑战尚未得到深入分析。本文旨在填补这一空白，探讨DPO和GRPO在自回归图像生成中的性能和泛化能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首次对DPO和GRPO算法在自回归图像生成中的性能进行了全面比较，评估了它们在领域内和领域外的表现，并深入分析了不同奖励模型对它们性能的影响。

💡 创新点2
本文系统地探索了三种常见的扩展策略，以提升DPO和GRPO在领域内和领域外的性能，为每种算法的效率扩展提供了独特的见解。

### 📈 实验结果
- **领域内与领域外表现**：DPO在领域内任务上表现出色，而GRPO在领域外任务上具有更强的泛化能力。
- **不同奖励模型的影响**：DPO对奖励模型的变动更为敏感，而一个具有更强泛化能力的奖励模型可能提升RL算法的泛化性能。
- **扩展策略的影响**：对于GRPO，通过扩展采样图像可以更高效地提升领域内性能；而对于DPO，迭代训练可以最大化领域内性能，但过多的迭代可能会损害泛化能力。

### 💬 可借鉴之处
本文的研究为理解DPO和GRPO在图像生成领域的优势和挑战提供了宝贵的数据和见解。研究者可以借鉴本文的方法来设计更有效的RL算法，特别是在奖励模型和扩展策略的选择上，以实现更稳健的CoT推理能力。此外，本文的代码和数据集已经公开，便于社区进一步验证和扩展研究。

## right-question-is-already-half-the-answer--fully-unsupervised-llm-reasoning-incentivization
### Abstract
Existing methods to enhance the reasoning capability of large language models
predominantly rely on supervised fine-tuning (SFT) followed by reinforcement
learning (RL) on reasoning-specific data. These approaches critically depend on
external supervisions--such as labeled reasoning traces, verified golden
answers, or pre-trained reward models. In this work, we propose Entropy
Minimized Policy Optimization (\ours), which makes an early attempt at fully
unsupervised LLM reasoning incentivization. By continuously minimizing the
predictive entropy of LLMs on unlabeled questions in a latent semantic space,
\ours achieves competitive performance compared to supervised counterparts on
both mathematical and free-form natural reasoning tasks. Specifically, without
any supervised signals, \ours boosts the accuracy of Qwen2.5-Math-7B Base from
30.7\% to 48.1\% on mathematical benchmarks and improves the accuracy of
Qwen2.5-7B Base from 32.1\% to 50.1\% on MMLU-Pro. Primary experiments and
analysis are also provided to interpret the effectiveness of \ours. Code is
available at https://github.com/QingyangZhang/EMPO.
### 🌟 论文解读 | “解锁LLM推理能力：无需监督的全自动路径探索”

### 📌 背景痛点/本文动机
在当前的大型语言模型（LLM）训练中，提升模型的推理能力通常依赖于监督微调（SFT）和基于推理特定数据的强化学习（RL）。这些方法高度依赖外部监督信号，例如标记的推理轨迹、验证的金标准答案或预训练的奖励模型。这种依赖性使得模型推理能力的培养变得耗时且成本高昂，限制了推理模型的扩展性和广泛应用。本文旨在探索一种完全无监督的方式，激励LLM的推理能力。

### 🚀 核心方法
💡 创新点1
本文提出了熵最小化策略优化（EMPO）算法，这是一种早期尝试完全无监督的LLM推理激励方法。EMPO通过在潜在语义空间中持续最小化LLM对未标记问题的预测熵，实现了与监督方法相当的性能。

💡 创新点2
EMPO方法将语义熵作为一个强大的内在奖励信号，引导LLM的推理。通过实验分析，本文确认了语义熵与模型准确性之间的强烈负相关性，验证了其作为稳健的无监督优化目标的有效性。

### 📈 实验结果
在数学推理任务和自由形式的自然推理任务中，EMPO方法均显示出其有效性和通用性。具体来说，在没有监督信号的情况下，EMPO将Qwen2.5-Math-7B Base在数学基准测试上的准确度从30.7%提升到48.1%，将Qwen2.5-7B Base在MMLU-Pro上的准确度从32.1%提升到50.1%。

### 💬 可借鉴之处
本文的方法为LLM推理能力的培养提供了一种新的视角，即通过熵最小化来激励模型内在的推理路径。这种方法不仅减少了对外部监督信号的依赖，而且能够有效挖掘和优化模型在预训练阶段学习到的潜在推理能力。此外，EMPO方法在实验中表现出的稳定性和有效性，为未来无监督学习的研究提供了宝贵的参考。

## toolrl--reward-is-all-tool-learning-needs
### Abstract
Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.
### 🌟 论文解读 | 工具学习，奖励机制至关重要：探索LLM工具使用的新路径

### 📌 背景痛点/本文动机
当前的大型语言模型（LLM）通常通过监督微调（SFT）来获取工具使用能力。然而，SFT在泛化到不熟悉或复杂的工具使用场景上存在困难。尽管最近强化学习（RL）的进展，特别是R1-like模型，展示了令人鼓舞的推理和泛化能力，但工具使用的奖励设计却面临着独特的挑战。本文旨在解决这一挑战，提出了一种针对工具选择和应用任务的奖励设计，以提升LLM的工具使用能力和泛化性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文是首个针对工具选择和应用任务在RL框架下的奖励设计进行全面研究的论文。作者系统地探索了多种奖励策略，包括奖励类型、尺度、粒度和时间动态。

💡 创新点2
基于这些探索，作者提出了一种原则性的奖励设计，并将其应用于LLM的训练中，使用了Group Relative Policy Optimization（GRPO）算法。这种设计考虑了工具使用的复杂性，提供了细粒度的反馈，有助于模型的有效学习。

### 📈 实验结果
在多个基准测试中，作者的方法展示了稳健、可扩展和稳定的训练效果，相较于基础模型提高了17%，相较于SFT模型提高了15%。这些结果突出了精心设计的奖励机制在增强LLM工具使用能力和泛化性能中的关键作用。

### 💬 可借鉴之处
本文提供了以下可借鉴之处：
- 长的推理轨迹并不总是更好，长度奖励可能会降低性能。
- 动态的奖励尺度有助于模型平滑地从简单行为过渡到复杂行为。
- 细粒度的奖励分解可以带来更稳定和有效的学习。
此外，论文还开源了所有代码，为未来的研究提供了便利。

## srpo--a-cross-domain-implementation-of-large-scale-reinforcement-learning-on-llm
### Abstract
Recent advances of reasoning models, exemplified by OpenAI's o1 and
DeepSeek's R1, highlight the significant potential of Reinforcement Learning
(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).
However, replicating these advancements across diverse domains remains
challenging due to limited methodological transparency. In this work, we
present two-Staged history-Resampling Policy Optimization (SRPO), which
surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and
LiveCodeBench benchmarks. SRPO achieves this using the same base model as
DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps
required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building
upon Group Relative Policy Optimization (GRPO), we introduce two key
methodological innovations: (1) a two-stage cross-domain training paradigm
designed to balance the development of mathematical reasoning and coding
proficiency, and (2) History Resampling (HR), a technique to address
ineffective samples. Our comprehensive experiments validate the effectiveness
of our approach, offering valuable insights into scaling LLM reasoning
capabilities across diverse tasks.
### 🌟 论文解读 | “SRPO：跨域大规模强化学习在大型语言模型上的应用”

### 📌 背景痛点/本文动机
近年来，以OpenAI的o1和DeepSeek的R1为代表的推理模型取得了显著进展，展示了强化学习（RL）在提升大型语言模型（LLM）推理能力方面的巨大潜力。然而，将这些进展应用到不同领域仍然具有挑战性，主要原因是缺乏方法论透明度。本文旨在解决这一问题，提出了一种名为SRPO（两阶段历史重采样策略优化）的算法，以实现跨域推理能力的提升。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：两阶段跨域训练范式
本文引入了一种两阶段的训练范式，旨在平衡数学推理和编码能力的培养。第一阶段主要训练数学数据，以促进反思性思考和逐步问题解决能力的形成；第二阶段则加入编码数据，建立在第一阶段推理技能的基础上，进一步发展编码能力。

💡 创新点2：历史重采样技术
为了解决Group Relative Policy Optimization（GRPO）中的零优势现象，本文提出了一种历史重采样技术。通过过滤掉始终正确的答案，确保了有意义的梯度更新，提高了样本效率，并加速了收敛。

### 📈 实验结果
SRPO在AIME24和LiveCodeBench基准测试中超越了DeepSeek-R1-Zero-32B的性能，使用与DeepSeek相同的基模型（Qwen2.5-32B），但仅需要DeepSeek-R1-Zero-32B训练步数的1/10，展示了卓越的效率。

### 💬 可借鉴之处
本文详细介绍了SRPO算法的设计和实现，为大规模强化学习在LLM中的应用提供了新的视角和方法。以下是几个值得借鉴的方面：
- 两阶段训练范式为LLM在数学和编码任务上的推理能力提升提供了有效的路径。
- 历史重采样技术提高了样本效率，对于强化学习中的梯度更新和收敛具有重要作用。
- 对数据清洗和难度等级分类的详细描述，为构建高质量训练数据集提供了参考。
- 对大规模RL训练中遇到的挑战和思考行为的深入分析，为未来的研究提供了实证见解。

## thought-augmented-policy-optimization--bridging-external-guidance-and-internal-capabilities
### Abstract
Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
("thought patterns"). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.
### 🌟 论文解读 | “思维增强策略优化：融合外部指导与内部能力”

### 📌 背景痛点/本文动机
强化学习（RL）作为一种有效的训练推理模型的方法，已经取得了显著的进展。然而，现有的强化学习策略往往倾向于将模型的输出分布偏向于奖励最大化的路径，而没有引入外部知识。这限制了模型的探索能力，并导致其推理能力边界相比基础模型更窄。为了解决这一局限性，本文提出了TAPO（思维增强策略优化）框架，通过整合外部高级指导（“思维模式”）来增强强化学习。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
TAPO框架通过引入外部高级思维模式，有效地平衡了模型内部的探索和外部指导的利用。这些思维模式是从先前的500个样本中抽象出来的，每个模板代表了一类问题的抽象问题解决策略，作为推理指导。

💡 创新点2
TAPO设计了一个“思维库”，这是一个存储高级思维模板的通用仓库。在GRPO采样过程中，对于每个新问题，系统会自适应地识别并应用思维库中相关的思维模板，以增强推理过程。

### 📈 实验结果
本文的TAPO方法在多个数据集上显著优于GRPO，平均提高了12.0个点，包括在AIME上提高了99%，在AMC上提高了41%，在Minerva Math上提高了17%。TAPO方法在各种模型规模和架构上都表现出有效性，并且能够有效地推广到分布外推理任务。进一步的分析确认，引入外部指导不仅增强了模型输出的可解释性，也提高了输出的可读性。

### 💬 可借鉴之处
本文提出的TAPO框架为强化学习在推理模型中的应用提供了新的视角，特别是如何结合外部高级指导和内部推理能力。TAPO的实验结果表明，这种方法不仅提高了模型的性能，还增强了其泛化能力和输出质量。此外，TAPO框架的设计理念和方法对于其他需要推理能力的机器学习任务也具有借鉴意义。

## think-when-you-need--self-adaptive-chain-of-thought-learning
### Abstract
Chain of Thought (CoT) reasoning enhances language models' performance but
often leads to inefficient "overthinking" on simple problems. We identify that
existing approaches directly penalizing reasoning length fail to account for
varying problem complexity. Our approach constructs rewards through length and
quality comparisons, guided by theoretical assumptions that jointly enhance
solution correctness with conciseness. Moreover, we further demonstrate our
method to fuzzy tasks where ground truth is unavailable. Experiments across
multiple reasoning benchmarks demonstrate that our method maintains accuracy
while generating significantly more concise explanations, effectively teaching
models to "think when needed."
### 🌟 论文解读 | "深度学习新策略：按需思考的链式推理学习"

### 📌 背景痛点/本文动机
随着推理模型如OpenAI o1和Deepseek-R1的广泛应用，链式推理（Chain of Thought, CoT）被证明可以显著提升语言模型的性能。然而，这种推理方式在处理简单问题时往往会出现过度推理的效率问题。现有的方法通常通过直接对推理长度进行惩罚来解决这个问题，但这种方法对超参数敏感，且缺乏普遍适用性，特别是在没有明确答案的模糊任务中。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种新的奖励算法，该算法不是直接对响应长度进行惩罚，而是通过样本间的长度和质量比较来构建奖励。这种方法基于理论假设，旨在同时提升解决方案的正确性和简洁性。

💡 创新点2
该方法自然地扩展到了具有明确答案的可验证任务和需要主观评价的模糊任务。通过成对比较所有可能的样本组合，并为每对样本计算成对奖励，最终通过聚合每个样本收到的所有成对奖励来确定每个样本的最终奖励。

### 📈 实验结果
本文在多个推理基准上进行了广泛实验，结果表明，该方法在保持准确性的同时，能够生成更加简洁的解释，有效地教会模型在需要时才进行思考。

### 💬 可借鉴之处
本文的方法论为处理推理模型中的效率问题提供了新的视角，特别是在处理模糊任务时，提供了一种在没有明确真相的情况下进行有效学习的方法。此外，该方法的设计基于清晰的理论假设，易于泛化和与其他奖励结构兼容，为未来的研究提供了丰富的探索空间。

## reinforced-mllm--a-survey-on-rl-based-reasoning-in-multimodal-large-language-models
### Abstract
The application of reinforcement learning (RL) to enhance the reasoning
capabilities of Multimodal Large Language Models (MLLMs) constitutes a rapidly
advancing research area. While MLLMs extend Large Language Models (LLMs) to
handle diverse modalities such as vision, audio, and video, enabling robust
reasoning across multimodal inputs remains challenging. This paper provides a
systematic review of recent advances in RL-based reasoning for MLLMs, covering
key algorithmic designs, reward mechanism innovations, and practical
applications. We highlight two main RL paradigms, value-model-free and
value-model-based methods, and analyze how RL enhances reasoning abilities by
optimizing reasoning trajectories and aligning multimodal information.
Additionally, we provide an extensive overview of benchmark datasets,
evaluation protocols, and current limitations, and propose future research
directions to address challenges such as sparse rewards, inefficient
cross-modal reasoning, and real-world deployment constraints. Our goal is to
provide a comprehensive and structured guide to RL-based multimodal reasoning.
### 🌟 论文解读 | "强化学习赋能多模态大语言模型：推理能力提升综述"

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）的出现，人工智能领域迎来了前所未有的新时代，LLM 展现出了卓越的指令遵循和少量样本学习的能力。然而，要实现人类级别的智能，不仅需要超越基本的感知能力，还需要发展复杂的认知推理技能，这些技能能够通过上下文理解和自我修正进行迭代推理。尽管如此，多模态大型语言模型（MLLM）在处理多种模态输入时的稳健推理仍然具有挑战性。本文旨在填补这一研究空白，提供了一篇关于基于强化学习（RL）的 MLLM 推理能力的综述，系统地回顾了最新的研究进展、关键算法设计、奖励机制创新以及实际应用。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首先介绍了两种主要的 RL 方法：价值模型无关（value-model-free）和价值模型基于（value-model-based）方法，并分析了它们如何通过优化推理轨迹和对齐多模态信息来增强推理能力。

💡 创新点2
本文详细调研了基于 RL 的 MLLM 推理方法，包括算法框架、奖励函数设计（包括准确性导向和结构导向的方法），以及多模态信息融合策略。这些组件如何共同解决多模态推理的核心挑战也得到了强调。

### 📈 实验结果
本文提供了对基准数据集、评估协议以及涵盖数学、科学、空间和基于交互的领域的基准的广泛概述，但具体的实验结果在摘要中未详细说明。

### 💬 可借鉴之处
本文为研究人员提供了一个全面而系统的指南，以识别在 MLLM 推理领域快速发展的背景下合适的方法，从而促进该领域的进一步创新和进步。以下是几个可借鉴之处：
- 对于 RL 在 LLM 和 MLLM 中的核心设计进行了深入分析，讨论了训练效率、稳定性和性能的创新，以及优化机会。
- 对基于 RL 的推理方法进行了详细分析，包括算法框架、奖励函数设计和多模态信息融合策略。
- 提供了关于推理任务的数据集和评估方法的概述，包括数据来源和注释构建流程。
- 指出了当前未解决的挑战，如稀疏奖励、低效的轨迹和跨模态协调，并提出了未来的研究方向，包括分层奖励建模、视觉引导的 CoT 和适用于实际应用的轻量级 RL 框架。

## echoink-r1--exploring-audio-visual-reasoning-in-multimodal-llms-via-reinforcement-learning
### Abstract
Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.
### 🌟 论文解读 | EchoInk-R1：通过强化学习提升多模态大语言模型的音视觉推理能力

### 📌 背景痛点/本文动机
随着多模态大语言模型（MLLMs）在文本、视觉和音频感知方面的进步，这些模型在处理结构化跨模态推理时仍然面临挑战，尤其是在整合音频和视觉信号时。本文旨在解决这一问题，提出了一种名为EchoInk-R1的强化学习框架，以增强MLLMs中的音视觉推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
EchoInk-R1框架建立在Qwen2.5-Omni-7B模型之上，并采用Group Relative Policy Optimization（GRPO）进行优化。该框架专门针对同步音频-图像对的多项选择题回答任务。

💡 创新点2
为了支持这一框架，本文构建了AVQA-R1-6K数据集，该数据集将音频-图像输入与OmniInstruct-v1数据集中的多项选择题相结合。这一数据集专门为音视觉推理任务设计。

### 📈 实验结果
实验结果显示，EchoInk-R1-7B在AVQA-R1-6K验证集上取得了85.77%的准确率，显著优于基线模型Qwen2.5-Omni-7B的80.53%，且仅使用了562步强化学习。此外，EchoInk-R1还展示了在面临模糊多模态输入时，重新审视初始解释并 refined 响应的能力。

### 💬 可借鉴之处
本文的研究表明，即使是轻量级的强化学习微调也能显著增强MLLMs的跨模态推理能力。EchoInk-R1是第一个将音频、视觉和文本模态统一于强化学习框架中，用于通用开放世界推理的框架。此外，论文中提出的“aha moments”现象，即模型在训练过程中自我修正推理的能力，为理解模型如何进行跨模态推理提供了新的视角。论文的代码和数据已公开，有助于进一步的研究和验证。

## think-or-not--selective-reasoning-via-reinforcement-learning-for-vision-language-models
### Abstract
Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across diverse vision-language tasks-covering
a range of reasoning difficulties under both 3B and 7B models-consistently
reveal that the model progressively learns to bypass unnecessary reasoning
steps as training advances. These findings shed light on the path toward
human-like reasoning patterns in reinforcement learning approaches. Our code is
available at https://github.com/kokolerk/TON.
### 🌟 论文解读 | "智能推理新策略：学会选择何时思考"

### 📌 背景痛点/本文动机
在视觉语言模型（VLMs）中，强化学习（RL）已被证明是一种有效的后训练策略，可以增强推理能力。然而，现有的方法如Group Relative Policy Optimization（GRPO）往往导致不必要的长推理过程，增加了token的使用和计算成本。本文的动机在于，模仿人类的思考过程——对于简单问题跳过推理，对于困难问题仔细思考——来提高VLMs的推理效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
提出了TON（Think or Not）训练框架，该框架包含两个阶段：监督微调（SFT）阶段和GRPO阶段。在SFT阶段，引入了“思维dropout”操作，随机将推理轨迹替换为空思维，从而引入了“思考与否”格式，为选择性推理提供了冷启动。

💡 创新点2
在GRPO阶段，模型可以自由探索何时思考或跳过思考，同时最大化任务感知的产出奖励。这种方法使得模型能够根据问题的复杂性适应性地学习跳过不必要的推理步骤。

### 📈 实验结果
实验结果显示，TON能够将完成长度减少多达90%，同时不牺牲性能，甚至在某些情况下还能提高性能。在多种视觉语言任务中的评估一致表明，随着训练的进行，模型逐步学会跳过不必要的推理步骤。

### 💬 可借鉴之处
本文提出的方法为视觉语言模型中的推理过程提供了新的视角，即“何时思考”而不是“如何思考”。通过让模型首先决定是否需要推理，可以显著提高推理效率和减少计算成本。此外，这种方法在保持或提升性能的同时，减少了token的使用，对于资源有限的场景尤其有价值。论文中的“思维dropout”操作和两阶段训练策略为未来视觉语言模型的研究提供了新的思路。

## unlocking-the-potential-of-difficulty-prior-in-rl-based-multimodal-reasoning
### Abstract
In this work, we investigate how explicitly modeling problem's difficulty
prior information shapes the effectiveness of reinforcement learning based
fine-tuning for multimodal reasoning. Our exploration mainly comprises of
following three perspective: First, through offline data curation, we analyze
the U-shaped difficulty distribution of two given datasets using the base model
by multi-round sampling, and then filter out prompts that are either too simple
or extremely difficult to provide meaningful gradients and perform subsequent
two-stage training. Second, we implement an online advantage differentiation,
computing group-wise empirical accuracy as a difficulty proxy to adaptively
reweight advantages estimation, providing stronger learning signals for more
challenging problems. Finally, we introduce difficulty hints as explicit
prompts for more complex samples in the second training stage, encouraging the
model to calibrate its reasoning depth and perform reflective validation
checks. Our comprehensive approach demonstrates significant performances across
various multi-modal mathematical reasoning benchmarks with only 2K+0.6K
two-stage training data.
### 🌟 论文解读 | 解锁强化学习在多模态推理中的难题优先潜力

### 📌 背景痛点/本文动机
随着大型推理模型在复杂问题解决任务中的表现越来越出色，如何提高这些模型在多模态推理任务中的有效性成为研究的热点。现有的方法往往面临着数据难度混合、奖励机制单一和平滑难度意识缺失等问题。本文旨在探索如何通过显式建模问题的难度先验信息来提升基于强化学习的微调效果。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首先通过离线数据筛选，使用基础模型对给定数据集进行多轮采样，分析出U形的难度分布，并过滤掉过于简单或过于困难的问题，从而进行后续的两阶段训练。

💡 创新点2
在在线训练过程中，本文实现了优势差异化的方法，通过计算组内经验准确率作为难度代理，自适应地调整优势估计，为更具挑战性的问题提供更强的学习信号。

💡 创新点3
本文还引入了难度提示，作为第二阶段训练中对更复杂样本的显式提示，鼓励模型调整其推理深度并进行反思性验证检查。

### 📈 实验结果
本文的方法在各种多模态数学推理基准测试中表现显著，仅使用2K+0.6K两阶段训练数据就取得了优异的性能。

### 💬 可借鉴之处
本文提出的方法在处理多模态推理任务时，充分考虑了问题的难度分布，通过离线筛选和在线调整，优化了模型的学习过程。此外，引入难度提示的创新思路，为模型提供了更有效的推理策略，对于提升大型推理模型在复杂任务中的表现具有借鉴意义。

## not-all-thoughts-are-generated-equal--efficient-llm-reasoning-via-multi-turn-reinforcement-learning
### Abstract
Compressing long chain-of-thought (CoT) from large language models (LLMs) is
an emerging strategy to improve the reasoning efficiency of LLMs. Despite its
promising benefits, existing studies equally compress all thoughts within a
long CoT, hindering more concise and effective reasoning. To this end, we first
investigate the importance of different thoughts by examining their
effectiveness and efficiency in contributing to reasoning through automatic
long CoT chunking and Monte Carlo rollouts. Building upon the insights, we
propose a theoretically bounded metric to jointly measure the effectiveness and
efficiency of different thoughts. We then propose Long$\otimes$Short, an
efficient reasoning framework that enables two LLMs to collaboratively solve
the problem: a long-thought LLM for more effectively generating important
thoughts, while a short-thought LLM for efficiently generating remaining
thoughts. Specifically, we begin by synthesizing a small amount of cold-start
data to fine-tune LLMs for long-thought and short-thought reasoning styles,
respectively. Furthermore, we propose a synergizing-oriented multi-turn
reinforcement learning, focusing on the model self-evolution and collaboration
between long-thought and short-thought LLMs. Experimental results show that our
method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance
compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while
reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and
GPQA Diamond benchmarks. Our data and code are available at
https://github.com/usail-hkust/LongShort.
### 🌟 论文解读 | “思维不等同：通过多轮强化学习实现大型语言模型的高效推理”

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）的发展，长链式思维（CoT）推理能力逐渐成为其关键特性。然而，长CoT推理的过度token长度问题限制了其效率和实用性。现有研究在压缩CoT长度方面取得了进展，但这些方法往往平等地压缩所有思维步骤，没有考虑到不同思维步骤的重要性差异。本文旨在解决这一问题，提出了一种新的高效推理框架。

### 🚀 核心方法
💡 创新点1
本文首先建立了一个思维分析框架，通过自动将长CoT分解为多个思维块，并量化它们对推理过程的贡献。这一框架包括三个步骤：自动长CoT分块、思维回滚和联合测量。

💡 创新点2
基于上述分析，本文提出了Long⊗Short框架，该框架通过两个LLM协同解决问题：一个长思维LLM生成重要思维，而一个短思维LLM生成剩余思维。具体来说，首先通过合成少量冷启动数据来微调LLM，使其适应长思维和短思维推理风格。然后，提出了一种面向协同的多轮强化学习方案，专注于模型的自我进化和长思维与短思维LLM之间的协作。

### 📈 实验结果
实验结果表明，Long⊗Short框架使Qwen2.5-7B和Llama3.1-8B在MATH500、AIME24/25、AMC23和GPQA Diamond基准测试中，与它们的蒸馏版本（DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B）相比，实现了相当的性能，同时将token长度减少了超过80%。

### 💬 可借鉴之处
本文提出的方法不仅为长CoT压缩提供了理论基础，还通过实验验证了其有效性。其创新点包括：
- 自动化思维分析框架，为量化思维贡献提供了新方法。
- Long⊗Short框架，通过长思维和短思维的协同工作，提高了推理效率。
- 面向协同的多轮强化学习方案，为模型自我进化和协作提供了新思路。
这些成果对于提升LLM的推理效率和实用性具有重要的参考价值。

## rl-in-name-only--analyzing-the-structural-assumptions-in-rl-post-training-for-llms
### Abstract
Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.
### 🌟 论文解读 | 深度解析：LLM后训练中的RL方法是否名不副实？

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLM）在复杂推理任务上的能力得到了显著提升，特别是在数学问题解决方面。其中，基于强化学习（RL）的后训练方法，如群相对策略优化（GRPO），受到了广泛关注。然而，本文作者对这类方法背后的结构假设进行了批判性分析，指出这些假设可能导致RL在实际应用中的效果并不如预期。

### 🚀 核心方法
💡 创新点1
本文首先揭示了在将LLM训练建模为马尔可夫决策过程（MDP）时所做的流行结构假设。这些假设包括：将MDP状态视为动作的简单连接，其中状态成为上下文窗口，动作成为LLM中的标记；以及将奖励均匀分配到状态-动作轨迹上。作者通过分析表明，这些简化的假设实际上使得这种方法等效于结果驱动的监督学习。

💡 创新点2
作者进一步通过实验验证了，在GSM8K和Countdown基准测试中，使用Qwen-2.5基模型的迭代监督微调（SFT）方法，结合正负样本，可以达到与GRPO基训练相当的性能。此外，作者还指出，这些结构假设间接激励RL生成更长的中间标记序列，这进一步强化了“RL生成更长的思考轨迹”的说法。

### 📈 实验结果
实验结果表明，均匀分配终端奖励的假设直接导致了训练过程中响应长度的增加。与DeepSeek-R1论文中提出的增加响应长度归因于计算规模、自我反思、自我验证和回溯等观点不同，本文作者证明了响应延长的主因是均匀的信用分配。

### 💬 可借鉴之处
本文对LLM后训练中RL方法的结构假设进行了深入分析，揭示了这些假设可能导致RL方法在实际应用中的局限性。作者建议考虑替代的MDP公式化方法，并重新审视传统的监督微调方法，这些方法可能以更简单的方式达到相似甚至更好的效率和效果。此外，本文的研究对于理解和改进LLM的推理能力提供了新的视角和启示。

## cec-zero--chinese-error-correction-solution-based-on-llm
### Abstract
Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.
### 🌟 论文解读 | "CEC-Zero：开启中文文本纠错新篇章"

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）的飞速发展，其在中文文本处理方面的能力日益凸显。然而，尽管LLM在中文文本纠错方面表现出色，依然存在可靠性和泛化能力方面的挑战。为此，本文提出了CEC-Zero，一个基于LLM和强化学习（RL）的框架，旨在通过模型自我生成的数据，无需外部监督或辅助模型，自主学习纠错策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出的CEC-Zero框架，通过结合LLM的生成能力和RL的自主学习能力，实现了无需标注数据的自我纠错。这种方法有效地解决了传统方法依赖大量标注数据和辅助模型的问题。

💡 创新点2
CEC-Zero框架通过自我生成的数据，使LLM能够在没有真实标签的情况下，通过重复采样策略和基于多数投票机制的伪奖励信号，进行稳定的强化学习。这种方法特别适用于文本纠错这类答案不唯一的场景。

### 📈 实验结果
本文在多个中文文本纠错数据集上进行了广泛实验，结果表明，经过RL增强的LLM在准确性和跨领域泛化能力上均取得了工业级可用的水平，为中文NLP应用中的可靠性优化提供了可扩展的解决方案。

### 💬 可借鉴之处
CEC-Zero框架为中文文本纠错领域提供了新的视角和方法，其基于LLM和RL的结合，为模型的自我学习和优化提供了新的途径。此外，本文的方法也为其他自然语言处理任务中的模型泛化能力提升提供了启示。

## disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization
### Abstract
The recent success and openness of DeepSeek-R1 have brought widespread
attention to Group Relative Policy Optimization (GRPO) as a reinforcement
learning method for large reasoning models (LRMs). In this work, we analyze the
GRPO objective under a binary reward setting and reveal an inherent limitation
of question-level difficulty bias. We also identify a connection between GRPO
and traditional discriminative methods in supervised learning. Motivated by
these insights, we introduce a new Discriminative Constrained Optimization
(DisCO) framework for reinforcing LRMs, grounded in the principle of
discriminative learning. The main differences between DisCO and GRPO and its
recent variants are: (1) it replaces the group relative objective with a
discriminative objective defined by a scoring function; (2) it abandons
clipping-based surrogates in favor of non-clipping RL surrogate objectives used
as scoring functions; (3) it employs a simple yet effective constrained
optimization approach to enforce the KL divergence constraint, ensuring stable
training. As a result, DisCO offers notable advantages over GRPO and its
variants: (i) it completely eliminates difficulty bias by adopting
discriminative objectives; (ii) it addresses the entropy instability in GRPO
and its variants through the use of non-clipping scoring functions and a
constrained optimization approach; (iii) it allows the incorporation of
advanced discriminative learning techniques to address data imbalance, where a
significant number of questions have more negative than positive generated
answers during training. Our experiments on enhancing the mathematical
reasoning capabilities of SFT-finetuned models show that DisCO significantly
outperforms GRPO and its improved variants such as DAPO, achieving average
gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B
model.
### 🌟 论文解读 | "DisCO：用判别式优化强化大型推理模型"

### 📌 背景痛点/本文动机
近年来，大型推理模型（LRMs）在数学和科学推理任务中取得了显著成功，特别是DeepSeek-R1模型的开放性引起了广泛关注。Group Relative Policy Optimization（GRPO）作为一种强化学习方法，在提升LRMs性能方面显示出潜力。然而，GRPO存在一些内在限制，如问题难度偏差和训练不稳定性。本文旨在设计一种新的优化框架，以克服GRPO的这些限制，并提高LRMs的性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文分析了GRPO在二元奖励设置下的目标函数，揭示了其问题难度偏差的根本原因，即其组相对优势函数导致对过于简单或过于困难的问题赋予不相称的权重。此外，本文还发现了GRPO与传统监督学习中的判别式方法之间的概念联系。

💡 创新点2
基于这些洞察，本文提出了一个新的判别式优化框架DisCO，用于强化LRMs。DisCO的主要特点包括：
- 使用判别式目标代替组相对目标，通过一个评分函数定义；
- 放弃基于剪辑的替代函数，转而使用非剪辑的强化学习替代目标作为评分函数；
- 采用简单有效的约束优化方法来强制执行KL散度约束，确保训练稳定性。

### 📈 实验结果
本文在增强SFT微调模型的数学推理能力方面进行了实验，结果显示DisCO在六个基准任务上显著优于GRPO及其改进版本DAPO，对于一个1.5B模型，平均增益超过GRPO的7%和DAPO的6%。

### 💬 可借鉴之处
- DisCO框架通过采用判别式学习原则，完全消除了难度偏差，并通过使用非剪辑评分函数和约束优化方法解决了GRPO及其变体的熵不稳定性问题。
- DisCO允许集成先进的判别式学习技术来处理生成回放中的数据不平衡问题，这在实际应用中具有很高的价值。
- 本文的实验和分析为强化LRMs提供了新的视角和方法，对于希望在数学推理等复杂任务中提升模型性能的研究者和工程师具有参考意义。

## ktae--a-model-free-algorithm-to-key-tokens-advantage-estimation-in-mathematical-reasoning
### Abstract
Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.
### 🌟 论文解读 | “KTAE：数学推理中的无模型关键令牌优势估计算法”

### 📌 背景痛点/本文动机
近年来，将强化学习与基于规则的奖励机制相结合，已经显著提高了大型语言模型在数学推理方面的能力，甚至在无需监督微调的情况下也能取得显著效果。然而，流行的强化学习算法如GRPO及其变体DAPO在计算优势时存在粒度粗糙的问题，它们计算的是回滚级别的优势，为序列中的每个令牌分配相同的值，无法捕捉到特定令牌的贡献，从而阻碍了有效的学习。针对这一局限性，本文提出了关键令牌优势估计（KTAE）算法，该算法在不引入额外模型的情况下，估计细粒度的令牌级优势。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
KTAE算法利用了采样回滚的正确性，并应用统计分析方法来量化序列中每个令牌对最终结果的重要性。通过构建列联表，并使用如Fisher精确检验和信息增益等统计方法，KTAE能够量化每个令牌与正确回滚之间的关联强度。

💡 创新点2
KTAE算法将量化后的令牌频率和对应回滚的奖励结合起来，进一步量化这种关联贡献的方向。最终，这些度量通过乘法等操作结合，为每个令牌产生一个“关键令牌值”。这种方法不仅提供了更细粒度的优势信息，而且由于其不引入额外的模型，因此具有更低的训练成本。

### 📈 实验结果
实验结果表明，使用GRPO+KTAE和DAPO+KTAE训练的模型在五个主要的数学推理基准测试中均优于基线方法。值得注意的是，这些模型在保持高准确性的同时，还能显著减少响应长度，从而提高推理效率。

### 💬 可借鉴之处
KTAE算法提供了以下可借鉴之处：
1. KTAE在不引入额外模型的情况下，提供了更细粒度的优势信息，降低了训练成本。
2. KTAE使用统计分析方法量化令牌之间的重要性差异，提供了较强的解释性。
3. KTAE的关键令牌值基于最终答案的正确性计算，并保留了原始的回滚级别优势，使其不易受到奖励黑客攻击。
4. KTAE帮助模型关注关键令牌，减少了对无关令牌的学习，从而有效减少了响应长度。

## s-grpo--early-exit-via-reinforcement-learning-in-reasoning-models
### Abstract
As Test-Time Scaling emerges as an active research focus in the large
language model community, advanced post-training methods increasingly emphasize
extending chain-of-thought (CoT) generation length, thereby enhancing reasoning
capabilities to approach Deepseek R1-like reasoning models. However, recent
studies reveal that reasoning models (even Qwen3) consistently exhibit
excessive thought redundancy in CoT generation. This overthinking issue arises
from the inherent limitations of conventional outcome-reward reinforcement
learning, which systematically overlooks the regulation of intermediate
reasoning processes. This paper introduces Serial-Group Decaying-Reward Policy
Optimization (S-GRPO), a novel reinforcement learning paradigm that enables
models to implicitly evaluate the sufficiency of intermediate reasoning steps,
thereby facilitating early exit in CoT generation. Unlike GRPO, which samples
multiple possible reasoning paths in parallel (parallel group), S-GRPO only
samples one reasoning path and serially selects multiple temporal positions
from the path to exit thinking and directly generate answers (serial group).
For correct answers within a serial group, rewards gradually decrease based on
the exit positions along the reasoning path from front to back. This design
encourages the model to produce more accurate and concise thoughts, while also
incentivizing early thinking termination when appropriate. Empirical
evaluations demonstrate that S-GRPO is compatible with state-of-the-art
reasoning models, including Qwen3 and Deepseek-distill. Across diverse
benchmarks such as GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond,
S-GRPO achieves a substantial reduction in sequence length (35.4% - 61.1%)
while simultaneously improving accuracy (absolute 0.72% - 6.08%).
### 🌟 论文解读 | S-GRPO：通过强化学习实现推理模型中的早期退出策略

### 📌 背景痛点/本文动机
随着测试时间扩展（Test-Time Scaling）成为大型语言模型社区的研究焦点，先进的训练后方法越来越注重延长链式思维（Chain-of-Thought, CoT）的生成长度，以提升推理能力，接近Deepseek R1等推理模型的水平。然而，最近的研究发现，即使是Qwen3这样的推理模型，在CoT生成过程中也普遍存在过度思考的问题，导致推理链过长，包含无关信息和多余的推理步骤。这一问题源于传统的结果奖励强化学习（如GRPO）的固有局限，它依赖于最终结果的奖励，忽略了中间推理过程的调节。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了序列组衰减奖励策略优化（Serial-Group Decaying-Reward Policy Optimization, S-GRPO），这是一种新的强化学习范式，使模型能够隐式评估中间推理步骤的充分性，从而在CoT生成过程中实现早期退出。与GRPO不同，GRPO在并行中采样多个可能的推理路径，而S-GRPO只采样一个推理路径，并从路径中选择多个时间位置来提前结束思考和直接生成答案。

💡 创新点2
S-GRPO通过在正确答案的早期退出位置赋予递减的奖励，鼓励模型在CoT生成的早期阶段就产生高质量的推理路径，并在达到充分性后立即退出。这种方法通过两个阶段的滚动过程保持了原始推理过程的完整性，确保模型现有的推理能力不受影响，适合作为训练后的最终阶段。

### 📈 实验结果
在GSM8K、AIME 2024、AMC 2023、MATH-500和GPQA Diamond等多个基准测试中，使用Qwen3和Deepseek系列推理模型进行的大量实验表明，S-GRPO实现了0.72%至6.08%的绝对准确度提升，同时平均减少了35.4%至61.1%的序列长度，证明了效率与准确度的协同提升。

### 💬 可借鉴之处
本文提出的S-GRPO方法为推理模型提供了一种新的强化学习策略，通过早期退出机制减少了过度思考的问题，同时提高了推理的效率和准确性。这种方法对于大型语言模型在推理任务中的应用具有很高的参考价值，尤其是在需要高效推理的场景中。此外，S-GRPO的训练框架的开源也为相关领域的研究者提供了便利。

## mimo--unlocking-the-reasoning-potential-of-language-model----from-pretraining-to-posttraining
### Abstract
We present MiMo-7B, a large language model born for reasoning tasks, with
optimization across both pre-training and post-training stages. During
pre-training, we enhance the data preprocessing pipeline and employ a
three-stage data mixing strategy to strengthen the base model's reasoning
potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional
Multi-Token Prediction objective for enhanced performance and accelerated
inference speed. During post-training, we curate a dataset of 130K verifiable
mathematics and programming problems for reinforcement learning, integrating a
test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and
employing strategic data resampling to stabilize training. Extensive
evaluations show that MiMo-7B-Base possesses exceptional reasoning potential,
outperforming even much larger 32B models. The final RL-tuned model,
MiMo-7B-RL, achieves superior performance on mathematics, code and general
reasoning tasks, surpassing the performance of OpenAI o1-mini. The model
checkpoints are available at https://github.com/xiaomimimo/MiMo.
### 🌟 论文解读 | 解锁语言模型的推理潜能：从预训练到后训练的全面优化

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在数学推理和代码生成等复杂任务中的表现日益突出，如何在小规模模型中同时提升数学和代码推理能力成为了一个挑战。本文旨在通过优化预训练和后训练策略，全面解锁语言模型的推理潜能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：优化预训练数据
- 对数据预处理流程进行优化，提高文本提取工具的质量，并应用多维数据过滤，增加预训练数据中的推理模式密度。
- 通过高级推理模型生成大量多样化的合成推理数据。
- 采用三阶段数据混合策略，以增强模型在不同任务和领域的推理潜能。

💡 创新点2：引入多标记预测目标
- 在预训练过程中引入多标记预测（Multiple-Token Prediction）作为额外训练目标，以提高模型性能并加速推理速度。

💡 创新点3：后训练策略
- 精心构建了13万个可验证的数学和编程问题作为强化学习（RL）训练数据，并采用基于规则的准确性奖励，避免奖励欺骗。
- 引入基于测试难度驱动的代码奖励，以解决困难代码问题的稀疏奖励问题。
- 实施数据重采样策略，以提高回放采样效率和稳定策略更新。

💡 创新点4：强化学习基础设施
- 开发了无缝回放引擎，加速RL训练和验证。
- 支持在vLLM中实现多标记预测，并增强RL系统中推理引擎的鲁棒性。

### 📈 实验结果
- MiMo-7B-Base在7B参数的开源模型中表现出色，尤其在通用知识和编码任务上。
- MiMo-7B-RL-Zero在数学和代码任务上的RL训练性能超过了32B基模型。
- MiMo-7B-RL在推理性能上表现出色，AIME 2025得分超过o1-mini 4.7分，在算法代码生成任务上也显著优于OpenAI o1-mini。

### 💬 可借鉴之处
本文提供了如何通过优化预训练和后训练策略来提升语言模型推理潜能的详细方法，包括数据预处理、模型架构设计、强化学习基础设施等方面的创新思路，对于未来语言模型的研究和开发具有很高的参考价值。同时，开源的MiMo-7B系列模型和代码也为社区提供了宝贵的资源。

## mind-the-gap--bridging-thought-leap-for-improved-chain-of-thought-tuning
### Abstract
Large language models (LLMs) have achieved remarkable progress on
mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.
### 🌟 论文解读 | “填补思维跳跃空白：提升链式思维调优的效能”

### 📌 背景痛点/本文动机
大型语言模型（LLM）通过链式思维（CoT）推理在数学任务上取得了显著进展。然而，现有的数学CoT数据集常常因为专家省略中间步骤而存在思维跳跃（Thought Leap），这会对模型的学习和泛化能力产生负面影响。本文旨在解决这一问题，提出了一种自动检测思维跳跃并生成缺失中间推理步骤的方法，以恢复CoT的完整性和连贯性。

### 🚀 核心方法
💡 创新点1
本文首次系统性地识别并形式化了CoT推理中的思维跳跃现象，并引入了CoT思维跳跃桥接任务（CoT Thought Leap Bridge Task），以及一个用于解决这一问题的评估框架。

💡 创新点2
构建了一个专门的训练数据集ScaleQM+，基于结构化的ScaleQuestMath数据集，通过系统地移除中间步骤，并将不完整的推理链与它们的完整对应物配对。然后，开发了一个名为CoT-Bridge的模型，基于Qwen2.5-Math-7B，专门设计用于识别和桥接数学推理中的思维跳跃。

### 📈 实验结果
通过在数学推理基准上进行综合实验，本文证明了在桥接数据集上微调的模型始终优于在原始数据集上训练的模型，NuminaMath上的改进达到了+5.87%。此外，该方法还能有效提升精炼数据（+3.02%）并为强化学习提供更好的起始点（+3.1%）。CoT-Bridge还在域外逻辑推理任务上展示了改进的泛化能力，确认了提升推理完整性带来的广泛适用性。

### 💬 可借鉴之处
本文的方法可以作为即插即用的增强模块，与现有的优化技术兼容，如知识蒸馏和强化学习，进一步放大模型性能。此外，该方法不仅适用于数学推理，还可以推广到其他需要链式思维推理的领域，为提升LLM的推理能力提供了新的视角和工具。

## adastar--adaptive-data-sampling-for-training-self-taught-reasoners
### Abstract
Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.
### 🌟 论文解读 | AdaSTaR：自适应数据采样提升自教式推理模型训练效率

### 📌 背景痛点/本文动机
自教式推理模型（Self-Taught Reasoners，简称STaR）是自我提升语言模型训练流程中的关键部分。这种模型通过自我生成的推理步骤（ Chains-of-Thought，简称CoT）进行迭代改进。然而，传统的STaR框架在数据采样方面存在效率低下的问题，随机采样容易导致训练数据的不平衡，模型在已解决示例上过度训练，而在更具挑战性的示例上训练不足。本文旨在解决这一问题，提出了一种自适应数据采样方法，以提高自教式推理模型的训练效率和效果。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：自适应采样多样性
本文提出的AdaSTaR算法通过优先考虑未充分训练的示例，确保训练的平衡性。这种方法有助于避免模型在已解决示例上的过度训练，同时确保在更具挑战性的示例上获得足够的训练。

💡 创新点2：自适应采样课程
AdaSTaR算法还通过动态调整数据难度，以匹配模型不断变化的性能，从而实现课程式的自适应采样。在模型较弱时，优先采样较容易的数据，随着模型性能的提升，逐渐增加难度较高的数据。

### 📈 实验结果
在六个推理数据集上的实验表明，AdaSTaR算法在所有实例中均取得了最佳测试准确率（6/6），并且平均减少了58.6%的训练浮点运算（FLOPs），相比其他基线方法具有显著性能和效率提升。这些改进在不同预训练语言模型和更大规模模型上同样有效。

### 💬 可借鉴之处
本文提出的自适应数据采样方法为提升自教式推理模型的训练效率和效果提供了新的思路。AdaSTaR算法通过平衡学习多样性和课程难度，有效解决了传统STaR框架中的训练数据不平衡问题。此外，该方法在不同规模和类型的语言模型上均具有普遍适用性，为未来自教式推理模型的研究和开发提供了有价值的参考。

## a-survey-on-test-time-scaling-in-large-language-models--what--how--where--and-how-well-
### Abstract
As enthusiasm for scaling computation (data and parameters) in the
pretraining era gradually diminished, test-time scaling (TTS), also referred to
as ``test-time computing'' has emerged as a prominent research focus. Recent
studies demonstrate that TTS can further elicit the problem-solving
capabilities of large language models (LLMs), enabling significant
breakthroughs not only in specialized reasoning tasks, such as mathematics and
coding, but also in general tasks like open-ended Q&A. However, despite the
explosion of recent efforts in this area, there remains an urgent need for a
comprehensive survey offering a systemic understanding. To fill this gap, we
propose a unified, multidimensional framework structured along four core
dimensions of TTS research: what to scale, how to scale, where to scale, and
how well to scale. Building upon this taxonomy, we conduct an extensive review
of methods, application scenarios, and assessment aspects, and present an
organized decomposition that highlights the unique functional roles of
individual techniques within the broader TTS landscape. From this analysis, we
distill the major developmental trajectories of TTS to date and offer hands-on
guidelines for practical deployment. Furthermore, we identify several open
challenges and offer insights into promising future directions, including
further scaling, clarifying the functional essence of techniques, generalizing
to more tasks, and more attributions. Our repository is available on
https://github.com/testtimescaling/testtimescaling.github.io/
### 🌟 论文解读 | 探索大型语言模型测试时扩展：全面梳理测试时扩展技术

### 📌 背景痛点/本文动机
随着预训练时代计算扩展的热情逐渐减弱，测试时扩展（Test-Time Scaling, TTS）作为一种新的研究焦点日益受到关注。近期研究表明，TTS能够进一步激发大型语言模型（LLMs）的问题解决能力，不仅在数学和编码等专门推理任务上取得显著突破，也在开放性问题回答等通用任务上表现出色。然而，尽管在这一领域的研究成果层出不穷，但仍然迫切需要一份全面的综述，以提供系统性的理解。本文旨在填补这一空白，提出了一种统一的多维度框架，对TTS研究进行了全面的回顾和分析。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一个统一的多维度框架，将TTS研究分为四个核心维度：扩展什么（What to scale）、如何扩展（How to scale）、在哪里扩展（Where to scale）以及扩展效果如何（How well to scale）。这一框架为分类、比较和扩展TTS方法提供了结构化的支持。

💡 创新点2
基于这一框架，本文对TTS领域的方法、应用场景和评估方面进行了广泛的回顾，并呈现了一个有组织的分解，突出了各种技术在TTS景观中的独特功能角色。此外，本文还提炼了TTS发展的主要轨迹，并提供了实际部署的实用指南。

### 📈 实验结果
本文没有具体介绍实验结果，而是提供了一个全面的文献综述和框架，帮助研究人员和实践者理解TTS的各种方法和评估方面。

### 💬 可借鉴之处
本文的可借鉴之处在于：
- 提供了一个全面的多维度框架，有助于研究人员系统地理解和分析TTS的各种方法。
- 通过对现有研究的组织和分析，为未来TTS技术的发展指明了方向，包括进一步扩展、明确技术的功能本质、推广到更多任务以及优化效率等方面。
- 识别了TTS领域面临的挑战，并提出了有前景的研究方向，有助于推动该领域的持续发展。

## arpo-end-to-end-policy-optimization-for-gui-agents-with-experience-replay
### Abstract
Training large language models (LLMs) as interactive agents for controlling
graphical user interfaces (GUIs) presents a unique challenge to optimize
long-horizon action sequences with multimodal feedback from complex
environments. While recent works have advanced multi-turn reinforcement
learning (RL) for reasoning and tool-using capabilities in LLMs, their
application to GUI-based agents remains relatively underexplored due to the
difficulty of sparse rewards, delayed feedback, and high rollout costs. In this
paper, we investigate end-to-end policy optimization for vision-language-based
GUI agents with the aim of improving performance on complex, long-horizon
computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an
end-to-end RL approach that augments Group Relative Policy Optimization (GRPO)
with a replay buffer to reuse the successful experience across training
iterations. To further stabilize the training process, we propose a task
selection strategy that filters tasks based on baseline agent performance,
allowing the agent to focus on learning from informative interactions.
Additionally, we compare ARPO with offline preference optimization approaches,
highlighting the advantages of policy-based methods in GUI environments.
Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive
results, establishing a new performance baseline for LLM-based GUI agents
trained via reinforcement learning. Our findings underscore the effectiveness
of reinforcement learning for training multi-turn, vision-language GUI agents
capable of managing complex real-world UI interactions. Codes and
models:https://github.com/dvlab-research/ARPO.git.
### 🌟 论文解读 | ARPO：面向图形界面代理的端到端策略优化与经验回放

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在交互式代理中的应用，如何优化控制图形用户界面（GUI）的长周期动作序列成为了一个挑战。现有的研究在多轮推理和工具使用方面取得了进展，但在GUI代理的应用上仍然探索不足，主要因为GUI环境中的稀疏奖励、延迟反馈和高成本回放等问题。本文旨在通过端到端的策略优化方法，提升基于视觉-语言模型的GUI代理在复杂、长周期计算机任务上的性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出了代理重放策略优化（ARPO）方法
本文提出了一种端到端的强化学习（RL）方法，即ARPO，该方法在Group Relative Policy Optimization（GRPO）的基础上增加了经验回放功能，以重用训练迭代中的成功经验，从而提高样本效率和训练稳定性。

💡 创新点2：任务选择策略和经验回放缓冲区
为了进一步稳定训练过程，本文提出了一种基于基线代理性能过滤任务的任务选择策略，使代理能够专注于从信息丰富的交互中学习。同时，引入了一个经验回放缓冲区，专门用于存储成功的轨迹，以增强在稀疏奖励设置中的样本效率和训练稳定性。

### 📈 实验结果
在OSWorld基准测试中，ARPO方法取得了具有竞争力的结果，为基于LLM的GUI代理通过强化学习训练设定了新的性能基准。实验表明，强化学习在训练多轮、视觉-语言GUI代理方面是有效的，这些代理能够管理复杂的现实世界UI交互。

### 💬 可借鉴之处
本文的研究为GUI代理的端到端策略优化提供了新的视角和方法，特别是在以下方面值得借鉴：
- 如何在GUI环境中有效地利用经验回放来提高训练效率和稳定性。
- 任务选择策略在维持奖励多样性和优化策略学习中的重要性。
- 强化学习在提升代理在特定领域任务性能方面的潜力，以及对跨领域任务性能的适度提升。

## using-reinforcement-learning-to-train-large-language-models-to-explain-human-decisions
### Abstract
A central goal of cognitive modeling is to develop models that not only
predict human behavior but also provide insight into the underlying cognitive
mechanisms. While neural network models trained on large-scale behavioral data
often achieve strong predictive performance, they typically fall short in
offering interpretable explanations of the cognitive processes they capture. In
this work, we explore the potential of pretrained large language models (LLMs)
to serve as dual-purpose cognitive models--capable of both accurate prediction
and interpretable explanation in natural language. Specifically, we employ
reinforcement learning with outcome-based rewards to guide LLMs toward
generating explicit reasoning traces for explaining human risky choices. Our
findings demonstrate that this approach produces high-quality explanations
alongside strong quantitative predictions of human decisions.
### 🌟 论文解读 | 利用强化学习训练大型语言模型解释人类决策

### 📌 背景痛点/本文动机
认知建模的核心目标是开发不仅能够预测人类行为，还能提供对底层认知机制洞察的模型。虽然基于大规模行为数据的神经网络模型通常能够实现强大的预测性能，但它们往往在提供可解释性的认知过程解释方面存在不足。本文探讨了预训练的大型语言模型（LLM）作为双重用途认知模型的潜力——既能实现准确的预测，也能以自然语言提供可解释的解释。具体而言，本文采用基于结果的强化学习来引导LLM生成解释人类风险选择的明确推理轨迹。

### 🚀 核心方法
💡 创新点1
本文提出了一种新的方法，即使用基于结果的强化学习来训练LLM，使其在生成人类风险选择预测的同时，也能生成明确的推理轨迹。这种方法将LLM的链式思维（CoT）作为底层认知机制的口头描述，以便认知科学家能够分析这些CoT，判断它们是否为观察数据提供了有意义且可解释的解释。

💡 创新点2
本文对比了三种不同的LLM后训练策略：监督微调（SFT）、针对认知任务设计的Centaur风格SFT，以及基于群体相对策略优化的强化学习（GRPO）。所有方法都应用于在最大可用的人类风险选择数据集choices13k上进行微调，以评估它们在生成有用认知模型方面的效果。

### 📈 实验结果
实验结果表明，基于强化学习的后训练方法能够从人类行为数据中引导出合理的CoT推理轨迹，并且其预测准确性可以与基于SFT的方法相媲美。此外，生成的CoT能够根据训练数据的结构进行适应，当人类行为数据被合成数据替代时，CoT也会相应地调整以反映合成数据集的结构。同时，CoT的质量依赖于基础LLM的强度，使用较弱的模型会导致推理质量的明显下降。

### 💬 可借鉴之处
本文的研究成果为认知建模领域提供了新的视角，即利用LLM的推理生成能力来创建既能够预测人类行为，又能够提供认知机制解释的模型。这种方法不仅有助于提高预测的准确性，还能够为理解人类决策过程提供更深入的洞察。此外，本文的方法也为自动发现认知模型的研究提供了新的思路，值得进一步探索和应用。

## uft--unifying-supervised-and-reinforcement-fine-tuning
### Abstract
Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.
### 🌟 论文解读 | 统一微调：融合监督与强化学习的突破

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）的推理能力提升，后训练方法在增强这些模型的能力方面显示出其重要性。目前主要的后训练方法分为监督微调（SFT）和强化微调（RFT）。SFT方法高效且适合小型语言模型，但可能导致过拟合并限制大型模型的推理能力。而RFT通常能获得更好的泛化能力，但它严重依赖于基础模型的强度。为了克服SFT和RFT的限制，本文提出了统一微调（UFT）方法，这是一种将SFT和RFT融合为一体的新型后训练范式。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
UFT方法通过将监督信号与奖励信号相结合，使模型能够在探索解决方案的同时，利用信息丰富的监督信号，从而弥补了现有方法中记忆与思考之间的鸿沟。

💡 创新点2
本文理论证明了UFT打破了RFT内在的指数样本复杂度瓶颈，显示出统一训练可以在长距离推理任务上指数级加速收敛。

### 📈 实验结果
在Countdown、MATH和Knights and Knaves逻辑谜题等多个任务上，UFT方法均优于SFT和RFT，无论模型大小如何。实验结果还表明，UFT在样本复杂度上具有显著优势，能够有效提高长距离推理任务的性能。

### 💬 可借鉴之处
本文提出的UFT方法为后训练大型语言模型提供了一种新的思路，通过融合监督和强化学习，不仅提高了模型的泛化能力，还减少了样本复杂度。这种方法对于理解和优化大型语言模型的推理能力具有重要的理论和实践价值。此外，UFT方法在多个任务和模型规模上的表现一致性，也证明了其稳健性和广泛的适用性。

## vl-rethinker--incentivizing-self-reflection-of-vision-language-models-with-reinforcement-learning
### Abstract
Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated
great potential in solving challenging problems through explicit reflection.
They significantly outperform the best fast-thinking models, such as GPT-4o, on
various math and science benchmarks. However, their multimodal reasoning
capabilities remain on par with fast-thinking models. For instance, GPT-o1's
performance on benchmarks like MathVista, MathVerse, and MathVision is similar
to fast-thinking models. In this paper, we aim to enhance the slow-thinking
capabilities of vision-language models using reinforcement learning (without
relying on distillation) to advance the state of the art. First, we adapt the
GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to
address the vanishing advantages problem. While this approach yields strong
performance, the resulting RL-trained models exhibit limited self-reflection or
self-verification. To further encourage slow-thinking, we introduce Forced
Rethinking, which appends a rethinking trigger token to the end of rollouts in
RL training, explicitly enforcing a self-reflection reasoning step. By
combining these two techniques, our model, VL-Rethinker, advances
state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%
respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary
benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the
gap with OpenAI-o1. Our empirical results show the effectiveness of our
approaches.
### 🌟 论文解读 | “VL-Rethinker：通过强化学习激励视觉语言模型的自我反思”

### 📌 背景痛点/本文动机
近年来，像GPT-o1和DeepSeek-R1这样的慢思考系统通过明确的反思过程，在解决复杂问题方面显示出巨大潜力。它们在各种数学和科学基准测试中显著超越了最快的思考模型，如GPT-4o。然而，这些模型在多模态推理能力上与快思考模型相当，特别是在MathVista、MathVerse和MathVision等基准测试上的表现与快思考模型相似。本文旨在通过强化学习（不依赖模型蒸馏）来提升视觉语言模型的慢思考能力，以推动该领域的发展。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首先对GRPO算法进行了改进，引入了选择性样本重放（SSR）技术，以解决梯度消失问题。SSR通过从过去迭代中抽取高价值经验，增强当前训练批次，从而提供更一致的梯度信号，并动态调整训练焦点，接近模型的决策边界。

💡 创新点2
为了进一步激励慢思考，本文提出了强制反思策略，即在RL训练中在输出响应的末尾添加一个反思触发令牌，明确强制执行自我反思推理步骤。这种方法促使模型在生成最终答案之前进行自我反思和自我验证。

### 📈 实验结果
通过结合这两种技术，本文提出的VL-Rethinker模型在MathVista和MathVerse基准测试上取得了80.4%和63.5%的最新成绩。此外，VL-Rethinker在MathVision、MMMU-Pro、EMMA和MEGA-Bench等多学科基准测试上达到了开源最新水平，与OpenAI-o1的差距显著缩小。

### 💬 可借鉴之处
本文提出了一种简单直接的RL方法来增强VLM的推理能力，为复杂的监督微调和蒸馏管道提供了一个可行的替代方案。同时，通过引入SSR和强制反思策略，本文为激励视觉语言模型中的自我反思提供了新的思路和方法。这些成果对于提升视觉语言模型的多模态推理能力具有重要意义，值得进一步研究和应用。

## a-new-dapo-algorithm-for-stock-trading
### Abstract
Recent advances in reinforcement learning, such as Dynamic Sampling Policy
Optimization (DAPO), show strong performance when paired with large language
models (LLMs). Motivated by this success, we ask whether similar gains can be
realized in financial trading. We design a trading agent that combines an
improved Group Relative Policy Optimization (GRPO) algorithm, augmented with
ideas from DAPO, with LLM-based risk and sentiment signals extracted from
financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a
cumulative return of 230.49 percent and an information ratio of 0.37,
outperforming the CPPO-DeepSeek baseline. It also cuts training time from about
8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The
proposed RL-LLM framework offers a scalable path toward data-efficient trading
agents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/
### 🌟 论文解读 | “DAPO算法革新：AI交易员在股市的新策略”

### 📌 背景痛点/本文动机
随着强化学习在算法交易中的广泛应用，如何自动化地在不确定性下做出投资组合决策成为研究的热点。然而，现有的方法仍然面临着诸如大幅回撤和解释性不足等问题。本文旨在探索结合大型语言模型（LLM）的强化学习算法，以实现更高效、更稳健的金融交易策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文采用了一种改进的群相对策略优化（GRPO）算法，该算法通过从每个状态中采样多个动作并标准化奖励，从而消除了对价值函数的需求，降低了内存使用。

💡 创新点2
本文引入了动态采样策略优化（DAPO）算法的元素，原本用于大型语言模型的偏好调整。通过使用解耦剪辑和动态采样，算法能够在高回报场景中增强探索，同时限制风险。

💡 创新点3
本文提出了一种可调整的奖励公式，允许更细致的控制，可以根据需要强调情绪或风险。

### 📈 实验结果
在NASDAQ-100指数的FNSPID数据集上，本文提出的改进DAPO算法实现了230.49%的累积回报和0.37的信息比率，超过了CPPO-DeepSeek基线。同时，该算法将训练时间从大约8小时减少到2.5小时，并显著降低了内存使用。

### 💬 可借鉴之处
本文的方法为构建数据高效的金融交易代理提供了一个可扩展的框架，同时展示了结合LLM的情绪和风险信号在金融交易中的潜力。研究中的可调整奖励公式和动态采样策略为提高交易策略的灵活性和效率提供了新的思路。

## a-sober-look-at-progress-in-language-model-reasoning--pitfalls-and-paths-to-reproducibility
### Abstract
Reasoning has emerged as the next major frontier for language models (LMs),
with rapid advances from both academic and industrial labs. However, this
progress often outpaces methodological rigor, with many evaluations relying on
benchmarking practices that lack transparency, robustness, or statistical
grounding. In this work, we conduct a comprehensive empirical study and find
that current mathematical reasoning benchmarks are highly sensitive to subtle
implementation choices - including decoding parameters, random seeds, prompt
formatting, and even hardware and software-framework configurations.
Performance gains reported in recent studies frequently hinge on unclear
comparisons or unreported sources of variance. To address these issues, we
propose a standardized evaluation framework with clearly defined best practices
and reporting standards. Using this framework, we reassess recent methods and
find that reinforcement learning (RL) approaches yield only modest improvements
- far below prior claims - and are prone to overfitting, especially on
small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)
methods show consistently stronger generalization. To foster reproducibility,
we release all code, prompts, and model outputs, for reasoning benchmarks,
establishing more rigorous foundations for future work.
### 🌟 论文解读 | 深度语言模型推理进展的冷思考：误区与可复现性路径

### 📌 背景痛点/本文动机
随着深度语言模型（LLMs）在推理能力上的快速进步，学术界和工业界都在积极推动这一领域的发展。然而，这种进展往往伴随着方法论上的不够严谨，许多评估依赖于缺乏透明度、鲁棒性或统计基础的基准测试实践。本文针对数学推理基准测试中的这些问题，进行了一项全面的实证研究，发现当前数学推理基准测试对微妙的实现选择高度敏感，包括解码参数、随机种子、提示格式，甚至硬件和软件框架配置。这导致了许多研究中报告的性能提升建立在不清晰的比较或未报告的方差来源上。

### 🚀 核心方法
💡 创新点1
本文提出了一套标准化的评估框架，其中包含了明确定义的最佳实践和报告标准。通过这个框架，作者重新评估了最近的方法，并发现强化学习（RL）方法仅带来微小的改进，远低于之前的宣称，并且容易在小规模基准测试上过拟合。

💡 创新点2
作者对比了监督微调（SFT）方法和RL方法，发现SFT方法在推理能力上表现更为稳定和普遍。为了促进可复现性，本文公开了所有代码、提示和模型输出，为未来的研究奠定了更严谨的基础。

### 📈 实验结果
通过对最近的方法进行标准化评估，本文发现RL方法在数学推理任务上的性能提升并不显著，而且容易过拟合。相比之下，SFT方法在各个基准测试上都表现出了更强的泛化能力。此外，本文还发现，由于样本量小，基准测试的性能指标非常不稳定，一个问题的变化就可能导致性能指标出现超过3个百分点的波动。

### 💬 可借鉴之处
本文的研究提醒了我们在评估深度语言模型推理能力时需要注意的几个关键点：
- 评估实践需要更加透明、鲁棒和有统计基础。
- 强化学习方法的性能提升可能被高估，需要谨慎对待。
- 监督微调方法在推理任务上更为可靠。
- 为了提高研究的可复现性，应该公开所有相关代码、数据和模型输出。
这些发现对于未来语言模型推理能力的研究具有重要的指导意义。

## sailing-ai-by-the-stars--a-survey-of-learning-from-rewards-in-post-training-and-test-time-scaling-of-large-language-models
### Abstract
Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities. In this survey,
we present a comprehensive overview of the paradigm of learning from rewards.
We categorize and analyze the strategies under this paradigm across training,
inference, and post-inference stages. We further discuss the benchmarks for
reward models and the primary applications. Finally we highlight the challenges
and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.
### 🌟 论文解读 | "引领大型语言模型进化的星辰大海：奖励学习全面解析"

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLM）如Chat-GPT、Claude和Llama等取得了飞速发展，这些模型通过预训练规模化（pre-training scaling）在大量语料库上进行训练，实现了广泛的语言和知识表示。然而，这种方法存在一些根本性限制，包括与人类价值观的错位、难以适应不同任务目标以及深度推理能力的不足。为了克服这些限制，研究转向了后训练规模化（post-training scaling）和测试时规模化（test-time scaling），以进一步优化LLM。本文综述了奖励学习（Learning from Rewards）这一关键统一范式，它在后训练和测试时规模化中扮演了核心角色，通过奖励信号来引导模型行为。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种统一的概念框架，用于理解奖励学习系统的关键组件和交互。该框架涵盖了语言模型、奖励模型和学习策略三个核心部分，为理解和分类现有方法提供了一个全面的视角。

💡 创新点2
文章详细分类了现有方法在奖励来源、奖励模型设计、奖励学习时机和学习策略方面的变化。奖励来源包括人类反馈和自动化反馈，奖励模型设计涉及模型架构、奖励格式、评分模式和奖励粒度等多个维度。

### 📈 实验结果
本文综述了在不同阶段（训练、推理和后推理）使用奖励学习的代表性技术，并总结了最近奖励模型基准测试的结果。这些技术和基准测试展示了奖励学习在数学推理、代码生成、多模态、代理和具身AI等多个应用领域的潜力。

### 💬 可借鉴之处
本文提供了一个全面的奖励学习综述，对于理解大型语言模型的后训练和测试时规模化具有重要意义。以下是一些可借鉴之处：
- 奖励学习范式为LLM提供了从静态数据被动学习到动态反馈主动学习的转变，使其具备了对齐的偏好和深度推理能力。
- 统一的概念框架有助于更好地理解奖励学习系统的设计和运作。
- 对奖励模型设计的分类和讨论为未来研究和应用提供了清晰的指导。
- 识别了奖励学习面临的挑战和未来研究方向，为相关领域的研究者提供了宝贵的参考。

## heimdall--test-time-scaling-on-the-generative-verification
### Abstract
An AI system can create and maintain knowledge only to the extent that it can
verify that knowledge itself. Recent work on long Chain-of-Thought reasoning
has demonstrated great potential of LLMs on solving competitive problems, but
their verification ability remains to be weak and not sufficiently
investigated. In this paper, we propose Heimdall, the long CoT verification LLM
that can accurately judge the correctness of solutions. With pure reinforcement
learning, we boost the verification accuracy from 62.5% to 94.5% on competitive
math problems. By scaling with repeated sampling, the accuracy further
increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive
generalization capabilities, successfully detecting most issues in challenging
math proofs, the type of which is not included during training. Furthermore, we
propose Pessimistic Verification to extend the functionality of Heimdall to
scaling up the problem solving. It calls Heimdall to judge the solutions from a
solver model and based on the pessimistic principle, selects the most likely
correct solution with the least uncertainty. Taking
DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification
improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute
budget and to 83.3% with more compute budget. With the stronger solver Gemini
2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge
discovery system, a ternary system where one poses questions, another provides
solutions, and the third verifies the solutions. Using the data synthesis work
NuminaMath for the first two components, Heimdall effectively identifies
problematic records within the dataset and reveals that nearly half of the data
is flawed, which interestingly aligns with the recent ablation studies from
NuminaMath.
### 🌟 论文解读 | “Heimdall：利用长链推理提升生成验证的准确性”

### 📌 背景痛点/本文动机
在人工智能领域，知识的创建与维护依赖于系统的自我验证能力。尽管最近的长链推理（CoT）研究展示了大型语言模型（LLM）在解决复杂问题上的巨大潜力，但这些模型的验证能力仍然较弱，且未得到充分研究。本文旨在解决这一问题，提出了一种名为Heimdall的长链推理验证LLM，能够准确判断解决方案的正确性。

### 🚀 核心方法
💡 创新点1
本文通过纯强化学习训练了Heimdall模型，显著提升了验证准确性。在竞争性数学问题上，验证准确性从62.5%提高到了94.5%。此外，通过重复采样和多数投票，准确性进一步增加到了97.5%。

💡 创新点2
本文提出了悲观验证（Pessimistic Verification）算法，该算法通过调用Heimdall对求解器模型的解决方案进行判断，并根据悲观原则选择最可能正确的解决方案，以最小化选择错误解决方案的不确定性。使用DeepSeek-R1-Distill-Qwen-32B作为求解器模型，悲观验证将AIME2025的解决方案准确性从54.2%提高到了70.0%，在更多计算资源下提高到了83.3%。使用更强大的求解器Gemini 2.5 Pro，准确率达到了93.0%。

### 📈 实验结果
实验结果表明，Heimdall不仅在训练数据集上表现出色，而且在未包含在训练中的挑战性数学证明问题上也展现了令人印象深刻的泛化能力。此外，通过与NuminaMath数据合成工作的结合，Heimdall有效地识别了数据集中的问题记录，揭示了近一半的数据存在缺陷，这与NuminaMath最近的消融研究相符。

### 💬 可借鉴之处
本文的研究为以下方面提供了可借鉴的经验：
- 通过强化学习训练长链推理验证模型，提高了LLM的验证能力。
- 利用悲观验证算法，优化了解决方案的选择过程，提高了问题解决的整体准确性。
- 在自动知识发现系统中，Heimdall的应用展示了其在识别合成数据集中的错误方面的潜力，为数据质量评估提供了新的视角。

## mesh-rft--enhancing-mesh-generation-via-fine-grained-reinforcement-fine-tuning
### Abstract
Existing pretrained models for 3D mesh generation often suffer from data
biases and produce low-quality results, while global reinforcement learning
(RL) methods rely on object-level rewards that struggle to capture local
structure details. To address these challenges, we present \textbf{Mesh-RFT}, a
novel fine-grained reinforcement fine-tuning framework that employs Masked
Direct Preference Optimization (M-DPO) to enable localized refinement via
quality-aware face masking. To facilitate efficient quality evaluation, we
introduce an objective topology-aware scoring system to evaluate geometric
integrity and topological regularity at both object and face levels through two
metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating
these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first
method to optimize mesh quality at the granularity of individual faces,
resolving localized errors while preserving global coherence. Experiment
results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\%
and improves Topology Score (TS) by 3.8\% over pre-trained models, while
outperforming global DPO methods with a 17.4\% HD reduction and 4.9\% TS gain.
These results demonstrate Mesh-RFT's ability to improve geometric integrity and
topological regularity, achieving new state-of-the-art performance in
production-ready mesh generation. Project Page:
\href{https://hitcslj.github.io/mesh-rft/}{this https URL}.
### 🌟 论文解读 | 精细化强化学习提升网格生成质量：Mesh-RFT框架

### 📌 背景痛点/本文动机
现有的3D网格生成模型往往存在数据偏差问题，生成的网格质量较低。而全局强化学习方法依赖于对象级别的奖励信号，难以捕捉到局部结构细节。为了解决这些挑战，本文提出了Mesh-RFT，一个新颖的细粒度强化微调框架，通过质量感知的面遮蔽和Masked Direct Preference Optimization (M-DPO)技术，实现对网格质量的局部化优化。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
引入了客观的拓扑感知评分系统，通过边界边比（BER）和拓扑得分（TS）两个指标，客观评估网格的几何完整性和拓扑规则性，消除了对人工注释的依赖。

💡 创新点2
采用M-DPO和质量感知遮蔽的局部优化机制，专门针对有缺陷的区域进行几何和拓扑优化，解决了全局奖励信号粗糙监督的问题。

### 📈 实验结果
实验结果显示，本文的M-DPO方法相比预训练模型，Hausdorff距离（HD）减少了24.6%，拓扑得分（TS）提高了3.8%。而与全局DPO方法相比，Hausdorff距离减少了17.4%，拓扑得分提高了4.9%。这些结果证明了Mesh-RFT在提高几何完整性和拓扑规则性方面的能力，实现了生成就绪网格的新一代最佳性能。

### 💬 可借鉴之处
本文提出的Mesh-RFT框架为3D网格生成提供了一种新的细粒度优化方法，不仅提高了网格的质量，还减少了人工干预的需求。其创新的拓扑感知评分系统和局部优化机制对于其他3D生成任务也具有借鉴意义，有望推动相关领域的研究和应用发展。

## totrl--unlock-llm-tree-of-thoughts-reasoning-potential-through-puzzles-solving
### Abstract
Large language models (LLMs) demonstrate significant reasoning capabilities,
particularly through long chain-of-thought (CoT) processes, which can be
elicited by reinforcement learning (RL). However, prolonged CoT reasoning
presents limitations, primarily verbose outputs due to excessive introspection.
The reasoning process in these LLMs often appears to follow a trial-and-error
methodology rather than a systematic, logical deduction. In contrast,
tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling
reasoning as an exploration within a tree structure. This reasoning structure
facilitates the parallel generation and evaluation of multiple reasoning
branches, allowing for the active identification, assessment, and pruning of
unproductive paths. This process can potentially lead to improved performance
and reduced token costs. Building upon the long CoT capability of LLMs, we
introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a
rule-based reward. ToTRL is designed to guide LLMs in developing the parallel
ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs
as players in a puzzle game during the ToTRL training process. Solving puzzle
games inherently necessitates exploring interdependent choices and managing
multiple constraints, which requires the construction and exploration of a
thought tree, providing challenging tasks for cultivating the ToT reasoning
capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,
trained with our ToTRL, achieves significant improvement in performance and
reasoning efficiency on complex reasoning tasks.
### 🌟 论文解读 | 解锁大规模语言模型思维树推理潜力：通过解谜游戏提升推理能力

### 📌 背景痛点/本文动机
大规模语言模型（LLM）在推理能力方面取得了显著进展，尤其是通过长链式思维（CoT）过程。然而，这种长时间的CoT推理存在局限性，主要表现为由于过度内省导致的冗长输出。此外，LLM的推理过程往往遵循试错方法，而非系统性的逻辑演绎。为了克服这些限制，本文提出了基于思维树（ToT）的推理方法，该方法通过在树结构中探索潜在思维或状态，提高了推理效率和性能。

### 🚀 核心方法
💡 创新点1
本文引入了树状思维强化学习（ToTRL），一种新颖的on-policy RL框架，使用基于规则的奖励机制。ToTRL旨在指导LLM基于顺序CoT策略发展并行的ToT策略。

💡 创新点2
为了培养LLM的ToT推理能力，本文将LLM作为解谜游戏中的玩家，在训练过程中解决谜题。解谜游戏需要探索相互依赖的选择和管理多个约束，这为培养ToT推理能力提供了挑战性任务。

### 📈 实验结果
通过实验评估，本文训练的ToTQwen3-8B模型在复杂推理任务上表现出显著的性能和推理效率提升。

### 💬 可借鉴之处
本文提出的ToTRL框架和训练策略为LLM提供了新的推理方法，有助于解决复杂推理任务。以下是一些可借鉴之处：
- 利用规则奖励的on-policy RL算法，有效指导LLM发展ToT推理策略。
- 通过解谜游戏训练LLM，培养其在复杂任务中的ToT推理能力。
- 实验证明了ToT推理在提高推理效率和减少冗余探索方面的潜力。

## scaling-reasoning--losing-control--evaluating-instruction-following-in-large-reasoning-models
### Abstract
Instruction-following is essential for aligning large language models (LLMs)
with user intent. While recent reasoning-oriented models exhibit impressive
performance on complex mathematical problems, their ability to adhere to
natural language instructions remains underexplored. In this work, we introduce
MathIF, a dedicated benchmark for evaluating instruction-following in
mathematical reasoning tasks. Our empirical analysis reveals a consistent
tension between scaling up reasoning capacity and maintaining controllability,
as models that reason more effectively often struggle to comply with user
directives. We find that models tuned on distilled long chains-of-thought or
trained with reasoning-oriented reinforcement learning often degrade in
instruction adherence, especially when generation length increases.
Furthermore, we show that even simple interventions can partially recover
obedience, though at the cost of reasoning performance. These findings
highlight a fundamental tension in current LLM training paradigms and motivate
the need for more instruction-aware reasoning models. We release the code and
data at https://github.com/TingchenFu/MathIF.
### 🌟 论文解读 | 大模型推理能力提升，却丢了控制？探究大型推理模型中的指令遵循问题

### 📌 背景痛点/本文动机
随着大型推理模型（LRMs）在数学推理领域的突破性进展，它们在解决复杂数学问题上的能力得到了显著提升。然而，这些模型在遵循自然语言指令方面的表现却鲜少被研究。本文的动机在于探究随着推理能力的增强，模型是否变得更加智能但同时也更难以控制，以及这一现象背后的原因。

### 🚀 核心方法
💡 创新点1
本文引入了MathIF，这是一个专门为评估数学领域中大型推理模型的指令遵循能力而设计的基准。MathIF通过程序化组合15种Python可验证的约束条件，创建了30个双约束和15个三约束的提示，应用于不同难度的数学问题，形成了总共420个高质量的评估样本。

💡 创新点2
通过对23个不同规模和架构的LRMs进行评估，本文发现大多数模型在遵循指令方面存在普遍问题，且性能并不随模型规模的增大而一致提升。此外，本文还揭示了指令遵循和推理能力之间的相互干扰，这种干扰在训练和推理阶段都有所体现。

### 📈 实验结果
实验结果显示，即使是表现最好的模型Qwen3-14B，在严格的指令遵循方面的准确率也只有50.71%。随着任务难度和约束复杂性的增加，性能进一步下降，显示出有很大的改进空间。此外，本文还发现，常见的推理导向的训练策略（如监督微调和强化学习）虽然增强了推理能力，但会降低指令遵循度，尤其是当推理链长度增加时。

### 💬 可借鉴之处
本文的研究揭示了当前大型推理模型训练范式中的基本矛盾，即推理能力的提升往往以指令遵循度的降低为代价。这一发现对于未来大型模型的发展具有重要的指导意义，提示我们需要开发更加注重指令遵循的推理模型。此外，MathIF基准的提出为评估和改进大型推理模型的指令遵循能力提供了一个新的工具，对于相关领域的研究者具有很高的参考价值。

## sophiavl-r1--reinforcing-mllms-reasoning-with-thinking-reward
### Abstract
Recent advances have shown success in eliciting strong reasoning abilities in
multimodal large language models (MLLMs) through rule-based reinforcement
learning (RL) with outcome rewards. However, this paradigm typically lacks
supervision over the thinking process leading to the final outcome.As a result,
the model may learn sub-optimal reasoning strategies, which can hinder its
generalization ability. In light of this, we propose SophiaVL-R1, as an attempt
to add reward signals for the thinking process in this paradigm. To achieve
this, we first train a thinking reward model that evaluates the quality of the
entire thinking process. Given that the thinking reward may be unreliable for
certain samples due to reward hacking, we propose the Trust-GRPO method, which
assigns a trustworthiness weight to the thinking reward during training. This
weight is computed based on the thinking reward comparison of responses leading
to correct answers versus incorrect answers, helping to mitigate the impact of
potentially unreliable thinking rewards. Moreover, we design an annealing
training strategy that gradually reduces the thinking reward over time,
allowing the model to rely more on the accurate rule-based outcome reward in
later training stages. Experiments show that our SophiaVL-R1 surpasses a series
of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),
demonstrating strong reasoning and generalization capabilities. Notably, our
SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite
the latter having 10 times more parameters. All code, models, and datasets are
made publicly available at https://github.com/kxfan2002/SophiaVL-R1.
### 🌟 论文解读 | SophiaVL-R1：通过思考奖励强化多模态大语言模型的推理能力

### 📌 背景痛点/本文动机
近年来，通过基于规则的强化学习（RL）以及结果奖励，多模态大语言模型（MLLMs）在推理能力上取得了显著进展。然而，这种范式通常缺乏对思考过程的监督，导致模型可能学习到次优的推理策略，从而影响其泛化能力。为了解决这个问题，本文提出了SophiaVL-R1模型，试图在训练过程中加入对思考过程的奖励信号。

### 🚀 核心方法
💡 创新点1
本文首先训练了一个思考奖励模型，该模型能够从整体上评估整个思考过程的质量。与传统的逐步骤提供反馈的过程奖励模型（PRMs）不同，SophiaVL-R1的思考奖励模型从逻辑合理性、步骤一致性以及思考过程中的冗余等多个维度进行评估。

💡 创新点2
考虑到模型生成的思考奖励可能在某些情况下不可靠，本文提出了Trust-GRPO方法。该方法通过为思考奖励分配一个信任度权重，来评估奖励的可靠性。这个权重是通过比较正确答案和错误答案的思考奖励来确定的。此外，本文还设计了一个退火训练策略，随着训练的进行，逐渐减少思考奖励的影响，使模型在后期更多地依赖准确的结果奖励。

### 📈 实验结果
实验表明，SophiaVL-R1在多个基准测试（如MathVisita、MMMU）上超过了其他一系列推理MLLMs，展示了强大的推理和泛化能力。特别值得一提的是，SophiaVL-R1-7B甚至在大多数基准测试上超过了参数数量是其10倍的LLaVA-OneVision-72B。

### 💬 可借鉴之处
本文提出的SophiaVL-R1模型为多模态大语言模型的推理能力提升提供了一种新的思路，即通过加入对思考过程的奖励信号来引导模型学习更合理的推理策略。此外，Trust-GRPO方法为处理模型生成奖励的不可靠性提供了一种有效手段，这些方法对于提升模型的泛化能力和推理质量具有借鉴意义。

## speculative-thinking--enhancing-small-model-reasoning-with-large-model-guidance-at-inference-time
### Abstract
Recent advances leverage post-training to enhance model reasoning
performance, which typically requires costly training pipelines and still
suffers from inefficient, overly lengthy outputs. We introduce Speculative
Thinking, a training-free framework that enables large reasoning models to
guide smaller ones during inference at the reasoning level, distinct from
speculative decoding, which operates at the token level. Our approach is based
on two observations: (1) reasoning-supportive tokens such as "wait" frequently
appear after structural delimiters like "\n\n", serving as signals for
reflection or continuation; and (2) larger models exhibit stronger control over
reflective behavior, reducing unnecessary backtracking while improving
reasoning quality. By strategically delegating reflective steps to a more
capable model, our method significantly boosts the reasoning accuracy of
reasoning models while shortening their output. With the assistance of the 32B
reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to
89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average
output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%
decrease. Moreover, when applied to a non-reasoning model
(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%
on the same benchmark, achieving a relative improvement of 7.8%.
### 🌟 论文解读 | “以大带小”：推理模型推理能力增强新框架

### 📌 背景痛点/本文动机
在当前的AI应用中，小型语言模型因计算和内存需求较低而被广泛使用。然而，它们在需要复杂推理的任务上往往表现不佳。虽然可以通过后训练（如监督微调或强化学习）来提升它们的推理能力，但这些方法成本高昂、数据密集且难以扩展。为了在不进行额外训练的情况下提升小型模型的推理能力，本文提出了一个创新的推理时增强框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了“Speculative Thinking”框架，这是一种无需训练的方法，它允许大型推理模型在推理时指导小型模型，从而提升其推理性能。与在令牌级别操作的投机解码不同，本文的方法关注于推理级别，即大型模型在关键推理步骤上指导小型模型。

💡 创新点2
该方法基于两个观察：首先，推理支持的令牌（如“wait”）经常在结构分隔符（如“\n\n”）之后出现，作为反思或继续的信号；其次，大型模型在控制反思行为方面表现出更强的能力，减少了不必要的回溯，同时提高了推理质量。通过动态检测这些点并将反思步骤委托给能力更强的模型，Speculative Thinking既保持了小型模型的效率，又在其最需要的地方利用了大型模型的推理优势。

### 📈 实验结果
实验结果显示，在32B推理模型的辅助下，1.5B模型的MATH500准确度从83.2%提升至89.4%，提高了6.2%。同时，平均输出长度从5439个令牌减少到4583个令牌，减少了15.7%。此外，当该方法应用于非推理模型（Qwen-2.5-7B-Instruct）时，其在MATH500上的准确度从74.0%提升至81.8%，相对提高了7.8%。

### 💬 可借鉴之处
本文提出的Speculative Thinking框架为小型模型推理能力的增强提供了一种新的思路，它通过结合小型模型的效率和大型模型的推理能力，为实际应用中的推理增强提供了一种成本效益高的方法。这种方法不仅适用于专门训练的推理模型，也适用于未经过推理训练的模型，具有广泛的适用性和实用价值。

## ovip--online-vision-language-preference-learning
### Abstract
Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.
### 🌟 论文解读 | “OViP：实时视觉-语言偏好学习框架，减少AI幻觉现象”

### 📌 背景痛点/本文动机
大型视觉-语言模型（LVLMs）在处理多模态任务时表现出色，但仍然容易产生幻觉现象，即生成的文本内容与视觉输入不匹配。虽然最近的研究通过多模态直接偏好优化（DPO）方法来减轻幻觉问题，但这些方法通常依赖于预定义或随机编辑的负样本，这些样本并不能真实反映模型的错误，从而限制了训练效果。本文旨在解决这一问题，提出了一种在线视觉-语言偏好学习（OViP）框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
OViP框架动态构建基于模型自身幻觉输出的对比训练数据。通过识别采样响应对之间的语义差异，并使用扩散模型合成负图像，OViP实时生成更相关的监督信号。

💡 创新点2
OViP框架结合了在线偏好学习和图像感知训练，通过实时采样和整合新的偏好对，根据模型出现的新的失败模式进行自适应学习，从而减少对静态数据集的依赖，提高了模型的鲁棒性和泛化能力。

### 📈 实验结果
本文在多种多模态基准测试中评估了OViP框架，包括专门针对幻觉和一般性的基准测试。实验结果表明，OViP框架在减少幻觉的同时，保持了模型的核心多模态能力。此外，本文还改进了现有的评估协议，引入了一种更稳健的评估策略，该策略同时评估幻觉和视觉-语言能力。

### 💬 可借鉴之处
本文提出的OViP框架为减少大型视觉-语言模型的幻觉现象提供了一种有效的方法。其动态构建对比训练数据和实时更新模型的能力，对于提高模型的视觉接地性和输出质量具有重要意义。此外，本文对评估协议的改进也为未来相关研究提供了更加全面和准确的评估手段。

## acereason-nemotron--advancing-math-and-code-reasoning-through-reinforcement-learning
### Abstract
Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.
### 🌟 论文解读 | 通过大规模强化学习提升数学与代码推理能力：AceReason-Nemotron

### 📌 背景痛点/本文动机
尽管近年来在推理领域的大规模强化学习（RL）取得了显著进展，但构建高性能推理模型的训练配方仍然不明确。前沿模型的关键实现细节，如数据筛选策略和RL训练配方，常常被省略。此外，最近的研究表明，对于小型模型，知识蒸馏通常比RL更有效。本文旨在展示大规模RL可以显著提升中小型模型的推理能力，达到甚至超过最先进的基于蒸馏的模型的表现。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种新的训练方法：先对数学问题进行RL训练，再对代码问题进行RL训练。这一方法不仅显著提高了数学基准测试上的表现（例如，7B模型在AIME 2025上的表现提升了14.6%，14B模型提升了17.2%），而且在代码推理任务上也取得了显著提升（例如，7B模型在LiveCodeBench上的表现提升了6.8%，14B模型提升了5.8%）。

💡 创新点2
本文开发了一个稳健的数据筛选管道，用于收集具有高质量、可验证答案和测试用例的挑战性问题，从而支持数学和代码领域的验证式RL。此外，通过详细的消融研究和分析，本文揭示了几个关键实验见解，包括逐步增加响应长度的课程学习以及策略参数更新的稳定效果。

### 📈 实验结果
实验结果表明，RL不仅激发了模型在预训练和监督微调（如蒸馏）期间获得的基础推理能力，还推动了模型推理能力的极限，使其能够解决之前无法解决的问题。具体来说，数学问题上的RL训练显著提升了模型在数学和代码任务上的表现，而代码问题上的RL训练进一步提高了代码任务的表现，同时对数学任务的性能影响最小。

### 💬 可借鉴之处
本文的研究表明，对于中小型模型，通过适当的数据筛选和训练策略，大规模RL可以显著提升推理能力。此外，本文提出的课程学习策略和策略参数更新方法为稳定和高效的RL训练提供了新的思路。最后，本文的开源数据集和模型将有助于社区进一步研究和改进推理模型。

## observe-r1--unlocking-reasoning-abilities-of-mllms-with-dynamic-progressive-reinforcement-learning
### Abstract
Reinforcement Learning (RL) has shown promise in improving the reasoning
abilities of Large Language Models (LLMs). However, the specific challenges of
adapting RL to multimodal data and formats remain relatively unexplored. In
this work, we present Observe-R1, a novel framework aimed at enhancing the
reasoning capabilities of multimodal large language models (MLLMs). We draw
inspirations from human learning progression--from simple to complex and easy
to difficult, and propose a gradual learning paradigm for MLLMs. To this end,
we construct the NeuraLadder dataset, which is organized and sampled according
to the difficulty and complexity of data samples for RL training. To tackle
multimodal tasks, we introduce a multimodal format constraint that encourages
careful observation of images, resulting in enhanced visual abilities and
clearer and more structured responses. Additionally, we implement a bonus
reward system that favors concise, correct answers within a length constraint,
alongside a dynamic weighting mechanism that prioritizes uncertain and
medium-difficulty problems, ensuring that more informative samples have a
greater impact on training. Our experiments with the Qwen2.5-VL-3B and
Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that
Observe-R1 outperforms a series of larger reasoning models on both reasoning
and general benchmarks, achieving superior clarity and conciseness in reasoning
chains. Ablation studies validate the effectiveness of our strategies,
highlighting the robustness and generalization of our approach. The dataset and
code will be released at https://github.com/zrguo/Observe-R1.
### 🌟 论文解读 | 解锁多模态大语言模型推理能力：动态渐进式强化学习框架Observe-R1

### 📌 背景痛点/本文动机
随着强化学习（RL）在提升大型语言模型（LLM）推理能力方面的成功，如何将这种技术应用于多模态大型语言模型（MLLM）成为一个新的挑战。现有的研究虽然证明了RL可以增强MLLM的推理能力，但尚未针对多模态数据和模型的特性进行深入优化和探索。本文旨在解决这一问题，提出了一种名为Observe-R1的框架，以动态渐进式强化学习的方式，提升MLLM的推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文受到人类学习进阶的启发，提出了一个渐进式学习范式，即从简单到复杂、从容易到困难的学习过程。为此，构建了一个名为NeuraLadder的数据集，该数据集根据问题样本的难度和复杂性进行组织和采样，以适应RL训练。

💡 创新点2
针对多模态任务，引入了一种多模态格式约束，鼓励模型仔细观察图像，从而提高视觉能力，并产生更清晰、结构化的响应。同时，实施了一个奖励系统，该系统偏好简洁正确的答案，并设置了长度限制，以及一个基于模型不确定性的动态权重机制，确保更信息丰富的样本在训练中产生更大的影响。

### 📈 实验结果
使用Qwen2.5-VL-3B和Qwen2.5-VL-7B作为基模型，在NeuraLadder数据集的20k个样本上进行实验，得到的Observe-R1-3B和7B模型在数学和科学推理基准测试上超过了7-11B参数的推理模型。此外，Observe-R1在推理链上表现出更清晰、更简洁的特点。消融研究也验证了本文策略的有效性和泛化能力。

### 💬 可借鉴之处
本文提出的动态渐进式强化学习框架为提升MLLM的推理能力提供了一种新的思路。通过构建专门的数据集、引入多模态格式约束和动态权重机制，本文的方法在实验中取得了显著效果。此外，本文的开源数据集、模型和代码也为后续的研究和开发提供了便利。

## solver-informed-rl--grounding-large-language-models-for-authentic-optimization-modeling
### Abstract
Optimization modeling is fundamental to decision-making across diverse
domains.Despite progress in automating optimization formulation from natural
language descriptions, Large Language Models (LLMs) often struggle to generate
formally correct and usable models due to hallucinations, posing a challenge
for reliable automation. Inspired by the success of Reinforcement Learning (RL)
in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement
Learning (SIRL).This novel framework leverages external optimization solvers as
verifiable reward mechanisms to significantly improve the authenticity of LLMs
for optimization modeling.Acting as precise verifiers, these solvers
automatically assess the executable code and the instance-level mathematical
model represented by the associated LP file, yielding precise and comprehensive
feedback signals -- including syntax, feasibility, and solution quality that
directly inform the RL process. This automated verification process, powered by
classic optimization solvers, also underpins our instance-enhanced
self-consistency method to synthesize high-quality training data. Extensive
experiments on diverse public benchmarks demonstrate that SIRL achieves
state-of-the-art performance, substantially outperforming existing methods in
generating accurate and executable optimization models.
### 🌟 论文解读 | 利用求解器指导的强化学习：为大型语言模型打造真实优化建模

### 📌 背景痛点/本文动机
优化建模是决策过程中不可或缺的工具，广泛应用于物流、金融、工程和机器学习等多个领域。尽管现代优化求解器如Gurobi、COPT和CPLEX等功能强大，但将复杂现实问题转化为精确的数学模型和可执行优化代码仍然是一个重大挑战，通常需要大量的领域知识和手动工作。大型语言模型（LLM）的出现为自动化或辅助这一复杂的数学建模和代码生成过程提供了希望，但确保LLM生成的优化模型的正确性、可行性和求解器兼容性仍然是一个研究难题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一个名为“求解器指导的强化学习（SIRL）”的新框架，通过利用外部优化求解器作为可验证的奖励机制，显著提高了LLM在优化建模中的真实性。这种方法利用了强化学习与可验证奖励（RLVR）的优势，直接根据验证结果优化LLM的策略。

💡 创新点2
SIRL框架采用了一种实例增强的自一致性方法来合成高质量的训练数据。通过执行生成的代码，利用经典优化求解器提供的丰富奖励信号，如可行性状态、目标函数值和数学模型统计信息，来评估LLM的性能和有效性。

### 📈 实验结果
在多个公共基准测试上的广泛实验表明，SIRL实现了最先进的表现，显著优于其他离线学习和基于代理的方法在生成正确和可靠模型方面的性能。训练出的7B参数模型在SIRL框架下展现了卓越的性能。

### 💬 可借鉴之处
本文的工作为LLM在优化建模领域的应用提供了新的视角和方法，特别是通过利用外部优化求解器作为验证工具和奖励信号来源，为提高LLM生成模型的真实性和可靠性提供了有效途径。此外，实例增强的自一致性方法为合成高质量训练数据提供了一种可扩展的解决方案，对于其他需要精确数据合成的任务也具有借鉴意义。

## not-all-rollouts-are-useful--down-sampling-rollouts-in-llm-reinforcement-learning
### Abstract
Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing
reasoning capabilities in large language models, but faces a fundamental
asymmetry in computation and memory requirements: inference is embarrassingly
parallel with a minimal memory footprint, while policy updates require
extensive synchronization and are memory-intensive. To address this asymmetry,
we introduce PODS (Policy Optimization with Down-Sampling), a framework that
strategically decouples these phases by generating numerous rollouts in
parallel but updating only on an informative subset. Within this framework, we
develop max-variance down-sampling, a theoretically motivated method that
selects rollouts with maximally diverse reward signals. We prove that this
approach has an efficient algorithmic solution, and empirically demonstrate
that GRPO with PODS using max-variance down-sampling achieves superior
performance over standard GRPO on the GSM8K benchmark.
### 🌟 论文解读 | “优化大规模语言模型推理能力：PODS框架下的高效强化学习策略”

### 📌 背景痛点/本文动机
随着强化学习（RL）在提升大型语言模型推理能力方面的应用日益广泛，其在数学、编程和通用问题解决任务上的性能得到了显著提升。然而，强化学习在计算和内存需求上存在根本性的不对称性：推理阶段可以轻松并行处理，而策略更新阶段则需要大量的同步和内存资源。这种不对称性限制了强化学习在资源受限环境下的应用，并影响了其在资源丰富环境下的可扩展性。本文旨在解决这一痛点，提出了一种新的策略优化框架，通过智能选择策略更新中使用的推理结果，从而提高训练效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了PODS（Policy Optimization with Down-Sampling）框架，该框架通过在推理阶段生成大量推理结果（rollouts），但在策略更新阶段只选择最有信息量的子集进行更新，从而有效地解耦了推理和训练阶段，优化了资源利用。

💡 创新点2
在PODS框架下，本文开发了一种理论驱动的最大方差抽样方法（max-variance down-sampling），该方法选择具有最大多样化奖励信号的推理结果，以促进对比信号的获取。作者证明了这种方法具有高效的算法解决方案，并在GSM8K基准测试中展示了其优越性能。

### 📈 实验结果
通过在GSM8K基准测试上的实验，本文证明了使用最大方差抽样的GRPO-PODS方法在性能上优于标准GRPO。实验结果显示，GRPO-PODS在保持或提高准确率的同时，显著减少了训练时间和内存需求。

### 💬 可借鉴之处
本文的方法为强化学习在大型语言模型中的应用提供了新的视角，特别是在优化资源利用和提高训练效率方面。PODS框架和最大方差抽样方法不仅适用于语言模型，也可能为其他强化学习应用提供启发。此外，本文的实验设计和结果分析为理解和评估强化学习策略提供了有价值的参考。

## trajectory-bellman-residual-minimization--a-simple-value-based-method-for-llm-reasoning
### Abstract
Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.
### 🌟 论文解读 | “解锁LLM推理新方法：轨迹贝尔曼残差最小化”

### 📌 背景痛点/本文动机
当前，强化学习（RL）在大型语言模型（LLM）推理中的应用主要采用基于策略的方法，如近端策略优化（PPO）和群组相对策略优化（GRPO）。这些方法虽然实证效果显著，但存在一些实际挑战，例如需要大量的计算资源进行新鲜的对策略滚动，依赖额外的组件如评估模型、优势归一化和剪辑机制，以及对于每个标记的决策过于简化。本文旨在探索一种基于价值的强化学习方法，以解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文重新审视了经典的贝尔曼残差最小化（BRM）范式，并引入了一种名为轨迹贝尔曼残差最小化（TBRM）的算法。TBRM是一种简单有效的离策略算法，它使用模型自身的日志概率作为Q值，优化单个轨迹级别的贝尔曼目标。

💡 创新点2
TBRM算法消除了对评估模型、重要性采样比或剪辑的需求，并且只需要每个提示进行一次滚动。此外，本文证明了在标准可实现性假设下，即使训练数据由任意行为策略生成，算法也能收敛到最优的KL正则化策略。

### 📈 实验结果
在六个数学推理基准测试中，TBRM算法一致地优于PPO和GRPO基线。特别地，在AIME24测试中，TBRM使用Qwen2.5-Math-7B模型达到了30.5%的准确率。与GRPO相比，TBRM平均基准测试得分提高了1.3%，在相同的条件下，与PPO相比，TBRM在更少的训练时间和更低的GPU内存消耗下实现了更好的性能。

### 💬 可借鉴之处
本文的研究表明，基于价值的强化学习方法为增强LLM的推理能力提供了一种有原则且高效的替代方案。通过将价值学习重新定位在轨迹级别，TBRM提供了一种原理清晰、效率高且理论上有据可依的方法，用于提高数学推理任务的表现，同时显著降低了计算需求。这项研究对于希望提高LLM推理能力的研究者和工程师具有很高的参考价值。

## t2i-r1--reinforcing-image-generation-with-collaborative-semantic-level-and-token-level-cot
### Abstract
Recent advancements in large language models have demonstrated how
chain-of-thought (CoT) and reinforcement learning (RL) can improve performance.
However, applying such reasoning strategies to the visual generation domain
remains largely unexplored. In this paper, we present T2I-R1, a novel
reasoning-enhanced text-to-image generation model, powered by RL with a
bi-level CoT reasoning process. Specifically, we identify two levels of CoT
that can be utilized to enhance different stages of generation: (1) the
semantic-level CoT for high-level planning of the prompt and (2) the
token-level CoT for low-level pixel processing during patch-by-patch
generation. To better coordinate these two levels of CoT, we introduce
BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes
both generation CoTs within the same training step. By applying our reasoning
strategies to the baseline model, Janus-Pro, we achieve superior performance
with 13% improvement on T2I-CompBench and 19% improvement on the WISE
benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available
at: https://github.com/CaraJ7/T2I-R1
### 🌟 论文解读 | “T2I-R1：协同语义级与标记级链式思维增强的文本到图像生成模型”

### 📌 背景痛点/本文动机
近年来，大型语言模型在数学、编程等多个领域的推理能力得到了显著提升，特别是通过链式思维（CoT）和强化学习（RL）的结合，能够显著提高模型的性能。然而，将这种推理策略应用于视觉生成领域的研究还相对较少。本文旨在探索如何将CoT和RL应用于文本到图像的生成任务，以提升图像生成的质量和准确性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了T2I-R1模型，这是一种新型的推理增强型文本到图像生成模型，通过RL和双级别CoT推理过程实现。具体来说，我们识别了两个级别的CoT，分别用于增强生成过程的不同阶段：（1）语义级CoT用于生成前的高级别规划；（2）标记级CoT用于生成过程中的低级别像素处理。

💡 创新点2
为了更好地协调这两个级别的CoT，本文引入了BiCoT-GRPO方法，这是一种结合了多种生成奖励的RL方法，能够在同一个训练步骤中无缝优化两个生成CoT。通过将推理策略应用于基线模型Janus-Pro，T2I-R1在T2I-CompBench上实现了13%的性能提升，在WISE基准上实现了19%的性能提升，甚至超过了当前最先进的模型FLUX。

### 📈 实验结果
通过实验验证，T2I-R1模型在处理需要推理或包含非常规场景的提示时，能够成功推导出提示背后的真实意图，或为非常规场景提供合理的想象，从而生成满意的结果。与基线模型Janus-Pro相比，T2I-R1在图像质量和与提示的对应关系上都有显著提升。

### 💬 可借鉴之处
本文的方法为视觉生成任务提供了一种新的推理增强框架，不仅能够提升图像生成的质量，还能够优化生成过程与提示之间的对应关系。此外，通过使用多种视觉专家作为奖励模型，本文提出了一种新的奖励设计策略，这对于其他视觉生成任务也具有借鉴意义。

## echo-chamber--rl-post-training-amplifies-behaviors-learned-in-pretraining
### Abstract
Reinforcement learning (RL)-based fine-tuning has become a crucial step in
post-training language models for advanced mathematical reasoning and coding.
Following the success of frontier reasoning models, recent work has
demonstrated that RL fine-tuning consistently improves performance, even in
smaller-scale models; however, the underlying mechanisms driving these
improvements are not well-understood. Understanding the effects of RL
fine-tuning requires disentangling its interaction with pretraining data
composition, hyperparameters, and model scale, but such problems are
exacerbated by the lack of transparency regarding the training data used in
many existing models. In this work, we present a systematic end-to-end study of
RL fine-tuning for mathematical reasoning by training models entirely from
scratch on different mixtures of fully open datasets. We investigate the
effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration)
across models of different scales. Our study reveals that RL algorithms
consistently converge towards a dominant output distribution, amplifying
patterns in the pretraining data. We also find that models of different scales
trained on the same data mixture will converge to distinct output
distributions, suggesting that there are scale-dependent biases in model
generalization. Moreover, we find that RL post-training on simpler questions
can lead to performance gains on harder ones, indicating that certain reasoning
capabilities generalize across tasks. Our findings show that small-scale
proxies in controlled settings can elicit interesting insights regarding the
role of RL in shaping language model behavior.
### 🌟 论文解读 | 深入探究强化学习微调对数学推理模型的影响

### 📌 背景痛点/本文动机
随着强化学习（RL）微调成为提升语言模型数学推理和编码能力的关键步骤，研究者们发现即使是小规模模型，RL微调也能显著提高性能。然而，这些改进背后的机制尚不明确。本文的动机在于深入理解RL微调如何与预训练数据组成、超参数设置和模型规模相互作用，从而影响模型的行为。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文通过从零开始训练模型，使用不同组合的完全开放数据集，进行了一项系统的端到端研究，以探究RL微调对数学推理的影响。这种方法允许研究者完全控制模型在预训练阶段接触到的数据。

💡 创新点2
本文研究了不同规模的模型在不同数据混合训练下的行为，揭示了RL算法倾向于收敛到主导输出分布，放大了预训练数据中的模式。此外，还发现了模型在不同规模下会收敛到不同的输出分布，表明模型泛化存在规模依赖性偏差。

### 📈 实验结果
实验结果表明，RL微调不仅提高了模型的准确率，还减少了输出的多样性。尽管偶尔会出现失败案例，但模型通常倾向于生成性能最优的分布格式。此外，RL微调还能在简单问题上的训练带来在更难问题上性能提升的积极迁移效应。

### 💬 可借鉴之处
本文的研究揭示了以下几个可借鉴之处：
1. RL微调可以显著提高数学推理模型的性能，但其效果与预训练数据的组成密切相关。
2. 模型的规模会影响其输出的分布，小规模模型更倾向于生成代码式的简单输出，而大规模模型则倾向于生成自然语言输出。
3. RL微调在简单问题上的训练可以促进模型在更难问题上的性能提升，表明某些推理能力可以在任务之间迁移。
4. 为了更好地理解RL微调的影响，需要对预训练数据有更高的透明度和控制力。

## seed-grpo--semantic-entropy-enhanced-grpo-for-uncertainty-aware-policy-optimization
### Abstract
Large language models (LLMs) exhibit varying levels of confidence across
input prompts (questions): some lead to consistent, semantically similar
answers, while others yield diverse or contradictory outputs. This variation
reflects LLM's uncertainty about the input prompt, a signal of how confidently
the model understands a given problem. However, vanilla Group Relative Policy
Optimization (GRPO) treats all prompts equally during policy updates, ignoring
this important information about the model's knowledge boundaries. To address
this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which
explicitly measures LLMs' uncertainty of the input prompts semantic entropy.
Semantic entropy measures the diversity of meaning in multiple generated
answers given a prompt and uses this to modulate the magnitude of policy
updates. This uncertainty-aware training mechanism enables dynamic adjustment
of policy update magnitudes based on question uncertainty. It allows more
conservative updates on high-uncertainty questions while maintaining the
original learning signal on confident ones. Experimental results on five
mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva
34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new
state-of-the-art performance in average accuracy, validating the effectiveness
of uncertainty-aware policy optimization.
### 🌟 论文解读 | 利用语义熵增强GRPO实现不确定性感知的策略优化

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在复杂任务上的应用，模型对于不同输入提示（问题）的响应表现出不同的置信度水平。一些问题会得到一致、语义相似的答案，而另一些则产生多样甚至矛盾的输出。这种变化反映了模型对输入提示的不确定性，这是模型对问题理解自信程度的一个信号。然而，传统的Group Relative Policy Optimization（GRPO）在策略更新时对所有提示一视同仁，忽略了这一关于模型知识边界的重要信息。本文旨在解决这一问题，提出了一种不确定性感知的策略优化方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了SEED-GRPO（Semantic Entropy EnhanceD GRPO），一种显式测量LLM对输入提示语义熵的不确定性感知策略优化算法。语义熵是一种基于熵的度量，它捕捉了给定提示的多个生成答案之间意义的多样性。

💡 创新点2
SEED-GRPO利用语义熵来调整策略更新的幅度，使得在问题不确定性高时进行更保守的更新，而在模型表现出更高自信的问题上保持原始的学习信号。这种方法实现了一种动态学习率机制，根据模型当前的能力和问题难度自动校准。

### 📈 实验结果
在五个数学推理基准（AIME24、AMC、MATH、Minerva和OlympiadBench）上的实验结果表明，SEED-GRPO在平均准确度上达到了新的最佳性能，验证了不确定性感知策略优化的有效性。

### 💬 可借鉴之处
本文的方法为LLM的推理能力提升提供了一种新的思路，即通过测量和利用模型的不确定性来优化策略更新。SEED-GRPO的成功表明，在策略学习中考虑模型的不确定性可以带来性能上的显著提升，这对于未来的模型训练和优化具有启发意义。此外，该方法在数学推理任务上的表现也证明了其在实际应用中的潜力。

## lmgame-bench--how-good-are-llms-at-playing-games-
### Abstract
Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.
### 🌟 论文解读 | "lmgame-Bench：大型语言模型在游戏中的表现如何？"

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在人工智能领域的快速发展，评估这些模型在复杂任务中的表现变得尤为重要。视频游戏因其需要感知、记忆和规划能力，成为了评估LLM的理想平台。然而，直接将LLM放入游戏环境中进行评估存在诸多挑战，如视觉感知脆弱、提示敏感性和数据污染等问题。本文旨在解决这些问题，提出了一种名为lmgame-Bench的评估框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首次提出了lmgame-Bench，这是一个基于视频游戏的评估框架，旨在通过引入感知和记忆模块来克服LLM在游戏中的常见挑战。这些模块帮助模型在视觉感知和长期规划方面进行优化。

💡 创新点2
lmgame-Bench通过一系列精心设计的游戏（包括平台跳跃、解谜和叙事驱动的侦探游戏）来评估LLM的表现。该框架还采用了一种标准化的提示优化技术，以减少提示敏感性，并适应游戏设置以减轻数据污染。

### 📈 实验结果
本文在13个领先模型上进行了评估，结果显示lmgame-Bench能够有效区分不同模型的表现。实验还发现，每个游戏都探测到了模型能力的独特组合，这些能力在其他孤立的环境中通常不会一起测试。更有趣的是，对lmgame-Bench中的单个游戏进行强化学习，不仅能够提高模型在该游戏上的表现，还能够转移到未见过的游戏和外部规划任务上。

### 💬 可借鉴之处
lmgame-Bench为评估LLM在复杂任务中的表现提供了一个新的视角。它不仅能够有效地区分模型的能力，还能够作为改进LLM能力的训练环境。此外，本文提供的定量分析方法和实验结果，为理解和改进LLM的游戏表现提供了宝贵的参考。

## rift--closed-loop-rl-fine-tuning-for-realistic-and-controllable-traffic-simulation
### Abstract
Achieving both realism and controllability in interactive closed-loop traffic
simulation remains a key challenge in autonomous driving. Data-driven
simulation methods reproduce realistic trajectories but suffer from covariate
shift in closed-loop deployment, compounded by simplified dynamics models that
further reduce reliability. Conversely, physics-based simulation methods
enhance reliable and controllable closed-loop interactions but often lack
expert demonstrations, compromising realism. To address these challenges, we
introduce a dual-stage AV-centered simulation framework that conducts open-loop
imitation learning pre-training in a data-driven simulator to capture
trajectory-level realism and multimodality, followed by closed-loop
reinforcement learning fine-tuning in a physics-based simulator to enhance
controllability and mitigate covariate shift. In the fine-tuning stage, we
propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that
preserves the trajectory-level multimodality through a GRPO-style
group-relative advantage formulation, while enhancing controllability and
training stability by replacing KL regularization with the dual-clip mechanism.
Extensive experiments demonstrate that RIFT significantly improves the realism
and controllability of generated traffic scenarios, providing a robust platform
for evaluating autonomous vehicle performance in diverse and interactive
scenarios.
### 🌟 论文解读 | “RIFT：实现交互式闭环交通模拟的真实性与可控性”

### 📌 背景痛点/本文动机
在自动驾驶系统的开发中，可靠的闭环交通模拟至关重要，它支持自动驾驶车辆的训练和评估。理想的交通模拟应具备真实性和可控性两大特性：真实性反映现实世界的驾驶行为，而可控性允许定制化的交互风格。然而，现有的研究往往只关注生成真实的交通场景或构建可控的交互，同时确保真实性和可控性在交互式闭环场景中仍然是一个未解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种双阶段自动驾驶车辆（AV）中心化模拟框架。首先，在数据驱动模拟器中进行开环模仿学习预训练，以捕获轨迹级别的真实性和多模态性；然后，在物理基础模拟器中进行闭环强化学习微调，以增强可控性和减轻协变量偏移。

💡 创新点2
在微调阶段，本文提出了RIFT（闭环RL微调策略），这是一种简单而有效的策略，通过GRPO风格的组相对优势公式保持轨迹级别的多模态性，同时通过替换KL正则化为双剪辑机制来增强可控性和训练稳定性。

### 📈 实验结果
广泛的实验表明，RIFT显著提高了生成交通场景的真实性和可控性，为评估自动驾驶车辆在多样化和交互式场景中的性能提供了一个稳健的平台。

### 💬 可借鉴之处
本文的双阶段模拟框架有效地结合了数据驱动和物理基础模拟器的优势，为解决真实性和可控性之间的根本性权衡提供了新的思路。RIFT策略在保持轨迹多模态性的同时，通过新颖的优化机制提高了闭环场景中的可控性和训练稳定性，对于自动驾驶系统的模拟和评估具有很高的实用价值。

## towards-reasoning-era--a-survey-of-long-chain-of-thought-for-reasoning-large-language-models
### Abstract
Recent advancements in reasoning with large language models (RLLMs), such as
OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in
complex domains like mathematics and coding. A central factor in their success
lies in the application of long chain-of-thought (Long CoT) characteristics,
which enhance reasoning abilities and enable the solution of intricate
problems. However, despite these developments, a comprehensive survey on Long
CoT is still lacking, limiting our understanding of its distinctions from
traditional short chain-of-thought (Short CoT) and complicating ongoing debates
on issues like "overthinking" and "test-time scaling." This survey seeks to
fill this gap by offering a unified perspective on Long CoT. (1) We first
distinguish Long CoT from Short CoT and introduce a novel taxonomy to
categorize current reasoning paradigms. (2) Next, we explore the key
characteristics of Long CoT: deep reasoning, extensive exploration, and
feasible reflection, which enable models to handle more complex tasks and
produce more efficient, coherent outcomes compared to the shallower Short CoT.
(3) We then investigate key phenomena such as the emergence of Long CoT with
these characteristics, including overthinking, and test-time scaling, offering
insights into how these processes manifest in practice. (4) Finally, we
identify significant research gaps and highlight promising future directions,
including the integration of multi-modal reasoning, efficiency improvements,
and enhanced knowledge frameworks. By providing a structured overview, this
survey aims to inspire future research and further the development of logical
reasoning in artificial intelligence.
### 🌟 论文解读 | 探索长链推理时代：大型语言模型的长链推理研究综述

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在数学和编程等复杂领域展现出令人瞩目的推理能力，长链推理（Long CoT）特性的应用成为其成功的关键。然而，尽管取得了这些进展，关于长链推理的全面综述仍然缺失，这限制了我们对长链推理与传统短链推理（Short CoT）差异的理解，并使得关于“过度思考”和“测试时扩展”等问题的讨论变得复杂。本文旨在填补这一空白，提供一个关于长链推理的统一视角。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文首先区分了长链推理（Long CoT）和传统短链推理（Short CoT），并引入了一种新颖的分类法来归类当前的推理范式。

💡 创新点2
本文探讨了长链推理的三个关键特性：深度推理、广泛探索和可行反思。这些特性使得模型能够处理更复杂的任务，并产生更高效、一致的结果。

💡 创新点3
本文研究了与长链推理特性相关的一些关键现象，如长链推理的出现、过度思考和测试时扩展等，提供了这些过程在实际中如何体现的见解。

💡 创新点4
本文识别了长链推理领域的重要研究空白，并突出了有前景的未来研究方向，包括多模态推理的整合、效率提升和增强知识框架。

### 📈 实验结果
本文通过提供一个结构化的概述，旨在激发未来的研究，并推动人工智能中逻辑推理的发展。

### 💬 可借鉴之处
本文为理解长链推理提供了清晰的框架，并对其关键特性进行了深入分析。研究者可以从本文中获得关于长链推理的最新进展和未来研究方向的宝贵信息，以及如何在实际应用中避免过度思考和无效探索的策略。此外，本文还提供了丰富的文献资源和开放资源框架，为未来的研究奠定了基础。

## tinyllava-video-r1--towards-smaller-lmms-for-video-reasoning
### Abstract
Recently, improving the reasoning ability of large multimodal models (LMMs)
through reinforcement learning has made great progress. However, most existing
works are based on highly reasoning-intensive datasets such as mathematics and
code, and researchers generally choose large-scale models as the foundation. We
argue that exploring small-scale models' reasoning capabilities remains
valuable for researchers with limited computational resources. Moreover,
enabling models to explain their reasoning processes on general
question-answering datasets is equally meaningful. Therefore, we present the
small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,
a traceably trained video understanding model with no more than 4B parameters,
it not only demonstrates significantly improved reasoning and thinking
capabilities after using reinforcement learning on general Video-QA datasets,
but also exhibits the emergent characteristic of "aha moments". Furthermore, we
share a series of experimental findings, aiming to provide practical insights
for future exploration of video reasoning (thinking) abilities in small-scale
models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.
### 🌟 论文解读 | 探索小型多模态模型在视频推理领域的潜力：TinyLLaVA-Video-R1

### 📌 背景痛点/本文动机
近年来，通过强化学习提升大型多模态模型（LMMs）推理能力的研究取得了显著进展。然而，大多数现有工作都基于高度推理密集型的数据集，如数学和代码，并且研究者通常选择大型模型作为基础。本文作者认为，对于计算资源有限的研究者来说，探索小型模型的推理能力仍然具有价值。此外，使模型能够解释其在一般性问题回答数据集上的推理过程也具有重要意义。因此，本文提出了小型视频推理模型TinyLLaVA-Video-R1。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文基于TinyLLaVA-Video模型，一个参数不超过40亿的视频理解模型，通过在一般视频问答（Video-QA）数据集上使用强化学习，显著提升了模型的推理和思考能力。

💡 创新点2
TinyLLaVA-Video-R1模型展现出了一种“灵感突现”的涌现特性，这意味着模型不仅能够通过感知生成答案，还能够经历推理过程中的“aha moments”，即突然理解问题关键的时刻。

### 📈 实验结果
本文通过广泛的实验配置，获得了一系列有洞察力的发现。这些实验结果表明，TinyLLaVA-Video-R1模型在视频推理任务上表现出色，不仅提升了推理能力，还能够生成详细的推理过程，为未来小型模型在视频推理领域的研究提供了实用见解。

### 💬 可借鉴之处
本文的研究表明，即使是小型多模态模型，通过适当的方法和训练策略，也能够在视频推理任务上取得显著进展。这对于计算资源有限的研究者来说，提供了一个新的研究方向。此外，本文提供的实验发现和模型设计思路，对于未来探索小型模型的视频推理能力具有参考价值。相关代码和数据集已公开，可供社区进一步研究和复现。

## o$^2$-searcher--a-searching-based-agent-model-for-open-domain-open-ended-question-answering
### Abstract
Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.
### 🌟 论文解读 | “O2-Searcher：开启开放式问题解答新篇章”

### 📌 背景痛点/本文动机
大型语言模型（LLM）虽然在数学推理和代码生成等任务上取得了显著进展，但由于其静态的参数化知识限制，难以应对需要实时更新的开放域任务。当前的研究主要关注封闭式问题，而对于缺乏标准答案或提供非唯一、多样化答案的开放式问题，研究仍然不足。本文旨在填补这一空白，提出了O2-Searcher，一种基于强化学习的搜索代理模型，能够有效处理开放域中的开放式和封闭式问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
O2-Searcher采用了一种高效、本地模拟的搜索环境，用于动态知识获取，有效地将外部世界知识与模型的复杂推理过程分离。这种设计使得模型能够快速、成本效益地获取外部信息。

💡 创新点2
本文提出了一种统一的训练机制，通过精心设计的奖励函数，使代理能够识别问题类型并适应性地调整答案生成策略。此外，为了评估模型在复杂开放式任务上的表现，构建了一个名为O2-QA的高质量开放域问答基准，包含300个来自不同领域的专家策划的开放式问题及其相关的网页缓存。

### 📈 实验结果
实验结果表明，O2-Searcher即使仅使用3B参数的模型，也显著超越了现有最先进的LLM代理在O2-QA基准上的表现。此外，在多个封闭式问答基准上，O2-Searcher不仅与参数大小相当的其他模型相比取得了最先进的表现，而且与参数更大的模型相比也表现出了相当的性能。

### 💬 可借鉴之处
本文提出的O2-Searcher模型为处理开放域中的开放式问题提供了一种新的思路，其高效的知识获取和灵活的应用策略对于提升LLM在开放域任务上的表现具有重要意义。此外，构建的O2-QA基准为评估LLM在开放式问题上的性能提供了一个高质量的标准，有助于推动相关研究的进一步发展。

## spectral-policy-optimization--coloring-your-incorrect-reasoning-in-grpo
### Abstract
Reinforcement learning (RL) has demonstrated significant success in enhancing
reasoning capabilities in large language models (LLMs). One of the most widely
used RL methods is Group Relative Policy Optimization
(GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and
success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls
when all sampled responses in a group are incorrect -- referred to as an
\emph{all-negative-sample} group -- as it fails to update the policy, hindering
learning progress. The contributions of this paper are two-fold. First, we
propose a simple yet effective framework that introduces response diversity
within all-negative-sample groups in GRPO using AI feedback. We also provide a
theoretical analysis, via a stylized model, showing how this diversification
improves learning dynamics. Second, we empirically validate our approach,
showing the improved performance across various model sizes (7B, 14B, 32B) in
both offline and online learning settings with 10 benchmarks, including base
and distilled variants. Our findings highlight that learning from
all-negative-sample groups is not only feasible but beneficial, advancing
recent insights from \citet{Xiong-2025-Minimalist}.
### 🌟 论文解读 | 利用AI反馈优化推理能力：Spectral Policy Optimization的探索

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在推理能力上的提升，强化学习（RL）方法在模型训练中扮演了重要角色。其中，Group Relative Policy Optimization（GRPO）因其内存效率高和成功训练DeepSeek-R1模型而受到广泛关注。然而，GRPO在处理所有样本均错误的组（称为全负样本组）时遇到了瓶颈，因为在这种情况下，政策无法接收到任何学习信号，导致学习进程停滞。本文旨在解决这一问题，提出了一种新的框架，通过AI反馈引入全负样本组内的响应多样性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种简单而有效的框架，称为Spectral Policy Optimization（SPO），它通过AI反馈在全负样本组内引入响应多样性。这种方法将原本的“黑白”结果奖励转变为多样化的“光谱”奖励，从而有效利用全负样本组。

💡 创新点2
SPO框架通过使用高级LLM（如o4-mini和Claude 3.74）的推理能力，对多步骤推理链进行整体评估，从而为负样本提供精细化的奖励。这种方法避免了传统过程奖励模型（PRM）的内存负担，也无需昂贵的步骤级人工标注，简化并加速了训练流程。

### 📈 实验结果
本文在多种模型大小（7B、14B、32B）的离线和在线学习设置下，对10个最先进的基准进行了评估，包括基础和精简变体。实验结果表明，SPO方法在提高LLM推理能力方面是有效的，解决了当前GRPO管道的局限性。

### 💬 可借鉴之处
本文的研究成果表明，通过AI反馈引入响应多样性，可以有效地利用全负样本组进行学习，这不仅可行，而且有益。这种方法为大型语言模型在推理任务上的训练提供了新的视角和工具，对于提升模型的推理能力和解决复杂任务具有重要意义。此外，SPO框架的设计和实验验证也为相关领域的研究提供了可借鉴的范例。

## group-in-group-policy-optimization-for-llm-agent-training
### Abstract
Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield sparse or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.
### 🌟 论文解读 | “迈向精细化的LLM智能体训练：GiGPO算法的革新之路”

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在决策制定领域的应用不断扩展，它们已经从静态的问答系统转变为能够在开放环境中感知、推理和行动的智能体。然而，现有的基于群体的强化学习（RL）算法在处理长周期任务时存在局限性，尤其是在奖励稀疏或延迟的情况下，对单个步骤的信用分配变得极具挑战性。本文旨在解决这一问题，提出了一种新的RL算法——GiGPO，它能够在保持群体RL算法优势的同时，为LLM智能体提供更精细的信用分配。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
GiGPO引入了一种双层结构来估计相对优势。在剧集级别，它基于一组完整的轨迹计算宏观相对优势；在步骤级别，它通过识别轨迹间重复的环境状态，引入了一种锚点状态分组机制，从而允许对局部步骤的有效性进行更精细的估计。

💡 创新点2
GiGPO利用在相同任务和初始环境条件下，许多轨迹多次遇到相同状态的自然特性，通过这些共享状态构建步骤级别的分组，从而实现对动作的局部化信用分配，而无需额外的模拟或辅助价值模型。

### 📈 实验结果
本文在两个具有挑战性的智能体基准测试——ALFWorld和WebShop上评估了GiGPO算法，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型。结果显示，GiGPO算法在保持与群体RL相同的GPU内存开销和LLM模拟的同时，提供了更精细的每步信用信号，并在ALFWorld上实现了超过12%的性能提升，在WebShop上实现了超过9%的性能提升。

### 💬 可借鉴之处
GiGPO算法在保持群体RL算法的无价值函数、低内存和稳定收敛特性的同时，引入了更精细的信用分配机制，这对于长周期任务中的LLM智能体训练具有很高的参考价值。此外，其提出的锚点状态分组机制为解决长周期任务中的信用分配问题提供了一种新的思路。

## beyond-semantics--the-unreasonable-effectiveness-of-reasonless-intermediate-tokens
### Abstract
Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.
### 🌟 论文解读 | 揭秘语言模型中的“无效思考”：无意义中间符号的非凡有效性

### 📌 背景痛点/本文动机
近年来，大型推理模型在Chain of Thought（CoT）策略的推动下取得了显著成果，尤其是通过在基础大型语言模型（LLM）上采样CoT进行训练，以帮助发现新的推理模式。然而，这种解释是否合理？本文通过研究中间符号的语义如何实际影响模型性能，对这一解释进行了批判性考察。作者们关注的是，这些常被拟人化为“思考”或推理轨迹的中间符号，是否真的以可预测的方式影响模型的表现。

### 🚀 核心方法
💡 创新点1
作者们训练了Transformer模型，使其在形式上可验证的推理轨迹和解决方案上进行学习，同时约束中间步骤和最终输出与形式求解器（如A*搜索）保持一致。通过构建一个形式解释器来评估问题语义和预期算法，他们系统地评估了解决方案准确性以及中间轨迹的正确性，从而能够判断后者是否因果影响了前者。

💡 创新点2
为了进一步证明轨迹准确性与解决方案准确性之间的联系松散，作者们还在噪声干扰、与特定问题无关的轨迹上训练了模型。结果发现，这些模型的性能不仅与在正确数据上训练的模型保持一致，有时甚至在分布外任务上表现得更加稳健。

### 📈 实验结果
实验表明，即使在完全正确的轨迹上训练，模型在得到正确解决方案时仍然会产生无效的推理轨迹。此外，在噪声干扰的轨迹上训练的模型，其性能与在正确数据上训练的模型相当，甚至在某些情况下表现得更好。

### 💬 可借鉴之处
本文挑战了中间符号或“链式思考”能够诱导可预测推理行为的假设，并警告不要过度拟人化这些输出或将它们过度解释为人类或算法行为的证据。研究结果表明，即使性能提升，假设人类般的或算法可解释的轨迹语义不仅是不必要的，甚至可能是误导性的。这对于理解和设计更有效的推理模型提供了新的视角和方法。

## deep-reasoning-translation-via-reinforcement-learning
### Abstract
Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown
promising performance in various complex tasks. Free translation is an
important and interesting task in the multilingual world, which requires going
beyond word-for-word translation and taking cultural differences into account.
This task is still under-explored in deep reasoning LLMs. In this paper, we
introduce DeepTrans, a deep reasoning translation model that learns free
translation via reinforcement learning. Specifically, we carefully build a
reward model with pre-defined scoring criteria on both the translation results
and the thought process. Given the source sentences, the reward model teaches
the deep translation model how to think and free-translate them during
reinforcement learning. In this way, training DeepTrans does not need any
labeled translations, avoiding the human-intensive annotation or
resource-intensive data synthesis. Experimental results show the effectiveness
of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance
by 16.3% in literature translation, and outperforms strong deep reasoning
baselines as well as baselines that are fine-tuned with synthesized data.
Moreover, we summarize the failures and interesting findings during our RL
exploration. We hope this work could inspire other researchers in free
translation.
### 🌟 论文解读 | 深度推理翻译的强化学习之路

### 📌 背景痛点/本文动机
近年来，深度推理大型语言模型（如OpenAI的o1/o3和DeepSeek-R1）在各种复杂任务中表现出令人瞩目的性能。然而，在多语言世界中，自由翻译是一项重要且有趣的任务，它要求翻译不仅仅停留在逐字逐句的层面，还需要考虑到文化差异。这项任务在深度推理大型语言模型中仍然鲜有人探索。本文旨在通过强化学习提升深度推理模型的自由翻译能力，提出了一种名为DeepTrans的深度推理翻译模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文采用了一种新颖的强化学习方法来训练DeepTrans模型，该模型能够学习自由翻译。具体来说，我们精心构建了一个奖励模型，该模型包含预定义的评分标准，既针对翻译结果，也针对思考过程。在强化学习过程中，奖励模型教会深度翻译模型如何思考和翻译源句子。

💡 创新点2
DeepTrans模型的训练不需要任何标记的翻译，这避免了耗时的注释或资源密集型数据合成。通过使用先进的指令型大型语言模型作为奖励模型，我们能够确保奖励模型的有效性。

### 📈 实验结果
使用Qwen2.5-7B作为基础模型，DeepTrans在文学翻译中提高了16.3%的性能，并且超过了强大的深度推理基线以及使用合成数据微调的基线。此外，本文还总结了在强化学习探索过程中的失败案例和有趣发现。

### 💬 可借鉴之处
本文的研究成果为自由翻译任务提供了新的视角，特别是如何通过强化学习提升深度推理模型的翻译能力。此外，本文详细总结了奖励模型的关键组成部分和失败案例，为后续研究提供了宝贵的经验教训。希望这项工作能够激发其他研究者在自由翻译领域的研究。

## dgro--enhancing-llm-reasoning-via-exploration-exploitation-control-and-reward-variance-management
### Abstract
Inference scaling further accelerates Large Language Models (LLMs) toward
Artificial General Intelligence (AGI), with large-scale Reinforcement Learning
(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning
approaches usually rely on handcrafted rule-based reward functions. However,
the tarde-offs of exploration and exploitation in RL algorithms involves
multiple complex considerations, and the theoretical and empirical impacts of
manually designed reward functions remain insufficiently explored. In this
paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL
algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional
regularization coefficient into two independent hyperparameters: one scales the
policy gradient term, and the other regulates the distance from the sampling
policy. This decoupling not only enables precise control over balancing
exploration and exploitation, but also can be seamlessly extended to Online
Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward
Optimization. On the other hand, we observe that reward variance significantly
affects both convergence speed and final model performance. We conduct both
theoretical analysis and extensive empirical validation to assess DGRO,
including a detailed ablation study that investigates its performance and
optimization dynamics. Experimental results show that DGRO achieves
state-of-the-art performance on the Logic dataset with an average accuracy of
96.9\%, and demonstrates strong generalization across mathematical benchmarks.
### 🌟 论文解读 | DGRO：通过探索-利用控制和奖励方差管理提升大规模语言模型推理能力

### 📌 背景痛点/本文动机
随着推理规模化的推进，大规模语言模型（LLM）正加速迈向人工通用智能（AGI）。大规模强化学习（RL）在释放长链推理能力方面扮演了关键角色。然而，现代推理方法通常依赖于手工设计的基于规则的奖励函数，这些方法在探索和利用之间的权衡涉及多个复杂的考量，而手动设计的奖励函数的理论和实证影响仍然探讨不足。本文旨在解决这一问题，提出了一种新的RL算法，以优化LLM的推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了分离组奖励优化（DGRO）算法，这是一种通用的RL算法，用于LLM推理任务。DGRO将传统的正则化系数分解为两个独立的超参数：一个用于缩放策略梯度项，另一个用于调节采样策略的距离。这种分离不仅能够精确控制探索和利用的平衡，还可以无缝扩展到Kimi k1.5中的在线策略镜像下降（OPMD）算法和直接奖励优化。

💡 创新点2
观察到奖励方差显著影响收敛速度和最终模型性能，本文进行了理论分析和广泛的实证验证，包括详细的消融研究，以评估DGRO的性能和优化动态。研究发现，适当的超参数组合和较高的奖励方差可以促进更快收敛和更好的最终准确度。

### 📈 实验结果
实验结果表明，DGRO在逻辑数据集上实现了平均96.9%的准确度，并在数学基准测试中展示了强大的泛化能力。这些结果证明了DGRO在LLM推理任务上的优越性能。

### 💬 可借鉴之处
本文提出的DGRO算法为LLM推理提供了一种新的优化方法，其理论分析和实验验证为探索-利用控制和奖励方差管理提供了实践指导。DGRO的成功表明，通过精细调整奖励函数和优化策略，可以显著提升LLM的推理能力和性能。此外，本文的理论分析为理解和设计更有效的奖励函数提供了重要见解。

## j4r--learning-to-judge-with-equivalent-initial-state-group-relative-policy-optimization
### Abstract
To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.
### 🌟 论文解读 | “J4R：利用等效初始状态组相对策略优化学习评判”

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）的发展速度不断加快，模型输出评价已经从耗时的人工评价转向自动评价。在自动评价中，LLM自身被用于评估和评判其他模型的输出。然而，现有的LLM作为评判者（LLM-as-judge）模型在处理相对简单的领域（如聊天质量）时表现出色，但在需要更多实质性内容和挑战性内容的推理密集型领域中却遇到了困难。为了解决现有评判者的不足，本文探索了使用强化学习（RL）来训练评判者。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了等效初始状态组相对策略优化（EIS-GRPO）算法，该算法允许训练评判者在面对更复杂的评价设置时对位置偏差具有鲁棒性。通过将输入上下文的不同转换视为等效，该算法在不增加额外训练负担的情况下，提高了评判者的一致性和鲁棒性。

💡 创新点2
本文引入了ReasoningJudgeBench，这是一个评判者在多样化推理设置中的评估基准，涵盖了之前工作中未涉及到的推理场景。这个基准包含了1483个具有挑战性的成对样本，为评判者的评估提供了更广泛的覆盖范围。

💡 创新点3
本文训练了Judge for Reasoning（J4R），一个使用EIS-GRPO算法训练的7B评判者模型。J4R在推理评价方面表现出色，超过了GPT-4o和下一个最佳小型评判者，性能分别提高了6.7%和9%，在JudgeBench和ReasoningJudgeBench上都匹配或超过了更大GRPO训练评判者的性能。

### 📈 实验结果
实验结果表明，使用EIS-GRPO算法训练的J4R模型在推理评价方面具有显著优势，不仅超过了GPT-4o，而且在JudgeBench和ReasoningJudgeBench上的表现与更大的GRPO训练评判者相当或更优。

### 💬 可借鉴之处
本文的研究为自动评价领域提供了新的视角和方法，特别是在推理密集型任务中。EIS-GRPO算法为训练鲁棒性强的评判者提供了一种有效的方法，而ReasoningJudgeBench则为评判者的评估提供了新的挑战和基准。此外，J4R模型的成功表明，即使是小型模型也可以通过适当的方法在推理评价中取得显著效果。这些发现对于未来的自动评价系统和LLM的发展都具有重要参考价值。

## an-empirical-study-on-reinforcement-learning-for-reasoning-search-interleaved-llm-agents
### Abstract
Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.
### 🌟 论文解读 | 探索强化学习在推理搜索混合型LLM代理中的应用

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在自然语言处理任务中的表现越来越出色，如何将这些模型训练成能够进行复杂推理并解决现实世界问题的智能代理成为研究的热点。近年来，强化学习（RL）被用于训练LLM，使其能够结合推理和搜索引擎使用，形成所谓的LLM-based搜索代理。然而，关于如何优化这些代理的设计，尤其是关于奖励设计、LLM的选择和搜索引擎在RL过程中的作用，目前仍缺乏深入的研究。本文旨在通过实证研究，提供关于这些关键因素的见解。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：奖励设计
本文发现，格式奖励（format rewards）对于提高最终性能非常有效，而中间检索奖励（intermediate retrieval rewards）的影响则有限。格式奖励有助于模型遵循代理行动的格式，从而提高性能。

💡 创新点2：LLM的选择和初始化
研究结果表明，LLM的规模和初始化（通用型与推理专用型）对RL的结果有显著影响。通用型LLM在RL设置中表现优于推理专用型LLM，可能是因为后者在训练初期阶段的指令遵循能力较弱。

💡 创新点3：搜索引擎的选择
搜索引擎的质量对RL的训练动态有强烈影响。使用非信息性搜索引擎（如随机噪声）会导致代理完全避免检索，而使用弱搜索引擎（如BM25）会导致频繁但效率低下的搜索调用。相比之下，强大的搜索引擎（如密集检索器）能够产生更稳定的训练效果。

### 📈 实验结果
实验结果表明，使用格式奖励的代理在性能上有显著提升，而中间检索奖励的加入并没有带来一致的改进。此外，通用型LLM在RL设置中表现更好，且模型规模的增加通常能提高最终性能，但回报递减。搜索引擎的选择对训练动态和代理的鲁棒性有重要影响，而在推理时，代理对不同的检索系统表现出一定的鲁棒性，强大的搜索引擎能够带来更好的下游性能。

### 💬 可借鉴之处
本文为构建和部署面向现实世界应用的LLM-based搜索代理提供了重要的指导原则。研究结果表明，合理的奖励设计、LLM的选择和搜索引擎的优化是成功训练搜索代理的关键因素。这些发现对于未来的研究和实践都具有重要的参考价值。代码和数据集已公开，便于社区进一步验证和扩展这些研究成果。

## gvpo--group-variance-policy-optimization-for-large-language-model-post-training
### Abstract
Post-training plays a crucial role in refining and aligning large language
models to meet specific tasks and human preferences. While recent advancements
in post-training techniques, such as Group Relative Policy Optimization (GRPO),
leverage increased sampling with relative reward scoring to achieve superior
performance, these methods often suffer from training instability that limits
their practical adoption. To address this challenge, we present Group Variance
Policy Optimization (GVPO). GVPO incorporates the analytical solution to
KL-constrained reward maximization directly into its gradient weights, ensuring
alignment with the optimal policy. The method provides intuitive physical
interpretations: its gradient mirrors the mean squared error between the
central distance of implicit rewards and that of actual rewards. GVPO offers
two key advantages: (1) it guarantees a unique optimal solution, exactly the
KL-constrained reward maximization objective, (2) it supports flexible sampling
distributions that avoids on-policy and importance sampling limitations. By
unifying theoretical guarantees with practical adaptability, GVPO establishes a
new paradigm for reliable and versatile LLM post-training.
### 🌟 论文解读 | GVPO：大规模语言模型后训练的新范式

### 📌 背景痛点/本文动机
大规模语言模型（LLM）在预训练阶段通过大量数据学习到了广泛的通用语言模式，但其实际应用和与人类价值观的契合度依赖于后训练的精细化调整。现有的后训练技术，如监督微调（SFT）和基于人类反馈的强化学习（RLHF），对于将模型适应特定应用和确保输出符合伦理、安全及用户中心标准至关重要。然而，最近的一些后训练方法，如群相对策略优化（GRPO），虽然通过增加采样和相对奖励评分来提升性能，但往往存在训练不稳定性问题，限制了其实际应用。本文旨在解决这一挑战，提出了群方差策略优化（GVPO）方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
GVPO方法将KL约束下的奖励最大化问题的解析解直接整合到梯度权重中，确保了与最优策略的对齐。这种方法提供了一个直观的物理解释：其梯度反映了隐含奖励的中心距离与实际奖励的中心距离之间的均方误差。

💡 创新点2
GVPO具有两个关键优势：（1）它保证了唯一的最优解，即KL约束下的奖励最大化目标；（2）它支持灵活的采样分布，避免了策略梯度方法中的on-policy和重要性采样的限制。这使得GVPO成为一个既可靠又灵活的LLM后训练新范式。

### 📈 实验结果
本文详细分析了GVPO的结构与传统的策略梯度强化学习方法之间的相似性，并通过实验证明了GVPO在保证理论最优解的同时，能够支持灵活的数据采样，避免了梯度爆炸风险，且不引入通过剪辑技术产生的偏差。

### 💬 可借鉴之处
GVPO方法为大规模语言模型的后训练提供了一种新的思路，特别是在处理训练不稳定性方面具有显著优势。其理论上的保证和实际应用的灵活性，为后续的研究和应用提供了宝贵的参考。此外，GVPO对于理解奖励模型与策略之间的关系也提供了新的视角。

## tool-star--empowering-llm-brained-multi-tool-reasoner-via-reinforcement-learning
### Abstract
Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.
### 🌟 论文解读 | Tool-Star：通过强化学习赋能LLM的多工具协同推理

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLM）通过大规模强化学习（RL）展现出显著的推理能力。然而，如何利用RL算法有效赋能LLM进行多工具协同推理仍然是一个开放性的挑战。在实际世界中，推理场景往往需要模型整合多种能力，例如深度信息搜索、长期知识记忆和精确计算，而现有的工具集成推理（TIR）方法主要关注单一工具的使用，对多工具协同推理的研究还相对较少。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了Tool-Star，一个基于强化学习的框架，旨在使LLM能够在逐步推理过程中自主调用多个外部工具。Tool-Star集成了六种类型的工具，并在数据合成和训练算法中采用了系统化设计。

💡 创新点2
为了解决工具使用数据的稀缺问题，本文设计了一个通用的TIR数据合成管道，该管道结合了工具集成提示和基于提示的采样，自动生成大规模的工具使用轨迹。此外，引入了质量归一化和难度感知分类过程，以过滤掉不合理的工具使用样本，并将数据集从易到难组织起来。

💡 创新点3
本文提出了一个两阶段的TIR训练框架，以增强LLM的多工具协同推理能力。第一阶段是冷启动微调，引导LLM通过工具调用反馈探索推理模式；第二阶段是多层次奖励设计的多工具自我批评强化学习算法，强化LLM对奖励原则的理解并促进有效的工具协作。

### 📈 实验结果
在超过10个具有挑战性的推理基准测试中，Tool-Star展现出了其有效性和效率。实验结果表明，Tool-Star在保证工具使用效率的同时，还能确保推理的准确性和可靠性。

### 💬 可借鉴之处
本文提出的Tool-Star框架为LLM的多工具协同推理提供了一种新的方法，其创新点包括：
- 设计了一个能够自动生成大规模工具使用轨迹的数据合成管道。
- 提出了一个两阶段的训练框架，有效提升了多工具协同推理的能力。
- 引入了多层次奖励机制，强化了模型对奖励原则的理解，促进了工具之间的有效协作。
这些方法对于理解和优化LLM的推理过程具有重要的参考价值。

## distilling-the-implicit-multi-branch-structure-in-llms--reasoning-via-reinforcement-learning
### Abstract
Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.
### 🌟 论文解读 | 深度解析：如何通过强化学习提炼大型语言模型中的隐式多分支推理结构

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在复杂推理任务中表现出色，如何将这些模型的推理能力传递给较小的LLM成为了一个挑战。传统的监督微调（SFT）方法虽然能够提高学生模型的推理能力，但它往往只能复制教师模型的表面推理路径，无法捕捉到其背后的真实推理过程。真实推理涉及到在多个候选子问题中选择适当的问题（元推理）和解决该问题（解决问题），形成了一个隐式的多分支结构。SFT方法无法提炼这种结构，导致学生模型无法进行真正的自主推理。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文提出了一种名为RLKD的强化学习（RL）基于的知识提炼框架，旨在解决SFT方法无法提炼LLM隐式多分支推理结构的问题。通过引入人类认知神经科学的观点，将真实推理分为元推理和解决问题两个阶段。

💡 创新点2
本文设计了一种新颖的生成结构奖励模型（GSRM），它可以将推理路径转换为多个元推理-解决问题步骤的序列，并计算学生模型与教师模型推理结构之间的匹配度作为奖励。结合RL，RLKD能够指导学生LLM在步骤级别上进行更好的采样，从而选择最合适的子问题并解决它。

### 📈 实验结果
实验结果表明，即使在只有0.1%训练数据的情况下，RLKD也能在Qwen2.5-Math任务中超越传统的SFT-RL流程。此外，RLKD还能够进一步释放学生LLM的推理潜力，比基于SFT的提炼方法表现得更好，并且超过了现有的RL基线。

### 💬 可借鉴之处
本文的研究表明，通过强化学习可以更有效地提炼和传递LLM的推理结构，这对于开发资源受限的团队来说是一个重要的启示。此外，GSRM的设计为如何评估和优化推理路径的结构提供了新的思路，这对于未来的LLM研究和应用具有借鉴意义。

## streamrl--scalable--heterogeneous--and-elastic-rl-for-llms-with-disaggregated-stream-generation
### Abstract
Reinforcement learning (RL) has become the core post-training technique for
large language models (LLMs). RL for LLMs involves two stages: generation and
training. The LLM first generates samples online, which are then used to derive
rewards for training. The conventional view holds that the colocated
architecture, where the two stages share resources via temporal multiplexing,
outperforms the disaggregated architecture, in which dedicated resources are
assigned to each stage. However, in real-world deployments, we observe that the
colocated architecture suffers from resource coupling, where the two stages are
constrained to use the same resources. This coupling compromises the
scalability and cost-efficiency of colocated RL in large-scale training. In
contrast, the disaggregated architecture allows for flexible resource
allocation, supports heterogeneous training setups, and facilitates
cross-datacenter deployment.
  StreamRL is designed with disaggregation from first principles and fully
unlocks its potential by addressing two types of performance bottlenecks in
existing disaggregated RL frameworks: pipeline bubbles, caused by stage
dependencies, and skewness bubbles, resulting from long-tail output length
distributions. To address pipeline bubbles, StreamRL breaks the traditional
stage boundary in synchronous RL algorithms through stream generation and
achieves full overlapping in asynchronous RL. To address skewness bubbles,
StreamRL employs an output-length ranker model to identify long-tail samples
and reduces generation time via skewness-aware dispatching and scheduling.
Experiments show that StreamRL improves throughput by up to 2.66x compared to
existing state-of-the-art systems, and improves cost-effectiveness by up to
1.33x in a heterogeneous, cross-datacenter setting.
### 🌟 论文解读 | StreamRL：面向大规模语言模型的弹性强化学习框架

### 📌 背景痛点/本文动机
随着强化学习（RL）成为提升大型语言模型（LLM）推理能力的关键后训练技术，传统的RL训练框架主要采用两种架构：集中式（colocated）和分布式（disaggregated）。集中式架构通过时间复用共享资源，而分布式架构为每个阶段分配专用的资源。尽管分布式架构在理论上具有资源分配的灵活性，但在实际部署中，传统的分布式框架存在资源闲置和长尾分布问题，导致资源利用不充分。

本文提出了StreamRL，一种针对LLM的分布式强化学习框架，旨在解决现有框架中的性能瓶颈问题，提高资源利用率和训练效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
StreamRL从原理上采用分布式架构，通过抽象出生成服务（SGS）和训练器（Trainer）两个阶段，使得Trainer可以流式地接收SGS生成的样本，从而减少资源闲置，提高并发执行效率。

💡 创新点2
为了解决长尾分布问题，StreamRL引入了一个输出长度排序模型来识别长尾样本，并通过偏斜感知的调度机制，有选择地为长尾样本分配资源，从而减少生成时间并提高整体效率。

### 📈 实验结果
实验表明，StreamRL在吞吐量上比现有最先进的系统提高了2.66倍，在异构、跨数据中心的设置中，成本效益提高了1.33倍。

### 💬 可借鉴之处
StreamRL的设计理念和方法为大规模语言模型的强化学习训练提供了新的视角，特别是在分布式架构下如何有效管理资源分配和调度。此外，其提出的动态资源调整机制为保持训练过程中各阶段平衡执行提供了实用的解决方案。这篇论文对于希望在强化学习领域进行深入研究的学者和工程师来说，具有很高的参考价值。

## ufo-rl--uncertainty-focused-optimization-for-efficient-reinforcement-learning-data-selection
### Abstract
Scaling RL for LLMs is computationally expensive, largely due to
multi-sampling for policy optimization and evaluation, making efficient data
selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,
we hypothesize LLMs learn best from data within their potential comprehension
zone. Addressing the limitation of conventional, computationally intensive
multi-sampling methods for data assessment, we introduce UFO-RL. This novel
framework uses a computationally efficient single-pass uncertainty estimation
to identify informative data instances, achieving up to 185x faster data
evaluation. UFO-RL leverages this metric to select data within the estimated
ZPD for training. Experiments show that training with just 10% of data selected
by UFO-RL yields performance comparable to or surpassing full-data training,
reducing overall training time by up to 16x while enhancing stability and
generalization. UFO-RL offers a practical and highly efficient strategy for
scaling RL fine-tuning of LLMs by focusing learning on valuable data.
### 🌟 论文解读 | "UFO-RL：聚焦不确定性的高效强化学习数据选择策略"

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在强化学习（RL）中的应用，一个主要的挑战是高昂的计算成本，尤其是在策略优化和评估中需要多次采样。这种计算上的负担使得高效的数据选择变得至关重要。本文的动机在于提出一种新的数据选择策略，以降低计算成本并提高学习效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1
本文受到“最近发展区”（Zone of Proximal Development, ZPD）理论的启发，假设LLM从它们尚未完全掌握但具有潜在理解能力的数据中学习效果最佳。这一假设为高效的数据选择提供了理论基础。

💡 创新点2
为了克服传统方法中计算成本高的缺点，本文提出了UFO-RL（Uncertainty-Focused Optimization for Reinforcement Learning）框架。该框架采用一种计算效率高的单次遍历不确定性估计技术，以识别训练实例中的信息性数据。这种方法仅需一次前向传播，避免了迭代计算下一个标记的需要，从而将数据评估的速度提高了高达185倍。

### 📈 实验结果
本文在多个数学推理基准和不同规模的语言模型架构上进行了广泛的实证验证。结果显示，通过精心选择仅10%的数据，UFO-RL所需的计算资源不到全数据训练的1/16，却实现了与全数据基线相当甚至更好的性能。此外，UFO-RL还展示了增强的训练稳定性和改进的泛化能力。

### 💬 可借鉴之处
本文提出的方法为大规模LLM的RL微调提供了一种实用且高效的策略，通过关注最有价值和信息性的数据，减轻了传统RL训练中的计算瓶颈。UFO-RL不仅提高了训练效率，还通过聚焦于模型不确定性较高的数据，增强了模型的泛化能力，这对于未来的RL研究和应用具有很高的参考价值。

