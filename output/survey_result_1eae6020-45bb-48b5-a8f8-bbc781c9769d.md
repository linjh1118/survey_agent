# Paper List of Terms(VLM+GRPO)
- [25/05] **Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.14677v1)] [[Code/Page]()] [[TLDR/Notes](#visionary-r1--mitigating-shortcuts-in-visual-reasoning-with-reinforcement-learning)]

- [25/05] **FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.12835v1)] [[Code/Page]()] [[TLDR/Notes](#flightgpt--towards-generalizable-and-interpretable-uav-vision-and-language-navigation-with-vision-language-models)]

- [25/04] **LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.19524v1)] [[Code/Page](https://github.com/LilaKen/LR-IAD.)] [[TLDR/Notes](#lr-iad-mask-free-industrial-anomaly-detection-with-logical-reasoning)]

- [25/04] **AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection**  
[[Paper](http://arxiv.org/pdf/2504.11914v1)] [[Code/Page]()] [[TLDR/Notes](#anomalyr1--a-grpo-based-end-to-end-mllm-for-industrial-anomaly-detection)]

- [25/04] **PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks**  
[[Paper](http://arxiv.org/pdf/2504.09258v2)] [[Code/Page]()] [[TLDR/Notes](#pathvlm-r1--a-reinforcement-learning-driven-reasoning-model-for-pathology-visual-language-tasks)]

- [25/03] **Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning**  
[[Paper](http://arxiv.org/pdf/2503.20752v2)] [[Code/Page](https://tanhuajie.github.io/ReasonRFT)] [[TLDR/Notes](#reason-rft--reinforcement-fine-tuning-for-visual-reasoning)]



# TLDR/Notes
## visionary-r1--mitigating-shortcuts-in-visual-reasoning-with-reinforcement-learning
### Abstract
Learning general-purpose reasoning capabilities has long been a challenging
problem in AI. Recent research in large language models (LLMs), such as
DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can
enable pre-trained LLMs to develop reasoning capabilities using simple
question-answer pairs. In this paper, we aim to train visual language models
(VLMs) to perform reasoning on image data through reinforcement learning and
visual question-answer pairs, without any explicit chain-of-thought (CoT)
supervision. Our findings indicate that simply applying reinforcement learning
to a VLM -- by prompting the model to produce a reasoning chain before
providing an answer -- can lead the model to develop shortcuts from easy
questions, thereby reducing its ability to generalize across unseen data
distributions. We argue that the key to mitigating shortcut learning is to
encourage the model to interpret images prior to reasoning. Therefore, we train
the model to adhere to a caption-reason-answer output format: initially
generating a detailed caption for an image, followed by constructing an
extensive reasoning chain. When trained on 273K CoT-free visual question-answer
pairs and using only reinforcement learning, our model, named Visionary-R1,
outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and
Gemini-1.5-Pro, on multiple visual reasoning benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "Visionary-R1ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å‰Šå¼±è§†è§‰æ¨ç†ä¸­çš„æ·å¾„å­¦ä¹ "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é•¿ä¹…ä»¥æ¥ï¼Œè®©AIæ¨¡å‹å…·å¤‡é€šç”¨æ¨ç†èƒ½åŠ›ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªéš¾é¢˜ã€‚å°½ç®¡æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚DeepSeek-R1çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å¦‚GRPOï¼Œå¯ä»¥ä½¿å¾—é¢„è®­ç»ƒçš„LLMsé€šè¿‡ç®€å•çš„é—®é¢˜-ç­”æ¡ˆå¯¹å‘å±•å‡ºæ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸Šè¿›è¡Œæ¨ç†ï¼Œå°¤å…¶æ˜¯æ²¡æœ‰æ˜¾å¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å½“å‰ç ”ç©¶ä¸­çš„è§†è§‰æ¨ç†æ¨¡å‹å¾€å¾€ä¾èµ–äºå¤æ‚çš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œè¿™äº›æµç¨‹æ—¢è®¡ç®—æ˜‚è´µåˆè€—æ—¶ï¼Œå¹¶ä¸”è¿™äº›æ¨¡å‹é«˜åº¦ä¾èµ–ä»ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oä¸­æå–çš„æ ‡è®°é“¾å¼æ€ç»´æ¨ç†æ•°æ®ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§å’Œå¼€æ”¾æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œé…å¯¹çš„è§†è§‰é—®é¢˜-ç­”æ¡ˆæ•°æ®ï¼Œæ¥é™ä½è®­ç»ƒVLMsè¿›è¡Œè§†è§‰æ¨ç†çš„å¼€å‘æˆæœ¬ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†Visionary-R1ï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨æ¨ç†ä¹‹å‰å¼ºåˆ¶è¿›è¡Œè§†è§‰ç†è§£æ¥å‰Šå¼±æ·å¾„å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹è¢«è®­ç»ƒéµå¾ªä¸€ä¸ªç»“æ„åŒ–çš„â€œæè¿°-æ¨ç†-å›ç­”â€è¾“å‡ºæ ¼å¼ï¼Œé¦–å…ˆç”Ÿæˆå›¾åƒçš„è¯¦ç»†æè¿°ï¼Œç„¶åè¿›è¡Œæ¨ç†å¹¶ç»™å‡ºç­”æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†ç¡®ä¿æè¿°çš„å‡†ç¡®æ€§ï¼Œç ”ç©¶è€…åœ¨æè¿°ä»¤ç‰Œä¸Šæ–½åŠ äº†è¾…åŠ©ç›‘ç£ï¼Œé€šè¿‡ä»AIåé¦ˆä¸­å­¦ä¹ å¼ºåŒ–å­¦ä¹ ã€‚è¿™ç§æè¿°å¥–åŠ±ä¸æ ‡å‡†çš„å‡†ç¡®æ€§å’Œæ ¼å¼å¥–åŠ±ç›¸ç»“åˆï¼Œç”¨äºç­–ç•¥ä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹äº§ç”Ÿçš„æ¨ç†ä»¤ç‰Œæ›´é•¿ã€æ›´æœ‰æ„ä¹‰ï¼Œä»è€Œåœ¨æœªè§æ•°æ®ä¸Šå±•ç°å‡ºæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ç ”ç©¶è€…ç¼–è¯‘äº†ä¸€ä¸ªåŒ…å«11ä¸ªæµè¡Œé—®é¢˜-ç­”æ¡ˆæ•°æ®é›†çš„ç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–äº†åœºæ™¯ç†è§£ã€å›¾è¡¨åˆ†æã€æ•°å­¦é—®é¢˜è§£å†³å’Œæ–‡æ¡£å¤„ç†ç­‰é¢†åŸŸï¼Œæ€»è®¡272.6Kä¸ªæ— CoTçš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚è®­ç»ƒåï¼ŒVisionary-R1åœ¨MathVistaã€MathVisionã€MMStarå’ŒMMBenchç­‰å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒVisionary-R1ä¼˜äºGPT-4oã€Claude3.5-Sonnetå’ŒGemini-1.5-Proç­‰å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ä»…æ­ç¤ºäº†GRPOç›´æ¥åº”ç”¨äºVLMsæ—¶ç”±äºæ·å¾„å­¦ä¹ è€Œå¤±æ•ˆçš„é—®é¢˜ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰å…ˆè¿›è¡Œå›¾åƒç†è§£ï¼Œä»è€Œæé«˜äº†æ¨ç†èƒ½åŠ›çš„æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®éªŒè¯æ˜äº†å³ä½¿ä»…ä½¿ç”¨é—®é¢˜-ç­”æ¡ˆå¯¹ï¼ŒVisionary-R1ä¹Ÿèƒ½åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨ç†åŸºå‡†ä¸Šå‡»è´¥å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œè¿™ä¸ºæœªæ¥è§†è§‰æ¨ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚

## flightgpt--towards-generalizable-and-interpretable-uav-vision-and-language-navigation-with-vision-language-models
### Abstract
Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital
for applications such as disaster response, logistics delivery, and urban
inspection. However, existing methods often struggle with insufficient
multimodal fusion, weak generalization, and poor interpretability. To address
these challenges, we propose FlightGPT, a novel UAV VLN framework built upon
Vision-Language Models (VLMs) with powerful multimodal perception capabilities.
We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)
using high-quality demonstrations to improve initialization and structured
reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by
a composite reward that considers goal accuracy, reasoning quality, and format
compliance, to enhance generalization and adaptability. Furthermore, FlightGPT
introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve
decision interpretability. Extensive experiments on the city-scale dataset
CityNav demonstrate that FlightGPT achieves state-of-the-art performance across
all scenarios, with a 9.22\% higher success rate than the strongest baseline in
unseen environments. Our implementation is publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | FlightGPTï¼šè¿ˆå‘é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§çš„æ— äººæœºè§†è§‰ä¸è¯­è¨€å¯¼èˆª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ— äººæœºè§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆUAV VLNï¼‰åœ¨ç¾éš¾å“åº”ã€ç‰©æµé…é€å’ŒåŸå¸‚æ£€æŸ¥ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€èåˆä¸è¶³ã€æ³›åŒ–èƒ½åŠ›å¼±å’Œå†³ç­–å¯è§£é‡Šæ€§å·®ç­‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†FlightGPTï¼Œä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ— äººæœºVLNæ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
FlightGPTé‡‡ç”¨åŸºäºVLMsçš„å¤šæ¨¡æ€èåˆï¼Œæœ‰æ•ˆæ•´åˆè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¢å¼ºå¤šæ¨¡æ€æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šé¦–å…ˆæ˜¯ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåˆ©ç”¨é«˜è´¨é‡æ¼”ç¤ºæ¥ä¼˜åŒ–åˆå§‹åŒ–å’Œç»“æ„åŒ–æ¨ç†ï¼›ç„¶åæ˜¯ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œé€šè¿‡è€ƒè™‘ç›®æ ‡å‡†ç¡®æ€§ã€æ¨ç†è´¨é‡å’Œæ ¼å¼ç¬¦åˆæ€§çš„å¤åˆå¥–åŠ±æ¥å¢å¼ºæ³›åŒ–å’Œé€‚åº”æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
å¼•å…¥äº†åŸºäºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰çš„æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡æ˜¾å¼çš„â€œæ€è€ƒâ€/â€œå›ç­”â€æ ‡ç­¾å½¢æˆCoTæ¨ç†è¿‡ç¨‹ï¼Œæé«˜å†³ç­–çš„å¯è§£é‡Šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤§å‹åŸå¸‚ç¯å¢ƒåŸºå‡†CityNavä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFlightGPTåœ¨æ‰€æœ‰åœºæ™¯ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”åœ¨æœªè§ç¯å¢ƒä¸­è¡¨ç°æœ€å¼ºçš„åŸºçº¿é«˜å‡º9.22%çš„æˆåŠŸç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
FlightGPTçš„æå‡ºä¸ºæ— äººæœºVLNé¢†åŸŸå¸¦æ¥äº†æ–°çš„è§†è§’ï¼Œå…¶åˆ›æ–°çš„å¤šæ¨¡æ€èåˆç­–ç•¥ã€ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ä»¥åŠCoTæ¨ç†æœºåˆ¶ä¸ºæé«˜æ— äººæœºå¯¼èˆªç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ã€é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡åœ¨çœŸå®åŸå¸‚ç¯å¢ƒåŸºå‡†ä¸Šçš„å®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## lr-iad-mask-free-industrial-anomaly-detection-with-logical-reasoning
### Abstract
Industrial Anomaly Detection (IAD) is critical for ensuring product quality
by identifying defects. Traditional methods such as feature embedding and
reconstruction-based approaches require large datasets and struggle with
scalability. Existing vision-language models (VLMs) and Multimodal Large
Language Models (MLLMs) address some limitations but rely on mask annotations,
leading to high implementation costs and false positives. Additionally,
industrial datasets like MVTec-AD and VisA suffer from severe class imbalance,
with defect samples constituting only 23.8% and 11.1% of total data
respectively. To address these challenges, we propose a reward function that
dynamically prioritizes rare defect patterns during training to handle class
imbalance. We also introduce a mask-free reasoning framework using Chain of
Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms,
enabling anomaly detection directly from raw images without annotated masks.
This approach generates interpretable step-by-step explanations for defect
localization. Our method achieves state-of-the-art performance, outperforming
prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating
mask dependency and reducing costs while providing explainable outputs, this
work advances industrial anomaly detection and supports scalable quality
control in manufacturing. Code to reproduce the experiment is available at
https://github.com/LilaKen/LR-IAD.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— é®æŒ¡å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„é€»è¾‘æ¨ç†æ–°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰å¯¹äºç¡®ä¿äº§å“è´¨é‡è‡³å…³é‡è¦ï¼Œå®ƒé€šè¿‡è¯†åˆ«ç¼ºé™·æ¥ä¿éšœäº§å“å“è´¨ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚ç‰¹å¾åµŒå…¥å’Œé‡å»º-basedæ–¹æ³•éœ€è¦å¤§é‡æ•°æ®é›†ï¼Œå¹¶ä¸”éš¾ä»¥æ‰©å±•ã€‚ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è™½ç„¶è§£å†³äº†ä¸€äº›é™åˆ¶ï¼Œä½†ä¾èµ–äºæ©ç æ³¨é‡Šï¼Œå¯¼è‡´å®æ–½æˆæœ¬é«˜æ˜‚å’Œè¯¯æŠ¥ç‡é«˜ã€‚æ­¤å¤–ï¼Œå·¥ä¸šæ•°æ®é›†å¦‚MVTec-ADå’ŒVisAå­˜åœ¨ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç¼ºé™·æ ·æœ¬åˆ†åˆ«åªå æ€»æ•°æ®çš„23.8%å’Œ11.1%ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±å‡½æ•°ï¼ŒåŠ¨æ€ä¼˜å…ˆå¤„ç†è®­ç»ƒä¸­çš„ç½•è§ç¼ºé™·æ¨¡å¼ï¼Œä»¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€ä¼˜å…ˆå¤„ç†ç½•è§çš„ç¼ºé™·æ¨¡å¼ï¼Œæœ‰æ•ˆè§£å†³äº†å·¥ä¸šæ•°æ®é›†ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œé¿å…äº†è¿‡åº¦æ‹Ÿåˆåˆ°ä¸»è¦ç±»åˆ«ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ— é®æŒ¡æ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æœºåˆ¶ï¼Œä½¿å¼‚å¸¸æ£€æµ‹èƒ½å¤Ÿç›´æ¥ä»åŸå§‹å›¾åƒè¿›è¡Œï¼Œæ— éœ€æ³¨é‡Šçš„æ©ç ã€‚è¿™ç§æ–¹æ³•ä¸ºç¼ºé™·å®šä½ç”Ÿæˆäº†å¯è§£é‡Šçš„é€æ­¥è§£é‡Šã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡çš„æ–¹æ³•åœ¨MVTec-ADæ•°æ®é›†ä¸Šæ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•æé«˜äº†36%çš„å‡†ç¡®æ€§ï¼Œåœ¨VisAæ•°æ®é›†ä¸Šæé«˜äº†16%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— é®æŒ¡æ¨ç†å’Œæä¾›å¯è§£é‡Šè¾“å‡ºæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹å’Œåˆ¶é€ è´¨é‡æ§åˆ¶çš„æ‰©å±•æä¾›äº†æ”¯æŒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•ä¸ä»…æé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œè¿˜é€šè¿‡æ¶ˆé™¤å¯¹æ©ç çš„ä¾èµ–å’Œé™ä½æˆæœ¬ï¼Œä¸ºå·¥ä¸šè´¨é‡æ§åˆ¶åœ¨å®é™…åº”ç”¨ä¸­æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å¯è§£é‡Šè¾“å‡ºå¢å¼ºäº†æ¨¡å‹å†³ç­–çš„é€æ˜åº¦ï¼Œæœ‰åŠ©äºæé«˜ç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ã€‚è®ºæ–‡ä¸­æå‡ºçš„åŠ¨æ€ä¼˜å…ˆå¤„ç†ç½•è§ç¼ºé™·æ¨¡å¼çš„å¥–åŠ±å‡½æ•°å’ŒåŸºäºé“¾å¼æ€ç»´çš„æ— é®æŒ¡æ¨ç†æ¡†æ¶ï¼Œå¯¹äºè§£å†³å·¥ä¸šæ•°æ®é›†ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å’Œé™ä½å®æ–½æˆæœ¬å…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## anomalyr1--a-grpo-based-end-to-end-mllm-for-industrial-anomaly-detection
### Abstract
Industrial Anomaly Detection (IAD) poses a formidable challenge due to the
scarcity of defective samples, making it imperative to deploy models capable of
robust generalization to detect unseen anomalies effectively. Traditional
approaches, often constrained by hand-crafted features or domain-specific
expert models, struggle to address this limitation, underscoring the need for a
paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages
VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional
generalization and interpretability, to revolutionize IAD. By integrating MLLM
with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned
Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution
that autonomously processes inputs of image and domain knowledge, reasons
through analysis, and generates precise anomaly localizations and masks. Based
on the latest multimodal IAD benchmark, our compact 3-billion-parameter model
outperforms existing methods, establishing state-of-the-art results. As MLLM
capabilities continue to advance, this study is the first to deliver an
end-to-end VLM-based IAD solution that demonstrates the transformative
potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking
cornerstone for next-generation intelligent anomaly detection systems in
industrial applications with limited defective data.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "AnomalyR1ï¼šå¼•é¢†å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–°ç¯‡ç« "

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºç¼ºé™·æ ·æœ¬çš„ç¨€ç¼ºæ€§ï¼Œè¿™è¦æ±‚æ¨¡å‹å¿…é¡»å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥æœ‰æ•ˆæ£€æµ‹æœªè§è¿‡çš„å¼‚å¸¸ã€‚ä¼ ç»Ÿçš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šå¸¸å—é™äºæ‰‹å·¥ç‰¹å¾æˆ–ç‰¹å®šé¢†åŸŸçš„ä¸“å®¶æ¨¡å‹ï¼Œéš¾ä»¥çªç ´è¿™ä¸€å±€é™ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AnomalyR1æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å“è¶Šæ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§ï¼Œä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹å¸¦æ¥é©å‘½æ€§çš„æ”¹å˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
AnomalyR1æ¡†æ¶åŸºäºVLM-R1ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥å…¶å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§è€Œé—»åã€‚é€šè¿‡å°†MLLMä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥æ–°é¢–çš„ç†ç”±ç»“æœå¯¹é½åº¦é‡ï¼ˆROAMï¼‰ï¼ŒAnomalyR1å®ç°äº†ä¸€ä¸ªå®Œå…¨ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿè‡ªä¸»å¤„ç†å›¾åƒå’Œé¢†åŸŸçŸ¥è¯†è¾“å…¥ï¼Œè¿›è¡Œæ¨ç†åˆ†æï¼Œå¹¶ç”Ÿæˆç²¾ç¡®çš„å¼‚å¸¸å®šä½å’Œæ©ç ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡æå‡ºçš„ROAMæ˜¯ä¸€ç§åˆ›æ–°çš„å¥–åŠ±æ¡†æ¶ï¼Œå®ƒä¸“é—¨ä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹ä»»åŠ¡è®¾è®¡ï¼Œèƒ½å¤Ÿè¿­ä»£åœ°å¼•å¯¼æ¨¡å‹é€šè¿‡å·¥ä¸šäº§å“å›¾åƒçš„æ¨ç†è¿‡ç¨‹ï¼Œé€æ­¥æé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒAnomalyR1åœ¨æœ€æ–°çš„å¤šæ¨¡æ€IADåŸºå‡†æµ‹è¯•MMADä¸Šå–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„å¤šæ¨¡æ€æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AnomalyR1æ¡†æ¶åœ¨MMADåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„30äº¿å‚æ•°æ¨¡å‹è¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•ï¼Œç¡®ç«‹äº†å…¶åœ¨å·¥ä¸šå¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„é¢†å…ˆåœ°ä½ã€‚è¯¥æ¡†æ¶åœ¨æ•°æ®ç¨€ç¼ºçš„æ¡ä»¶ä¸‹ï¼Œé€šè¿‡ä»…ä½¿ç”¨2-5ä¸ªæ¯ç±»çš„å·¥ä¸šç‰©å“å›¾åƒå³å¯å®ç°è¶³å¤Ÿæ€§èƒ½çš„é²æ£’å°‘é‡æ ·æœ¬å­¦ä¹ ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹é¢†åŸŸæä¾›äº†ä»¥ä¸‹å‡ ä¸ªå¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. åˆ©ç”¨MLLMè¿›è¡Œç«¯åˆ°ç«¯çš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼Œè¿™æ˜¯ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„æ–¹æ³•ï¼Œä¸ä¼ ç»Ÿçš„ä¾èµ–å¤æ‚ä¸“å®¶æ¨¡å‹æˆ–é¢„è®­ç»ƒè§†è§‰æ–‡æœ¬ç¼–ç å™¨çš„æ–¹æ³•æœ‰æ˜¾è‘—åŒºåˆ«ã€‚
2. é€šè¿‡GRPOå¢å¼ºçš„VLM-R1æ„å»ºçš„AnomalyR1æ¡†æ¶ï¼Œä¸ºæ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹çš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚
3. ROAMå¥–åŠ±æ¡†æ¶çš„åˆ›æ–°è®¾è®¡ï¼Œä¸ºæ”¹è¿›GRPOè®­ç»ƒæ–¹æ³•åœ¨å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½æä¾›äº†é‡è¦æ”¯æŒã€‚
4. åœ¨æœ€æ–°çš„å¤šæ¨¡æ€IADåŸºå‡†æµ‹è¯•ä¸­å–å¾—SOTAæ€§èƒ½ï¼Œå±•ç¤ºäº†AnomalyR1æ¡†æ¶åœ¨å¤šæ¨¡æ€æ¨ç†å’Œæ³›åŒ–æ–¹é¢çš„æ½œåŠ›ã€‚

## pathvlm-r1--a-reinforcement-learning-driven-reasoning-model-for-pathology-visual-language-tasks
### Abstract
The diagnosis of pathological images is often limited by expert availability
and regional disparities, highlighting the importance of automated diagnosis
using Vision-Language Models (VLMs). Traditional multimodal models typically
emphasize outcomes over the reasoning process, compromising the reliability of
clinical decisions. To address the weak reasoning abilities and lack of
supervised processes in pathological VLMs, we have innovatively proposed
PathVLM-R1, a visual language model designed specifically for pathological
images. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its
performance for pathological tasks through meticulously designed post-training
strategies. Firstly, we conduct supervised fine-tuning guided by pathological
data to imbue the model with foundational pathological knowledge, forming a new
pathological base model. Subsequently, we introduce Group Relative Policy
Optimization (GRPO) and propose a dual reward-driven reinforcement learning
optimization, ensuring strict constraint on logical supervision of the
reasoning process and accuracy of results via cross-modal process reward and
outcome accuracy reward. In the pathological image question-answering tasks,
the testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy
compared to baseline methods, and it demonstrated superior performance compared
to the Qwen2.5-VL-32B version despite having a significantly smaller parameter
size. Furthermore, in out-domain data evaluation involving four medical imaging
modalities: Computed Tomography (CT), dermoscopy, fundus photography, and
Optical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance
improved by an average of 17.3% compared to traditional SFT methods. These
results clearly indicate that PathVLM-R1 not only enhances accuracy but also
possesses broad applicability and expansion potential.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PathVLM-R1ï¼šæ¨åŠ¨ç—…ç†è§†è§‰è¯­è¨€ä»»åŠ¡æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŒ»å­¦ç—…ç†è¯Šæ–­ä¸­ï¼Œå‡†ç¡®åˆ†æç—…ç†åˆ‡ç‰‡å¯¹äºä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¸“ä¸šç—…ç†å­¦å®¶çš„æ°´å¹³å‚å·®ä¸é½ï¼Œä¸”è¯Šæ–­éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œå¯¼è‡´åŒ»ç–—äººåŠ›èµ„æºå‹åŠ›å¢å¤§ã€‚ä¸ºäº†ç¼“è§£åœ°åŒºé—´çš„è¯Šæ–­å’Œæ²»ç–—æ°´å¹³å·®å¼‚ï¼Œç ”ç©¶äººå‘˜å¼€å§‹å¼€å‘è§†è§‰è¯­è¨€æ¨¡å‹ä»¥è¾…åŠ©ç—…ç†å›¾åƒçš„è¯Šæ–­ã€‚å½“å‰ï¼Œå¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹ä¾èµ–äºç«¯åˆ°ç«¯çš„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼Œè¿™äº›æ¨¡å‹å¾€å¾€å€¾å‘äºè®°å¿†å’Œæ¨¡ä»¿æ•°æ®ä¸­çš„æ¨¡å¼ï¼Œç¼ºä¹é€æ˜å’Œå¯ä¿¡çš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†PathVLM-R1ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºç—…ç†å›¾åƒè®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åŸºäºQwen2.5-VL-7B-Instructï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒç­–ç•¥æå‡äº†å…¶åœ¨ç—…ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œé€šè¿‡ç—…ç†æ•°æ®æŒ‡å¯¼çš„ç›‘ç£å¾®è°ƒï¼Œä½¿æ¨¡å‹å…·å¤‡äº†åŸºæœ¬çš„ç—…ç†çŸ¥è¯†ï¼Œå½¢æˆäº†æ–°çš„ç—…ç†åŸºç¡€æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡å¼•å…¥äº†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¹¶æå‡ºäº†ä¸€ç§åŒå¥–åŠ±é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è·¨æ¨¡æ€è¿‡ç¨‹å¥–åŠ±å’Œç»“æœå‡†ç¡®åº¦å¥–åŠ±ï¼Œä¸¥æ ¼çº¦æŸæ¨ç†è¿‡ç¨‹çš„é€»è¾‘ç›‘ç£å’Œç»“æœçš„å‡†ç¡®æ€§ï¼Œå½¢æˆäº†ä¸€ä¸ªâ€œè¿‡ç¨‹-ç»“æœâ€çš„é—­ç¯ç›‘ç£ã€‚æœ€ç»ˆï¼Œé€šè¿‡GRPOå®ç°äº†è·¨æ¨¡æ€è¿‡ç¨‹å¥–åŠ±å’Œç»“æœå‡†ç¡®åº¦å¥–åŠ±çš„æ•´åˆï¼Œç¡®ä¿äº†è¿‡ç¨‹åˆç†æ€§å’Œç»“æœå‡†ç¡®æ€§çš„å¹³è¡¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç—…ç†å›¾åƒé—®ç­”ä»»åŠ¡ä¸­ï¼ŒPathVLM-R1çš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå‡†ç¡®åº¦æé«˜äº†14%ã€‚æ­¤å¤–ï¼Œå°½ç®¡å‚æ•°è§„æ¨¡è¾ƒå°ï¼ŒPathVLM-R1çš„æ€§èƒ½ä»ä¼˜äºQwen2.5-VL-32Bç‰ˆæœ¬ã€‚åœ¨æ¶‰åŠå››ç§åŒ»å­¦æˆåƒæ¨¡æ€ï¼ˆè®¡ç®—æœºæ–­å±‚æ‰«æã€çš®è‚¤é•œæ£€æŸ¥ã€çœ¼åº•æ‘„å½±å’Œå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æå›¾åƒï¼‰çš„è·¨åŸŸæ•°æ®è¯„ä¼°ä¸­ï¼ŒPathVLM-R1çš„è¿ç§»æ€§èƒ½å¹³å‡æé«˜äº†17.3%ï¼Œæ˜æ˜¾ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PathVLM-R1ä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œæ‰©å±•æ½œåŠ›ã€‚æœ¬æ–‡çš„æ–¹æ³•ä¸ºåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†æ–°çš„æ€è·¯ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¼•å…¥åŒå¥–åŠ±æœºåˆ¶ï¼Œå°†åŒ»å­¦ä¸­å¯¹è¿‡ç¨‹è§£é‡Šæ€§å’Œç»“æœå‡†ç¡®æ€§çš„éœ€æ±‚è½¬åŒ–ä¸ºå¯å­¦ä¹ çš„åŒå¥–åŠ±ä¿¡å·ï¼Œä¸ºåŒ»å­¦è¯Šæ–­é¢†åŸŸæä¾›äº†æœ‰æ•ˆçš„æ¨¡å‹ä¼˜åŒ–ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒPathVLM-R1åœ¨è®­ç»ƒå‚æ•°å’Œæ ·æœ¬æ•°é‡ä¸Šçš„é«˜æ•ˆæ€§ï¼Œä¹Ÿä¸ºç±»ä¼¼ç ”ç©¶æä¾›äº†å‚è€ƒã€‚

## reason-rft--reinforcement-fine-tuning-for-visual-reasoning
### Abstract
Visual reasoning abilities play a crucial role in understanding complex
multimodal data, advancing both domain-specific applications and artificial
general intelligence (AGI). Existing methods improve VLM reasoning via
Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated
training data to enhance visual reasoning capabilities. However, this training
paradigm may lead to overfitting and cognitive rigidity, restricting the
model's ability to transfer visual reasoning skills across domains and limiting
its real-world applicability. To address these limitations, we propose
Reason-RFT, a novel reinforcement fine-tuning framework that significantly
enhances generalization capabilities in visual reasoning tasks. Reason-RFT
introduces a two-phase training framework for visual reasoning: (1) Supervised
Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the
reasoning potential of Vision-Language Models (VLMs), followed by (2) Group
Relative Policy Optimization (GRPO)-based reinforcement learning that generates
multiple reasoning-response pairs, significantly enhancing generalization in
visual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,
we reconstructed a comprehensive dataset spanning visual counting, structure
perception, and spatial transformation. Experimental results demonstrate
Reasoning-RFT's three key advantages: (1) Performance Enhancement: achieving
state-of-the-art results across multiple tasks, outperforming most mainstream
open-source and proprietary models; (2) Generalization Superiority:
consistently maintaining robust performance across diverse tasks and domains,
outperforming alternative training paradigms; (3) Data Efficiency: excelling in
few-shot learning scenarios while surpassing full-dataset SFT baselines.
Project website: https://tanhuajie.github.io/ReasonRFT
### ğŸŒŸ è®ºæ–‡è§£è¯» | Reason-RFTï¼šæå‡è§†è§‰æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰æ¨ç†èƒ½åŠ›å¯¹äºç†è§£å¤æ‚çš„å¤šç§æ¨¡æ€æ•°æ®å’Œæ¨åŠ¨äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰ç›‘ç£å¾®è°ƒæ¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ç§æ–¹æ³•ä¾èµ–äºç²¾å¿ƒæ ‡æ³¨çš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ç§è®­ç»ƒèŒƒå¼å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸä¹‹é—´è½¬ç§»è§†è§‰æ¨ç†æŠ€èƒ½çš„èƒ½åŠ›ï¼Œä»¥åŠå…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†Reason-RFTï¼Œä¸€ä¸ªæ–°é¢–çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
Reason-RFTå¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆï¼Œé€šè¿‡ä½¿ç”¨ç²¾å¿ƒæŒ‘é€‰çš„CoTæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ¿€æ´»VLMsçš„æ¨ç†æ½œåŠ›ï¼›å…¶æ¬¡ï¼Œé€šè¿‡åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œç”Ÿæˆå¤šä¸ªæ¨ç†-å“åº”å¯¹ï¼Œæ˜¾è‘—æå‡è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
ä¸ºäº†è¯„ä¼°Reason-RFTçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½œè€…é‡æ„äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†ï¼Œæ¶µç›–è§†è§‰è®¡æ•°ã€ç»“æ„æ„ŸçŸ¥å’Œç©ºé—´è½¬æ¢ä¸‰ä¸ªæ ¸å¿ƒé¢†åŸŸï¼Œä½œä¸ºè¯„ä¼°è§†è§‰è®¤çŸ¥ã€å‡ ä½•ç†è§£å’Œè·¨ä»»åŠ¡æ³›åŒ–çš„åŸºå‡†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜Reason-RFTå…·æœ‰ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼š
1. æ€§èƒ½æå‡ï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¿‡å¤§å¤šæ•°ä¸»æµçš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼›
2. æ³›åŒ–ä¼˜åŠ¿ï¼šåœ¨å¤šç§ä»»åŠ¡å’Œé¢†åŸŸä¸Šä¿æŒç¨³å¥çš„æ€§èƒ½ï¼Œè¶…è¿‡å…¶ä»–è®­ç»ƒèŒƒå¼ï¼›
3. æ•°æ®æ•ˆç‡ï¼šåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶è¶…è¿‡å…¨æ•°æ®é›†SFTåŸºçº¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Reason-RFTæ¡†æ¶ä¸ºè§†è§‰æ¨ç†ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæœ‰æ•ˆæå‡äº†VLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä¸­é‡æ„çš„æ•°æ®é›†ä¸ºè¯„ä¼°è§†è§‰æ¨ç†æ¨¡å‹æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæœ‰åŠ©äºæœªæ¥ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚

