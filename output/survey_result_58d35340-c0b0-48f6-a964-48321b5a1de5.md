# Paper List of Terms(Reward+RL+Medical)
- [25/06] **CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making**  
[[Paper](http://arxiv.org/pdf/2506.12849v1)] [[Code/Page]()] [[TLDR/Notes](#capo--reinforcing-consistent-reasoning-in-medical-decision-making)]

- [25/06] **Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback**  
[[Paper](http://arxiv.org/pdf/2506.12323v1)] [[Code/Page]()] [[TLDR/Notes](#doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback)]

- [25/06] **RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints**  
[[Paper](http://arxiv.org/pdf/2506.06600v2)] [[Code/Page]()] [[TLDR/Notes](#rarl--improving-medical-vlm-reasoning-and-generalization-with-reinforcement-learning-and-lora-under-data-and-hardware-constraints)]

- [25/05] **DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue**  
[[Paper](http://arxiv.org/pdf/2505.19630v1)] [[Code/Page](https://github.com/JarvisUSTC/DoctorAgent-RL)] [[TLDR/Notes](#doctoragent-rl--a-multi-agent-collaborative-reinforcement-learning-system-for-multi-turn-clinical-dialogue)]

- [25/05] **Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.19213v1)] [[Code/Page]()] [[TLDR/Notes](#improving-medical-reasoning-with-curriculum-aware-reinforcement-learning)]

- [25/05] **Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL**  
[[Paper](http://arxiv.org/pdf/2505.17952v1)] [[Code/Page]()] [[TLDR/Notes](#beyond-distillation--pushing-the-limits-of-medical-llm-reasoning-with-minimalist-rule-based-rl)]

- [25/05] **WiNGPT-3.0 Technical Report**  
[[Paper](http://arxiv.org/pdf/2505.17387v2)] [[Code/Page]()] [[TLDR/Notes](#wingpt-3-0-technical-report)]

- [25/05] **Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies**  
[[Paper](http://arxiv.org/pdf/2505.16242v1)] [[Code/Page]()] [[TLDR/Notes](#offline-guarded-safe-reinforcement-learning-for-medical-treatment-optimization-strategies)]

- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.13973v1)] [[Code/Page]()] [[TLDR/Notes](#toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models)]



# TLDR/Notes
## capo--reinforcing-consistent-reasoning-in-medical-decision-making
### Abstract
In medical visual question answering (Med-VQA), achieving accurate responses
relies on three critical steps: precise perception of medical imaging data,
logical reasoning grounded in visual input and textual questions, and coherent
answer derivation from the reasoning process. Recent advances in general
vision-language models (VLMs) show that large-scale reinforcement learning (RL)
could significantly enhance both reasoning capabilities and overall model
performance. However, their application in medical domains is hindered by two
fundamental challenges: 1) misalignment between perceptual understanding and
reasoning stages, and 2) inconsistency between reasoning pathways and answer
generation, both compounded by the scarcity of high-quality medical datasets
for effective large-scale RL. In this paper, we first introduce Med-Zero-17K, a
curated dataset for pure RL-based training, encompassing over 30 medical image
modalities and 24 clinical tasks. Moreover, we propose a novel large-scale RL
framework for Med-VLMs, Consistency-Aware Preference Optimization (CAPO), which
integrates rewards to ensure fidelity between perception and reasoning,
consistency in reasoning-to-answer derivation, and rule-based accuracy for
final responses. Extensive experiments on both in-domain and out-of-domain
scenarios demonstrate the superiority of our method over strong VLM baselines,
showcasing strong generalization capability to 3D Med-VQA benchmarks and
R1-like training paradigms.
### 🌟 论文解读 | CAPO：强化医疗决策中的一致推理

### 📌 背景痛点/本文动机
在医疗视觉问答（Med - VQA）领域，准确响应依赖精准感知医学影像数据、基于视觉和文本问题的逻辑推理以及从推理过程得到连贯答案这三个关键步骤。通用视觉 - 语言模型（VLMs）中大规模强化学习（RL）能增强推理能力和模型性能，但在医疗领域应用受两大挑战阻碍：一是感知理解与推理阶段的错位，通用VLMs预训练时接触医疗图像有限，易过度依赖LLM backbone的参数知识，产生脱离医学图像的虚假推理；二是推理路径与答案生成的不一致，且高质量医疗数据集稀缺加剧了这些问题。同时，将通用领域的RL - Zero风格训练范式直接适配到医疗领域改进有限，因此需要新方法解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建Med - Zero - 17K数据集
为解决医疗领域高质量RL数据集稀缺问题，构建了Med - Zero - 17K数据集。该数据集整合多个公开医疗数据源，利用Qwen - VL - 72B模型从Pubmedvision的图像 - 标题数据生成VQA对，增强了模态多样性和任务覆盖度。采用混合难度数据过滤策略，从基础模型采样10个响应（温度设为0.9），排除全对或全错的QA对，保留至少有一个正确响应的对，保证难度平衡。还将临床问题分为三个复杂度级别，涵盖从基础视觉分析到复杂诊断推理的任务，包含30多种医学影像模态。

💡 创新点2：提出CAPO强化学习框架
提出Consistency - Aware Preference Optimization（CAPO）框架用于医疗VLMs。除基于准确性的奖励外，整合两种新的一致性奖励。感知一致性奖励通过扰动输入医学图像，暴露文本偏向的推理过程，奖励模型在原始图像下更偏好正确推理，鼓励与图像一致的可信推理；决策一致性奖励利用外部LLM作为评判，评估推理过程和答案之间的一致性。

### 📈 实验结果
在分布内和分布外数据集上的大量实验表明，CAPO框架取得了最先进的结果，超越了特定领域和通用目的的VLMs。与传统GRPO相比，该方法能获得更稳定和更高的奖励，且CAPO引入的一致性相关奖励鼓励了更长更深的临床推理模式。不同于传统SFT在如PMC - VQA数据集上的过拟合问题，CAPO在零样本条件下持续带来稳定且鲁棒的改进。此外，CAPO在未见过的模态和任务上表现出强泛化能力，在OmnimedVQA和MMMU Health & Medicine基准测试中，无需访问重叠训练数据就超越了基线模型和SFT方法。

### 💬 可借鉴之处
1. 数据集构建方面：面对特定领域高质量数据稀缺问题时，可借鉴Med - Zero - 17K的构建思路，整合多源数据、利用大模型生成数据增强多样性、设计策略保证数据难度平衡等，为模型训练提供优质数据支撑。
2. 强化学习框架设计方面：在解决领域内模型推理与感知、推理与答案生成一致性问题时，CAPO的思路值得参考，通过设计针对性的一致性奖励来引导模型学习，可应用到其他需要保证推理一致性的领域任务中。
3. 实验验证方面：其在分布内和分布外、零样本等多种场景下的实验验证方式，为评估模型泛化能力等提供了全面的参考范式，在后续研究中可借鉴这种多场景验证的思路来更全面评估方法有效性。

## doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback
### Abstract
Paucity of medical data severely limits the generalizability of diagnostic ML
models, as the full spectrum of disease variability can not be represented by a
small clinical dataset. To address this, diffusion models (DMs) have been
considered as a promising avenue for synthetic image generation and
augmentation. However, they frequently produce medically inaccurate images,
deteriorating the model performance. Expert domain knowledge is critical for
synthesizing images that correctly encode clinical information, especially when
data is scarce and quality outweighs quantity. Existing approaches for
incorporating human feedback, such as reinforcement learning (RL) and Direct
Preference Optimization (DPO), rely on robust reward functions or demand
labor-intensive expert evaluations. Recent progress in Multimodal Large
Language Models (MLLMs) reveals their strong visual reasoning capabilities,
making them adept candidates as evaluators. In this work, we propose a novel
framework, coined MAGIC (Medically Accurate Generation of Images through
AI-Expert Collaboration), that synthesizes clinically accurate skin disease
images for data augmentation. Our method creatively translates expert-defined
criteria into actionable feedback for image synthesis of DMs, significantly
improving clinical accuracy while reducing the direct human workload.
Experiments demonstrate that our method greatly improves the clinical quality
of synthesized skin disease images, with outputs aligning with dermatologist
assessments. Additionally, augmenting training data with these synthesized
images improves diagnostic accuracy by +9.02% on a challenging 20-condition
skin disease classification task, and by +13.89% in the few-shot setting.
### 🌟 论文解读 | 医生认可：借AI专家反馈生成医学准确皮肤病图像

### 📌 背景痛点/本文动机
在皮肤病诊断的机器学习模型领域，医疗数据匮乏是一大难题，小临床数据集难以涵盖疾病变异全貌，限制了诊断模型泛化性。扩散模型虽为合成图像生成与数据增强提供可能，但生成图像常存在医学不准确问题，影响模型表现。现有融入人类反馈的方法（如强化学习、直接偏好优化）依赖可靠奖励函数或需大量专家评估工作。而多模态大语言模型（MLLMs）视觉推理能力强，适合作为评估者，基于此，论文提出MAGIC框架解决医学图像合成准确性与减少专家工作量问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出MAGIC框架  
MAGIC（Medically Accurate Generation of Images through AI - Expert Collaboration）框架旨在合成临床准确的皮肤病图像用于数据增强。该框架创造性地将专家定义标准转化为扩散模型（DMs）图像合成的可操作反馈，在提升临床准确性同时减少人类直接工作量。框架还纳入Image - to - Image（I2I）模块，从中间时间步而非纯高斯噪声开始去噪，加速采样阶段且保证病变变换贴合真实数据分布。

💡 创新点2：AI - 专家协作范式  
人类专家主要完成两项工作：一是从可靠来源制定易被MLLM验证的检查清单；二是在文本到图像（T2I）扩散模型训练期间监督MLLM对合成图像的反馈。借助MLLM强大视觉推理能力，将视觉评估工作在最少专家监督下交由MLLM完成，有效利用领域知识且无需大量标注工作，让扩散模型在迭代学习中生成更贴合医学一致性的图像。

💡 创新点3：结合不同微调方式展现优势  
MAGIC框架能与基于奖励的微调（RFT）和直接偏好优化（DPO）结合使用，且在DPO下表现突出。MAGIC - DPO pipeline优化扩散模型，随着训练推进和更多图像 - 反馈对使用，生成准确代表每种病症独特视觉特征的合成数据。

### 📈 实验结果
实验表明MAGIC大幅提升合成皮肤病图像临床质量，输出与皮肤科医生评估一致。在具挑战性的20种皮肤病分类任务中，用合成图像增强训练数据后诊断准确率提升9.02%；在少样本设置下，准确率提升13.89%。同时，皮肤科医生评估分数提升、Fréchet Inception Distance（FID）分数降低，表明临床准确性和保真度提升。

### 💬 可借鉴之处
1. 框架设计层面：MAGIC整合专家知识到扩散模型的思路，为领域知识融入生成模型提供范例，且I2I模块加速采样与保证数据分布贴合的设计，在其他需数据增强且关注生成效率与真实性场景可借鉴。
2. 协作范式层面：AI - 专家协作，利用MLLM承担视觉评估减轻专家负担的模式，可推广到其他医疗图像甚至跨领域（如放射科图像等）的生成与评估任务，为解决领域专家资源有限问题提供思路。
3. 应用效果层面：在数据增强后提升模型诊断性能，证明合成高质量医学图像对下游任务价值，为医疗AI领域数据稀缺场景下模型性能提升提供有效途径。

## rarl--improving-medical-vlm-reasoning-and-generalization-with-reinforcement-learning-and-lora-under-data-and-hardware-constraints
### Abstract
The growing integration of vision-language models (VLMs) in medical
applications offers promising support for diagnostic reasoning. However,
current medical VLMs often face limitations in generalization, transparency,
and computational efficiency-barriers that hinder deployment in real-world,
resource-constrained settings. To address these challenges, we propose a
Reasoning-Aware Reinforcement Learning framework, \textbf{RARL}, that enhances
the reasoning capabilities of medical VLMs while remaining efficient and
adaptable to low-resource environments. Our approach fine-tunes a lightweight
base model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward
functions that jointly consider diagnostic accuracy and reasoning quality.
Training is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the
feasibility of deploying such models in constrained environments. We evaluate
the model using an LLM-as-judge framework that scores both correctness and
explanation quality. Experimental results show that RARL significantly improves
VLM performance in medical image analysis and clinical reasoning, outperforming
supervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while
requiring fewer computational resources. Additionally, we demonstrate the
generalization capabilities of our approach on unseen datasets, achieving
around 27% improved performance compared to supervised fine-tuning and about 4%
over traditional RL fine-tuning. Our experiments also illustrate that diversity
prompting during training and reasoning prompting during inference are crucial
for enhancing VLM performance. Our findings highlight the potential of
reasoning-guided learning and reasoning prompting to steer medical VLMs toward
more transparent, accurate, and resource-efficient clinical decision-making.
Code and data are publicly available.
### 🌟 论文解读 | RARL：资源受限下用强化学习与LoRA提升医疗VLM推理与泛化能力

### 📌 背景痛点/本文动机
在医疗应用中，视觉语言模型（VLMs）虽为诊断推理提供了有力支持，但当前医疗VLMs在泛化性、透明度和计算效率方面存在局限，阻碍了其在资源受限的真实场景中部署。比如高性能医疗VLM依赖大规模数据集与大量计算资源，小医疗机构难以承受；且很多模型针对特定任务或数据集优化，对未见过的临床场景泛化差，同时缺乏临床信任所需的透明推理能力，评估也常忽略推理过程只关注最终答案准确性。为解决这些问题，本文提出RARL框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RARL框架  
结合Reasoning - Aware Reinforcement Learning（推理感知强化学习）与Low - Rank Adaptation（LoRA）微调技术，增强医疗VLMs推理能力的同时，保证效率并适应低资源环境。以轻量级模型Qwen2 - VL - 2B - Instruct为基础，用LoRA和考虑诊断准确性与推理质量的自定义奖励函数进行微调。  
💡 创新点2：单GPU训练验证可行性  
在单个NVIDIA A100 - PCIE - 40GB GPU上完成训练，证明了在受限环境部署此类模型的可行性，为资源有限的医疗场景应用提供可能。  
💡 创新点3：LLM - as - judge评估框架  
采用该框架评估模型，全面考量答案正确性与推理质量，弥补了以往评估只关注最终答案的不足。  

### 📈 实验结果
在医疗图像分析和临床推理任务中，RARL显著提升VLM性能：在推理聚焦任务上，比有监督微调性能提升约7.78%且计算资源需求更少；在未见过的数据集上泛化能力强，比有监督微调性能提升约27%，比传统RL微调提升约4%；还表明训练时的多样性提示和推理时的推理提示对提升VLM性能至关重要。  

### 💬 可借鉴之处
1. 资源高效利用思路：RARL结合RL与LoRA，在单GPU上实现有效训练，为资源受限场景下模型优化提供了范例，小机构或低资源医疗场景可参考这种资源高效利用的模型训练方式。  
2. 推理与泛化能力提升：通过推理感知强化学习关注推理质量与诊断准确性，以及对泛化能力的提升策略，为医疗领域及其他领域VLMs提升推理和泛化性能提供了方法参考。  
3. 评估方式创新：LLM - as - judge框架兼顾答案正确性与推理质量，在需要关注过程的AI任务评估中，这种全面评估的思路值得借鉴。  
4. 提示策略重要性：训练时多样性提示和推理时推理提示对性能提升的发现，为后续模型训练和推理阶段的优化提供了方向，可在其他VLMs任务中尝试此类提示策略来提升性能。

## doctoragent-rl--a-multi-agent-collaborative-reinforcement-learning-system-for-multi-turn-clinical-dialogue
### Abstract
Large language models (LLMs) have demonstrated excellent capabilities in the
field of biomedical question answering, but their application in real-world
clinical consultations still faces core challenges. Existing systems rely on a
one-way information transmission mode where patients must fully describe their
symptoms in a single round, leading to nonspecific diagnostic recommendations
when complaints are vague. Traditional multi-turn dialogue methods based on
supervised learning are constrained by static data-driven paradigms, lacking
generalizability and struggling to intelligently extract key clinical
information. To address these limitations, we propose DoctorAgent-RL, a
reinforcement learning (RL)-based multi-agent collaborative framework that
models medical consultations as a dynamic decision-making process under
uncertainty. The doctor agent continuously optimizes its questioning strategy
within the RL framework through multi-turn interactions with the patient agent,
dynamically adjusting its information-gathering path based on comprehensive
rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables
LLMs to autonomously develop interaction strategies aligned with clinical
reasoning logic, rather than superficially imitating patterns in existing
dialogue data. Notably, we constructed MTMedDialog, the first English
multi-turn medical consultation dataset capable of simulating patient
interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing
models in both multi-turn reasoning capability and final diagnostic
performance, demonstrating practical value in assisting clinical consultations.
https://github.com/JarvisUSTC/DoctorAgent-RL
### 🌟 论文解读 | DoctorAgent-RL：用强化学习重塑多轮临床对话的智能诊疗系统

### 📌 背景痛点/本文动机
大语言模型（LLMs）在生物医学问答领域展现出卓越能力，但在真实临床问诊场景中仍面临核心挑战。现有系统多采用单向信息传递模式，要求患者单轮完整描述症状，模糊主诉下易产生非特异性诊断建议；传统基于监督学习的多轮对话方法受限于静态数据驱动范式，缺乏泛化性且难智能提取关键临床信息。临床诊断需主动、多轮地针对性问诊来明确病情，而现有模式与这一需求存在本质矛盾，催生了对动态决策式临床对话框架的迫切需求。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出DoctorAgent - RL多智能体协作强化学习框架  
将临床问诊建模为不确定性下的马尔可夫决策过程（MDP），构建三个核心智能体：基于LLM的高保真患者智能体，能模拟真实沟通多样性并生成病理一致的回应；由真实问诊记录克隆医疗行为初始化、经强化学习（RL）优化的医生智能体，以此掌握有效问诊策略；依据诊断准确性、患者信息响应度和问题规范性提供多维度奖励的问诊评估器。通过RL框架，医生智能体在与患者智能体多轮交互中持续优化问诊策略，基于评估器综合奖励动态调整信息收集路径，让LLM自主发展契合临床推理逻辑的交互策略，而非表面模仿现有对话数据模式。  

💡 创新点2：构建MTMedDialog多轮医疗对话数据集  
打造首个可模拟真实患者交互的英文多轮医疗问诊数据集MTMedDialog，为多智能体系统在动态临床推理场景下的训练与评估提供数据支撑，填补了该领域高质量多轮交互数据集的空白。  


### 📈 实验结果
实验表明，DoctorAgent - RL在多轮推理能力和最终诊断性能上超越现有模型（如在诊断与建议的平均准确率分数、平均交互轮次等维度表现更优，见图1对比），在辅助临床问诊中展现出实用价值，能更高效地通过多轮交互实现精准诊断。  

### 💬 可借鉴之处
从方法层面，创新性地将强化学习与多智能体协作结合，为解决静态数据驱动范式局限、实现动态决策式的领域对话系统提供了范本，可启发其他垂直领域（如金融咨询、教育辅导等）构建智能交互框架；从数据层面，构建专属多轮对话数据集的思路，为领域内模型训练与评估数据资源建设提供了借鉴，助力后续研究突破数据瓶颈；从应用层面，验证了强化学习驱动下智能体自主优化交互策略在复杂专业场景的可行性，为AI辅助专业服务（如临床问诊）的落地提供了新路径。

## improving-medical-reasoning-with-curriculum-aware-reinforcement-learning
### Abstract
Recent advances in reinforcement learning with verifiable, rule-based rewards
have greatly enhanced the reasoning capabilities and out-of-distribution
generalization of VLMs/LLMs, obviating the need for manually crafted reasoning
chains. Despite these promising developments in the general domain, their
translation to medical imaging remains limited. Current medical reinforcement
fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby
restricting the model's ability to engage in world knowledge retrieval and
flexible task adaptation. More critically, these methods fall short of
addressing the critical clinical demand for open-ended, reasoning-intensive
decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first
multimodal reinforcement learning framework tailored for medical VQA that
unifies close-ended and open-ended data within a curriculum-driven RFT
paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of
close-ended medical VQA tasks to establish domain-grounded reasoning
capabilities, and is then progressively adapted to open-ended tasks to foster
deeper knowledge enhancement and clinical interpretability. We validate MedCCO
across eight challenging medical VQA benchmarks, spanning both close-ended and
open-ended settings. Experimental results show that MedCCO consistently
enhances performance and generalization, achieving a 11.4\% accuracy gain
across three in-domain tasks, and a 5.7\% improvement on five out-of-domain
benchmarks. These findings highlight the promise of curriculum-guided RL in
advancing robust, clinically-relevant reasoning in medical multimodal language
models.
```
### 🌟 论文解读 | 用课程感知强化学习提升医疗推理：MedCCO框架来袭

### 📌 背景痛点/本文动机
在通用领域，基于可验证、规则奖励的强化学习已大幅提升大模型推理与泛化能力，但在医疗影像领域的应用有限。当前医疗强化微调方法多聚焦封闭域VQA（视觉问答），限制了模型知识检索与任务适配能力，也难以满足临床开放式、高推理决策需求。为填补这一空白，本文提出面向医疗VQA的多模态强化学习框架MedCCO。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：首个统一处理封闭与开放式医疗VQA的多模态框架  
MedCCO是首个为医疗VQA量身打造的多模态强化学习框架，在课程驱动的强化微调范式下整合封闭域与开放域数据。先基于多样封闭域医疗VQA任务微调，建立领域基础推理能力；再逐步适配开放域任务，强化知识增强与临床可解释性。  

💡 创新点2：课程式强化微调策略  
采用两阶段课程驱动的强化学习范式：第一阶段借封闭域医疗VQA的强化学习获取领域特定推理能力；第二阶段以渐进方式适配更具挑战的开放域任务。这种训练 scheme 平衡了判别准确性与生成灵活性，让模型无需人工标注就能检索知识、构建推理链。同时，消融实验表明“先封闭后开放”的课程训练，比同时训练两类任务效果更优，避免了离散与连续奖励冲突及任务难度差异带来的负面影响。  

### 📈 实验结果
在8个医疗VQA基准（涵盖封闭与开放场景）验证MedCCO：域内3项任务精度提升11.4%；5项域外基准性能提升5.7%；SLAKE基准评估也展现跨模态鲁棒性，持续超越竞品基线，证明课程引导强化学习在医疗多模态模型中提升推理能力的潜力。  

### 💬 可借鉴之处
1. 任务统一视角：将封闭与开放式任务纳入同一框架，为多类型医疗问答任务融合提供思路，启发跨任务场景的模型设计。  
2. 课程学习范式：“从简到难”的强化学习训练策略，在保留已有知识同时注入新能力，为需分阶段提升的复杂任务（如医疗、教育等领域）提供训练范式参考。  
3. 医疗领域适配：针对医疗VQA设计强化学习方案，推动强化学习在垂直领域（医疗影像+语言）的落地，为行业应用中模型推理能力优化提供实践范例。  
```

## beyond-distillation--pushing-the-limits-of-medical-llm-reasoning-with-minimalist-rule-based-rl
### Abstract
Improving performance on complex tasks and enabling interpretable decision
making in large language models (LLMs), especially for clinical applications,
requires effective reasoning. Yet this remains challenging without supervised
fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from
closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the
first medical LLM to show that reasoning capability can emerge purely through
reinforcement learning (RL), using minimalist rule-based rewards on public
multiple-choice QA datasets, without relying on SFT or distilled CoT data.
AlphaMed achieves state-of-the-art results on six medical QA benchmarks,
outperforming models trained with conventional SFT+RL pipelines. On challenging
benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source
models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the
factors behind this success, we conduct a comprehensive data-centric analysis
guided by three questions: (i) Can minimalist rule-based RL incentivize
reasoning without distilled CoT supervision? (ii) How do dataset quantity and
diversity impact reasoning? (iii) How does question difficulty shape the
emergence and generalization of reasoning? Our findings show that dataset
informativeness is a key driver of reasoning performance, and that minimalist
RL on informative, multiple-choice QA data is effective at inducing reasoning
without CoT supervision. We also observe divergent trends across benchmarks,
underscoring limitations in current evaluation and the need for more
challenging, reasoning-oriented medical QA benchmarks.
### 🌟 论文解读 | 无需蒸馏！用极简规则强化学习突破医疗大模型推理极限

### 📌 背景痛点/本文动机
在医疗领域，大语言模型（LLMs）要完成复杂任务并实现可解释的决策，有效推理能力至关重要。但当前多数医疗LLM获取推理能力依赖于在思维链（CoT）数据上进行有监督微调（SFT），而这类CoT数据要么人工标注成本高昂，要么从闭源模型（如GPT - 4o）蒸馏而来，存在可扩展性与可及性难题。因此，本文试图探索：能否在不依赖蒸馏CoT数据的情况下，通过极简规则的强化学习（RL）实现医疗推理能力？

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出AlphaMed模型，首次仅通过极简规则强化学习激发推理能力  
AlphaMed摆脱了对SFT和蒸馏CoT数据的依赖，直接基于公开多选问答（QA）数据集设计简单的规则奖励，借助强化学习来训练模型，证明了医疗LLM的推理能力可以在无蒸馏CoT数据监督下，通过极简规则RL培养出来。  

💡 创新点2：围绕数据开展全面分析，揭示推理能力提升关键因素  
从三个问题出发分析：规则RL能否无CoT监督激励推理、数据集数量和多样性如何影响推理、问题难度怎样塑造推理泛化。发现数据集信息性是推理性能关键驱动因素，信息丰富的多选QA数据上的极简RL能有效诱导推理；同时推理可在低难度数据下被激励，高难度数据能增强推理，且不同难度混合对泛化很关键。  

### 📈 实验结果
AlphaMed在六个医疗QA基准测试中取得state - of - the - art结果，超越了用传统SFT + RL pipeline训练的模型；在如MedXpert等具有挑战性的基准上，甚至超过了更大规模或闭源的模型（如DeepSeek - V3 - 671B、Claude - 3.5 - Sonnet）。此外，不同基准上的实验还发现当前评估存在局限性，凸显了需要更具挑战性、面向推理的医疗QA基准。  

### 💬 可借鉴之处
1. 模型训练思路创新：展示了不依赖昂贵CoT数据和SFT，仅用规则RL训练出强推理能力模型的可行性，为医疗LLM训练开辟新路径。  
2. 数据驱动分析：从数据数量、多样性、信息性、问题难度等维度分析对推理的影响，为后续医疗领域数据集构建、模型训练中数据选择提供了分析框架与参考依据。  
3. 评估方向启示：实验中发现现有基准局限性，提示后续需打造更具挑战性、聚焦推理的医疗QA评估基准，这对整个医疗NLP评估生态发展有借鉴意义。

## wingpt-3-0-technical-report
### Abstract
Current Large Language Models (LLMs) exhibit significant limitations, notably
in structured, interpretable, and verifiable medical reasoning, alongside
practical deployment challenges related to computational resources and data
privacy. This report focused on the development of WiNGPT-3.0, the 32-billion
parameter LLMs, engineered with the objective of enhancing its capacity for
medical reasoning and exploring its potential for effective integration within
healthcare IT infrastructures. The broader aim is to advance towards clinically
applicable models. The approach involved a multi-stage training pipeline
tailored for general, medical, and clinical reasoning. This pipeline
incorporated supervised fine-tuning (SFT) and reinforcement learning (RL),
leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward
models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0
demonstrated strong performance: specific model variants achieved scores of
66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training
improved performance on a clinical reasoning task from a baseline score of 58.1
to 62.5. These findings suggest that reinforcement learning, even when applied
with a limited dataset of only a few thousand examples, can enhance medical
reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited
data and computation paves the way for more trustworthy and practically
deployable LLMs within clinical workflows and health information
infrastructures.
### 🌟 论文解读 | WiNGPT - 3.0：医疗场景下大模型的突破与实践

### 📌 背景痛点/本文动机
当前大语言模型（LLMs）在医疗推理任务中存在诸多局限，一方面，在结构化、可解释且可验证的医疗推理方面表现不佳，大模型推理多依赖模式识别而非动态多因素临床评估，难以追溯和验证；另一方面，在医疗场景部署时面临计算资源需求高、数据隐私严格等挑战，且大模型易产生“幻觉”、知识更新难、适配不同医疗标准复杂等问题。同时，医疗专业人员也反馈了这些痛点。为克服这些障碍，团队开发了医疗领域针对性的大模型WiNGPT - 3.0，致力于提升医疗推理能力并探索其在医疗IT基础设施集成的潜力，向临床适用模型迈进。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多阶段训练 pipeline 适配医疗推理
设计了针对通用、医疗和临床推理的多阶段训练流程，整合了监督微调（SFT）和强化学习（RL）。利用精心策划的长思维链（Long Chain - of - Thought，CoT）数据集、辅助奖励模型以及基于证据的诊断链模拟来优化训练，让模型能更好地进行医疗推理相关学习。
💡 创新点2：数据处理与辅助模型结合
构建近200万问题的大规模多类别（数学、编程、常识、医疗等）且多语言（中英）数据集，通过多阶段数据处理 pipeline（长思维链答案生成、数据过滤分类、数据采样等）生成适合SFT和RL训练的数据。同时开发训练了三个专业辅助模型：基于偏好的奖励模型、基于验证器的奖励模型和思维追踪模型来支撑数据处理与训练。
💡 创新点3：聚焦医疗场景深度集成与实用化
WiNGPT - 3.0基于Qwen - 2.5的320亿参数架构，在推理能力和现场部署可行性间取得平衡。它紧密集成到WiNEX医疗信息系统，通过医生审核、结构化知识库、可定制规则模板三重保障辅助临床医生；还优先考虑与医院工作流程深度集成，具备预配置电子病历模板、既定规则和从用户交互中持续学习等特性，适配特定机构标准和操作流程。

### 📈 实验结果
WiNGPT - 3.0展现出强劲性能：特定模型变体在MedCalc上得分66.6，在MedQA - USMLE上得分87.1；针对临床推理任务的定向训练，将基线分数从58.1提升到62.5。这些结果表明即使在仅几千个样本的有限数据集上应用强化学习，也能提升医疗推理准确性，为临床工作流和医疗信息基础设施中更可信、更易部署的大模型铺路。

### 💬 可借鉴之处
在数据利用上，展示了如何通过数据蒸馏、思维链生成等技术在资源约束下高效生成高质量医疗数据集，为领域特定数据构建提供思路；训练策略方面，验证了多阶段训练结合强化学习在提升特定领域（医疗）推理能力的有效性，小数据量下RL也能发挥作用，为资源有限场景下模型优化提供参考；场景集成角度，强调模型与医疗信息系统、医院工作流程深度集成，通过多种机制保障临床适用性、准确性与可解释性，为大模型在垂直领域落地提供了从技术到流程的完整借鉴范例，尤其是在合规性、实用性平衡上给出实践方向。

## offline-guarded-safe-reinforcement-learning-for-medical-treatment-optimization-strategies
### Abstract
When applying offline reinforcement learning (RL) in healthcare scenarios,
the out-of-distribution (OOD) issues pose significant risks, as inappropriate
generalization beyond clinical expertise can result in potentially harmful
recommendations. While existing methods like conservative Q-learning (CQL)
attempt to address the OOD issue, their effectiveness is limited by only
constraining action selection by suppressing uncertain actions. This
action-only regularization imitates clinician actions that prioritize
short-term rewards, but it fails to regulate downstream state trajectories,
thereby limiting the discovery of improved long-term treatment strategies. To
safely improve policy beyond clinician recommendations while ensuring that
state-action trajectories remain in-distribution, we propose \textit{Offline
Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically
grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel
dual constraint mechanism for improving policy with reliability and safety.
First, the OOD guardian is established to specify clinically validated regions
for safe policy exploration. By constraining optimization within these regions,
it enables the reliable exploration of treatment strategies that outperform
clinician behavior by leveraging the full patient state history, without
drifting into unsupported state-action trajectories. Second, we introduce a
safety cost constraint that encodes medical knowledge about physiological
safety boundaries, providing domain-specific safeguards even in areas where
training data might contain potentially unsafe interventions. Notably, we
provide theoretical guarantees on safety and near-optimality: policies that
satisfy these constraints remain in safe and reliable regions and achieve
performance close to the best possible policy supported by the data.
### 🌟 论文解读 | 医疗优化新突破：Offline Guarded Safe RL保障安全与效果

### 📌 背景痛点/本文动机
深度强化学习（RL）在医疗等安全关键领域应用时，离线强化学习（Offline RL）面临分布外（OOD）问题的重大风险。在医疗场景中，超出临床专业知识的不恰当泛化可能给出有害建议。现有方法如保守Q学习（CQL）仅通过抑制不确定动作约束动作选择，只能模仿临床医生优先短期奖励的行为，却无法调控下游状态轨迹，限制了更优长期治疗策略的发现。同时，医疗优化还存在无法主动交互探索、需整合安全约束与奖励函数等挑战，因此需要一种能在保障状态 - 动作轨迹分布内的前提下，安全改进策略的方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出OOD guardian机制
建立OOD guardian来明确临床验证的安全策略探索区域。与仅抑制OOD动作的CQL等方法不同，OGSRL明确约束状态和动作，充分利用数据集中嵌入的临床医生知识。通过将优化约束在这些区域内，能够借助完整患者状态历史，可靠地探索优于临床医生行为的治疗策略，且不会陷入无支持的状态 - 动作轨迹。
💡 创新点2：引入安全成本约束与提供理论保障
引入编码生理安全边界医疗知识的安全成本约束，即便在训练数据可能包含潜在不安全干预的区域，也能提供特定领域的保护。同时从理论上保证了安全性和近似最优性：满足约束的策略能留在安全可靠区域，且性能接近数据支持的最优策略。还结合基于模型的RL，提供了安全性和近似最优性的概率保证，并量化了数据集大小对策略可靠性的影响。

### 📈 实验结果
在MIMIC - III脓毒症治疗数据集上评估，OGSRL在分布外处理上显著优于基线方法。与临床医生决策相比，OGSRL使死亡率估计降低了78%，奖励增加了51%，在累积奖励、安全约束满足度和与临床行为的一致性方面持续超越强大的离线RL基线。

### 💬 可借鉴之处
1. 对于安全关键领域的离线强化学习应用，提供了一种结合领域知识（如医疗领域的临床知识）来约束策略探索和保障安全的思路，可推广到机器人、自动驾驶等其他安全关键领域。
2. 理论保障方面的工作为后续相关研究提供了如何从理论层面证明方法安全性与最优性的范例，有助于推动领域内方法的理论严谨性发展。
3. 模型中对数据集中专家知识的充分利用方式，为其他需要结合专业领域先验知识的离线RL任务提供了参考，指导如何从数据中挖掘和应用这类知识来改进策略。

## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### 🌟 论文解读 | s3：小数据量下用强化学习训练搜索智能体的新范式

### 📌 背景痛点/本文动机
在检索增强生成（RAG）系统中，大语言模型（LLMs）能借助外部知识提升推理能力，而近期用强化学习（RL）让LLM扮演搜索智能体的进展，可通过多轮交互优化信息获取。但现有方法存在缺陷：要么用仅关注搜索的指标（如NDCG）优化检索，忽略下游效用；要么微调整个LLM来联合推理与检索，把检索和生成耦合，限制了实际搜索效用且难适配冻结或闭源模型。因此，需要一种解耦搜索与生成、轻量且模型无关的框架，在小数据量下提升下游性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出s3框架，解耦搜索器与生成器  
s3是轻量、模型无关的框架，将搜索器（Searcher）从生成器（Generator）中解耦。训练时固定生成器（可复用任意冻结的LLM），仅专注于训练搜索器，让搜索优化聚焦在对下游生成质量的提升上，避免了生成与检索的耦合问题，也能适配闭源或冻结的LLM。  

💡 创新点2：定义“Gain Beyond RAG（GBR）”奖励信号  
GBR作为全新的奖励指标，衡量了s3检索到的文档让生成器表现，相比“朴素RAG（naïve RAG）检索”在生成准确率上的提升幅度。通过该奖励训练搜索器，能直接针对下游生成效用优化检索组件，摆脱仅关注搜索指标或易过拟合的精确匹配（EM）类指标的局限，让检索优化更贴合实际生成需求。  

### 📈 实验结果
在6个通用QA基准和5个医疗QA基准测试中，s3展现出强大性能：仅用2.4k训练样本，就超越了使用超70倍数据（如70k甚至更多）训练的基线方法（像DeepRetrieval、Search - R1等）。在通用和医疗领域的平均得分上，s3对比其他经典RAG、Active RAG、RL - Zero阶段的方法，都实现了更优的下游生成表现，验证了小数据量下高效训练搜索智能体的能力。  

### 💬 可借鉴之处
1. 模块化设计思路：将复杂的“检索 + 生成”任务解耦为搜索器和生成器独立优化，为构建更灵活的RAG系统提供了模块化思路，后续可针对不同模块分别迭代升级。  
2. 奖励信号设计：GBR奖励将下游生成效用作为检索优化的核心依据，跳出了传统仅看搜索环节指标的思维定式，为强化学习在检索增强场景下的奖励函数设计提供了新范式，可启发其他需要跨环节优化任务的奖励设计。  
3. 小数据高效训练：证明了在少量优质样本下也能训练出高性能搜索智能体，降低了大规模数据标注与训练成本，对资源有限但需构建RAG系统的场景（如垂直领域小团队）有很强的借鉴意义。

## toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models
### Abstract
Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.
```
### 🌟 论文解读 | 医疗VQA场景下基于强化学习微调视觉语言模型的有效性探索

### 📌 背景痛点/本文动机
近年来，基于强化学习（RL）的调优推动了多模态大语言模型（MLLMs）的发展，尤其是Group Relative Policy Optimization（GRPO）方法出现后。但将其直接应用于医疗任务以实现符合临床要求的模型行为仍具挑战。为使模型响应与临床预期对齐，本文针对医疗视觉问答（VQA）中影响RL调优有效性的四个关键维度展开研究：基础模型初始化策略、医疗语义对齐的作用、基于长度的奖励对长链推理的影响以及偏差的作用，同时验证GRPO - 基于RL的调优相较标准监督微调（SFT）的优势。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多维度分析GRPO - 基于RL在医疗MLLMs的表现 
从五个关键方面分析：对比从头训练与微调策略，探究训练初始方式对模型在医疗知识和推理能力上的权衡；引入医疗语义奖励，利用大语言模型生成的评估结合提示工程，解决通用奖励对临床任务不足的问题；研究仅依赖基于长度的奖励对医疗VQA长链推理的影响，发现其易导致回答冗长且不准确；评估问题级归一化在医疗VQA中对模型偏差的影响，通过实现Dr.GRPO验证其在提升回答准确性和token效率上的作用；对比SFT与GRPO - 基于RL调优，展现GRPO - RL的优势。

💡 创新点2：系统分析与实验验证结合 
对GRPO - 基于RL在医疗MLLMs中围绕初始化策略、医疗语义对齐、基于长度奖励影响和偏差相关行为展开系统分析；在医疗VQA基准测试集上进行大规模实验验证发现，为RL与临床有意义行为对齐提供实用见解。

### 📈 实验结果
选择Qwen2 - VL - 2B作为基础模型，在PMC - VQA子集（10K训练样本、7K测试样本）上实验。从头训练与微调对比中，从头训练模型推理更对齐有用但回答正确性和流畅性欠佳，微调模型回答更准确流畅；引入医疗语义对齐奖励后，模型性能和推理质量提升，准确率提高1.82%；仅基于长度的奖励易产生冗长不准确回答；Dr.GRPO能提升回答准确性和token效率；GRPO - 基于RL调优在准确率和推理质量上持续优于SFT方法。

### 💬 可借鉴之处
在领域特定模型调优方面，本文对基础模型初始化策略的研究为医疗等垂直领域模型初始化提供参考，帮助平衡回答质量与推理能力；奖励设计上，医疗语义奖励的引入思路可推广到其他垂直领域，利用领域内大模型评估来设计针对性奖励；方法对比上，验证了GRPO - 基于RL调优在医疗VQA任务中相较SFT的优势，为后续医疗MLLMs调优方法选择提供依据；偏差分析方面，对问题级归一化偏差的研究提醒在模型训练中关注此类潜在影响因素，Dr.GRPO的实践也提供了应对思路。
```

