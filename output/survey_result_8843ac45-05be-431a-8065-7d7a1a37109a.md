# Paper List of Terms(MLLM+Medical)
- [25/06] **Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning**  
[[Paper](http://arxiv.org/pdf/2506.15477v1)] [[Code/Page]()] [[TLDR/Notes](#multimodal-large-language-models-for-medical-report-generation-via-customized-prompt-tuning)]

- [25/06] **Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback**  
[[Paper](http://arxiv.org/pdf/2506.12323v1)] [[Code/Page]()] [[TLDR/Notes](#doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback)]

- [25/06] **MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.10465v1)] [[Code/Page]()] [[TLDR/Notes](#medseg-r--reasoning-segmentation-in-medical-images-with-multimodal-large-language-models)]

- [25/06] **HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding**  
[[Paper](http://arxiv.org/pdf/2506.09634v1)] [[Code/Page](https://github.com/YanzhaoShi/HSENet.)] [[TLDR/Notes](#hsenet--hybrid-spatial-encoding-network-for-3d-medical-vision-language-understanding)]

- [25/06] **HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains**  
[[Paper](http://arxiv.org/pdf/2506.07837v1)] [[Code/Page](https://github.com/ShiDaizi/ReMUD,)] [[TLDR/Notes](#haibu-remud--reasoning-multimodal-ultrasound-dataset-and-model-bridging-to-general-specific-domains)]

- [25/06] **Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.07044v4)] [[Code/Page]()] [[TLDR/Notes](#lingshu--a-generalist-foundation-model-for-unified-multimodal-medical-understanding-and-reasoning)]

- [25/06] **Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling**  
[[Paper](http://arxiv.org/pdf/2506.05831v2)] [[Code/Page](https://github.com/DCDmllm/Heartcare-Suite)] [[TLDR/Notes](#heartcare-suite--multi-dimensional-understanding-of-ecg-with-raw-multi-lead-signal-modeling)]

- [25/06] **STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset**  
[[Paper](http://arxiv.org/pdf/2506.01738v1)] [[Code/Page](https://storm-bench.github.io/.)] [[TLDR/Notes](#storm--benchmarking-visual-rating-of-mllms-with-a-comprehensive-ordinal-regression-dataset)]

- [25/06] **MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book**  
[[Paper](http://arxiv.org/pdf/2506.00855v1)] [[Code/Page]()] [[TLDR/Notes](#medbookvqa--a-systematic-and-comprehensive-medical-benchmark-derived-from-open-access-book)]

- [25/06] **QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training**  
[[Paper](http://arxiv.org/pdf/2506.00711v1)] [[Code/Page](https://github.com/DDVD233/QoQ_Med.)] [[TLDR/Notes](#qoq-med--building-multimodal-clinical-foundation-models-with-domain-aware-grpo-training)]



# TLDR/Notes
## multimodal-large-language-models-for-medical-report-generation-via-customized-prompt-tuning
### Abstract
Medical report generation from imaging data remains a challenging task in
clinical practice. While large language models (LLMs) show great promise in
addressing this challenge, their effective integration with medical imaging
data still deserves in-depth exploration. In this paper, we present MRG-LLM, a
novel multimodal large language model (MLLM) that combines a frozen LLM with a
learnable visual encoder and introduces a dynamic prompt customization
mechanism. Our key innovation lies in generating instance-specific prompts
tailored to individual medical images through conditional affine
transformations derived from visual features. We propose two implementations:
prompt-wise and promptbook-wise customization, enabling precise and targeted
report generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets
demonstrate that MRG-LLM achieves state-of-the-art performance in medical
report generation. Our code will be made publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒ»ç–—æŠ¥å‘Šç”Ÿæˆæ–°çªç ´ï¼šMRG - LLM å€ŸåŠ©å®šåˆ¶åŒ–æç¤ºè°ƒä¼˜æ‰“é€ å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŒ»ç–—æŠ¥å‘Šç”Ÿæˆæ˜¯ä¸´åºŠå®è·µçš„å…³é”®ç¯èŠ‚ï¼Œä½†ä»å½±åƒæ•°æ®ç”ŸæˆåŒ»ç–—æŠ¥å‘Šè€—æ—¶è´¹åŠ›ï¼Œç»™åŒ»æŠ¤äººå‘˜å¸¦æ¥æ²‰é‡è´Ÿæ‹…ã€‚è‡ªåŠ¨åŒ»ç–—æŠ¥å‘Šç”Ÿæˆæ–¹æ³•è™½æœ‰ç ”ç©¶ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•å—é™äºåŒ»ç–—é¢†åŸŸè®­ç»ƒæ•°æ®é‡å°‘ï¼ˆå¦‚ IU X - ray å’Œ MIMIC - CXR æ•°æ®é›†è¿œå°äºé€šç”¨å›¾åƒæè¿°æ•°æ®é›†ï¼‰ï¼Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›å­˜åœ¨å±€é™ã€‚å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½æœ‰æ½œåŠ›ï¼Œä½†ä¸åŒ»å­¦å½±åƒæ•°æ®çš„æœ‰æ•ˆæ•´åˆä»éœ€æ·±å…¥æ¢ç´¢ï¼Œä¸”ç°æœ‰ç»“åˆ LLMs çš„æ–¹æ³•æ‰‹åŠ¨è®¾è®¡æŒ‡ä»¤éš¾åº¦å¤§ï¼Œé€šç”¨æç¤ºä¹Ÿæ— æ³•å……åˆ†æ•æ‰å•ä¸ªåŒ»å­¦å½±åƒçš„ç‹¬ç‰¹ç‰¹å¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºæ–°æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º MRG - LLM æ¨¡å‹
MRG - LLM æ˜¯ç”¨äºåŒ»ç–—æŠ¥å‘Šç”Ÿæˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†å†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹ã€å¯å­¦ä¹ çš„è§†è§‰ç¼–ç å™¨å’Œå®šåˆ¶åŒ–æç¤ºè°ƒä¼˜ã€‚è¯¥æ¨¡å‹åŒ…å«è§†è§‰ç¼–ç å™¨$f_{ve}$ã€æŠ•å½±å±‚$f_{proj}$å’Œé¢„è®­ç»ƒçš„ LLM  backbone $f_{llm}$ è¿™ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œèƒ½åœ¨è§†è§‰å’Œæ–‡æœ¬é¢†åŸŸé—´è¿›è¡Œæœ‰æ•ˆè·¨æ¨¡æ€å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒ LLM å¼ºå¤§çš„è¯­è¨€ç”Ÿæˆèƒ½åŠ›ã€‚è§†è§‰ç¼–ç å™¨å¤„ç†åŒ»å­¦å›¾åƒå¾—åˆ°è§†è§‰ç‰¹å¾ï¼ŒæŠ•å½±å±‚å°†è§†è§‰ç‰¹å¾æ˜ å°„ä¸ºè§†è§‰ tokensï¼ŒLLM  backbone å¤„ç†åŒ…å«è§†è§‰ tokensã€æ–‡æœ¬ tokens å’Œå¯å­¦ä¹ æç¤º tokens çš„æ··åˆåºåˆ—æ¥ç”ŸæˆæŠ¥å‘Šã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºæç¤ºå®šåˆ¶æœºåˆ¶åŠä¸¤ç§å®ç°æ–¹å¼
æå‡ºåŠ¨æ€æç¤ºå®šåˆ¶æœºåˆ¶ï¼ŒåŸºäºåŒ»å­¦å›¾åƒè§†è§‰ç‰¹å¾ç”Ÿæˆç‰¹å®šå®ä¾‹çš„æç¤ºï¼Œä»¥å®ç°æ›´ç²¾å‡†æœ‰é’ˆå¯¹æ€§çš„æŠ¥å‘Šç”Ÿæˆã€‚
 - Prompt - Wise Customizationï¼šå­¦ä¹ å‡½æ•°$\phi$ä»è§†è§‰ç‰¹å¾$X$ç”Ÿæˆå›¾åƒç‰¹å®šçš„å˜æ¢å‚æ•°$(\gamma, \beta)$ï¼Œé€šè¿‡é€æç¤ºçš„ä»¿å°„å˜æ¢$p'_i = \gamma_i p_i + \beta_i$ä¿®æ”¹æç¤ºç°¿ä¸­çš„æ¯ä¸ªåŸºç¡€æç¤ºï¼Œå…¶ä¸­$p_i$æ˜¯æç¤ºç°¿$P$ä¸­ç¬¬$i$ä¸ªå¯å­¦ä¹ åŸºç¡€æç¤ºã€‚
 - Promptbook - Wise Customizationï¼šä½¿ç”¨å…¨å±€å˜æ¢ä½œç”¨äºæ•´ä¸ªæç¤ºç°¿ï¼Œç”±å‡½æ•°$\phi$ä»è§†è§‰ç‰¹å¾$X$ç”Ÿæˆå˜æ¢å‚æ•°$(\gamma, \beta)$ï¼Œä»¥ç¼©æ”¾å’Œå¹³ç§»æ•´ä¸ªæç¤ºç°¿$P$ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ IU X - ray å’Œ MIMIC - CXR ä¸¤ä¸ªå…¬å¼€åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ MRG - LLM åœ¨åŒ»ç–—æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - æ¨¡å‹æ¶æ„è®¾è®¡ä¸Šï¼Œå°†å†»ç»“ LLM ä¸å¯å­¦ä¹ è§†è§‰ç¼–ç å™¨ç»“åˆçš„æ€è·¯ï¼Œä¸ºå¤šæ¨¡æ€ä»»åŠ¡ä¸­åˆ©ç”¨ LLM çŸ¥è¯†å’Œå¤„ç†ç‰¹å®šé¢†åŸŸè§†è§‰ä¿¡æ¯æä¾›äº†å‚è€ƒï¼Œå¯å€Ÿé‰´äºå…¶ä»–éœ€è¦è·¨æ¨¡æ€å­¦ä¹ çš„ä»»åŠ¡åœºæ™¯ã€‚
 - æç¤ºå®šåˆ¶æœºåˆ¶é’ˆå¯¹ä¸åŒå®ä¾‹ç”Ÿæˆç‰¹å®šæç¤ºï¼Œè§£å†³é€šç”¨æç¤ºæ— æ³•é€‚é…ä¸ªä½“å·®å¼‚çš„é—®é¢˜ï¼Œè¿™ç§æ ¹æ®å®ä¾‹ç‰¹å¾åŠ¨æ€è°ƒæ•´æç¤ºçš„æ€æƒ³ï¼Œåœ¨éœ€è¦ç²¾å‡†é€‚é…ä¸ªä½“æ•°æ®çš„ä»»åŠ¡ï¼ˆå¦‚ä¸ªæ€§åŒ–æ¨èã€ç‰¹å®šé¢†åŸŸæ–‡æœ¬ç”Ÿæˆç­‰ï¼‰ä¸­å…·æœ‰å€Ÿé‰´ä»·å€¼ï¼Œå¯ç”¨äºæ”¹è¿›æç¤ºè°ƒä¼˜ç­–ç•¥ã€‚
 - å®éªŒéªŒè¯å……åˆ†ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸ŠéªŒè¯æ€§èƒ½è¾¾åˆ° SOTAï¼Œè¿™ç§åŸºäºå…¬å¼€æ•°æ®é›†éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§çš„æ–¹å¼ï¼Œä¸ºç ”ç©¶å·¥ä½œçš„å¯é æ€§éªŒè¯æä¾›äº†èŒƒä¾‹ï¼Œåç»­ç ”ç©¶åœ¨æ–¹æ³•éªŒè¯æ—¶å¯å‚è€ƒè¿™ç§å¯¹å…¬å¼€åŸºå‡†æ•°æ®é›†çš„å……åˆ†å®éªŒã€‚

## doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback
### Abstract
Paucity of medical data severely limits the generalizability of diagnostic ML
models, as the full spectrum of disease variability can not be represented by a
small clinical dataset. To address this, diffusion models (DMs) have been
considered as a promising avenue for synthetic image generation and
augmentation. However, they frequently produce medically inaccurate images,
deteriorating the model performance. Expert domain knowledge is critical for
synthesizing images that correctly encode clinical information, especially when
data is scarce and quality outweighs quantity. Existing approaches for
incorporating human feedback, such as reinforcement learning (RL) and Direct
Preference Optimization (DPO), rely on robust reward functions or demand
labor-intensive expert evaluations. Recent progress in Multimodal Large
Language Models (MLLMs) reveals their strong visual reasoning capabilities,
making them adept candidates as evaluators. In this work, we propose a novel
framework, coined MAGIC (Medically Accurate Generation of Images through
AI-Expert Collaboration), that synthesizes clinically accurate skin disease
images for data augmentation. Our method creatively translates expert-defined
criteria into actionable feedback for image synthesis of DMs, significantly
improving clinical accuracy while reducing the direct human workload.
Experiments demonstrate that our method greatly improves the clinical quality
of synthesized skin disease images, with outputs aligning with dermatologist
assessments. Additionally, augmenting training data with these synthesized
images improves diagnostic accuracy by +9.02% on a challenging 20-condition
skin disease classification task, and by +13.89% in the few-shot setting.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒ»ç”Ÿè®¤å¯ï¼šé€šè¿‡AI-ä¸“å®¶åé¦ˆç”ŸæˆåŒ»å­¦å‡†ç¡®çš„çš®è‚¤ç—…å›¾åƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨çš®è‚¤ç—…è¯Šæ–­çš„æœºå™¨å­¦ä¹ æ¨¡å‹é¢†åŸŸï¼ŒåŒ»ç–—æ•°æ®åŒ®ä¹ä¸¥é‡é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºå°çš„ä¸´åºŠæ•°æ®é›†æ— æ³•æ¶µç›–ç–¾ç—…å˜å¼‚çš„å…¨èŒƒå›´ã€‚æ‰©æ•£æ¨¡å‹è™½è¢«è§†ä¸ºåˆæˆå›¾åƒç”Ÿæˆä¸æ•°æ®å¢å¼ºçš„æœ‰å‰æ™¯é€”å¾„ï¼Œä½†å¸¸ç”ŸæˆåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚ç°æœ‰èå…¥äººç±»åé¦ˆçš„æ–¹æ³•ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼‰ä¾èµ–å¯é å¥–åŠ±å‡½æ•°æˆ–éœ€è¦å¤§é‡ä¸“å®¶è¯„ä¼°å·¥ä½œã€‚è€Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰æ¨ç†èƒ½åŠ›ä½¿å…¶é€‚åˆä½œä¸ºè¯„ä¼°è€…ï¼ŒåŸºäºæ­¤ï¼Œæœ¬æ–‡å¸Œæœ›æå‡ºæ–°æ¡†æ¶æ¥ç”ŸæˆåŒ»å­¦å‡†ç¡®çš„çš®è‚¤ç—…å›¾åƒç”¨äºæ•°æ®å¢å¼ºï¼ŒåŒæ—¶å‡å°‘ä¸“å®¶ç›´æ¥å·¥ä½œé‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMAGICæ¡†æ¶  
æå‡ºåä¸ºMAGICï¼ˆMedically Accurate Generation of Images through AI - Expert Collaborationï¼‰çš„æ–°é¢–æ¡†æ¶ï¼Œå°†ä¸“å®¶å®šä¹‰çš„æ ‡å‡†è½¬åŒ–ä¸ºæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å›¾åƒåˆæˆçš„å¯æ“ä½œåé¦ˆï¼Œåœ¨æé«˜ä¸´åºŠå‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘äººç±»ç›´æ¥å·¥ä½œé‡ã€‚è¯¥æ¡†æ¶è®©äººç±»ä¸“å®¶ä¸»è¦å®Œæˆä¸¤é¡¹å·¥ä½œï¼šä»å¯ä¿¡æ¥æºåˆ¶å®šMLLMæ˜“éªŒè¯çš„æ£€æŸ¥æ¸…å•ï¼›åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹è®­ç»ƒæœŸé—´ç›‘ç£MLLMå¯¹åˆæˆå›¾åƒçš„åé¦ˆï¼Œé€šè¿‡è¿­ä»£å­¦ä¹ è¿™äº›åé¦ˆå¼•å¯¼T2I DMsç”Ÿæˆæ›´ç¬¦åˆåŒ»å­¦ä¸€è‡´æ€§çš„å›¾åƒã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé›†æˆI2Iæ¨¡å—  
åœ¨è®­ç»ƒæµç¨‹ä¸­é›†æˆå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰æ¨¡å—ï¼Œä»ä¸­é—´æ—¶é—´æ­¥è€Œä¸æ˜¯çº¯é«˜æ–¯å™ªå£°å¼€å§‹å»å™ªï¼Œè¿™åœ¨åŠ é€Ÿé‡‡æ ·é˜¶æ®µçš„åŒæ—¶ç¡®ä¿ç—…å˜å˜æ¢ä¸ä¼šè¿‡åº¦åç¦»çœŸå®æ•°æ®åˆ†å¸ƒã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAI - ä¸“å®¶åä½œèŒƒå¼  
é‡‡ç”¨AI - ä¸“å®¶åä½œèŒƒå¼ï¼Œåœ¨æœ€å°‘çš„ä¸“å®¶ç›‘ç£ä¸‹å°†è§†è§‰è¯„ä¼°å·¥ä½œè½¬ç§»ç»™å¼ºå¤§çš„MLLMï¼Œå¤§å¹…å‡å°‘åŒ»å­¦ä¸“å®¶æ‰€éœ€çš„æ—¶é—´å’ŒåŠ³åŠ¨ã€‚å¹¶ä¸”è¯¥æ¡†æ¶èƒ½ç»“åˆåŸºäºå¥–åŠ±çš„å¾®è°ƒï¼ˆRFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­ä¸DPOç»“åˆï¼ˆMAGIC - DPOï¼‰è¡¨ç°çªå‡ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜è¯¥æ–¹æ³•å¤§å¹…æå‡äº†åˆæˆçš®è‚¤ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œè¾“å‡ºä¸çš®è‚¤ç§‘åŒ»ç”Ÿè¯„ä¼°ä¸€è‡´ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„20ç§çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç”¨åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ä½¿è¯Šæ–­å‡†ç¡®ç‡æé«˜äº†9.02%ï¼›åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå‡†ç¡®ç‡æé«˜äº†13.89%ã€‚åŒæ—¶ï¼Œçš®è‚¤ç§‘åŒ»ç”Ÿè¯„ä¼°åˆ†æ•°æé«˜ã€FrÃ©chet Inception Distanceï¼ˆFIDï¼‰åˆ†æ•°é™ä½ï¼Œè¡¨æ˜ä¸´åºŠå‡†ç¡®æ€§å’Œä¿çœŸåº¦æå‡ï¼Œä¸”éšç€è®­ç»ƒæ¨è¿›å’Œæ›´å¤šå›¾åƒ - åé¦ˆå¯¹çš„ä½¿ç”¨ï¼Œæ¨¡å‹ç”Ÿæˆæ•ˆæœæŒç»­æ”¹å–„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šMAGICæ¡†æ¶æ•´åˆä¸“å®¶çŸ¥è¯†åˆ°æ‰©æ•£æ¨¡å‹çš„æ€è·¯ï¼Œä¸ºé¢†åŸŸçŸ¥è¯†èå…¥ç”Ÿæˆæ¨¡å‹æä¾›äº†å‚è€ƒï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—ç­‰ä¸“ä¸šé¢†åŸŸï¼Œå¯å€Ÿé‰´è¿™ç§å°†ä¸“ä¸šæ ‡å‡†è½¬åŒ–ä¸ºæ¨¡å‹å¯æ“ä½œåé¦ˆçš„æ–¹å¼ã€‚  
2. æ¨¡å—é›†æˆï¼šå…¶é›†æˆI2Iæ¨¡å—åŠ é€Ÿé‡‡æ ·å¹¶ä¿è¯æ•°æ®åˆ†å¸ƒåˆç†æ€§çš„åšæ³•ï¼Œå¯¹äºå…¶ä»–ç”Ÿæˆç±»ä»»åŠ¡åœ¨æ•ˆç‡å’Œæ•°æ®çœŸå®æ€§å¹³è¡¡ä¸Šæœ‰å€Ÿé‰´æ„ä¹‰ã€‚  
3. åä½œèŒƒå¼ï¼šAI - ä¸“å®¶åä½œå‡å°‘ä¸“å®¶å·¥ä½œé‡çš„èŒƒå¼ï¼Œåœ¨éœ€è¦ä¸“ä¸šé¢†åŸŸçŸ¥è¯†è¯„ä¼°ä½†ä¸“å®¶èµ„æºæœ‰é™çš„åœºæ™¯ä¸‹ï¼Œå¦‚å…¶ä»–åŒ»å­¦å½±åƒç”Ÿæˆã€ç‰¹å®šå·¥ä¸šæ£€æµ‹å›¾åƒç”Ÿæˆç­‰é¢†åŸŸï¼Œéƒ½æœ‰å‚è€ƒä»·å€¼ã€‚  
4. ç»“åˆä¼˜åŒ–æ–¹æ³•ï¼šè¯¥æ¡†æ¶ç»“åˆRFTå’ŒDPOå¯¹æ‰©æ•£æ¨¡å‹å¾®è°ƒçš„æ–¹å¼ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ä¼˜åŒ–æä¾›äº†æ–°çš„å®è·µè·¯å¾„ï¼Œå¯å¯å‘åç»­é’ˆå¯¹ä¸åŒä»»åŠ¡é€‰æ‹©åˆé€‚ä¼˜åŒ–ç­–ç•¥æ¥æå‡æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸè¡¨ç°ã€‚

## medseg-r--reasoning-segmentation-in-medical-images-with-multimodal-large-language-models
### Abstract
Medical image segmentation is crucial for clinical diagnosis, yet existing
models are limited by their reliance on explicit human instructions and lack
the active reasoning capabilities to understand complex clinical questions.
While recent advancements in multimodal large language models (MLLMs) have
improved medical question-answering (QA) tasks, most methods struggle to
generate precise segmentation masks, limiting their application in automatic
medical diagnosis. In this paper, we introduce medical image reasoning
segmentation, a novel task that aims to generate segmentation masks based on
complex and implicit medical instructions. To address this, we propose
MedSeg-R, an end-to-end framework that leverages the reasoning abilities of
MLLMs to interpret clinical questions while also capable of producing
corresponding precise segmentation masks for medical images. It is built on two
core components: 1) a global context understanding module that interprets
images and comprehends complex medical instructions to generate multi-modal
intermediate tokens, and 2) a pixel-level grounding module that decodes these
tokens to produce precise segmentation masks and textual responses.
Furthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the
medical image reasoning segmentation task. It includes over 10,000 image-mask
pairs and multi-turn conversations, automatically annotated using large
language models and refined through physician reviews. Experiments show
MedSeg-R's superior performance across several benchmarks, achieving high
segmentation accuracy and enabling interpretable textual analysis of medical
images.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MedSeg-Rï¼šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°åŒ»å­¦å›¾åƒçš„æ¨ç†å¼åˆ†å‰²

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠè¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ¨¡å‹å­˜åœ¨æ˜æ˜¾å±€é™ï¼šä¸€æ˜¯è¿‡åº¦ä¾èµ–**æ˜ç¡®çš„äººå·¥æŒ‡ä»¤**ï¼ˆå¦‚â€œåˆ†å‰²æ–°å† æ„ŸæŸ“åŒºåŸŸâ€ï¼‰ï¼Œéš¾ä»¥åº”å¯¹å¼€æ”¾ã€éšå«çš„ä¸´åºŠé—®é¢˜ï¼ˆå¦‚â€œè¿™æ¬¡æ£€æŸ¥èƒ½æ­ç¤ºæ‚£è€…å“ªäº›æ½œåœ¨ç—…ç—‡ï¼Ÿâ€ï¼‰ï¼›äºŒæ˜¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è™½åœ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå´**ç¼ºä¹åƒç´ çº§çš„åˆ†å‰²èƒ½åŠ›**ï¼Œæ— æ³•ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©ç ï¼Œé™åˆ¶äº†è‡ªåŠ¨è¯Šæ–­çš„è½åœ°ã€‚å› æ­¤ï¼Œè®ºæ–‡æå‡ºâ€œåŒ»å­¦å›¾åƒæ¨ç†å¼åˆ†å‰²â€è¿™ä¸€å…¨æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è®©æ¨¡å‹åŸºäºå¤æ‚ã€éšå«çš„åŒ»ç–—æŒ‡ä»¤ï¼ŒåŒæ—¶è¾“å‡ºæ–‡å­—è¯Šæ–­å’Œå¯¹åº”åˆ†å‰²ç»“æœï¼Œå¡«è¡¥ç°æœ‰æŠ€æœ¯ gapã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰â€œåŒ»å­¦å›¾åƒæ¨ç†å¼åˆ†å‰²â€æ–°ä»»åŠ¡  
ä¼ ç»ŸåŒ»å­¦å›¾åƒåˆ†å‰²éœ€æ˜ç¡®ç±»åˆ«æŒ‡ä»¤ï¼ˆå¦‚â€œåˆ†å‰²è‚¿ç˜¤â€ï¼‰ï¼Œè€Œæ–°ä»»åŠ¡è¦æ±‚æ¨¡å‹ç†è§£**å¤æ‚ã€éšå«çš„ä¸´åºŠé—®é¢˜**ï¼ˆå¦‚â€œæ£€æŸ¥ç»“æœåæ˜ æ‚£è€…å“ªäº›æ½œåœ¨ç–¾ç—…ï¼Ÿâ€ï¼‰ï¼Œå¹¶åŒæ­¥ç”Ÿæˆæ–‡å­—è§£é‡Šä¸åƒç´ çº§åˆ†å‰²æ©ç ï¼Œæ›´è´´è¿‘çœŸå®è¯Šç–—åœºæ™¯ä¸­åŒ»ç”Ÿå¼€æ”¾å¼é—®è¯Šã€ç³»ç»Ÿè‡ªä¸»åˆ†æçš„éœ€æ±‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºç«¯åˆ°ç«¯æ¡†æ¶ MedSeg-R  
MedSeg-R ç”±ä¸¤å¤§æ ¸å¿ƒæ¨¡å—æ„æˆï¼š  
- **å…¨å±€ä¸Šä¸‹æ–‡ç†è§£æ¨¡å—ï¼ˆGCUï¼‰**ï¼šè´Ÿè´£è§£æåŒ»å­¦å›¾åƒä¸å¤æ‚åŒ»ç–—æŒ‡ä»¤ï¼Œç”Ÿæˆå¤šæ¨¡æ€ä¸­é—´ tokensï¼Œå®ç°â€œå›¾åƒ + æ–‡æœ¬â€çš„æ·±åº¦è¯­ä¹‰ç†è§£ï¼›  
- **åƒç´ çº§é”šå®šæ¨¡å—ï¼ˆPGï¼‰**ï¼šè§£ç  GCU è¾“å‡ºçš„ tokensï¼Œä¸€æ–¹é¢ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©ç ï¼Œå¦ä¸€æ–¹é¢è¾“å‡ºå¯è§£é‡Šçš„æ–‡å­—è¯Šæ–­ï¼Œè®©â€œæ¨ç† + åˆ†å‰² + æ–‡æœ¬è§£é‡Šâ€åœ¨ä¸€ä¸ªæ¡†æ¶å†…å®Œæˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºå¤§è§„æ¨¡æ•°æ®é›† MedSeg-QA  
ä¸ºæ”¯æ’‘æ–°ä»»åŠ¡ï¼Œè®ºæ–‡æ‰“é€ äº† MedSeg-QA æ•°æ®é›†ï¼šåŒ…å«è¶… 1 ä¸‡ç»„â€œå›¾åƒ - æ©ç â€å¯¹ï¼Œæ­é…å¤šè½®å¯¹è¯ã€‚æ•°æ®é›†é€šè¿‡å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ ‡æ³¨ + åŒ»ç”Ÿå¤å®¡çš„æ–¹å¼æ„å»ºï¼Œæ—¢ä¿è¯è§„æ¨¡åˆç¡®ä¿åŒ»å­¦ä¸“ä¸šæ€§ï¼Œä¸ºæ¨ç†å¼åˆ†å‰²ä»»åŠ¡æä¾›äº†é«˜è´¨é‡è®­ç»ƒèµ„æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº† MedSeg-R çš„æ€§èƒ½ï¼š  
- **åˆ†å‰²ç²¾åº¦**ï¼šåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜æ¨¡å‹èƒ½ç²¾å‡†å®šä½ç—…ç†åŒºåŸŸï¼›  
- **æ¨ç†ä¸æ–‡æœ¬åˆ†æèƒ½åŠ›**ï¼šèƒ½åŸºäºéšå«æŒ‡ä»¤ç”Ÿæˆåˆç†æ–‡å­—è¯Šæ–­ï¼Œä¸”åˆ†å‰²ç»“æœä¸æ–‡å­—è§£é‡Šé€»è¾‘ä¸€è‡´ï¼Œå®ç°äº†â€œå¯è§£é‡Šçš„åŒ»å­¦å›¾åƒåˆ†æâ€ã€‚  

å¯¹æ¯”ç°æœ‰åŒ»ç–—å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚ miniGPT4ã€BiomedGPT ç­‰ï¼‰ï¼ŒMedSeg-R åŒæ—¶å…·å¤‡â€œåƒç´ çº§åˆ†å‰²èƒ½åŠ›â€â€œå¤šè½®å¯¹è¯äº¤äº’â€ä¸â€œç«¯åˆ°ç«¯è®­ç»ƒâ€ä¸‰å¤§ç‰¹æ€§ï¼Œåœ¨ç»¼åˆæ€§åŒ»ç–—å›¾åƒåˆ†æä¸­ä¼˜åŠ¿æ˜¾è‘—ï¼ˆè§è®ºæ–‡ä¸­ Table 1 å¯¹æ¯”ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ä»»åŠ¡å®šä¹‰åˆ›æ–°**ï¼šæå‡ºâ€œæ¨ç†å¼åˆ†å‰²â€è¿™ä¸€æ›´è´´è¿‘çœŸå®ä¸´åºŠåœºæ™¯çš„ä»»åŠ¡ï¼Œæ‹“å±•äº†åŒ»å­¦å›¾åƒåˆ†æçš„è¾¹ç•Œï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜æ–°æ–¹å‘ï¼›  
2. **å¤šæ¨¡æ€èåˆæ€è·¯**ï¼šé€šè¿‡ GCU + PG æ¨¡å—è®¾è®¡ï¼Œå®ç°â€œå¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›â€ä¸â€œåˆ†å‰²æ¨¡å‹åƒç´ çº§ç²¾åº¦â€çš„æ·±åº¦èåˆï¼Œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„è½åœ°æä¾›äº†æ¶æ„å‚è€ƒï¼›  
3. **æ•°æ®é›†æ„å»ºèŒƒå¼**ï¼šâ€œè‡ªåŠ¨æ ‡æ³¨ + ä¸“å®¶å¤å®¡â€çš„æµç¨‹é«˜æ•ˆæ„å»ºå¤§è§„æ¨¡åŒ»ç–—æ•°æ®é›†ï¼Œå¹³è¡¡äº†è§„æ¨¡ä¸ä¸“ä¸šæ€§ï¼Œå¯å¤ç”¨è‡³å…¶ä»–åŒ»ç–— AI ä»»åŠ¡çš„æ•°æ®é›†å»ºè®¾ï¼›  
4. **ä¸´åºŠä»·å€¼å¯¼å‘**ï¼šä»â€œä¾èµ–æ˜ç¡®æŒ‡ä»¤â€è½¬å‘â€œç†è§£éšå«é—®é¢˜å¹¶è‡ªä¸»åˆ†æâ€ï¼Œè®© AI ç³»ç»Ÿæ›´æ¥è¿‘çœŸå®è¯Šç–—é€»è¾‘ï¼Œæ¨åŠ¨åŒ»ç–— AI ä»â€œå·¥å…·è¾…åŠ©â€å‘â€œæ™ºèƒ½è¯Šæ–­ä¼™ä¼´â€è¿›åŒ–ã€‚  

MedSeg-R ä¸ä»…åœ¨æŠ€æœ¯ä¸Šçªç ´äº†ç°æœ‰æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œæ›´åœ¨è½åœ°åœºæ™¯ä¸­ä¸ºâ€œå…¨è‡ªåŠ¨ã€å¯è§£é‡Šçš„æ™ºèƒ½åŒ»ç–—è¯Šæ–­â€æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œæ˜¯å¤šæ¨¡æ€å¤§æ¨¡å‹èµ‹èƒ½åŒ»ç–—å½±åƒåˆ†æçš„ä¸€æ¬¡é‡è¦æ¢ç´¢ã€‚

## hsenet--hybrid-spatial-encoding-network-for-3d-medical-vision-language-understanding
### Abstract
Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.
### ğŸŒŸ è®ºæ–‡è§£è¯» | HSENetï¼šçªç ´3DåŒ»ç–—è§†è§‰è¯­è¨€ç†è§£çš„æ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
3Dè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰é©æ–°äº†åŒ»å­¦è¯Šæ–­ï¼Œä½†è§£è¯»3D CTå›¾åƒå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿæ¥è¯´è€—æ—¶ä¸”æ˜“å‡ºé”™ã€‚å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•å¤šèšç„¦äº2DåŒ»å­¦å›¾åƒï¼Œéš¾ä»¥æ•æ‰å¤æ‚çš„3Dè§£å‰–ç»“æ„ï¼Œæ˜“å¯¼è‡´å¯¹ç»†å¾®ç—…å˜çš„è¯¯åˆ¤å’Œè¯Šæ–­å¹»è§‰ã€‚åŒæ—¶ï¼Œç°æœ‰æ–¹æ³•è¿˜å­˜åœ¨è§†è§‰æ„ŸçŸ¥æœ‰é™ï¼ˆ3Dä½“ç§¯ - æŠ¥å‘Šå¯¹ç¨€ç¼ºé™åˆ¶ç‰¹å¾æ”¶æ•›ï¼‰å’Œè¯­ä¹‰æŠ•å½±å—æŸï¼ˆéš¾ä»¥ä¿ç•™3Dè§£å‰–ç»“æ„çš„ç©ºé—´å’Œå‡ ä½•ç»†èŠ‚ï¼‰ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºHSENetæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒ3Dè§†è§‰ç¼–ç å™¨ä¸åŒé˜¶æ®µé¢„è®­ç»ƒ  
HSENeté‡‡ç”¨åŒ3Dè§†è§‰ç¼–ç å™¨æ¥æ„ŸçŸ¥å…¨å±€ä½“ç§¯ä¸Šä¸‹æ–‡å’Œç»†ç²’åº¦è§£å‰–ç»†èŠ‚ã€‚å…¶ä¸­ï¼Œ3D Vision Encoderå­¦ä¹ ä¸æŠ¥å‘Šå¯¹é½çš„å…¨å±€ä½“ç§¯è¡¨ç¤ºï¼›2D - Enhanced 3D Vision Encoderï¼ˆ2E3 Vision Encoderï¼‰åˆ™å€ŸåŠ©ä»2Dåˆ‡ç‰‡ä¸­è¯†åˆ«çš„ä¸°å¯Œè¯Šæ–­è§è§£ï¼Œä¼˜åŒ–ä¸æŠ¥å‘Šå¯¹é½çš„è§£å‰–ç»†èŠ‚ã€‚ä¸”è¿™ä¸¤ä¸ªç¼–ç å™¨é€šè¿‡ä¸è¯Šæ–­æŠ¥å‘Šçš„åŒé˜¶æ®µå¯¹é½è¿›è¡Œé¢„è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSpatial Packerä¸Voxel2Point Cross - Attention  
æå‡ºSpatial Packerè¿™ä¸€é«˜æ•ˆçš„å¤šæ¨¡æ€æŠ•å½±ä»ªï¼Œç”¨äºå°†3Dè§†è§‰ä¸Šä¸‹æ–‡å‹ç¼©ä¸ºç´§å‡‘çš„ä¿¡æ¯è§†è§‰æ ‡è®°é›†åˆã€‚è¯¥æŠ•å½±ä»ªé›†æˆäº†æ–°é¢–çš„Voxel2Point Cross - Attentionï¼ˆV2P - CAï¼‰ï¼Œé€šè¿‡å°†é«˜åˆ†è¾¨ç‡3Dä½“ç´ è¡¨ç¤ºèšåˆåˆ°å…¶è´¨å¿ƒç‚¹ï¼Œä¿ç•™äº†å…³é”®çš„ç©ºé—´å’Œå‡ ä½•ä¿¡æ¯ï¼Œä»è€Œå°†æå–çš„è§†è§‰è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„è¯­ä¹‰ç©ºé—´ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨3Dè¯­è¨€ - è§†è§‰æ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒHSENetè¾¾åˆ°39.85%çš„R@100ï¼Œç›¸æ¯”ä¹‹å‰æå‡äº†5.96%ï¼›åœ¨3DåŒ»ç–—æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒBLEU - 4æŒ‡æ ‡è¾¾åˆ°24.01%ï¼Œæå‡äº†8.01%ï¼›åœ¨3Dè§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼ŒMajor Class Accuracyè¾¾åˆ°73.60%ï¼Œæå‡äº†1.99%ã€‚å®éªŒç»“æœè¡¨æ˜HSENetåœ¨ç”Ÿæˆæœ‰åˆ¤åˆ«æ€§çš„è§†è§‰è¡¨ç¤ºå’Œé«˜è´¨é‡è¯Šæ–­å“åº”æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€èåˆæ€è·¯ï¼šé€šè¿‡åŒ3Dè§†è§‰ç¼–ç å™¨åˆ†åˆ«å¤„ç†å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ï¼Œä¸ºå¤„ç†å¤æ‚å¤šå°ºåº¦æ•°æ®æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å¤šæ¨¡æ€ç‰¹å¾æå–èŒƒå¼ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–éœ€è¦å¤„ç†å¤šå°ºåº¦ä¿¡æ¯çš„3Dè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­ã€‚
2. ç©ºé—´ä¿¡æ¯ä¿ç•™æ–¹æ³•ï¼šSpatial Packerä¸­åˆ©ç”¨Voxel2Point Cross - Attentionæ¥ä¿ç•™ç©ºé—´å’Œå‡ ä½•ä¿¡æ¯çš„æ–¹å¼ï¼Œä¸ºè§£å†³3Dæ•°æ®åœ¨æŠ•å½±åˆ°è¯­è¨€æ¨¡å‹è¯­ä¹‰ç©ºé—´æ—¶çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜æä¾›äº†åˆ›æ–°æ€è·¯ï¼Œåœ¨æ¶‰åŠ3Dæ•°æ®ä¸è¯­è¨€æ¨¡å‹ç»“åˆçš„ä»»åŠ¡ä¸­å…·æœ‰å‚è€ƒä»·å€¼ã€‚
3. é¢„è®­ç»ƒç­–ç•¥ï¼šåŒé˜¶æ®µå¯¹é½é¢„è®­ç»ƒçš„æ–¹å¼ï¼Œåˆ©ç”¨2Dåˆ‡ç‰‡è¾…åŠ©å¢å¼º3Dè§†è§‰æ„ŸçŸ¥ï¼Œä¸ºåœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹æå‡æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰æ•°æ®çš„ç†è§£èƒ½åŠ›æä¾›äº†å€Ÿé‰´æ–¹å‘ã€‚

## haibu-remud--reasoning-multimodal-ultrasound-dataset-and-model-bridging-to-general-specific-domains
### Abstract
Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | HAIBU-ReMUDï¼šè§£å†³ç‰¹å®šé¢†åŸŸå¤šæ¨¡æ€å¤§æ¨¡å‹æ•°æ®çŸ­ç¼ºéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨é€šç”¨é¢†åŸŸï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦è¶…å£°é¢†åŸŸï¼‰è¡¨ç°ä¸ä½³ï¼ŒåŸå› æ˜¯ç¼ºä¹é¢†åŸŸç‰¹å®šæ•°æ®ï¼ˆåƒå›¾åƒ - æ–‡æœ¬ã€è§†é¢‘ - æ–‡æœ¬æ•°æ®ï¼‰ã€‚ç‰¹å®šé¢†åŸŸå­˜åœ¨å¤§é‡å›¾æ–‡æ•°æ®ï¼Œä½†å¤šä»¥PDFã€å›¾åƒç­‰å½¢å¼é›¶æ•£ä¿å­˜ï¼Œæ— æ³•ç›´æ¥ç”¨äºMLLMsè®­ç»ƒã€‚æ¯”å¦‚åŒ»å­¦è¶…å£°é¢†åŸŸï¼Œè™½æœ‰è¶…å£°è¯Šæ–­ä¹¦ç±ã€ä¸´åºŠæŒ‡å—ã€è¯Šæ–­æŠ¥å‘Šç­‰èµ„æºï¼Œå´éš¾ä»¥ç›´æ¥ä¸ºæ¨¡å‹è®­ç»ƒæ‰€ç”¨ã€‚æ‰€ä»¥ï¼Œéœ€è¦ä¸€ç§ç®€å•è‡ªåŠ¨çš„æ–¹å¼æ¥æ„å»ºç‰¹å®šé¢†åŸŸçš„å¤šæ¨¡æ€æ•°æ®ï¼Œä»¥æå‡MLLMsåœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç‰¹å®šé¢†åŸŸå››å…ƒç»„ç”Ÿæˆ pipeline
æå‡ºä¸€ç§æ–°é¢–çš„å›¾åƒ - æ–‡æœ¬æ¨ç†æœ‰ç›‘ç£å¾®è°ƒæ•°æ®ç”Ÿæˆ pipelineï¼Œä»ç‰¹å®šé¢†åŸŸææ–™ä¸­åˆ›å»ºç‰¹å®šé¢†åŸŸå››å…ƒç»„ï¼ˆå›¾åƒã€é—®é¢˜ã€æ€è€ƒè½¨è¿¹ã€ç­”æ¡ˆï¼‰ã€‚è¯¥ pipeline æ— éœ€äººå·¥æ ‡æ³¨æˆ–é¢„ç”Ÿæˆæ•°æ®é›†ï¼Œèƒ½ç¡®ä¿ä¸“ä¸šçŸ¥è¯†çš„å‡†ç¡®æ€§ï¼Œä¸”é€‚ç”¨äºå„ç§é€šç”¨ç‰¹å®šé¢†åŸŸã€‚ä»¥åŒ»å­¦è¶…å£°é¢†åŸŸä¸ºä¾‹ï¼Œåˆ©ç”¨Qwen2.5 - VLçš„è¾¹ç•Œæ¡†åŠŸèƒ½ä»¥åŠGPT - 4oã€Gemini - 2.0 - Flash - Thinking - Expçš„å¤šæ¨¡æ€å›¾æ–‡è¯†åˆ«ä¸ç”Ÿæˆèƒ½åŠ›æ¥æ„å»ºæ•°æ®ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºReMUDæ•°æ®é›†
å»ºç«‹äº†åŒ»å­¦è¶…å£°é¢†åŸŸæ•°æ®é›†ReMUDï¼ŒåŒ…å«è¶…è¿‡45000æ¡æ¨ç†å’Œéæ¨ç†çš„æœ‰ç›‘ç£å¾®è°ƒé—®ç­”ï¼ˆQAï¼‰å’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®ã€‚è¿™äº›æ•°æ®ä¸ºç‰¹å®šé¢†åŸŸMLLMsè®­ç»ƒæä¾›äº†ä¸°å¯Œèµ„æºï¼Œæ¶µç›–äº†å¤šç§åŒ»å­¦è¶…å£°ç›¸å…³çš„ç”Ÿç‰©ç»“æ„ã€ä¸“ä¸šçŸ¥è¯†ç­‰å†…å®¹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®­ç»ƒReMUD - 7Bæ¨¡å‹
ä»¥Qwen2.5 - VL - 7B - Instructä¸ºåŸºç¡€æ¨¡å‹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œå¼€å‘å‡ºReMUD - 7Bæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨åŒ»å­¦è¶…å£°é¢†åŸŸçš„æµ‹è¯•æ•°æ®é›†ï¼ˆå¦‚USTQ - Knowledgeå’ŒUVQA - Diagnosisï¼‰ä¸Šï¼Œè¡¨ç°ä¼˜äºé€šç”¨é¢†åŸŸçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡æ›´å¼ºçš„é¢†åŸŸæ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ReMUD - 7Bæ¨¡å‹åœ¨åŒ»å­¦è¶…å£°é¢†åŸŸçš„æµ‹è¯•ä¸­ï¼Œç›¸æ¯”é€šç”¨é¢†åŸŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¡¨ç°æ›´ä¼˜ã€‚æ‰€æ„å»ºçš„ReMUDæ•°æ®é›†åŒ…å«ä¸°å¯Œçš„åŒ»å­¦è¶…å£°é¢†åŸŸQAå’ŒVQAæ•°æ®ï¼Œæ¶µç›–æ¨ç†ä¸éæ¨ç†ç±»å‹ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†å……è¶³çš„é¢†åŸŸç‰¹å®šæ•°æ®æ”¯æ’‘ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®ç”Ÿæˆ pipeline å…·æœ‰é€šç”¨æ€§ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–ç‰¹å®šé¢†åŸŸï¼Œä¸ºä¸åŒé¢†åŸŸæ„å»ºå¤šæ¨¡æ€è®­ç»ƒæ•°æ®æä¾›äº†æ€è·¯ï¼Œè§£å†³ç‰¹å®šé¢†åŸŸæ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚
2. å…¬å¼€ReMUDæ•°æ®é›†ã€æ•°æ®ç”Ÿæˆä»£ç åº“å’ŒReMUD - 7Bæ¨¡å‹å‚æ•°ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç ”ç©¶ï¼Œæ¨åŠ¨ç‰¹å®šé¢†åŸŸå¤šæ¨¡æ€å¤§æ¨¡å‹çš„å‘å±•ã€‚
3. é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»æ•°æ®æ„å»ºåˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼Œä¸ºé¢†åŸŸé€‚é…çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ç ”ç©¶æä¾›äº†ä¸€å¥—å¯å‚è€ƒçš„èŒƒå¼ï¼Œæœ‰åŠ©äºåŠ é€Ÿç‰¹å®šé¢†åŸŸAIåº”ç”¨çš„è½åœ°ã€‚

## lingshu--a-generalist-foundation-model-for-unified-multimodal-medical-understanding-and-reasoning
### Abstract
Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...
### ğŸŒŸ è®ºæ–‡è§£è¯» | Lingshuï¼šé¢å‘åŒ»ç–—å¤šæ¨¡æ€ç†è§£ä¸æ¨ç†çš„é€šç”¨åŸºç¡€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨è§†è§‰å…ƒç´ ç†è§£ä¸Šå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨åŒ»ç–—åœºæ™¯åº”ç”¨ä¸­å—é™äºè¯¸å¤šé—®é¢˜ã€‚ç°æœ‰åŒ»ç–—MLLMså­˜åœ¨ä¸‰å¤§å…³é”®å±€é™ï¼šä¸€æ˜¯åŒ»ç–—çŸ¥è¯†è¦†ç›–ä»…èšç„¦å½±åƒï¼Œå¯¹å½±åƒå¤–çŸ¥è¯†æ¶‰åŠä¸è¶³ï¼›äºŒæ˜¯æ•°æ®å¤„ç†æµç¨‹æ¬ ä½³ï¼Œæ˜“å¼•å‘å¹»è§‰ï¼ˆè¾“å‡ºé”™è¯¯ä¿¡æ¯ï¼‰ï¼›ä¸‰æ˜¯ç¼ºä¹é’ˆå¯¹å¤æ‚åŒ»ç–—åœºæ™¯çš„æ¨ç†èƒ½åŠ›ã€‚åŒ»ç–—åœºæ™¯çš„æ•°æ®ä¸ä»»åŠ¡å’Œé€šç”¨é¢†åŸŸå­˜åœ¨æœ¬è´¨å·®å¼‚ï¼Œæ¯”å¦‚ç”Ÿç‰©åŒ»å­¦å›¾æ–‡å¯¹å’Œæ™®é€šç½‘ç»œå†…å®¹ä¸åŒï¼Œé€šç”¨è§†è§‰åŠ©æ‰‹éš¾åº”å¯¹ï¼Œæ¨¡å‹å¸¸å¯¹ç”Ÿç‰©åŒ»å­¦é—®é¢˜ç»™å‡ºä¸ç¡®å®šæˆ–é”™è¯¯ä¿¡æ¯ï¼Œå› æ­¤éœ€è¦ä¸“é—¨é’ˆå¯¹åŒ»ç–—åœºæ™¯æ‰“é€ æ›´ä¼˜æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢çš„æ•°æ®æ„å»ºæµç¨‹  
æå‡ºä¸€å¥—ç»¼åˆçš„æ•°æ®æ•´ç†æµç¨‹ï¼Œä¸€æ–¹é¢ä»åŒ»å­¦å½±åƒã€å¤§é‡åŒ»å­¦æ–‡æœ¬å’Œé€šç”¨é¢†åŸŸæ•°æ®ä¸­é«˜æ•ˆè·å–ä¸°å¯ŒåŒ»ç–—çŸ¥è¯†æ•°æ®ï¼›å¦ä¸€æ–¹é¢åˆæˆå‡†ç¡®çš„åŒ»ç–—æè¿°ã€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»¥åŠæ¨ç†æ ·æœ¬ï¼Œæœ€ç»ˆæ„å»ºå‡ºå¯Œå«å¹¿æ³›åŒ»ç–—çŸ¥è¯†çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›ä¼˜è´¨æ•°æ®åŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒ»ç–—ä¸“ç”¨MLLM - LingshuåŠå¤šé˜¶æ®µè®­ç»ƒ  
åŸºäºç²¾å¿ƒæ„å»ºçš„æ•°æ®ï¼Œæ‰“é€ åŒ»ç–—ä¸“ç”¨MLLM Lingshuã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒé€æ­¥åµŒå…¥åŒ»ç–—ä¸“ä¸šçŸ¥è¯†ï¼Œæå‡ä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚åŒæ—¶ï¼Œåˆæ­¥æ¢ç´¢å°†å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼åº”ç”¨äºå¢å¼ºLingshuçš„åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼Œå°è¯•çªç ´ç°æœ‰æ¨¡å‹æ¨ç†çŸ­æ¿ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»Ÿä¸€è¯„ä¼°æ¡†æ¶MedEvalKit  
å¼€å‘MedEvalKitï¼Œæ•´åˆä¸»æµå¤šæ¨¡æ€å’Œæ–‡æœ¬åŒ»ç–—åŸºå‡†ï¼Œä¸ºæ¨¡å‹æä¾›æ ‡å‡†åŒ–ã€å…¬å¹³ä¸”é«˜æ•ˆçš„è¯„ä¼°æ–¹å¼ï¼Œè§£å†³åŒ»ç–—æ¨¡å‹è¯„ä¼°ç¼ºä¹ç»Ÿä¸€é«˜æ•ˆæ¡†æ¶çš„é—®é¢˜ï¼ŒåŠ©åŠ›æ›´ç§‘å­¦åœ°è¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šæ¨¡æ€é—®ç­”ã€åŸºäºæ–‡æœ¬çš„é—®ç­”ã€åŒ»ç–—æŠ¥å‘Šç”Ÿæˆè¿™ä¸‰å¤§åŸºç¡€åŒ»ç–—ä»»åŠ¡ä¸Šè¯„ä¼°Lingshuæ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºLingshuåœ¨å¤šæ•°ä»»åŠ¡ä¸ŠæŒç»­è¶…è¶Šç°æœ‰å¼€æºå¤šæ¨¡æ€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¼€å±•äº”ä¸ªè´´è¿‘çœŸå®åœºæ™¯çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç°å‡ºLingshuåœ¨åŒ»ç–—å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºå±‚é¢ï¼šå¼ºè°ƒå¤šæ¥æºï¼ˆåŒ»ç–—å½±åƒã€æ–‡æœ¬ã€é€šç”¨é¢†åŸŸï¼‰æ•°æ®æ•´åˆä¸é«˜è´¨é‡æ ·æœ¬åˆæˆï¼Œä¸ºé¢†åŸŸæ¨¡å‹è®­ç»ƒæ•°æ®æ„å»ºæä¾›äº†ä»æ•°æ®è·å–åˆ°å¤„ç†çš„å®Œæ•´æ€è·¯ï¼Œåœ¨å‚ç›´é¢†åŸŸæ¨¡å‹æ‰“é€ æ—¶ï¼Œå¯å­¦ä¹ è¿™ç§å…¨é¢ä¸”ç²¾ç»†çš„æ•°æ®å¤„ç†æ–¹å¼ã€‚  
2. æ¨¡å‹è®­ç»ƒå±‚é¢ï¼šå¤šé˜¶æ®µè®­ç»ƒåµŒå…¥ä¸“ä¸šçŸ¥è¯†ä»¥åŠæ¢ç´¢å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„æ€è·¯ï¼Œä¸ºå‚ç›´é¢†åŸŸå¤§æ¨¡å‹è®­ç»ƒç­–ç•¥æä¾›å‚è€ƒï¼Œå°¤å…¶æ˜¯åŒ»ç–—è¿™ç±»å¯¹ä¸“ä¸šæ€§å’Œæ¨ç†è¦æ±‚é«˜çš„é¢†åŸŸï¼Œåˆ†å±‚è®­ç»ƒä¸å¼ºåŒ–å­¦ä¹ ç»“åˆå€¼å¾—å€Ÿé‰´ã€‚  
3. è¯„ä¼°æ¡†æ¶å±‚é¢ï¼šæ‰“é€ ç»Ÿä¸€å‚ç›´é¢†åŸŸè¯„ä¼°æ¡†æ¶MedEvalKitï¼Œè§£å†³é¢†åŸŸå†…æ¨¡å‹è¯„ä¼°æ ‡å‡†ä¸ä¸€é—®é¢˜ï¼Œå…¶ä»–å‚ç›´é¢†åŸŸï¼ˆå¦‚æ•™è‚²ã€é‡‘èï¼‰åœ¨æ„å»ºæ¨¡å‹è¯„ä¼°ä½“ç³»æ—¶ï¼Œå¯å‚è€ƒè¿™ç§æ•´åˆé¢†åŸŸåŸºå‡†çš„æ–¹å¼æ¥å®ç°å…¬å¹³é«˜æ•ˆè¯„ä¼°ã€‚

## heartcare-suite--multi-dimensional-understanding-of-ecg-with-raw-multi-lead-signal-modeling
### Abstract
We present Heartcare Suite, a multimodal comprehensive framework for
finegrained electrocardiogram (ECG) understanding. It comprises three key
components: (i) Heartcare-220K, a high-quality, structured, and comprehensive
multimodal ECG dataset covering essential tasks such as disease diagnosis,
waveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench,
a systematic and multi-dimensional benchmark designed to evaluate diagnostic
intelligence and guide the optimization of Medical Multimodal Large Language
Models (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored
tokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw
multi-lead signals into semantically rich discrete tokens via duallevel vector
quantization and query-guided bidirectional diffusion mechanism. Built upon
Heartcare-220K, HeartcareGPT achieves strong generalization and SoTA
performance across multiple clinically meaningful tasks. Extensive experiments
demonstrate that Heartcare Suite is highly effective in advancing ECGspecific
multimodal understanding and evaluation. Our project is available at
https://github.com/DCDmllm/Heartcare-Suite .
### ğŸŒŸ è®ºæ–‡è§£è¯» | Heartcare Suiteï¼šå¤šç»´åº¦è§£æå¿ƒç”µå›¾çš„æ¨¡æ€å¤§æ¨¡å‹æ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMed - MLLMsï¼‰çš„å‘å±•è¿›ç¨‹ä¸­ï¼Œé¢å‘å¿ƒç”µå›¾ï¼ˆECGï¼‰é¢†åŸŸçš„åº”ç”¨å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼Œç¼ºä¹é«˜è´¨é‡ã€å¯Œå«åŒ»å­¦çŸ¥è¯†çš„å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚è¯Šæ–­åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ï¼›ç°æœ‰æ•°æ®é›†ç–¾ç—…è°±è¦†ç›–æœ‰é™ã€å›¾åƒåˆ†è¾¨ç‡æ¬ ä½³ä¸”ä¸´åºŠæ ‡æ³¨ç»“æ„åŒ–ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³ç»†ç²’åº¦è¯Šæ–­å»ºæ¨¡éœ€æ±‚ã€‚å¦ä¸€æ–¹é¢ï¼Œç¼ºå°‘å…¨é¢çš„åŸºå‡†æµ‹è¯•æ¥ä»å¤šç»´åº¦è¯„ä¼°æ¨¡å‹ï¼Œå½“å‰è¯„ä¼°æ¡†æ¶å¤šä¾èµ–åˆ†ç±»å‡†ç¡®ç‡ç­‰åˆ¤åˆ«å¼æŒ‡æ ‡ï¼Œå¯¹ç”Ÿæˆç±»ä»»åŠ¡ï¼ˆå¦‚ä¸´åºŠæŠ¥å‘Šç”Ÿæˆã€å¼€æ”¾å¼é—®ç­”ï¼‰ç¼ºä¹ç³»ç»Ÿæ ‡å‡†ï¼Œé˜»ç¢äº†æ¨¡å‹åœ¨ECGç‰¹å®šåº”ç”¨ä¸­çš„ä¼˜åŒ–ä¸å‘å±•ã€‚æ­¤å¤–ï¼ŒECGä½œä¸ºé«˜åˆ†è¾¨ç‡å¤šå¯¼è”ç”Ÿç†ä¿¡å·ï¼Œå…·æœ‰é«˜é‡‡æ ·ç‡ã€å¤šåŒæ­¥é€šé“å’Œå¯¹æ•°å€¼å˜åŒ–æ•æ„Ÿç­‰ç‰¹æ€§ï¼Œç°æœ‰Med - MLLMså°†ECGæ³¢å½¢å›¾åƒä½œä¸ºè¾“å…¥ï¼Œæ˜“å¯¼è‡´ç‰¹å¾å†—ä½™å’Œé•¿å°¾åˆ†å¸ƒï¼Œä¸”ECGæ¨¡æ€ç¼ºä¹ç±»ä¼¼è‡ªç„¶è¯­è¨€tokenizationçš„ç¦»æ•£è¡¨ç¤ºæœºåˆ¶ï¼Œéš¾ä»¥åœ¨MLLMsè‡ªå›å½’æ¡†æ¶ä¸‹ç›´æ¥å»ºæ¨¡ECGåˆ°æ–‡æœ¬çš„è·¯å¾„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…å›¢é˜Ÿæå‡ºäº†Heartcare Suiteè¿™ä¸€é’ˆå¯¹ECGé¢†åŸŸç»†ç²’åº¦ç†è§£ä»»åŠ¡çš„å¤šæ¨¡æ€ç»¼åˆæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºHeartcare - 220Kå¤šæ¨¡æ€ECGæ•°æ®é›†
å›¢é˜Ÿæ•´åˆäº†å…¬å…±æ•°æ®é›†PTB - XLï¼ˆå«21,799æ¡12å¯¼è”ECGä¿¡å·åŠ179ä¸ªSCP - ECGç±»åˆ«æ ‡æ³¨ï¼‰ä¸æ¥è‡ªé¡¶å°–åŒ»é™¢çš„12,170å¼ å¸¦ç»“æ„åŒ–æŠ¥å‘Šçš„ECGå›¾åƒï¼ˆåŒ…å«æ‰«æè½¨è¿¹ã€ä¸´åºŠç»“è®ºå’Œå»æ ‡è¯†å…ƒæ•°æ®ï¼‰ï¼Œä¸°å¯Œäº†æ¨¡æ€å’Œæ ‡ç­¾å¤šæ ·æ€§ã€‚åŒæ—¶å¼€å‘HeartAgentå¤šæ™ºèƒ½ä½“å¼•æ“ï¼Œé€šè¿‡è‡ªåº•å‘ä¸Šçš„ pipeline ç¡®ä¿æ ‡æ³¨ä¸€è‡´æ€§ï¼Œç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤å¼é—®ç­”å¯¹ï¼Œå°†å¼‚æ„ECGæ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–ç›‘ç£ï¼Œæ”¯æŒç–¾ç—…è¯Šæ–­ã€æ³¢å½¢å½¢æ€åˆ†æã€èŠ‚å¾‹è§£è¯»ã€æŠ¥å‘Šç”Ÿæˆç­‰å…³é”®ä»»åŠ¡çš„ç»Ÿä¸€å»ºæ¨¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºHeartcare - Benchå¤šç»´åº¦åŸºå‡†æµ‹è¯•æ¡†æ¶
è¯¥æ¡†æ¶ç”¨äºç³»ç»Ÿè¯„ä¼°ECGåœºæ™¯ä¸‹çš„è¯Šæ–­æ™ºèƒ½ï¼Œæ¶µç›–å°é—­å¼é—®ç­”ã€å¼€æ”¾å¼é—®ç­”ã€æŠ¥å‘Šç”Ÿæˆã€ä¿¡å·é‡å»ºå’Œè¶‹åŠ¿é¢„æµ‹ç­‰ä»»åŠ¡ï¼ŒæŒ‰ä¸´åºŠåœºæ™¯åˆ†ä¸ºè¯Šæ–­ã€å½¢æ€ã€èŠ‚å¾‹ä¸‰å¤§ç±»ã€‚é‡‡ç”¨åˆ†å±‚å¤šæŒ‡æ ‡è¯„åˆ†ç³»ç»Ÿï¼Œè¯„ä¼°çŸ¥è¯†æ¨ç†ã€ç”Ÿæˆå‡†ç¡®æ€§å’Œè·¨æ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œå¡«è¡¥äº†å¤šæ¨¡æ€ECGä»»åŠ¡æ ‡å‡†åŒ–è¯„ä¼°çš„å…³é”®ç©ºç™½ï¼Œä¸ºMed - MLLMsåœ¨ç”Ÿç†ä¿¡å·è§£è¯»æ–¹é¢çš„ç³»ç»Ÿå¼€å‘å’ŒåŸºå‡†æµ‹è¯•æä¾›æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡HeartcareGPTæ¨¡å‹åŠBeat tokenizer
é’ˆå¯¹ECGæ—¶é—´åºåˆ—å»ºæ¨¡çš„é«˜ç»´ç¨€ç–æ€§ã€å¯¼è”é—´åŒæ­¥ä¾èµ–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºBidirectional ECG Abstract Tokenizationï¼ˆBeatï¼‰ç¦»æ•£ç¼–ç æ¡†æ¶ã€‚BeatåŸºäºå‘é‡é‡åŒ–å°†åŸå§‹ECGä¿¡å·å‹ç¼©ä¸ºtokenåºåˆ—ä¾›MLLMsç›´æ¥å¤„ç†ï¼ŒåŒ…å«ä¸‰å¤§æ ¸å¿ƒæœºåˆ¶ï¼šåŒçº§å‘é‡é‡åŒ–ï¼ˆDVQï¼‰ç”¨æ ¸å¿ƒç æœ¬æ•æ‰èŠ‚å¾‹æ¨¡å¼ã€æ®‹å·®ç æœ¬ç»†åŒ–ç—…ç†ç‰¹å¾ï¼Œå®ç°é«˜ä¿çœŸå‹ç¼©ä¸ä¿¡å·ç»“æ„ä¿ç•™ï¼›æŸ¥è¯¢å¼•å¯¼åŒå‘æ‰©æ•£ï¼ˆQBDï¼‰æ¨¡å—åœ¨ç¦»æ•£æ½œåœ¨ç©ºé—´å»ºæ¨¡è¿‡å»ä¸Šä¸‹æ–‡å’Œæœªæ¥è¶‹åŠ¿ï¼Œèµ‹äºˆtokené‡å»ºä¸é¢„æµ‹èƒ½åŠ›ï¼›è”åˆç›‘ç£ç­–ç•¥åˆ©ç”¨é‡å»ºå’Œé¢„æµ‹ç›®æ ‡ä¼˜åŒ–ç¼–ç å™¨ - é‡åŒ–å™¨ - è§£ç å™¨ pipelineï¼Œç¡®ä¿tokenä¿ç•™ä¸´åºŠè¯Šæ–­å’Œé¢„è­¦ç›¸å…³ä¿¡æ¯ã€‚è¿™äº›ç¦»æ•£è¡¨ç¤ºåµŒå…¥MLLMsè¯æ±‡è¡¨ï¼Œä½¿HeartcareGPTèƒ½è·¨ä¿¡å·ã€æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œç«¯åˆ°ç«¯æ¨ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡ä¸­è™½æœªè¯¦ç»†å±•å¼€å®éªŒç»“æœæ•°å€¼ï¼Œä½†ä»è®ºè¿°å¯çŸ¥ï¼ŒåŸºäºHeartcare - 220Kæ„å»ºçš„HeartcareGPTåœ¨å¤šä¸ªä¸´åºŠæœ‰æ„ä¹‰çš„ä»»åŠ¡ï¼ˆå¦‚ç–¾ç—…è¯Šæ–­ã€æŠ¥å‘Šç”Ÿæˆç­‰ï¼‰ä¸Šå®ç°äº†å¼ºæ³›åŒ–æ€§å’ŒSOTAæ€§èƒ½ï¼Œä¸”å¤§é‡å®éªŒè¯æ˜Heartcare Suiteåœ¨æ¨è¿›ECGç‰¹å®šçš„å¤šæ¨¡æ€ç†è§£å’Œè¯„ä¼°æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œèƒ½ä¸ºECGé¢†åŸŸçš„Med - MLLMså‘å±•æä¾›æœ‰åŠ›æ”¯æŒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºæ–¹é¢ï¼šå±•ç¤ºäº†å¦‚ä½•æ•´åˆå¤šæºå¼‚æ„åŒ»ç–—æ•°æ®ï¼Œå¹¶é€šè¿‡æ™ºèƒ½ä½“æŠ€æœ¯ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œä¸ºé¢†åŸŸç‰¹å®šæ•°æ®é›†æ„å»ºæä¾›äº†ä»æ•°æ®æ”¶é›†ã€å¤„ç†åˆ°æ ‡æ³¨ä¼˜åŒ–çš„å®Œæ•´æ€è·¯ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–åŒ»ç–—æ¨¡æ€æˆ–ä¸“ä¸šé¢†åŸŸæ•°æ®é›†å»ºè®¾ã€‚
2. åŸºå‡†æµ‹è¯•æ¡†æ¶æ–¹é¢ï¼šæå‡ºçš„å¤šç»´åº¦ã€åˆ†å±‚å¤šæŒ‡æ ‡è¯„ä¼°ä½“ç³»ï¼Œä¸ºè§£å†³å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡çš„è¯„ä¼°éš¾é¢˜æä¾›äº†èŒƒä¾‹ï¼Œå¯ç”¨äºæŒ‡å¯¼å…¶ä»–åŒ»ç–—æˆ–éåŒ»ç–—é¢†åŸŸå¤šæ¨¡æ€ä»»åŠ¡çš„è¯„ä¼°æ¡†æ¶è®¾è®¡ï¼ŒåŠ©åŠ›æ¨¡å‹å…¨é¢æ€§èƒ½è€ƒé‡ã€‚
3. æ¨¡æ€ç¼–ç ä¸æ¨¡å‹è®¾è®¡æ–¹é¢ï¼šBeaté’ˆå¯¹ECGä¿¡å·ç‰¹æ€§è®¾è®¡çš„åˆ†å±‚ç¦»æ•£ç¼–ç æ¡†æ¶ï¼Œä¸ºå¤„ç†ç±»ä¼¼é«˜ç»´ã€å¤šç»“æ„ä¾èµ–çš„æ—¶é—´åºåˆ—æˆ–ç”Ÿç†ä¿¡å·æä¾›äº†åˆ›æ–°æ€è·¯ï¼Œå…¶åŒçº§é‡åŒ–ã€åŒå‘æ‰©æ•£å’Œè”åˆç›‘ç£ç­‰æœºåˆ¶åœ¨æ¨¡æ€ç‰¹å¼‚æ€§ç¼–ç å’Œæ¨¡å‹ - æ•°æ®å¯¹é½ä¸Šå…·æœ‰å€Ÿé‰´ä»·å€¼ï¼Œå¯å¯å‘å…¶ä»–é¢†åŸŸç‰¹å®šæ¨¡æ€ä¸å¤§æ¨¡å‹ç»“åˆçš„æŠ€æœ¯è·¯çº¿ã€‚

## storm--benchmarking-visual-rating-of-mllms-with-a-comprehensive-ordinal-regression-dataset
### Abstract
Visual rating is an essential capability of artificial intelligence (AI) for
multi-dimensional quantification of visual content, primarily applied in
ordinal regression (OR) tasks such as image quality assessment, facial age
estimation, and medical image grading. However, current multi-modal large
language models (MLLMs) under-perform in such visual rating ability while also
suffering the lack of relevant datasets and benchmarks. In this work, we
collect and present STORM, a data collection and benchmark for Stimulating
Trustworthy Ordinal Regression Ability of MLLMs for universal visual rating.
STORM encompasses 14 ordinal regression datasets across five common visual
rating domains, comprising 655K image-level pairs and the corresponding
carefully curated VQAs. Importantly, we also propose a coarse-to-fine
processing pipeline that dynamically considers label candidates and provides
interpretable thoughts, providing MLLMs with a general and trustworthy ordinal
thinking paradigm. This benchmark aims to evaluate the all-in-one and zero-shot
performance of MLLMs in scenarios requiring understanding of the essential
common ordinal relationships of rating labels. Extensive experiments
demonstrate the effectiveness of our framework and shed light on better
fine-tuning strategies. The STORM dataset, benchmark, and pre-trained models
are available on the following webpage to support further research in this
area. Datasets and codes are released on the project page:
https://storm-bench.github.io/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | STORMï¼šä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹è§†è§‰è¯„çº§èƒ½åŠ›æ‰“é€ å…¨é¢åºæ•°å›å½’åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼Œå…¶åœ¨è§†è§‰é—®ç­”ç­‰åœºæ™¯å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨è§†è§‰è¯„çº§è¿™ä¸€å…³é”®èƒ½åŠ›ä¸Šå´è¡¨ç°ä¸è¶³ï¼Œä¸”ç¼ºä¹ç›¸å…³æ•°æ®é›†ä¸åŸºå‡†æ¥æ”¯æ’‘ç ”ç©¶ã€‚è§†è§‰è¯„çº§æ˜¯AIå¯¹è§†è§‰å†…å®¹å¤šç»´åº¦é‡åŒ–çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œåº”ç”¨äºå›¾åƒè´¨é‡è¯„ä¼°ã€é¢éƒ¨å¹´é¾„ä¼°è®¡ã€åŒ»å­¦å›¾åƒåˆ†çº§ç­‰åºæ•°å›å½’ï¼ˆORï¼‰ä»»åŠ¡ã€‚å½“å‰MLLMsé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šä»»åŠ¡æ ‡ç­¾å¤æ‚ï¼ˆä¸åŒä»»åŠ¡æ ‡ç­¾æ•°é‡ä¸å±‚çº§å®šä¹‰ä¸ä¸€ï¼‰ã€æ•°å€¼æ ‡ç­¾æ˜“â€œå¹»è§‰â€ï¼ˆé¢„è®­ç»ƒä¾§é‡é«˜å±‚è¯­ä¹‰ï¼Œå¯¹ç²¾ç¡®æ•°å€¼å…³æ³¨å°‘ä¸”å—æ ‡æ³¨å™ªå£°å½±å“ï¼‰ã€é›¶æ ·æœ¬æ€§èƒ½å·®ï¼ˆä»…é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œè·¨åŸŸæ³›åŒ–å—é™ï¼‰ã€‚å› æ­¤ï¼Œæ‰“é€ èƒ½è®­ç»ƒå’Œè¯„ä¼°MLLMsé€šç”¨è§†è§‰è¯„çº§èƒ½åŠ›çš„æ•°æ®é›†ä¸åŸºå‡†è¿«åœ¨çœ‰ç«ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºSTORMæ•°æ®é›†ï¼Œè¦†ç›–å¹¿æ³›è§†è§‰è¯„çº§é¢†åŸŸ  
STORMæ•´åˆäº†5å¤§å¸¸è§è§†è§‰è¯„çº§é¢†åŸŸï¼ˆå›¾åƒè´¨é‡è¯„ä¼°ã€å›¾åƒç¾å­¦è¯„ä¼°ã€é¢éƒ¨å¹´é¾„ä¼°è®¡ã€åŒ»å­¦ç–¾ç—…åˆ†çº§ã€å†å²å¹´ä»£ä¼°è®¡ï¼‰ä¸‹çš„14ä¸ªåºæ•°å›å½’æ•°æ®é›†ï¼ŒåŒ…å«65.5ä¸‡å›¾åƒçº§é—®ç­”å¯¹ã€‚è¿˜æä¾›çº¦25ä¸‡æ ·æœ¬çš„è½»é‡ç‰ˆæœ¬ï¼Œä¾¿äºæ¨¡å‹å¿«é€Ÿè®­ç»ƒã€‚é€šè¿‡è¯¥ç»¼åˆæ•°æ®é›†çš„è”åˆè®­ç»ƒï¼Œèƒ½è®©MLLMsåˆæ­¥å…·å¤‡å¤„ç†å¤šæ•°è§†è§‰è¯„çº§ä»»åŠ¡çš„åŸºç¡€èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ç²—ç»†ç²’åº¦ç»“åˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰ pipeline  
ä¸ºç¼“è§£æ¨¡å‹æ•°å€¼â€œå¹»è§‰â€å¹¶æå‡é›¶æ ·æœ¬æ€§èƒ½ï¼ŒSTORMä¸ºé—®ç­”å¯¹è®¾è®¡äº†é¢å¤–ä¸­é—´é¢„æµ‹æ­¥éª¤ï¼Œå¼•å¯¼MLLMséµå¾ªâ€œå…ˆç²—åç»†â€çš„é€»è¾‘æ€ç»´è¿‡ç¨‹ç†è§£åºæ•°å›å½’é—®é¢˜ã€‚æ¯”å¦‚å…ˆè®©æ¨¡å‹å¯¹å¹´é¾„åšâ€œå„¿ç«¥ã€é’å°‘å¹´ã€é’å¹´â€ç­‰ç²—åˆ†ç±»ï¼Œå†åŸºäºæ­¤åšç»†ç²’åº¦é¢„æµ‹ï¼Œä¸ºæ¨¡å‹æä¾›é€šç”¨ä¸”å¯ä¿¡çš„åºæ•°æ€è€ƒèŒƒå¼ï¼Œå¢å¼ºå¯¹è·¨åŸŸè§†è§‰è¯„çº§ä»»åŠ¡çš„é›¶æ ·æœ¬è¡¨ç°ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ‰“é€ ä¸€ä½“åŒ–è¯„ä¼°æ¡†æ¶ä¸å¤šæ ·æ ‡æ³¨ä½“ç³»  
STORMä¸ä»…æœ‰åŸºç¡€æ•°å€¼æ ‡ç­¾æ»¡è¶³è§†è§‰è¯„çº§åŸºæœ¬è®¾å®šï¼Œè¿˜èå…¥å¤šæ ·æ–‡æœ¬æ ‡ç­¾å¼ºåŒ–æ¨¡å‹å¯¹ä¸åŒä»»åŠ¡è¯­ä¹‰ç†è§£ä¸å¯è§£é‡Šæ€§é¢„æµ‹èƒ½åŠ›ï¼›åŒæ—¶æå‡ºç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹è¯•MLLMsåœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šçš„ä¸€ä½“åŒ–è§†è§‰è¯„çº§èƒ½åŠ›ï¼Œæ˜¯é¦–ä¸ªèšç„¦MLLMsé€šç”¨è§†è§‰è¯„çº§èƒ½åŠ›çš„åŸºå‡†ä¸æ•°æ®é›†å»ºè®¾å·¥ä½œã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­å¤§é‡å®éªŒéªŒè¯äº†STORMæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œèƒ½ä¸ºMLLMsè§†è§‰è¯„çº§èƒ½åŠ›è¯„ä¼°æä¾›å¯é æ”¯æ’‘ï¼Œä¹Ÿä¸ºæ›´ä¼˜çš„å¾®è°ƒç­–ç•¥æŒ‡æ˜æ–¹å‘ï¼ˆè®ºæ–‡æœªè¯¦ç»†å±•å¼€å®éªŒæ•°æ®ï¼Œä½†å¼ºè°ƒå®éªŒå……åˆ†å±•ç¤ºæ¡†æ¶ä»·å€¼ï¼‰ã€‚æ­¤å¤–ï¼ŒSTORMæ•°æ®é›†ã€åŸºå‡†å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å¼€æºï¼ŒåŠ©åŠ›è¯¥é¢†åŸŸåç»­ç ”ç©¶ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸè¦†ç›–ä¸æ•°æ®é›†æ„å»ºæ€è·¯ï¼šæ•´åˆå¤šé¢†åŸŸç»å…¸æ•°æ®é›†å½¢æˆç»¼åˆåŸºå‡†ï¼Œä¸ºæ¨¡å‹å¤šä»»åŠ¡æ³›åŒ–èƒ½åŠ›è®­ç»ƒæä¾›ä¸°å¯Œç´ æï¼Œè¿™ç§è·¨åŸŸæ•°æ®èšåˆæ€è·¯å¯ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡åŸºå‡†å»ºè®¾ã€‚  
2. æ€ç»´é“¾å¼•å¯¼ç­–ç•¥ï¼šâ€œå…ˆç²—åç»†â€çš„CoTè®¾è®¡ä¸ºè§£å†³åºæ•°å›å½’ç±»éœ€å±‚çº§æ¨ç†çš„ä»»åŠ¡æä¾›äº†å¯è§£é‡Šã€å¯å¤ç°çš„æ€è€ƒèŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€é€æ­¥æ¨ç†çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­æå‡æ¨¡å‹é€»è¾‘æ€§ã€‚  
3. è¯„ä¼°æ¡†æ¶æ­å»ºï¼šå…³æ³¨æ¨¡å‹â€œä¸€ä½“åŒ–â€å’Œâ€œé›¶æ ·æœ¬â€è¡¨ç°ï¼Œå…¼é¡¾åŸŸå†…åŸŸå¤–ä»»åŠ¡ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹é€šç”¨èƒ½åŠ›è¯„ä¼°æä¾›äº†å…¨é¢æ€§å‚è€ƒèŒƒå¼ï¼Œå¯å‘åç»­åŸºå‡†åœ¨è¯„ä¼°ç»´åº¦ä¸Šçš„å®Œå–„ã€‚

## medbookvqa--a-systematic-and-comprehensive-medical-benchmark-derived-from-open-access-book
### Abstract
The accelerating development of general medical artificial intelligence
(GMAI), powered by multimodal large language models (MLLMs), offers
transformative potential for addressing persistent healthcare challenges,
including workforce deficits and escalating costs. The parallel development of
systematic evaluation benchmarks emerges as a critical imperative to enable
performance assessment and provide technological guidance. Meanwhile, as an
invaluable knowledge source, the potential of medical textbooks for benchmark
development remains underexploited. Here, we present MedBookVQA, a systematic
and comprehensive multimodal benchmark derived from open-access medical
textbooks. To curate this benchmark, we propose a standardized pipeline for
automated extraction of medical figures while contextually aligning them with
corresponding medical narratives. Based on this curated data, we generate 5,000
clinically relevant questions spanning modality recognition, disease
classification, anatomical identification, symptom diagnosis, and surgical
procedures. A multi-tier annotation system categorizes queries through
hierarchical taxonomies encompassing medical imaging modalities (42
categories), body anatomies (125 structures), and clinical specialties (31
departments), enabling nuanced analysis across medical subdomains. We evaluate
a wide array of MLLMs, including proprietary, open-sourced, medical, and
reasoning models, revealing significant performance disparities across task
types and model categories. Our findings highlight critical capability gaps in
current GMAI systems while establishing textbook-derived multimodal benchmarks
as essential evaluation tools. MedBookVQA establishes textbook-derived
benchmarking as a critical paradigm for advancing clinical AI, exposing
limitations in GMAI systems while providing anatomically structured performance
metrics across specialties.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MedBookVQAï¼šåŸºäºå¼€æºåŒ»å­¦ä¹¦ç±çš„ç³»ç»Ÿæ€§ç»¼åˆåŒ»ç–—åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å…¨çƒåŒ»ç–—ç³»ç»Ÿé¢ä¸´ç€åŒ»æŠ¤äººå‘˜çŸ­ç¼ºä¸æˆæœ¬æ”€å‡ç­‰æŒ‘æˆ˜ï¼Œé€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½ï¼ˆGMAIï¼‰å€ŸåŠ©å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ‰æ½œåŠ›è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†éœ€è¦ç³»ç»Ÿçš„è¯„ä¼°åŸºå‡†æ¥è¯„ä¼°æ€§èƒ½ä¸æä¾›æŠ€æœ¯æŒ‡å¯¼ã€‚ç°æœ‰åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†å­˜åœ¨èŒƒå›´å’Œé€‚ç”¨æ€§å±€é™ï¼Œå¤šé’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼›è€ŒåŒ»å­¦æ•™ç§‘ä¹¦ä½œä¸ºä¼˜è´¨çŸ¥è¯†æºï¼Œåœ¨åŸºå‡†å¼€å‘ä¸Šçš„æ½œåŠ›æœªè¢«å……åˆ†æŒ–æ˜ã€‚å› æ­¤ï¼Œæ„å»ºåŸºäºåŒ»å­¦æ•™ç§‘ä¹¦çš„ç»¼åˆåŸºå‡†ååˆ†å¿…è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒ»ç–—VQAæ•°æ®é›†æ„å»ºçš„è‡ªé€‚åº” pipeline  
æå‡ºæ ‡å‡†åŒ– pipeline ä»åŒ»å­¦æ•™ç§‘ä¹¦ä¸­è‡ªåŠ¨æå–åŒ»å­¦å›¾åƒï¼Œå¹¶å°†å…¶ä¸å¯¹åº”çš„åŒ»å­¦æ–‡æœ¬æè¿°åœ¨ä¸Šä¸‹æ–‡å±‚é¢å¯¹é½ï¼Œè¿˜è®¾è®¡äº†ç”Ÿæˆ VQA æ ¼å¼æ•°æ®é›†çš„æ–° pipelineï¼Œè¦†ç›–æ¨¡æ€è¯†åˆ«ã€ç–¾ç—…åˆ†ç±»ã€è§£å‰–è¯†åˆ«ã€ç—‡çŠ¶è¯Šæ–­å’Œæ‰‹æœ¯æµç¨‹è¿™äº”å¤§åŒ»ç–—ä»»åŠ¡ç±»åˆ«ï¼Œåˆ©ç”¨é€šç”¨åŒ¹é…è§„åˆ™å’Œå…³é”®åŒ»å­¦æ¦‚å¿µè¯­ä¹‰æè¿°ç¡®ä¿å¯æ‰©å±•æ€§ä¸æ³›åŒ–æ€§ï¼ŒåŒæ—¶ç»“åˆä¸¥æ ¼è¿‡æ»¤å’Œäººå·¥éªŒè¯ä¿éšœæ•°æ®è´¨é‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨å‡ºå…¨é¢çš„ MedBookVQA åŸºå‡†  
æ„å»ºäº† MedBookVQA è¿™ä¸€ç²¾å¿ƒç­–åˆ’çš„åŸºå‡†æ•°æ®é›†ï¼Œå®ƒæºè‡ªå…¬å¼€å¯ç”¨çš„åŒ»å­¦æ•™ç§‘ä¹¦ï¼Œåœ¨å¤šæ ·æ€§å’Œå®Œæ•´æ€§ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¶µç›– 42 ç§åŒ»å­¦æˆåƒæ¨¡æ€ã€125 ä¸ªäººä½“è§£å‰–ç»“æ„ä»¥åŠ 31 ä¸ªåŒ»å­¦ç§‘å®¤ï¼Œè¿˜è®¾è®¡äº†åˆ†å±‚æ ‡æ³¨ç³»ç»Ÿï¼ŒæŒ‰æ¨¡æ€ã€è§£å‰–ç»“æ„å’ŒåŒ»å­¦ç§‘å®¤ç»„ç»‡æ•°æ®é›†æ¡ç›®ï¼Œä¾¿äºçµæ´»æ•°æ®ç®¡ç†ä¸ç»†ç²’åº¦åˆ†æã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¹¿æ³›è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹  
å¯¹å¤šç§ MLLMs è¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…å« 4 ä¸ªä¸“æœ‰é€šç”¨ç³»åˆ—ã€6 ä¸ªå¼€æºé€šç”¨ç³»åˆ—ã€2 ä¸ªå¼€æºåŒ»ç–—ç³»åˆ—å’Œ 5 ä¸ªæ¨ç†èšç„¦ç³»åˆ—çš„æ¨¡å‹ï¼Œä»¥æ­¤æ¢ç©¶å½“å‰ GMAI ç³»ç»Ÿæ€§èƒ½è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¯¹ä¸åŒ MLLMs åœ¨ MedBookVQA ä¸Šè¯„ä¼°å¾—åˆ°ä¸¤ä¸ªå…³é”®å‘ç°ï¼šä¸€æ˜¯ä¸åŒåŒ»ç–— VQA ä»»åŠ¡æ€§èƒ½å·®å¼‚å¤§ï¼Œå°¤å…¶éœ€è¦é«˜çº§åŒ»å­¦çŸ¥è¯†å’Œè·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ï¼›äºŒæ˜¯ä¸“æœ‰é€šç”¨ MLLMs è¡¨ç°ä¼˜äºå…¶ä»–é€šç”¨ MLLMs ç”šè‡³åŒ»ç–—ä¸“ç”¨æ¨¡å‹ï¼Œè€Œæ¨ç†æ¨¡å‹åœ¨åŒ»ç–—ä»»åŠ¡ä¸Šæœªå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚åŒæ—¶é€šè¿‡ä¸å…¶ä»–åŒ»ç–— VQA åŸºå‡†ã€æ¨ç†åŸºå‡†å¯¹æ¯”ï¼Œå‡¸æ˜¾å‡º MedBookVQA åœ¨ä»»åŠ¡è¦†ç›–ã€æ•°æ®æºç­‰æ–¹é¢çš„ç‰¹è‰²ä¸ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ•°æ®é›†æ„å»ºè§’åº¦ï¼Œå…¶æå‡ºçš„è§£æåŒ»å­¦æ•™ç§‘ä¹¦ã€ç”Ÿæˆå¤šæ · VQA ä»»åŠ¡å¹¶åœ¨åˆ†å±‚æ¡†æ¶å†…ç»„ç»‡æ•°æ®é›†çš„ pipeline æ€è·¯ï¼Œä¸ºé¢†åŸŸç‰¹å®šæ•°æ®é›†æ„å»ºæä¾›äº†å¯å¤ç”¨çš„è‡ªé€‚åº”æµç¨‹å‚è€ƒï¼›åœ¨åŸºå‡†å»ºè®¾ä¸Šï¼ŒMedBookVQA å±•ç¤ºäº†åˆ©ç”¨æƒå¨çŸ¥è¯†æºï¼ˆåŒ»å­¦æ•™ç§‘ä¹¦ï¼‰æ„å»ºç»¼åˆåŒ»ç–—åŸºå‡†çš„èŒƒå¼ï¼Œä¸ºæ¨è¿›åŒ»ç–— AI è¯„ä¼°æä¾›äº†æœ‰ä»·å€¼èµ„æºï¼›åœ¨æ¨¡å‹è¯„ä¼°ç»´åº¦ï¼Œå¯¹å¤šç±»å‹ MLLMs å¹¿æ³›è¯„ä¼°çš„æ–¹å¼ï¼Œèƒ½æŒ‡å¯¼åç»­æ›´å…¨é¢åˆ†ææ¨¡å‹åœ¨åŒ»ç–—åœºæ™¯ä¸‹èƒ½åŠ›çŸ­æ¿ä¸ä¼˜åŠ¿ï¼ŒåŠ©åŠ›é’ˆå¯¹æ€§æŠ€æœ¯æ”¹è¿›ã€‚ 

## qoq-med--building-multimodal-clinical-foundation-models-with-domain-aware-grpo-training
### Abstract
Clinical decision-making routinely demands reasoning over heterogeneous data,
yet existing multimodal language models (MLLMs) remain largely vision-centric
and fail to generalize across clinical specialties. To bridge this gap, we
introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model
that jointly reasons across medical images, time-series signals, and text
reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization
(DRPO), a novel reinforcement-learning objective that hierarchically scales
normalized rewards according to domain rarity and modality difficulty,
mitigating performance imbalance caused by skewed clinical data distributions.
Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains,
we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on
average across all visual domains as compared to other critic-free training
methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation
data, it is able to highlight salient regions related to the diagnosis, with an
IoU 10x higher than open models while reaching the performance of OpenAI
o4-mini. To foster reproducibility and downstream research, we release (i) the
full model weights, (ii) the modular training pipeline, and (iii) all
intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.
### ğŸŒŸ è®ºæ–‡è§£è¯» | QoQ-Medï¼šæ‰“é€ å¤šæ¨¡æ€ä¸´åºŠåŸºç¡€æ¨¡å‹ï¼Œçªç ´åŒ»ç–—AIå¤šé¢†åŸŸæ¨ç†ç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¸´åºŠè¯Šæ–­éœ€å¯¹å¤šæ¨¡æ€ï¼ˆå¦‚1Då¿ƒç”µã€2Då½±åƒã€3Dæ‰«æç­‰ï¼‰å¼‚æ„æ•°æ®æ¨ç†ï¼Œä½†ç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­˜åœ¨ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ˜¯**æ¨¡æ€ä¸é¢†åŸŸå±€é™**ï¼Œå¤šèšç„¦è§†è§‰ä¸”éš¾è·¨ä¸´åºŠä¸“ç§‘æ³›åŒ–ï¼Œå°šæ— æ¨¡å‹èƒ½æ•´åˆ1Dä¼ æ„Ÿå™¨æ•°æ®ä¸2D/3Då½±åƒï¼›äºŒæ˜¯**å¯è§£é‡Šæ€§ç¼ºå¤±**ï¼Œä¼ ç»Ÿæ¨¡å‹è¾“å‡ºâ€œé»‘ç®±â€å¼ç»“è®ºï¼ŒåŒ»ç–—ä»ä¸šè€…éš¾ä¿¡ä»»æ— æ¨ç†è¿‡ç¨‹çš„è¯Šæ–­ï¼Œé˜»ç¢ä¸´åºŠè½åœ°ã€‚åŒæ—¶ï¼Œä¸´åºŠæ•°æ®åˆ†å¸ƒä¸å‡ï¼ˆä¸åŒé¢†åŸŸ/æ¨¡æ€æ•°æ®é‡ã€éš¾åº¦å·®å¼‚å¤§ï¼‰ä¹Ÿå¯¼è‡´æ¨¡å‹è®­ç»ƒæ—¶æ€§èƒ½å¤±è¡¡ã€‚ä¸ºè§£å†³è¿™äº›ç—›ç‚¹ï¼ŒMITå›¢é˜Ÿæå‡ºQoQ-Medå¤šæ¨¡æ€ä¸´åºŠåŸºç¡€æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDomain-aware Relative Policy Optimizationï¼ˆDRPOï¼‰è®­ç»ƒç›®æ ‡  
DRPOæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–°é¢–ä¼˜åŒ–ç›®æ ‡ï¼Œ**æŒ‰é¢†åŸŸç¨€ç¼ºæ€§å’Œæ¨¡æ€éš¾åº¦åˆ†å±‚ç¼©æ”¾å½’ä¸€åŒ–å¥–åŠ±**ï¼Œè§£å†³ä¸´åºŠæ•°æ®åˆ†å¸ƒå€¾æ–œå¯¼è‡´çš„æ€§èƒ½å¤±è¡¡é—®é¢˜ã€‚ç›¸æ¯”GRPOç­‰æ—  critic çš„è®­ç»ƒæ–¹æ³•ï¼ŒDRPOèƒ½åœ¨ç¨€ç¼ºã€é«˜éš¾åº¦é¢†åŸŸâ€œé’ˆå¯¹æ€§å­¦ä¹ â€ï¼Œå®ç°è·¨éš¾åº¦çš„å‡è¡¡è®­ç»ƒï¼Œè®©æ¨¡å‹åœ¨å¤šé¢†åŸŸä¸‹æ›´ç¨³å¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºQoQ-Med-7B/32Bå¤šæ¨¡æ€ä¸´åºŠé€šç”¨æ¨¡å‹  
QoQ-Medæ˜¯é¦–ä¸ªå¼€æºçš„**è·¨åŒ»å­¦å½±åƒã€æ—¶é—´åºåˆ—ä¿¡å·ã€æ–‡æœ¬æŠ¥å‘Šè”åˆæ¨ç†**çš„é€šç”¨ä¸´åºŠåŸºç¡€æ¨¡å‹ã€‚å®ƒæ•´åˆ1Dï¼ˆå¦‚ECGï¼‰ã€2Dï¼ˆå¦‚Xå…‰ï¼‰ã€3Dï¼ˆå¦‚CTï¼‰å¤šæ¨¡æ€æ•°æ®ï¼Œå¡«è¡¥äº†â€œæ— æ¨¡å‹èƒ½åŒæ—¶å¤„ç†1Dä¼ æ„Ÿå™¨ä¸ä¼ ç»Ÿå½±åƒâ€çš„ç©ºç™½ï¼›åŒæ—¶ï¼Œæ¨¡å‹è®­ç»ƒæ—¶èå…¥å¤§é‡åˆ†å‰²æ•°æ®ï¼Œå¯é«˜äº®è¯Šæ–­ç›¸å…³çš„æ˜¾è‘—åŒºåŸŸï¼Œæå‡å¯è§£é‡Šæ€§ï¼Œè®©ä¸´åºŠåŒ»ç”Ÿèƒ½ç›´è§‚éªŒè¯è¯Šæ–­é€»è¾‘ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
- è¯Šæ–­æ€§èƒ½æå‡ï¼šåœ¨è¦†ç›–9ä¸ªä¸´åºŠé¢†åŸŸã€261ä¸‡æŒ‡ä»¤è°ƒä¼˜å¯¹çš„è®­ç»ƒä¸‹ï¼ŒDRPOè®­ç»ƒç›¸æ¯”GRPOç­‰æ–¹æ³•ï¼Œ**æ‰€æœ‰è§†è§‰é¢†åŸŸçš„macro - F1å¹³å‡æå‡43%**ï¼Œè¯æ˜å…¶åœ¨å¤šé¢†åŸŸå‡è¡¡å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚  
- åˆ†å‰²ä¸å¯è§£é‡Šæ€§ä¼˜åŠ¿ï¼šç»å¯†é›†åˆ†å‰²æ•°æ®è®­ç»ƒåï¼ŒQoQ-Medé«˜äº®è¯Šæ–­ç›¸å…³åŒºåŸŸçš„IoUï¼ˆäº¤å¹¶æ¯”ï¼‰æ¯”å¼€æºæ¨¡å‹é«˜10å€ï¼Œä¸”æ€§èƒ½è¿½å¹³OpenAI o4 - miniï¼Œåœ¨å¯è§£é‡Šæ€§ä¸ç²¾åº¦ä¸Šå®ç°å…¼é¡¾ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- å¤šæ¨¡æ€æ•´åˆæ€è·¯ï¼šä¸ºåŒ»ç–—AIæ•´åˆå¼‚æ„æ•°æ®ï¼ˆ1D/2D/3D + æ–‡æœ¬ï¼‰æä¾›èŒƒä¾‹ï¼Œæ¨åŠ¨æ¨¡å‹ä»â€œå•æ¨¡æ€/å•é¢†åŸŸâ€å‘â€œå¤šæ¨¡æ€å…¨é¢†åŸŸâ€ä¸´åºŠæ¨ç†è¿›åŒ–ã€‚  
- å¼ºåŒ–å­¦ä¹ é€‚é…ä¸´åºŠæ•°æ®ï¼šDRPOé€šè¿‡é¢†åŸŸæ„ŸçŸ¥çš„å¥–åŠ±ç¼©æ”¾ï¼Œè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡ä¸‹çš„è®­ç»ƒå¤±è¡¡ï¼Œä¸ºåŒ»ç–—ç­‰æ•°æ®é•¿å°¾åˆ†å¸ƒåœºæ™¯çš„RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰æä¾›æ–°èŒƒå¼ã€‚  
- å¼€æºç”Ÿæ€å»ºè®¾ï¼šå…¬å¼€æ¨¡å‹æƒé‡ã€æ¨¡å—åŒ–è®­ç»ƒ pipelineã€ä¸­é—´æ¨ç†è½¨è¿¹ï¼Œä¸ºåŒ»ç–—AIé¢†åŸŸçš„å¯å¤ç°ç ”ç©¶å’Œä¸‹æ¸¸åº”ç”¨ï¼ˆå¦‚è¯Šæ–­è¾…åŠ©ã€æŠ¥å‘Šç”Ÿæˆï¼‰é“ºå°±åŸºç¡€ï¼ŒåŠ é€Ÿè¡Œä¸šè¿­ä»£ã€‚  

