# Paper List of Terms(MLLM+Medical)
- [25/06] **Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning**  
[[Paper](http://arxiv.org/pdf/2506.15477v1)] [[Code/Page]()] [[TLDR/Notes](#multimodal-large-language-models-for-medical-report-generation-via-customized-prompt-tuning)]

- [25/06] **Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback**  
[[Paper](http://arxiv.org/pdf/2506.12323v1)] [[Code/Page]()] [[TLDR/Notes](#doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback)]

- [25/06] **MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.10465v1)] [[Code/Page]()] [[TLDR/Notes](#medseg-r--reasoning-segmentation-in-medical-images-with-multimodal-large-language-models)]

- [25/06] **HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding**  
[[Paper](http://arxiv.org/pdf/2506.09634v1)] [[Code/Page](https://github.com/YanzhaoShi/HSENet.)] [[TLDR/Notes](#hsenet--hybrid-spatial-encoding-network-for-3d-medical-vision-language-understanding)]

- [25/06] **HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains**  
[[Paper](http://arxiv.org/pdf/2506.07837v1)] [[Code/Page](https://github.com/ShiDaizi/ReMUD,)] [[TLDR/Notes](#haibu-remud--reasoning-multimodal-ultrasound-dataset-and-model-bridging-to-general-specific-domains)]

- [25/06] **Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.07044v4)] [[Code/Page]()] [[TLDR/Notes](#lingshu--a-generalist-foundation-model-for-unified-multimodal-medical-understanding-and-reasoning)]

- [25/06] **Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling**  
[[Paper](http://arxiv.org/pdf/2506.05831v2)] [[Code/Page](https://github.com/DCDmllm/Heartcare-Suite)] [[TLDR/Notes](#heartcare-suite--multi-dimensional-understanding-of-ecg-with-raw-multi-lead-signal-modeling)]

- [25/06] **STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset**  
[[Paper](http://arxiv.org/pdf/2506.01738v1)] [[Code/Page](https://storm-bench.github.io/.)] [[TLDR/Notes](#storm--benchmarking-visual-rating-of-mllms-with-a-comprehensive-ordinal-regression-dataset)]

- [25/06] **MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book**  
[[Paper](http://arxiv.org/pdf/2506.00855v1)] [[Code/Page]()] [[TLDR/Notes](#medbookvqa--a-systematic-and-comprehensive-medical-benchmark-derived-from-open-access-book)]

- [25/06] **QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training**  
[[Paper](http://arxiv.org/pdf/2506.00711v1)] [[Code/Page](https://github.com/DDVD233/QoQ_Med.)] [[TLDR/Notes](#qoq-med--building-multimodal-clinical-foundation-models-with-domain-aware-grpo-training)]



# TLDR/Notes
## multimodal-large-language-models-for-medical-report-generation-via-customized-prompt-tuning
### Abstract
Medical report generation from imaging data remains a challenging task in
clinical practice. While large language models (LLMs) show great promise in
addressing this challenge, their effective integration with medical imaging
data still deserves in-depth exploration. In this paper, we present MRG-LLM, a
novel multimodal large language model (MLLM) that combines a frozen LLM with a
learnable visual encoder and introduces a dynamic prompt customization
mechanism. Our key innovation lies in generating instance-specific prompts
tailored to individual medical images through conditional affine
transformations derived from visual features. We propose two implementations:
prompt-wise and promptbook-wise customization, enabling precise and targeted
report generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets
demonstrate that MRG-LLM achieves state-of-the-art performance in medical
report generation. Our code will be made publicly available.
### 🌟 论文解读 | 医疗报告生成新突破：MRG - LLM 借助定制化提示调优打造多模态大语言模型

### 📌 背景痛点/本文动机
医疗报告生成是临床实践的关键环节，但从影像数据生成医疗报告耗时费力，给医护人员带来沉重负担。自动医疗报告生成方法虽有研究，但传统方法受限于医疗领域训练数据量少（如 IU X - ray 和 MIMIC - CXR 数据集远小于通用图像描述数据集），文本生成能力存在局限。大语言模型（LLMs）虽有潜力，但与医学影像数据的有效整合仍需深入探索，且现有结合 LLMs 的方法手动设计指令难度大，通用提示也无法充分捕捉单个医学影像的独特特征。因此，本文旨在提出新方法解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 MRG - LLM 模型
MRG - LLM 是用于医疗报告生成的多模态大语言模型，结合了冻结的大语言模型、可学习的视觉编码器和定制化提示调优。该模型包含视觉编码器$f_{ve}$、投影层$f_{proj}$和预训练的 LLM  backbone $f_{llm}$ 这三个核心组件，能在视觉和文本领域间进行有效跨模态学习，同时保持 LLM 强大的语言生成能力。视觉编码器处理医学图像得到视觉特征，投影层将视觉特征映射为视觉 tokens，LLM  backbone 处理包含视觉 tokens、文本 tokens 和可学习提示 tokens 的混合序列来生成报告。

💡 创新点2：提出提示定制机制及两种实现方式
提出动态提示定制机制，基于医学图像视觉特征生成特定实例的提示，以实现更精准有针对性的报告生成。
 - Prompt - Wise Customization：学习函数$\phi$从视觉特征$X$生成图像特定的变换参数$(\gamma, \beta)$，通过逐提示的仿射变换$p'_i = \gamma_i p_i + \beta_i$修改提示簿中的每个基础提示，其中$p_i$是提示簿$P$中第$i$个可学习基础提示。
 - Promptbook - Wise Customization：使用全局变换作用于整个提示簿，由函数$\phi$从视觉特征$X$生成变换参数$(\gamma, \beta)$，以缩放和平移整个提示簿$P$。

### 📈 实验结果
在 IU X - ray 和 MIMIC - CXR 两个公开医疗数据集上进行了大量实验，结果表明 MRG - LLM 在医疗报告生成任务中实现了最先进的性能，超越了现有方法。

### 💬 可借鉴之处
 - 模型架构设计上，将冻结 LLM 与可学习视觉编码器结合的思路，为多模态任务中利用 LLM 知识和处理特定领域视觉信息提供了参考，可借鉴于其他需要跨模态学习的任务场景。
 - 提示定制机制针对不同实例生成特定提示，解决通用提示无法适配个体差异的问题，这种根据实例特征动态调整提示的思想，在需要精准适配个体数据的任务（如个性化推荐、特定领域文本生成等）中具有借鉴价值，可用于改进提示调优策略。
 - 实验验证充分，在公开数据集上验证性能达到 SOTA，这种基于公开数据集验证方法有效性的方式，为研究工作的可靠性验证提供了范例，后续研究在方法验证时可参考这种对公开基准数据集的充分实验。

## doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback
### Abstract
Paucity of medical data severely limits the generalizability of diagnostic ML
models, as the full spectrum of disease variability can not be represented by a
small clinical dataset. To address this, diffusion models (DMs) have been
considered as a promising avenue for synthetic image generation and
augmentation. However, they frequently produce medically inaccurate images,
deteriorating the model performance. Expert domain knowledge is critical for
synthesizing images that correctly encode clinical information, especially when
data is scarce and quality outweighs quantity. Existing approaches for
incorporating human feedback, such as reinforcement learning (RL) and Direct
Preference Optimization (DPO), rely on robust reward functions or demand
labor-intensive expert evaluations. Recent progress in Multimodal Large
Language Models (MLLMs) reveals their strong visual reasoning capabilities,
making them adept candidates as evaluators. In this work, we propose a novel
framework, coined MAGIC (Medically Accurate Generation of Images through
AI-Expert Collaboration), that synthesizes clinically accurate skin disease
images for data augmentation. Our method creatively translates expert-defined
criteria into actionable feedback for image synthesis of DMs, significantly
improving clinical accuracy while reducing the direct human workload.
Experiments demonstrate that our method greatly improves the clinical quality
of synthesized skin disease images, with outputs aligning with dermatologist
assessments. Additionally, augmenting training data with these synthesized
images improves diagnostic accuracy by +9.02% on a challenging 20-condition
skin disease classification task, and by +13.89% in the few-shot setting.
### 🌟 论文解读 | 医生认可：通过AI-专家反馈生成医学准确的皮肤病图像

### 📌 背景痛点/本文动机
在皮肤病诊断的机器学习模型领域，医疗数据匮乏严重限制了模型的泛化能力，因为小的临床数据集无法涵盖疾病变异的全范围。扩散模型虽被视为合成图像生成与数据增强的有前景途径，但常生成医学上不准确的图像，影响模型性能。现有融入人类反馈的方法（如强化学习、直接偏好优化）依赖可靠奖励函数或需要大量专家评估工作。而多模态大语言模型（MLLMs）的视觉推理能力使其适合作为评估者，基于此，本文希望提出新框架来生成医学准确的皮肤病图像用于数据增强，同时减少专家直接工作量。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出MAGIC框架  
提出名为MAGIC（Medically Accurate Generation of Images through AI - Expert Collaboration）的新颖框架，将专家定义的标准转化为扩散模型（DMs）图像合成的可操作反馈，在提高临床准确性的同时减少人类直接工作量。该框架让人类专家主要完成两项工作：从可信来源制定MLLM易验证的检查清单；在文本到图像（T2I）扩散模型训练期间监督MLLM对合成图像的反馈，通过迭代学习这些反馈引导T2I DMs生成更符合医学一致性的图像。  
💡 创新点2：集成I2I模块  
在训练流程中集成图像到图像（I2I）模块，从中间时间步而不是纯高斯噪声开始去噪，这在加速采样阶段的同时确保病变变换不会过度偏离真实数据分布。  
💡 创新点3：AI - 专家协作范式  
采用AI - 专家协作范式，在最少的专家监督下将视觉评估工作转移给强大的MLLM，大幅减少医学专家所需的时间和劳动。并且该框架能结合基于奖励的微调（RFT）和直接偏好优化（DPO）对扩散模型进行微调，其中与DPO结合（MAGIC - DPO）表现突出。

### 📈 实验结果
实验表明该方法大幅提升了合成皮肤病图像的临床质量，输出与皮肤科医生评估一致。在具有挑战性的20种皮肤病分类任务中，用合成图像增强训练数据使诊断准确率提高了9.02%；在少样本设置下，准确率提高了13.89%。同时，皮肤科医生评估分数提高、Fréchet Inception Distance（FID）分数降低，表明临床准确性和保真度提升，且随着训练推进和更多图像 - 反馈对的使用，模型生成效果持续改善。

### 💬 可借鉴之处
1. 框架设计思路：MAGIC框架整合专家知识到扩散模型的思路，为领域知识融入生成模型提供了参考，尤其是在医疗等专业领域，可借鉴这种将专业标准转化为模型可操作反馈的方式。  
2. 模块集成：其集成I2I模块加速采样并保证数据分布合理性的做法，对于其他生成类任务在效率和数据真实性平衡上有借鉴意义。  
3. 协作范式：AI - 专家协作减少专家工作量的范式，在需要专业领域知识评估但专家资源有限的场景下，如其他医学影像生成、特定工业检测图像生成等领域，都有参考价值。  
4. 结合优化方法：该框架结合RFT和DPO对扩散模型微调的方式，为扩散模型在特定领域的优化提供了新的实践路径，可启发后续针对不同任务选择合适优化策略来提升模型在专业领域表现。

## medseg-r--reasoning-segmentation-in-medical-images-with-multimodal-large-language-models
### Abstract
Medical image segmentation is crucial for clinical diagnosis, yet existing
models are limited by their reliance on explicit human instructions and lack
the active reasoning capabilities to understand complex clinical questions.
While recent advancements in multimodal large language models (MLLMs) have
improved medical question-answering (QA) tasks, most methods struggle to
generate precise segmentation masks, limiting their application in automatic
medical diagnosis. In this paper, we introduce medical image reasoning
segmentation, a novel task that aims to generate segmentation masks based on
complex and implicit medical instructions. To address this, we propose
MedSeg-R, an end-to-end framework that leverages the reasoning abilities of
MLLMs to interpret clinical questions while also capable of producing
corresponding precise segmentation masks for medical images. It is built on two
core components: 1) a global context understanding module that interprets
images and comprehends complex medical instructions to generate multi-modal
intermediate tokens, and 2) a pixel-level grounding module that decodes these
tokens to produce precise segmentation masks and textual responses.
Furthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the
medical image reasoning segmentation task. It includes over 10,000 image-mask
pairs and multi-turn conversations, automatically annotated using large
language models and refined through physician reviews. Experiments show
MedSeg-R's superior performance across several benchmarks, achieving high
segmentation accuracy and enabling interpretable textual analysis of medical
images.
### 🌟 论文解读 | MedSeg-R：用多模态大语言模型实现医学图像的推理式分割

### 📌 背景痛点/本文动机
医学图像分割在临床诊断中至关重要，但现有模型存在明显局限：一是过度依赖**明确的人工指令**（如“分割新冠感染区域”），难以应对开放、隐含的临床问题（如“这次检查能揭示患者哪些潜在病症？”）；二是多模态大语言模型（MLLMs）虽在医疗问答任务中表现出色，却**缺乏像素级的分割能力**，无法生成精确的分割掩码，限制了自动诊断的落地。因此，论文提出“医学图像推理式分割”这一全新任务，旨在让模型基于复杂、隐含的医疗指令，同时输出文字诊断和对应分割结果，填补现有技术 gap。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：定义“医学图像推理式分割”新任务  
传统医学图像分割需明确类别指令（如“分割肿瘤”），而新任务要求模型理解**复杂、隐含的临床问题**（如“检查结果反映患者哪些潜在疾病？”），并同步生成文字解释与像素级分割掩码，更贴近真实诊疗场景中医生开放式问诊、系统自主分析的需求。

💡 创新点2：提出端到端框架 MedSeg-R  
MedSeg-R 由两大核心模块构成：  
- **全局上下文理解模块（GCU）**：负责解析医学图像与复杂医疗指令，生成多模态中间 tokens，实现“图像 + 文本”的深度语义理解；  
- **像素级锚定模块（PG）**：解码 GCU 输出的 tokens，一方面生成精确的分割掩码，另一方面输出可解释的文字诊断，让“推理 + 分割 + 文本解释”在一个框架内完成。  

💡 创新点3：构建大规模数据集 MedSeg-QA  
为支撑新任务，论文打造了 MedSeg-QA 数据集：包含超 1 万组“图像 - 掩码”对，搭配多轮对话。数据集通过大语言模型自动标注 + 医生复审的方式构建，既保证规模又确保医学专业性，为推理式分割任务提供了高质量训练资源。

### 📈 实验结果
论文在多个基准测试中验证了 MedSeg-R 的性能：  
- **分割精度**：在医学图像分割任务上表现优异，证明模型能精准定位病理区域；  
- **推理与文本分析能力**：能基于隐含指令生成合理文字诊断，且分割结果与文字解释逻辑一致，实现了“可解释的医学图像分析”。  

对比现有医疗多模态模型（如 miniGPT4、BiomedGPT 等），MedSeg-R 同时具备“像素级分割能力”“多轮对话交互”与“端到端训练”三大特性，在综合性医疗图像分析中优势显著（见论文中 Table 1 对比）。

### 💬 可借鉴之处
1. **任务定义创新**：提出“推理式分割”这一更贴近真实临床场景的任务，拓展了医学图像分析的边界，为后续研究指明新方向；  
2. **多模态融合思路**：通过 GCU + PG 模块设计，实现“大语言模型推理能力”与“分割模型像素级精度”的深度融合，为多模态大模型在医疗领域的落地提供了架构参考；  
3. **数据集构建范式**：“自动标注 + 专家复审”的流程高效构建大规模医疗数据集，平衡了规模与专业性，可复用至其他医疗 AI 任务的数据集建设；  
4. **临床价值导向**：从“依赖明确指令”转向“理解隐含问题并自主分析”，让 AI 系统更接近真实诊疗逻辑，推动医疗 AI 从“工具辅助”向“智能诊断伙伴”进化。  

MedSeg-R 不仅在技术上突破了现有模型的能力边界，更在落地场景中为“全自动、可解释的智能医疗诊断”提供了可行路径，是多模态大模型赋能医疗影像分析的一次重要探索。

## hsenet--hybrid-spatial-encoding-network-for-3d-medical-vision-language-understanding
### Abstract
Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.
### 🌟 论文解读 | HSENet：突破3D医疗视觉语言理解的新框架

### 📌 背景痛点/本文动机
3D计算机断层扫描（CT）革新了医学诊断，但解读3D CT图像对放射科医生来说耗时且易出错。多模态大语言模型（MLLMs）在医学图像分析中表现出潜力，然而现有方法多聚焦于2D医学图像，难以捕捉复杂的3D解剖结构，易导致对细微病变的误判和诊断幻觉。同时，现有方法还存在视觉感知有限（3D体积 - 报告对稀缺限制特征收敛）和语义投影受损（难以保留3D解剖结构的空间和几何细节）等问题。为解决这些挑战，本文提出HSENet框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：双3D视觉编码器与双阶段预训练  
HSENet采用双3D视觉编码器来感知全局体积上下文和细粒度解剖细节。其中，3D Vision Encoder学习与报告对齐的全局体积表示；2D - Enhanced 3D Vision Encoder（2E3 Vision Encoder）则借助从2D切片中识别的丰富诊断见解，优化与报告对齐的解剖细节。且这两个编码器通过与诊断报告的双阶段对齐进行预训练。

💡 创新点2：Spatial Packer与Voxel2Point Cross - Attention  
提出Spatial Packer这一高效的多模态投影仪，用于将3D视觉上下文压缩为紧凑的信息视觉标记集合。该投影仪集成了新颖的Voxel2Point Cross - Attention（V2P - CA），通过将高分辨率3D体素表示聚合到其质心点，保留了关键的空间和几何信息，从而将提取的视觉表示映射到LLM的语义空间。

### 📈 实验结果
在3D语言 - 视觉检索任务中，HSENet达到39.85%的R@100，相比之前提升了5.96%；在3D医疗报告生成任务中，BLEU - 4指标达到24.01%，提升了8.01%；在3D视觉问答任务中，Major Class Accuracy达到73.60%，提升了1.99%。实验结果表明HSENet在生成有判别性的视觉表示和高质量诊断响应方面实现了最先进的性能，证实了其有效性。

### 💬 可借鉴之处
1. 多模态融合思路：通过双3D视觉编码器分别处理全局和局部信息，为处理复杂多尺度数据提供了一种有效的多模态特征提取范式，可借鉴到其他需要处理多尺度信息的3D视觉语言任务中。
2. 空间信息保留方法：Spatial Packer中利用Voxel2Point Cross - Attention来保留空间和几何信息的方式，为解决3D数据在投影到语言模型语义空间时的信息丢失问题提供了创新思路，在涉及3D数据与语言模型结合的任务中具有参考价值。
3. 预训练策略：双阶段对齐预训练的方式，利用2D切片辅助增强3D视觉感知，为在数据稀缺场景下提升模型对特定领域（如医疗）数据的理解能力提供了借鉴方向。

## haibu-remud--reasoning-multimodal-ultrasound-dataset-and-model-bridging-to-general-specific-domains
### Abstract
Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.
### 🌟 论文解读 | HAIBU-ReMUD：解决特定领域多模态大模型数据短缺难题

### 📌 背景痛点/本文动机
在通用领域，多模态大语言模型（MLLMs）展现出强大能力，但在特定领域（如医学超声领域）表现不佳，原因是缺乏领域特定数据（像图像 - 文本、视频 - 文本数据）。特定领域存在大量图文数据，但多以PDF、图像等形式零散保存，无法直接用于MLLMs训练。比如医学超声领域，虽有超声诊断书籍、临床指南、诊断报告等资源，却难以直接为模型训练所用。所以，需要一种简单自动的方式来构建特定领域的多模态数据，以提升MLLMs在特定领域的性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：特定领域四元组生成 pipeline
提出一种新颖的图像 - 文本推理有监督微调数据生成 pipeline，从特定领域材料中创建特定领域四元组（图像、问题、思考轨迹、答案）。该 pipeline 无需人工标注或预生成数据集，能确保专业知识的准确性，且适用于各种通用特定领域。以医学超声领域为例，利用Qwen2.5 - VL的边界框功能以及GPT - 4o、Gemini - 2.0 - Flash - Thinking - Exp的多模态图文识别与生成能力来构建数据。
💡 创新点2：构建ReMUD数据集
建立了医学超声领域数据集ReMUD，包含超过45000条推理和非推理的有监督微调问答（QA）和视觉问答（VQA）数据。这些数据为特定领域MLLMs训练提供了丰富资源，涵盖了多种医学超声相关的生物结构、专业知识等内容。
💡 创新点3：训练ReMUD - 7B模型
以Qwen2.5 - VL - 7B - Instruct为基础模型进行有监督微调，开发出ReMUD - 7B模型。该模型在医学超声领域的测试数据集（如USTQ - Knowledge和UVQA - Diagnosis）上，表现优于通用领域的多模态大语言模型，具备更强的领域推理能力。

### 📈 实验结果
ReMUD - 7B模型在医学超声领域的测试中，相比通用领域多模态大语言模型表现更优。所构建的ReMUD数据集包含丰富的医学超声领域QA和VQA数据，涵盖推理与非推理类型，为模型训练提供了充足的领域特定数据支撑，有效提升了模型在该领域的性能。

### 💬 可借鉴之处
1. 数据生成 pipeline 具有通用性，可推广到其他特定领域，为不同领域构建多模态训练数据提供了思路，解决特定领域数据短缺问题。
2. 公开ReMUD数据集、数据生成代码库和ReMUD - 7B模型参数，方便其他研究者在此基础上进行研究，推动特定领域多模态大模型的发展。
3. 针对特定领域从数据构建到模型训练的完整流程，为领域适配的多模态大模型研究提供了一套可参考的范式，有助于加速特定领域AI应用的落地。

## lingshu--a-generalist-foundation-model-for-unified-multimodal-medical-understanding-and-reasoning
### Abstract
Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...
### 🌟 论文解读 | Lingshu：面向医疗多模态理解与推理的通用基础模型

### 📌 背景痛点/本文动机
多模态大语言模型（MLLMs）在通用视觉元素理解上展现出强大能力，但在医疗场景应用中受限于诸多问题。现有医疗MLLMs存在三大关键局限：一是医疗知识覆盖仅聚焦影像，对影像外知识涉及不足；二是数据处理流程欠佳，易引发幻觉（输出错误信息）；三是缺乏针对复杂医疗场景的推理能力。医疗场景的数据与任务和通用领域存在本质差异，比如生物医学图文对和普通网络内容不同，通用视觉助手难应对，模型常对生物医学问题给出不确定或错误信息，因此需要专门针对医疗场景打造更优模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：全面的数据构建流程  
提出一套综合的数据整理流程，一方面从医学影像、大量医学文本和通用领域数据中高效获取丰富医疗知识数据；另一方面合成准确的医疗描述、视觉问答（VQA）以及推理样本，最终构建出富含广泛医疗知识的多模态数据集，为模型训练提供优质数据基础。  

💡 创新点2：医疗专用MLLM - Lingshu及多阶段训练  
基于精心构建的数据，打造医疗专用MLLM Lingshu。通过多阶段训练逐步嵌入医疗专业知识，提升任务解决能力。同时，初步探索将带可验证奖励的强化学习范式应用于增强Lingshu的医疗推理能力，尝试突破现有模型推理短板。  

💡 创新点3：统一评估框架MedEvalKit  
开发MedEvalKit，整合主流多模态和文本医疗基准，为模型提供标准化、公平且高效的评估方式，解决医疗模型评估缺乏统一高效框架的问题，助力更科学地衡量模型性能。  

### 📈 实验结果
在多模态问答、基于文本的问答、医疗报告生成这三大基础医疗任务上评估Lingshu性能，结果显示Lingshu在多数任务上持续超越现有开源多模态模型。此外，开展五个贴近真实场景的案例研究，展现出Lingshu在医疗实际应用中的潜力。  

### 💬 可借鉴之处
1. 数据构建层面：强调多来源（医疗影像、文本、通用领域）数据整合与高质量样本合成，为领域模型训练数据构建提供了从数据获取到处理的完整思路，在垂直领域模型打造时，可学习这种全面且精细的数据处理方式。  
2. 模型训练层面：多阶段训练嵌入专业知识以及探索强化学习提升推理能力的思路，为垂直领域大模型训练策略提供参考，尤其是医疗这类对专业性和推理要求高的领域，分层训练与强化学习结合值得借鉴。  
3. 评估框架层面：打造统一垂直领域评估框架MedEvalKit，解决领域内模型评估标准不一问题，其他垂直领域（如教育、金融）在构建模型评估体系时，可参考这种整合领域基准的方式来实现公平高效评估。

## heartcare-suite--multi-dimensional-understanding-of-ecg-with-raw-multi-lead-signal-modeling
### Abstract
We present Heartcare Suite, a multimodal comprehensive framework for
finegrained electrocardiogram (ECG) understanding. It comprises three key
components: (i) Heartcare-220K, a high-quality, structured, and comprehensive
multimodal ECG dataset covering essential tasks such as disease diagnosis,
waveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench,
a systematic and multi-dimensional benchmark designed to evaluate diagnostic
intelligence and guide the optimization of Medical Multimodal Large Language
Models (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored
tokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw
multi-lead signals into semantically rich discrete tokens via duallevel vector
quantization and query-guided bidirectional diffusion mechanism. Built upon
Heartcare-220K, HeartcareGPT achieves strong generalization and SoTA
performance across multiple clinically meaningful tasks. Extensive experiments
demonstrate that Heartcare Suite is highly effective in advancing ECGspecific
multimodal understanding and evaluation. Our project is available at
https://github.com/DCDmllm/Heartcare-Suite .
### 🌟 论文解读 | Heartcare Suite：多维度解析心电图的模态大模型新框架

### 📌 背景痛点/本文动机
在医疗多模态大语言模型（Med - MLLMs）的发展进程中，面向心电图（ECG）领域的应用存在诸多挑战。一方面，缺乏高质量、富含医学知识的多模态指令数据集，限制了模型在复杂诊断场景的泛化能力；现有数据集疾病谱覆盖有限、图像分辨率欠佳且临床标注结构化不足，难以满足细粒度诊断建模需求。另一方面，缺少全面的基准测试来从多维度评估模型，当前评估框架多依赖分类准确率等判别式指标，对生成类任务（如临床报告生成、开放式问答）缺乏系统标准，阻碍了模型在ECG特定应用中的优化与发展。此外，ECG作为高分辨率多导联生理信号，具有高采样率、多同步通道和对数值变化敏感等特性，现有Med - MLLMs将ECG波形图像作为输入，易导致特征冗余和长尾分布，且ECG模态缺乏类似自然语言tokenization的离散表示机制，难以在MLLMs自回归框架下直接建模ECG到文本的路径。为解决这些问题，作者团队提出了Heartcare Suite这一针对ECG领域细粒度理解任务的多模态综合框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建Heartcare - 220K多模态ECG数据集
团队整合了公共数据集PTB - XL（含21,799条12导联ECG信号及179个SCP - ECG类别标注）与来自顶尖医院的12,170张带结构化报告的ECG图像（包含扫描轨迹、临床结论和去标识元数据），丰富了模态和标签多样性。同时开发HeartAgent多智能体引擎，通过自底向上的 pipeline 确保标注一致性，生成高质量指令式问答对，将异构ECG数据转化为结构化监督，支持疾病诊断、波形形态分析、节律解读、报告生成等关键任务的统一建模。

💡 创新点2：提出Heartcare - Bench多维度基准测试框架
该框架用于系统评估ECG场景下的诊断智能，涵盖封闭式问答、开放式问答、报告生成、信号重建和趋势预测等任务，按临床场景分为诊断、形态、节律三大类。采用分层多指标评分系统，评估知识推理、生成准确性和跨模态理解能力，填补了多模态ECG任务标准化评估的关键空白，为Med - MLLMs在生理信号解读方面的系统开发和基准测试提供支撑。

💡 创新点3：设计HeartcareGPT模型及Beat tokenizer
针对ECG时间序列建模的高维稀疏性、导联间同步依赖等挑战，提出Bidirectional ECG Abstract Tokenization（Beat）离散编码框架。Beat基于向量量化将原始ECG信号压缩为token序列供MLLMs直接处理，包含三大核心机制：双级向量量化（DVQ）用核心码本捕捉节律模式、残差码本细化病理特征，实现高保真压缩与信号结构保留；查询引导双向扩散（QBD）模块在离散潜在空间建模过去上下文和未来趋势，赋予token重建与预测能力；联合监督策略利用重建和预测目标优化编码器 - 量化器 - 解码器 pipeline，确保token保留临床诊断和预警相关信息。这些离散表示嵌入MLLMs词汇表，使HeartcareGPT能跨信号、文本和图像进行端到端推理。

### 📈 实验结果
论文中虽未详细展开实验结果数值，但从论述可知，基于Heartcare - 220K构建的HeartcareGPT在多个临床有意义的任务（如疾病诊断、报告生成等）上实现了强泛化性和SOTA性能，且大量实验证明Heartcare Suite在推进ECG特定的多模态理解和评估方面非常有效，能为ECG领域的Med - MLLMs发展提供有力支持。

### 💬 可借鉴之处
1. 数据集构建方面：展示了如何整合多源异构医疗数据，并通过智能体技术生成高质量标注数据，为领域特定数据集构建提供了从数据收集、处理到标注优化的完整思路，可推广到其他医疗模态或专业领域数据集建设。
2. 基准测试框架方面：提出的多维度、分层多指标评估体系，为解决复杂多模态任务的评估难题提供了范例，可用于指导其他医疗或非医疗领域多模态任务的评估框架设计，助力模型全面性能考量。
3. 模态编码与模型设计方面：Beat针对ECG信号特性设计的分层离散编码框架，为处理类似高维、多结构依赖的时间序列或生理信号提供了创新思路，其双级量化、双向扩散和联合监督等机制在模态特异性编码和模型 - 数据对齐上具有借鉴价值，可启发其他领域特定模态与大模型结合的技术路线。

## storm--benchmarking-visual-rating-of-mllms-with-a-comprehensive-ordinal-regression-dataset
### Abstract
Visual rating is an essential capability of artificial intelligence (AI) for
multi-dimensional quantification of visual content, primarily applied in
ordinal regression (OR) tasks such as image quality assessment, facial age
estimation, and medical image grading. However, current multi-modal large
language models (MLLMs) under-perform in such visual rating ability while also
suffering the lack of relevant datasets and benchmarks. In this work, we
collect and present STORM, a data collection and benchmark for Stimulating
Trustworthy Ordinal Regression Ability of MLLMs for universal visual rating.
STORM encompasses 14 ordinal regression datasets across five common visual
rating domains, comprising 655K image-level pairs and the corresponding
carefully curated VQAs. Importantly, we also propose a coarse-to-fine
processing pipeline that dynamically considers label candidates and provides
interpretable thoughts, providing MLLMs with a general and trustworthy ordinal
thinking paradigm. This benchmark aims to evaluate the all-in-one and zero-shot
performance of MLLMs in scenarios requiring understanding of the essential
common ordinal relationships of rating labels. Extensive experiments
demonstrate the effectiveness of our framework and shed light on better
fine-tuning strategies. The STORM dataset, benchmark, and pre-trained models
are available on the following webpage to support further research in this
area. Datasets and codes are released on the project page:
https://storm-bench.github.io/.
### 🌟 论文解读 | STORM：为多模态大模型视觉评级能力打造全面序数回归基准

### 📌 背景痛点/本文动机
随着多模态大语言模型（MLLMs）的发展，其在视觉问答等场景展现出潜力，但在视觉评级这一关键能力上却表现不足，且缺乏相关数据集与基准来支撑研究。视觉评级是AI对视觉内容多维度量化的核心能力，应用于图像质量评估、面部年龄估计、医学图像分级等序数回归（OR）任务。当前MLLMs面临三大挑战：任务标签复杂（不同任务标签数量与层级定义不一）、数值标签易“幻觉”（预训练侧重高层语义，对精确数值关注少且受标注噪声影响）、零样本性能差（仅针对特定任务训练，跨域泛化受限）。因此，打造能训练和评估MLLMs通用视觉评级能力的数据集与基准迫在眉睫。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建STORM数据集，覆盖广泛视觉评级领域  
STORM整合了5大常见视觉评级领域（图像质量评估、图像美学评估、面部年龄估计、医学疾病分级、历史年代估计）下的14个序数回归数据集，包含65.5万图像级问答对。还提供约25万样本的轻量版本，便于模型快速训练。通过该综合数据集的联合训练，能让MLLMs初步具备处理多数视觉评级任务的基础能力。  

💡 创新点2：设计粗细粒度结合的思维链（CoT） pipeline  
为缓解模型数值“幻觉”并提升零样本性能，STORM为问答对设计了额外中间预测步骤，引导MLLMs遵循“先粗后细”的逻辑思维过程理解序数回归问题。比如先让模型对年龄做“儿童、青少年、青年”等粗分类，再基于此做细粒度预测，为模型提供通用且可信的序数思考范式，增强对跨域视觉评级任务的零样本表现。  

💡 创新点3：打造一体化评估框架与多样标注体系  
STORM不仅有基础数值标签满足视觉评级基本设定，还融入多样文本标签强化模型对不同任务语义理解与可解释性预测能力；同时提出综合评估框架，用于测试MLLMs在域内和域外数据集上的一体化视觉评级能力，是首个聚焦MLLMs通用视觉评级能力的基准与数据集建设工作。  

### 📈 实验结果
文中大量实验验证了STORM框架的有效性，能为MLLMs视觉评级能力评估提供可靠支撑，也为更优的微调策略指明方向（论文未详细展开实验数据，但强调实验充分展示框架价值）。此外，STORM数据集、基准和预训练模型已开源，助力该领域后续研究。  

### 💬 可借鉴之处
1. 领域覆盖与数据集构建思路：整合多领域经典数据集形成综合基准，为模型多任务泛化能力训练提供丰富素材，这种跨域数据聚合思路可用于其他多模态任务基准建设。  
2. 思维链引导策略：“先粗后细”的CoT设计为解决序数回归类需层级推理的任务提供了可解释、可复现的思考范式，可迁移到其他需逐步推理的多模态任务中提升模型逻辑性。  
3. 评估框架搭建：关注模型“一体化”和“零样本”表现，兼顾域内域外任务，为多模态模型通用能力评估提供了全面性参考范式，启发后续基准在评估维度上的完善。

## medbookvqa--a-systematic-and-comprehensive-medical-benchmark-derived-from-open-access-book
### Abstract
The accelerating development of general medical artificial intelligence
(GMAI), powered by multimodal large language models (MLLMs), offers
transformative potential for addressing persistent healthcare challenges,
including workforce deficits and escalating costs. The parallel development of
systematic evaluation benchmarks emerges as a critical imperative to enable
performance assessment and provide technological guidance. Meanwhile, as an
invaluable knowledge source, the potential of medical textbooks for benchmark
development remains underexploited. Here, we present MedBookVQA, a systematic
and comprehensive multimodal benchmark derived from open-access medical
textbooks. To curate this benchmark, we propose a standardized pipeline for
automated extraction of medical figures while contextually aligning them with
corresponding medical narratives. Based on this curated data, we generate 5,000
clinically relevant questions spanning modality recognition, disease
classification, anatomical identification, symptom diagnosis, and surgical
procedures. A multi-tier annotation system categorizes queries through
hierarchical taxonomies encompassing medical imaging modalities (42
categories), body anatomies (125 structures), and clinical specialties (31
departments), enabling nuanced analysis across medical subdomains. We evaluate
a wide array of MLLMs, including proprietary, open-sourced, medical, and
reasoning models, revealing significant performance disparities across task
types and model categories. Our findings highlight critical capability gaps in
current GMAI systems while establishing textbook-derived multimodal benchmarks
as essential evaluation tools. MedBookVQA establishes textbook-derived
benchmarking as a critical paradigm for advancing clinical AI, exposing
limitations in GMAI systems while providing anatomically structured performance
metrics across specialties.
### 🌟 论文解读 | MedBookVQA：基于开源医学书籍的系统性综合医疗基准

### 📌 背景痛点/本文动机
全球医疗系统面临着医护人员短缺与成本攀升等挑战，通用医疗人工智能（GMAI）借助多模态大语言模型（MLLMs）有潜力解决这些问题，但需要系统的评估基准来评估性能与提供技术指导。现有医疗视觉问答（VQA）基准存在范围和适用性局限，多针对特定任务；而医学教科书作为优质知识源，在基准开发上的潜力未被充分挖掘。因此，构建基于医学教科书的综合基准十分必要。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：医疗VQA数据集构建的自适应 pipeline  
提出标准化 pipeline 从医学教科书中自动提取医学图像，并将其与对应的医学文本描述在上下文层面对齐，还设计了生成 VQA 格式数据集的新 pipeline，覆盖模态识别、疾病分类、解剖识别、症状诊断和手术流程这五大医疗任务类别，利用通用匹配规则和关键医学概念语义描述确保可扩展性与泛化性，同时结合严格过滤和人工验证保障数据质量。

💡 创新点2：推出全面的 MedBookVQA 基准  
构建了 MedBookVQA 这一精心策划的基准数据集，它源自公开可用的医学教科书，在多样性和完整性上表现出色，涵盖 42 种医学成像模态、125 个人体解剖结构以及 31 个医学科室，还设计了分层标注系统，按模态、解剖结构和医学科室组织数据集条目，便于灵活数据管理与细粒度分析。

💡 创新点3：广泛评估多模态大语言模型  
对多种 MLLMs 进行全面评估，包含 4 个专有通用系列、6 个开源通用系列、2 个开源医疗系列和 5 个推理聚焦系列的模型，以此探究当前 GMAI 系统性能表现。

### 📈 实验结果
对不同 MLLMs 在 MedBookVQA 上评估得到两个关键发现：一是不同医疗 VQA 任务性能差异大，尤其需要高级医学知识和跨模态推理的任务；二是专有通用 MLLMs 表现优于其他通用 MLLMs 甚至医疗专用模型，而推理模型在医疗任务上未展现出显著优势。同时通过与其他医疗 VQA 基准、推理基准对比，凸显出 MedBookVQA 在任务覆盖、数据源等方面的特色与优势。

### 💬 可借鉴之处
从数据集构建角度，其提出的解析医学教科书、生成多样 VQA 任务并在分层框架内组织数据集的 pipeline 思路，为领域特定数据集构建提供了可复用的自适应流程参考；在基准建设上，MedBookVQA 展示了利用权威知识源（医学教科书）构建综合医疗基准的范式，为推进医疗 AI 评估提供了有价值资源；在模型评估维度，对多类型 MLLMs 广泛评估的方式，能指导后续更全面分析模型在医疗场景下能力短板与优势，助力针对性技术改进。 

## qoq-med--building-multimodal-clinical-foundation-models-with-domain-aware-grpo-training
### Abstract
Clinical decision-making routinely demands reasoning over heterogeneous data,
yet existing multimodal language models (MLLMs) remain largely vision-centric
and fail to generalize across clinical specialties. To bridge this gap, we
introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model
that jointly reasons across medical images, time-series signals, and text
reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization
(DRPO), a novel reinforcement-learning objective that hierarchically scales
normalized rewards according to domain rarity and modality difficulty,
mitigating performance imbalance caused by skewed clinical data distributions.
Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains,
we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on
average across all visual domains as compared to other critic-free training
methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation
data, it is able to highlight salient regions related to the diagnosis, with an
IoU 10x higher than open models while reaching the performance of OpenAI
o4-mini. To foster reproducibility and downstream research, we release (i) the
full model weights, (ii) the modular training pipeline, and (iii) all
intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.
### 🌟 论文解读 | QoQ-Med：打造多模态临床基础模型，突破医疗AI多领域推理瓶颈

### 📌 背景痛点/本文动机
临床诊断需对多模态（如1D心电、2D影像、3D扫描等）异构数据推理，但现有多模态语言模型（MLLMs）存在两大核心问题：一是**模态与领域局限**，多聚焦视觉且难跨临床专科泛化，尚无模型能整合1D传感器数据与2D/3D影像；二是**可解释性缺失**，传统模型输出“黑箱”式结论，医疗从业者难信任无推理过程的诊断，阻碍临床落地。同时，临床数据分布不均（不同领域/模态数据量、难度差异大）也导致模型训练时性能失衡。为解决这些痛点，MIT团队提出QoQ-Med多模态临床基础模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Domain-aware Relative Policy Optimization（DRPO）训练目标  
DRPO是一种基于强化学习的新颖优化目标，**按领域稀缺性和模态难度分层缩放归一化奖励**，解决临床数据分布倾斜导致的性能失衡问题。相比GRPO等无 critic 的训练方法，DRPO能在稀缺、高难度领域“针对性学习”，实现跨难度的均衡训练，让模型在多领域下更稳健。

💡 创新点2：构建QoQ-Med-7B/32B多模态临床通用模型  
QoQ-Med是首个开源的**跨医学影像、时间序列信号、文本报告联合推理**的通用临床基础模型。它整合1D（如ECG）、2D（如X光）、3D（如CT）多模态数据，填补了“无模型能同时处理1D传感器与传统影像”的空白；同时，模型训练时融入大量分割数据，可高亮诊断相关的显著区域，提升可解释性，让临床医生能直观验证诊断逻辑。

### 📈 实验结果
- 诊断性能提升：在覆盖9个临床领域、261万指令调优对的训练下，DRPO训练相比GRPO等方法，**所有视觉领域的macro - F1平均提升43%**，证明其在多领域均衡学习的有效性。  
- 分割与可解释性优势：经密集分割数据训练后，QoQ-Med高亮诊断相关区域的IoU（交并比）比开源模型高10倍，且性能追平OpenAI o4 - mini，在可解释性与精度上实现兼顾。  

### 💬 可借鉴之处
- 多模态整合思路：为医疗AI整合异构数据（1D/2D/3D + 文本）提供范例，推动模型从“单模态/单领域”向“多模态全领域”临床推理进化。  
- 强化学习适配临床数据：DRPO通过领域感知的奖励缩放，解决数据分布不均下的训练失衡，为医疗等数据长尾分布场景的RLHF（基于人类反馈的强化学习）提供新范式。  
- 开源生态建设：公开模型权重、模块化训练 pipeline、中间推理轨迹，为医疗AI领域的可复现研究和下游应用（如诊断辅助、报告生成）铺就基础，加速行业迭代。  

