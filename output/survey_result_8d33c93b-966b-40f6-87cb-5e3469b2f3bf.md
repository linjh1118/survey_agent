# Paper List of Terms(MoE+LLM)
- [25/05] **Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach**  
[[Paper](http://arxiv.org/pdf/2505.14336v1)] [[Code/Page]()] [[TLDR/Notes](#scaling-and-enhancing-llm-based-avsr--a-sparse-mixture-of-projectors-approach)]

- [25/05] **FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation**  
[[Paper](http://arxiv.org/pdf/2505.14256v1)] [[Code/Page]()] [[TLDR/Notes](#fuximt--sparsifying-large-language-models-for-chinese-centric-multilingual-machine-translation)]

- [25/05] **U-SAM: An audio language Model for Unified Speech, Audio, and Music Understanding**  
[[Paper](http://arxiv.org/pdf/2505.13880v1)] [[Code/Page](https://github.com/Honee-W/U-SAM/).)] [[TLDR/Notes](#u-sam--an-audio-language-model-for-unified-speech--audio--and-music-understanding)]

- [25/05] **EfficientLLM: Efficiency in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2505.13840v1)] [[Code/Page]()] [[TLDR/Notes](#efficientllm--efficiency-in-large-language-models)]

- [25/05] **Occult: Optimizing Collaborative Communication across Experts for Accelerated Parallel MoE Training and Inference**  
[[Paper](http://arxiv.org/pdf/2505.13345v1)] [[Code/Page](https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.)] [[TLDR/Notes](#occult--optimizing-collaborative-communication-across-experts-for-accelerated-parallel-moe-training-and-inference)]



# TLDR/Notes
## scaling-and-enhancing-llm-based-avsr--a-sparse-mixture-of-projectors-approach
### Abstract
Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy
environments by integrating visual cues. While recent advances integrate Large
Language Models (LLMs) into AVSR, their high computational cost hinders
deployment in resource-constrained settings. To address this, we propose
Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of
Projectors (SMoP) module to scale model capacity without increasing inference
costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,
Llama-SMoP enables the use of smaller LLMs while maintaining strong
performance. We explore three SMoP configurations and show that Llama-SMoP DEDR
(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and
experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation
studies confirm its effectiveness in expert activation, scalability, and noise
robustness.
### ğŸŒŸ è®ºæ–‡è§£è¯» | "æå‡éŸ³é¢‘è§†è§‰è¯†åˆ«æ•ˆç‡ï¼šLlama-SMoPç¨€ç–æ··åˆæŠ•å½±å™¨æ–¹æ³•"

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éŸ³é¢‘è§†è§‰è¯†åˆ«ï¼ˆAVSRï¼‰é€šè¿‡ç»“åˆéŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œåœ¨å˜ˆæ‚ç¯å¢ƒä¸­æé«˜äº†è¯†åˆ«çš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œè¿‘å¹´æ¥å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°AVSRä¸­è™½ç„¶å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œä½†è¿™äº›æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å®ƒä»¬åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLlama-SMoPçš„ç¨€ç–æ··åˆæŠ•å½±å™¨æ–¹æ³•ï¼Œä»¥åœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹æ‰©å±•æ¨¡å‹å®¹é‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†Llama-SMoPï¼Œä¸€ç§é«˜æ•ˆçš„Multimodal LLMï¼Œå®ƒé‡‡ç”¨ç¨€ç–æ··åˆæŠ•å½±å™¨ï¼ˆSMoPï¼‰æ¨¡å—æ¥æ‰©å±•æ¨¡å‹å®¹é‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æˆæœ¬ä¸å˜ã€‚é€šè¿‡å¼•å…¥ç¨€ç–é—¨æ§çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ•å½±å™¨ï¼ŒLlama-SMoPä½¿å¾—å¯ä»¥ä½¿ç”¨æ›´å°çš„LLMï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡æ¢ç´¢äº†ä¸‰ç§SMoPé…ç½®ï¼Œå¹¶è¯æ˜äº†Llama-SMoP DEDRï¼ˆåˆ†ç¦»ä¸“å®¶ï¼Œåˆ†ç¦»è·¯ç”±å™¨ï¼‰é…ç½®åœ¨ASRã€VSRå’ŒAVSRä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™ç§é…ç½®ä½¿ç”¨æ¨¡æ€ç‰¹å®šçš„è·¯ç”±å™¨å’Œä¸“å®¶ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama-SMoP DEDRåœ¨å¤šç§è§„æ¨¡çš„Llama-based LLMä¸Šå‡ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ASRå’ŒVSRä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¯å®äº†Llama-SMoPåœ¨ä¸“å®¶æ¿€æ´»é¢‘ç‡ã€å¯æ‰©å±•æ€§å’Œå™ªå£°é²æ£’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Llama-SMoPæ–¹æ³•ä¸ºåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²LLM-based AVSRç³»ç»Ÿæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä½¿ç”¨SMoPæ¨¡å—ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¯¹äºå¸Œæœ›åœ¨å®é™…åº”ç”¨ä¸­é›†æˆLLMçš„ researchers å’Œå·¥ç¨‹å¸ˆå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹SMoPçš„ä¸åŒé…ç½®è¿›è¡Œäº†è¯¦ç»†æ¢è®¨ï¼Œä¸ºåç»­ç ”ç©¶å’Œä¼˜åŒ–æä¾›äº†ä¸°å¯Œçš„å®éªŒåŸºç¡€ã€‚

## fuximt--sparsifying-large-language-models-for-chinese-centric-multilingual-machine-translation
### Abstract
In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œFuxiMTï¼šæ‰“é€ ä»¥ä¸­æ–‡ä¸ºæ ¸å¿ƒçš„ç¨€ç–åŒ–å¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›å¤šè¯­è¨€æœºå™¨ç¿»è¯‘â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæœºå™¨ç¿»è¯‘é¢†åŸŸè¿æ¥äº†æ–°çš„çªç ´ã€‚ç„¶è€Œï¼Œå°½ç®¡ä¸­æ–‡åœ¨å…¨çƒè¯­è¨€ä¸­å æœ‰é‡è¦åœ°ä½ï¼Œç›®å‰çš„æŠ€æœ¯è¿›å±•å¹¶æœªèƒ½æ»¡è¶³ä¸­æ–‡åœ¨å¤šè¯­è¨€ç¿»è¯‘æ–¹é¢çš„å·¨å¤§éœ€æ±‚ã€‚ç°æœ‰çš„LLMæ¨¡å‹å¦‚BLOOMå’ŒLLaMAè™½ç„¶å…·æœ‰å¤šè¯­è¨€èƒ½åŠ›ï¼Œä½†æ”¯æŒçš„è¯­ç§æœ‰é™ï¼Œè¿œè¿œä¸èƒ½æ»¡è¶³å…¨çƒ7000å¤šç§è¯­è¨€çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FuxiMTæ¨¡å‹ï¼Œä¸€ç§ä»¥ä¸­æ–‡ä¸ºæ ¸å¿ƒï¼ŒåŸºäºç¨€ç–åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹å¤šè¯­è¨€æœºå™¨ç¿»è¯‘æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
FuxiMTé‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆåœ¨è¶…è¿‡50äº¿ä¸­æ–‡æ ‡è®°çš„å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹æ·±å…¥ç†è§£ä¸­æ–‡ï¼Œç¡®ä¿äº†FuxiMTçš„ä¸­æ–‡æ ¸å¿ƒç‰¹æ€§ã€‚éšåï¼Œåœ¨åŒ…å«65ç§è¯­è¨€çš„è¶…è¿‡100äº¿å¥å¯¹çš„å¹³è¡Œè¯­æ–™åº“ä¸Šè¿›è¡Œå¤šè¯­è¨€å¾®è°ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
FuxiMTå¼•å…¥äº†æ··åˆä¸“å®¶ï¼ˆMixture-of-Experts, MoEsï¼‰æœºåˆ¶ï¼Œå¹¶åœ¨è§£ç å™¨å †ä¸­å®šæœŸæ’å…¥MoEæ¨¡å—ã€‚è¿™ç§æ¨¡å—åŒ–çš„ç»“æ„ä¸ä»…æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¢åŠ è¯­è¨€è¦†ç›–èŒƒå›´å’Œæ•°æ®å¤æ‚æ€§ï¼Œä»¥å®ç°è·¨ä¸åŒèµ„æºæ°´å¹³çš„ç¨³å¥æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒFuxiMTåœ¨ä½èµ„æºåœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„LLMå’Œæœºå™¨ç¿»è¯‘æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒFuxiMTå±•ç°å‡ºäº†å“è¶Šçš„é›¶æ ·æœ¬ç¿»è¯‘èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†æœªè§è¿‡çš„è¯­è¨€å¯¹ï¼Œè¿™è¡¨æ˜å…¶åœ¨å¹³è¡Œæ•°æ®ç¨€ç¼ºæˆ–ä¸å¯ç”¨çš„ç¯å¢ƒä¸­æ¶èµ·æ²Ÿé€šæ¡¥æ¢çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
FuxiMTçš„æå‡ºä¸ºä¸­æ–‡å¤šè¯­è¨€ç¿»è¯‘æä¾›äº†æ–°çš„è§†è§’ï¼Œå…¶åˆ›æ–°çš„è®­ç»ƒç­–ç•¥å’Œæ¨¡å‹ç»“æ„å¯¹äºå¤„ç†å¤§è§„æ¨¡å¤šè¯­è¨€æ•°æ®å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚æ­¤å¤–ï¼ŒFuxiMTåœ¨ä½èµ„æºè¯­è¨€å¯¹çš„ç¿»è¯‘ä¸Šå±•ç°å‡ºçš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºå¤šè¯­è¨€æœºå™¨ç¿»è¯‘çš„å¹¿æ³›åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## u-sam--an-audio-language-model-for-unified-speech--audio--and-music-understanding
### Abstract
The text generation paradigm for audio tasks has opened new possibilities for
unified audio understanding. However, existing models face significant
challenges in achieving a comprehensive understanding across diverse audio
types, such as speech, general audio events, and music. Furthermore, their
exclusive reliance on cross-entropy loss for alignment often falls short, as it
treats all tokens equally and fails to account for redundant audio features,
leading to weaker cross-modal alignment. To deal with the above challenges,
this paper introduces U-SAM, an advanced audio language model that integrates
specialized encoders for speech, audio, and music with a pre-trained large
language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for
task-aware feature fusion, dynamically routing and integrating the
domain-specific encoder outputs. Additionally, U-SAM incorporates a
Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant
audio features under language supervision and rectifies their semantic and
spectral representations to enhance cross-modal alignment. Extensive
experiments demonstrate that U-SAM consistently outperforms both specialized
models and existing audio language models across multiple benchmarks. Moreover,
it exhibits emergent capabilities on unseen tasks, showcasing its
generalization potential. Code is available
(https://github.com/Honee-W/U-SAM/).
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œU-SAMï¼šå¼€å¯è¯­éŸ³ã€éŸ³é¢‘ä¸éŸ³ä¹ç»Ÿä¸€ç†è§£çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—çš„æ˜¾è‘—æˆå°±ï¼Œç ”ç©¶è€…ä»¬å¼€å§‹å°è¯•å°†è¿™äº›æ¨¡å‹æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œå¦‚å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰ä½œä¸ºä¸€ä¸ªæ–°å…´çš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆéŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå®ç°å¯¹éŸ³é¢‘å†…å®¹çš„æ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸åŒç±»å‹çš„éŸ³é¢‘ï¼ˆå¦‚è¯­éŸ³ã€ä¸€èˆ¬éŸ³é¢‘äº‹ä»¶å’ŒéŸ³ä¹ï¼‰æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸”åœ¨éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹é½ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½è¾ƒå¼±ã€‚æœ¬æ–‡æå‡ºäº†U-SAMæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡æ•´åˆç‰¹å®šé¢†åŸŸçš„éŸ³é¢‘ç¼–ç å™¨å’Œä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°å¯¹è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹çš„ç»Ÿä¸€ç†è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
U-SAMå¼•å…¥äº†ä¸€ä¸ªä»»åŠ¡æ„ŸçŸ¥æŠ•å½±æ¨¡å—ï¼ˆTAPMï¼‰ï¼Œè¯¥æ¨¡å—é€šè¿‡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æœºåˆ¶åŠ¨æ€åœ°è·¯ç”±å’Œèåˆæ¥è‡ªå¤šä¸ªéŸ³é¢‘ç¼–ç å™¨çš„åµŒå…¥ï¼Œæ ¹æ®ç›¸åº”ä»»åŠ¡ç”Ÿæˆä»»åŠ¡è‡ªé€‚åº”çš„ç‰¹å¾ï¼Œä»è€Œå®ç°æ›´æ·±å…¥çš„å¤šæ¨¡æ€å¯¹é½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
U-SAMè¿˜åŒ…å«ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼ˆSACLMï¼‰ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€ç›‘ç£ä¸‹æ˜¾å¼è¯†åˆ«å¹¶ä¿®æ­£å†—ä½™çš„éŸ³é¢‘å¸§ï¼Œä»¥å¢å¼ºéŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹é½ã€‚æ­¤å¤–ï¼ŒU-SAMå°†éŸ³é¢‘ä»»åŠ¡è§†ä¸ºå¼€æ”¾å¼çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¹¿æ³›çš„ä»»åŠ¡ï¼Œå¹¶å±•ç°å‡ºåœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ¶Œç°èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†U-SAMæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§èƒ½ï¼Œæ— è®ºæ˜¯åœ¨ç‰¹å®šé¢†åŸŸæ¨¡å‹è¿˜æ˜¯ç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸Šï¼ŒU-SAMå‡å±•ç°å‡ºæ›´å¥½çš„ç»“æœã€‚åŒæ—¶ï¼ŒU-SAMåœ¨æœªè§ä»»åŠ¡ä¸Šå±•ç°å‡ºçš„æ¶Œç°èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶æ³›åŒ–æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
U-SAMæ¨¡å‹çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯¹äºéŸ³é¢‘å¤„ç†é¢†åŸŸå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ï¼Œç‰¹åˆ«æ˜¯å…¶ä»»åŠ¡æ„ŸçŸ¥çš„ç‰¹å¾èåˆç­–ç•¥å’Œè¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±çš„è®¾è®¡ï¼Œä¸ºéŸ³é¢‘å’Œæ–‡æœ¬çš„è·¨æ¨¡æ€å¯¹é½æä¾›äº†æ–°çš„è§†è§’ã€‚æ­¤å¤–ï¼ŒU-SAMåœ¨å¤„ç†ä¸åŒç±»å‹éŸ³é¢‘æ—¶çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿä¸ºæ„å»ºæ›´é€šç”¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å¯ç¤ºã€‚

## efficientllm--efficiency-in-large-language-models
### Abstract
Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ•ˆç‡çš„å…¨é¢ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‚æ•°é‡çš„æ¿€å¢å’Œä¸Šä¸‹æ–‡çª—å£çš„æ‰©å¤§ï¼Œè®¡ç®—ã€èƒ½è€—å’Œæˆæœ¬å˜å¾—è¶Šæ¥è¶Šé«˜æ˜‚ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ç—›ç‚¹ï¼Œé€šè¿‡å¼•å…¥EfficientLLMè¿™ä¸€æ–°å‹åŸºå‡†ï¼Œå¯¹LLMsçš„æ•ˆç‡æŠ€æœ¯è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç³»ç»Ÿæ€§ç ”ç©¶
æœ¬æ–‡ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šæ¶æ„é¢„è®­ç»ƒï¼ˆåŒ…æ‹¬MQAã€GQAã€MLAã€NSAç­‰é«˜æ•ˆæ³¨æ„åŠ›å˜ä½“å’Œç¨€ç–Mixture-of-Expertsï¼‰ã€å¾®è°ƒï¼ˆåŒ…æ‹¬LoRAã€RSLoRAã€DoRAç­‰å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼‰ä»¥åŠæ¨ç†ï¼ˆåŒ…æ‹¬int4ã€float16ç­‰é‡åŒ–æ–¹æ³•ï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢è¯„ä¼°æŒ‡æ ‡
å®šä¹‰äº†å…­ä¸ªç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå†…å­˜åˆ©ç”¨ç‡ã€è®¡ç®—åˆ©ç”¨ç‡ã€å»¶è¿Ÿã€ååé‡ã€èƒ½è€—å’Œå‹ç¼©ç‡ï¼‰ï¼Œä»¥å…¨é¢æ•æ‰ç¡¬ä»¶é¥±å’Œåº¦ã€å»¶è¿Ÿ-ååé‡å¹³è¡¡å’Œç¢³æˆæœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯„ä¼°äº†è¶…è¿‡100ç§æ¨¡å‹-æŠ€æœ¯ç»„åˆï¼Œæ¶µç›–0.5Bè‡³72Bå‚æ•°çš„LLMsï¼Œå¾—å‡ºä»¥ä¸‹æ ¸å¿ƒè§è§£ï¼š
1. æ•ˆç‡æ¶‰åŠå¯é‡åŒ–çš„æƒè¡¡ï¼šæ²¡æœ‰ä¸€ç§æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½æ˜¯æœ€ä¼˜çš„ã€‚ä¾‹å¦‚ï¼ŒMixture-of-Expertså‡å°‘äº†FLOPså¹¶æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†å¢åŠ äº†40%çš„VRAMï¼›è€Œint4é‡åŒ–åœ¨å¹³å‡3-5%çš„ä»»åŠ¡åˆ†æ•°ä¸‹é™ä¸‹ï¼Œå‡å°‘äº†å†…å­˜å’Œèƒ½è€—é«˜è¾¾3.9å€ã€‚
2. æœ€ä¼˜è§£ä¾èµ–äºä»»åŠ¡å’Œè§„æ¨¡ï¼šMQAåœ¨å—é™è®¾å¤‡ä¸Šæä¾›äº†æœ€ä½³çš„å†…å­˜-å»¶è¿Ÿæƒè¡¡ï¼ŒMLAåœ¨è´¨é‡å…³é”®ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½çš„å›°æƒ‘åº¦ï¼ŒRSLoRAä»…åœ¨è¶…è¿‡14Bå‚æ•°æ—¶æ‰æ¯”LoRAæ›´é«˜æ•ˆã€‚
3. æŠ€æœ¯å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼šå°†è¯„ä¼°æ‰©å±•åˆ°å¤§å‹è§†è§‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒéªŒè¯äº†åœ¨LLMsä¸ŠéªŒè¯çš„æ•ˆç‡æŠ€æœ¯åœ¨å…¶ä»–æ¨¡æ€ä¸Šçš„æœ‰æ•ˆè¿ç§»æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬ç ”ç©¶ä¸ºç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæä¾›äº†ä¸€ä»½é‡è¦çš„æŒ‡å¯¼ï¼Œå¸®åŠ©ä»–ä»¬äº†è§£ä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹æ•ˆç‡-æ€§èƒ½æ™¯è§‚ï¼Œå¹¶é€šè¿‡å¼€æºæ•°æ®é›†ã€è¯„ä¼°ç®¡é“å’Œæ’è¡Œæ¦œï¼Œä¸ºæ¢ç´¢æ•ˆç‡æŠ€æœ¯çš„è½¬ç§»æ€§å’Œé€‚ç”¨æ€§æä¾›äº†å®è´µçš„èµ„æºã€‚

## occult--optimizing-collaborative-communication-across-experts-for-accelerated-parallel-moe-training-and-inference
### Abstract
Mixture-of-experts (MoE) architectures could achieve impressive computational
efficiency with expert parallelism, which relies heavily on all-to-all
communication across devices. Unfortunately, such communication overhead
typically constitutes a significant portion of the total runtime, hampering the
scalability of distributed training and inference for modern MoE models
(consuming over $40\%$ runtime in large-scale training). In this paper, we
first define collaborative communication to illustrate this intrinsic
limitation, and then propose system- and algorithm-level innovations to reduce
communication costs. Specifically, given a pair of experts co-activated by one
token, we call them "collaborated", which comprises $2$ cases as intra- and
inter-collaboration, depending on whether they are kept on the same device. Our
pilot investigations reveal that augmenting the proportion of
intra-collaboration can accelerate expert parallelism at scale. It motivates us
to strategically optimize collaborative communication for accelerated MoE
training and inference, dubbed Occult. Our designs are capable of either
delivering exact results with reduced communication cost or controllably
minimizing the cost with collaboration pruning, materialized by modified
fine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that
Occult can be faster than popular state-of-the-art inference or training
frameworks (more than $1.5\times$ speed up across multiple tasks and models)
with comparable or superior quality compared to the standard fine-tuning. Code
is available at
$\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¼˜åŒ–ä¸“å®¶åä½œé€šä¿¡ï¼ŒåŠ é€Ÿæ··åˆä¸“å®¶æ¨¡å‹è®­ç»ƒä¸æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼Œé€šè¿‡å¢åŠ æ¨¡å‹å‚æ•°æ¥æå‡æ¨¡å‹èƒ½åŠ›å·²æˆä¸ºå¸¸æ€ã€‚æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„é€šè¿‡ç¨€ç–æ€§å®ç°äº†å‚æ•°çš„é«˜æ•ˆæ‰©å±•ã€‚ç„¶è€Œï¼ŒMoEæ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ä¸­ï¼Œè·¨è®¾å¤‡é€šä¿¡å¼€é”€å·¨å¤§ï¼Œé€šå¸¸å æ®æ€»è¿è¡Œæ—¶é—´çš„40%ä»¥ä¸Šï¼Œä¸¥é‡é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€ç—›ç‚¹ï¼Œæå‡ºäº†ä¼˜åŒ–é€šä¿¡ç­–ç•¥ä»¥åŠ é€ŸMoEæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåä½œé€šä¿¡æ¦‚å¿µ
æœ¬æ–‡å°†MoEå·¥ä½œæµç¨‹ä¸­çš„å…¨å¯¹å…¨é€šä¿¡é‡æ–°å®šä¹‰ä¸ºåä½œé€šä¿¡ã€‚ç»™å®šä¸€ä¸ªtokenåŒæ—¶æ¿€æ´»çš„ä¸“å®¶å¯¹ï¼Œç§°ä¸ºâ€œåä½œâ€ï¼Œå¹¶æ ¹æ®å®ƒä»¬æ˜¯å¦ä½äºåŒä¸€è®¾å¤‡ä¸Šï¼Œåˆ†ä¸ºå†…éƒ¨åä½œå’Œå¤–éƒ¨åä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸“å®¶åä½œä¼˜åŒ–
é€šè¿‡æœ€å¤§åŒ–å†…éƒ¨åä½œå’Œæœ€å°åŒ–å¤–éƒ¨åä½œï¼Œä¼˜åŒ–é€šä¿¡æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–çŸ©é˜µä¹˜æ³•æ ¸ï¼Œä»¥å‡å°‘ä¸å¿…è¦çš„å†…å­˜åˆ†é…æˆ–è®¿é—®ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†æåä½œæ•°æ®é›†ï¼Œé‡æ–°å®‰æ’ä¸“å®¶æ”¾ç½®ä½ç½®ï¼Œä»¥åŠæå‡ºåä½œå‰ªæç­–ç•¥ï¼Œä»¥å¯æ§åœ°é™ä½é€šä¿¡æˆæœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å„ç§MoE-LLMä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæå‡ºçš„Occultæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡å‹ä¸Šå‡èƒ½å®ç°è¶…è¿‡1.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒæˆ–ä¼˜äºæ ‡å‡†å¾®è°ƒçš„è´¨é‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä»ç³»ç»Ÿçº§å’Œç®—æ³•çº§æå‡ºäº†åˆ›æ–°çš„é€šä¿¡ä¼˜åŒ–ç­–ç•¥ï¼Œä¸ä»…æé«˜äº†MoEæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼Œè¿˜ä¸ºé€šä¿¡å¯†é›†å‹ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæå‡ºçš„åä½œé€šä¿¡æ¦‚å¿µä¸ºç†è§£å’Œä¼˜åŒ–å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—æä¾›äº†æ–°çš„è§†è§’ã€‚

