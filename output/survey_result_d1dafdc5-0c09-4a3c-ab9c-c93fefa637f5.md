# Paper List of Terms(grm+RL)
- [25/06] **ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2506.16712v1)] [[Code/Page]()] [[TLDR/Notes](#reasongrm--enhancing-generative-reward-models-through-large-reasoning-models)]

- [25/05] **Generative RLHF-V: Learning Principles from Multi-modal Human Preference**  
[[Paper](http://arxiv.org/pdf/2505.18531v1)] [[Code/Page](https://generative-rlhf-v.github.io.)] [[TLDR/Notes](#generative-rlhf-v--learning-principles-from-multi-modal-human-preference)]

- [25/05] **Action-Dependent Optimality-Preserving Reward Shaping**  
[[Paper](http://arxiv.org/pdf/2505.12611v1)] [[Code/Page]()] [[TLDR/Notes](#action-dependent-optimality-preserving-reward-shaping)]

- [25/04] **Inference-Time Scaling for Generalist Reward Modeling**  
[[Paper](http://arxiv.org/pdf/2504.02495v2)] [[Code/Page]()] [[TLDR/Notes](#inference-time-scaling-for-generalist-reward-modeling)]



# TLDR/Notes
## reasongrm--enhancing-generative-reward-models-through-large-reasoning-models
### Abstract
Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ReasonGRMï¼šå€Ÿå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›é©æ–°ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£ã€ç”Ÿæˆä¸å†³ç­–ä¸Šå–å¾—é•¿è¶³è¿›æ­¥ï¼Œä½†è¦è®©æ¨¡å‹è¾“å‡ºè´´åˆäººç±»ä»·å€¼è§‚ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯å…³é”®ã€‚ä¼ ç»Ÿæ ‡é‡å¥–åŠ±æ¨¡å‹ï¼ˆSRMsï¼‰æŠŠå¤æ‚äººç±»åå¥½å‹ç¼©æˆå•ä¸€æ ‡é‡ï¼Œæ˜“ä¿¡æ¯ä¸¢å¤±ã€æ³›åŒ–æ€§å¼±ï¼›æ–°å…´ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰è™½æ›´çµæ´»ï¼Œä½†æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå¸¸å‡ºç°æ¨ç†è·¯å¾„ä¸å®Œæ•´æˆ–è¿‡åº¦æ¨æµ‹ï¼Œå¯¼è‡´ä»»åŠ¡ä¸­â€œå¹»è§‰â€æˆ–å…³é”®ä¿¡æ¯ç¼ºå¤±ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡GRMsçš„æ¨ç†è´¨é‡ä»¥å®ç°å¯é åå¥½å»ºæ¨¡ï¼Œæˆäº†æ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºReasonGRMä¸‰é˜¶æ®µæ¡†æ¶  
ReasonGRMåˆ†ä¸‰æ­¥æ‰“é€ æ›´ä¼˜ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼š  
- é˜¶æ®µä¸€ï¼ˆç”Ÿæˆæ¨ç†è·¯å¾„ï¼‰ï¼šç”¨Zero - RLç”Ÿæˆç®€æ´ã€ä»¥ç»“æœä¸ºå¯¼å‘çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘å…³é”®ä¿¡æ¯é—æ¼é£é™©ï¼›  
- é˜¶æ®µäºŒï¼ˆç­›é€‰ä¼˜è´¨è·¯å¾„ï¼‰ï¼šå¼•å…¥å…¨æ–°è¯„ä¼°æŒ‡æ ‡\( R^\star \)ï¼Œä¾æ®ç”Ÿæˆå¯èƒ½æ€§ä¸ºæ¨ç†è·¯å¾„æ‰“åˆ†ï¼Œåå¥½â€œç”¨æœ€å°‘æ¢ç´¢è¾¾æ­£ç¡®ç­”æ¡ˆâ€çš„è·¯å¾„ï¼Œå‰Šå‡è®­ç»ƒä¸­æ˜“å¼•å‘å¹»è§‰çš„æ•°æ®ï¼›  
- é˜¶æ®µä¸‰ï¼ˆå¼ºåŒ–æ¨¡å‹èƒ½åŠ›ï¼‰ï¼šé’ˆå¯¹é«˜éš¾åº¦ç¤ºä¾‹ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ç²¾è°ƒæ¨¡å‹ï¼Œå¢å¼ºå…¶åå¥½åŒºåˆ†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰\( R^\star \)è¯„ä¼°æŒ‡æ ‡è§£å†³æ•°æ®è´¨é‡éš¾é¢˜  
\( R^\star \)ç»“åˆâ€œæœ‰æ•ˆæ€§ï¼ˆValidityï¼Œæ¨ç†å¯¼å‘æ­£ç¡®ç»“æœï¼‰â€ä¸â€œè‡ªä¸€è‡´æ€§ï¼ˆSelf - Consistencyï¼Œæ¨ç†é€»è¾‘è¿è´¯æ— å†—ä½™ï¼‰â€ä¸¤å¤§å…³é”®å±æ€§ï¼Œé€šè¿‡ç”Ÿæˆå¯èƒ½æ€§æ¥è¯„ä¼°æ¨ç†è·¯å¾„ï¼Œèƒ½ä»å™ªå£°å€™é€‰é›†ä¸­è‡ªåŠ¨é€‰ä¼˜è´¨æ¨ç†è·¯å¾„ï¼Œç ´è§£å¤æ‚ä»»åŠ¡å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æ•°æ®è´¨é‡ç“¶é¢ˆã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨RM - Benchã€RewardBenchã€RMBä¸‰å¤§å…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonGRMè¡¨ç°äº®çœ¼ï¼šå¹³å‡è¶…è¶Šæ­¤å‰æœ€ä¼˜GRMs 1.8%ï¼Œåœ¨éƒ¨åˆ†åœºæ™¯ä¸‹æ¯”GPT - 4oç­‰é—­æºæ¨¡å‹é¢†å…ˆè¾¾5.6%ï¼Œè¿˜æ¯”ä¸»æµSRMså¹³å‡é«˜4.5%ã€‚å®éªŒä¸ä»…éªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œæ¶ˆèå®éªŒä¹Ÿå‰–æäº†æ¨ç†è´¨é‡ã€\( R^\star \)è¿‡æ»¤æ•ˆæœã€å„è®­ç»ƒé˜¶æ®µå¯¹æœ€ç»ˆå¥–åŠ±æ¨¡å‹çš„å½±å“ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é‡è§†æ¨ç†è´¨é‡åœ¨å¥–åŠ±æ¨¡å‹ä¸­çš„ä»·å€¼ï¼šæ­ç¤ºäº†é«˜è´¨é‡æ¨ç†è·¯å¾„ï¼ˆå…¼é¡¾æœ‰æ•ˆæ€§ä¸è‡ªä¸€è‡´æ€§ï¼‰å¯¹åå¥½å»ºæ¨¡çš„å…³é”®ä½œç”¨ï¼Œä¸ºåç»­å¥–åŠ±æ¨¡å‹è®¾è®¡æŒ‡æ˜â€œæ¨ç†æ„ŸçŸ¥â€æ–¹å‘ï¼›  
2. åˆ›æ–°è¯„ä¼°ä¸è¿‡æ»¤æœºåˆ¶ï¼š\( R^\star \)å±•ç¤ºäº†å¦‚ä½•ç”¨ç”Ÿæˆå¯èƒ½æ€§é‡åŒ–æ¨ç†è´¨é‡ï¼Œä¸ºå¤„ç†å™ªå£°æ•°æ®ã€æ„å»ºä¼˜è´¨è®­ç»ƒé›†æä¾›äº†å¯å¤ç”¨æ€è·¯ï¼›  
3. å¤šé˜¶æ®µè®­ç»ƒPipelineï¼šä»ç”Ÿæˆåˆ°ç­›é€‰å†åˆ°å¼ºåŒ–å­¦ä¹ çš„æµç¨‹ï¼Œä¸ºé€šç”¨LLMå‘ä¸“ç²¾å¥–åŠ±æ¨¡å‹è½¬åŒ–æä¾›äº†å·¥ç¨‹åŒ–å‚è€ƒèŒƒå¼ï¼›  
4. å…¨é¢å®éªŒéªŒè¯ï¼šè·¨å¤šä¸ªæƒå¨åŸºå‡†çš„æµ‹è¯•+æ¶ˆèå®éªŒï¼Œæ˜¯å­¦æœ¯ç ”ç©¶ä¸­éªŒè¯æ–¹æ³•æ™®é€‚æ€§ä¸æ¨¡å—ä»·å€¼çš„å…¸èŒƒï¼Œå€¼å¾—å€Ÿé‰´ä»¥å¢å¼ºç ”ç©¶è¯´æœåŠ›ã€‚  
```

## generative-rlhf-v--learning-principles-from-multi-modal-human-preference
### Abstract
Training multi-modal large language models (MLLMs) that align with human
intentions is a long-term challenge. Traditional score-only reward models for
alignment suffer from low accuracy, weak generalization, and poor
interpretability, blocking the progress of alignment methods, e.g.,
reinforcement learning from human feedback (RLHF). Generative reward models
(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate
pair-wise responses, but their pair-wise paradigm makes it hard to generalize
to learnable rewards. We introduce Generative RLHF-V, a novel alignment
framework that integrates GRMs with multi-modal RLHF. We propose a two-stage
pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL
guides GRMs to actively capture human intention, then predict the correct
pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which
enhances multi-modal RL scoring precision by grouped responses comparison.
Experimental results demonstrate that, besides out-of-distribution
generalization of RM discrimination, our framework improves 4 MLLMs'
performance across 7 benchmarks by $18.1\%$, while the baseline RLHF is only
$5.3\%$. We further validate that Generative RLHF-V achieves a near-linear
improvement with an increasing number of candidate responses. Our code and
models can be found at https://generative-rlhf-v.github.io.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Generative RLHF-Vï¼šä»å¤šæ¨¡æ€äººç±»åå¥½ä¸­å­¦ä¹ åŸåˆ™ï¼Œçªç ´MLLMå¯¹é½éš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è®­ç»ƒä¸äººç±»æ„å›¾å¯¹é½çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯ä¸€é¡¹é•¿æœŸæŒ‘æˆ˜ã€‚ä¼ ç»Ÿä»…åŸºäºåˆ†æ•°çš„å¥–åŠ±æ¨¡å‹ï¼ˆscore - only reward modelï¼‰åœ¨å¯¹é½ä»»åŠ¡ä¸­å­˜åœ¨å‡†ç¡®ç‡ä½ã€æ³›åŒ–èƒ½åŠ›å¼±å’Œå¯è§£é‡Šæ€§å·®çš„é—®é¢˜ï¼Œé˜»ç¢äº†å¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç­‰å¯¹é½æ–¹æ³•çš„å‘å±•ã€‚ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰è™½åˆ©ç”¨MLLMsçš„å†…åœ¨æ¨ç†èƒ½åŠ›åŒºåˆ†æˆå¯¹å“åº”ï¼Œä½†æˆå¯¹èŒƒå¼éš¾ä»¥æ³›åŒ–åˆ°å¯å­¦ä¹ çš„å¥–åŠ±ã€‚åŒæ—¶ï¼Œéšç€MLLMsæ„ˆå‘å¤æ‚ï¼Œå¯¹å…¶å“åº”çš„äººç±»åå¥½è¯„ä¼°ä¹Ÿæ›´å¤æ‚å¤šæ ·ï¼Œä»…ä¾èµ–å•ä¸€çš„score - only RMæ¨ç†ä¸è¶³ä»¥å­¦ä¹ å¯æ³›åŒ–çš„äººç±»åå¥½ï¼›ä¸”æˆå¯¹æ¯”è¾ƒçš„åé¦ˆéš¾ä»¥è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ‰€éœ€çš„ç‚¹å¼åˆ†æ•°ï¼Œé˜»ç¢äº†å­¦ä¹ åˆ°çš„åŸåˆ™æœ‰æ•ˆæŒ‡å¯¼å¤šæ¨¡æ€RLHFã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºRLçš„å¤šæ¨¡æ€GRMså­¦ä¹ å¤šæ¨¡æ€åå¥½åŸåˆ™
å¼€å‘é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤šæ¨¡æ€ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿæ¨ç†åŸåˆ™å¹¶ç²¾ç¡®é¢„æµ‹å¥–åŠ±ã€‚è¯¥GRMè®­ç»ƒå°†è‡ªåŸåˆ™æ€§æ‰¹åˆ¤è°ƒä¼˜ï¼ˆSPCTï¼‰æ‰©å±•åˆ°è§†è§‰åœºæ™¯ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒMLLMsä½œä¸ºGRMsï¼Œä»¥åå¥½æ•°æ®é›†ä¸­å¸¦æ³¨é‡Šçš„çœŸå®æ ‡ç­¾ä½œä¸ºåŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œåœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­è®©GRMsè‡ªä¸»æ¢ç´¢åŸåˆ™ï¼Œç›¸æ¯”ä»å‚è€ƒé›†ä¸­é€‰æ‹©åŸåˆ™æ³›åŒ–æ€§æ›´å¼ºã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€ç”Ÿæˆå¼RLHFæ¡†æ¶
æå‡ºä¸¤é˜¶æ®µçš„Generative RLHF - Væ¡†æ¶ï¼Œæ•´åˆGRMsä¸å¤šæ¨¡æ€RLHFã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯â€œä»RLè¿›è¡Œå¤šæ¨¡æ€ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡â€ï¼ŒRLå¼•å¯¼GRMsä¸»åŠ¨æ•æ‰äººç±»æ„å›¾ï¼Œè¿›è€Œé¢„æµ‹æ­£ç¡®çš„æˆå¯¹åˆ†æ•°ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯â€œä»åˆ†ç»„æ¯”è¾ƒè¿›è¡ŒRLä¼˜åŒ–â€ï¼Œé€šè¿‡å¯¹å“åº”åˆ†ç»„æ¯”è¾ƒæ¥æé«˜å¤šæ¨¡æ€RLè¯„åˆ†ç²¾åº¦ï¼Œå®ç°ä»æˆå¯¹æ¯”è¾ƒå­¦åˆ°çš„åŸåˆ™åˆ°ç‚¹å¼åˆ†æ•°çš„æ³›åŒ–ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆ†ç»„æ¯”è¾ƒå®ç°è®­ç»ƒåæ‰©å±•
å‘ç°GRM + RLä¸åˆ†ç»„æ¯”è¾ƒçš„ç»“åˆï¼Œèƒ½è®©RLä¼˜åŒ–çš„æ€§èƒ½åœ¨ä¸€å®šèŒƒå›´å†…éšç€å€™é€‰å“åº”æ•°é‡nçš„å¢åŠ è¿‘ä¹çº¿æ€§æå‡ï¼Œç§»é™¤ä»»ä¸€ç»„ä»¶åˆ™è¿™ç§æå‡æ¶ˆå¤±ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šæ¨¡æ€GRMå¥–åŠ±é»‘å®¢çš„å…ˆé©±æ€§æ¡ˆä¾‹ç ”ç©¶
å‘ç°åœ¨è¿‡åº¦è®­ç»ƒçš„GRMä¸‹è¿›è¡ŒRLè¿‡åº¦è®­ç»ƒï¼Œä¼šå¯¼è‡´æ¨¡å‹é‡‡ç”¨è‡ªæˆ‘è¡¨æ‰¬è¡Œä¸ºæ¥è·å–é«˜å¥–åŠ±ï¼Œç”šè‡³åœ¨é‡‡ç”¨MLLMä½œä¸ºè¯„åˆ¤èŒƒå¼çš„åŸºå‡†æµ‹è¯•ä¸­è·å¾—æé«˜åˆ†æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åˆ†å¸ƒå¤–çš„åˆ¤åˆ«ä»»åŠ¡ä¸­ï¼ŒåŸºäºRLè®­ç»ƒçš„å¤šæ¨¡æ€GRMsåœ¨å¥–åŠ±åˆ¤åˆ«å‡†ç¡®ç‡ä¸Šå¹³å‡æå‡20.4%ï¼›åœ¨7ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶ä½¿4ä¸ªMLLMsçš„æ€§èƒ½æå‡äº†18.1%ï¼Œè€ŒåŸºçº¿RLHFä»…æå‡5.3%ï¼›è¿˜éªŒè¯äº†Generative RLHF - Véšç€å€™é€‰å“åº”æ•°é‡å¢åŠ èƒ½å®ç°è¿‘ä¹çº¿æ€§çš„æ€§èƒ½æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹é½é¢†åŸŸï¼Œæä¾›äº†ä»äººç±»åå¥½ä¸­å­¦ä¹ åŸåˆ™çš„æ–°æ¡†æ¶æ€è·¯ï¼Œå°¤å…¶æ˜¯åŸºäºRLçš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ„å»ºä»¥åŠåˆ†ç»„æ¯”è¾ƒä¼˜åŒ–RLçš„æ–¹å¼ï¼Œä¸ºè§£å†³ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ç¼ºé™·æä¾›äº†åˆ›æ–°æ–¹å‘ï¼›å…¶å¯¹å¤šæ¨¡æ€GRMå¥–åŠ±é»‘å®¢ç°è±¡çš„ç ”ç©¶ï¼Œä¹Ÿä¸ºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„é£é™©è¯†åˆ«ä¸è§„é¿æä¾›äº†å‚è€ƒï¼›åœ¨æ¨¡å‹æ€§èƒ½éšå€™é€‰å“åº”æ•°é‡æ‰©å±•æ–¹é¢çš„å‘ç°ï¼Œä¸ºåç»­æ¨¡å‹è®­ç»ƒè§„æ¨¡æ‰©å¤§å’Œæ€§èƒ½æå‡ç­–ç•¥æä¾›äº†å€Ÿé‰´ã€‚

## action-dependent-optimality-preserving-reward-shaping
### Abstract
Recent RL research has utilized reward shaping--particularly complex shaping
rewards such as intrinsic motivation (IM)--to encourage agent exploration in
sparse-reward environments. While often effective, ``reward hacking'' can lead
to the shaping reward being optimized at the expense of the extrinsic reward,
resulting in a suboptimal policy. Potential-Based Reward Shaping (PBRS)
techniques such as Generalized Reward Matching (GRM) and Policy-Invariant
Explicit Shaping (PIES) have mitigated this. These methods allow for
implementing IM without altering optimal policies. In this work we show that
they are effectively unsuitable for complex, exploration-heavy environments
with long-duration episodes. To remedy this, we introduce Action-Dependent
Optimality Preserving Shaping (ADOPS), a method of converting intrinsic rewards
to an optimality-preserving form that allows agents to utilize IM more
effectively in the extremely sparse environment of Montezuma's Revenge. We also
prove ADOPS accommodates reward shaping functions that cannot be written in a
potential-based form: while PBRS-based methods require the cumulative
discounted intrinsic return be independent of actions, ADOPS allows for
intrinsic cumulative returns to be dependent on agents' actions while still
preserving the optimal policy set. We show how action-dependence enables
ADOPS's to preserve optimality while learning in complex, sparse-reward
environments where other methods struggle.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹æ›´ä¼˜çš„å¥–åŠ±å¡‘é€ ï¼šADOPSæ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢†åŸŸï¼Œåˆ©ç”¨å¥–åŠ±å¡‘é€ ï¼ˆå°¤å…¶æ˜¯å†…åœ¨åŠ¨æœºIMè¿™ç±»å¤æ‚å¥–åŠ±ï¼‰æ¥é¼“åŠ±æ™ºèƒ½ä½“åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­æ¢ç´¢æ˜¯ç ”ç©¶çƒ­ç‚¹ã€‚ä½†ä¼ ç»Ÿå¥–åŠ±å¡‘é€ å’ŒIMå­˜åœ¨â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜â€”â€”æ™ºèƒ½ä½“å¯èƒ½ä¸ºä¼˜åŒ–å¡‘é€ å¥–åŠ±è€Œç‰ºç‰²å¤–åœ¨å¥–åŠ±ï¼Œå¯¼è‡´æ¬¡ä¼˜ç­–ç•¥ã€‚åŸºäºåŠ¿èƒ½çš„å¥–åŠ±å¡‘é€ ï¼ˆPBRSï¼‰æŠ€æœ¯ï¼ˆå¦‚GRMã€PIESï¼‰è™½èƒ½åœ¨ä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥ä¸‹å®ç°IMï¼Œå´åœ¨å¤æ‚ã€æ¢ç´¢éœ€æ±‚é«˜ä¸”é•¿æ—¶é•¿çš„ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼ˆå¦‚åœ¨Montezuma's Revengeè¿™ç±»å…¬è®¤æå…·æŒ‘æˆ˜æ€§çš„ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­ï¼Œç°æœ‰æ–¹æ³•ç”šè‡³ä¸å¦‚ä»…ç”¨RNDè®­ç»ƒçš„æ™ºèƒ½ä½“ï¼‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºæ–°æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçªç ´å‰æå‡è®¾é™åˆ¶  
æ‘’å¼ƒäº†å…ˆå‰æ–¹æ³•ä¿éšœæœ€ä¼˜æ€§æ‰€éœ€çš„å…³é”®å‡è®¾ï¼Œæ¯”å¦‚ä¸å†è¦æ±‚ç¯å¢ƒæ˜¯ episodicï¼ˆ episodic æŒ‡æœ‰æ˜ç¡® episode ç»“æŸçš„ç¯å¢ƒç±»å‹ï¼‰ï¼Œä¹Ÿä¸å†è¦æ±‚å†…åœ¨åŠ¨æœºæ˜¯â€œæœªæ¥æ— å…³â€çš„ï¼Œè®©æ–¹æ³•é€‚ç”¨åœºæ™¯æ›´çµæ´»ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‰©å¤§æœ€ä¼˜æ€§ä¿ç•™å¡‘é€ å‡½æ•°èŒƒå›´  
ç›¸æ¯”GRMå’ŒPBIMè¦æ±‚æ¯ä¸€æ­¥é¢„æœŸå†…åœ¨å›æŠ¥ä¸åŠ¨ä½œæ— å…³ï¼ŒADOPSå…è®¸å†…åœ¨ç´¯ç§¯å›æŠ¥ä¾èµ–æ™ºèƒ½ä½“åŠ¨ä½œï¼ŒåŒæ—¶ä»ä¿ç•™æœ€ä¼˜ç­–ç•¥é›†åˆï¼›ä¸”ä¸åƒPIESæœ€ç»ˆè¦åœæ­¢æä¾›å¡‘é€ å¥–åŠ±ï¼ŒADOPSèƒ½è®©æ™ºèƒ½ä½“åœ¨ä»»æ„é•¿æ—¶é—´å†…æ¥æ”¶å¡‘é€ å¥–åŠ±ã€‚è¿™ç§å¯¹åŠ¨ä½œçš„ä¾èµ–ç‰¹æ€§ï¼Œæ˜¯å…¶åœ¨å¤æ‚ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è¶…è¶Šå…¶ä»–æ–¹æ³•çš„å…³é”®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŠ¨æ€è°ƒæ•´å†…åœ¨å¥–åŠ±é€»è¾‘  
ADOPSé€šè¿‡å‚è€ƒæ™ºèƒ½ä½“çš„ critic ç½‘ç»œå¯¹å¤–åœ¨å’Œå†…åœ¨ä»·å€¼å‡½æ•°çš„ä¼°è®¡ï¼Œå½“ä¸”ä»…å½“å†…åœ¨å¥–åŠ±ä¼šå¯¼è‡´åŠ¨ä½œåå¥½ä¸ä»…å¤–åœ¨å¥–åŠ±ä¸‹çš„åå¥½ä¸ä¸€è‡´æ—¶ï¼Œä¸»åŠ¨è°ƒæ•´æ™ºèƒ½ä½“çœ‹åˆ°çš„å†…åœ¨å¥–åŠ±ï¼Œä»¥æ­¤ä¿éšœæœ€ä¼˜æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Montezuma's Revengeè¿™ä¸€å¤æ‚ä¸”æåº¦ç¨€ç–å¥–åŠ±çš„åŸºå‡†ç¯å¢ƒä¸­ï¼ŒADOPSåœ¨å®ç°å†…åœ¨åŠ¨æœºï¼ˆIMï¼‰çš„åŒæ—¶ä¿ç•™æœ€ä¼˜ç­–ç•¥ï¼Œå®è¯ä¸Šè¶…è¶Šäº†å…¶ä»–ä¿ç•™æœ€ä¼˜æ€§çš„æ–¹æ³•ä»¥åŠåŸºçº¿IMï¼Œè¾¾æˆè¯¥åœºæ™¯ä¸‹çš„æ–°SOTAï¼ˆå½“å‰æœ€ä¼˜æ€§èƒ½ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•è®¾è®¡æ€è·¯ä¸Šï¼Œæ‰“ç ´ä¼ ç»Ÿå‡è®¾ã€æ‹“å±•å‡½æ•°é€‚ç”¨èŒƒå›´çš„æ€è·¯ï¼Œä¸ºåç»­å¤„ç†æ›´å¤æ‚ç¯å¢ƒä¸‹çš„å¥–åŠ±å¡‘é€ é—®é¢˜æä¾›äº†æ–°æ–¹å‘ï¼Œå¯å‘ç ”ç©¶è€…æ€è€ƒå¦‚ä½•åœ¨ä¿éšœç†è®ºæ€§è´¨æ—¶æ”¾å®½çº¦æŸã€‚  
2. æŠ€æœ¯å®ç°å±‚é¢ï¼ŒåŸºäº critic ç½‘ç»œä¼°è®¡æ¥åŠ¨æ€è°ƒæ•´å¥–åŠ±çš„æ–¹å¼ï¼Œä¸ºè§£å†³â€œå¥–åŠ±é»‘å®¢â€ç­‰é—®é¢˜æä¾›äº†å¯å‚è€ƒçš„åŠ¨æ€è°ƒèŠ‚èŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€å¹³è¡¡å¤šç±»å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ã€‚  
3. å®éªŒé€‰æ‹©ä¸Šï¼Œé’ˆå¯¹å…¬è®¤éš¾è®­çš„Montezuma's Revengeç¯å¢ƒéªŒè¯ï¼Œè¯æ˜æ–¹æ³•åœ¨æç«¯åœºæ™¯æœ‰æ•ˆæ€§ï¼Œä¹Ÿè®©åç»­ç ”ç©¶åœ¨æµ‹è¯•å¤æ‚ç¨€ç–å¥–åŠ±ä»»åŠ¡æ—¶æ›´å…³æ³¨ç±»ä¼¼é«˜éš¾åº¦åŸºå‡†ã€‚

## inference-time-scaling-for-generalist-reward-modeling
### Abstract
Reinforcement learning (RL) has been widely adopted in post-training for
large language models (LLMs) at scale. Recently, the incentivization of
reasoning capabilities in LLMs from RL indicates that $\textit{proper learning
methods could enable effective inference-time scalability}$. A key challenge of
RL is to obtain accurate reward signals for LLMs in various domains beyond
verifiable questions or artificial rules. In this work, we investigate how to
improve reward modeling (RM) with more inference compute for general queries,
i.e. the $\textbf{inference-time scalability of generalist RM}$, and further,
how to improve the effectiveness of performance-compute scaling with proper
learning methods. For the RM approach, we adopt pointwise generative reward
modeling (GRM) to enable flexibility for different input types and potential
for inference-time scaling. For the learning method, we propose Self-Principled
Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs
through online RL, to generate principles adaptively and critiques accurately,
resulting in $\textbf{DeepSeek-GRM}$ models. Furthermore, for effective
inference-time scaling, we use parallel sampling to expand compute usage, and
introduce a meta RM to guide voting process for better scaling performance.
Empirically, we show that SPCT significantly improves the quality and
scalability of GRMs, outperforming existing methods and models in various RM
benchmarks without severe biases, and could achieve better performance compared
to training-time scaling. DeepSeek-GRM still meets challenges in some tasks,
which we believe can be addressed by future efforts in generalist reward
systems. The models will be released and open-sourced.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šç”¨å¥–åŠ±å»ºæ¨¡çš„æ¨ç†æ—¶å¯æ‰©å±•æ€§æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡å‹è®­ç»ƒåé˜¶æ®µå·²è¢«å¹¿æ³›åº”ç”¨ä»¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨é€šç”¨é¢†åŸŸè·å–å‡†ç¡®å¥–åŠ±ä¿¡å·ä»é¢ä¸´æŒ‘æˆ˜ã€‚å½“å‰å¥–åŠ±å»ºæ¨¡ï¼ˆRMï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„é«˜è´¨é‡å¥–åŠ±å¤šä¾èµ–äººå·¥è®¾è®¡ç¯å¢ƒæˆ–è§„åˆ™ï¼Œé€šç”¨é¢†åŸŸå¥–åŠ±ç”Ÿæˆå› æ ‡å‡†å¤šæ ·å¤æ‚ä¸”ç¼ºä¹æ˜ç¡®å‚è€ƒæ›´å…·éš¾åº¦ã€‚åŒæ—¶ï¼Œç°æœ‰ç ”ç©¶åœ¨è®©å¥–åŠ±æ¨¡å‹å…¼å…·é€šç”¨æ€§ä¸æ¨ç†æ—¶æœ‰æ•ˆå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼šå¥–åŠ±ç”ŸæˆèŒƒå¼åœ¨è¾“å…¥çµæ´»æ€§å’Œæ¨ç†æ—¶å¯æ‰©å±•æ€§ä¸Šæœ‰å±€é™ï¼Œå­¦ä¹ æ–¹æ³•ä¹Ÿè¾ƒå°‘å…³æ³¨æ¨ç†æ—¶å¯æ‰©å±•æ€§åŠå¥–åŠ±ç”Ÿæˆè¡Œä¸ºä¸å¯æ‰©å±•æ€§çš„å…³è”ã€‚å› æ­¤ï¼Œæœ¬æ–‡èšç„¦å¦‚ä½•æå‡é€šç”¨å¥–åŠ±å»ºæ¨¡åœ¨æ¨ç†æ—¶çš„å¯æ‰©å±•æ€§ï¼Œå¹¶æ¢ç´¢åˆé€‚å­¦ä¹ æ–¹æ³•å¢å¼ºæ€§èƒ½ - è®¡ç®—ç¼©æ”¾æœ‰æ•ˆæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡‡ç”¨é€ç‚¹ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡ï¼ˆGRMï¼‰  
é€‰æ‹©é€ç‚¹ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡ï¼ˆGRMï¼‰ä½œä¸ºå¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œå…¶èƒ½åœ¨çº¯è¯­è¨€è¡¨ç¤ºå†…ç»Ÿä¸€å•æ¡ã€æˆå¯¹åŠå¤šæ¡å“åº”çš„è¯„åˆ†ï¼Œä¸ºä¸åŒè¾“å…¥ç±»å‹æä¾›çµæ´»æ€§ï¼Œä¹Ÿä¸ºæ¨ç†æ—¶å¯æ‰©å±•æ€§æä¾›æ½œåŠ›ï¼Œå…‹æœäº†é€šç”¨å¥–åŠ±å»ºæ¨¡å¯¹ä¸åŒè¾“å…¥ç±»å‹é€‚åº”æ€§çš„æŒ‘æˆ˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºè‡ªåŸåˆ™æ€§æ‰¹åˆ¤è°ƒä¼˜ï¼ˆSPCTï¼‰  
æå‡ºSelf - Principled Critique Tuningï¼ˆSPCTï¼‰å­¦ä¹ æ–¹æ³•ï¼Œå€ŸåŠ©åŸºäºè§„åˆ™çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä¿ƒä½¿GRMså­¦ä¹ æ ¹æ®è¾“å…¥æŸ¥è¯¢å’Œå“åº”è‡ªé€‚åº”è®¾å®šåŸåˆ™ä¸æ‰¹åˆ¤ï¼Œåœ¨é€šç”¨é¢†åŸŸç”Ÿæˆæ›´ä¼˜å¥–åŠ±ç»“æœï¼Œæå‡å¥–åŠ±ç”Ÿæˆè´¨é‡ï¼Œè§£å†³é€šç”¨é¢†åŸŸå‡†ç¡®å¥–åŠ±ç”Ÿæˆéš¾é¢˜ã€‚åŸºäºæ­¤æ–¹æ³•è®­ç»ƒå¾—åˆ°DeepSeek - GRM - 27Bæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥Gemma - 2 - 27Bä¸ºåŸºç¡€è¿›è¡Œè®­ç»ƒåä¼˜åŒ–ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨ç†æ—¶å¯æ‰©å±•æ€§ä¼˜åŒ–æ‰‹æ®µ  
ä¸ºå®ç°æœ‰æ•ˆæ¨ç†æ—¶å¯æ‰©å±•æ€§ï¼Œé‡‡ç”¨å¹¶è¡Œé‡‡æ ·æ‰©å¤§è®¡ç®—èµ„æºä½¿ç”¨ï¼ŒDeepSeek - GRMå¯ç”Ÿæˆä¸åŒåŸåˆ™å’Œå¯¹åº”æ‰¹åˆ¤é›†åˆåæŠ•ç¥¨ç¡®å®šæœ€ç»ˆå¥–åŠ±ï¼›ç»“åˆæ›´å¤§è§„æ¨¡é‡‡æ ·ï¼Œèƒ½åŸºäºæ›´å…·å¤šæ ·æ€§çš„åŸåˆ™æ›´å‡†ç¡®åˆ¤æ–­ï¼Œè¾“å‡ºæ›´ç»†ç²’åº¦å¥–åŠ±ã€‚æ­¤å¤–ï¼Œè®­ç»ƒå…ƒå¥–åŠ±æ¨¡å‹ï¼ˆmeta RMï¼‰è¾…åŠ©æŠ•ç¥¨è¿‡ç¨‹ä»¥æå‡ç¼©æ”¾æ€§èƒ½ï¼Œè§£å†³æ¨ç†æ—¶éšè®¡ç®—å¢åŠ ç”Ÿæˆæ›´é«˜è´¨é‡å¥–åŠ±ä¿¡å·åŠå­¦ä¹ å¯æ‰©å±•è¡Œä¸ºçš„æŒ‘æˆ˜ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜SPCTæ˜¾è‘—æå‡äº†GRMsçš„è´¨é‡ä¸å¯æ‰©å±•æ€§ï¼Œåœ¨å¤šä¸ªç»¼åˆå¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•å’Œæ¨¡å‹ä¸”æ— ä¸¥é‡é¢†åŸŸåå·®ï¼›åŒæ—¶ï¼Œä¸è®­ç»ƒæ—¶ç¼©æ”¾ç›¸æ¯”èƒ½å–å¾—æ›´ä¼˜æ€§èƒ½ã€‚ä¸è¿‡DeepSeek - GRMåœ¨éƒ¨åˆ†ä»»åŠ¡ä¸­ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½œè€…è®¤ä¸ºæœªæ¥é€šç”¨å¥–åŠ±ç³»ç»Ÿçš„ç ”ç©¶å¯è§£å†³è¿™äº›é—®é¢˜ï¼Œä¸”æ¨¡å‹å°†å¼€æºå‘å¸ƒã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼é€‰æ‹©è§’åº¦ï¼šå±•ç¤ºäº†é€ç‚¹ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡åœ¨é€šç”¨å¥–åŠ±å»ºæ¨¡ä¸­å…¼é¡¾è¾“å…¥çµæ´»æ€§ä¸æ¨ç†æ—¶å¯æ‰©å±•æ€§çš„ä¼˜åŠ¿ï¼Œä¸ºåç»­å¥–åŠ±å»ºæ¨¡èŒƒå¼é€‰æ‹©æä¾›å‚è€ƒï¼Œå½“éœ€å¤„ç†å¤šæ ·è¾“å…¥ç±»å‹å¹¶è€ƒè™‘æ¨ç†æ—¶æ€§èƒ½æå‡æ—¶ï¼Œå¯è€ƒè™‘æ­¤ç±»ç”Ÿæˆå¼èŒƒå¼ã€‚  
2. å­¦ä¹ æ–¹æ³•è§’åº¦ï¼šSPCTå€ŸåŠ©åœ¨çº¿RLè®©æ¨¡å‹è‡ªé€‚åº”ç”ŸæˆåŸåˆ™å’Œæ‰¹åˆ¤çš„æ€è·¯ï¼Œä¸ºæå‡é€šç”¨é¢†åŸŸå¥–åŠ±ç”Ÿæˆè´¨é‡æä¾›äº†æ–°çš„å­¦ä¹ æ–¹æ³•èŒƒå¼ï¼Œåç»­å¯å€Ÿé‰´è¯¥æ€è·¯æ¢ç´¢å¦‚ä½•è®©æ¨¡å‹åœ¨æ— æ˜ç¡®è§„åˆ™é¢†åŸŸè‡ªä¸»å­¦ä¹ æœ‰æ•ˆè¯„åˆ¤é€»è¾‘ã€‚  
3. æ¨ç†æ—¶ä¼˜åŒ–è§’åº¦ï¼šå¹¶è¡Œé‡‡æ ·ç»“åˆå…ƒå¥–åŠ±æ¨¡å‹è¾…åŠ©æŠ•ç¥¨çš„æ¨ç†æ—¶å¯æ‰©å±•æ€§ä¼˜åŒ–æ‰‹æ®µï¼Œä¸ºåˆ©ç”¨è®¡ç®—èµ„æºæå‡æ¨ç†æ—¶æ¨¡å‹æ€§èƒ½æä¾›äº†å®è·µè·¯å¾„ï¼Œåœ¨éœ€é€šè¿‡å¢åŠ æ¨ç†æ—¶è®¡ç®—æå‡æ•ˆæœçš„åœºæ™¯ï¼ˆå¦‚å¤æ‚æŸ¥è¯¢å¥–åŠ±è¯„ä¼°ï¼‰ä¸­å¯å‚è€ƒè¯¥æ–¹å¼ã€‚

