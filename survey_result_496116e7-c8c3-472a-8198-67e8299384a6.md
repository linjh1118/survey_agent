# Paper List of Terms(reward model+vlm)
- [25/06] **DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models**  
[[Paper](http://arxiv.org/pdf/2506.03517v1)] [[Code/Page]()] [[TLDR/Notes](#densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models)]

- [25/06] **Aligning VLM Assistants with Personalized Situated Cognition**  
[[Paper](http://arxiv.org/pdf/2506.00930v1)] [[Code/Page](https://github.com/NLPGM/PCogAlign.)] [[TLDR/Notes](#aligning-vlm-assistants-with-personalized-situated-cognition)]

- [25/05] **DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving**  
[[Paper](http://arxiv.org/pdf/2505.20665v1)] [[Code/Page]()] [[TLDR/Notes](#driverx--a-vision-language-reasoning-model-for-cross-task-autonomous-driving)]

- [25/05] **Enhance Mobile Agents Thinking Process Via Iterative Preference Learning**  
[[Paper](http://arxiv.org/pdf/2505.12299v2)] [[Code/Page]()] [[TLDR/Notes](#enhance-mobile-agents-thinking-process-via-iterative-preference-learning)]

- [25/05] **Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.07263v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-vl-reward--an-effective-reward-model-for-multimodal-understanding-and-reasoning)]

- [25/05] **TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations**  
[[Paper](http://arxiv.org/pdf/2505.06079v1)] [[Code/Page](https://shuaiyihuang.github.io/publications/TREND.)] [[TLDR/Notes](#trend--tri-teaching-for-robust-preference-based-reinforcement-learning-with-demonstrations)]

- [25/04] **PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations**  
[[Paper](http://arxiv.org/pdf/2504.20520v1)] [[Code/Page]()] [[TLDR/Notes](#prism--projection-based-reward-integration-for-scene-aware-real-to-sim-to-real-transfer-with-few-demonstrations)]

- [25/04] **Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation**  
[[Paper](http://arxiv.org/pdf/2504.16073v1)] [[Code/Page]()] [[TLDR/Notes](#guiding-vlm-agents-with-process-rewards-at-inference-time-for-gui-navigation)]

- [25/04] **Seedream 3.0 Technical Report**  
[[Paper](http://arxiv.org/pdf/2504.11346v3)] [[Code/Page]()] [[TLDR/Notes](#seedream-3-0-technical-report)]

- [25/03] **VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences**  
[[Paper](http://arxiv.org/pdf/2503.13817v1)] [[Code/Page]()] [[TLDR/Notes](#varp--reinforcement-learning-from-vision-language-model-feedback-with-agent-regularized-preferences)]

- [25/02] **Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models**  
[[Paper](http://arxiv.org/pdf/2502.14191v1)] [[Code/Page](https://github.com/facebookresearch/multimodal_rewardbench.)] [[TLDR/Notes](#multimodal-rewardbench--holistic-evaluation-of-reward-models-for-vision-language-models)]

- [25/02] **Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data**  
[[Paper](http://arxiv.org/pdf/2502.14044v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-cognition-and-explainability-of-multimodal-foundation-models-with-self-synthesized-data)]

- [25/02] **Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization**  
[[Paper](http://arxiv.org/pdf/2502.01051v3)] [[Code/Page](https://github.com/Kwai-Kolors/LPO.)] [[TLDR/Notes](#diffusion-model-as-a-noise-aware-latent-reward-model-for-step-level-preference-optimization)]

- [24/12] **OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis**  
[[Paper](http://arxiv.org/pdf/2412.19723v3)] [[Code/Page](https://qiushisun.github.io/OS-Genesis-Home/.)] [[TLDR/Notes](#os-genesis--automating-gui-agent-trajectory-construction-via-reverse-task-synthesis)]

- [24/12] **CLIP-RLDrive: Human-Aligned Autonomous Driving via CLIP-Based Reward Shaping in Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2412.16201v1)] [[Code/Page]()] [[TLDR/Notes](#clip-rldrive--human-aligned-autonomous-driving-via-clip-based-reward-shaping-in-reinforcement-learning)]

- [24/12] **AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?**  
[[Paper](http://arxiv.org/pdf/2412.03002v3)] [[Code/Page]()] [[TLDR/Notes](#advdreamer-unveils--are-vision-language-models-truly-ready-for-real-world-3d-variations-)]

- [24/11] **ViSTa Dataset: Do vision-language models understand sequential tasks?**  
[[Paper](http://arxiv.org/pdf/2411.13211v2)] [[Code/Page]()] [[TLDR/Notes](#vista-dataset--do-vision-language-models-understand-sequential-tasks-)]

- [24/10] **Gen-Drive: Enhancing Diffusion Generative Driving Policies with Reward Modeling and Reinforcement Learning Fine-tuning**  
[[Paper](http://arxiv.org/pdf/2410.05582v1)] [[Code/Page](https://mczhi.github.io/GenDrive.)] [[TLDR/Notes](#gen-drive--enhancing-diffusion-generative-driving-policies-with-reward-modeling-and-reinforcement-learning-fine-tuning)]

- [24/05] **Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding**  
[[Paper](http://arxiv.org/pdf/2405.19567v2)] [[Code/Page]()] [[TLDR/Notes](#dr-llava--visual-instruction-tuning-with-symbolic-clinical-grounding)]

- [24/03] **Parameter Efficient Reinforcement Learning from Human Feedback**  
[[Paper](http://arxiv.org/pdf/2403.10704v2)] [[Code/Page]()] [[TLDR/Notes](#parameter-efficient-reinforcement-learning-from-human-feedback)]

- [24/02] **Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback**  
[[Paper](http://arxiv.org/pdf/2402.03746v3)] [[Code/Page]()] [[TLDR/Notes](#tuning-large-multimodal-models-for-videos-using-reinforcement-learning-from-ai-feedback)]

- [23/10] **Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2310.12921v2)] [[Code/Page](https://sites.google.com/view/vlm-rm.)] [[TLDR/Notes](#vision-language-models-are-zero-shot-reward-models-for-reinforcement-learning)]

- [23/05] **Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2305.18010v2)] [[Code/Page](https://github.com/mzhaoshuai/RLCF.)] [[TLDR/Notes](#test-time-adaptation-with-clip-reward-for-zero-shot-generalization-in-vision-language-models)]



# TLDR/Notes
## densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models
### Abstract
Direct Preference Optimization (DPO) has recently been applied as a
post-training technique for text-to-video diffusion models. To obtain training
data, annotators are asked to provide preferences between two videos generated
from independent noise. However, this approach prohibits fine-grained
comparisons, and we point out that it biases the annotators towards low-motion
clips as they often contain fewer visual artifacts. In this work, we introduce
DenseDPO, a method that addresses these shortcomings by making three
contributions. First, we create each video pair for DPO by denoising corrupted
copies of a ground truth video. This results in aligned pairs with similar
motion structures while differing in local details, effectively neutralizing
the motion bias. Second, we leverage the resulting temporal alignment to label
preferences on short segments rather than entire clips, yielding a denser and
more precise learning signal. With only one-third of the labeled data, DenseDPO
greatly improves motion generation over vanilla DPO, while matching it in text
alignment, visual quality, and temporal consistency. Finally, we show that
DenseDPO unlocks automatic preference annotation using off-the-shelf Vision
Language Models (VLMs): GPT accurately predicts segment-level preferences
similar to task-specifically fine-tuned video reward models, and DenseDPO
trained on these labels achieves performance close to using human labels.
### 🌟 论文解读 | DenseDPO：为视频扩散模型带来细粒度时序偏好优化

### 📌 背景痛点/本文动机
在文本到视频生成领域，扩散模型取得了不少进展，但现有视频生成器在时间一致性、视觉保真度和文本对齐等方面仍有不足。Direct Preference Optimization（DPO）技术被用于文本到视频扩散模型的后训练，但传统DPO方法存在缺陷：训练数据通过让标注者对独立噪声生成的两个视频做偏好选择得到，这种方式无法进行细粒度比较，还会使标注者偏向低运动片段（因为这类片段视觉瑕疵少），导致模型更倾向生成慢动作内容，抑制了生成动态丰富视频的能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：改进DPO视频对构建方式  
不再用独立噪声生成视频对，而是对真实视频的损坏副本去噪来创建DPO的视频对。这样得到的视频对具有相似运动结构但局部细节不同，能有效消除运动偏差，让视频对在高层语义和运动轨迹上保持一致，减少虚假相关性。  

💡 创新点2：细粒度时序偏好标注  
借助上述构建方式带来的时间对齐性，在短片段而非整个视频 clip 上标注偏好，产生更密集、精确的学习信号。仅用三分之一标注数据时，DenseDPO 在运动生成上远超 vanilla DPO，同时在文本对齐、视觉质量和时间一致性上能与之持平，数据效率更高。  

💡 创新点3：解锁基于通用视觉语言模型的自动偏好标注  
证明DenseDPO 可利用现成视觉语言模型（VLMs）实现自动偏好标注。比如GPT能精准预测片段级偏好，效果接近为特定任务微调的视频奖励模型；用这些自动标注训练DenseDPO，性能接近使用人类标注的情况。  

### 📈 实验结果
文中未详细展开实验数据表格等，但从论述可知：DenseDPO 在运动生成能力上大幅超越 vanilla DPO；在文本对齐、视觉质量、时间一致性等指标上能匹配 vanilla DPO；利用通用VLMs自动标注训练时，性能接近人类标注训练的效果，且数据效率更高（仅需1/3标注数据就能实现更好运动生成）。  

### 💬 可借鉴之处
1. 数据构建思路：在需要控制变量减少偏差的场景下，可参考“基于真实样本的变体生成”思路来构造对比数据，避免独立生成带来的偏差问题。  
2. 细粒度监督：对于有时间、空间等维度的生成任务（如视频、长文本等），可考虑将整体监督拆解为细粒度的局部监督，提升学习信号质量。  
3. 自动标注探索：展示了通用大模型在特定任务自动标注上的潜力，为减少人工标注成本、拓展数据规模提供了参考方向，后续可探索更多领域下通用模型辅助标注的可能性。

## aligning-vlm-assistants-with-personalized-situated-cognition
### Abstract
Vision-language models (VLMs) aligned with general human objectives, such as
being harmless and hallucination-free, have become valuable assistants of
humans in managing visual tasks. However, people with diversified backgrounds
have different cognition even in the same situation. Consequently, they may
have personalized expectations for VLM assistants. This highlights the urgent
need to align VLM assistants with personalized situated cognition for
real-world assistance. To study this problem, we first simplify it by
characterizing individuals based on the sociological concept of Role-Set. Then,
we propose to evaluate the individuals' actions to examine whether the
personalized alignment is achieved. Further, we construct a benchmark named
PCogAlignBench, which includes 18k instances and 20 individuals with different
Role-Sets. Finally, we present a framework called PCogAlign, which constructs a
cognition-aware and action-based reward model for personalized alignment.
Experimental results and human evaluations demonstrate the reliability of the
PCogAlignBench and the effectiveness of our proposed PCogAlign. We will
open-source the constructed benchmark and code at
https://github.com/NLPGM/PCogAlign.
### 🌟 论文解读 | 让视觉语言模型助手适配个性化情境认知：PCogAlign的探索

### 📌 背景痛点/本文动机
近年来，视觉语言模型（VLMs）在通用人类目标对齐（如无害、无幻觉）方面取得进展，成为视觉任务的得力助手。但现实中，不同背景的人即便面对同一情境，认知和对VLM助手的期望也存在个性化差异（比如面对“损坏的秋千”，孩子需要安抚引导、修理工需要专业维修建议）。现有VLM助手“一刀切”的响应方式无法满足这种个性化需求，因此**让VLM助手与个性化情境认知对齐**成为现实辅助场景的迫切需求。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于“角色集（Role - Set）”简化个体表征  
受社会学角色理论启发，将个体抽象为“Role@Location”的集合（如“Child@Home, Member@Community”）。不同Role - Set对应不同的个性化情境认知与响应期望，把复杂的人类多样性简化为可操作的角色组合，为后续研究锚定了基础。  

💡 创新点2：构建PCogAlignBench基准测试集  
打造包含1.8万实例、覆盖20种不同Role - Set的基准。具体来说，选定8个社交场景，为每个场景定义3 - 5种角色，再通过组合约束得到20种Role - Set；自动采集1.2万训练样本与6千测试样本，测试样本经人工标注确保质量，每个样本包含Role - Set、图像、问题，测试样本还附带“预期个性化响应特征”的“oracle guidance”，为评估个性化对齐提供了标准数据集。  

💡 创新点3：提出PCogAlign框架实现个性化对齐  
该框架分三步：① 估计个体的情境认知与最优行动；② 借助协作智能体生成多个个性化响应候选；③ 构建“认知感知 + 行动导向”的奖励模型，迭代筛选最优响应，让响应既贴合个性化情境认知，又辅助个体采取最优行动，还能用于对齐训练。  

### 📈 实验结果
实验结果与人工评估均验证了PCogAlignBench的可靠性（测试样本质量、场景覆盖合理性等），以及PCogAlign框架的有效性（在多设置下对比基线方法表现更优，能更好满足不同Role - Set的个性化期望）。  

### 💬 可借鉴之处
1. **问题定义创新**：将社会学概念引入AI模型对齐任务，为“个性化”这一抽象需求找到可落地的简化表征方式（Role - Set），提供了跨学科融合解决AI问题的思路。  
2. **基准构建范式**：PCogAlignBench从场景选择、角色定义到样本采集与质量控制的全流程，为打造垂直领域/细分任务的基准测试集提供了参考模板。  
3. **框架设计逻辑**：PCogAlign“认知估计→候选生成→奖励筛选”的三步法，为解决“个性化响应生成与对齐”这类需兼顾多维度需求的任务，提供了模块化、可复现的技术路线。

## driverx--a-vision-language-reasoning-model-for-cross-task-autonomous-driving
### Abstract
Autonomous driving requires real-time, robust reasoning across perception,
prediction, planning, and behavior. However, conventional end-to-end models
fail to generalize in complex scenarios due to the lack of structured
reasoning. Recent vision-language models (VLMs) have been applied to driving
tasks, but they typically rely on isolated modules and static supervision,
limiting their ability to support multi-stage decision-making. We present
AutoDriveRL, a unified training framework that formulates autonomous driving as
a structured reasoning process over four core tasks. Each task is independently
modeled as a vision-language question-answering problem and optimized using
task-specific reward models, enabling fine-grained reinforcement signals at
different reasoning stages. Within this framework, we train DriveRX, a
cross-task reasoning VLM designed for real-time decision-making. DriveRX
achieves strong performance on a public benchmark, outperforming GPT-4o in
behavior reasoning and demonstrating robustness under complex or corrupted
driving conditions. Our analysis further highlights the impact of vision
encoder design and reward-guided reasoning compression. We will release the
AutoDriveRL framework and the DriveRX model to support future research.
### 🌟 论文解读 | DriveRX：面向跨任务自动驾驶的视觉-语言推理模型

### 📌 背景痛点/本文动机
自动驾驶需在感知、预测、规划和行为等方面进行实时且鲁棒的推理，但传统端到端模型因缺乏结构化推理，在复杂场景下泛化能力不足。虽已有视觉 - 语言模型（VLMs）应用于驾驶任务，却常依赖孤立模块与静态监督，限制了多阶段决策能力。同时，现有VLMs应用于自动驾驶还面临中间推理链缺失、任务间孤立、依赖静态提示无自适应反馈等挑战，而大语言模型（LLMs）的一些方法为解决这些问题提供了思路，如链式思维（CoT）生成中间推理步骤、强化学习（RL）实现动态反馈等，这促使研究人员开发统一的视觉 - 语言框架来支持自动驾驶核心任务的结构化推理、强化学习和多任务协调。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出AutoDriveRL统一训练框架  
将自动驾驶构建为跨越感知、预测、规划、行为四个核心任务的结构化推理过程，每个任务独立建模为视觉 - 语言问答（VQA）问题。同时引入任务特定奖励模型，在训练时提供细粒度的阶段级强化信号，从全局视角实现推理步骤的过程级反馈，支持可解释的中间推理、促进跨任务优化，提升模型在自动驾驶决策中的稳定性与整体性能。

💡 创新点2：训练DriveRX跨任务推理模型  
在AutoDriveRL框架下训练DriveRX，该模型聚焦自动驾驶中的推理与跨任务泛化。它能在感知、预测、规划和行为等多任务上表现出色，在分布内和分布外（OOD）基准设置中都能超越现有模型，且在复杂驾驶条件下输出更紧凑推理轨迹与更快响应，契合自动驾驶实时性要求。

### 📈 实验结果
在公开基准DriveBench上，DriveRX在行为任务中得分为62.02，超过GPT - 4o的55.04；在DriveBench - Corruption子集上，DriveRX达到55.24，也优于GPT - 4o的53.97。同时，DriveRX在雨、遮挡、光照变化等具有挑战性的条件下能保持稳定输出，展现出强鲁棒性与泛化性。

### 💬 可借鉴之处
1. 框架设计层面：AutoDriveRL将复杂自动驾驶任务分解为核心子任务并结合强化学习与任务特定奖励模型的思路，为多任务场景下的模型训练提供了结构化监督与反馈的范例，可借鉴到其他需多阶段决策、跨任务协调的领域。
2. 模型优化层面：DriveRX在追求实时性与推理紧凑性上的设计，对于实时性要求高的AI应用（如智能机器人交互等）有参考价值，提示在模型开发中需关注推理效率与输出简洁性。
3. 分析层面：论文对训练动态、视觉编码器瓶颈、奖励引导的推理压缩等方面的深入分析，为后续模型改进与优化指明了方向，让研究者明白在模型迭代时需关注哪些关键因素来提升性能。

## enhance-mobile-agents-thinking-process-via-iterative-preference-learning
### Abstract
The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to
improve the reasoning performance of VLM-based mobile agents in GUI tasks.
However, the scarcity of diverse CoaT trajectories limits the expressiveness
and generalization ability of such agents. While self-training is commonly
employed to address data scarcity, existing approaches either overlook the
correctness of intermediate reasoning steps or depend on expensive
process-level annotations to construct process reward models (PRM). To address
the above problems, we propose an Iterative Preference Learning (IPL) that
constructs a CoaT-tree through interative sampling, scores leaf nodes using
rule-based reward, and backpropagates feedback to derive Thinking-level Direct
Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up
supervised fine-tuning, we further introduce a three-stage instruction
evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real
mobile UI screenshots, enhancing both generality and layout understanding.
Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our
agent MobileIPL outperforms strong baselines, including continual pretraining
models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance
across three standard Mobile GUI-Agents benchmarks and shows strong
generalization to out-of-domain scenarios.
### 🌟 论文解读 | 迭代偏好学习助力移动智能体思维升级

### 📌 背景痛点/本文动机
基于视觉语言模型（VLM）的移动智能体在图形用户界面（GUI）任务中展现出与界面交互、自主执行日常任务的潜力，Chain of Action - Planning Thoughts（CoaT）范式能提升这类智能体推理性能，但多样CoaT轨迹稀缺限制了智能体表达与泛化能力。现有自训练方法要么忽视中间推理步骤正确性，要么依赖昂贵过程级标注构建过程奖励模型（PRM）；且直接在CoaT轨迹上做监督微调易过拟合。为解决这些问题，论文提出新方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：迭代偏好学习（IPL）框架
构建CoaT - tree：基于蒙特卡洛树搜索（MCTS）进行交互式采样构建CoaT - tree，树中每个节点对应推理步骤的采样响应，捕捉多样推理路径。奖励分配与反向传播：用基于规则的奖励给叶节点打分，再将奖励信号沿CoaT - tree反向传播到更早推理步骤，基于此构建Thinking - level Direct Preference Optimization（T - DPO）对，优化最终动作与整体推理质量，既关注中间步骤又无需昂贵标注。
💡 创新点2：三阶段指令演化策略
利用GPT - 4o基于真实移动UI截图生成多样问答对，一是引入多样推理上下文防止智能体在热身监督微调时过拟合到静态下游指令；二是通过视觉 grounded 问答提升智能体对UI布局的理解，增强泛化性。

### 📈 实验结果
在三个标准移动GUI智能体基准测试（AITZ、AMEX、AndroidControl）上评估，所提MobileIPL智能体超越强基线（如OS - ATLAS、UI - TARS等持续预训练模型），在三个基准上达当前最优性能；在AndroidControl数据集上展现对未见过应用和指令的强泛化能力；有限训练资源下，IPL比naive - DPO表现好，一轮迭代训练用一半数据时性能高4.5%，两轮迭代用五分之一数据时高0.3%；分析实验显示指令演化同时提升推理多样性与质量。

### 💬 可借鉴之处
1. 处理数据稀缺与中间步骤关注问题时，IPL的CoaT - tree构建、基于规则奖励反向传播形成T - DPO对的思路，为模型推理优化提供了不依赖昂贵标注且关注中间过程的新范式，可借鉴到需中间推理步骤优化的任务场景。
2. 指令演化策略利用外部强大模型生成多样数据来防止过拟合、增强泛化与特定领域理解，在模型微调阶段数据增强、泛化能力提升方面提供了参考，可用于其他需应对过拟合与领域理解的模型训练任务。
3. 在多基准测试上验证方法有效性，且对比多种强基线，这种全面的实验验证思路值得科研中借鉴，以充分说明方法优势。

## skywork-vl-reward--an-effective-reward-model-for-multimodal-understanding-and-reasoning
### Abstract
We propose Skywork-VL Reward, a multimodal reward model that provides reward
signals for both multimodal understanding and reasoning tasks. Our technical
approach comprises two key components: First, we construct a large-scale
multimodal preference dataset that covers a wide range of tasks and scenarios,
with responses collected from both standard vision-language models (VLMs) and
advanced VLM reasoners. Second, we design a reward model architecture based on
Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage
fine-tuning using pairwise ranking loss on pairwise preference data.
Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art
results on multimodal VL-RewardBench and exhibits competitive performance on
the text-only RewardBench benchmark. Furthermore, preference data constructed
based on our Skywork-VL Reward proves highly effective for training Mixed
Preference Optimization (MPO), leading to significant improvements in
multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as
a significant advancement toward general-purpose, reliable reward models for
multimodal alignment. Our model has been publicly released to promote
transparency and reproducibility.
### 🌟 论文解读 | Skywork-VL Reward：多模态理解与推理的高效奖励模型

### 📌 背景痛点/本文动机
大语言模型（LLMs）和视觉语言模型（VLMs）虽取得显著进展，但让其行为与人类偏好对齐仍是一大挑战。奖励模型（RMs）是解决该问题的关键组件，然而多模态奖励模型的发展尚处早期，存在两大局限：现有多模态奖励模型在不同任务中泛化性不足，且难以有效评估具备复杂推理能力的先进VLM推理器。因此，迫切需要能在多样领域和任务中评估标准VLMs与先进VLM推理器输出的多模态奖励模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建大规模多模态偏好数据集  
整合多个开源偏好数据集与内部标注来构建训练数据集，涵盖LLaVA-Critic-113k、Skywork-Reward-Preference-80K-v0.2、RLAIF-V-Dataset等来源。这些数据覆盖从基础图像描述到复杂推理场景的任务，候选响应来自标准VLMs和先进VLM推理器，为模型训练提供丰富多样的偏好对比数据。  

💡 创新点2：设计基于Qwen2.5-VL-7B-Instruct的奖励模型架构与训练范式  
以Qwen2.5-VL-7B-Instruct为基础模型，集成奖励头，采用两阶段训练范式，在成对偏好数据上使用成对排序损失进行多阶段微调。结合纯文本和多模态数据训练，增强模型在多模态场景下的泛化性与性能，使模型能输出与人类偏好一致的标量分数，有效评估多模态模型输出。  

### 📈 实验结果
Skywork-VL Reward在多模态VL-RewardBench基准测试中取得当前最优（SOTA）结果；在纯文本的RewardBench基准测试中也展现出有竞争力的性能。此外，基于Skywork-VL Reward构建的偏好数据用于训练混合偏好优化（MPO）时效果显著，能大幅提升多模态推理能力，充分证明该奖励模型在多模态对齐方向迈向通用、可靠奖励模型的重大进展。  

### 💬 可借鉴之处
在数据集构建上，通过整合多来源数据（开源+内部标注）、覆盖多任务场景来打造高质量训练数据，为模型性能奠基；模型架构与训练上，基于强基础模型设计奖励头并采用多阶段微调、结合多类型数据训练的范式，提升模型泛化与评估能力；在应用拓展上，展示了奖励模型生成的偏好数据对MPO训练的实用价值，为多模态模型推理能力提升提供新思路，且模型开源促进了领域透明性与可复现性。

## trend--tri-teaching-for-robust-preference-based-reinforcement-learning-with-demonstrations
### Abstract
Preference feedback collected by human or VLM annotators is often noisy,
presenting a significant challenge for preference-based reinforcement learning
that relies on accurate preference labels. To address this challenge, we
propose TREND, a novel framework that integrates few-shot expert demonstrations
with a tri-teaching strategy for effective noise mitigation. Our method trains
three reward models simultaneously, where each model views its small-loss
preference pairs as useful knowledge and teaches such useful pairs to its peer
network for updating the parameters. Remarkably, our approach requires as few
as one to three expert demonstrations to achieve high performance. We evaluate
TREND on various robotic manipulation tasks, achieving up to 90% success rates
even with noise levels as high as 40%, highlighting its effective robustness in
handling noisy preference feedback. Project page:
https://shuaiyihuang.github.io/publications/TREND.
### 🌟 论文解读 | TREND：小样本专家示范+三模型互教，让基于偏好的强化学习更鲁棒

### 📌 背景痛点/本文动机
在强化学习（RL）中，设计合适的奖励函数是一大挑战。基于偏好的强化学习（PbRL）通过人类或视觉语言模型（VLM）提供的偏好反馈来替代显式奖励函数，虽有优势但偏好标签易受噪声干扰（比如人类反馈有偏差、VLM难处理复杂视觉/文本/时序信息）。已有研究表明哪怕10%的标签噪声都会严重影响性能，而VLM生成的偏好标签噪声率甚至能高达40%，这对依赖准确偏好标签的PbRL来说是巨大挑战，因此亟需鲁棒的方法来处理带噪偏好反馈。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出TREND框架，融合小样本专家示范与三教（tri - teaching）策略  
TREND一方面在预训练和在线PbRL适配阶段融入少量（1 - 3个）专家示范，为策略提供强初始化，保证在线学习时至少部分数据无噪声；另一方面设计三教策略，让三个奖励模型协同工作。每个模型将自身小损失的偏好对视为“干净知识”，并把这些有用对传递给其他模型用于参数更新，以此减轻噪声影响。

💡 创新点2：三教策略实现动态清洁样本选择  
不同于现有简单平均多奖励模型预测来选样本的方法，TREND中三个奖励模型各自独立学习样本选择的“专长”。每个模型通过计算偏好对的损失，把小损失的偏好对标记为可能干净的样本，再用这些样本更新其他模型。这样清洁样本的定义由模型间动态学习而来，而非固定规则，增强了对噪声的鲁棒性。

### 📈 实验结果
在Meta - world的机器人操作任务（如Button - Press、Drawer - Open、Hammer等）上评估TREND：  
- 即便在40%的高噪声水平下，TREND仍能取得优异成绩，成功率先在部分任务上提升明显（如Button - Press任务较基线提升近40%、Drawer - Open提升60%、Hammer提升70%左右）；  
- 面对VLM生成的带噪偏好标签时，在Drawer - Open任务上成功律提升超40%；  
- 仅用1 - 3个专家示范就能实现高性能，在40%高噪声下也能达到约80%的成功率。整体上在不同噪声水平下，TREND持续大幅超越现有PbRL基线方法。

### 💬 可借鉴之处
1. 多模型协同与动态清洁样本选择思路：当面临带噪数据场景时，可参考这种让多个模型互相“教学”、动态判断有效样本的方式，避免单一模型对噪声估计的偏差，提升鲁棒性；  
2. 小样本专家示范的融合：在数据稀缺或噪声极高时，引入少量专家示范做初始化或辅助训练，能为模型学习提供可靠锚点，这种“小样本借力”思路在其他依赖带噪反馈或数据稀缺的学习任务中也有借鉴价值；  
3. 应对VLM生成噪声的方案：在结合VLMs做偏好生成等任务时，TREND处理噪声的方式为提升这类结合方案的稳定性提供了参考，可用于优化依赖VLM反馈的RL或其他学习任务流程。

## prism--projection-based-reward-integration-for-scene-aware-real-to-sim-to-real-transfer-with-few-demonstrations
### Abstract
Learning from few demonstrations to develop policies robust to variations in
robot initial positions and object poses is a problem of significant practical
interest in robotics. Compared to imitation learning, which often struggles to
generalize from limited samples, reinforcement learning (RL) can autonomously
explore to obtain robust behaviors. Training RL agents through direct
interaction with the real world is often impractical and unsafe, while building
simulation environments requires extensive manual effort, such as designing
scenes and crafting task-specific reward functions. To address these
challenges, we propose an integrated real-to-sim-to-real pipeline that
constructs simulation environments based on expert demonstrations by
identifying scene objects from images and retrieving their corresponding 3D
models from existing libraries. We introduce a projection-based reward model
for RL policy training that is supervised by a vision-language model (VLM)
using human-guided object projection relationships as prompts, with the policy
further fine-tuned using expert demonstrations. In general, our work focuses on
the construction of simulation environments and RL-based policy training,
ultimately enabling the deployment of reliable robotic control policies in
real-world scenarios.
### 🌟 论文解读 | PRISM：少样本场景感知下虚实迁移的投影奖励集成方案

### 📌 背景痛点/本文动机
在机器人学中，从少量演示中学习出能应对机器人初始位姿和物体姿态变化的鲁棒策略是极具实际意义的问题。模仿学习难以从有限样本泛化，强化学习（RL）虽能自主探索获得鲁棒行为，但直接在真实世界训练RL智能体不切实际且不安全；构建仿真环境又需大量人工工作（如场景设计、定制任务奖励函数）。同时，现有虚实迁移方法多聚焦高保真仿真环境重建，对仿真中如何开展RL以弥合虚实差距关注不足，且依赖手工奖励函数、未充分利用少量真实数据。因此，本文提出PRISM方案来应对这些挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：设计虚实迁移 pipeline  
构建视觉和几何一致的仿真环境：利用3D模型库，基于真实场景图像构建仿真环境；并通过与专家演示共训练以及动作可行性预测器，将策略迁移到真实世界，解决了仿真环境构建与策略迁移难题，减少领域差异。  

💡 创新点2：提出基于投影的奖励模型  
把人类引导的物体投影关系（从人类视角因相互遮挡产生的物体2D投影的视点相关排序）作为提示，从视觉 - 语言基础模型（VLMs）迁移到RL中，为RL策略训练提供高效奖励监督。还利用仿真中的多视角观测验证VLM推断奖励在不同视角的一致性，减轻VLM误判影响。  

💡 创新点3：充分利用少量专家演示  
基于真实场景图像构建仿真环境后，联合基于投影的奖励模型与重放演示来进行RL策略训练，把学习到的奖励模型转化为动作可行性预测器，将预测器和策略都迁移回真实场景执行，支撑稳定策略学习。  

### 📈 实验结果
在具有不同机器人初始姿态和物体配置的六个操作任务中验证，PRISM学习到的鲁棒策略相比基线平均成功率提升68%，证明了方法在应对多样场景下任务的有效性。  

### 💬 可借鉴之处
1. 虚实迁移思路：利用真实数据构建更贴合现实的仿真环境来辅助策略学习与迁移，为解决真实世界训练RL的安全、可行性问题提供了路径参考。  
2. 奖励模型构建：借助视觉 - 语言模型结合人类引导的空间关系作为监督来源，为RL中奖励函数的自动、泛化性设计提供了新方向，减少手工设计依赖。  
3. 少量样本利用：展示了如何充分挖掘少量真实世界专家演示价值，在仿真和真实环境衔接中助力策略学习，对样本稀缺场景下的机器人学习有借鉴意义。

## guiding-vlm-agents-with-process-rewards-at-inference-time-for-gui-navigation
### Abstract
Recent advancements in visual language models (VLMs) have notably enhanced
their capabilities in handling complex Graphical User Interface (GUI)
interaction tasks. Despite these improvements, current frameworks often
struggle to generate correct actions in challenging GUI environments.
State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source
VLMs for GUI tasks requires significant resources. Additionally, existing
trajectory-level evaluation and refinement techniques frequently fall short due
to delayed feedback and local optimization issues. To address these challenges,
we propose an approach that guides VLM agents with process supervision by a
reward model during GUI navigation and control at inference time. This guidance
allows the VLM agent to optimize actions at each inference step, thereby
improving performance in both static and dynamic environments. In particular,
our method demonstrates significant performance gains in three GUI navigation
tasks, achieving a 3.4% improvement in single step action accuracy for static
environments, along with a around 33% increase in task success rate in one
dynamic environment. With further integration of trajectory reflection and
retry mechanisms, we also demonstrate even greater enhancement in task success.
### 🌟 论文解读 | 推理时用过程奖励引导VLM智能体实现GUI导航

### 📌 背景痛点/本文动机
视觉语言模型（VLMs）在处理复杂图形用户界面（GUI）交互任务上取得进展，但现有框架在具挑战性的GUI环境中生成正确动作仍有困难。商业VLMs是黑盒难以调优，开源VLMs针对GUI任务微调资源消耗大；现有轨迹级评估和优化技术因延迟反馈与局部优化问题表现不佳。同时，轨迹级评估会导致个体动作优化不足、纠错延迟，强化学习（RL）类方法训练成本高且不稳定。为解决这些问题，论文提出GuidNav方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出GuidNav，推理时用过程奖励模型引导VLM智能体  
训练一个过程奖励模型，该模型基于少量人类演示和VLM生成的合成数据训练，学习GUI数据中的反馈信号。在GUI导航推理时，用此模型引导VLM智能体，让智能体在每个推理步骤都能优化动作，在静态和动态环境中提升表现，还能实时适应环境变化、避免延迟反馈导致的错误累积。  
💡 创新点2：与轨迹级优化 pipeline 集成增强性能  
过程奖励模型可整合到结果监督流程中，进一步改进动作生成与选择；结合轨迹反思和重试机制后，能进一步提升任务成功率。  

### 📈 实验结果
在Android-in-the-Wild（AitW）的静态和动态场景评估，使GPT - 4o在静态环境动作准确率提升约5%，动态环境任务成功率提升约33%；结合轨迹反思和重试机制后成功率峰值达71.6%。在GUI Odyssey和Mind2Web基准测试也验证泛化性，GUI Odyssey上三个大语言模型（LLMs）平均单步动作准确率提升约3.2%，Mind2Web静态环境下单步动作准确率提升2.1%。  

### 💬 可借鉴之处
从方法设计角度，利用过程奖励实现细粒度优化、实时反馈的思路，可启发在其他需多步骤交互、每一步对结果影响大的任务场景（如机器人操作流程、复杂软件交互等）中改进智能体表现；从工程落地角度，展示了结合少量人类演示与合成数据训练奖励模型以降低数据成本的方式，为资源有限情况下开发辅助模型提供参考；从多模块整合角度，过程奖励模型与轨迹级机制结合提升性能的模式，为构建分层、多阶段优化的智能系统提供了借鉴方向。

## seedream-3-0-technical-report
### Abstract
We present Seedream 3.0, a high-performance Chinese-English bilingual image
generation foundation model. We develop several technical improvements to
address existing challenges in Seedream 2.0, including alignment with
complicated prompts, fine-grained typography generation, suboptimal visual
aesthetics and fidelity, and limited image resolutions. Specifically, the
advancements of Seedream 3.0 stem from improvements across the entire pipeline,
from data construction to model deployment. At the data stratum, we double the
dataset using a defect-aware training paradigm and a dual-axis collaborative
data-sampling framework. Furthermore, we adopt several effective techniques
such as mixed-resolution training, cross-modality RoPE, representation
alignment loss, and resolution-aware timestep sampling in the pre-training
phase. During the post-training stage, we utilize diversified aesthetic
captions in SFT, and a VLM-based reward model with scaling, thereby achieving
outputs that well align with human preferences. Furthermore, Seedream 3.0
pioneers a novel acceleration paradigm. By employing consistent noise
expectation and importance-aware timestep sampling, we achieve a 4 to 8 times
speedup while maintaining image quality. Seedream 3.0 demonstrates significant
improvements over Seedream 2.0: it enhances overall capabilities, in particular
for text-rendering in complicated Chinese characters which is important to
professional typography generation. In addition, it provides native
high-resolution output (up to 2K), allowing it to generate images with high
visual quality.
### 🌟 论文解读 | Seedream 3.0：多维度升级的中英双语图像生成大模型

### 📌 背景痛点/本文动机
扩散模型推动图像生成技术取得巨大进展，Seedream 2.0在中英双语文本到图像生成中是重要里程碑，但在商业应用中存在挑战：复杂提示词的对齐度需提升（如数值精度、多物体空间关系）；精细排版生成能力有限（小尺寸文字、多行文意组合等）；视觉美学与保真度欠佳（如电影场景美感、人像纹理）；图像分辨率受限（原生输出小分辨率，依赖后处理超分）。为解决这些问题，Seedream 3.0 从数据构建到模型部署全流程优化升级。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：数据层革新
采用缺陷感知训练范式与双轴协同数据采样框架。缺陷感知训练范式通过主动学习引擎选15000个手动标注样本训练缺陷检测器，精准定位缺陷区域，突破Seedream 2.0严格数据过滤导致的数据量限制，使数据集规模翻倍且质量提升；双轴协同数据采样基于图像簇分布和文本语义连贯性两个正交维度构建动态采样机制。

💡 创新点2：预训练阶段技术升级
采用混合分辨率训练、跨模态RoPE、表示对齐损失、分辨率感知时间步采样等技术。混合分辨率训练提升模型对不同分辨率图像的处理能力；跨模态RoPE增强文本与图像模态间的关联；表示对齐损失优化模态间表示一致性；分辨率感知时间步采样让训练更适配不同分辨率场景，整体提升视觉 - 语言对齐度与模型泛化性。

💡 创新点3：后训练阶段优化
在SFT（监督微调）中使用多样化美学标题，并用基于VLM（视觉语言模型）的带缩放的奖励模型。多样化美学标题为模型注入美学创作能力；VLM奖励模型结合缩放机制，让模型输出更贴合人类偏好。

💡 创新点4：模型加速新范式
采用一致噪声期望与重要性感知时间步采样。一致噪声期望保障采样稳定性，重要性感知时间步采样减少推理时函数评估次数（NFE），实现4 - 8倍加速同时保持图像质量。

### 📈 实验结果
Seedream 3.0 在多方面表现卓越：在Artificial Analysis文本到图像模型排行榜中，以17000多次出现下Arena ELO分数1158排名第一；对比Seedream 2.0等模型，在对齐度、结构、美学等全评估维度表现突出；文本渲染能力显著增强，在复杂中英文字、长文本美观排版等方面表现优异，甚至超过Canva等平台人工设计模板；图像美学质量大幅提升，电影场景、人像生成真实感更强；原生支持2K高分辨率输出，无需后处理且适配多样宽高比；推理效率高，生成1K分辨率图像仅需约3秒（无PE时），成本显著降低。

### 💬 可借鉴之处
数据层面，缺陷感知与双轴采样的思路为解决数据质量与规模平衡问题提供参考，可用于处理含“小瑕疵”但有价值的数据；模型训练阶段，混合分辨率、跨模态关联增强、表示对齐等技术，对多模态模型在不同场景下的泛化与对齐优化有借鉴意义；后训练的美学引导与奖励模型缩放，为提升模型输出符合人类审美偏好提供方法；模型加速的噪声期望与时间步采样策略，在追求推理效率同时保障质量方面给出了创新方向，这些技术点可在其他生成类大模型研发中参考复用。

## varp--reinforcement-learning-from-vision-language-model-feedback-with-agent-regularized-preferences
### Abstract
Designing reward functions for continuous-control robotics often leads to
subtle misalignments or reward hacking, especially in complex tasks.
Preference-based RL mitigates some of these pitfalls by learning rewards from
comparative feedback rather than hand-crafted signals, yet scaling human
annotations remains challenging. Recent work uses Vision-Language Models (VLMs)
to automate preference labeling, but a single final-state image generally fails
to capture the agent's full motion. In this paper, we present a two-part
solution that both improves feedback accuracy and better aligns reward learning
with the agent's policy. First, we overlay trajectory sketches on final
observations to reveal the path taken, allowing VLMs to provide more reliable
preferences-improving preference accuracy by approximately 15-20% in metaworld
tasks. Second, we regularize reward learning by incorporating the agent's
performance, ensuring that the reward model is optimized based on data
generated by the current policy; this addition boosts episode returns by 20-30%
in locomotion tasks. Empirical studies on metaworld demonstrate that our method
achieves, for instance, around 70-80% success rate in all tasks, compared to
below 50% for standard approaches. These results underscore the efficacy of
combining richer visual representations with agent-aware reward regularization.
### 🌟 论文解读 | VARP：用视觉语言模型反馈+智能体正则化偏好革新强化学习

### 📌 背景痛点/本文动机
在连续控制机器人领域，设计奖励函数时常出现细微偏差或“奖励黑客”（即智能体利用奖励设计漏洞达成非预期行为）问题，尤其是复杂任务中。基于偏好的强化学习（RL）通过从比较反馈而非手工设计信号中学习奖励，缓解了部分缺陷，但规模化的人类标注成本高昂。近期用视觉语言模型（VLM）自动生成偏好标签的工作，又因仅用单张最终状态图像难以捕捉智能体完整运动过程（比如路径效率、中间步骤流畅性等关键信息易丢失），导致反馈准确性受限；且奖励模型训练若不考虑智能体策略演变，易出现训练次优问题。因此，如何提升反馈准确性、让奖励学习与智能体策略更对齐，成为待解难题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：轨迹感知的偏好标注技术  
不再依赖单张最终状态图像与文本提示，而是在最终观测上叠加**2D轨迹草图**，以此捕捉机器人完整运动过程。丰富后的视觉表征为VLM提供关键时间维度上下文，使其能做出更可靠精准的偏好判断。在MetaWorld任务中，该方法将偏好准确率提升约15 - 20%。  

💡 创新点2：VARP框架（结合智能体感知的奖励正则化）  
提出VARP框架，平衡基于草图的外部偏好信号与智能体性能以优化奖励。因静态偏好数据会随策略演变而错位，若仅用其优化奖励模型，可能无法反映智能体当前表现。为此，把智能体性能整合到奖励学习目标中，动态惩罚低回报的奖励函数。这一智能体感知的正则化，让学习到的奖励与实时有效行为对齐，在 locomotion 任务中能提升单幕回报20 - 30%。  


### 📈 实验结果
在MetaWorld基准测试中，VARP展现出显著优势：所有任务成功率达70 - 80%左右，而标准方法成功率低于50%。实验有力证明了VARP能得到更准确的偏好模型与更稳健的策略提升，凸显了“更丰富视觉表征 + 智能体感知奖励正则化”结合的有效性。

### 💬 可借鉴之处
1. 多模态增强思路：在依赖视觉 - 语言模型的场景下，通过轨迹草图这类轻量但有效的方式补充时间维度信息，为提升模型对动态过程理解提供了新范式，可迁移到需捕捉序列/运动信息的视觉任务中。  
2. 策略 - 奖励对齐：将智能体自身性能纳入奖励学习正则化，解决静态数据与动态策略脱节问题，为强化学习中“奖励模型如何适配策略演化”提供了实用正则化方向，在机器人控制、复杂决策任务等场景有参考价值。  
3. 自动化偏好标注优化：用VLM自动处理偏好标注同时，针对性弥补单帧图像缺陷，为降低人类标注成本、提升自动化反馈质量提供了落地路径，在需大规模偏好数据的RLHF类任务中值得借鉴。  

## multimodal-rewardbench--holistic-evaluation-of-reward-models-for-vision-language-models
### Abstract
Reward models play an essential role in training vision-language models
(VLMs) by assessing output quality to enable aligning with human preferences.
Despite their importance, the research community lacks comprehensive open
benchmarks for evaluating multimodal reward models in VLMs. To address this
gap, we introduce Multimodal RewardBench, an expert-annotated benchmark
covering six domains: general correctness, preference, knowledge, reasoning,
safety, and visual question-answering. Our dataset comprises 5,211 annotated
(prompt, chosen response, rejected response) triplets collected from various
VLMs. In evaluating a range of VLM judges, we find that even the top-performing
models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall
accuracy. Notably, most models struggle in the reasoning and safety domains.
These findings suggest that Multimodal RewardBench offers a challenging testbed
for advancing reward model development across multiple domains. We release the
benchmark at https://github.com/facebookresearch/multimodal_rewardbench.
### 🌟 论文解读 | 多模态RewardBench：为视觉语言模型Reward Model提供全面评估基准

### 📌 背景痛点/本文动机
奖励模型（Reward Model）在视觉语言模型（VLM）训练中至关重要，它能评估输出质量，让模型与人类偏好对齐。然而，研究社区缺乏用于评估VLM中多模态奖励模型的全面公开基准。现有基准多局限于文本模态，针对VLM奖励模型评估的工作也多限于通用视觉问答（VQA）任务且标注非专家完成，所以需要一个全面、专家标注的基准来评估VLM的奖励模型，这就是本文工作的动机。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建Multimodal RewardBench基准  
提出Multimodal RewardBench这一专家标注的基准，覆盖通用正确性、偏好、知识、推理、安全和视觉问答这六个领域。收集了来自不同VLM的5211个标注好的（提示、选中响应、拒绝响应）三元组，这些三元组可直接用于评估奖励模型的响应排序准确率，还涵盖了“正确vs错误响应”“人类偏好vs非偏好响应（在响应都正确或都错误情况下）”两种情况以支持更细粒度评估。

💡 创新点2：全面分析VLM法官性能  
对多种VLM法官（包括专有模型如GPT - 4o、Claude、Gemini，开源模型如Molmo、Aria以及不同规模的Llama3等）在Multimodal RewardBench上的性能进行分析，探究模型在不同领域的表现情况，为理解当前VLM奖励模型能力提供依据。

### 📈 实验结果
在评估多种VLM法官时发现：即使是表现最好的Gemini 1.5 Pro和Claude 3.5 Sonnet模型，整体准确率也仅为72%；并且大多数模型在推理（数学和编码任务）和安全（尤其是毒性检测）领域表现不佳。这表明Multimodal RewardBench为多领域奖励模型发展提供了具有挑战性的测试平台。

### 💬 可借鉴之处
1. 基准构建角度：本文构建全面且专家标注的多模态奖励模型评估基准的思路，为后续相关领域基准建设提供了范例，尤其是在覆盖多领域（如知识、推理、安全等此前VLM奖励模型评估未涉及领域）方面的尝试值得借鉴。
2. 模型评估角度：对不同类型VLM法官在多领域的性能分析方法，有助于后续研究更全面地理解模型能力边界，为改进模型提供方向，这种多维度、多模型的评估方式值得在模型评估工作中参考。
3. 数据资源角度：公开释放基准数据集，推动了整个研究社区对多模态奖励模型的研究，这种开放共享的理念也为领域发展提供了助力，值得相关研究学习。

## enhancing-cognition-and-explainability-of-multimodal-foundation-models-with-self-synthesized-data
### Abstract
Large Multimodal Models (LMMs), or Vision-Language Models (VLMs), have shown
impressive capabilities in a wide range of visual tasks. However, they often
struggle with fine-grained visual reasoning, failing to identify
domain-specific objectives and provide justifiable explanations for their
predictions. To address the above challenge, we propose a novel visual
rejection sampling framework to improve the cognition and explainability of
LMMs using self-synthesized data. Specifically, visual fine-tuning requires
images, queries, and target answers. Our approach begins by synthesizing
interpretable answers that include human-verifiable visual features. These
features are based on expert-defined concepts, and carefully selected based on
their alignment with the image content. After each round of fine-tuning, we
apply a reward model-free filtering mechanism to select the highest-quality
interpretable answers for the next round of tuning. This iterative process of
synthetic data generation and fine-tuning progressively improves the model's
ability to generate accurate and reasonable explanations. Experimental results
demonstrate the effectiveness of our method in improving both the accuracy and
explainability of specialized visual classification tasks.
### 🌟 论文解读 | 用自合成数据增强多模态基础模型的认知与可解释性

### 📌 背景痛点/本文动机
大的多模态模型（LMMs，也称为视觉 - 语言模型VLMs）在众多视觉任务中展现出了令人瞩目的能力，然而在细粒度视觉推理任务中却存在不足，难以识别特定领域目标且无法为预测提供合理的解释。比如在斯坦福狗狗数据集上，像LLaVA - 1.5这样的先进模型分类准确率也仅为12.2%。同时，为解决该问题进行微调时，缺乏高质量数据（创建特征级图像标注复杂且资源密集），若仅用标签或通用标签关联特征训练又会导致捷径学习或过度泛化等问题，这些都促使研究者探索新方法来增强LMMs的认知和可解释性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出视觉拒绝采样框架实现自合成数据增强认知与可解释性
该框架无需大量手动标注，让LMMs自合成可解释答案。对于给定图像，先利用LMM的图像描述能力生成描述以识别图像特定视觉特征，单个描述可能仅覆盖部分关键特征，但收集大量描述能近似图像特征的真实分布以减少单个描述的不完整性，还通过信息瓶颈技术选择最相关特征，再将这些图像特定概念重写成可解释答案。
💡 创新点2：引入信息论方法与无奖励模型过滤机制保障数据质量与选择
用信息论方法为每个图像选择可解释视觉概念，确保精准识别图像特征；设计无奖励模型的过滤机制，在每轮微调后从合成输出中选择最高质量的可解释答案用于下一轮调优，保障训练数据质量。
💡 创新点3：设计数据合成与模型微调的迭代过程逐步提升能力
先提取图像级特征并转化为可解释答案，与对应图像和查询构成初始训练数据集，微调得到更新模型；用更新模型重复生成答案，选最优的用于下一轮微调，通过这种自增强过程逐步提升LMM生成可靠解释的能力。

### 📈 实验结果
文中实验结果表明该方法在提升特定视觉分类任务的准确性和可解释性方面是有效的（虽未详细阐述实验对比细节，但从摘要和引言可明确方法对解决LMMs在细粒度视觉推理等问题的有效性得到了验证）。

### 💬 可借鉴之处
1. 面对数据稀缺或标注困难的任务时，可借鉴自合成数据的思路，利用模型自身能力生成训练所需的部分数据，减少对大量人工标注的依赖。
2. 信息瓶颈技术用于特征选择以及无奖励模型过滤机制在数据选择上的设计，为处理数据质量和相关性问题提供了新的思路，在其他需要对数据进行筛选和特征处理的模型训练任务中可参考。
3. 迭代式的数据合成与模型微调过程为持续提升模型性能和特定能力（如可解释性）提供了范式，在改进模型针对特定领域任务的表现时可借鉴这种迭代优化的方式。

## diffusion-model-as-a-noise-aware-latent-reward-model-for-step-level-preference-optimization
### Abstract
Preference optimization for diffusion models aims to align them with human
preferences for images. Previous methods typically use Vision-Language Models
(VLMs) as pixel-level reward models to approximate human preferences. However,
when used for step-level preference optimization, these models face challenges
in handling noisy images of different timesteps and require complex
transformations into pixel space. In this work, we show that pre-trained
diffusion models are naturally suited for step-level reward modeling in the
noisy latent space, as they are explicitly designed to process latent images at
various noise levels. Accordingly, we propose the Latent Reward Model (LRM),
which repurposes components of the diffusion model to predict preferences of
latent images at arbitrary timesteps. Building on LRM, we introduce Latent
Preference Optimization (LPO), a step-level preference optimization method
conducted directly in the noisy latent space. Experimental results indicate
that LPO significantly improves the model's alignment with general, aesthetic,
and text-image alignment preferences, while achieving a 2.5-28x training
speedup over existing preference optimization methods. Our code and models are
available at https://github.com/Kwai-Kolors/LPO.
### 🌟 论文解读 | 扩散模型化身噪声感知潜在奖励模型，革新步骤级偏好优化

### 📌 背景痛点/本文动机
在扩散模型的偏好优化领域，以往方法常采用视觉-语言模型（VLMs）作为像素级奖励模型来逼近人类偏好。但用于步骤级偏好优化时，这类像素级奖励模型（PRMs）存在诸多挑战：一是复杂转换，每一步 timestep 都要额外进行扩散去噪和 VAE 解码，把含噪潜在图像转成像素图像，推理流程冗长；二是高噪声不兼容，大 timestep 下噪声强，生成的预测图像模糊，和 VLMs 训练数据（清晰图像）分布偏移大，导致预测不可靠；三是时间步不敏感，PRMs 一般不把 timestep 作为输入，难理解不同时间步对图像评估的影响。这些问题阻碍了其在步骤级奖励建模中的效果，因此需要一种能在潜在空间自然捕捉人类偏好、感知时间步且适配高噪声的模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出潜在奖励模型（LRM）
发现预训练的文本到图像扩散模型天然适合在含噪潜在空间做步骤级奖励建模，因为它有处理不同噪声水平潜在图像的显式设计。LRM 复用扩散模型组件，针对任意 timestep 的潜在图像 xt，利用 U - Net 或 DiT 的视觉特征以及文本编码器的文本特征来预测步骤级偏好标签，还引入视觉特征增强（VFE）模块提升对文本 - 图像对齐的关注。

💡 创新点2：提出潜在偏好优化（LPO）方法
基于 LRM，LPO 是直接在含噪潜在空间开展的步骤级偏好优化方法。为解决 LRM 训练数据中偏好不一致问题，提出多偏好一致过滤（MPCF）策略，确保获胜图像在多个维度持续优于失败图像，以此用 LRM 进行步骤级偏好优化。

### 📈 实验结果
在 SD1.5 和 SDXL 上的大量实验表明，LPO 在通用、美学、文本 - 图像对齐偏好方面大幅提升生成图像质量，持续超越现有 DPO 和 SPO 方法；训练效率出色，相比 Diffusion - DPO 有 10 - 28 倍加速，相比 SPO 有 2.5 - 3.5 倍加速；还探索了基于 LRM 的 GRPO 逐步变体，并将 LPO 应用于基于 DiT 的 SD3 模型，展现了 LRM 和 LPO 的泛化能力。

### 💬 可借鉴之处
1. 模型复用思路：发现预训练扩散模型自身特性并将其复用为奖励模型，为利用已有成熟模型解决新问题提供了思路，不再依赖额外 VLMs 做像素级奖励模型，开辟了奖励建模新路径。
2. 空间优化角度：在潜在空间直接开展偏好优化，避免像素级奖励模型的复杂转换和高噪声不兼容等问题，为扩散模型偏好优化的空间选择提供了更高效的方向。
3. 训练策略创新：提出的 MPCF 策略解决训练数据偏好不一致问题，这种针对训练数据特性设计过滤策略的方式，在处理有偏好标签的数据时具有参考价值。

## os-genesis--automating-gui-agent-trajectory-construction-via-reverse-task-synthesis
### Abstract
Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
https://qiushisun.github.io/OS-Genesis-Home/.
### 🌟 论文解读 | OS-Genesis：颠覆传统，反向合成打造高质量GUI智能体轨迹数据

### 📌 背景痛点/本文动机
基于视觉-语言模型（VLMs）的图形用户界面（GUI）智能体已展现出类人的计算机控制能力，推动数字自动化发展。但**高质量训练轨迹数据的采集**成为关键瓶颈：现有方法依赖人类监督或预定义任务生成合成数据，存在资源消耗大、数据质量难保障、多样性不足以及合成数据与真实环境差距大等问题。为突破这些限制，论文提出OS - Genesis这一创新性GUI数据合成 pipeline。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：交互驱动 + 反向任务合成，颠覆传统流程  
摒弃传统依赖预定义任务的轨迹采集方式，OS - Genesis采用**交互驱动**思路。先让智能体感知GUI环境并进行逐步交互（如点击等操作），再通过**反向任务合成**，从观测到的状态和动作中逆向生成低层次指令，进而推导高层次指令，实现轨迹层面的探索。这种方式突破了预定义任务对数据规模与多样性的限制，还能自然衔接抽象指令与GUI动态特性。  

💡 创新点2：轨迹奖励模型保障数据质量  
在合成任务转化为轨迹后，引入**轨迹奖励模型**，对生成的轨迹进行质量评估与筛选，确保用于训练的轨迹具备高可用性，为GUI智能体训练提供优质数据支撑。  

💡 创新点3：端到端支持多环境GUI智能体训练  
OS - Genesis无需人类监督，能高效合成高质量轨迹数据，支持在不同环境（如移动、网页等）下对GUI智能体进行端到端训练，助力智能体从动作自动化向全流程自主化演进。  


### 📈 实验结果
在AndroidWorld和WebArena这两个高挑战性在线基准测试中，OS - Genesis展现出强大效能：在AndroidWorld上，相较任务驱动方法性能大幅提升，从9.82%提升至17.41%，近乎翻倍。实验充分验证了OS - Genesis合成轨迹的高质量，以及将通用VLMs转化为专业GUI智能体的巨大潜力，同时也体现出其在数据质量、多样性方面远超现有合成方法。  


### 💬 可借鉴之处
1. **思路转变**：从任务驱动到交互驱动的范式转换，为解决数据采集瓶颈提供了全新视角，在其他需大量数据且依赖预定义任务易受限的领域（如其他交互类智能体训练），这种思路值得借鉴。  
2. **反向合成机制**：反向任务合成的设计，为挖掘环境功能、生成有意义可执行任务提供了有效手段，可启发类似需从交互行为反推任务逻辑的场景方案设计。  
3. **质量保障手段**：轨迹奖励模型对数据质量的把控策略，在各类需对生成数据进行质量筛选的任务（如文本生成、图像生成后筛选优质样本）中，都有参考价值。  
4. **多环境适配**：其支持多环境端到端训练的特性，为跨平台智能体开发提供了可参考的技术路线，助力打造更通用的智能系统。  

## clip-rldrive--human-aligned-autonomous-driving-via-clip-based-reward-shaping-in-reinforcement-learning
### Abstract
This paper presents CLIP-RLDrive, a new reinforcement learning (RL)-based
framework for improving the decision-making of autonomous vehicles (AVs) in
complex urban driving scenarios, particularly in unsignalized intersections. To
achieve this goal, the decisions for AVs are aligned with human-like
preferences through Contrastive Language-Image Pretraining (CLIP)-based reward
shaping. One of the primary difficulties in RL scheme is designing a suitable
reward model, which can often be challenging to achieve manually due to the
complexity of the interactions and the driving scenarios. To deal with this
issue, this paper leverages Vision-Language Models (VLMs), particularly CLIP,
to build an additional reward model based on visual and textual cues.
### 🌟 论文解读 | CLIP-RLDrive：基于CLIP奖励塑造的类人自动驾驶强化学习框架

### 📌 背景痛点/本文动机
自动驾驶汽车（AVs）大规模商业化面临边缘案例处理难题，传统决策系统因设计僵化难以在复杂交互、意外条件等场景有效泛化或适应，而人类驾驶员凭借直觉、社交智能等在边缘案例表现更好。强化学习（RL）虽提升了AVs决策能力，但在长尾场景因训练数据集模式有限仍有不足。同时，RL中奖励函数设计关键且具挑战性，稀疏或延迟奖励、反馈慢等会阻碍智能体学习，奖励塑造虽能加速学习但可能使智能体优化偏离原始目标的奖励函数。此外，很多RL任务是视觉类的，需用视觉 - 语言模型（VLMs）。因此，本文旨在提出CLIP - RLDrive框架，利用CLIP基于视觉和文本线索构建奖励模型，改进AVs在复杂城市驾驶场景（尤其是无信号交叉口）的决策，使其与类人偏好对齐。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出CLIP - RLDrive框架，利用CLIP实现基于奖励塑造的类人自动驾驶决策。借助CLIP对齐图像和文本嵌入的能力，将类人指令转化为奖励信号以引导AV决策过程，解决RL中奖励模型设计难题，为自动驾驶决策提供额外基于视觉和文本线索的奖励模型。
💡 创新点2：在复杂无信号交叉口环境中应用两种RL算法（PPO和DQN）训练智能体，并对比有无CLIP奖励模型时的性能，明确CLIP对智能体学习和优化行为以契合期望驾驶动作的影响。

### 📈 实验结果
实验对比了基于CLIP的DQN和PPO算法性能，基于CLIP的DQN实现了96%的成功率且碰撞率仅4%；而基于CLIP的PPO成功率为38%，超时率达54%。这表明基于CLIP的DQN在使自动驾驶行为与类人驾驶标准对齐方面更有效，凸显了所提框架在类人驾驶行为对齐上的优势。

### 💬 可借鉴之处
在强化学习与自动驾驶结合领域，利用视觉 - 语言模型（如CLIP）来构建奖励模型是一种创新思路，为解决RL中奖励函数设计难题提供了新方向，可借鉴此思路处理其他需结合视觉和文本信息、需类人偏好对齐的RL任务；同时，通过对比不同RL算法在引入新奖励模型后的性能，为后续选择合适算法处理特定场景自动驾驶任务提供了参考，也启示在其他领域结合多种算法对比实验来验证方法有效性。

## advdreamer-unveils--are-vision-language-models-truly-ready-for-real-world-3d-variations-
### Abstract
Vision Language Models (VLMs) have exhibited remarkable generalization
capabilities, yet their robustness in dynamic real-world scenarios remains
largely unexplored. To systematically evaluate VLMs' robustness to real-world
3D variations, we propose AdvDreamer, the first framework capable of generating
physically reproducible Adversarial 3D Transformation (Adv-3DT) samples from
single-view observations. In AdvDreamer, we integrate three key innovations:
Firstly, to characterize real-world 3D variations with limited prior knowledge
precisely, we design a zero-shot Monocular Pose Manipulation pipeline built
upon generative 3D priors. Secondly, to ensure the visual quality of worst-case
Adv-3DT samples, we propose a Naturalness Reward Model that provides continuous
naturalness regularization during adversarial optimization, effectively
preventing convergence to hallucinated or unnatural elements. Thirdly, to
enable systematic evaluation across diverse VLM architectures and
visual-language tasks, we introduce the Inverse Semantic Probability loss as
the adversarial optimization objective, which solely operates in the
fundamental visual-textual alignment space. Based on the captured Adv-3DT
samples with high aggressiveness and transferability, we establish MM3DTBench,
the first VQA benchmark dataset tailored to evaluate VLM robustness under
challenging 3D variations. Extensive evaluations of representative VLMs with
varying architectures reveal that real-world 3D variations can pose severe
threats to model performance across various tasks.
### 🌟 论文解读 | AdvDreamer：揭开视觉语言模型在真实世界3D变化下的鲁棒性面纱

### 📌 背景痛点/本文动机
视觉语言模型（VLMs）在视觉感知与自然语言理解的跨模态任务中展现出卓越泛化能力，也逐步应用于自动驾驶、机器人系统等安全关键领域。但现有研究多聚焦数字域2D扰动下的鲁棒性，**真实世界中普遍存在的3D变化对VLMs鲁棒性的挑战却未被充分探索**。比如模型在面对物体姿态、视角等3D维度变化时，能否稳定完成视觉问答、图像描述等任务？为系统评估VLMs对真实世界3D变化的鲁棒性，论文提出AdvDreamer框架，试图解决“如何在有限先验（如单视角观测）下精准刻画真实世界3D变化”等核心问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：零样本单目姿态操控（Monocular Pose Manipulation） pipeline  
为在有限先验下精准刻画真实世界3D变化，论文基于生成式3D先验设计零样本单目姿态操控流程。区别于依赖显式3D结构或多视角密集观测的传统方法，该流程利用预训练模型中蕴含的丰富3D先验，仅通过单视角自然图像就能实现对物体3D姿态等变化的操控，摆脱了对大量场景先验的依赖，让3D变化刻画更贴合真实世界“单视角观测居多”的场景。  

💡 创新点2：自然性奖励模型（Naturalness Reward Model）  
 adversarial优化易生成“幻觉化、不自然”的样本，为保证对抗性3D变换（Adv - 3DT）样本在“最坏情况”下仍有合理视觉质量，论文提出自然性奖励模型。在对抗优化过程中持续提供自然性正则化，避免优化收敛到不真实元素，让生成的Adv - 3DT样本既具备攻击性又符合真实世界视觉规律，能在物理世界复现。  

💡 创新点3：逆语义概率损失（Inverse Semantic Probability loss）作为对抗优化目标  
为在不同VLM架构和视觉 - 语言任务上实现系统评估，论文提出逆语义概率损失作为对抗优化目标。该损失仅在“视觉 - 文本对齐”这一基础空间运作，能针对性地构造让VLMs表现恶化的对抗样本，助力系统探究3D变化下模型鲁棒性。  

💡 创新点4：构建MM3DTBench基准数据集  
基于AdvDreamer生成的高攻击性、高迁移性Adv - 3DT样本，论文构建了MM3DTBench——首个专为评估VLMs在富挑战性3D变化下鲁棒性设计的VQA基准数据集。填补了3D变化场景下VLMs鲁棒性评估数据集的空白。

### 📈 实验结果
对不同架构的代表性VLMs开展广泛评估后发现：**真实世界3D变化能对模型在各类视觉 - 语言任务（如视觉问答、图像描述等）上的性能造成严重威胁**。比如在物体识别、功能描述等任务中，面对AdvDreamer生成的Adv - 3DT样本，模型准确率显著下降；且这些对抗样本在物理世界复现后、背景变化时仍保留攻击性，验证了方法与数据集的有效性。

### 💬 可借鉴之处
1. 问题视角创新：首次聚焦“真实世界3D变化对VLMs鲁棒性影响”这一空白领域，为VLMs鲁棒性研究开辟新方向，启发后续关注更贴近真实场景的多维度鲁棒性分析。  
2. 技术路线创新：从3D变化刻画（单目姿态操控）、样本质量保障（自然性奖励模型）、对抗目标设计（逆语义概率损失）到基准构建（MM3DTBench），形成完整的“生成 - 评估”技术链条，为领域内对抗样本生成、鲁棒性评估提供了端到端的方法论参考。  
3. 应用价值启发：让研究者与开发者意识到VLMs在真实动态场景部署的潜在风险，推动模型在安全关键领域（如自动驾驶视觉交互模块）的鲁棒性优化工作，也为3D感知与视觉语言跨模态结合的研究提供新思路。

## vista-dataset--do-vision-language-models-understand-sequential-tasks-
### Abstract
Using vision-language models (VLMs) as reward models in reinforcement
learning holds promise for reducing costs and improving safety. So far, VLM
reward models have only been used for goal-oriented tasks, where the agent must
reach a particular final outcome. We explore VLMs' potential to supervise tasks
that cannot be scored by the final state alone. To this end, we introduce
ViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.
ViSTa comprises over 4,000 videos with step-by-step descriptions in virtual
home, Minecraft, and real-world environments. Its novel hierarchical structure
-- basic single-step tasks composed into more and more complex sequential tasks
-- allows a fine-grained understanding of how well VLMs can judge tasks with
varying complexity. To illustrate this, we use ViSTa to evaluate
state-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while
they are all good at object recognition, they fail to understand sequential
tasks, with only GPT-4o achieving non-trivial performance.
### 🌟 论文解读 | ViSTa Dataset：探索视觉语言模型对序列任务的理解能力

### 📌 背景痛点/本文动机
强化学习（RL）在游戏、机器人等复杂序列决策任务中表现出色，但设计可靠的奖励函数却颇具挑战，人工监督成本高昂且易被“欺骗”。视觉语言模型（VLMs）有望作为奖励模型解决这些问题，此前VLMs仅用于面向目标的任务（依据最终状态评判），而本文关注“无法仅通过最终状态评分”的序列任务，探索VLMs监督这类任务的潜力，为此提出ViSTa数据集。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建ViSTa数据集  
ViSTa包含超4000个视频及分步描述，覆盖虚拟家庭、Minecraft、真实世界3类环境。其采用**分层结构**：从单步基础任务（Level 1，如“捡起香蕉”）逐步组合成更复杂的多步序列任务（Level 2到Level 8，如“捡起香蕉→把香蕉放进壁橱”），能细粒度测试VLMs对不同复杂度序列任务的理解。  

💡 创新点2：设计问题集（Problem Sets）  
将视频 - 描述对分组为问题集，每个问题集针对特定能力（如物体识别、动作顺序理解）。评估时让模型为视频与描述的匹配度打分，通过不同问题集聚合结果，系统评估VLMs在序列任务理解上的能力。  

### 📈 实验结果
用ViSTa评估CLIP、ViCLIP、GPT - 4o等前沿VLMs发现：  
- 物体识别能力：这些模型在单步任务的物体识别上表现较好；  
- 序列任务理解：除GPT - 4o取得一定性能外，其他模型在理解序列任务（如动作顺序、多步逻辑）上表现不佳，暴露了VLMs在序列推理能力上的短板。  

### 💬 可借鉴之处
1. 数据集构建思路：ViSTa的分层结构与问题集设计为评估模型特定能力提供了细粒度框架，可借鉴这种“从简单到复杂、针对性测试”的数据集构建逻辑，用于其他需评估序列/多步推理的任务场景。  
2. 研究视角拓展：聚焦“非最终状态评判”的序列任务监督，填补了VLMs在RL奖励模型应用中的研究空白，启发后续探索VLMs在过程导向型任务中的潜力。  
3. 实验结论价值：明确当前VLMs在序列任务理解上的不足，为后续模型改进（如优化序列推理、跨环境泛化）指明方向，也让研究者更清晰认识VLMs作为奖励模型的局限与潜力。

## gen-drive--enhancing-diffusion-generative-driving-policies-with-reward-modeling-and-reinforcement-learning-fine-tuning
### Abstract
Autonomous driving necessitates the ability to reason about future
interactions between traffic agents and to make informed evaluations for
planning. This paper introduces the \textit{Gen-Drive} framework, which shifts
from the traditional prediction and deterministic planning framework to a
generation-then-evaluation planning paradigm. The framework employs a behavior
diffusion model as a scene generator to produce diverse possible future
scenarios, thereby enhancing the capability for joint interaction reasoning. To
facilitate decision-making, we propose a scene evaluator (reward) model,
trained with pairwise preference data collected through VLM assistance, thereby
reducing human workload and enhancing scalability. Furthermore, we utilize an
RL fine-tuning framework to improve the generation quality of the diffusion
model, rendering it more effective for planning tasks. We conduct training and
closed-loop planning tests on the nuPlan dataset, and the results demonstrate
that employing such a generation-then-evaluation strategy outperforms other
learning-based approaches. Additionally, the fine-tuned generative driving
policy shows significant enhancements in planning performance. We further
demonstrate that utilizing our learned reward model for evaluation or RL
fine-tuning leads to better planning performance compared to relying on
human-designed rewards. Project website: https://mczhi.github.io/GenDrive.
### 🌟 论文解读 | Gen - Drive：用奖励建模与强化学习微调革新扩散生成式驾驶策略

### 📌 背景痛点/本文动机
在自动驾驶领域，传统的预测加确定性规划框架存在不足。常规预测与确定性规划方法常分离预测和规划过程，使自车脱离社交情境，易产生不符合社交驾驶规范的行为；即便集成预测 - 规划框架也依赖确定性规划，难处理智能体行为的不确定性、多模态与交互动态性。同时，生成模型在自动驾驶决策任务中应用有限，一方面评估生成场景并选最优用于决策以契合人类期望很复杂，另一方面规划任务需生成更可能的未来场景且样本少以减计算开销和延迟。所以，本文提出Gen - Drive框架，转向生成 - 评估规划范式来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多智能体轨迹扩散模型构建
开发多智能体轨迹扩散模型，将自车融入社交交互情境，为场景中所有智能体生成多样且场景一致的未来场景，增强联合交互推理能力，该模型包含基于查询的场景上下文编码器和扩散式场景生成器，先利用大量真实驾驶数据训练基础扩散模型（场景编码器 + 生成器）。
💡 创新点2：VLM辅助的奖励模型训练
提出场景评估（奖励）模型，用VLM辅助收集的成对偏好数据训练。通过对基础扩散模型生成的场景采样，用视觉语言模型（VLMs）辅助的混合标记 pipeline 构建成对偏好数据集，再基于该数据集训练场景评估器，以解决生成场景评估难的问题，让决策更优。
💡 创新点3：强化学习微调 pipeline 搭建
构建强化学习微调 pipeline，基于学到的奖励模型增强扩散驾驶策略性能。利用从AI反馈的强化学习来微调扩散生成器，提升其在规划任务中的效力，解决生成模型在规划任务中需少样本生成更可能未来场景的问题。

### 📈 实验结果
在nuPlan数据集上进行训练和闭环规划测试，结果表明：采用生成 - 评估策略优于其他基于学习的方法；微调后的生成式驾驶策略在规划性能上有显著提升；与依赖人工设计奖励相比，用所学奖励模型评估或RL微调能带来更好的规划性能。

### 💬 可借鉴之处
1. 范式转换思路：从传统预测和确定性规划转向生成 - 评估框架，为自动驾驶规划任务提供了新的范式参考，在处理多智能体交互和不确定性上有借鉴意义。
2. 数据利用与模型训练：利用VLM辅助构建偏好数据集训练奖励模型，以及结合强化学习微调生成模型的流程，为解决生成模型在决策任务中的评估和生成质量问题提供了可参考的技术路线，在其他需处理复杂交互和不确定性的生成式任务中也可能适用。
3. 多模块协同设计：Gen - Drive中场景编码器、扩散生成器、场景评估器等多模块协同的架构设计，对于构建复杂任务下的智能系统有参考价值，展示了如何通过不同模块分工与协作来实现更优性能。

## dr-llava--visual-instruction-tuning-with-symbolic-clinical-grounding
### Abstract
Vision-Language Models (VLM) can support clinicians by analyzing medical
images and engaging in natural language interactions to assist in diagnostic
and treatment tasks. However, VLMs often exhibit "hallucinogenic" behavior,
generating textual outputs not grounded in contextual multimodal information.
This challenge is particularly pronounced in the medical domain, where we do
not only require VLM outputs to be accurate in single interactions but also to
be consistent with clinical reasoning and diagnostic pathways throughout
multi-turn conversations. For this purpose, we propose a new alignment
algorithm that uses symbolic representations of clinical reasoning to ground
VLMs in medical knowledge. These representations are utilized to (i) generate
GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM
conversations with demonstrations of clinical reasoning, and (ii) create an
automatic reward function that evaluates the clinical validity of VLM
generations throughout clinician-VLM interactions. Our algorithm eliminates the
need for human involvement in training data generation or reward model
construction, reducing costs compared to standard reinforcement learning with
human feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a
conversational VLM finetuned for analyzing bone marrow pathology slides,
demonstrating strong performance in multi-turn medical conversations.
### 🌟 论文解读 | Dr-LLaVA：用符号化临床推理锚定医疗场景下的视觉语言模型

### 📌 背景痛点/本文动机
视觉语言模型（VLM）在医疗领域有潜力辅助临床诊断与交流，但现有VLM存在“幻觉”问题——输出既可能脱离视觉输入，也可能在多轮对话中与临床推理、诊断路径矛盾。传统基于人类反馈的强化学习（RLHF）在医疗场景受限：医疗多轮对话的训练数据稀缺，且收集临床专家反馈成本高、难规模化。同时现有医疗VLM多仅支持单轮问答，无法应对复杂多轮临床推理对话，因此需要新方法让VLM在医疗知识与临床逻辑下对齐。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：符号化临床推理表征驱动数据生成  
将临床诊断过程（如血液肿瘤诊断）形式化为**符号化规则集合S**（类似决策树，规定从图像质量评估到细胞分析再到最终诊断的合法路径）。基于该规则，结合骨髓病理图像与病理学家标注，用GPT - 4规模化生成“临床医生 - VLM”多轮对话训练数据。数据里每个图像对应多轮问答（如先问图像质量、再问细胞类型、最后问诊断），问答内容由符号规则和图像标注指导生成，模拟真实临床推理对话。

💡 创新点2：自动符号化奖励函数实现对齐  
设计**基于符号规则的自动奖励机制**，替代人工反馈。奖励分两部分：一是“正确性奖励”，检查单轮回答是否符合当前步骤的临床知识；二是“一致性奖励”，确保多轮对话整体符合临床推理路径（如先判定图像高质量，后续诊断才合法）。该奖励函数无需人工标注，降低RLHF的人力依赖，让VLM在多轮对话中既单轮准确、又整体逻辑自洽。

💡 创新点3：面向骨髓病理的多轮对话VLM落地  
基于上述数据生成与奖励机制，对LLaVA模型微调得到**Dr - LLaVA**，专攻骨髓病理切片分析的多轮对话诊断。从数据构建到模型训练全流程，用符号化临床知识锚定VLM，使其适应医疗场景下“视觉 + 多轮语言交互 + 临床逻辑”的复杂任务。

### 📈 实验结果
1. 单轮与多轮对话性能超越SOTA：在骨髓病理图像的问答任务中，Dr - LLaVA在单轮准确性、多轮对话的连贯性与临床合规性上，优于现有通用或医疗领域VLM（如LLaVA - Med等）。  
2. 鲁棒性与纠错能力突出：消融实验显示，基于符号规则的指令微调让Dr - LLaVA对问题顺序变化更鲁棒，且在识别、修正临床提问里的错误信息时，表现远超基线模型。  

### 💬 可借鉴之处
1. 领域知识符号化：将专业领域（如医疗）的推理逻辑拆解为符号规则，为数据生成和模型评估提供结构化锚点，这种“领域知识显式建模”思路可推广到法律、工业检测等强专业逻辑领域。  
2. 自动化替代人工反馈：用符号规则构建自动奖励函数，减少RLHF对大规模人工标注的依赖，为资源有限或需专业知识的领域模型对齐提供了低成本路径。  
3. 多轮对话场景适配：聚焦多轮交互下的逻辑一致性，突破单轮QA局限，为VLM在客服、诊疗、教育等需连续对话的场景落地提供方法论参考。

## parameter-efficient-reinforcement-learning-from-human-feedback
### Abstract
While Reinforcement Learning from Human Feedback (RLHF) effectively aligns
pretrained Large Language and Vision-Language Models (LLMs, and VLMs) with
human preferences, its computational cost and complexity hamper its wider
adoption. To alleviate some of the computational burden of fine-tuning,
parameter efficient methods, like LoRA were introduced. In this work, we
empirically evaluate the setup of Parameter Efficient Reinforcement Learning
from Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward
Modeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six
diverse datasets spanning summarization, harmless/helpful response generation,
UI automation, and visual question answering in terms of effectiveness of the
trained models, and the training resources required. Our findings show, for the
first time, that PE-RLHF achieves comparable performance to RLHF, while
significantly reducing training time (up to 90% faster for reward models, and
30% faster for RL), and memory footprint (up to 50% reduction for reward
models, and 27% for RL). We provide comprehensive ablations across LoRA ranks,
and model sizes for both reward modeling and reinforcement learning. By
mitigating the computational burden associated with RLHF, we push for a broader
adoption of PE-RLHF as an alignment technique for LLMs and VLMs.
### 🌟 论文解读 | 高效参数化：让人类反馈强化学习更易用

### 📌 背景痛点/本文动机
大语言模型（LLM）和视觉语言模型（VLM）要与人类偏好对齐，强化学习从人类反馈（RLHF）是常用方法，但它计算成本高、复杂度大，限制了广泛应用。比如RLHF训练时需要额外的模型副本（奖励模型、KL正则化的锚模型等），内存占用比标准微调大很多。为缓解微调的计算负担，参数高效方法（如LoRA）应运而生，本文就聚焦于基于LoRA的参数高效人类反馈强化学习（PE - RLHF）展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：PE - RLHF整体框架
PE - RLHF把参数高效微调技术应用到RLHF的两个关键阶段（奖励模型训练和策略模型的强化学习）。在奖励模型训练时，构建带LoRA适配器的奖励模型，适配器连到模型每个注意力投影矩阵，训练仅更新适配器，模型 backbone 冻结；强化学习阶段，策略和价值模型也用LoRA适配器，同样冻结 backbone 训练适配器，再基于价值模型计算的策略梯度优化策略，价值模型结合奖励分数和与锚策略的KL正则化训练。
💡 创新点2：全面对比与消融实验
在六个不同数据集（涵盖摘要、无害/有用响应生成、UI自动化、视觉问答等任务）上对比PE - RLHF和标准RLHF；还对LoRA的秩（rank）、模型大小在奖励建模和强化学习中做了系统消融实验，探究LoRA对训练的影响。

### 📈 实验结果
在效果上，PE - RLHF能达到和标准RLHF相当的性能。资源消耗上，奖励模型训练时间最多快90%，内存占用最多降50%；RL阶段训练时间最多快30%，内存占用最多降27%。比如奖励模型训练时，PE - RLHF在部分数据集HBM峰值占用仅为标准训练的43 - 74%，训练速度快1.4 - 1.9倍；RL阶段HBM峰值占用为标准的73 - 80%，训练速度快1.15 - 1.3倍等（不同数据集有差异）。

### 💬 可借鉴之处
方法层面，证明了LoRA这类参数高效方法在RLHF中能在保持效果的同时大幅降低资源消耗，为后续用其他参数高效微调（PEFT）或表示微调（ReFT）方法做RLHF任务的基准测试提供了参考；实践层面，让RLHF的计算负担降低，推动其更广泛应用，为大模型对齐人类偏好提供了更高效的技术路线，后续研究参数高效的对齐方法时可借鉴其设计实验和对比分析的思路。

## tuning-large-multimodal-models-for-videos-using-reinforcement-learning-from-ai-feedback
### Abstract
Recent advancements in large language models have influenced the development
of video large multimodal models (VLMMs). The previous approaches for VLMMs
involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets,
integrating LLM with visual encoders, and adding additional learnable modules.
Video and text multimodal alignment remains challenging, primarily due to the
deficient volume and quality of multimodal instruction-tune data compared to
text-only data. We present a novel alignment strategy that employs multimodal
AI system to oversee itself called Reinforcement Learning from AI Feedback
(RLAIF), providing self-preference feedback to refine itself and facilitating
the alignment of video and text modalities. In specific, we propose
context-aware reward modeling by providing detailed video descriptions as
context during the generation of preference feedback in order to enrich the
understanding of video content. Demonstrating enhanced performance across
diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms
existing approaches, including the SFT model. We commit to open-sourcing our
code, models, and datasets to foster further research in this area.
### 🌟 论文解读 | 用AI反馈强化学习解锁视频大模型对齐新姿势

### 📌 背景痛点/本文动机
大语言模型的发展推动了视频大 multimodal 模型（VLMMs）的进步，但视频与文本的多模态对齐仍面临挑战。以往 VLMMs 采用有监督微调（SFT）等方法，然而多模态指令调优数据在数量和质量上远不及纯文本数据，这导致视频内容与文本的对齐效果不佳，模型生成的响应难以很好地基于视频视觉内容。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出基于AI反馈的强化学习（RLAIF）的视频文本对齐方法  
不同于依赖人类反馈的强化学习（RLHF），RLAIF 利用多模态AI系统自我监督，通过提供生成响应的自我偏好反馈来优化自身，实现视频和文本模态的对齐，减少了对大规模人类标注偏好的依赖，让监督更具可扩展性。  

💡 创新点2：上下文感知的奖励建模  
在生成偏好反馈时，将详细的视频描述作为上下文融入多模态AI系统，以此丰富对视频内容的理解，提升视频内容在反馈过程中的清晰度，让奖励模型能更精准评估响应与视频内容的匹配度。  

💡 创新点3：扩充SFT训练数据与课程训练策略  
为弥补SFT训练时多模态指令调优数据的不足，引入人类标注的视频问答和以对象为中心的多模态指令调优数据集；同时提出简单的课程训练策略，来增强视频与文本模态间的对齐效果。  

### 📈 实验结果
在多种视频基准测试（如视频问答、文本到视频检索、动作识别等任务对应的基准）上，提出的 VLM - RLAIF 方法表现优于现有方法（包括 SFT 模型），在不同类型的视频任务中都展现出性能提升，验证了方法在视频多模态对齐与任务处理上的有效性。  

### 💬 可借鉴之处
1. 多模态对齐新思路：RLAIF 为解决多模态数据不足下的模态对齐问题提供了创新方向，展示了用AI自我反馈来优化多模态模型的潜力。  
2. 上下文利用方式：上下文感知奖励建模中对视频描述等上下文的运用，启发在多模态任务中可通过补充详细模态信息来提升模型对内容的理解深度。  
3. 数据与训练策略：扩充数据结合课程训练的方式，为应对数据稀缺问题、高效训练模型提供了可参考的实践思路，在其他数据有限的多模态任务中也有借鉴价值。  
4. 开源承诺：作者团队承诺开源代码、模型和数据集，利于该领域后续研究人员基于此进一步探索，推动整个视频大 multimodal 模型领域发展。

## vision-language-models-are-zero-shot-reward-models-for-reinforcement-learning
### Abstract
Reinforcement learning (RL) requires either manually specifying a reward
function, which is often infeasible, or learning a reward model from a large
amount of human feedback, which is often very expensive. We study a more
sample-efficient alternative: using pretrained vision-language models (VLMs) as
zero-shot reward models (RMs) to specify tasks via natural language. We propose
a natural and general approach to using VLMs as reward models, which we call
VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn
complex tasks without a manually specified reward function, such as kneeling,
doing the splits, and sitting in a lotus position. For each of these tasks, we
only provide a single sentence text prompt describing the desired task with
minimal prompt engineering. We provide videos of the trained agents at:
https://sites.google.com/view/vlm-rm. We can improve performance by providing a
second "baseline" prompt and projecting out parts of the CLIP embedding space
irrelevant to distinguish between goal and baseline. Further, we find a strong
scaling effect for VLM-RMs: larger VLMs trained with more compute and data are
better reward models. The failure modes of VLM-RMs we encountered are all
related to known capability limitations of current VLMs, such as limited
spatial reasoning ability or visually unrealistic environments that are far
off-distribution for the VLM. We find that VLM-RMs are remarkably robust as
long as the VLM is large enough. This suggests that future VLMs will become
more and more useful reward models for a wide range of RL applications.
### 🌟 论文解读 | 视觉-语言模型：强化学习的零样本奖励模型

### 📌 背景痛点/本文动机
强化学习（RL）在基于视觉的任务中训练智能体执行复杂任务时，奖励函数的指定成本很高。手动指定现实世界任务的奖励函数往往不可行，而从人类反馈中学习奖励模型通常成本昂贵。为了让强化学习在实际应用中更有用，需要找到更高效且自然的奖励函数指定方式。此前利用视觉-语言模型（VLMs）提供奖励的尝试，要么需要大量微调VLMs，要么需要复杂的临时程序来提取奖励，而本文探索用预训练视觉-语言模型作为零样本奖励模型（RMs），通过自然语言指定任务，以更高效地解决奖励函数指定难题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出VLM - RM方法  
提出VLM - RM这一通用方法，将预训练的视觉-语言模型用作基于视觉的强化学习任务的奖励模型。具体实现上，使用CLIP作为视觉-语言模型，将当前环境状态的CLIP嵌入与简单语言提示之间的余弦相似度作为奖励函数。还可通过提供描述环境中性状态的“基线提示”，在计算奖励时将表示部分投影到基线和目标提示之间的方向上，对奖励模型进行正则化。

💡 创新点2：多场景验证与任务拓展  
在标准的CartPole和MountainCar强化学习基准测试中验证方法，观察到VLM - RMs与环境的真实奖励高度相关，且能成功训练策略解决任务；还训练MuJoCo人形机器人学习复杂任务，如举臂、莲花坐、劈叉和 kneeling 等，仅用单句文本提示（如“a humanoid robot kneeling”）衍生的CLIP奖励模型来实现。

💡 创新点3：探索模型规模与性能关系  
研究VLM - RMs的性能如何随视觉-语言模型规模扩展，发现视觉-语言模型规模与VLM - RM质量强相关，像图1中的人形机器人任务只能用最大的公开可用CLIP模型完成。

### 📈 实验结果
在CartPole和MountainCar基准测试中，VLM - RMs与环境真实奖励高度相关，能成功训练策略完成任务，且更逼真的环境纹理渲染能提升CLIP作为奖励模型的质量；在MuJoCo人形机器人复杂任务训练中，仅用单句文本提示的CLIP奖励模型就实现了如举臂、莲花坐、劈叉和 kneeling 等任务；还发现VLM - RMs存在强缩放效应，更大的VLMs（用更多计算和数据训练）是更好的奖励模型，同时遇到的失败模式与当前VLMs已知能力限制（如空间推理能力有限、视觉不真实环境分布偏移大）相关，且足够大的VLMs下VLM - RMs非常稳健。

### 💬 可借鉴之处
为强化学习奖励函数指定提供了更高效自然的思路，证明预训练视觉-语言模型可作为零样本奖励模型，无需手动设计奖励函数或收集昂贵数据来学习奖励模型；提出的Goal - Baseline Regularization等技巧可提升奖励模型质量，为后续优化奖励模型提供方法参考；揭示的视觉-语言模型规模与VLM - RM质量的强相关性，为后续模型选择和优化指明方向，即更大更强大的VLMs有望成为更有用的奖励模型，助力从人类编写的任务描述中训练模型执行更复杂任务。

## test-time-adaptation-with-clip-reward-for-zero-shot-generalization-in-vision-language-models
### Abstract
One fascinating aspect of pre-trained vision-language models~(VLMs) learning
under language supervision is their impressive zero-shot generalization
capability. However, this ability is hindered by distribution shifts between
the training and testing data. Previous test time adaptation~(TTA) methods for
VLMs in zero-shot classification rely on minimizing the entropy of model
outputs, tending to be stuck in incorrect model predictions. In this work, we
propose TTA with feedback to rectify the model output and prevent the model
from becoming blindly confident. Specifically, a CLIP model is adopted as the
reward model during TTA and provides feedback for the VLM. Given a single test
sample, the VLM is forced to maximize the CLIP reward between the input and
sampled results from the VLM output distribution. The proposed
\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly
flexible and universal. Beyond the classification task, with task-specific
sampling strategies and a proper reward baseline choice, RLCF can be easily
extended to not only discrimination tasks like retrieval but also
generalization tasks like image captioning, improving the zero-shot
generalization capacity of VLMs. According to the characteristics of these VL
tasks, we build different fully TTA pipelines with RLCF to improve the
zero-shot generalization ability of various VLMs. Extensive experiments along
with promising empirical results demonstrate the effectiveness of RLCF. The
code is available at https://github.com/mzhaoshuai/RLCF.
### 🌟 论文解读 | 用CLIP奖励实现测试时自适应，提升视觉语言模型零样本泛化能力

### 📌 背景痛点/本文动机
预训练视觉 - 语言模型（VLMs）在语言监督下学习具备出色的零样本泛化能力，但训练和测试数据间的分布偏移会阻碍这一能力。此前针对VLMs零样本分类的测试时自适应（TTA）方法依赖最小化模型输出熵，易陷入错误预测且使模型盲目自信。同时，在无标注情况下如何有效修正模型输出、提升零样本泛化能力是待解决的问题，本文由此展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RLCF框架
提出强化学习结合CLIP反馈（RLCF）的框架用于测试时自适应。采用CLIP作为奖励模型在测试时提供反馈，对于单个测试样本，迫使VLM最大化输入与从VLM输出分布采样结果间的CLIP奖励，以修正模型输出，避免模型盲目自信。
💡 创新点2：框架的灵活性与通用性
RLCF框架极具灵活性和通用性。借助特定任务的采样策略与合适的奖励基线选择，除分类任务外，还能轻松扩展到检索等判别任务以及图像描述等泛化任务，提升VLMs的零样本泛化能力。
💡 创新点3：针对不同任务设计TTA pipeline
依据视觉 - 语言（VL）任务特性，为不同任务构建TTA流程。如分类任务继承相关 pipeline 适配前缀调优和骨干网络自适应；检索任务为效率仅更新与查询相关参数；图像描述任务构建基于大语言模型的TTA流程，且应用任务无关实用技巧（多奖励模型、情景式TTA、增量学习动量缓冲等）。

### 📈 实验结果
文中通过大量实验验证RLCF的有效性，在零样本分类、文本 - 图像检索、图像描述等任务中提升了不同VLMs的零样本性能，有力证明了RLCF在增强VLMs零样本泛化能力方面的作用。

### 💬 可借鉴之处
1. 思路创新：将反馈机制引入测试时自适应，为解决分布偏移下模型泛化问题提供新视角，且利用CLIP作为无标注下的可靠奖励模型，为无监督或零样本场景下的模型优化提供思路。
2. 框架复用：RLCF框架的灵活性设计，展示了如何打造通用型的跨任务自适应框架，在多模态任务拓展方面有借鉴意义。
3. 任务定制：针对不同VL任务定制TTA pipeline的方式，为后续研究中根据任务特性设计适配方法提供了参考范式，包括任务特定采样、参数更新策略及实用技巧运用等方面。

