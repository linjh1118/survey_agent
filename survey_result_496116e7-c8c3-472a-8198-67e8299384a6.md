# Paper List of Terms(reward model+vlm)
- [25/06] **DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models**  
[[Paper](http://arxiv.org/pdf/2506.03517v1)] [[Code/Page]()] [[TLDR/Notes](#densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models)]

- [25/06] **Aligning VLM Assistants with Personalized Situated Cognition**  
[[Paper](http://arxiv.org/pdf/2506.00930v1)] [[Code/Page](https://github.com/NLPGM/PCogAlign.)] [[TLDR/Notes](#aligning-vlm-assistants-with-personalized-situated-cognition)]

- [25/05] **DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving**  
[[Paper](http://arxiv.org/pdf/2505.20665v1)] [[Code/Page]()] [[TLDR/Notes](#driverx--a-vision-language-reasoning-model-for-cross-task-autonomous-driving)]

- [25/05] **Enhance Mobile Agents Thinking Process Via Iterative Preference Learning**  
[[Paper](http://arxiv.org/pdf/2505.12299v2)] [[Code/Page]()] [[TLDR/Notes](#enhance-mobile-agents-thinking-process-via-iterative-preference-learning)]

- [25/05] **Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.07263v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-vl-reward--an-effective-reward-model-for-multimodal-understanding-and-reasoning)]

- [25/05] **TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations**  
[[Paper](http://arxiv.org/pdf/2505.06079v1)] [[Code/Page](https://shuaiyihuang.github.io/publications/TREND.)] [[TLDR/Notes](#trend--tri-teaching-for-robust-preference-based-reinforcement-learning-with-demonstrations)]

- [25/04] **PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations**  
[[Paper](http://arxiv.org/pdf/2504.20520v1)] [[Code/Page]()] [[TLDR/Notes](#prism--projection-based-reward-integration-for-scene-aware-real-to-sim-to-real-transfer-with-few-demonstrations)]

- [25/04] **Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation**  
[[Paper](http://arxiv.org/pdf/2504.16073v1)] [[Code/Page]()] [[TLDR/Notes](#guiding-vlm-agents-with-process-rewards-at-inference-time-for-gui-navigation)]

- [25/04] **Seedream 3.0 Technical Report**  
[[Paper](http://arxiv.org/pdf/2504.11346v3)] [[Code/Page]()] [[TLDR/Notes](#seedream-3-0-technical-report)]

- [25/03] **VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences**  
[[Paper](http://arxiv.org/pdf/2503.13817v1)] [[Code/Page]()] [[TLDR/Notes](#varp--reinforcement-learning-from-vision-language-model-feedback-with-agent-regularized-preferences)]

- [25/02] **Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models**  
[[Paper](http://arxiv.org/pdf/2502.14191v1)] [[Code/Page](https://github.com/facebookresearch/multimodal_rewardbench.)] [[TLDR/Notes](#multimodal-rewardbench--holistic-evaluation-of-reward-models-for-vision-language-models)]

- [25/02] **Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data**  
[[Paper](http://arxiv.org/pdf/2502.14044v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-cognition-and-explainability-of-multimodal-foundation-models-with-self-synthesized-data)]

- [25/02] **Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization**  
[[Paper](http://arxiv.org/pdf/2502.01051v3)] [[Code/Page](https://github.com/Kwai-Kolors/LPO.)] [[TLDR/Notes](#diffusion-model-as-a-noise-aware-latent-reward-model-for-step-level-preference-optimization)]

- [24/12] **OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis**  
[[Paper](http://arxiv.org/pdf/2412.19723v3)] [[Code/Page](https://qiushisun.github.io/OS-Genesis-Home/.)] [[TLDR/Notes](#os-genesis--automating-gui-agent-trajectory-construction-via-reverse-task-synthesis)]

- [24/12] **CLIP-RLDrive: Human-Aligned Autonomous Driving via CLIP-Based Reward Shaping in Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2412.16201v1)] [[Code/Page]()] [[TLDR/Notes](#clip-rldrive--human-aligned-autonomous-driving-via-clip-based-reward-shaping-in-reinforcement-learning)]

- [24/12] **AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?**  
[[Paper](http://arxiv.org/pdf/2412.03002v3)] [[Code/Page]()] [[TLDR/Notes](#advdreamer-unveils--are-vision-language-models-truly-ready-for-real-world-3d-variations-)]

- [24/11] **ViSTa Dataset: Do vision-language models understand sequential tasks?**  
[[Paper](http://arxiv.org/pdf/2411.13211v2)] [[Code/Page]()] [[TLDR/Notes](#vista-dataset--do-vision-language-models-understand-sequential-tasks-)]

- [24/10] **Gen-Drive: Enhancing Diffusion Generative Driving Policies with Reward Modeling and Reinforcement Learning Fine-tuning**  
[[Paper](http://arxiv.org/pdf/2410.05582v1)] [[Code/Page](https://mczhi.github.io/GenDrive.)] [[TLDR/Notes](#gen-drive--enhancing-diffusion-generative-driving-policies-with-reward-modeling-and-reinforcement-learning-fine-tuning)]

- [24/05] **Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding**  
[[Paper](http://arxiv.org/pdf/2405.19567v2)] [[Code/Page]()] [[TLDR/Notes](#dr-llava--visual-instruction-tuning-with-symbolic-clinical-grounding)]

- [24/03] **Parameter Efficient Reinforcement Learning from Human Feedback**  
[[Paper](http://arxiv.org/pdf/2403.10704v2)] [[Code/Page]()] [[TLDR/Notes](#parameter-efficient-reinforcement-learning-from-human-feedback)]

- [24/02] **Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback**  
[[Paper](http://arxiv.org/pdf/2402.03746v3)] [[Code/Page]()] [[TLDR/Notes](#tuning-large-multimodal-models-for-videos-using-reinforcement-learning-from-ai-feedback)]

- [23/10] **Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2310.12921v2)] [[Code/Page](https://sites.google.com/view/vlm-rm.)] [[TLDR/Notes](#vision-language-models-are-zero-shot-reward-models-for-reinforcement-learning)]

- [23/05] **Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2305.18010v2)] [[Code/Page](https://github.com/mzhaoshuai/RLCF.)] [[TLDR/Notes](#test-time-adaptation-with-clip-reward-for-zero-shot-generalization-in-vision-language-models)]



# TLDR/Notes
## densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models
### Abstract
Direct Preference Optimization (DPO) has recently been applied as a
post-training technique for text-to-video diffusion models. To obtain training
data, annotators are asked to provide preferences between two videos generated
from independent noise. However, this approach prohibits fine-grained
comparisons, and we point out that it biases the annotators towards low-motion
clips as they often contain fewer visual artifacts. In this work, we introduce
DenseDPO, a method that addresses these shortcomings by making three
contributions. First, we create each video pair for DPO by denoising corrupted
copies of a ground truth video. This results in aligned pairs with similar
motion structures while differing in local details, effectively neutralizing
the motion bias. Second, we leverage the resulting temporal alignment to label
preferences on short segments rather than entire clips, yielding a denser and
more precise learning signal. With only one-third of the labeled data, DenseDPO
greatly improves motion generation over vanilla DPO, while matching it in text
alignment, visual quality, and temporal consistency. Finally, we show that
DenseDPO unlocks automatic preference annotation using off-the-shelf Vision
Language Models (VLMs): GPT accurately predicts segment-level preferences
similar to task-specifically fine-tuned video reward models, and DenseDPO
trained on these labels achieves performance close to using human labels.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DenseDPOï¼šä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹å¸¦æ¥ç»†ç²’åº¦æ—¶åºåå¥½ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸï¼Œæ‰©æ•£æ¨¡å‹å–å¾—äº†ä¸å°‘è¿›å±•ï¼Œä½†ç°æœ‰è§†é¢‘ç”Ÿæˆå™¨åœ¨æ—¶é—´ä¸€è‡´æ€§ã€è§†è§‰ä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½ç­‰æ–¹é¢ä»æœ‰ä¸è¶³ã€‚Direct Preference Optimizationï¼ˆDPOï¼‰æŠ€æœ¯è¢«ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„åè®­ç»ƒï¼Œä½†ä¼ ç»ŸDPOæ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šè®­ç»ƒæ•°æ®é€šè¿‡è®©æ ‡æ³¨è€…å¯¹ç‹¬ç«‹å™ªå£°ç”Ÿæˆçš„ä¸¤ä¸ªè§†é¢‘åšåå¥½é€‰æ‹©å¾—åˆ°ï¼Œè¿™ç§æ–¹å¼æ— æ³•è¿›è¡Œç»†ç²’åº¦æ¯”è¾ƒï¼Œè¿˜ä¼šä½¿æ ‡æ³¨è€…åå‘ä½è¿åŠ¨ç‰‡æ®µï¼ˆå› ä¸ºè¿™ç±»ç‰‡æ®µè§†è§‰ç‘•ç–µå°‘ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹æ›´å€¾å‘ç”Ÿæˆæ…¢åŠ¨ä½œå†…å®¹ï¼ŒæŠ‘åˆ¶äº†ç”ŸæˆåŠ¨æ€ä¸°å¯Œè§†é¢‘çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ”¹è¿›DPOè§†é¢‘å¯¹æ„å»ºæ–¹å¼  
ä¸å†ç”¨ç‹¬ç«‹å™ªå£°ç”Ÿæˆè§†é¢‘å¯¹ï¼Œè€Œæ˜¯å¯¹çœŸå®è§†é¢‘çš„æŸåå‰¯æœ¬å»å™ªæ¥åˆ›å»ºDPOçš„è§†é¢‘å¯¹ã€‚è¿™æ ·å¾—åˆ°çš„è§†é¢‘å¯¹å…·æœ‰ç›¸ä¼¼è¿åŠ¨ç»“æ„ä½†å±€éƒ¨ç»†èŠ‚ä¸åŒï¼Œèƒ½æœ‰æ•ˆæ¶ˆé™¤è¿åŠ¨åå·®ï¼Œè®©è§†é¢‘å¯¹åœ¨é«˜å±‚è¯­ä¹‰å’Œè¿åŠ¨è½¨è¿¹ä¸Šä¿æŒä¸€è‡´ï¼Œå‡å°‘è™šå‡ç›¸å…³æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»†ç²’åº¦æ—¶åºåå¥½æ ‡æ³¨  
å€ŸåŠ©ä¸Šè¿°æ„å»ºæ–¹å¼å¸¦æ¥çš„æ—¶é—´å¯¹é½æ€§ï¼Œåœ¨çŸ­ç‰‡æ®µè€Œéæ•´ä¸ªè§†é¢‘ clip ä¸Šæ ‡æ³¨åå¥½ï¼Œäº§ç”Ÿæ›´å¯†é›†ã€ç²¾ç¡®çš„å­¦ä¹ ä¿¡å·ã€‚ä»…ç”¨ä¸‰åˆ†ä¹‹ä¸€æ ‡æ³¨æ•°æ®æ—¶ï¼ŒDenseDPO åœ¨è¿åŠ¨ç”Ÿæˆä¸Šè¿œè¶… vanilla DPOï¼ŒåŒæ—¶åœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šèƒ½ä¸ä¹‹æŒå¹³ï¼Œæ•°æ®æ•ˆç‡æ›´é«˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§£é”åŸºäºé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åå¥½æ ‡æ³¨  
è¯æ˜DenseDPO å¯åˆ©ç”¨ç°æˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®ç°è‡ªåŠ¨åå¥½æ ‡æ³¨ã€‚æ¯”å¦‚GPTèƒ½ç²¾å‡†é¢„æµ‹ç‰‡æ®µçº§åå¥½ï¼Œæ•ˆæœæ¥è¿‘ä¸ºç‰¹å®šä»»åŠ¡å¾®è°ƒçš„è§†é¢‘å¥–åŠ±æ¨¡å‹ï¼›ç”¨è¿™äº›è‡ªåŠ¨æ ‡æ³¨è®­ç»ƒDenseDPOï¼Œæ€§èƒ½æ¥è¿‘ä½¿ç”¨äººç±»æ ‡æ³¨çš„æƒ…å†µã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­æœªè¯¦ç»†å±•å¼€å®éªŒæ•°æ®è¡¨æ ¼ç­‰ï¼Œä½†ä»è®ºè¿°å¯çŸ¥ï¼šDenseDPO åœ¨è¿åŠ¨ç”Ÿæˆèƒ½åŠ›ä¸Šå¤§å¹…è¶…è¶Š vanilla DPOï¼›åœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§ç­‰æŒ‡æ ‡ä¸Šèƒ½åŒ¹é… vanilla DPOï¼›åˆ©ç”¨é€šç”¨VLMsè‡ªåŠ¨æ ‡æ³¨è®­ç»ƒæ—¶ï¼Œæ€§èƒ½æ¥è¿‘äººç±»æ ‡æ³¨è®­ç»ƒçš„æ•ˆæœï¼Œä¸”æ•°æ®æ•ˆç‡æ›´é«˜ï¼ˆä»…éœ€1/3æ ‡æ³¨æ•°æ®å°±èƒ½å®ç°æ›´å¥½è¿åŠ¨ç”Ÿæˆï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºæ€è·¯ï¼šåœ¨éœ€è¦æ§åˆ¶å˜é‡å‡å°‘åå·®çš„åœºæ™¯ä¸‹ï¼Œå¯å‚è€ƒâ€œåŸºäºçœŸå®æ ·æœ¬çš„å˜ä½“ç”Ÿæˆâ€æ€è·¯æ¥æ„é€ å¯¹æ¯”æ•°æ®ï¼Œé¿å…ç‹¬ç«‹ç”Ÿæˆå¸¦æ¥çš„åå·®é—®é¢˜ã€‚  
2. ç»†ç²’åº¦ç›‘ç£ï¼šå¯¹äºæœ‰æ—¶é—´ã€ç©ºé—´ç­‰ç»´åº¦çš„ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚è§†é¢‘ã€é•¿æ–‡æœ¬ç­‰ï¼‰ï¼Œå¯è€ƒè™‘å°†æ•´ä½“ç›‘ç£æ‹†è§£ä¸ºç»†ç²’åº¦çš„å±€éƒ¨ç›‘ç£ï¼Œæå‡å­¦ä¹ ä¿¡å·è´¨é‡ã€‚  
3. è‡ªåŠ¨æ ‡æ³¨æ¢ç´¢ï¼šå±•ç¤ºäº†é€šç”¨å¤§æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡è‡ªåŠ¨æ ‡æ³¨ä¸Šçš„æ½œåŠ›ï¼Œä¸ºå‡å°‘äººå·¥æ ‡æ³¨æˆæœ¬ã€æ‹“å±•æ•°æ®è§„æ¨¡æä¾›äº†å‚è€ƒæ–¹å‘ï¼Œåç»­å¯æ¢ç´¢æ›´å¤šé¢†åŸŸä¸‹é€šç”¨æ¨¡å‹è¾…åŠ©æ ‡æ³¨çš„å¯èƒ½æ€§ã€‚

## aligning-vlm-assistants-with-personalized-situated-cognition
### Abstract
Vision-language models (VLMs) aligned with general human objectives, such as
being harmless and hallucination-free, have become valuable assistants of
humans in managing visual tasks. However, people with diversified backgrounds
have different cognition even in the same situation. Consequently, they may
have personalized expectations for VLM assistants. This highlights the urgent
need to align VLM assistants with personalized situated cognition for
real-world assistance. To study this problem, we first simplify it by
characterizing individuals based on the sociological concept of Role-Set. Then,
we propose to evaluate the individuals' actions to examine whether the
personalized alignment is achieved. Further, we construct a benchmark named
PCogAlignBench, which includes 18k instances and 20 individuals with different
Role-Sets. Finally, we present a framework called PCogAlign, which constructs a
cognition-aware and action-based reward model for personalized alignment.
Experimental results and human evaluations demonstrate the reliability of the
PCogAlignBench and the effectiveness of our proposed PCogAlign. We will
open-source the constructed benchmark and code at
https://github.com/NLPGM/PCogAlign.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©è§†è§‰è¯­è¨€æ¨¡å‹åŠ©æ‰‹é€‚é…ä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥ï¼šPCogAlignçš„æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é€šç”¨äººç±»ç›®æ ‡å¯¹é½ï¼ˆå¦‚æ— å®³ã€æ— å¹»è§‰ï¼‰æ–¹é¢å–å¾—è¿›å±•ï¼Œæˆä¸ºè§†è§‰ä»»åŠ¡çš„å¾—åŠ›åŠ©æ‰‹ã€‚ä½†ç°å®ä¸­ï¼Œä¸åŒèƒŒæ™¯çš„äººå³ä¾¿é¢å¯¹åŒä¸€æƒ…å¢ƒï¼Œè®¤çŸ¥å’Œå¯¹VLMåŠ©æ‰‹çš„æœŸæœ›ä¹Ÿå­˜åœ¨ä¸ªæ€§åŒ–å·®å¼‚ï¼ˆæ¯”å¦‚é¢å¯¹â€œæŸåçš„ç§‹åƒâ€ï¼Œå­©å­éœ€è¦å®‰æŠšå¼•å¯¼ã€ä¿®ç†å·¥éœ€è¦ä¸“ä¸šç»´ä¿®å»ºè®®ï¼‰ã€‚ç°æœ‰VLMåŠ©æ‰‹â€œä¸€åˆ€åˆ‡â€çš„å“åº”æ–¹å¼æ— æ³•æ»¡è¶³è¿™ç§ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œå› æ­¤**è®©VLMåŠ©æ‰‹ä¸ä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥å¯¹é½**æˆä¸ºç°å®è¾…åŠ©åœºæ™¯çš„è¿«åˆ‡éœ€æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºâ€œè§’è‰²é›†ï¼ˆRole - Setï¼‰â€ç®€åŒ–ä¸ªä½“è¡¨å¾  
å—ç¤¾ä¼šå­¦è§’è‰²ç†è®ºå¯å‘ï¼Œå°†ä¸ªä½“æŠ½è±¡ä¸ºâ€œRole@Locationâ€çš„é›†åˆï¼ˆå¦‚â€œChild@Home, Member@Communityâ€ï¼‰ã€‚ä¸åŒRole - Setå¯¹åº”ä¸åŒçš„ä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥ä¸å“åº”æœŸæœ›ï¼ŒæŠŠå¤æ‚çš„äººç±»å¤šæ ·æ€§ç®€åŒ–ä¸ºå¯æ“ä½œçš„è§’è‰²ç»„åˆï¼Œä¸ºåç»­ç ”ç©¶é”šå®šäº†åŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºPCogAlignBenchåŸºå‡†æµ‹è¯•é›†  
æ‰“é€ åŒ…å«1.8ä¸‡å®ä¾‹ã€è¦†ç›–20ç§ä¸åŒRole - Setçš„åŸºå‡†ã€‚å…·ä½“æ¥è¯´ï¼Œé€‰å®š8ä¸ªç¤¾äº¤åœºæ™¯ï¼Œä¸ºæ¯ä¸ªåœºæ™¯å®šä¹‰3 - 5ç§è§’è‰²ï¼Œå†é€šè¿‡ç»„åˆçº¦æŸå¾—åˆ°20ç§Role - Setï¼›è‡ªåŠ¨é‡‡é›†1.2ä¸‡è®­ç»ƒæ ·æœ¬ä¸6åƒæµ‹è¯•æ ·æœ¬ï¼Œæµ‹è¯•æ ·æœ¬ç»äººå·¥æ ‡æ³¨ç¡®ä¿è´¨é‡ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«Role - Setã€å›¾åƒã€é—®é¢˜ï¼Œæµ‹è¯•æ ·æœ¬è¿˜é™„å¸¦â€œé¢„æœŸä¸ªæ€§åŒ–å“åº”ç‰¹å¾â€çš„â€œoracle guidanceâ€ï¼Œä¸ºè¯„ä¼°ä¸ªæ€§åŒ–å¯¹é½æä¾›äº†æ ‡å‡†æ•°æ®é›†ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºPCogAlignæ¡†æ¶å®ç°ä¸ªæ€§åŒ–å¯¹é½  
è¯¥æ¡†æ¶åˆ†ä¸‰æ­¥ï¼šâ‘  ä¼°è®¡ä¸ªä½“çš„æƒ…å¢ƒè®¤çŸ¥ä¸æœ€ä¼˜è¡ŒåŠ¨ï¼›â‘¡ å€ŸåŠ©åä½œæ™ºèƒ½ä½“ç”Ÿæˆå¤šä¸ªä¸ªæ€§åŒ–å“åº”å€™é€‰ï¼›â‘¢ æ„å»ºâ€œè®¤çŸ¥æ„ŸçŸ¥ + è¡ŒåŠ¨å¯¼å‘â€çš„å¥–åŠ±æ¨¡å‹ï¼Œè¿­ä»£ç­›é€‰æœ€ä¼˜å“åº”ï¼Œè®©å“åº”æ—¢è´´åˆä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥ï¼Œåˆè¾…åŠ©ä¸ªä½“é‡‡å–æœ€ä¼˜è¡ŒåŠ¨ï¼Œè¿˜èƒ½ç”¨äºå¯¹é½è®­ç»ƒã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœä¸äººå·¥è¯„ä¼°å‡éªŒè¯äº†PCogAlignBenchçš„å¯é æ€§ï¼ˆæµ‹è¯•æ ·æœ¬è´¨é‡ã€åœºæ™¯è¦†ç›–åˆç†æ€§ç­‰ï¼‰ï¼Œä»¥åŠPCogAlignæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼ˆåœ¨å¤šè®¾ç½®ä¸‹å¯¹æ¯”åŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œèƒ½æ›´å¥½æ»¡è¶³ä¸åŒRole - Setçš„ä¸ªæ€§åŒ–æœŸæœ›ï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **é—®é¢˜å®šä¹‰åˆ›æ–°**ï¼šå°†ç¤¾ä¼šå­¦æ¦‚å¿µå¼•å…¥AIæ¨¡å‹å¯¹é½ä»»åŠ¡ï¼Œä¸ºâ€œä¸ªæ€§åŒ–â€è¿™ä¸€æŠ½è±¡éœ€æ±‚æ‰¾åˆ°å¯è½åœ°çš„ç®€åŒ–è¡¨å¾æ–¹å¼ï¼ˆRole - Setï¼‰ï¼Œæä¾›äº†è·¨å­¦ç§‘èåˆè§£å†³AIé—®é¢˜çš„æ€è·¯ã€‚  
2. **åŸºå‡†æ„å»ºèŒƒå¼**ï¼šPCogAlignBenchä»åœºæ™¯é€‰æ‹©ã€è§’è‰²å®šä¹‰åˆ°æ ·æœ¬é‡‡é›†ä¸è´¨é‡æ§åˆ¶çš„å…¨æµç¨‹ï¼Œä¸ºæ‰“é€ å‚ç›´é¢†åŸŸ/ç»†åˆ†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•é›†æä¾›äº†å‚è€ƒæ¨¡æ¿ã€‚  
3. **æ¡†æ¶è®¾è®¡é€»è¾‘**ï¼šPCogAlignâ€œè®¤çŸ¥ä¼°è®¡â†’å€™é€‰ç”Ÿæˆâ†’å¥–åŠ±ç­›é€‰â€çš„ä¸‰æ­¥æ³•ï¼Œä¸ºè§£å†³â€œä¸ªæ€§åŒ–å“åº”ç”Ÿæˆä¸å¯¹é½â€è¿™ç±»éœ€å…¼é¡¾å¤šç»´åº¦éœ€æ±‚çš„ä»»åŠ¡ï¼Œæä¾›äº†æ¨¡å—åŒ–ã€å¯å¤ç°çš„æŠ€æœ¯è·¯çº¿ã€‚

## driverx--a-vision-language-reasoning-model-for-cross-task-autonomous-driving
### Abstract
Autonomous driving requires real-time, robust reasoning across perception,
prediction, planning, and behavior. However, conventional end-to-end models
fail to generalize in complex scenarios due to the lack of structured
reasoning. Recent vision-language models (VLMs) have been applied to driving
tasks, but they typically rely on isolated modules and static supervision,
limiting their ability to support multi-stage decision-making. We present
AutoDriveRL, a unified training framework that formulates autonomous driving as
a structured reasoning process over four core tasks. Each task is independently
modeled as a vision-language question-answering problem and optimized using
task-specific reward models, enabling fine-grained reinforcement signals at
different reasoning stages. Within this framework, we train DriveRX, a
cross-task reasoning VLM designed for real-time decision-making. DriveRX
achieves strong performance on a public benchmark, outperforming GPT-4o in
behavior reasoning and demonstrating robustness under complex or corrupted
driving conditions. Our analysis further highlights the impact of vision
encoder design and reward-guided reasoning compression. We will release the
AutoDriveRL framework and the DriveRX model to support future research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DriveRXï¼šé¢å‘è·¨ä»»åŠ¡è‡ªåŠ¨é©¾é©¶çš„è§†è§‰-è¯­è¨€æ¨ç†æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªåŠ¨é©¾é©¶éœ€åœ¨æ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’å’Œè¡Œä¸ºç­‰æ–¹é¢è¿›è¡Œå®æ—¶ä¸”é²æ£’çš„æ¨ç†ï¼Œä½†ä¼ ç»Ÿç«¯åˆ°ç«¯æ¨¡å‹å› ç¼ºä¹ç»“æ„åŒ–æ¨ç†ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚è™½å·²æœ‰è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åº”ç”¨äºé©¾é©¶ä»»åŠ¡ï¼Œå´å¸¸ä¾èµ–å­¤ç«‹æ¨¡å—ä¸é™æ€ç›‘ç£ï¼Œé™åˆ¶äº†å¤šé˜¶æ®µå†³ç­–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç°æœ‰VLMsåº”ç”¨äºè‡ªåŠ¨é©¾é©¶è¿˜é¢ä¸´ä¸­é—´æ¨ç†é“¾ç¼ºå¤±ã€ä»»åŠ¡é—´å­¤ç«‹ã€ä¾èµ–é™æ€æç¤ºæ— è‡ªé€‚åº”åé¦ˆç­‰æŒ‘æˆ˜ï¼Œè€Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€äº›æ–¹æ³•ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†æ€è·¯ï¼Œå¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°åŠ¨æ€åé¦ˆç­‰ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶äººå‘˜å¼€å‘ç»Ÿä¸€çš„è§†è§‰ - è¯­è¨€æ¡†æ¶æ¥æ”¯æŒè‡ªåŠ¨é©¾é©¶æ ¸å¿ƒä»»åŠ¡çš„ç»“æ„åŒ–æ¨ç†ã€å¼ºåŒ–å­¦ä¹ å’Œå¤šä»»åŠ¡åè°ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAutoDriveRLç»Ÿä¸€è®­ç»ƒæ¡†æ¶  
å°†è‡ªåŠ¨é©¾é©¶æ„å»ºä¸ºè·¨è¶Šæ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’ã€è¡Œä¸ºå››ä¸ªæ ¸å¿ƒä»»åŠ¡çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæ¯ä¸ªä»»åŠ¡ç‹¬ç«‹å»ºæ¨¡ä¸ºè§†è§‰ - è¯­è¨€é—®ç­”ï¼ˆVQAï¼‰é—®é¢˜ã€‚åŒæ—¶å¼•å…¥ä»»åŠ¡ç‰¹å®šå¥–åŠ±æ¨¡å‹ï¼Œåœ¨è®­ç»ƒæ—¶æä¾›ç»†ç²’åº¦çš„é˜¶æ®µçº§å¼ºåŒ–ä¿¡å·ï¼Œä»å…¨å±€è§†è§’å®ç°æ¨ç†æ­¥éª¤çš„è¿‡ç¨‹çº§åé¦ˆï¼Œæ”¯æŒå¯è§£é‡Šçš„ä¸­é—´æ¨ç†ã€ä¿ƒè¿›è·¨ä»»åŠ¡ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶å†³ç­–ä¸­çš„ç¨³å®šæ€§ä¸æ•´ä½“æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒDriveRXè·¨ä»»åŠ¡æ¨ç†æ¨¡å‹  
åœ¨AutoDriveRLæ¡†æ¶ä¸‹è®­ç»ƒDriveRXï¼Œè¯¥æ¨¡å‹èšç„¦è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨ç†ä¸è·¨ä»»åŠ¡æ³›åŒ–ã€‚å®ƒèƒ½åœ¨æ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’å’Œè¡Œä¸ºç­‰å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰åŸºå‡†è®¾ç½®ä¸­éƒ½èƒ½è¶…è¶Šç°æœ‰æ¨¡å‹ï¼Œä¸”åœ¨å¤æ‚é©¾é©¶æ¡ä»¶ä¸‹è¾“å‡ºæ›´ç´§å‡‘æ¨ç†è½¨è¿¹ä¸æ›´å¿«å“åº”ï¼Œå¥‘åˆè‡ªåŠ¨é©¾é©¶å®æ—¶æ€§è¦æ±‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…¬å¼€åŸºå‡†DriveBenchä¸Šï¼ŒDriveRXåœ¨è¡Œä¸ºä»»åŠ¡ä¸­å¾—åˆ†ä¸º62.02ï¼Œè¶…è¿‡GPT - 4oçš„55.04ï¼›åœ¨DriveBench - Corruptionå­é›†ä¸Šï¼ŒDriveRXè¾¾åˆ°55.24ï¼Œä¹Ÿä¼˜äºGPT - 4oçš„53.97ã€‚åŒæ—¶ï¼ŒDriveRXåœ¨é›¨ã€é®æŒ¡ã€å…‰ç…§å˜åŒ–ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹èƒ½ä¿æŒç¨³å®šè¾“å‡ºï¼Œå±•ç°å‡ºå¼ºé²æ£’æ€§ä¸æ³›åŒ–æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡å±‚é¢ï¼šAutoDriveRLå°†å¤æ‚è‡ªåŠ¨é©¾é©¶ä»»åŠ¡åˆ†è§£ä¸ºæ ¸å¿ƒå­ä»»åŠ¡å¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ä»»åŠ¡ç‰¹å®šå¥–åŠ±æ¨¡å‹çš„æ€è·¯ï¼Œä¸ºå¤šä»»åŠ¡åœºæ™¯ä¸‹çš„æ¨¡å‹è®­ç»ƒæä¾›äº†ç»“æ„åŒ–ç›‘ç£ä¸åé¦ˆçš„èŒƒä¾‹ï¼Œå¯å€Ÿé‰´åˆ°å…¶ä»–éœ€å¤šé˜¶æ®µå†³ç­–ã€è·¨ä»»åŠ¡åè°ƒçš„é¢†åŸŸã€‚
2. æ¨¡å‹ä¼˜åŒ–å±‚é¢ï¼šDriveRXåœ¨è¿½æ±‚å®æ—¶æ€§ä¸æ¨ç†ç´§å‡‘æ€§ä¸Šçš„è®¾è®¡ï¼Œå¯¹äºå®æ—¶æ€§è¦æ±‚é«˜çš„AIåº”ç”¨ï¼ˆå¦‚æ™ºèƒ½æœºå™¨äººäº¤äº’ç­‰ï¼‰æœ‰å‚è€ƒä»·å€¼ï¼Œæç¤ºåœ¨æ¨¡å‹å¼€å‘ä¸­éœ€å…³æ³¨æ¨ç†æ•ˆç‡ä¸è¾“å‡ºç®€æ´æ€§ã€‚
3. åˆ†æå±‚é¢ï¼šè®ºæ–‡å¯¹è®­ç»ƒåŠ¨æ€ã€è§†è§‰ç¼–ç å™¨ç“¶é¢ˆã€å¥–åŠ±å¼•å¯¼çš„æ¨ç†å‹ç¼©ç­‰æ–¹é¢çš„æ·±å…¥åˆ†æï¼Œä¸ºåç»­æ¨¡å‹æ”¹è¿›ä¸ä¼˜åŒ–æŒ‡æ˜äº†æ–¹å‘ï¼Œè®©ç ”ç©¶è€…æ˜ç™½åœ¨æ¨¡å‹è¿­ä»£æ—¶éœ€å…³æ³¨å“ªäº›å…³é”®å› ç´ æ¥æå‡æ€§èƒ½ã€‚

## enhance-mobile-agents-thinking-process-via-iterative-preference-learning
### Abstract
The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to
improve the reasoning performance of VLM-based mobile agents in GUI tasks.
However, the scarcity of diverse CoaT trajectories limits the expressiveness
and generalization ability of such agents. While self-training is commonly
employed to address data scarcity, existing approaches either overlook the
correctness of intermediate reasoning steps or depend on expensive
process-level annotations to construct process reward models (PRM). To address
the above problems, we propose an Iterative Preference Learning (IPL) that
constructs a CoaT-tree through interative sampling, scores leaf nodes using
rule-based reward, and backpropagates feedback to derive Thinking-level Direct
Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up
supervised fine-tuning, we further introduce a three-stage instruction
evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real
mobile UI screenshots, enhancing both generality and layout understanding.
Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our
agent MobileIPL outperforms strong baselines, including continual pretraining
models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance
across three standard Mobile GUI-Agents benchmarks and shows strong
generalization to out-of-domain scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¿­ä»£åå¥½å­¦ä¹ åŠ©åŠ›ç§»åŠ¨æ™ºèƒ½ä½“æ€ç»´å‡çº§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç§»åŠ¨æ™ºèƒ½ä½“åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºä¸ç•Œé¢äº¤äº’ã€è‡ªä¸»æ‰§è¡Œæ—¥å¸¸ä»»åŠ¡çš„æ½œåŠ›ï¼ŒChain of Action - Planning Thoughtsï¼ˆCoaTï¼‰èŒƒå¼èƒ½æå‡è¿™ç±»æ™ºèƒ½ä½“æ¨ç†æ€§èƒ½ï¼Œä½†å¤šæ ·CoaTè½¨è¿¹ç¨€ç¼ºé™åˆ¶äº†æ™ºèƒ½ä½“è¡¨è¾¾ä¸æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰è‡ªè®­ç»ƒæ–¹æ³•è¦ä¹ˆå¿½è§†ä¸­é—´æ¨ç†æ­¥éª¤æ­£ç¡®æ€§ï¼Œè¦ä¹ˆä¾èµ–æ˜‚è´µè¿‡ç¨‹çº§æ ‡æ³¨æ„å»ºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼›ä¸”ç›´æ¥åœ¨CoaTè½¨è¿¹ä¸Šåšç›‘ç£å¾®è°ƒæ˜“è¿‡æ‹Ÿåˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºæ–°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¿­ä»£åå¥½å­¦ä¹ ï¼ˆIPLï¼‰æ¡†æ¶
æ„å»ºCoaT - treeï¼šåŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œäº¤äº’å¼é‡‡æ ·æ„å»ºCoaT - treeï¼Œæ ‘ä¸­æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”æ¨ç†æ­¥éª¤çš„é‡‡æ ·å“åº”ï¼Œæ•æ‰å¤šæ ·æ¨ç†è·¯å¾„ã€‚å¥–åŠ±åˆ†é…ä¸åå‘ä¼ æ’­ï¼šç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ç»™å¶èŠ‚ç‚¹æ‰“åˆ†ï¼Œå†å°†å¥–åŠ±ä¿¡å·æ²¿CoaT - treeåå‘ä¼ æ’­åˆ°æ›´æ—©æ¨ç†æ­¥éª¤ï¼ŒåŸºäºæ­¤æ„å»ºThinking - level Direct Preference Optimizationï¼ˆT - DPOï¼‰å¯¹ï¼Œä¼˜åŒ–æœ€ç»ˆåŠ¨ä½œä¸æ•´ä½“æ¨ç†è´¨é‡ï¼Œæ—¢å…³æ³¨ä¸­é—´æ­¥éª¤åˆæ— éœ€æ˜‚è´µæ ‡æ³¨ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰é˜¶æ®µæŒ‡ä»¤æ¼”åŒ–ç­–ç•¥
åˆ©ç”¨GPT - 4oåŸºäºçœŸå®ç§»åŠ¨UIæˆªå›¾ç”Ÿæˆå¤šæ ·é—®ç­”å¯¹ï¼Œä¸€æ˜¯å¼•å…¥å¤šæ ·æ¨ç†ä¸Šä¸‹æ–‡é˜²æ­¢æ™ºèƒ½ä½“åœ¨çƒ­èº«ç›‘ç£å¾®è°ƒæ—¶è¿‡æ‹Ÿåˆåˆ°é™æ€ä¸‹æ¸¸æŒ‡ä»¤ï¼›äºŒæ˜¯é€šè¿‡è§†è§‰ grounded é—®ç­”æå‡æ™ºèƒ½ä½“å¯¹UIå¸ƒå±€çš„ç†è§£ï¼Œå¢å¼ºæ³›åŒ–æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸‰ä¸ªæ ‡å‡†ç§»åŠ¨GUIæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ï¼ˆAITZã€AMEXã€AndroidControlï¼‰ä¸Šè¯„ä¼°ï¼Œæ‰€æMobileIPLæ™ºèƒ½ä½“è¶…è¶Šå¼ºåŸºçº¿ï¼ˆå¦‚OS - ATLASã€UI - TARSç­‰æŒç»­é¢„è®­ç»ƒæ¨¡å‹ï¼‰ï¼Œåœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šè¾¾å½“å‰æœ€ä¼˜æ€§èƒ½ï¼›åœ¨AndroidControlæ•°æ®é›†ä¸Šå±•ç°å¯¹æœªè§è¿‡åº”ç”¨å’ŒæŒ‡ä»¤çš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼›æœ‰é™è®­ç»ƒèµ„æºä¸‹ï¼ŒIPLæ¯”naive - DPOè¡¨ç°å¥½ï¼Œä¸€è½®è¿­ä»£è®­ç»ƒç”¨ä¸€åŠæ•°æ®æ—¶æ€§èƒ½é«˜4.5%ï¼Œä¸¤è½®è¿­ä»£ç”¨äº”åˆ†ä¹‹ä¸€æ•°æ®æ—¶é«˜0.3%ï¼›åˆ†æå®éªŒæ˜¾ç¤ºæŒ‡ä»¤æ¼”åŒ–åŒæ—¶æå‡æ¨ç†å¤šæ ·æ€§ä¸è´¨é‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤„ç†æ•°æ®ç¨€ç¼ºä¸ä¸­é—´æ­¥éª¤å…³æ³¨é—®é¢˜æ—¶ï¼ŒIPLçš„CoaT - treeæ„å»ºã€åŸºäºè§„åˆ™å¥–åŠ±åå‘ä¼ æ’­å½¢æˆT - DPOå¯¹çš„æ€è·¯ï¼Œä¸ºæ¨¡å‹æ¨ç†ä¼˜åŒ–æä¾›äº†ä¸ä¾èµ–æ˜‚è´µæ ‡æ³¨ä¸”å…³æ³¨ä¸­é—´è¿‡ç¨‹çš„æ–°èŒƒå¼ï¼Œå¯å€Ÿé‰´åˆ°éœ€ä¸­é—´æ¨ç†æ­¥éª¤ä¼˜åŒ–çš„ä»»åŠ¡åœºæ™¯ã€‚
2. æŒ‡ä»¤æ¼”åŒ–ç­–ç•¥åˆ©ç”¨å¤–éƒ¨å¼ºå¤§æ¨¡å‹ç”Ÿæˆå¤šæ ·æ•°æ®æ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€å¢å¼ºæ³›åŒ–ä¸ç‰¹å®šé¢†åŸŸç†è§£ï¼Œåœ¨æ¨¡å‹å¾®è°ƒé˜¶æ®µæ•°æ®å¢å¼ºã€æ³›åŒ–èƒ½åŠ›æå‡æ–¹é¢æä¾›äº†å‚è€ƒï¼Œå¯ç”¨äºå…¶ä»–éœ€åº”å¯¹è¿‡æ‹Ÿåˆä¸é¢†åŸŸç†è§£çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡ã€‚
3. åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œä¸”å¯¹æ¯”å¤šç§å¼ºåŸºçº¿ï¼Œè¿™ç§å…¨é¢çš„å®éªŒéªŒè¯æ€è·¯å€¼å¾—ç§‘ç ”ä¸­å€Ÿé‰´ï¼Œä»¥å……åˆ†è¯´æ˜æ–¹æ³•ä¼˜åŠ¿ã€‚

## skywork-vl-reward--an-effective-reward-model-for-multimodal-understanding-and-reasoning
### Abstract
We propose Skywork-VL Reward, a multimodal reward model that provides reward
signals for both multimodal understanding and reasoning tasks. Our technical
approach comprises two key components: First, we construct a large-scale
multimodal preference dataset that covers a wide range of tasks and scenarios,
with responses collected from both standard vision-language models (VLMs) and
advanced VLM reasoners. Second, we design a reward model architecture based on
Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage
fine-tuning using pairwise ranking loss on pairwise preference data.
Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art
results on multimodal VL-RewardBench and exhibits competitive performance on
the text-only RewardBench benchmark. Furthermore, preference data constructed
based on our Skywork-VL Reward proves highly effective for training Mixed
Preference Optimization (MPO), leading to significant improvements in
multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as
a significant advancement toward general-purpose, reliable reward models for
multimodal alignment. Our model has been publicly released to promote
transparency and reproducibility.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-VL Rewardï¼šå¤šæ¨¡æ€ç†è§£ä¸æ¨ç†çš„é«˜æ•ˆå¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è®©å…¶è¡Œä¸ºä¸äººç±»åå¥½å¯¹é½ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æ˜¯è§£å†³è¯¥é—®é¢˜çš„å…³é”®ç»„ä»¶ï¼Œç„¶è€Œå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„å‘å±•å°šå¤„æ—©æœŸï¼Œå­˜åœ¨ä¸¤å¤§å±€é™ï¼šç°æœ‰å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­æ³›åŒ–æ€§ä¸è¶³ï¼Œä¸”éš¾ä»¥æœ‰æ•ˆè¯„ä¼°å…·å¤‡å¤æ‚æ¨ç†èƒ½åŠ›çš„å…ˆè¿›VLMæ¨ç†å™¨ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦èƒ½åœ¨å¤šæ ·é¢†åŸŸå’Œä»»åŠ¡ä¸­è¯„ä¼°æ ‡å‡†VLMsä¸å…ˆè¿›VLMæ¨ç†å™¨è¾“å‡ºçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€åå¥½æ•°æ®é›†  
æ•´åˆå¤šä¸ªå¼€æºåå¥½æ•°æ®é›†ä¸å†…éƒ¨æ ‡æ³¨æ¥æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–LLaVA-Critic-113kã€Skywork-Reward-Preference-80K-v0.2ã€RLAIF-V-Datasetç­‰æ¥æºã€‚è¿™äº›æ•°æ®è¦†ç›–ä»åŸºç¡€å›¾åƒæè¿°åˆ°å¤æ‚æ¨ç†åœºæ™¯çš„ä»»åŠ¡ï¼Œå€™é€‰å“åº”æ¥è‡ªæ ‡å‡†VLMså’Œå…ˆè¿›VLMæ¨ç†å™¨ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›ä¸°å¯Œå¤šæ ·çš„åå¥½å¯¹æ¯”æ•°æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡åŸºäºQwen2.5-VL-7B-Instructçš„å¥–åŠ±æ¨¡å‹æ¶æ„ä¸è®­ç»ƒèŒƒå¼  
ä»¥Qwen2.5-VL-7B-Instructä¸ºåŸºç¡€æ¨¡å‹ï¼Œé›†æˆå¥–åŠ±å¤´ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œåœ¨æˆå¯¹åå¥½æ•°æ®ä¸Šä½¿ç”¨æˆå¯¹æ’åºæŸå¤±è¿›è¡Œå¤šé˜¶æ®µå¾®è°ƒã€‚ç»“åˆçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®è®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„æ³›åŒ–æ€§ä¸æ€§èƒ½ï¼Œä½¿æ¨¡å‹èƒ½è¾“å‡ºä¸äººç±»åå¥½ä¸€è‡´çš„æ ‡é‡åˆ†æ•°ï¼Œæœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹è¾“å‡ºã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
Skywork-VL Rewardåœ¨å¤šæ¨¡æ€VL-RewardBenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—å½“å‰æœ€ä¼˜ï¼ˆSOTAï¼‰ç»“æœï¼›åœ¨çº¯æ–‡æœ¬çš„RewardBenchåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå±•ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒåŸºäºSkywork-VL Rewardæ„å»ºçš„åå¥½æ•°æ®ç”¨äºè®­ç»ƒæ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ—¶æ•ˆæœæ˜¾è‘—ï¼Œèƒ½å¤§å¹…æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œå……åˆ†è¯æ˜è¯¥å¥–åŠ±æ¨¡å‹åœ¨å¤šæ¨¡æ€å¯¹é½æ–¹å‘è¿ˆå‘é€šç”¨ã€å¯é å¥–åŠ±æ¨¡å‹çš„é‡å¤§è¿›å±•ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æ•°æ®é›†æ„å»ºä¸Šï¼Œé€šè¿‡æ•´åˆå¤šæ¥æºæ•°æ®ï¼ˆå¼€æº+å†…éƒ¨æ ‡æ³¨ï¼‰ã€è¦†ç›–å¤šä»»åŠ¡åœºæ™¯æ¥æ‰“é€ é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œä¸ºæ¨¡å‹æ€§èƒ½å¥ åŸºï¼›æ¨¡å‹æ¶æ„ä¸è®­ç»ƒä¸Šï¼ŒåŸºäºå¼ºåŸºç¡€æ¨¡å‹è®¾è®¡å¥–åŠ±å¤´å¹¶é‡‡ç”¨å¤šé˜¶æ®µå¾®è°ƒã€ç»“åˆå¤šç±»å‹æ•°æ®è®­ç»ƒçš„èŒƒå¼ï¼Œæå‡æ¨¡å‹æ³›åŒ–ä¸è¯„ä¼°èƒ½åŠ›ï¼›åœ¨åº”ç”¨æ‹“å±•ä¸Šï¼Œå±•ç¤ºäº†å¥–åŠ±æ¨¡å‹ç”Ÿæˆçš„åå¥½æ•°æ®å¯¹MPOè®­ç»ƒçš„å®ç”¨ä»·å€¼ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡æä¾›æ–°æ€è·¯ï¼Œä¸”æ¨¡å‹å¼€æºä¿ƒè¿›äº†é¢†åŸŸé€æ˜æ€§ä¸å¯å¤ç°æ€§ã€‚

## trend--tri-teaching-for-robust-preference-based-reinforcement-learning-with-demonstrations
### Abstract
Preference feedback collected by human or VLM annotators is often noisy,
presenting a significant challenge for preference-based reinforcement learning
that relies on accurate preference labels. To address this challenge, we
propose TREND, a novel framework that integrates few-shot expert demonstrations
with a tri-teaching strategy for effective noise mitigation. Our method trains
three reward models simultaneously, where each model views its small-loss
preference pairs as useful knowledge and teaches such useful pairs to its peer
network for updating the parameters. Remarkably, our approach requires as few
as one to three expert demonstrations to achieve high performance. We evaluate
TREND on various robotic manipulation tasks, achieving up to 90% success rates
even with noise levels as high as 40%, highlighting its effective robustness in
handling noisy preference feedback. Project page:
https://shuaiyihuang.github.io/publications/TREND.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TRENDï¼šå°æ ·æœ¬ä¸“å®¶ç¤ºèŒƒ+ä¸‰æ¨¡å‹äº’æ•™ï¼Œè®©åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ æ›´é²æ£’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œè®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡äººç±»æˆ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æä¾›çš„åå¥½åé¦ˆæ¥æ›¿ä»£æ˜¾å¼å¥–åŠ±å‡½æ•°ï¼Œè™½æœ‰ä¼˜åŠ¿ä½†åå¥½æ ‡ç­¾æ˜“å—å™ªå£°å¹²æ‰°ï¼ˆæ¯”å¦‚äººç±»åé¦ˆæœ‰åå·®ã€VLMéš¾å¤„ç†å¤æ‚è§†è§‰/æ–‡æœ¬/æ—¶åºä¿¡æ¯ï¼‰ã€‚å·²æœ‰ç ”ç©¶è¡¨æ˜å“ªæ€•10%çš„æ ‡ç­¾å™ªå£°éƒ½ä¼šä¸¥é‡å½±å“æ€§èƒ½ï¼Œè€ŒVLMç”Ÿæˆçš„åå¥½æ ‡ç­¾å™ªå£°ç‡ç”šè‡³èƒ½é«˜è¾¾40%ï¼Œè¿™å¯¹ä¾èµ–å‡†ç¡®åå¥½æ ‡ç­¾çš„PbRLæ¥è¯´æ˜¯å·¨å¤§æŒ‘æˆ˜ï¼Œå› æ­¤äºŸéœ€é²æ£’çš„æ–¹æ³•æ¥å¤„ç†å¸¦å™ªåå¥½åé¦ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºTRENDæ¡†æ¶ï¼Œèåˆå°æ ·æœ¬ä¸“å®¶ç¤ºèŒƒä¸ä¸‰æ•™ï¼ˆtri - teachingï¼‰ç­–ç•¥  
TRENDä¸€æ–¹é¢åœ¨é¢„è®­ç»ƒå’Œåœ¨çº¿PbRLé€‚é…é˜¶æ®µèå…¥å°‘é‡ï¼ˆ1 - 3ä¸ªï¼‰ä¸“å®¶ç¤ºèŒƒï¼Œä¸ºç­–ç•¥æä¾›å¼ºåˆå§‹åŒ–ï¼Œä¿è¯åœ¨çº¿å­¦ä¹ æ—¶è‡³å°‘éƒ¨åˆ†æ•°æ®æ— å™ªå£°ï¼›å¦ä¸€æ–¹é¢è®¾è®¡ä¸‰æ•™ç­–ç•¥ï¼Œè®©ä¸‰ä¸ªå¥–åŠ±æ¨¡å‹ååŒå·¥ä½œã€‚æ¯ä¸ªæ¨¡å‹å°†è‡ªèº«å°æŸå¤±çš„åå¥½å¯¹è§†ä¸ºâ€œå¹²å‡€çŸ¥è¯†â€ï¼Œå¹¶æŠŠè¿™äº›æœ‰ç”¨å¯¹ä¼ é€’ç»™å…¶ä»–æ¨¡å‹ç”¨äºå‚æ•°æ›´æ–°ï¼Œä»¥æ­¤å‡è½»å™ªå£°å½±å“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰æ•™ç­–ç•¥å®ç°åŠ¨æ€æ¸…æ´æ ·æœ¬é€‰æ‹©  
ä¸åŒäºç°æœ‰ç®€å•å¹³å‡å¤šå¥–åŠ±æ¨¡å‹é¢„æµ‹æ¥é€‰æ ·æœ¬çš„æ–¹æ³•ï¼ŒTRENDä¸­ä¸‰ä¸ªå¥–åŠ±æ¨¡å‹å„è‡ªç‹¬ç«‹å­¦ä¹ æ ·æœ¬é€‰æ‹©çš„â€œä¸“é•¿â€ã€‚æ¯ä¸ªæ¨¡å‹é€šè¿‡è®¡ç®—åå¥½å¯¹çš„æŸå¤±ï¼ŒæŠŠå°æŸå¤±çš„åå¥½å¯¹æ ‡è®°ä¸ºå¯èƒ½å¹²å‡€çš„æ ·æœ¬ï¼Œå†ç”¨è¿™äº›æ ·æœ¬æ›´æ–°å…¶ä»–æ¨¡å‹ã€‚è¿™æ ·æ¸…æ´æ ·æœ¬çš„å®šä¹‰ç”±æ¨¡å‹é—´åŠ¨æ€å­¦ä¹ è€Œæ¥ï¼Œè€Œéå›ºå®šè§„åˆ™ï¼Œå¢å¼ºäº†å¯¹å™ªå£°çš„é²æ£’æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Meta - worldçš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼ˆå¦‚Button - Pressã€Drawer - Openã€Hammerç­‰ï¼‰ä¸Šè¯„ä¼°TRENDï¼š  
- å³ä¾¿åœ¨40%çš„é«˜å™ªå£°æ°´å¹³ä¸‹ï¼ŒTRENDä»èƒ½å–å¾—ä¼˜å¼‚æˆç»©ï¼ŒæˆåŠŸç‡å…ˆåœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šæå‡æ˜æ˜¾ï¼ˆå¦‚Button - Pressä»»åŠ¡è¾ƒåŸºçº¿æå‡è¿‘40%ã€Drawer - Openæå‡60%ã€Hammeræå‡70%å·¦å³ï¼‰ï¼›  
- é¢å¯¹VLMç”Ÿæˆçš„å¸¦å™ªåå¥½æ ‡ç­¾æ—¶ï¼Œåœ¨Drawer - Openä»»åŠ¡ä¸ŠæˆåŠŸå¾‹æå‡è¶…40%ï¼›  
- ä»…ç”¨1 - 3ä¸ªä¸“å®¶ç¤ºèŒƒå°±èƒ½å®ç°é«˜æ€§èƒ½ï¼Œåœ¨40%é«˜å™ªå£°ä¸‹ä¹Ÿèƒ½è¾¾åˆ°çº¦80%çš„æˆåŠŸç‡ã€‚æ•´ä½“ä¸Šåœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹ï¼ŒTRENDæŒç»­å¤§å¹…è¶…è¶Šç°æœ‰PbRLåŸºçº¿æ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡å‹ååŒä¸åŠ¨æ€æ¸…æ´æ ·æœ¬é€‰æ‹©æ€è·¯ï¼šå½“é¢ä¸´å¸¦å™ªæ•°æ®åœºæ™¯æ—¶ï¼Œå¯å‚è€ƒè¿™ç§è®©å¤šä¸ªæ¨¡å‹äº’ç›¸â€œæ•™å­¦â€ã€åŠ¨æ€åˆ¤æ–­æœ‰æ•ˆæ ·æœ¬çš„æ–¹å¼ï¼Œé¿å…å•ä¸€æ¨¡å‹å¯¹å™ªå£°ä¼°è®¡çš„åå·®ï¼Œæå‡é²æ£’æ€§ï¼›  
2. å°æ ·æœ¬ä¸“å®¶ç¤ºèŒƒçš„èåˆï¼šåœ¨æ•°æ®ç¨€ç¼ºæˆ–å™ªå£°æé«˜æ—¶ï¼Œå¼•å…¥å°‘é‡ä¸“å®¶ç¤ºèŒƒåšåˆå§‹åŒ–æˆ–è¾…åŠ©è®­ç»ƒï¼Œèƒ½ä¸ºæ¨¡å‹å­¦ä¹ æä¾›å¯é é”šç‚¹ï¼Œè¿™ç§â€œå°æ ·æœ¬å€ŸåŠ›â€æ€è·¯åœ¨å…¶ä»–ä¾èµ–å¸¦å™ªåé¦ˆæˆ–æ•°æ®ç¨€ç¼ºçš„å­¦ä¹ ä»»åŠ¡ä¸­ä¹Ÿæœ‰å€Ÿé‰´ä»·å€¼ï¼›  
3. åº”å¯¹VLMç”Ÿæˆå™ªå£°çš„æ–¹æ¡ˆï¼šåœ¨ç»“åˆVLMsåšåå¥½ç”Ÿæˆç­‰ä»»åŠ¡æ—¶ï¼ŒTRENDå¤„ç†å™ªå£°çš„æ–¹å¼ä¸ºæå‡è¿™ç±»ç»“åˆæ–¹æ¡ˆçš„ç¨³å®šæ€§æä¾›äº†å‚è€ƒï¼Œå¯ç”¨äºä¼˜åŒ–ä¾èµ–VLMåé¦ˆçš„RLæˆ–å…¶ä»–å­¦ä¹ ä»»åŠ¡æµç¨‹ã€‚

## prism--projection-based-reward-integration-for-scene-aware-real-to-sim-to-real-transfer-with-few-demonstrations
### Abstract
Learning from few demonstrations to develop policies robust to variations in
robot initial positions and object poses is a problem of significant practical
interest in robotics. Compared to imitation learning, which often struggles to
generalize from limited samples, reinforcement learning (RL) can autonomously
explore to obtain robust behaviors. Training RL agents through direct
interaction with the real world is often impractical and unsafe, while building
simulation environments requires extensive manual effort, such as designing
scenes and crafting task-specific reward functions. To address these
challenges, we propose an integrated real-to-sim-to-real pipeline that
constructs simulation environments based on expert demonstrations by
identifying scene objects from images and retrieving their corresponding 3D
models from existing libraries. We introduce a projection-based reward model
for RL policy training that is supervised by a vision-language model (VLM)
using human-guided object projection relationships as prompts, with the policy
further fine-tuned using expert demonstrations. In general, our work focuses on
the construction of simulation environments and RL-based policy training,
ultimately enabling the deployment of reliable robotic control policies in
real-world scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PRISMï¼šå°‘æ ·æœ¬åœºæ™¯æ„ŸçŸ¥ä¸‹è™šå®è¿ç§»çš„æŠ•å½±å¥–åŠ±é›†æˆæ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æœºå™¨äººå­¦ä¸­ï¼Œä»å°‘é‡æ¼”ç¤ºä¸­å­¦ä¹ å‡ºèƒ½åº”å¯¹æœºå™¨äººåˆå§‹ä½å§¿å’Œç‰©ä½“å§¿æ€å˜åŒ–çš„é²æ£’ç­–ç•¥æ˜¯æå…·å®é™…æ„ä¹‰çš„é—®é¢˜ã€‚æ¨¡ä»¿å­¦ä¹ éš¾ä»¥ä»æœ‰é™æ ·æœ¬æ³›åŒ–ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½èƒ½è‡ªä¸»æ¢ç´¢è·å¾—é²æ£’è¡Œä¸ºï¼Œä½†ç›´æ¥åœ¨çœŸå®ä¸–ç•Œè®­ç»ƒRLæ™ºèƒ½ä½“ä¸åˆ‡å®é™…ä¸”ä¸å®‰å…¨ï¼›æ„å»ºä»¿çœŸç¯å¢ƒåˆéœ€å¤§é‡äººå·¥å·¥ä½œï¼ˆå¦‚åœºæ™¯è®¾è®¡ã€å®šåˆ¶ä»»åŠ¡å¥–åŠ±å‡½æ•°ï¼‰ã€‚åŒæ—¶ï¼Œç°æœ‰è™šå®è¿ç§»æ–¹æ³•å¤šèšç„¦é«˜ä¿çœŸä»¿çœŸç¯å¢ƒé‡å»ºï¼Œå¯¹ä»¿çœŸä¸­å¦‚ä½•å¼€å±•RLä»¥å¼¥åˆè™šå®å·®è·å…³æ³¨ä¸è¶³ï¼Œä¸”ä¾èµ–æ‰‹å·¥å¥–åŠ±å‡½æ•°ã€æœªå……åˆ†åˆ©ç”¨å°‘é‡çœŸå®æ•°æ®ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºPRISMæ–¹æ¡ˆæ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡è™šå®è¿ç§» pipeline  
æ„å»ºè§†è§‰å’Œå‡ ä½•ä¸€è‡´çš„ä»¿çœŸç¯å¢ƒï¼šåˆ©ç”¨3Dæ¨¡å‹åº“ï¼ŒåŸºäºçœŸå®åœºæ™¯å›¾åƒæ„å»ºä»¿çœŸç¯å¢ƒï¼›å¹¶é€šè¿‡ä¸ä¸“å®¶æ¼”ç¤ºå…±è®­ç»ƒä»¥åŠåŠ¨ä½œå¯è¡Œæ€§é¢„æµ‹å™¨ï¼Œå°†ç­–ç•¥è¿ç§»åˆ°çœŸå®ä¸–ç•Œï¼Œè§£å†³äº†ä»¿çœŸç¯å¢ƒæ„å»ºä¸ç­–ç•¥è¿ç§»éš¾é¢˜ï¼Œå‡å°‘é¢†åŸŸå·®å¼‚ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºåŸºäºæŠ•å½±çš„å¥–åŠ±æ¨¡å‹  
æŠŠäººç±»å¼•å¯¼çš„ç‰©ä½“æŠ•å½±å…³ç³»ï¼ˆä»äººç±»è§†è§’å› ç›¸äº’é®æŒ¡äº§ç”Ÿçš„ç‰©ä½“2DæŠ•å½±çš„è§†ç‚¹ç›¸å…³æ’åºï¼‰ä½œä¸ºæç¤ºï¼Œä»è§†è§‰ - è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMsï¼‰è¿ç§»åˆ°RLä¸­ï¼Œä¸ºRLç­–ç•¥è®­ç»ƒæä¾›é«˜æ•ˆå¥–åŠ±ç›‘ç£ã€‚è¿˜åˆ©ç”¨ä»¿çœŸä¸­çš„å¤šè§†è§’è§‚æµ‹éªŒè¯VLMæ¨æ–­å¥–åŠ±åœ¨ä¸åŒè§†è§’çš„ä¸€è‡´æ€§ï¼Œå‡è½»VLMè¯¯åˆ¤å½±å“ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå……åˆ†åˆ©ç”¨å°‘é‡ä¸“å®¶æ¼”ç¤º  
åŸºäºçœŸå®åœºæ™¯å›¾åƒæ„å»ºä»¿çœŸç¯å¢ƒåï¼Œè”åˆåŸºäºæŠ•å½±çš„å¥–åŠ±æ¨¡å‹ä¸é‡æ”¾æ¼”ç¤ºæ¥è¿›è¡ŒRLç­–ç•¥è®­ç»ƒï¼ŒæŠŠå­¦ä¹ åˆ°çš„å¥–åŠ±æ¨¡å‹è½¬åŒ–ä¸ºåŠ¨ä½œå¯è¡Œæ€§é¢„æµ‹å™¨ï¼Œå°†é¢„æµ‹å™¨å’Œç­–ç•¥éƒ½è¿ç§»å›çœŸå®åœºæ™¯æ‰§è¡Œï¼Œæ”¯æ’‘ç¨³å®šç­–ç•¥å­¦ä¹ ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰ä¸åŒæœºå™¨äººåˆå§‹å§¿æ€å’Œç‰©ä½“é…ç½®çš„å…­ä¸ªæ“ä½œä»»åŠ¡ä¸­éªŒè¯ï¼ŒPRISMå­¦ä¹ åˆ°çš„é²æ£’ç­–ç•¥ç›¸æ¯”åŸºçº¿å¹³å‡æˆåŠŸç‡æå‡68%ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨åº”å¯¹å¤šæ ·åœºæ™¯ä¸‹ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è™šå®è¿ç§»æ€è·¯ï¼šåˆ©ç”¨çœŸå®æ•°æ®æ„å»ºæ›´è´´åˆç°å®çš„ä»¿çœŸç¯å¢ƒæ¥è¾…åŠ©ç­–ç•¥å­¦ä¹ ä¸è¿ç§»ï¼Œä¸ºè§£å†³çœŸå®ä¸–ç•Œè®­ç»ƒRLçš„å®‰å…¨ã€å¯è¡Œæ€§é—®é¢˜æä¾›äº†è·¯å¾„å‚è€ƒã€‚  
2. å¥–åŠ±æ¨¡å‹æ„å»ºï¼šå€ŸåŠ©è§†è§‰ - è¯­è¨€æ¨¡å‹ç»“åˆäººç±»å¼•å¯¼çš„ç©ºé—´å…³ç³»ä½œä¸ºç›‘ç£æ¥æºï¼Œä¸ºRLä¸­å¥–åŠ±å‡½æ•°çš„è‡ªåŠ¨ã€æ³›åŒ–æ€§è®¾è®¡æä¾›äº†æ–°æ–¹å‘ï¼Œå‡å°‘æ‰‹å·¥è®¾è®¡ä¾èµ–ã€‚  
3. å°‘é‡æ ·æœ¬åˆ©ç”¨ï¼šå±•ç¤ºäº†å¦‚ä½•å……åˆ†æŒ–æ˜å°‘é‡çœŸå®ä¸–ç•Œä¸“å®¶æ¼”ç¤ºä»·å€¼ï¼Œåœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒè¡”æ¥ä¸­åŠ©åŠ›ç­–ç•¥å­¦ä¹ ï¼Œå¯¹æ ·æœ¬ç¨€ç¼ºåœºæ™¯ä¸‹çš„æœºå™¨äººå­¦ä¹ æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## guiding-vlm-agents-with-process-rewards-at-inference-time-for-gui-navigation
### Abstract
Recent advancements in visual language models (VLMs) have notably enhanced
their capabilities in handling complex Graphical User Interface (GUI)
interaction tasks. Despite these improvements, current frameworks often
struggle to generate correct actions in challenging GUI environments.
State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source
VLMs for GUI tasks requires significant resources. Additionally, existing
trajectory-level evaluation and refinement techniques frequently fall short due
to delayed feedback and local optimization issues. To address these challenges,
we propose an approach that guides VLM agents with process supervision by a
reward model during GUI navigation and control at inference time. This guidance
allows the VLM agent to optimize actions at each inference step, thereby
improving performance in both static and dynamic environments. In particular,
our method demonstrates significant performance gains in three GUI navigation
tasks, achieving a 3.4% improvement in single step action accuracy for static
environments, along with a around 33% increase in task success rate in one
dynamic environment. With further integration of trajectory reflection and
retry mechanisms, we also demonstrate even greater enhancement in task success.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¨ç†æ—¶ç”¨è¿‡ç¨‹å¥–åŠ±å¼•å¯¼VLMæ™ºèƒ½ä½“å®ç°GUIå¯¼èˆª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤æ‚å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰äº¤äº’ä»»åŠ¡ä¸Šå–å¾—è¿›å±•ï¼Œä½†ç°æœ‰æ¡†æ¶åœ¨å…·æŒ‘æˆ˜æ€§çš„GUIç¯å¢ƒä¸­ç”Ÿæˆæ­£ç¡®åŠ¨ä½œä»æœ‰å›°éš¾ã€‚å•†ä¸šVLMsæ˜¯é»‘ç›’éš¾ä»¥è°ƒä¼˜ï¼Œå¼€æºVLMsé’ˆå¯¹GUIä»»åŠ¡å¾®è°ƒèµ„æºæ¶ˆè€—å¤§ï¼›ç°æœ‰è½¨è¿¹çº§è¯„ä¼°å’Œä¼˜åŒ–æŠ€æœ¯å› å»¶è¿Ÿåé¦ˆä¸å±€éƒ¨ä¼˜åŒ–é—®é¢˜è¡¨ç°ä¸ä½³ã€‚åŒæ—¶ï¼Œè½¨è¿¹çº§è¯„ä¼°ä¼šå¯¼è‡´ä¸ªä½“åŠ¨ä½œä¼˜åŒ–ä¸è¶³ã€çº é”™å»¶è¿Ÿï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç±»æ–¹æ³•è®­ç»ƒæˆæœ¬é«˜ä¸”ä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºGuidNavæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºGuidNavï¼Œæ¨ç†æ—¶ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¼•å¯¼VLMæ™ºèƒ½ä½“  
è®­ç»ƒä¸€ä¸ªè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºå°‘é‡äººç±»æ¼”ç¤ºå’ŒVLMç”Ÿæˆçš„åˆæˆæ•°æ®è®­ç»ƒï¼Œå­¦ä¹ GUIæ•°æ®ä¸­çš„åé¦ˆä¿¡å·ã€‚åœ¨GUIå¯¼èˆªæ¨ç†æ—¶ï¼Œç”¨æ­¤æ¨¡å‹å¼•å¯¼VLMæ™ºèƒ½ä½“ï¼Œè®©æ™ºèƒ½ä½“åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½èƒ½ä¼˜åŒ–åŠ¨ä½œï¼Œåœ¨é™æ€å’ŒåŠ¨æ€ç¯å¢ƒä¸­æå‡è¡¨ç°ï¼Œè¿˜èƒ½å®æ—¶é€‚åº”ç¯å¢ƒå˜åŒ–ã€é¿å…å»¶è¿Ÿåé¦ˆå¯¼è‡´çš„é”™è¯¯ç´¯ç§¯ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸è½¨è¿¹çº§ä¼˜åŒ– pipeline é›†æˆå¢å¼ºæ€§èƒ½  
è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¯æ•´åˆåˆ°ç»“æœç›‘ç£æµç¨‹ä¸­ï¼Œè¿›ä¸€æ­¥æ”¹è¿›åŠ¨ä½œç”Ÿæˆä¸é€‰æ‹©ï¼›ç»“åˆè½¨è¿¹åæ€å’Œé‡è¯•æœºåˆ¶åï¼Œèƒ½è¿›ä¸€æ­¥æå‡ä»»åŠ¡æˆåŠŸç‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Android-in-the-Wildï¼ˆAitWï¼‰çš„é™æ€å’ŒåŠ¨æ€åœºæ™¯è¯„ä¼°ï¼Œä½¿GPT - 4oåœ¨é™æ€ç¯å¢ƒåŠ¨ä½œå‡†ç¡®ç‡æå‡çº¦5%ï¼ŒåŠ¨æ€ç¯å¢ƒä»»åŠ¡æˆåŠŸç‡æå‡çº¦33%ï¼›ç»“åˆè½¨è¿¹åæ€å’Œé‡è¯•æœºåˆ¶åæˆåŠŸç‡å³°å€¼è¾¾71.6%ã€‚åœ¨GUI Odysseyå’ŒMind2WebåŸºå‡†æµ‹è¯•ä¹ŸéªŒè¯æ³›åŒ–æ€§ï¼ŒGUI Odysseyä¸Šä¸‰ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹³å‡å•æ­¥åŠ¨ä½œå‡†ç¡®ç‡æå‡çº¦3.2%ï¼ŒMind2Webé™æ€ç¯å¢ƒä¸‹å•æ­¥åŠ¨ä½œå‡†ç¡®ç‡æå‡2.1%ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œåˆ©ç”¨è¿‡ç¨‹å¥–åŠ±å®ç°ç»†ç²’åº¦ä¼˜åŒ–ã€å®æ—¶åé¦ˆçš„æ€è·¯ï¼Œå¯å¯å‘åœ¨å…¶ä»–éœ€å¤šæ­¥éª¤äº¤äº’ã€æ¯ä¸€æ­¥å¯¹ç»“æœå½±å“å¤§çš„ä»»åŠ¡åœºæ™¯ï¼ˆå¦‚æœºå™¨äººæ“ä½œæµç¨‹ã€å¤æ‚è½¯ä»¶äº¤äº’ç­‰ï¼‰ä¸­æ”¹è¿›æ™ºèƒ½ä½“è¡¨ç°ï¼›ä»å·¥ç¨‹è½åœ°è§’åº¦ï¼Œå±•ç¤ºäº†ç»“åˆå°‘é‡äººç±»æ¼”ç¤ºä¸åˆæˆæ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ä»¥é™ä½æ•°æ®æˆæœ¬çš„æ–¹å¼ï¼Œä¸ºèµ„æºæœ‰é™æƒ…å†µä¸‹å¼€å‘è¾…åŠ©æ¨¡å‹æä¾›å‚è€ƒï¼›ä»å¤šæ¨¡å—æ•´åˆè§’åº¦ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä¸è½¨è¿¹çº§æœºåˆ¶ç»“åˆæå‡æ€§èƒ½çš„æ¨¡å¼ï¼Œä¸ºæ„å»ºåˆ†å±‚ã€å¤šé˜¶æ®µä¼˜åŒ–çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å€Ÿé‰´æ–¹å‘ã€‚

## seedream-3-0-technical-report
### Abstract
We present Seedream 3.0, a high-performance Chinese-English bilingual image
generation foundation model. We develop several technical improvements to
address existing challenges in Seedream 2.0, including alignment with
complicated prompts, fine-grained typography generation, suboptimal visual
aesthetics and fidelity, and limited image resolutions. Specifically, the
advancements of Seedream 3.0 stem from improvements across the entire pipeline,
from data construction to model deployment. At the data stratum, we double the
dataset using a defect-aware training paradigm and a dual-axis collaborative
data-sampling framework. Furthermore, we adopt several effective techniques
such as mixed-resolution training, cross-modality RoPE, representation
alignment loss, and resolution-aware timestep sampling in the pre-training
phase. During the post-training stage, we utilize diversified aesthetic
captions in SFT, and a VLM-based reward model with scaling, thereby achieving
outputs that well align with human preferences. Furthermore, Seedream 3.0
pioneers a novel acceleration paradigm. By employing consistent noise
expectation and importance-aware timestep sampling, we achieve a 4 to 8 times
speedup while maintaining image quality. Seedream 3.0 demonstrates significant
improvements over Seedream 2.0: it enhances overall capabilities, in particular
for text-rendering in complicated Chinese characters which is important to
professional typography generation. In addition, it provides native
high-resolution output (up to 2K), allowing it to generate images with high
visual quality.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Seedream 3.0ï¼šå¤šç»´åº¦å‡çº§çš„ä¸­è‹±åŒè¯­å›¾åƒç”Ÿæˆå¤§æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ‰©æ•£æ¨¡å‹æ¨åŠ¨å›¾åƒç”ŸæˆæŠ€æœ¯å–å¾—å·¨å¤§è¿›å±•ï¼ŒSeedream 2.0åœ¨ä¸­è‹±åŒè¯­æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­æ˜¯é‡è¦é‡Œç¨‹ç¢‘ï¼Œä½†åœ¨å•†ä¸šåº”ç”¨ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼šå¤æ‚æç¤ºè¯çš„å¯¹é½åº¦éœ€æå‡ï¼ˆå¦‚æ•°å€¼ç²¾åº¦ã€å¤šç‰©ä½“ç©ºé—´å…³ç³»ï¼‰ï¼›ç²¾ç»†æ’ç‰ˆç”Ÿæˆèƒ½åŠ›æœ‰é™ï¼ˆå°å°ºå¯¸æ–‡å­—ã€å¤šè¡Œæ–‡æ„ç»„åˆç­‰ï¼‰ï¼›è§†è§‰ç¾å­¦ä¸ä¿çœŸåº¦æ¬ ä½³ï¼ˆå¦‚ç”µå½±åœºæ™¯ç¾æ„Ÿã€äººåƒçº¹ç†ï¼‰ï¼›å›¾åƒåˆ†è¾¨ç‡å—é™ï¼ˆåŸç”Ÿè¾“å‡ºå°åˆ†è¾¨ç‡ï¼Œä¾èµ–åå¤„ç†è¶…åˆ†ï¼‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼ŒSeedream 3.0 ä»æ•°æ®æ„å»ºåˆ°æ¨¡å‹éƒ¨ç½²å…¨æµç¨‹ä¼˜åŒ–å‡çº§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ•°æ®å±‚é©æ–°
é‡‡ç”¨ç¼ºé™·æ„ŸçŸ¥è®­ç»ƒèŒƒå¼ä¸åŒè½´ååŒæ•°æ®é‡‡æ ·æ¡†æ¶ã€‚ç¼ºé™·æ„ŸçŸ¥è®­ç»ƒèŒƒå¼é€šè¿‡ä¸»åŠ¨å­¦ä¹ å¼•æ“é€‰15000ä¸ªæ‰‹åŠ¨æ ‡æ³¨æ ·æœ¬è®­ç»ƒç¼ºé™·æ£€æµ‹å™¨ï¼Œç²¾å‡†å®šä½ç¼ºé™·åŒºåŸŸï¼Œçªç ´Seedream 2.0ä¸¥æ ¼æ•°æ®è¿‡æ»¤å¯¼è‡´çš„æ•°æ®é‡é™åˆ¶ï¼Œä½¿æ•°æ®é›†è§„æ¨¡ç¿»å€ä¸”è´¨é‡æå‡ï¼›åŒè½´ååŒæ•°æ®é‡‡æ ·åŸºäºå›¾åƒç°‡åˆ†å¸ƒå’Œæ–‡æœ¬è¯­ä¹‰è¿è´¯æ€§ä¸¤ä¸ªæ­£äº¤ç»´åº¦æ„å»ºåŠ¨æ€é‡‡æ ·æœºåˆ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢„è®­ç»ƒé˜¶æ®µæŠ€æœ¯å‡çº§
é‡‡ç”¨æ··åˆåˆ†è¾¨ç‡è®­ç»ƒã€è·¨æ¨¡æ€RoPEã€è¡¨ç¤ºå¯¹é½æŸå¤±ã€åˆ†è¾¨ç‡æ„ŸçŸ¥æ—¶é—´æ­¥é‡‡æ ·ç­‰æŠ€æœ¯ã€‚æ··åˆåˆ†è¾¨ç‡è®­ç»ƒæå‡æ¨¡å‹å¯¹ä¸åŒåˆ†è¾¨ç‡å›¾åƒçš„å¤„ç†èƒ½åŠ›ï¼›è·¨æ¨¡æ€RoPEå¢å¼ºæ–‡æœ¬ä¸å›¾åƒæ¨¡æ€é—´çš„å…³è”ï¼›è¡¨ç¤ºå¯¹é½æŸå¤±ä¼˜åŒ–æ¨¡æ€é—´è¡¨ç¤ºä¸€è‡´æ€§ï¼›åˆ†è¾¨ç‡æ„ŸçŸ¥æ—¶é—´æ­¥é‡‡æ ·è®©è®­ç»ƒæ›´é€‚é…ä¸åŒåˆ†è¾¨ç‡åœºæ™¯ï¼Œæ•´ä½“æå‡è§†è§‰ - è¯­è¨€å¯¹é½åº¦ä¸æ¨¡å‹æ³›åŒ–æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåè®­ç»ƒé˜¶æ®µä¼˜åŒ–
åœ¨SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ä¸­ä½¿ç”¨å¤šæ ·åŒ–ç¾å­¦æ ‡é¢˜ï¼Œå¹¶ç”¨åŸºäºVLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰çš„å¸¦ç¼©æ”¾çš„å¥–åŠ±æ¨¡å‹ã€‚å¤šæ ·åŒ–ç¾å­¦æ ‡é¢˜ä¸ºæ¨¡å‹æ³¨å…¥ç¾å­¦åˆ›ä½œèƒ½åŠ›ï¼›VLMå¥–åŠ±æ¨¡å‹ç»“åˆç¼©æ”¾æœºåˆ¶ï¼Œè®©æ¨¡å‹è¾“å‡ºæ›´è´´åˆäººç±»åå¥½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæ¨¡å‹åŠ é€Ÿæ–°èŒƒå¼
é‡‡ç”¨ä¸€è‡´å™ªå£°æœŸæœ›ä¸é‡è¦æ€§æ„ŸçŸ¥æ—¶é—´æ­¥é‡‡æ ·ã€‚ä¸€è‡´å™ªå£°æœŸæœ›ä¿éšœé‡‡æ ·ç¨³å®šæ€§ï¼Œé‡è¦æ€§æ„ŸçŸ¥æ—¶é—´æ­¥é‡‡æ ·å‡å°‘æ¨ç†æ—¶å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEï¼‰ï¼Œå®ç°4 - 8å€åŠ é€ŸåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Seedream 3.0 åœ¨å¤šæ–¹é¢è¡¨ç°å“è¶Šï¼šåœ¨Artificial Analysisæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ’è¡Œæ¦œä¸­ï¼Œä»¥17000å¤šæ¬¡å‡ºç°ä¸‹Arena ELOåˆ†æ•°1158æ’åç¬¬ä¸€ï¼›å¯¹æ¯”Seedream 2.0ç­‰æ¨¡å‹ï¼Œåœ¨å¯¹é½åº¦ã€ç»“æ„ã€ç¾å­¦ç­‰å…¨è¯„ä¼°ç»´åº¦è¡¨ç°çªå‡ºï¼›æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›æ˜¾è‘—å¢å¼ºï¼Œåœ¨å¤æ‚ä¸­è‹±æ–‡å­—ã€é•¿æ–‡æœ¬ç¾è§‚æ’ç‰ˆç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¿‡Canvaç­‰å¹³å°äººå·¥è®¾è®¡æ¨¡æ¿ï¼›å›¾åƒç¾å­¦è´¨é‡å¤§å¹…æå‡ï¼Œç”µå½±åœºæ™¯ã€äººåƒç”ŸæˆçœŸå®æ„Ÿæ›´å¼ºï¼›åŸç”Ÿæ”¯æŒ2Ké«˜åˆ†è¾¨ç‡è¾“å‡ºï¼Œæ— éœ€åå¤„ç†ä¸”é€‚é…å¤šæ ·å®½é«˜æ¯”ï¼›æ¨ç†æ•ˆç‡é«˜ï¼Œç”Ÿæˆ1Kåˆ†è¾¨ç‡å›¾åƒä»…éœ€çº¦3ç§’ï¼ˆæ— PEæ—¶ï¼‰ï¼Œæˆæœ¬æ˜¾è‘—é™ä½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ•°æ®å±‚é¢ï¼Œç¼ºé™·æ„ŸçŸ¥ä¸åŒè½´é‡‡æ ·çš„æ€è·¯ä¸ºè§£å†³æ•°æ®è´¨é‡ä¸è§„æ¨¡å¹³è¡¡é—®é¢˜æä¾›å‚è€ƒï¼Œå¯ç”¨äºå¤„ç†å«â€œå°ç‘•ç–µâ€ä½†æœ‰ä»·å€¼çš„æ•°æ®ï¼›æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œæ··åˆåˆ†è¾¨ç‡ã€è·¨æ¨¡æ€å…³è”å¢å¼ºã€è¡¨ç¤ºå¯¹é½ç­‰æŠ€æœ¯ï¼Œå¯¹å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–ä¸å¯¹é½ä¼˜åŒ–æœ‰å€Ÿé‰´æ„ä¹‰ï¼›åè®­ç»ƒçš„ç¾å­¦å¼•å¯¼ä¸å¥–åŠ±æ¨¡å‹ç¼©æ”¾ï¼Œä¸ºæå‡æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»å®¡ç¾åå¥½æä¾›æ–¹æ³•ï¼›æ¨¡å‹åŠ é€Ÿçš„å™ªå£°æœŸæœ›ä¸æ—¶é—´æ­¥é‡‡æ ·ç­–ç•¥ï¼Œåœ¨è¿½æ±‚æ¨ç†æ•ˆç‡åŒæ—¶ä¿éšœè´¨é‡æ–¹é¢ç»™å‡ºäº†åˆ›æ–°æ–¹å‘ï¼Œè¿™äº›æŠ€æœ¯ç‚¹å¯åœ¨å…¶ä»–ç”Ÿæˆç±»å¤§æ¨¡å‹ç ”å‘ä¸­å‚è€ƒå¤ç”¨ã€‚

## varp--reinforcement-learning-from-vision-language-model-feedback-with-agent-regularized-preferences
### Abstract
Designing reward functions for continuous-control robotics often leads to
subtle misalignments or reward hacking, especially in complex tasks.
Preference-based RL mitigates some of these pitfalls by learning rewards from
comparative feedback rather than hand-crafted signals, yet scaling human
annotations remains challenging. Recent work uses Vision-Language Models (VLMs)
to automate preference labeling, but a single final-state image generally fails
to capture the agent's full motion. In this paper, we present a two-part
solution that both improves feedback accuracy and better aligns reward learning
with the agent's policy. First, we overlay trajectory sketches on final
observations to reveal the path taken, allowing VLMs to provide more reliable
preferences-improving preference accuracy by approximately 15-20% in metaworld
tasks. Second, we regularize reward learning by incorporating the agent's
performance, ensuring that the reward model is optimized based on data
generated by the current policy; this addition boosts episode returns by 20-30%
in locomotion tasks. Empirical studies on metaworld demonstrate that our method
achieves, for instance, around 70-80% success rate in all tasks, compared to
below 50% for standard approaches. These results underscore the efficacy of
combining richer visual representations with agent-aware reward regularization.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VARPï¼šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åé¦ˆ+æ™ºèƒ½ä½“æ­£åˆ™åŒ–åå¥½é©æ–°å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è¿ç»­æ§åˆ¶æœºå™¨äººé¢†åŸŸï¼Œè®¾è®¡å¥–åŠ±å‡½æ•°æ—¶å¸¸å‡ºç°ç»†å¾®åå·®æˆ–â€œå¥–åŠ±é»‘å®¢â€ï¼ˆå³æ™ºèƒ½ä½“åˆ©ç”¨å¥–åŠ±è®¾è®¡æ¼æ´è¾¾æˆéé¢„æœŸè¡Œä¸ºï¼‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¤æ‚ä»»åŠ¡ä¸­ã€‚åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡ä»æ¯”è¾ƒåé¦ˆè€Œéæ‰‹å·¥è®¾è®¡ä¿¡å·ä¸­å­¦ä¹ å¥–åŠ±ï¼Œç¼“è§£äº†éƒ¨åˆ†ç¼ºé™·ï¼Œä½†è§„æ¨¡åŒ–çš„äººç±»æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚è¿‘æœŸç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è‡ªåŠ¨ç”Ÿæˆåå¥½æ ‡ç­¾çš„å·¥ä½œï¼Œåˆå› ä»…ç”¨å•å¼ æœ€ç»ˆçŠ¶æ€å›¾åƒéš¾ä»¥æ•æ‰æ™ºèƒ½ä½“å®Œæ•´è¿åŠ¨è¿‡ç¨‹ï¼ˆæ¯”å¦‚è·¯å¾„æ•ˆç‡ã€ä¸­é—´æ­¥éª¤æµç•…æ€§ç­‰å…³é”®ä¿¡æ¯æ˜“ä¸¢å¤±ï¼‰ï¼Œå¯¼è‡´åé¦ˆå‡†ç¡®æ€§å—é™ï¼›ä¸”å¥–åŠ±æ¨¡å‹è®­ç»ƒè‹¥ä¸è€ƒè™‘æ™ºèƒ½ä½“ç­–ç•¥æ¼”å˜ï¼Œæ˜“å‡ºç°è®­ç»ƒæ¬¡ä¼˜é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡åé¦ˆå‡†ç¡®æ€§ã€è®©å¥–åŠ±å­¦ä¹ ä¸æ™ºèƒ½ä½“ç­–ç•¥æ›´å¯¹é½ï¼Œæˆä¸ºå¾…è§£éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè½¨è¿¹æ„ŸçŸ¥çš„åå¥½æ ‡æ³¨æŠ€æœ¯  
ä¸å†ä¾èµ–å•å¼ æœ€ç»ˆçŠ¶æ€å›¾åƒä¸æ–‡æœ¬æç¤ºï¼Œè€Œæ˜¯åœ¨æœ€ç»ˆè§‚æµ‹ä¸Šå åŠ **2Dè½¨è¿¹è‰å›¾**ï¼Œä»¥æ­¤æ•æ‰æœºå™¨äººå®Œæ•´è¿åŠ¨è¿‡ç¨‹ã€‚ä¸°å¯Œåçš„è§†è§‰è¡¨å¾ä¸ºVLMæä¾›å…³é”®æ—¶é—´ç»´åº¦ä¸Šä¸‹æ–‡ï¼Œä½¿å…¶èƒ½åšå‡ºæ›´å¯é ç²¾å‡†çš„åå¥½åˆ¤æ–­ã€‚åœ¨MetaWorldä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å°†åå¥½å‡†ç¡®ç‡æå‡çº¦15 - 20%ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šVARPæ¡†æ¶ï¼ˆç»“åˆæ™ºèƒ½ä½“æ„ŸçŸ¥çš„å¥–åŠ±æ­£åˆ™åŒ–ï¼‰  
æå‡ºVARPæ¡†æ¶ï¼Œå¹³è¡¡åŸºäºè‰å›¾çš„å¤–éƒ¨åå¥½ä¿¡å·ä¸æ™ºèƒ½ä½“æ€§èƒ½ä»¥ä¼˜åŒ–å¥–åŠ±ã€‚å› é™æ€åå¥½æ•°æ®ä¼šéšç­–ç•¥æ¼”å˜è€Œé”™ä½ï¼Œè‹¥ä»…ç”¨å…¶ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ï¼Œå¯èƒ½æ— æ³•åæ˜ æ™ºèƒ½ä½“å½“å‰è¡¨ç°ã€‚ä¸ºæ­¤ï¼ŒæŠŠæ™ºèƒ½ä½“æ€§èƒ½æ•´åˆåˆ°å¥–åŠ±å­¦ä¹ ç›®æ ‡ä¸­ï¼ŒåŠ¨æ€æƒ©ç½šä½å›æŠ¥çš„å¥–åŠ±å‡½æ•°ã€‚è¿™ä¸€æ™ºèƒ½ä½“æ„ŸçŸ¥çš„æ­£åˆ™åŒ–ï¼Œè®©å­¦ä¹ åˆ°çš„å¥–åŠ±ä¸å®æ—¶æœ‰æ•ˆè¡Œä¸ºå¯¹é½ï¼Œåœ¨ locomotion ä»»åŠ¡ä¸­èƒ½æå‡å•å¹•å›æŠ¥20 - 30%ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MetaWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVARPå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼šæ‰€æœ‰ä»»åŠ¡æˆåŠŸç‡è¾¾70 - 80%å·¦å³ï¼Œè€Œæ ‡å‡†æ–¹æ³•æˆåŠŸç‡ä½äº50%ã€‚å®éªŒæœ‰åŠ›è¯æ˜äº†VARPèƒ½å¾—åˆ°æ›´å‡†ç¡®çš„åå¥½æ¨¡å‹ä¸æ›´ç¨³å¥çš„ç­–ç•¥æå‡ï¼Œå‡¸æ˜¾äº†â€œæ›´ä¸°å¯Œè§†è§‰è¡¨å¾ + æ™ºèƒ½ä½“æ„ŸçŸ¥å¥–åŠ±æ­£åˆ™åŒ–â€ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€å¢å¼ºæ€è·¯ï¼šåœ¨ä¾èµ–è§†è§‰ - è¯­è¨€æ¨¡å‹çš„åœºæ™¯ä¸‹ï¼Œé€šè¿‡è½¨è¿¹è‰å›¾è¿™ç±»è½»é‡ä½†æœ‰æ•ˆçš„æ–¹å¼è¡¥å……æ—¶é—´ç»´åº¦ä¿¡æ¯ï¼Œä¸ºæå‡æ¨¡å‹å¯¹åŠ¨æ€è¿‡ç¨‹ç†è§£æä¾›äº†æ–°èŒƒå¼ï¼Œå¯è¿ç§»åˆ°éœ€æ•æ‰åºåˆ—/è¿åŠ¨ä¿¡æ¯çš„è§†è§‰ä»»åŠ¡ä¸­ã€‚  
2. ç­–ç•¥ - å¥–åŠ±å¯¹é½ï¼šå°†æ™ºèƒ½ä½“è‡ªèº«æ€§èƒ½çº³å…¥å¥–åŠ±å­¦ä¹ æ­£åˆ™åŒ–ï¼Œè§£å†³é™æ€æ•°æ®ä¸åŠ¨æ€ç­–ç•¥è„±èŠ‚é—®é¢˜ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­â€œå¥–åŠ±æ¨¡å‹å¦‚ä½•é€‚é…ç­–ç•¥æ¼”åŒ–â€æä¾›äº†å®ç”¨æ­£åˆ™åŒ–æ–¹å‘ï¼Œåœ¨æœºå™¨äººæ§åˆ¶ã€å¤æ‚å†³ç­–ä»»åŠ¡ç­‰åœºæ™¯æœ‰å‚è€ƒä»·å€¼ã€‚  
3. è‡ªåŠ¨åŒ–åå¥½æ ‡æ³¨ä¼˜åŒ–ï¼šç”¨VLMè‡ªåŠ¨å¤„ç†åå¥½æ ‡æ³¨åŒæ—¶ï¼Œé’ˆå¯¹æ€§å¼¥è¡¥å•å¸§å›¾åƒç¼ºé™·ï¼Œä¸ºé™ä½äººç±»æ ‡æ³¨æˆæœ¬ã€æå‡è‡ªåŠ¨åŒ–åé¦ˆè´¨é‡æä¾›äº†è½åœ°è·¯å¾„ï¼Œåœ¨éœ€å¤§è§„æ¨¡åå¥½æ•°æ®çš„RLHFç±»ä»»åŠ¡ä¸­å€¼å¾—å€Ÿé‰´ã€‚  

## multimodal-rewardbench--holistic-evaluation-of-reward-models-for-vision-language-models
### Abstract
Reward models play an essential role in training vision-language models
(VLMs) by assessing output quality to enable aligning with human preferences.
Despite their importance, the research community lacks comprehensive open
benchmarks for evaluating multimodal reward models in VLMs. To address this
gap, we introduce Multimodal RewardBench, an expert-annotated benchmark
covering six domains: general correctness, preference, knowledge, reasoning,
safety, and visual question-answering. Our dataset comprises 5,211 annotated
(prompt, chosen response, rejected response) triplets collected from various
VLMs. In evaluating a range of VLM judges, we find that even the top-performing
models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall
accuracy. Notably, most models struggle in the reasoning and safety domains.
These findings suggest that Multimodal RewardBench offers a challenging testbed
for advancing reward model development across multiple domains. We release the
benchmark at https://github.com/facebookresearch/multimodal_rewardbench.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€RewardBenchï¼šä¸ºè§†è§‰è¯­è¨€æ¨¡å‹Reward Modelæä¾›å…¨é¢è¯„ä¼°åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è®­ç»ƒä¸­è‡³å…³é‡è¦ï¼Œå®ƒèƒ½è¯„ä¼°è¾“å‡ºè´¨é‡ï¼Œè®©æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œç ”ç©¶ç¤¾åŒºç¼ºä¹ç”¨äºè¯„ä¼°VLMä¸­å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„å…¨é¢å…¬å¼€åŸºå‡†ã€‚ç°æœ‰åŸºå‡†å¤šå±€é™äºæ–‡æœ¬æ¨¡æ€ï¼Œé’ˆå¯¹VLMå¥–åŠ±æ¨¡å‹è¯„ä¼°çš„å·¥ä½œä¹Ÿå¤šé™äºé€šç”¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸”æ ‡æ³¨éä¸“å®¶å®Œæˆï¼Œæ‰€ä»¥éœ€è¦ä¸€ä¸ªå…¨é¢ã€ä¸“å®¶æ ‡æ³¨çš„åŸºå‡†æ¥è¯„ä¼°VLMçš„å¥–åŠ±æ¨¡å‹ï¼Œè¿™å°±æ˜¯æœ¬æ–‡å·¥ä½œçš„åŠ¨æœºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºMultimodal RewardBenchåŸºå‡†  
æå‡ºMultimodal RewardBenchè¿™ä¸€ä¸“å®¶æ ‡æ³¨çš„åŸºå‡†ï¼Œè¦†ç›–é€šç”¨æ­£ç¡®æ€§ã€åå¥½ã€çŸ¥è¯†ã€æ¨ç†ã€å®‰å…¨å’Œè§†è§‰é—®ç­”è¿™å…­ä¸ªé¢†åŸŸã€‚æ”¶é›†äº†æ¥è‡ªä¸åŒVLMçš„5211ä¸ªæ ‡æ³¨å¥½çš„ï¼ˆæç¤ºã€é€‰ä¸­å“åº”ã€æ‹’ç»å“åº”ï¼‰ä¸‰å…ƒç»„ï¼Œè¿™äº›ä¸‰å…ƒç»„å¯ç›´æ¥ç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹çš„å“åº”æ’åºå‡†ç¡®ç‡ï¼Œè¿˜æ¶µç›–äº†â€œæ­£ç¡®vsé”™è¯¯å“åº”â€â€œäººç±»åå¥½vséåå¥½å“åº”ï¼ˆåœ¨å“åº”éƒ½æ­£ç¡®æˆ–éƒ½é”™è¯¯æƒ…å†µä¸‹ï¼‰â€ä¸¤ç§æƒ…å†µä»¥æ”¯æŒæ›´ç»†ç²’åº¦è¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢åˆ†æVLMæ³•å®˜æ€§èƒ½  
å¯¹å¤šç§VLMæ³•å®˜ï¼ˆåŒ…æ‹¬ä¸“æœ‰æ¨¡å‹å¦‚GPT - 4oã€Claudeã€Geminiï¼Œå¼€æºæ¨¡å‹å¦‚Molmoã€Ariaä»¥åŠä¸åŒè§„æ¨¡çš„Llama3ç­‰ï¼‰åœ¨Multimodal RewardBenchä¸Šçš„æ€§èƒ½è¿›è¡Œåˆ†æï¼Œæ¢ç©¶æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°æƒ…å†µï¼Œä¸ºç†è§£å½“å‰VLMå¥–åŠ±æ¨¡å‹èƒ½åŠ›æä¾›ä¾æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è¯„ä¼°å¤šç§VLMæ³•å®˜æ—¶å‘ç°ï¼šå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„Gemini 1.5 Proå’ŒClaude 3.5 Sonnetæ¨¡å‹ï¼Œæ•´ä½“å‡†ç¡®ç‡ä¹Ÿä»…ä¸º72%ï¼›å¹¶ä¸”å¤§å¤šæ•°æ¨¡å‹åœ¨æ¨ç†ï¼ˆæ•°å­¦å’Œç¼–ç ä»»åŠ¡ï¼‰å’Œå®‰å…¨ï¼ˆå°¤å…¶æ˜¯æ¯’æ€§æ£€æµ‹ï¼‰é¢†åŸŸè¡¨ç°ä¸ä½³ã€‚è¿™è¡¨æ˜Multimodal RewardBenchä¸ºå¤šé¢†åŸŸå¥–åŠ±æ¨¡å‹å‘å±•æä¾›äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŸºå‡†æ„å»ºè§’åº¦ï¼šæœ¬æ–‡æ„å»ºå…¨é¢ä¸”ä¸“å®¶æ ‡æ³¨çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è¯„ä¼°åŸºå‡†çš„æ€è·¯ï¼Œä¸ºåç»­ç›¸å…³é¢†åŸŸåŸºå‡†å»ºè®¾æä¾›äº†èŒƒä¾‹ï¼Œå°¤å…¶æ˜¯åœ¨è¦†ç›–å¤šé¢†åŸŸï¼ˆå¦‚çŸ¥è¯†ã€æ¨ç†ã€å®‰å…¨ç­‰æ­¤å‰VLMå¥–åŠ±æ¨¡å‹è¯„ä¼°æœªæ¶‰åŠé¢†åŸŸï¼‰æ–¹é¢çš„å°è¯•å€¼å¾—å€Ÿé‰´ã€‚
2. æ¨¡å‹è¯„ä¼°è§’åº¦ï¼šå¯¹ä¸åŒç±»å‹VLMæ³•å®˜åœ¨å¤šé¢†åŸŸçš„æ€§èƒ½åˆ†ææ–¹æ³•ï¼Œæœ‰åŠ©äºåç»­ç ”ç©¶æ›´å…¨é¢åœ°ç†è§£æ¨¡å‹èƒ½åŠ›è¾¹ç•Œï¼Œä¸ºæ”¹è¿›æ¨¡å‹æä¾›æ–¹å‘ï¼Œè¿™ç§å¤šç»´åº¦ã€å¤šæ¨¡å‹çš„è¯„ä¼°æ–¹å¼å€¼å¾—åœ¨æ¨¡å‹è¯„ä¼°å·¥ä½œä¸­å‚è€ƒã€‚
3. æ•°æ®èµ„æºè§’åº¦ï¼šå…¬å¼€é‡Šæ”¾åŸºå‡†æ•°æ®é›†ï¼Œæ¨åŠ¨äº†æ•´ä¸ªç ”ç©¶ç¤¾åŒºå¯¹å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„ç ”ç©¶ï¼Œè¿™ç§å¼€æ”¾å…±äº«çš„ç†å¿µä¹Ÿä¸ºé¢†åŸŸå‘å±•æä¾›äº†åŠ©åŠ›ï¼Œå€¼å¾—ç›¸å…³ç ”ç©¶å­¦ä¹ ã€‚

## enhancing-cognition-and-explainability-of-multimodal-foundation-models-with-self-synthesized-data
### Abstract
Large Multimodal Models (LMMs), or Vision-Language Models (VLMs), have shown
impressive capabilities in a wide range of visual tasks. However, they often
struggle with fine-grained visual reasoning, failing to identify
domain-specific objectives and provide justifiable explanations for their
predictions. To address the above challenge, we propose a novel visual
rejection sampling framework to improve the cognition and explainability of
LMMs using self-synthesized data. Specifically, visual fine-tuning requires
images, queries, and target answers. Our approach begins by synthesizing
interpretable answers that include human-verifiable visual features. These
features are based on expert-defined concepts, and carefully selected based on
their alignment with the image content. After each round of fine-tuning, we
apply a reward model-free filtering mechanism to select the highest-quality
interpretable answers for the next round of tuning. This iterative process of
synthetic data generation and fine-tuning progressively improves the model's
ability to generate accurate and reasonable explanations. Experimental results
demonstrate the effectiveness of our method in improving both the accuracy and
explainability of specialized visual classification tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨è‡ªåˆæˆæ•°æ®å¢å¼ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è®¤çŸ¥ä¸å¯è§£é‡Šæ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼Œä¹Ÿç§°ä¸ºè§†è§‰ - è¯­è¨€æ¨¡å‹VLMsï¼‰åœ¨ä¼—å¤šè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºäº†ä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œç„¶è€Œåœ¨ç»†ç²’åº¦è§†è§‰æ¨ç†ä»»åŠ¡ä¸­å´å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥è¯†åˆ«ç‰¹å®šé¢†åŸŸç›®æ ‡ä¸”æ— æ³•ä¸ºé¢„æµ‹æä¾›åˆç†çš„è§£é‡Šã€‚æ¯”å¦‚åœ¨æ–¯å¦ç¦ç‹—ç‹—æ•°æ®é›†ä¸Šï¼ŒåƒLLaVA - 1.5è¿™æ ·çš„å…ˆè¿›æ¨¡å‹åˆ†ç±»å‡†ç¡®ç‡ä¹Ÿä»…ä¸º12.2%ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³è¯¥é—®é¢˜è¿›è¡Œå¾®è°ƒæ—¶ï¼Œç¼ºä¹é«˜è´¨é‡æ•°æ®ï¼ˆåˆ›å»ºç‰¹å¾çº§å›¾åƒæ ‡æ³¨å¤æ‚ä¸”èµ„æºå¯†é›†ï¼‰ï¼Œè‹¥ä»…ç”¨æ ‡ç­¾æˆ–é€šç”¨æ ‡ç­¾å…³è”ç‰¹å¾è®­ç»ƒåˆä¼šå¯¼è‡´æ·å¾„å­¦ä¹ æˆ–è¿‡åº¦æ³›åŒ–ç­‰é—®é¢˜ï¼Œè¿™äº›éƒ½ä¿ƒä½¿ç ”ç©¶è€…æ¢ç´¢æ–°æ–¹æ³•æ¥å¢å¼ºLMMsçš„è®¤çŸ¥å’Œå¯è§£é‡Šæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè§†è§‰æ‹’ç»é‡‡æ ·æ¡†æ¶å®ç°è‡ªåˆæˆæ•°æ®å¢å¼ºè®¤çŸ¥ä¸å¯è§£é‡Šæ€§
è¯¥æ¡†æ¶æ— éœ€å¤§é‡æ‰‹åŠ¨æ ‡æ³¨ï¼Œè®©LMMsè‡ªåˆæˆå¯è§£é‡Šç­”æ¡ˆã€‚å¯¹äºç»™å®šå›¾åƒï¼Œå…ˆåˆ©ç”¨LMMçš„å›¾åƒæè¿°èƒ½åŠ›ç”Ÿæˆæè¿°ä»¥è¯†åˆ«å›¾åƒç‰¹å®šè§†è§‰ç‰¹å¾ï¼Œå•ä¸ªæè¿°å¯èƒ½ä»…è¦†ç›–éƒ¨åˆ†å…³é”®ç‰¹å¾ï¼Œä½†æ”¶é›†å¤§é‡æè¿°èƒ½è¿‘ä¼¼å›¾åƒç‰¹å¾çš„çœŸå®åˆ†å¸ƒä»¥å‡å°‘å•ä¸ªæè¿°çš„ä¸å®Œæ•´æ€§ï¼Œè¿˜é€šè¿‡ä¿¡æ¯ç“¶é¢ˆæŠ€æœ¯é€‰æ‹©æœ€ç›¸å…³ç‰¹å¾ï¼Œå†å°†è¿™äº›å›¾åƒç‰¹å®šæ¦‚å¿µé‡å†™æˆå¯è§£é‡Šç­”æ¡ˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥ä¿¡æ¯è®ºæ–¹æ³•ä¸æ— å¥–åŠ±æ¨¡å‹è¿‡æ»¤æœºåˆ¶ä¿éšœæ•°æ®è´¨é‡ä¸é€‰æ‹©
ç”¨ä¿¡æ¯è®ºæ–¹æ³•ä¸ºæ¯ä¸ªå›¾åƒé€‰æ‹©å¯è§£é‡Šè§†è§‰æ¦‚å¿µï¼Œç¡®ä¿ç²¾å‡†è¯†åˆ«å›¾åƒç‰¹å¾ï¼›è®¾è®¡æ— å¥–åŠ±æ¨¡å‹çš„è¿‡æ»¤æœºåˆ¶ï¼Œåœ¨æ¯è½®å¾®è°ƒåä»åˆæˆè¾“å‡ºä¸­é€‰æ‹©æœ€é«˜è´¨é‡çš„å¯è§£é‡Šç­”æ¡ˆç”¨äºä¸‹ä¸€è½®è°ƒä¼˜ï¼Œä¿éšœè®­ç»ƒæ•°æ®è´¨é‡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡æ•°æ®åˆæˆä¸æ¨¡å‹å¾®è°ƒçš„è¿­ä»£è¿‡ç¨‹é€æ­¥æå‡èƒ½åŠ›
å…ˆæå–å›¾åƒçº§ç‰¹å¾å¹¶è½¬åŒ–ä¸ºå¯è§£é‡Šç­”æ¡ˆï¼Œä¸å¯¹åº”å›¾åƒå’ŒæŸ¥è¯¢æ„æˆåˆå§‹è®­ç»ƒæ•°æ®é›†ï¼Œå¾®è°ƒå¾—åˆ°æ›´æ–°æ¨¡å‹ï¼›ç”¨æ›´æ–°æ¨¡å‹é‡å¤ç”Ÿæˆç­”æ¡ˆï¼Œé€‰æœ€ä¼˜çš„ç”¨äºä¸‹ä¸€è½®å¾®è°ƒï¼Œé€šè¿‡è¿™ç§è‡ªå¢å¼ºè¿‡ç¨‹é€æ­¥æå‡LMMç”Ÿæˆå¯é è§£é‡Šçš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æå‡ç‰¹å®šè§†è§‰åˆ†ç±»ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢æ˜¯æœ‰æ•ˆçš„ï¼ˆè™½æœªè¯¦ç»†é˜è¿°å®éªŒå¯¹æ¯”ç»†èŠ‚ï¼Œä½†ä»æ‘˜è¦å’Œå¼•è¨€å¯æ˜ç¡®æ–¹æ³•å¯¹è§£å†³LMMsåœ¨ç»†ç²’åº¦è§†è§‰æ¨ç†ç­‰é—®é¢˜çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢å¯¹æ•°æ®ç¨€ç¼ºæˆ–æ ‡æ³¨å›°éš¾çš„ä»»åŠ¡æ—¶ï¼Œå¯å€Ÿé‰´è‡ªåˆæˆæ•°æ®çš„æ€è·¯ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«èƒ½åŠ›ç”Ÿæˆè®­ç»ƒæ‰€éœ€çš„éƒ¨åˆ†æ•°æ®ï¼Œå‡å°‘å¯¹å¤§é‡äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚
2. ä¿¡æ¯ç“¶é¢ˆæŠ€æœ¯ç”¨äºç‰¹å¾é€‰æ‹©ä»¥åŠæ— å¥–åŠ±æ¨¡å‹è¿‡æ»¤æœºåˆ¶åœ¨æ•°æ®é€‰æ‹©ä¸Šçš„è®¾è®¡ï¼Œä¸ºå¤„ç†æ•°æ®è´¨é‡å’Œç›¸å…³æ€§é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œåœ¨å…¶ä»–éœ€è¦å¯¹æ•°æ®è¿›è¡Œç­›é€‰å’Œç‰¹å¾å¤„ç†çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡ä¸­å¯å‚è€ƒã€‚
3. è¿­ä»£å¼çš„æ•°æ®åˆæˆä¸æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸ºæŒç»­æå‡æ¨¡å‹æ€§èƒ½å’Œç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚å¯è§£é‡Šæ€§ï¼‰æä¾›äº†èŒƒå¼ï¼Œåœ¨æ”¹è¿›æ¨¡å‹é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡çš„è¡¨ç°æ—¶å¯å€Ÿé‰´è¿™ç§è¿­ä»£ä¼˜åŒ–çš„æ–¹å¼ã€‚

## diffusion-model-as-a-noise-aware-latent-reward-model-for-step-level-preference-optimization
### Abstract
Preference optimization for diffusion models aims to align them with human
preferences for images. Previous methods typically use Vision-Language Models
(VLMs) as pixel-level reward models to approximate human preferences. However,
when used for step-level preference optimization, these models face challenges
in handling noisy images of different timesteps and require complex
transformations into pixel space. In this work, we show that pre-trained
diffusion models are naturally suited for step-level reward modeling in the
noisy latent space, as they are explicitly designed to process latent images at
various noise levels. Accordingly, we propose the Latent Reward Model (LRM),
which repurposes components of the diffusion model to predict preferences of
latent images at arbitrary timesteps. Building on LRM, we introduce Latent
Preference Optimization (LPO), a step-level preference optimization method
conducted directly in the noisy latent space. Experimental results indicate
that LPO significantly improves the model's alignment with general, aesthetic,
and text-image alignment preferences, while achieving a 2.5-28x training
speedup over existing preference optimization methods. Our code and models are
available at https://github.com/Kwai-Kolors/LPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ‰©æ•£æ¨¡å‹åŒ–èº«å™ªå£°æ„ŸçŸ¥æ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼Œé©æ–°æ­¥éª¤çº§åå¥½ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ‰©æ•£æ¨¡å‹çš„åå¥½ä¼˜åŒ–é¢†åŸŸï¼Œä»¥å¾€æ–¹æ³•å¸¸é‡‡ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºåƒç´ çº§å¥–åŠ±æ¨¡å‹æ¥é€¼è¿‘äººç±»åå¥½ã€‚ä½†ç”¨äºæ­¥éª¤çº§åå¥½ä¼˜åŒ–æ—¶ï¼Œè¿™ç±»åƒç´ çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ï¼šä¸€æ˜¯å¤æ‚è½¬æ¢ï¼Œæ¯ä¸€æ­¥ timestep éƒ½è¦é¢å¤–è¿›è¡Œæ‰©æ•£å»å™ªå’Œ VAE è§£ç ï¼ŒæŠŠå«å™ªæ½œåœ¨å›¾åƒè½¬æˆåƒç´ å›¾åƒï¼Œæ¨ç†æµç¨‹å†—é•¿ï¼›äºŒæ˜¯é«˜å™ªå£°ä¸å…¼å®¹ï¼Œå¤§ timestep ä¸‹å™ªå£°å¼ºï¼Œç”Ÿæˆçš„é¢„æµ‹å›¾åƒæ¨¡ç³Šï¼Œå’Œ VLMs è®­ç»ƒæ•°æ®ï¼ˆæ¸…æ™°å›¾åƒï¼‰åˆ†å¸ƒåç§»å¤§ï¼Œå¯¼è‡´é¢„æµ‹ä¸å¯é ï¼›ä¸‰æ˜¯æ—¶é—´æ­¥ä¸æ•æ„Ÿï¼ŒPRMs ä¸€èˆ¬ä¸æŠŠ timestep ä½œä¸ºè¾“å…¥ï¼Œéš¾ç†è§£ä¸åŒæ—¶é—´æ­¥å¯¹å›¾åƒè¯„ä¼°çš„å½±å“ã€‚è¿™äº›é—®é¢˜é˜»ç¢äº†å…¶åœ¨æ­¥éª¤çº§å¥–åŠ±å»ºæ¨¡ä¸­çš„æ•ˆæœï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½åœ¨æ½œåœ¨ç©ºé—´è‡ªç„¶æ•æ‰äººç±»åå¥½ã€æ„ŸçŸ¥æ—¶é—´æ­¥ä¸”é€‚é…é«˜å™ªå£°çš„æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼ˆLRMï¼‰
å‘ç°é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¤©ç„¶é€‚åˆåœ¨å«å™ªæ½œåœ¨ç©ºé—´åšæ­¥éª¤çº§å¥–åŠ±å»ºæ¨¡ï¼Œå› ä¸ºå®ƒæœ‰å¤„ç†ä¸åŒå™ªå£°æ°´å¹³æ½œåœ¨å›¾åƒçš„æ˜¾å¼è®¾è®¡ã€‚LRM å¤ç”¨æ‰©æ•£æ¨¡å‹ç»„ä»¶ï¼Œé’ˆå¯¹ä»»æ„ timestep çš„æ½œåœ¨å›¾åƒ xtï¼Œåˆ©ç”¨ U - Net æˆ– DiT çš„è§†è§‰ç‰¹å¾ä»¥åŠæ–‡æœ¬ç¼–ç å™¨çš„æ–‡æœ¬ç‰¹å¾æ¥é¢„æµ‹æ­¥éª¤çº§åå¥½æ ‡ç­¾ï¼Œè¿˜å¼•å…¥è§†è§‰ç‰¹å¾å¢å¼ºï¼ˆVFEï¼‰æ¨¡å—æå‡å¯¹æ–‡æœ¬ - å›¾åƒå¯¹é½çš„å…³æ³¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºæ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰æ–¹æ³•
åŸºäº LRMï¼ŒLPO æ˜¯ç›´æ¥åœ¨å«å™ªæ½œåœ¨ç©ºé—´å¼€å±•çš„æ­¥éª¤çº§åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚ä¸ºè§£å†³ LRM è®­ç»ƒæ•°æ®ä¸­åå¥½ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºå¤šåå¥½ä¸€è‡´è¿‡æ»¤ï¼ˆMPCFï¼‰ç­–ç•¥ï¼Œç¡®ä¿è·èƒœå›¾åƒåœ¨å¤šä¸ªç»´åº¦æŒç»­ä¼˜äºå¤±è´¥å›¾åƒï¼Œä»¥æ­¤ç”¨ LRM è¿›è¡Œæ­¥éª¤çº§åå¥½ä¼˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ SD1.5 å’Œ SDXL ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLPO åœ¨é€šç”¨ã€ç¾å­¦ã€æ–‡æœ¬ - å›¾åƒå¯¹é½åå¥½æ–¹é¢å¤§å¹…æå‡ç”Ÿæˆå›¾åƒè´¨é‡ï¼ŒæŒç»­è¶…è¶Šç°æœ‰ DPO å’Œ SPO æ–¹æ³•ï¼›è®­ç»ƒæ•ˆç‡å‡ºè‰²ï¼Œç›¸æ¯” Diffusion - DPO æœ‰ 10 - 28 å€åŠ é€Ÿï¼Œç›¸æ¯” SPO æœ‰ 2.5 - 3.5 å€åŠ é€Ÿï¼›è¿˜æ¢ç´¢äº†åŸºäº LRM çš„ GRPO é€æ­¥å˜ä½“ï¼Œå¹¶å°† LPO åº”ç”¨äºåŸºäº DiT çš„ SD3 æ¨¡å‹ï¼Œå±•ç°äº† LRM å’Œ LPO çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹å¤ç”¨æ€è·¯ï¼šå‘ç°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è‡ªèº«ç‰¹æ€§å¹¶å°†å…¶å¤ç”¨ä¸ºå¥–åŠ±æ¨¡å‹ï¼Œä¸ºåˆ©ç”¨å·²æœ‰æˆç†Ÿæ¨¡å‹è§£å†³æ–°é—®é¢˜æä¾›äº†æ€è·¯ï¼Œä¸å†ä¾èµ–é¢å¤– VLMs åšåƒç´ çº§å¥–åŠ±æ¨¡å‹ï¼Œå¼€è¾Ÿäº†å¥–åŠ±å»ºæ¨¡æ–°è·¯å¾„ã€‚
2. ç©ºé—´ä¼˜åŒ–è§’åº¦ï¼šåœ¨æ½œåœ¨ç©ºé—´ç›´æ¥å¼€å±•åå¥½ä¼˜åŒ–ï¼Œé¿å…åƒç´ çº§å¥–åŠ±æ¨¡å‹çš„å¤æ‚è½¬æ¢å’Œé«˜å™ªå£°ä¸å…¼å®¹ç­‰é—®é¢˜ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åå¥½ä¼˜åŒ–çš„ç©ºé—´é€‰æ‹©æä¾›äº†æ›´é«˜æ•ˆçš„æ–¹å‘ã€‚
3. è®­ç»ƒç­–ç•¥åˆ›æ–°ï¼šæå‡ºçš„ MPCF ç­–ç•¥è§£å†³è®­ç»ƒæ•°æ®åå¥½ä¸ä¸€è‡´é—®é¢˜ï¼Œè¿™ç§é’ˆå¯¹è®­ç»ƒæ•°æ®ç‰¹æ€§è®¾è®¡è¿‡æ»¤ç­–ç•¥çš„æ–¹å¼ï¼Œåœ¨å¤„ç†æœ‰åå¥½æ ‡ç­¾çš„æ•°æ®æ—¶å…·æœ‰å‚è€ƒä»·å€¼ã€‚

## os-genesis--automating-gui-agent-trajectory-construction-via-reverse-task-synthesis
### Abstract
Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
https://qiushisun.github.io/OS-Genesis-Home/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | OS-Genesisï¼šé¢ è¦†ä¼ ç»Ÿï¼Œåå‘åˆæˆæ‰“é€ é«˜è´¨é‡GUIæ™ºèƒ½ä½“è½¨è¿¹æ•°æ®

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ™ºèƒ½ä½“å·²å±•ç°å‡ºç±»äººçš„è®¡ç®—æœºæ§åˆ¶èƒ½åŠ›ï¼Œæ¨åŠ¨æ•°å­—è‡ªåŠ¨åŒ–å‘å±•ã€‚ä½†**é«˜è´¨é‡è®­ç»ƒè½¨è¿¹æ•°æ®çš„é‡‡é›†**æˆä¸ºå…³é”®ç“¶é¢ˆï¼šç°æœ‰æ–¹æ³•ä¾èµ–äººç±»ç›‘ç£æˆ–é¢„å®šä¹‰ä»»åŠ¡ç”Ÿæˆåˆæˆæ•°æ®ï¼Œå­˜åœ¨èµ„æºæ¶ˆè€—å¤§ã€æ•°æ®è´¨é‡éš¾ä¿éšœã€å¤šæ ·æ€§ä¸è¶³ä»¥åŠåˆæˆæ•°æ®ä¸çœŸå®ç¯å¢ƒå·®è·å¤§ç­‰é—®é¢˜ã€‚ä¸ºçªç ´è¿™äº›é™åˆ¶ï¼Œè®ºæ–‡æå‡ºOS - Genesisè¿™ä¸€åˆ›æ–°æ€§GUIæ•°æ®åˆæˆ pipelineã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤äº’é©±åŠ¨ + åå‘ä»»åŠ¡åˆæˆï¼Œé¢ è¦†ä¼ ç»Ÿæµç¨‹  
æ‘’å¼ƒä¼ ç»Ÿä¾èµ–é¢„å®šä¹‰ä»»åŠ¡çš„è½¨è¿¹é‡‡é›†æ–¹å¼ï¼ŒOS - Genesisé‡‡ç”¨**äº¤äº’é©±åŠ¨**æ€è·¯ã€‚å…ˆè®©æ™ºèƒ½ä½“æ„ŸçŸ¥GUIç¯å¢ƒå¹¶è¿›è¡Œé€æ­¥äº¤äº’ï¼ˆå¦‚ç‚¹å‡»ç­‰æ“ä½œï¼‰ï¼Œå†é€šè¿‡**åå‘ä»»åŠ¡åˆæˆ**ï¼Œä»è§‚æµ‹åˆ°çš„çŠ¶æ€å’ŒåŠ¨ä½œä¸­é€†å‘ç”Ÿæˆä½å±‚æ¬¡æŒ‡ä»¤ï¼Œè¿›è€Œæ¨å¯¼é«˜å±‚æ¬¡æŒ‡ä»¤ï¼Œå®ç°è½¨è¿¹å±‚é¢çš„æ¢ç´¢ã€‚è¿™ç§æ–¹å¼çªç ´äº†é¢„å®šä¹‰ä»»åŠ¡å¯¹æ•°æ®è§„æ¨¡ä¸å¤šæ ·æ€§çš„é™åˆ¶ï¼Œè¿˜èƒ½è‡ªç„¶è¡”æ¥æŠ½è±¡æŒ‡ä»¤ä¸GUIåŠ¨æ€ç‰¹æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè½¨è¿¹å¥–åŠ±æ¨¡å‹ä¿éšœæ•°æ®è´¨é‡  
åœ¨åˆæˆä»»åŠ¡è½¬åŒ–ä¸ºè½¨è¿¹åï¼Œå¼•å…¥**è½¨è¿¹å¥–åŠ±æ¨¡å‹**ï¼Œå¯¹ç”Ÿæˆçš„è½¨è¿¹è¿›è¡Œè´¨é‡è¯„ä¼°ä¸ç­›é€‰ï¼Œç¡®ä¿ç”¨äºè®­ç»ƒçš„è½¨è¿¹å…·å¤‡é«˜å¯ç”¨æ€§ï¼Œä¸ºGUIæ™ºèƒ½ä½“è®­ç»ƒæä¾›ä¼˜è´¨æ•°æ®æ”¯æ’‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç«¯åˆ°ç«¯æ”¯æŒå¤šç¯å¢ƒGUIæ™ºèƒ½ä½“è®­ç»ƒ  
OS - Genesisæ— éœ€äººç±»ç›‘ç£ï¼Œèƒ½é«˜æ•ˆåˆæˆé«˜è´¨é‡è½¨è¿¹æ•°æ®ï¼Œæ”¯æŒåœ¨ä¸åŒç¯å¢ƒï¼ˆå¦‚ç§»åŠ¨ã€ç½‘é¡µç­‰ï¼‰ä¸‹å¯¹GUIæ™ºèƒ½ä½“è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼ŒåŠ©åŠ›æ™ºèƒ½ä½“ä»åŠ¨ä½œè‡ªåŠ¨åŒ–å‘å…¨æµç¨‹è‡ªä¸»åŒ–æ¼”è¿›ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AndroidWorldå’ŒWebArenaè¿™ä¸¤ä¸ªé«˜æŒ‘æˆ˜æ€§åœ¨çº¿åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOS - Genesiså±•ç°å‡ºå¼ºå¤§æ•ˆèƒ½ï¼šåœ¨AndroidWorldä¸Šï¼Œç›¸è¾ƒä»»åŠ¡é©±åŠ¨æ–¹æ³•æ€§èƒ½å¤§å¹…æå‡ï¼Œä»9.82%æå‡è‡³17.41%ï¼Œè¿‘ä¹ç¿»å€ã€‚å®éªŒå……åˆ†éªŒè¯äº†OS - Genesisåˆæˆè½¨è¿¹çš„é«˜è´¨é‡ï¼Œä»¥åŠå°†é€šç”¨VLMsè½¬åŒ–ä¸ºä¸“ä¸šGUIæ™ºèƒ½ä½“çš„å·¨å¤§æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿä½“ç°å‡ºå…¶åœ¨æ•°æ®è´¨é‡ã€å¤šæ ·æ€§æ–¹é¢è¿œè¶…ç°æœ‰åˆæˆæ–¹æ³•ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ€è·¯è½¬å˜**ï¼šä»ä»»åŠ¡é©±åŠ¨åˆ°äº¤äº’é©±åŠ¨çš„èŒƒå¼è½¬æ¢ï¼Œä¸ºè§£å†³æ•°æ®é‡‡é›†ç“¶é¢ˆæä¾›äº†å…¨æ–°è§†è§’ï¼Œåœ¨å…¶ä»–éœ€å¤§é‡æ•°æ®ä¸”ä¾èµ–é¢„å®šä¹‰ä»»åŠ¡æ˜“å—é™çš„é¢†åŸŸï¼ˆå¦‚å…¶ä»–äº¤äº’ç±»æ™ºèƒ½ä½“è®­ç»ƒï¼‰ï¼Œè¿™ç§æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚  
2. **åå‘åˆæˆæœºåˆ¶**ï¼šåå‘ä»»åŠ¡åˆæˆçš„è®¾è®¡ï¼Œä¸ºæŒ–æ˜ç¯å¢ƒåŠŸèƒ½ã€ç”Ÿæˆæœ‰æ„ä¹‰å¯æ‰§è¡Œä»»åŠ¡æä¾›äº†æœ‰æ•ˆæ‰‹æ®µï¼Œå¯å¯å‘ç±»ä¼¼éœ€ä»äº¤äº’è¡Œä¸ºåæ¨ä»»åŠ¡é€»è¾‘çš„åœºæ™¯æ–¹æ¡ˆè®¾è®¡ã€‚  
3. **è´¨é‡ä¿éšœæ‰‹æ®µ**ï¼šè½¨è¿¹å¥–åŠ±æ¨¡å‹å¯¹æ•°æ®è´¨é‡çš„æŠŠæ§ç­–ç•¥ï¼Œåœ¨å„ç±»éœ€å¯¹ç”Ÿæˆæ•°æ®è¿›è¡Œè´¨é‡ç­›é€‰çš„ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆåç­›é€‰ä¼˜è´¨æ ·æœ¬ï¼‰ä¸­ï¼Œéƒ½æœ‰å‚è€ƒä»·å€¼ã€‚  
4. **å¤šç¯å¢ƒé€‚é…**ï¼šå…¶æ”¯æŒå¤šç¯å¢ƒç«¯åˆ°ç«¯è®­ç»ƒçš„ç‰¹æ€§ï¼Œä¸ºè·¨å¹³å°æ™ºèƒ½ä½“å¼€å‘æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼ŒåŠ©åŠ›æ‰“é€ æ›´é€šç”¨çš„æ™ºèƒ½ç³»ç»Ÿã€‚  

## clip-rldrive--human-aligned-autonomous-driving-via-clip-based-reward-shaping-in-reinforcement-learning
### Abstract
This paper presents CLIP-RLDrive, a new reinforcement learning (RL)-based
framework for improving the decision-making of autonomous vehicles (AVs) in
complex urban driving scenarios, particularly in unsignalized intersections. To
achieve this goal, the decisions for AVs are aligned with human-like
preferences through Contrastive Language-Image Pretraining (CLIP)-based reward
shaping. One of the primary difficulties in RL scheme is designing a suitable
reward model, which can often be challenging to achieve manually due to the
complexity of the interactions and the driving scenarios. To deal with this
issue, this paper leverages Vision-Language Models (VLMs), particularly CLIP,
to build an additional reward model based on visual and textual cues.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CLIP-RLDriveï¼šåŸºäºCLIPå¥–åŠ±å¡‘é€ çš„ç±»äººè‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAVsï¼‰å¤§è§„æ¨¡å•†ä¸šåŒ–é¢ä¸´è¾¹ç¼˜æ¡ˆä¾‹å¤„ç†éš¾é¢˜ï¼Œä¼ ç»Ÿå†³ç­–ç³»ç»Ÿå› è®¾è®¡åƒµåŒ–éš¾ä»¥åœ¨å¤æ‚äº¤äº’ã€æ„å¤–æ¡ä»¶ç­‰åœºæ™¯æœ‰æ•ˆæ³›åŒ–æˆ–é€‚åº”ï¼Œè€Œäººç±»é©¾é©¶å‘˜å‡­å€Ÿç›´è§‰ã€ç¤¾äº¤æ™ºèƒ½ç­‰åœ¨è¾¹ç¼˜æ¡ˆä¾‹è¡¨ç°æ›´å¥½ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½æå‡äº†AVså†³ç­–èƒ½åŠ›ï¼Œä½†åœ¨é•¿å°¾åœºæ™¯å› è®­ç»ƒæ•°æ®é›†æ¨¡å¼æœ‰é™ä»æœ‰ä¸è¶³ã€‚åŒæ—¶ï¼ŒRLä¸­å¥–åŠ±å‡½æ•°è®¾è®¡å…³é”®ä¸”å…·æŒ‘æˆ˜æ€§ï¼Œç¨€ç–æˆ–å»¶è¿Ÿå¥–åŠ±ã€åé¦ˆæ…¢ç­‰ä¼šé˜»ç¢æ™ºèƒ½ä½“å­¦ä¹ ï¼Œå¥–åŠ±å¡‘é€ è™½èƒ½åŠ é€Ÿå­¦ä¹ ä½†å¯èƒ½ä½¿æ™ºèƒ½ä½“ä¼˜åŒ–åç¦»åŸå§‹ç›®æ ‡çš„å¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼Œå¾ˆå¤šRLä»»åŠ¡æ˜¯è§†è§‰ç±»çš„ï¼Œéœ€ç”¨è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºCLIP - RLDriveæ¡†æ¶ï¼Œåˆ©ç”¨CLIPåŸºäºè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œæ”¹è¿›AVsåœ¨å¤æ‚åŸå¸‚é©¾é©¶åœºæ™¯ï¼ˆå°¤å…¶æ˜¯æ— ä¿¡å·äº¤å‰å£ï¼‰çš„å†³ç­–ï¼Œä½¿å…¶ä¸ç±»äººåå¥½å¯¹é½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºCLIP - RLDriveæ¡†æ¶ï¼Œåˆ©ç”¨CLIPå®ç°åŸºäºå¥–åŠ±å¡‘é€ çš„ç±»äººè‡ªåŠ¨é©¾é©¶å†³ç­–ã€‚å€ŸåŠ©CLIPå¯¹é½å›¾åƒå’Œæ–‡æœ¬åµŒå…¥çš„èƒ½åŠ›ï¼Œå°†ç±»äººæŒ‡ä»¤è½¬åŒ–ä¸ºå¥–åŠ±ä¿¡å·ä»¥å¼•å¯¼AVå†³ç­–è¿‡ç¨‹ï¼Œè§£å†³RLä¸­å¥–åŠ±æ¨¡å‹è®¾è®¡éš¾é¢˜ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å†³ç­–æä¾›é¢å¤–åŸºäºè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢çš„å¥–åŠ±æ¨¡å‹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨å¤æ‚æ— ä¿¡å·äº¤å‰å£ç¯å¢ƒä¸­åº”ç”¨ä¸¤ç§RLç®—æ³•ï¼ˆPPOå’ŒDQNï¼‰è®­ç»ƒæ™ºèƒ½ä½“ï¼Œå¹¶å¯¹æ¯”æœ‰æ— CLIPå¥–åŠ±æ¨¡å‹æ—¶çš„æ€§èƒ½ï¼Œæ˜ç¡®CLIPå¯¹æ™ºèƒ½ä½“å­¦ä¹ å’Œä¼˜åŒ–è¡Œä¸ºä»¥å¥‘åˆæœŸæœ›é©¾é©¶åŠ¨ä½œçš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒå¯¹æ¯”äº†åŸºäºCLIPçš„DQNå’ŒPPOç®—æ³•æ€§èƒ½ï¼ŒåŸºäºCLIPçš„DQNå®ç°äº†96%çš„æˆåŠŸç‡ä¸”ç¢°æ’ç‡ä»…4%ï¼›è€ŒåŸºäºCLIPçš„PPOæˆåŠŸç‡ä¸º38%ï¼Œè¶…æ—¶ç‡è¾¾54%ã€‚è¿™è¡¨æ˜åŸºäºCLIPçš„DQNåœ¨ä½¿è‡ªåŠ¨é©¾é©¶è¡Œä¸ºä¸ç±»äººé©¾é©¶æ ‡å‡†å¯¹é½æ–¹é¢æ›´æœ‰æ•ˆï¼Œå‡¸æ˜¾äº†æ‰€ææ¡†æ¶åœ¨ç±»äººé©¾é©¶è¡Œä¸ºå¯¹é½ä¸Šçš„ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨å¼ºåŒ–å­¦ä¹ ä¸è‡ªåŠ¨é©¾é©¶ç»“åˆé¢†åŸŸï¼Œåˆ©ç”¨è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ¥æ„å»ºå¥–åŠ±æ¨¡å‹æ˜¯ä¸€ç§åˆ›æ–°æ€è·¯ï¼Œä¸ºè§£å†³RLä¸­å¥–åŠ±å‡½æ•°è®¾è®¡éš¾é¢˜æä¾›äº†æ–°æ–¹å‘ï¼Œå¯å€Ÿé‰´æ­¤æ€è·¯å¤„ç†å…¶ä»–éœ€ç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€éœ€ç±»äººåå¥½å¯¹é½çš„RLä»»åŠ¡ï¼›åŒæ—¶ï¼Œé€šè¿‡å¯¹æ¯”ä¸åŒRLç®—æ³•åœ¨å¼•å…¥æ–°å¥–åŠ±æ¨¡å‹åçš„æ€§èƒ½ï¼Œä¸ºåç»­é€‰æ‹©åˆé€‚ç®—æ³•å¤„ç†ç‰¹å®šåœºæ™¯è‡ªåŠ¨é©¾é©¶ä»»åŠ¡æä¾›äº†å‚è€ƒï¼Œä¹Ÿå¯ç¤ºåœ¨å…¶ä»–é¢†åŸŸç»“åˆå¤šç§ç®—æ³•å¯¹æ¯”å®éªŒæ¥éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ã€‚

## advdreamer-unveils--are-vision-language-models-truly-ready-for-real-world-3d-variations-
### Abstract
Vision Language Models (VLMs) have exhibited remarkable generalization
capabilities, yet their robustness in dynamic real-world scenarios remains
largely unexplored. To systematically evaluate VLMs' robustness to real-world
3D variations, we propose AdvDreamer, the first framework capable of generating
physically reproducible Adversarial 3D Transformation (Adv-3DT) samples from
single-view observations. In AdvDreamer, we integrate three key innovations:
Firstly, to characterize real-world 3D variations with limited prior knowledge
precisely, we design a zero-shot Monocular Pose Manipulation pipeline built
upon generative 3D priors. Secondly, to ensure the visual quality of worst-case
Adv-3DT samples, we propose a Naturalness Reward Model that provides continuous
naturalness regularization during adversarial optimization, effectively
preventing convergence to hallucinated or unnatural elements. Thirdly, to
enable systematic evaluation across diverse VLM architectures and
visual-language tasks, we introduce the Inverse Semantic Probability loss as
the adversarial optimization objective, which solely operates in the
fundamental visual-textual alignment space. Based on the captured Adv-3DT
samples with high aggressiveness and transferability, we establish MM3DTBench,
the first VQA benchmark dataset tailored to evaluate VLM robustness under
challenging 3D variations. Extensive evaluations of representative VLMs with
varying architectures reveal that real-world 3D variations can pose severe
threats to model performance across various tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdvDreamerï¼šæ­å¼€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œ3Då˜åŒ–ä¸‹çš„é²æ£’æ€§é¢çº±

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ç†è§£çš„è·¨æ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿé€æ­¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººç³»ç»Ÿç­‰å®‰å…¨å…³é”®é¢†åŸŸã€‚ä½†ç°æœ‰ç ”ç©¶å¤šèšç„¦æ•°å­—åŸŸ2Dæ‰°åŠ¨ä¸‹çš„é²æ£’æ€§ï¼Œ**çœŸå®ä¸–ç•Œä¸­æ™®éå­˜åœ¨çš„3Då˜åŒ–å¯¹VLMsé²æ£’æ€§çš„æŒ‘æˆ˜å´æœªè¢«å……åˆ†æ¢ç´¢**ã€‚æ¯”å¦‚æ¨¡å‹åœ¨é¢å¯¹ç‰©ä½“å§¿æ€ã€è§†è§’ç­‰3Dç»´åº¦å˜åŒ–æ—¶ï¼Œèƒ½å¦ç¨³å®šå®Œæˆè§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç­‰ä»»åŠ¡ï¼Ÿä¸ºç³»ç»Ÿè¯„ä¼°VLMså¯¹çœŸå®ä¸–ç•Œ3Då˜åŒ–çš„é²æ£’æ€§ï¼Œè®ºæ–‡æå‡ºAdvDreameræ¡†æ¶ï¼Œè¯•å›¾è§£å†³â€œå¦‚ä½•åœ¨æœ‰é™å…ˆéªŒï¼ˆå¦‚å•è§†è§’è§‚æµ‹ï¼‰ä¸‹ç²¾å‡†åˆ»ç”»çœŸå®ä¸–ç•Œ3Då˜åŒ–â€ç­‰æ ¸å¿ƒé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé›¶æ ·æœ¬å•ç›®å§¿æ€æ“æ§ï¼ˆMonocular Pose Manipulationï¼‰ pipeline  
ä¸ºåœ¨æœ‰é™å…ˆéªŒä¸‹ç²¾å‡†åˆ»ç”»çœŸå®ä¸–ç•Œ3Då˜åŒ–ï¼Œè®ºæ–‡åŸºäºç”Ÿæˆå¼3Då…ˆéªŒè®¾è®¡é›¶æ ·æœ¬å•ç›®å§¿æ€æ“æ§æµç¨‹ã€‚åŒºåˆ«äºä¾èµ–æ˜¾å¼3Dç»“æ„æˆ–å¤šè§†è§’å¯†é›†è§‚æµ‹çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥æµç¨‹åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­è•´å«çš„ä¸°å¯Œ3Då…ˆéªŒï¼Œä»…é€šè¿‡å•è§†è§’è‡ªç„¶å›¾åƒå°±èƒ½å®ç°å¯¹ç‰©ä½“3Då§¿æ€ç­‰å˜åŒ–çš„æ“æ§ï¼Œæ‘†è„±äº†å¯¹å¤§é‡åœºæ™¯å…ˆéªŒçš„ä¾èµ–ï¼Œè®©3Då˜åŒ–åˆ»ç”»æ›´è´´åˆçœŸå®ä¸–ç•Œâ€œå•è§†è§’è§‚æµ‹å±…å¤šâ€çš„åœºæ™¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªç„¶æ€§å¥–åŠ±æ¨¡å‹ï¼ˆNaturalness Reward Modelï¼‰  
 adversarialä¼˜åŒ–æ˜“ç”Ÿæˆâ€œå¹»è§‰åŒ–ã€ä¸è‡ªç„¶â€çš„æ ·æœ¬ï¼Œä¸ºä¿è¯å¯¹æŠ—æ€§3Då˜æ¢ï¼ˆAdv - 3DTï¼‰æ ·æœ¬åœ¨â€œæœ€åæƒ…å†µâ€ä¸‹ä»æœ‰åˆç†è§†è§‰è´¨é‡ï¼Œè®ºæ–‡æå‡ºè‡ªç„¶æ€§å¥–åŠ±æ¨¡å‹ã€‚åœ¨å¯¹æŠ—ä¼˜åŒ–è¿‡ç¨‹ä¸­æŒç»­æä¾›è‡ªç„¶æ€§æ­£åˆ™åŒ–ï¼Œé¿å…ä¼˜åŒ–æ”¶æ•›åˆ°ä¸çœŸå®å…ƒç´ ï¼Œè®©ç”Ÿæˆçš„Adv - 3DTæ ·æœ¬æ—¢å…·å¤‡æ”»å‡»æ€§åˆç¬¦åˆçœŸå®ä¸–ç•Œè§†è§‰è§„å¾‹ï¼Œèƒ½åœ¨ç‰©ç†ä¸–ç•Œå¤ç°ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé€†è¯­ä¹‰æ¦‚ç‡æŸå¤±ï¼ˆInverse Semantic Probability lossï¼‰ä½œä¸ºå¯¹æŠ—ä¼˜åŒ–ç›®æ ‡  
ä¸ºåœ¨ä¸åŒVLMæ¶æ„å’Œè§†è§‰ - è¯­è¨€ä»»åŠ¡ä¸Šå®ç°ç³»ç»Ÿè¯„ä¼°ï¼Œè®ºæ–‡æå‡ºé€†è¯­ä¹‰æ¦‚ç‡æŸå¤±ä½œä¸ºå¯¹æŠ—ä¼˜åŒ–ç›®æ ‡ã€‚è¯¥æŸå¤±ä»…åœ¨â€œè§†è§‰ - æ–‡æœ¬å¯¹é½â€è¿™ä¸€åŸºç¡€ç©ºé—´è¿ä½œï¼Œèƒ½é’ˆå¯¹æ€§åœ°æ„é€ è®©VLMsè¡¨ç°æ¶åŒ–çš„å¯¹æŠ—æ ·æœ¬ï¼ŒåŠ©åŠ›ç³»ç»Ÿæ¢ç©¶3Då˜åŒ–ä¸‹æ¨¡å‹é²æ£’æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæ„å»ºMM3DTBenchåŸºå‡†æ•°æ®é›†  
åŸºäºAdvDreamerç”Ÿæˆçš„é«˜æ”»å‡»æ€§ã€é«˜è¿ç§»æ€§Adv - 3DTæ ·æœ¬ï¼Œè®ºæ–‡æ„å»ºäº†MM3DTBenchâ€”â€”é¦–ä¸ªä¸“ä¸ºè¯„ä¼°VLMsåœ¨å¯ŒæŒ‘æˆ˜æ€§3Då˜åŒ–ä¸‹é²æ£’æ€§è®¾è®¡çš„VQAåŸºå‡†æ•°æ®é›†ã€‚å¡«è¡¥äº†3Då˜åŒ–åœºæ™¯ä¸‹VLMsé²æ£’æ€§è¯„ä¼°æ•°æ®é›†çš„ç©ºç™½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¯¹ä¸åŒæ¶æ„çš„ä»£è¡¨æ€§VLMså¼€å±•å¹¿æ³›è¯„ä¼°åå‘ç°ï¼š**çœŸå®ä¸–ç•Œ3Då˜åŒ–èƒ½å¯¹æ¨¡å‹åœ¨å„ç±»è§†è§‰ - è¯­è¨€ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç­‰ï¼‰ä¸Šçš„æ€§èƒ½é€ æˆä¸¥é‡å¨èƒ**ã€‚æ¯”å¦‚åœ¨ç‰©ä½“è¯†åˆ«ã€åŠŸèƒ½æè¿°ç­‰ä»»åŠ¡ä¸­ï¼Œé¢å¯¹AdvDreamerç”Ÿæˆçš„Adv - 3DTæ ·æœ¬ï¼Œæ¨¡å‹å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼›ä¸”è¿™äº›å¯¹æŠ—æ ·æœ¬åœ¨ç‰©ç†ä¸–ç•Œå¤ç°åã€èƒŒæ™¯å˜åŒ–æ—¶ä»ä¿ç•™æ”»å‡»æ€§ï¼ŒéªŒè¯äº†æ–¹æ³•ä¸æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜è§†è§’åˆ›æ–°ï¼šé¦–æ¬¡èšç„¦â€œçœŸå®ä¸–ç•Œ3Då˜åŒ–å¯¹VLMsé²æ£’æ€§å½±å“â€è¿™ä¸€ç©ºç™½é¢†åŸŸï¼Œä¸ºVLMsé²æ£’æ€§ç ”ç©¶å¼€è¾Ÿæ–°æ–¹å‘ï¼Œå¯å‘åç»­å…³æ³¨æ›´è´´è¿‘çœŸå®åœºæ™¯çš„å¤šç»´åº¦é²æ£’æ€§åˆ†æã€‚  
2. æŠ€æœ¯è·¯çº¿åˆ›æ–°ï¼šä»3Då˜åŒ–åˆ»ç”»ï¼ˆå•ç›®å§¿æ€æ“æ§ï¼‰ã€æ ·æœ¬è´¨é‡ä¿éšœï¼ˆè‡ªç„¶æ€§å¥–åŠ±æ¨¡å‹ï¼‰ã€å¯¹æŠ—ç›®æ ‡è®¾è®¡ï¼ˆé€†è¯­ä¹‰æ¦‚ç‡æŸå¤±ï¼‰åˆ°åŸºå‡†æ„å»ºï¼ˆMM3DTBenchï¼‰ï¼Œå½¢æˆå®Œæ•´çš„â€œç”Ÿæˆ - è¯„ä¼°â€æŠ€æœ¯é“¾æ¡ï¼Œä¸ºé¢†åŸŸå†…å¯¹æŠ—æ ·æœ¬ç”Ÿæˆã€é²æ£’æ€§è¯„ä¼°æä¾›äº†ç«¯åˆ°ç«¯çš„æ–¹æ³•è®ºå‚è€ƒã€‚  
3. åº”ç”¨ä»·å€¼å¯å‘ï¼šè®©ç ”ç©¶è€…ä¸å¼€å‘è€…æ„è¯†åˆ°VLMsåœ¨çœŸå®åŠ¨æ€åœºæ™¯éƒ¨ç½²çš„æ½œåœ¨é£é™©ï¼Œæ¨åŠ¨æ¨¡å‹åœ¨å®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶è§†è§‰äº¤äº’æ¨¡å—ï¼‰çš„é²æ£’æ€§ä¼˜åŒ–å·¥ä½œï¼Œä¹Ÿä¸º3Dæ„ŸçŸ¥ä¸è§†è§‰è¯­è¨€è·¨æ¨¡æ€ç»“åˆçš„ç ”ç©¶æä¾›æ–°æ€è·¯ã€‚

## vista-dataset--do-vision-language-models-understand-sequential-tasks-
### Abstract
Using vision-language models (VLMs) as reward models in reinforcement
learning holds promise for reducing costs and improving safety. So far, VLM
reward models have only been used for goal-oriented tasks, where the agent must
reach a particular final outcome. We explore VLMs' potential to supervise tasks
that cannot be scored by the final state alone. To this end, we introduce
ViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.
ViSTa comprises over 4,000 videos with step-by-step descriptions in virtual
home, Minecraft, and real-world environments. Its novel hierarchical structure
-- basic single-step tasks composed into more and more complex sequential tasks
-- allows a fine-grained understanding of how well VLMs can judge tasks with
varying complexity. To illustrate this, we use ViSTa to evaluate
state-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while
they are all good at object recognition, they fail to understand sequential
tasks, with only GPT-4o achieving non-trivial performance.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ViSTa Datasetï¼šæ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹å¯¹åºåˆ—ä»»åŠ¡çš„ç†è§£èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¸¸æˆã€æœºå™¨äººç­‰å¤æ‚åºåˆ—å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è®¾è®¡å¯é çš„å¥–åŠ±å‡½æ•°å´é¢‡å…·æŒ‘æˆ˜ï¼Œäººå·¥ç›‘ç£æˆæœ¬é«˜æ˜‚ä¸”æ˜“è¢«â€œæ¬ºéª—â€ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ‰æœ›ä½œä¸ºå¥–åŠ±æ¨¡å‹è§£å†³è¿™äº›é—®é¢˜ï¼Œæ­¤å‰VLMsä»…ç”¨äºé¢å‘ç›®æ ‡çš„ä»»åŠ¡ï¼ˆä¾æ®æœ€ç»ˆçŠ¶æ€è¯„åˆ¤ï¼‰ï¼Œè€Œæœ¬æ–‡å…³æ³¨â€œæ— æ³•ä»…é€šè¿‡æœ€ç»ˆçŠ¶æ€è¯„åˆ†â€çš„åºåˆ—ä»»åŠ¡ï¼Œæ¢ç´¢VLMsç›‘ç£è¿™ç±»ä»»åŠ¡çš„æ½œåŠ›ï¼Œä¸ºæ­¤æå‡ºViSTaæ•°æ®é›†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºViSTaæ•°æ®é›†  
ViSTaåŒ…å«è¶…4000ä¸ªè§†é¢‘åŠåˆ†æ­¥æè¿°ï¼Œè¦†ç›–è™šæ‹Ÿå®¶åº­ã€Minecraftã€çœŸå®ä¸–ç•Œ3ç±»ç¯å¢ƒã€‚å…¶é‡‡ç”¨**åˆ†å±‚ç»“æ„**ï¼šä»å•æ­¥åŸºç¡€ä»»åŠ¡ï¼ˆLevel 1ï¼Œå¦‚â€œæ¡èµ·é¦™è•‰â€ï¼‰é€æ­¥ç»„åˆæˆæ›´å¤æ‚çš„å¤šæ­¥åºåˆ—ä»»åŠ¡ï¼ˆLevel 2åˆ°Level 8ï¼Œå¦‚â€œæ¡èµ·é¦™è•‰â†’æŠŠé¦™è•‰æ”¾è¿›å£æ©±â€ï¼‰ï¼Œèƒ½ç»†ç²’åº¦æµ‹è¯•VLMså¯¹ä¸åŒå¤æ‚åº¦åºåˆ—ä»»åŠ¡çš„ç†è§£ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡é—®é¢˜é›†ï¼ˆProblem Setsï¼‰  
å°†è§†é¢‘ - æè¿°å¯¹åˆ†ç»„ä¸ºé—®é¢˜é›†ï¼Œæ¯ä¸ªé—®é¢˜é›†é’ˆå¯¹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚ç‰©ä½“è¯†åˆ«ã€åŠ¨ä½œé¡ºåºç†è§£ï¼‰ã€‚è¯„ä¼°æ—¶è®©æ¨¡å‹ä¸ºè§†é¢‘ä¸æè¿°çš„åŒ¹é…åº¦æ‰“åˆ†ï¼Œé€šè¿‡ä¸åŒé—®é¢˜é›†èšåˆç»“æœï¼Œç³»ç»Ÿè¯„ä¼°VLMsåœ¨åºåˆ—ä»»åŠ¡ç†è§£ä¸Šçš„èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
ç”¨ViSTaè¯„ä¼°CLIPã€ViCLIPã€GPT - 4oç­‰å‰æ²¿VLMså‘ç°ï¼š  
- ç‰©ä½“è¯†åˆ«èƒ½åŠ›ï¼šè¿™äº›æ¨¡å‹åœ¨å•æ­¥ä»»åŠ¡çš„ç‰©ä½“è¯†åˆ«ä¸Šè¡¨ç°è¾ƒå¥½ï¼›  
- åºåˆ—ä»»åŠ¡ç†è§£ï¼šé™¤GPT - 4oå–å¾—ä¸€å®šæ€§èƒ½å¤–ï¼Œå…¶ä»–æ¨¡å‹åœ¨ç†è§£åºåˆ—ä»»åŠ¡ï¼ˆå¦‚åŠ¨ä½œé¡ºåºã€å¤šæ­¥é€»è¾‘ï¼‰ä¸Šè¡¨ç°ä¸ä½³ï¼Œæš´éœ²äº†VLMsåœ¨åºåˆ—æ¨ç†èƒ½åŠ›ä¸Šçš„çŸ­æ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºæ€è·¯ï¼šViSTaçš„åˆ†å±‚ç»“æ„ä¸é—®é¢˜é›†è®¾è®¡ä¸ºè¯„ä¼°æ¨¡å‹ç‰¹å®šèƒ½åŠ›æä¾›äº†ç»†ç²’åº¦æ¡†æ¶ï¼Œå¯å€Ÿé‰´è¿™ç§â€œä»ç®€å•åˆ°å¤æ‚ã€é’ˆå¯¹æ€§æµ‹è¯•â€çš„æ•°æ®é›†æ„å»ºé€»è¾‘ï¼Œç”¨äºå…¶ä»–éœ€è¯„ä¼°åºåˆ—/å¤šæ­¥æ¨ç†çš„ä»»åŠ¡åœºæ™¯ã€‚  
2. ç ”ç©¶è§†è§’æ‹“å±•ï¼šèšç„¦â€œéæœ€ç»ˆçŠ¶æ€è¯„åˆ¤â€çš„åºåˆ—ä»»åŠ¡ç›‘ç£ï¼Œå¡«è¡¥äº†VLMsåœ¨RLå¥–åŠ±æ¨¡å‹åº”ç”¨ä¸­çš„ç ”ç©¶ç©ºç™½ï¼Œå¯å‘åç»­æ¢ç´¢VLMsåœ¨è¿‡ç¨‹å¯¼å‘å‹ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚  
3. å®éªŒç»“è®ºä»·å€¼ï¼šæ˜ç¡®å½“å‰VLMsåœ¨åºåˆ—ä»»åŠ¡ç†è§£ä¸Šçš„ä¸è¶³ï¼Œä¸ºåç»­æ¨¡å‹æ”¹è¿›ï¼ˆå¦‚ä¼˜åŒ–åºåˆ—æ¨ç†ã€è·¨ç¯å¢ƒæ³›åŒ–ï¼‰æŒ‡æ˜æ–¹å‘ï¼Œä¹Ÿè®©ç ”ç©¶è€…æ›´æ¸…æ™°è®¤è¯†VLMsä½œä¸ºå¥–åŠ±æ¨¡å‹çš„å±€é™ä¸æ½œåŠ›ã€‚

## gen-drive--enhancing-diffusion-generative-driving-policies-with-reward-modeling-and-reinforcement-learning-fine-tuning
### Abstract
Autonomous driving necessitates the ability to reason about future
interactions between traffic agents and to make informed evaluations for
planning. This paper introduces the \textit{Gen-Drive} framework, which shifts
from the traditional prediction and deterministic planning framework to a
generation-then-evaluation planning paradigm. The framework employs a behavior
diffusion model as a scene generator to produce diverse possible future
scenarios, thereby enhancing the capability for joint interaction reasoning. To
facilitate decision-making, we propose a scene evaluator (reward) model,
trained with pairwise preference data collected through VLM assistance, thereby
reducing human workload and enhancing scalability. Furthermore, we utilize an
RL fine-tuning framework to improve the generation quality of the diffusion
model, rendering it more effective for planning tasks. We conduct training and
closed-loop planning tests on the nuPlan dataset, and the results demonstrate
that employing such a generation-then-evaluation strategy outperforms other
learning-based approaches. Additionally, the fine-tuned generative driving
policy shows significant enhancements in planning performance. We further
demonstrate that utilizing our learned reward model for evaluation or RL
fine-tuning leads to better planning performance compared to relying on
human-designed rewards. Project website: https://mczhi.github.io/GenDrive.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Gen - Driveï¼šç”¨å¥–åŠ±å»ºæ¨¡ä¸å¼ºåŒ–å­¦ä¹ å¾®è°ƒé©æ–°æ‰©æ•£ç”Ÿæˆå¼é©¾é©¶ç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œä¼ ç»Ÿçš„é¢„æµ‹åŠ ç¡®å®šæ€§è§„åˆ’æ¡†æ¶å­˜åœ¨ä¸è¶³ã€‚å¸¸è§„é¢„æµ‹ä¸ç¡®å®šæ€§è§„åˆ’æ–¹æ³•å¸¸åˆ†ç¦»é¢„æµ‹å’Œè§„åˆ’è¿‡ç¨‹ï¼Œä½¿è‡ªè½¦è„±ç¦»ç¤¾äº¤æƒ…å¢ƒï¼Œæ˜“äº§ç”Ÿä¸ç¬¦åˆç¤¾äº¤é©¾é©¶è§„èŒƒçš„è¡Œä¸ºï¼›å³ä¾¿é›†æˆé¢„æµ‹ - è§„åˆ’æ¡†æ¶ä¹Ÿä¾èµ–ç¡®å®šæ€§è§„åˆ’ï¼Œéš¾å¤„ç†æ™ºèƒ½ä½“è¡Œä¸ºçš„ä¸ç¡®å®šæ€§ã€å¤šæ¨¡æ€ä¸äº¤äº’åŠ¨æ€æ€§ã€‚åŒæ—¶ï¼Œç”Ÿæˆæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶å†³ç­–ä»»åŠ¡ä¸­åº”ç”¨æœ‰é™ï¼Œä¸€æ–¹é¢è¯„ä¼°ç”Ÿæˆåœºæ™¯å¹¶é€‰æœ€ä¼˜ç”¨äºå†³ç­–ä»¥å¥‘åˆäººç±»æœŸæœ›å¾ˆå¤æ‚ï¼Œå¦ä¸€æ–¹é¢è§„åˆ’ä»»åŠ¡éœ€ç”Ÿæˆæ›´å¯èƒ½çš„æœªæ¥åœºæ™¯ä¸”æ ·æœ¬å°‘ä»¥å‡è®¡ç®—å¼€é”€å’Œå»¶è¿Ÿã€‚æ‰€ä»¥ï¼Œæœ¬æ–‡æå‡ºGen - Driveæ¡†æ¶ï¼Œè½¬å‘ç”Ÿæˆ - è¯„ä¼°è§„åˆ’èŒƒå¼æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ™ºèƒ½ä½“è½¨è¿¹æ‰©æ•£æ¨¡å‹æ„å»º
å¼€å‘å¤šæ™ºèƒ½ä½“è½¨è¿¹æ‰©æ•£æ¨¡å‹ï¼Œå°†è‡ªè½¦èå…¥ç¤¾äº¤äº¤äº’æƒ…å¢ƒï¼Œä¸ºåœºæ™¯ä¸­æ‰€æœ‰æ™ºèƒ½ä½“ç”Ÿæˆå¤šæ ·ä¸”åœºæ™¯ä¸€è‡´çš„æœªæ¥åœºæ™¯ï¼Œå¢å¼ºè”åˆäº¤äº’æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹åŒ…å«åŸºäºæŸ¥è¯¢çš„åœºæ™¯ä¸Šä¸‹æ–‡ç¼–ç å™¨å’Œæ‰©æ•£å¼åœºæ™¯ç”Ÿæˆå™¨ï¼Œå…ˆåˆ©ç”¨å¤§é‡çœŸå®é©¾é©¶æ•°æ®è®­ç»ƒåŸºç¡€æ‰©æ•£æ¨¡å‹ï¼ˆåœºæ™¯ç¼–ç å™¨ + ç”Ÿæˆå™¨ï¼‰ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šVLMè¾…åŠ©çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒ
æå‡ºåœºæ™¯è¯„ä¼°ï¼ˆå¥–åŠ±ï¼‰æ¨¡å‹ï¼Œç”¨VLMè¾…åŠ©æ”¶é›†çš„æˆå¯¹åå¥½æ•°æ®è®­ç»ƒã€‚é€šè¿‡å¯¹åŸºç¡€æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åœºæ™¯é‡‡æ ·ï¼Œç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¾…åŠ©çš„æ··åˆæ ‡è®° pipeline æ„å»ºæˆå¯¹åå¥½æ•°æ®é›†ï¼Œå†åŸºäºè¯¥æ•°æ®é›†è®­ç»ƒåœºæ™¯è¯„ä¼°å™¨ï¼Œä»¥è§£å†³ç”Ÿæˆåœºæ™¯è¯„ä¼°éš¾çš„é—®é¢˜ï¼Œè®©å†³ç­–æ›´ä¼˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼ºåŒ–å­¦ä¹ å¾®è°ƒ pipeline æ­å»º
æ„å»ºå¼ºåŒ–å­¦ä¹ å¾®è°ƒ pipelineï¼ŒåŸºäºå­¦åˆ°çš„å¥–åŠ±æ¨¡å‹å¢å¼ºæ‰©æ•£é©¾é©¶ç­–ç•¥æ€§èƒ½ã€‚åˆ©ç”¨ä»AIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¥å¾®è°ƒæ‰©æ•£ç”Ÿæˆå™¨ï¼Œæå‡å…¶åœ¨è§„åˆ’ä»»åŠ¡ä¸­çš„æ•ˆåŠ›ï¼Œè§£å†³ç”Ÿæˆæ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸­éœ€å°‘æ ·æœ¬ç”Ÿæˆæ›´å¯èƒ½æœªæ¥åœºæ™¯çš„é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨nuPlanæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œé—­ç¯è§„åˆ’æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼šé‡‡ç”¨ç”Ÿæˆ - è¯„ä¼°ç­–ç•¥ä¼˜äºå…¶ä»–åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼›å¾®è°ƒåçš„ç”Ÿæˆå¼é©¾é©¶ç­–ç•¥åœ¨è§„åˆ’æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼›ä¸ä¾èµ–äººå·¥è®¾è®¡å¥–åŠ±ç›¸æ¯”ï¼Œç”¨æ‰€å­¦å¥–åŠ±æ¨¡å‹è¯„ä¼°æˆ–RLå¾®è°ƒèƒ½å¸¦æ¥æ›´å¥½çš„è§„åˆ’æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼è½¬æ¢æ€è·¯ï¼šä»ä¼ ç»Ÿé¢„æµ‹å’Œç¡®å®šæ€§è§„åˆ’è½¬å‘ç”Ÿæˆ - è¯„ä¼°æ¡†æ¶ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶è§„åˆ’ä»»åŠ¡æä¾›äº†æ–°çš„èŒƒå¼å‚è€ƒï¼Œåœ¨å¤„ç†å¤šæ™ºèƒ½ä½“äº¤äº’å’Œä¸ç¡®å®šæ€§ä¸Šæœ‰å€Ÿé‰´æ„ä¹‰ã€‚
2. æ•°æ®åˆ©ç”¨ä¸æ¨¡å‹è®­ç»ƒï¼šåˆ©ç”¨VLMè¾…åŠ©æ„å»ºåå¥½æ•°æ®é›†è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä»¥åŠç»“åˆå¼ºåŒ–å­¦ä¹ å¾®è°ƒç”Ÿæˆæ¨¡å‹çš„æµç¨‹ï¼Œä¸ºè§£å†³ç”Ÿæˆæ¨¡å‹åœ¨å†³ç­–ä»»åŠ¡ä¸­çš„è¯„ä¼°å’Œç”Ÿæˆè´¨é‡é—®é¢˜æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼Œåœ¨å…¶ä»–éœ€å¤„ç†å¤æ‚äº¤äº’å’Œä¸ç¡®å®šæ€§çš„ç”Ÿæˆå¼ä»»åŠ¡ä¸­ä¹Ÿå¯èƒ½é€‚ç”¨ã€‚
3. å¤šæ¨¡å—ååŒè®¾è®¡ï¼šGen - Driveä¸­åœºæ™¯ç¼–ç å™¨ã€æ‰©æ•£ç”Ÿæˆå™¨ã€åœºæ™¯è¯„ä¼°å™¨ç­‰å¤šæ¨¡å—ååŒçš„æ¶æ„è®¾è®¡ï¼Œå¯¹äºæ„å»ºå¤æ‚ä»»åŠ¡ä¸‹çš„æ™ºèƒ½ç³»ç»Ÿæœ‰å‚è€ƒä»·å€¼ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ä¸åŒæ¨¡å—åˆ†å·¥ä¸åä½œæ¥å®ç°æ›´ä¼˜æ€§èƒ½ã€‚

## dr-llava--visual-instruction-tuning-with-symbolic-clinical-grounding
### Abstract
Vision-Language Models (VLM) can support clinicians by analyzing medical
images and engaging in natural language interactions to assist in diagnostic
and treatment tasks. However, VLMs often exhibit "hallucinogenic" behavior,
generating textual outputs not grounded in contextual multimodal information.
This challenge is particularly pronounced in the medical domain, where we do
not only require VLM outputs to be accurate in single interactions but also to
be consistent with clinical reasoning and diagnostic pathways throughout
multi-turn conversations. For this purpose, we propose a new alignment
algorithm that uses symbolic representations of clinical reasoning to ground
VLMs in medical knowledge. These representations are utilized to (i) generate
GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM
conversations with demonstrations of clinical reasoning, and (ii) create an
automatic reward function that evaluates the clinical validity of VLM
generations throughout clinician-VLM interactions. Our algorithm eliminates the
need for human involvement in training data generation or reward model
construction, reducing costs compared to standard reinforcement learning with
human feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a
conversational VLM finetuned for analyzing bone marrow pathology slides,
demonstrating strong performance in multi-turn medical conversations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Dr-LLaVAï¼šç”¨ç¬¦å·åŒ–ä¸´åºŠæ¨ç†é”šå®šåŒ»ç–—åœºæ™¯ä¸‹çš„è§†è§‰è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸæœ‰æ½œåŠ›è¾…åŠ©ä¸´åºŠè¯Šæ–­ä¸äº¤æµï¼Œä½†ç°æœ‰VLMå­˜åœ¨â€œå¹»è§‰â€é—®é¢˜â€”â€”è¾“å‡ºæ—¢å¯èƒ½è„±ç¦»è§†è§‰è¾“å…¥ï¼Œä¹Ÿå¯èƒ½åœ¨å¤šè½®å¯¹è¯ä¸­ä¸ä¸´åºŠæ¨ç†ã€è¯Šæ–­è·¯å¾„çŸ›ç›¾ã€‚ä¼ ç»ŸåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨åŒ»ç–—åœºæ™¯å—é™ï¼šåŒ»ç–—å¤šè½®å¯¹è¯çš„è®­ç»ƒæ•°æ®ç¨€ç¼ºï¼Œä¸”æ”¶é›†ä¸´åºŠä¸“å®¶åé¦ˆæˆæœ¬é«˜ã€éš¾è§„æ¨¡åŒ–ã€‚åŒæ—¶ç°æœ‰åŒ»ç–—VLMå¤šä»…æ”¯æŒå•è½®é—®ç­”ï¼Œæ— æ³•åº”å¯¹å¤æ‚å¤šè½®ä¸´åºŠæ¨ç†å¯¹è¯ï¼Œå› æ­¤éœ€è¦æ–°æ–¹æ³•è®©VLMåœ¨åŒ»ç–—çŸ¥è¯†ä¸ä¸´åºŠé€»è¾‘ä¸‹å¯¹é½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç¬¦å·åŒ–ä¸´åºŠæ¨ç†è¡¨å¾é©±åŠ¨æ•°æ®ç”Ÿæˆ  
å°†ä¸´åºŠè¯Šæ–­è¿‡ç¨‹ï¼ˆå¦‚è¡€æ¶²è‚¿ç˜¤è¯Šæ–­ï¼‰å½¢å¼åŒ–ä¸º**ç¬¦å·åŒ–è§„åˆ™é›†åˆS**ï¼ˆç±»ä¼¼å†³ç­–æ ‘ï¼Œè§„å®šä»å›¾åƒè´¨é‡è¯„ä¼°åˆ°ç»†èƒåˆ†æå†åˆ°æœ€ç»ˆè¯Šæ–­çš„åˆæ³•è·¯å¾„ï¼‰ã€‚åŸºäºè¯¥è§„åˆ™ï¼Œç»“åˆéª¨é«“ç—…ç†å›¾åƒä¸ç—…ç†å­¦å®¶æ ‡æ³¨ï¼Œç”¨GPT - 4è§„æ¨¡åŒ–ç”Ÿæˆâ€œä¸´åºŠåŒ»ç”Ÿ - VLMâ€å¤šè½®å¯¹è¯è®­ç»ƒæ•°æ®ã€‚æ•°æ®é‡Œæ¯ä¸ªå›¾åƒå¯¹åº”å¤šè½®é—®ç­”ï¼ˆå¦‚å…ˆé—®å›¾åƒè´¨é‡ã€å†é—®ç»†èƒç±»å‹ã€æœ€åé—®è¯Šæ–­ï¼‰ï¼Œé—®ç­”å†…å®¹ç”±ç¬¦å·è§„åˆ™å’Œå›¾åƒæ ‡æ³¨æŒ‡å¯¼ç”Ÿæˆï¼Œæ¨¡æ‹ŸçœŸå®ä¸´åºŠæ¨ç†å¯¹è¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªåŠ¨ç¬¦å·åŒ–å¥–åŠ±å‡½æ•°å®ç°å¯¹é½  
è®¾è®¡**åŸºäºç¬¦å·è§„åˆ™çš„è‡ªåŠ¨å¥–åŠ±æœºåˆ¶**ï¼Œæ›¿ä»£äººå·¥åé¦ˆã€‚å¥–åŠ±åˆ†ä¸¤éƒ¨åˆ†ï¼šä¸€æ˜¯â€œæ­£ç¡®æ€§å¥–åŠ±â€ï¼Œæ£€æŸ¥å•è½®å›ç­”æ˜¯å¦ç¬¦åˆå½“å‰æ­¥éª¤çš„ä¸´åºŠçŸ¥è¯†ï¼›äºŒæ˜¯â€œä¸€è‡´æ€§å¥–åŠ±â€ï¼Œç¡®ä¿å¤šè½®å¯¹è¯æ•´ä½“ç¬¦åˆä¸´åºŠæ¨ç†è·¯å¾„ï¼ˆå¦‚å…ˆåˆ¤å®šå›¾åƒé«˜è´¨é‡ï¼Œåç»­è¯Šæ–­æ‰åˆæ³•ï¼‰ã€‚è¯¥å¥–åŠ±å‡½æ•°æ— éœ€äººå·¥æ ‡æ³¨ï¼Œé™ä½RLHFçš„äººåŠ›ä¾èµ–ï¼Œè®©VLMåœ¨å¤šè½®å¯¹è¯ä¸­æ—¢å•è½®å‡†ç¡®ã€åˆæ•´ä½“é€»è¾‘è‡ªæ´½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé¢å‘éª¨é«“ç—…ç†çš„å¤šè½®å¯¹è¯VLMè½åœ°  
åŸºäºä¸Šè¿°æ•°æ®ç”Ÿæˆä¸å¥–åŠ±æœºåˆ¶ï¼Œå¯¹LLaVAæ¨¡å‹å¾®è°ƒå¾—åˆ°**Dr - LLaVA**ï¼Œä¸“æ”»éª¨é«“ç—…ç†åˆ‡ç‰‡åˆ†æçš„å¤šè½®å¯¹è¯è¯Šæ–­ã€‚ä»æ•°æ®æ„å»ºåˆ°æ¨¡å‹è®­ç»ƒå…¨æµç¨‹ï¼Œç”¨ç¬¦å·åŒ–ä¸´åºŠçŸ¥è¯†é”šå®šVLMï¼Œä½¿å…¶é€‚åº”åŒ»ç–—åœºæ™¯ä¸‹â€œè§†è§‰ + å¤šè½®è¯­è¨€äº¤äº’ + ä¸´åºŠé€»è¾‘â€çš„å¤æ‚ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. å•è½®ä¸å¤šè½®å¯¹è¯æ€§èƒ½è¶…è¶ŠSOTAï¼šåœ¨éª¨é«“ç—…ç†å›¾åƒçš„é—®ç­”ä»»åŠ¡ä¸­ï¼ŒDr - LLaVAåœ¨å•è½®å‡†ç¡®æ€§ã€å¤šè½®å¯¹è¯çš„è¿è´¯æ€§ä¸ä¸´åºŠåˆè§„æ€§ä¸Šï¼Œä¼˜äºç°æœ‰é€šç”¨æˆ–åŒ»ç–—é¢†åŸŸVLMï¼ˆå¦‚LLaVA - Medç­‰ï¼‰ã€‚  
2. é²æ£’æ€§ä¸çº é”™èƒ½åŠ›çªå‡ºï¼šæ¶ˆèå®éªŒæ˜¾ç¤ºï¼ŒåŸºäºç¬¦å·è§„åˆ™çš„æŒ‡ä»¤å¾®è°ƒè®©Dr - LLaVAå¯¹é—®é¢˜é¡ºåºå˜åŒ–æ›´é²æ£’ï¼Œä¸”åœ¨è¯†åˆ«ã€ä¿®æ­£ä¸´åºŠæé—®é‡Œçš„é”™è¯¯ä¿¡æ¯æ—¶ï¼Œè¡¨ç°è¿œè¶…åŸºçº¿æ¨¡å‹ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸçŸ¥è¯†ç¬¦å·åŒ–ï¼šå°†ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰çš„æ¨ç†é€»è¾‘æ‹†è§£ä¸ºç¬¦å·è§„åˆ™ï¼Œä¸ºæ•°æ®ç”Ÿæˆå’Œæ¨¡å‹è¯„ä¼°æä¾›ç»“æ„åŒ–é”šç‚¹ï¼Œè¿™ç§â€œé¢†åŸŸçŸ¥è¯†æ˜¾å¼å»ºæ¨¡â€æ€è·¯å¯æ¨å¹¿åˆ°æ³•å¾‹ã€å·¥ä¸šæ£€æµ‹ç­‰å¼ºä¸“ä¸šé€»è¾‘é¢†åŸŸã€‚  
2. è‡ªåŠ¨åŒ–æ›¿ä»£äººå·¥åé¦ˆï¼šç”¨ç¬¦å·è§„åˆ™æ„å»ºè‡ªåŠ¨å¥–åŠ±å‡½æ•°ï¼Œå‡å°‘RLHFå¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä¸ºèµ„æºæœ‰é™æˆ–éœ€ä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸæ¨¡å‹å¯¹é½æä¾›äº†ä½æˆæœ¬è·¯å¾„ã€‚  
3. å¤šè½®å¯¹è¯åœºæ™¯é€‚é…ï¼šèšç„¦å¤šè½®äº¤äº’ä¸‹çš„é€»è¾‘ä¸€è‡´æ€§ï¼Œçªç ´å•è½®QAå±€é™ï¼Œä¸ºVLMåœ¨å®¢æœã€è¯Šç–—ã€æ•™è‚²ç­‰éœ€è¿ç»­å¯¹è¯çš„åœºæ™¯è½åœ°æä¾›æ–¹æ³•è®ºå‚è€ƒã€‚

## parameter-efficient-reinforcement-learning-from-human-feedback
### Abstract
While Reinforcement Learning from Human Feedback (RLHF) effectively aligns
pretrained Large Language and Vision-Language Models (LLMs, and VLMs) with
human preferences, its computational cost and complexity hamper its wider
adoption. To alleviate some of the computational burden of fine-tuning,
parameter efficient methods, like LoRA were introduced. In this work, we
empirically evaluate the setup of Parameter Efficient Reinforcement Learning
from Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward
Modeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six
diverse datasets spanning summarization, harmless/helpful response generation,
UI automation, and visual question answering in terms of effectiveness of the
trained models, and the training resources required. Our findings show, for the
first time, that PE-RLHF achieves comparable performance to RLHF, while
significantly reducing training time (up to 90% faster for reward models, and
30% faster for RL), and memory footprint (up to 50% reduction for reward
models, and 27% for RL). We provide comprehensive ablations across LoRA ranks,
and model sizes for both reward modeling and reinforcement learning. By
mitigating the computational burden associated with RLHF, we push for a broader
adoption of PE-RLHF as an alignment technique for LLMs and VLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜æ•ˆå‚æ•°åŒ–ï¼šè®©äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ›´æ˜“ç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¦ä¸äººç±»åå¥½å¯¹é½ï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¸¸ç”¨æ–¹æ³•ï¼Œä½†å®ƒè®¡ç®—æˆæœ¬é«˜ã€å¤æ‚åº¦å¤§ï¼Œé™åˆ¶äº†å¹¿æ³›åº”ç”¨ã€‚æ¯”å¦‚RLHFè®­ç»ƒæ—¶éœ€è¦é¢å¤–çš„æ¨¡å‹å‰¯æœ¬ï¼ˆå¥–åŠ±æ¨¡å‹ã€KLæ­£åˆ™åŒ–çš„é”šæ¨¡å‹ç­‰ï¼‰ï¼Œå†…å­˜å ç”¨æ¯”æ ‡å‡†å¾®è°ƒå¤§å¾ˆå¤šã€‚ä¸ºç¼“è§£å¾®è°ƒçš„è®¡ç®—è´Ÿæ‹…ï¼Œå‚æ•°é«˜æ•ˆæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰åº”è¿è€Œç”Ÿï¼Œæœ¬æ–‡å°±èšç„¦äºåŸºäºLoRAçš„å‚æ•°é«˜æ•ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆPE - RLHFï¼‰å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPE - RLHFæ•´ä½“æ¡†æ¶
PE - RLHFæŠŠå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åº”ç”¨åˆ°RLHFçš„ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼ˆå¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œç­–ç•¥æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼‰ã€‚åœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒæ—¶ï¼Œæ„å»ºå¸¦LoRAé€‚é…å™¨çš„å¥–åŠ±æ¨¡å‹ï¼Œé€‚é…å™¨è¿åˆ°æ¨¡å‹æ¯ä¸ªæ³¨æ„åŠ›æŠ•å½±çŸ©é˜µï¼Œè®­ç»ƒä»…æ›´æ–°é€‚é…å™¨ï¼Œæ¨¡å‹ backbone å†»ç»“ï¼›å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œç­–ç•¥å’Œä»·å€¼æ¨¡å‹ä¹Ÿç”¨LoRAé€‚é…å™¨ï¼ŒåŒæ ·å†»ç»“ backbone è®­ç»ƒé€‚é…å™¨ï¼Œå†åŸºäºä»·å€¼æ¨¡å‹è®¡ç®—çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ç­–ç•¥ï¼Œä»·å€¼æ¨¡å‹ç»“åˆå¥–åŠ±åˆ†æ•°å’Œä¸é”šç­–ç•¥çš„KLæ­£åˆ™åŒ–è®­ç»ƒã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢å¯¹æ¯”ä¸æ¶ˆèå®éªŒ
åœ¨å…­ä¸ªä¸åŒæ•°æ®é›†ï¼ˆæ¶µç›–æ‘˜è¦ã€æ— å®³/æœ‰ç”¨å“åº”ç”Ÿæˆã€UIè‡ªåŠ¨åŒ–ã€è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ï¼‰ä¸Šå¯¹æ¯”PE - RLHFå’Œæ ‡å‡†RLHFï¼›è¿˜å¯¹LoRAçš„ç§©ï¼ˆrankï¼‰ã€æ¨¡å‹å¤§å°åœ¨å¥–åŠ±å»ºæ¨¡å’Œå¼ºåŒ–å­¦ä¹ ä¸­åšäº†ç³»ç»Ÿæ¶ˆèå®éªŒï¼Œæ¢ç©¶LoRAå¯¹è®­ç»ƒçš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•ˆæœä¸Šï¼ŒPE - RLHFèƒ½è¾¾åˆ°å’Œæ ‡å‡†RLHFç›¸å½“çš„æ€§èƒ½ã€‚èµ„æºæ¶ˆè€—ä¸Šï¼Œå¥–åŠ±æ¨¡å‹è®­ç»ƒæ—¶é—´æœ€å¤šå¿«90%ï¼Œå†…å­˜å ç”¨æœ€å¤šé™50%ï¼›RLé˜¶æ®µè®­ç»ƒæ—¶é—´æœ€å¤šå¿«30%ï¼Œå†…å­˜å ç”¨æœ€å¤šé™27%ã€‚æ¯”å¦‚å¥–åŠ±æ¨¡å‹è®­ç»ƒæ—¶ï¼ŒPE - RLHFåœ¨éƒ¨åˆ†æ•°æ®é›†HBMå³°å€¼å ç”¨ä»…ä¸ºæ ‡å‡†è®­ç»ƒçš„43 - 74%ï¼Œè®­ç»ƒé€Ÿåº¦å¿«1.4 - 1.9å€ï¼›RLé˜¶æ®µHBMå³°å€¼å ç”¨ä¸ºæ ‡å‡†çš„73 - 80%ï¼Œè®­ç»ƒé€Ÿåº¦å¿«1.15 - 1.3å€ç­‰ï¼ˆä¸åŒæ•°æ®é›†æœ‰å·®å¼‚ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ–¹æ³•å±‚é¢ï¼Œè¯æ˜äº†LoRAè¿™ç±»å‚æ•°é«˜æ•ˆæ–¹æ³•åœ¨RLHFä¸­èƒ½åœ¨ä¿æŒæ•ˆæœçš„åŒæ—¶å¤§å¹…é™ä½èµ„æºæ¶ˆè€—ï¼Œä¸ºåç»­ç”¨å…¶ä»–å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æˆ–è¡¨ç¤ºå¾®è°ƒï¼ˆReFTï¼‰æ–¹æ³•åšRLHFä»»åŠ¡çš„åŸºå‡†æµ‹è¯•æä¾›äº†å‚è€ƒï¼›å®è·µå±‚é¢ï¼Œè®©RLHFçš„è®¡ç®—è´Ÿæ‹…é™ä½ï¼Œæ¨åŠ¨å…¶æ›´å¹¿æ³›åº”ç”¨ï¼Œä¸ºå¤§æ¨¡å‹å¯¹é½äººç±»åå¥½æä¾›äº†æ›´é«˜æ•ˆçš„æŠ€æœ¯è·¯çº¿ï¼Œåç»­ç ”ç©¶å‚æ•°é«˜æ•ˆçš„å¯¹é½æ–¹æ³•æ—¶å¯å€Ÿé‰´å…¶è®¾è®¡å®éªŒå’Œå¯¹æ¯”åˆ†æçš„æ€è·¯ã€‚

## tuning-large-multimodal-models-for-videos-using-reinforcement-learning-from-ai-feedback
### Abstract
Recent advancements in large language models have influenced the development
of video large multimodal models (VLMMs). The previous approaches for VLMMs
involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets,
integrating LLM with visual encoders, and adding additional learnable modules.
Video and text multimodal alignment remains challenging, primarily due to the
deficient volume and quality of multimodal instruction-tune data compared to
text-only data. We present a novel alignment strategy that employs multimodal
AI system to oversee itself called Reinforcement Learning from AI Feedback
(RLAIF), providing self-preference feedback to refine itself and facilitating
the alignment of video and text modalities. In specific, we propose
context-aware reward modeling by providing detailed video descriptions as
context during the generation of preference feedback in order to enrich the
understanding of video content. Demonstrating enhanced performance across
diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms
existing approaches, including the SFT model. We commit to open-sourcing our
code, models, and datasets to foster further research in this area.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨AIåé¦ˆå¼ºåŒ–å­¦ä¹ è§£é”è§†é¢‘å¤§æ¨¡å‹å¯¹é½æ–°å§¿åŠ¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•æ¨åŠ¨äº†è§†é¢‘å¤§ multimodal æ¨¡å‹ï¼ˆVLMMsï¼‰çš„è¿›æ­¥ï¼Œä½†è§†é¢‘ä¸æ–‡æœ¬çš„å¤šæ¨¡æ€å¯¹é½ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä»¥å¾€ VLMMs é‡‡ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç­‰æ–¹æ³•ï¼Œç„¶è€Œå¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®åœ¨æ•°é‡å’Œè´¨é‡ä¸Šè¿œä¸åŠçº¯æ–‡æœ¬æ•°æ®ï¼Œè¿™å¯¼è‡´è§†é¢‘å†…å®¹ä¸æ–‡æœ¬çš„å¯¹é½æ•ˆæœä¸ä½³ï¼Œæ¨¡å‹ç”Ÿæˆçš„å“åº”éš¾ä»¥å¾ˆå¥½åœ°åŸºäºè§†é¢‘è§†è§‰å†…å®¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºåŸºäºAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰çš„è§†é¢‘æ–‡æœ¬å¯¹é½æ–¹æ³•  
ä¸åŒäºä¾èµ–äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼ŒRLAIF åˆ©ç”¨å¤šæ¨¡æ€AIç³»ç»Ÿè‡ªæˆ‘ç›‘ç£ï¼Œé€šè¿‡æä¾›ç”Ÿæˆå“åº”çš„è‡ªæˆ‘åå¥½åé¦ˆæ¥ä¼˜åŒ–è‡ªèº«ï¼Œå®ç°è§†é¢‘å’Œæ–‡æœ¬æ¨¡æ€çš„å¯¹é½ï¼Œå‡å°‘äº†å¯¹å¤§è§„æ¨¡äººç±»æ ‡æ³¨åå¥½çš„ä¾èµ–ï¼Œè®©ç›‘ç£æ›´å…·å¯æ‰©å±•æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¥–åŠ±å»ºæ¨¡  
åœ¨ç”Ÿæˆåå¥½åé¦ˆæ—¶ï¼Œå°†è¯¦ç»†çš„è§†é¢‘æè¿°ä½œä¸ºä¸Šä¸‹æ–‡èå…¥å¤šæ¨¡æ€AIç³»ç»Ÿï¼Œä»¥æ­¤ä¸°å¯Œå¯¹è§†é¢‘å†…å®¹çš„ç†è§£ï¼Œæå‡è§†é¢‘å†…å®¹åœ¨åé¦ˆè¿‡ç¨‹ä¸­çš„æ¸…æ™°åº¦ï¼Œè®©å¥–åŠ±æ¨¡å‹èƒ½æ›´ç²¾å‡†è¯„ä¼°å“åº”ä¸è§†é¢‘å†…å®¹çš„åŒ¹é…åº¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ‰©å……SFTè®­ç»ƒæ•°æ®ä¸è¯¾ç¨‹è®­ç»ƒç­–ç•¥  
ä¸ºå¼¥è¡¥SFTè®­ç»ƒæ—¶å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®çš„ä¸è¶³ï¼Œå¼•å…¥äººç±»æ ‡æ³¨çš„è§†é¢‘é—®ç­”å’Œä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼›åŒæ—¶æå‡ºç®€å•çš„è¯¾ç¨‹è®­ç»ƒç­–ç•¥ï¼Œæ¥å¢å¼ºè§†é¢‘ä¸æ–‡æœ¬æ¨¡æ€é—´çš„å¯¹é½æ•ˆæœã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šç§è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚è§†é¢‘é—®ç­”ã€æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢ã€åŠ¨ä½œè¯†åˆ«ç­‰ä»»åŠ¡å¯¹åº”çš„åŸºå‡†ï¼‰ä¸Šï¼Œæå‡ºçš„ VLM - RLAIF æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬ SFT æ¨¡å‹ï¼‰ï¼Œåœ¨ä¸åŒç±»å‹çš„è§†é¢‘ä»»åŠ¡ä¸­éƒ½å±•ç°å‡ºæ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨è§†é¢‘å¤šæ¨¡æ€å¯¹é½ä¸ä»»åŠ¡å¤„ç†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¤šæ¨¡æ€å¯¹é½æ–°æ€è·¯ï¼šRLAIF ä¸ºè§£å†³å¤šæ¨¡æ€æ•°æ®ä¸è¶³ä¸‹çš„æ¨¡æ€å¯¹é½é—®é¢˜æä¾›äº†åˆ›æ–°æ–¹å‘ï¼Œå±•ç¤ºäº†ç”¨AIè‡ªæˆ‘åé¦ˆæ¥ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„æ½œåŠ›ã€‚  
2. ä¸Šä¸‹æ–‡åˆ©ç”¨æ–¹å¼ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥å¥–åŠ±å»ºæ¨¡ä¸­å¯¹è§†é¢‘æè¿°ç­‰ä¸Šä¸‹æ–‡çš„è¿ç”¨ï¼Œå¯å‘åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å¯é€šè¿‡è¡¥å……è¯¦ç»†æ¨¡æ€ä¿¡æ¯æ¥æå‡æ¨¡å‹å¯¹å†…å®¹çš„ç†è§£æ·±åº¦ã€‚  
3. æ•°æ®ä¸è®­ç»ƒç­–ç•¥ï¼šæ‰©å……æ•°æ®ç»“åˆè¯¾ç¨‹è®­ç»ƒçš„æ–¹å¼ï¼Œä¸ºåº”å¯¹æ•°æ®ç¨€ç¼ºé—®é¢˜ã€é«˜æ•ˆè®­ç»ƒæ¨¡å‹æä¾›äº†å¯å‚è€ƒçš„å®è·µæ€è·¯ï¼Œåœ¨å…¶ä»–æ•°æ®æœ‰é™çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ä¹Ÿæœ‰å€Ÿé‰´ä»·å€¼ã€‚  
4. å¼€æºæ‰¿è¯ºï¼šä½œè€…å›¢é˜Ÿæ‰¿è¯ºå¼€æºä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ï¼Œåˆ©äºè¯¥é¢†åŸŸåç»­ç ”ç©¶äººå‘˜åŸºäºæ­¤è¿›ä¸€æ­¥æ¢ç´¢ï¼Œæ¨åŠ¨æ•´ä¸ªè§†é¢‘å¤§ multimodal æ¨¡å‹é¢†åŸŸå‘å±•ã€‚

## vision-language-models-are-zero-shot-reward-models-for-reinforcement-learning
### Abstract
Reinforcement learning (RL) requires either manually specifying a reward
function, which is often infeasible, or learning a reward model from a large
amount of human feedback, which is often very expensive. We study a more
sample-efficient alternative: using pretrained vision-language models (VLMs) as
zero-shot reward models (RMs) to specify tasks via natural language. We propose
a natural and general approach to using VLMs as reward models, which we call
VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn
complex tasks without a manually specified reward function, such as kneeling,
doing the splits, and sitting in a lotus position. For each of these tasks, we
only provide a single sentence text prompt describing the desired task with
minimal prompt engineering. We provide videos of the trained agents at:
https://sites.google.com/view/vlm-rm. We can improve performance by providing a
second "baseline" prompt and projecting out parts of the CLIP embedding space
irrelevant to distinguish between goal and baseline. Further, we find a strong
scaling effect for VLM-RMs: larger VLMs trained with more compute and data are
better reward models. The failure modes of VLM-RMs we encountered are all
related to known capability limitations of current VLMs, such as limited
spatial reasoning ability or visually unrealistic environments that are far
off-distribution for the VLM. We find that VLM-RMs are remarkably robust as
long as the VLM is large enough. This suggests that future VLMs will become
more and more useful reward models for a wide range of RL applications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§†è§‰-è¯­è¨€æ¨¡å‹ï¼šå¼ºåŒ–å­¦ä¹ çš„é›¶æ ·æœ¬å¥–åŠ±æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŸºäºè§†è§‰çš„ä»»åŠ¡ä¸­è®­ç»ƒæ™ºèƒ½ä½“æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶ï¼Œå¥–åŠ±å‡½æ•°çš„æŒ‡å®šæˆæœ¬å¾ˆé«˜ã€‚æ‰‹åŠ¨æŒ‡å®šç°å®ä¸–ç•Œä»»åŠ¡çš„å¥–åŠ±å‡½æ•°å¾€å¾€ä¸å¯è¡Œï¼Œè€Œä»äººç±»åé¦ˆä¸­å­¦ä¹ å¥–åŠ±æ¨¡å‹é€šå¸¸æˆæœ¬æ˜‚è´µã€‚ä¸ºäº†è®©å¼ºåŒ–å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­æ›´æœ‰ç”¨ï¼Œéœ€è¦æ‰¾åˆ°æ›´é«˜æ•ˆä¸”è‡ªç„¶çš„å¥–åŠ±å‡½æ•°æŒ‡å®šæ–¹å¼ã€‚æ­¤å‰åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›å¥–åŠ±çš„å°è¯•ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡å¾®è°ƒVLMsï¼Œè¦ä¹ˆéœ€è¦å¤æ‚çš„ä¸´æ—¶ç¨‹åºæ¥æå–å¥–åŠ±ï¼Œè€Œæœ¬æ–‡æ¢ç´¢ç”¨é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å®šä»»åŠ¡ï¼Œä»¥æ›´é«˜æ•ˆåœ°è§£å†³å¥–åŠ±å‡½æ•°æŒ‡å®šéš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºVLM - RMæ–¹æ³•  
æå‡ºVLM - RMè¿™ä¸€é€šç”¨æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ç”¨ä½œåŸºäºè§†è§‰çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„å¥–åŠ±æ¨¡å‹ã€‚å…·ä½“å®ç°ä¸Šï¼Œä½¿ç”¨CLIPä½œä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå°†å½“å‰ç¯å¢ƒçŠ¶æ€çš„CLIPåµŒå…¥ä¸ç®€å•è¯­è¨€æç¤ºä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±å‡½æ•°ã€‚è¿˜å¯é€šè¿‡æä¾›æè¿°ç¯å¢ƒä¸­æ€§çŠ¶æ€çš„â€œåŸºçº¿æç¤ºâ€ï¼Œåœ¨è®¡ç®—å¥–åŠ±æ—¶å°†è¡¨ç¤ºéƒ¨åˆ†æŠ•å½±åˆ°åŸºçº¿å’Œç›®æ ‡æç¤ºä¹‹é—´çš„æ–¹å‘ä¸Šï¼Œå¯¹å¥–åŠ±æ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šåœºæ™¯éªŒè¯ä¸ä»»åŠ¡æ‹“å±•  
åœ¨æ ‡å‡†çš„CartPoleå’ŒMountainCarå¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­éªŒè¯æ–¹æ³•ï¼Œè§‚å¯Ÿåˆ°VLM - RMsä¸ç¯å¢ƒçš„çœŸå®å¥–åŠ±é«˜åº¦ç›¸å…³ï¼Œä¸”èƒ½æˆåŠŸè®­ç»ƒç­–ç•¥è§£å†³ä»»åŠ¡ï¼›è¿˜è®­ç»ƒMuJoCoäººå½¢æœºå™¨äººå­¦ä¹ å¤æ‚ä»»åŠ¡ï¼Œå¦‚ä¸¾è‡‚ã€è²èŠ±åã€åŠˆå‰å’Œ kneeling ç­‰ï¼Œä»…ç”¨å•å¥æ–‡æœ¬æç¤ºï¼ˆå¦‚â€œa humanoid robot kneelingâ€ï¼‰è¡ç”Ÿçš„CLIPå¥–åŠ±æ¨¡å‹æ¥å®ç°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¢ç´¢æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½å…³ç³»  
ç ”ç©¶VLM - RMsçš„æ€§èƒ½å¦‚ä½•éšè§†è§‰-è¯­è¨€æ¨¡å‹è§„æ¨¡æ‰©å±•ï¼Œå‘ç°è§†è§‰-è¯­è¨€æ¨¡å‹è§„æ¨¡ä¸VLM - RMè´¨é‡å¼ºç›¸å…³ï¼Œåƒå›¾1ä¸­çš„äººå½¢æœºå™¨äººä»»åŠ¡åªèƒ½ç”¨æœ€å¤§çš„å…¬å¼€å¯ç”¨CLIPæ¨¡å‹å®Œæˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨CartPoleå’ŒMountainCaråŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLM - RMsä¸ç¯å¢ƒçœŸå®å¥–åŠ±é«˜åº¦ç›¸å…³ï¼Œèƒ½æˆåŠŸè®­ç»ƒç­–ç•¥å®Œæˆä»»åŠ¡ï¼Œä¸”æ›´é€¼çœŸçš„ç¯å¢ƒçº¹ç†æ¸²æŸ“èƒ½æå‡CLIPä½œä¸ºå¥–åŠ±æ¨¡å‹çš„è´¨é‡ï¼›åœ¨MuJoCoäººå½¢æœºå™¨äººå¤æ‚ä»»åŠ¡è®­ç»ƒä¸­ï¼Œä»…ç”¨å•å¥æ–‡æœ¬æç¤ºçš„CLIPå¥–åŠ±æ¨¡å‹å°±å®ç°äº†å¦‚ä¸¾è‡‚ã€è²èŠ±åã€åŠˆå‰å’Œ kneeling ç­‰ä»»åŠ¡ï¼›è¿˜å‘ç°VLM - RMså­˜åœ¨å¼ºç¼©æ”¾æ•ˆåº”ï¼Œæ›´å¤§çš„VLMsï¼ˆç”¨æ›´å¤šè®¡ç®—å’Œæ•°æ®è®­ç»ƒï¼‰æ˜¯æ›´å¥½çš„å¥–åŠ±æ¨¡å‹ï¼ŒåŒæ—¶é‡åˆ°çš„å¤±è´¥æ¨¡å¼ä¸å½“å‰VLMså·²çŸ¥èƒ½åŠ›é™åˆ¶ï¼ˆå¦‚ç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ã€è§†è§‰ä¸çœŸå®ç¯å¢ƒåˆ†å¸ƒåç§»å¤§ï¼‰ç›¸å…³ï¼Œä¸”è¶³å¤Ÿå¤§çš„VLMsä¸‹VLM - RMséå¸¸ç¨³å¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä¸ºå¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°æŒ‡å®šæä¾›äº†æ›´é«˜æ•ˆè‡ªç„¶çš„æ€è·¯ï¼Œè¯æ˜é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹å¯ä½œä¸ºé›¶æ ·æœ¬å¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡å¥–åŠ±å‡½æ•°æˆ–æ”¶é›†æ˜‚è´µæ•°æ®æ¥å­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼›æå‡ºçš„Goal - Baseline Regularizationç­‰æŠ€å·§å¯æå‡å¥–åŠ±æ¨¡å‹è´¨é‡ï¼Œä¸ºåç»­ä¼˜åŒ–å¥–åŠ±æ¨¡å‹æä¾›æ–¹æ³•å‚è€ƒï¼›æ­ç¤ºçš„è§†è§‰-è¯­è¨€æ¨¡å‹è§„æ¨¡ä¸VLM - RMè´¨é‡çš„å¼ºç›¸å…³æ€§ï¼Œä¸ºåç»­æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–æŒ‡æ˜æ–¹å‘ï¼Œå³æ›´å¤§æ›´å¼ºå¤§çš„VLMsæœ‰æœ›æˆä¸ºæ›´æœ‰ç”¨çš„å¥–åŠ±æ¨¡å‹ï¼ŒåŠ©åŠ›ä»äººç±»ç¼–å†™çš„ä»»åŠ¡æè¿°ä¸­è®­ç»ƒæ¨¡å‹æ‰§è¡Œæ›´å¤æ‚ä»»åŠ¡ã€‚

## test-time-adaptation-with-clip-reward-for-zero-shot-generalization-in-vision-language-models
### Abstract
One fascinating aspect of pre-trained vision-language models~(VLMs) learning
under language supervision is their impressive zero-shot generalization
capability. However, this ability is hindered by distribution shifts between
the training and testing data. Previous test time adaptation~(TTA) methods for
VLMs in zero-shot classification rely on minimizing the entropy of model
outputs, tending to be stuck in incorrect model predictions. In this work, we
propose TTA with feedback to rectify the model output and prevent the model
from becoming blindly confident. Specifically, a CLIP model is adopted as the
reward model during TTA and provides feedback for the VLM. Given a single test
sample, the VLM is forced to maximize the CLIP reward between the input and
sampled results from the VLM output distribution. The proposed
\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly
flexible and universal. Beyond the classification task, with task-specific
sampling strategies and a proper reward baseline choice, RLCF can be easily
extended to not only discrimination tasks like retrieval but also
generalization tasks like image captioning, improving the zero-shot
generalization capacity of VLMs. According to the characteristics of these VL
tasks, we build different fully TTA pipelines with RLCF to improve the
zero-shot generalization ability of various VLMs. Extensive experiments along
with promising empirical results demonstrate the effectiveness of RLCF. The
code is available at https://github.com/mzhaoshuai/RLCF.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨CLIPå¥–åŠ±å®ç°æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é¢„è®­ç»ƒè§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¯­è¨€ç›‘ç£ä¸‹å­¦ä¹ å…·å¤‡å‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é—´çš„åˆ†å¸ƒåç§»ä¼šé˜»ç¢è¿™ä¸€èƒ½åŠ›ã€‚æ­¤å‰é’ˆå¯¹VLMsé›¶æ ·æœ¬åˆ†ç±»çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ä¾èµ–æœ€å°åŒ–æ¨¡å‹è¾“å‡ºç†µï¼Œæ˜“é™·å…¥é”™è¯¯é¢„æµ‹ä¸”ä½¿æ¨¡å‹ç›²ç›®è‡ªä¿¡ã€‚åŒæ—¶ï¼Œåœ¨æ— æ ‡æ³¨æƒ…å†µä¸‹å¦‚ä½•æœ‰æ•ˆä¿®æ­£æ¨¡å‹è¾“å‡ºã€æå‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›æ˜¯å¾…è§£å†³çš„é—®é¢˜ï¼Œæœ¬æ–‡ç”±æ­¤å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRLCFæ¡†æ¶
æå‡ºå¼ºåŒ–å­¦ä¹ ç»“åˆCLIPåé¦ˆï¼ˆRLCFï¼‰çš„æ¡†æ¶ç”¨äºæµ‹è¯•æ—¶è‡ªé€‚åº”ã€‚é‡‡ç”¨CLIPä½œä¸ºå¥–åŠ±æ¨¡å‹åœ¨æµ‹è¯•æ—¶æä¾›åé¦ˆï¼Œå¯¹äºå•ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œè¿«ä½¿VLMæœ€å¤§åŒ–è¾“å…¥ä¸ä»VLMè¾“å‡ºåˆ†å¸ƒé‡‡æ ·ç»“æœé—´çš„CLIPå¥–åŠ±ï¼Œä»¥ä¿®æ­£æ¨¡å‹è¾“å‡ºï¼Œé¿å…æ¨¡å‹ç›²ç›®è‡ªä¿¡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¡†æ¶çš„çµæ´»æ€§ä¸é€šç”¨æ€§
RLCFæ¡†æ¶æå…·çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚å€ŸåŠ©ç‰¹å®šä»»åŠ¡çš„é‡‡æ ·ç­–ç•¥ä¸åˆé€‚çš„å¥–åŠ±åŸºçº¿é€‰æ‹©ï¼Œé™¤åˆ†ç±»ä»»åŠ¡å¤–ï¼Œè¿˜èƒ½è½»æ¾æ‰©å±•åˆ°æ£€ç´¢ç­‰åˆ¤åˆ«ä»»åŠ¡ä»¥åŠå›¾åƒæè¿°ç­‰æ³›åŒ–ä»»åŠ¡ï¼Œæå‡VLMsçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé’ˆå¯¹ä¸åŒä»»åŠ¡è®¾è®¡TTA pipeline
ä¾æ®è§†è§‰ - è¯­è¨€ï¼ˆVLï¼‰ä»»åŠ¡ç‰¹æ€§ï¼Œä¸ºä¸åŒä»»åŠ¡æ„å»ºTTAæµç¨‹ã€‚å¦‚åˆ†ç±»ä»»åŠ¡ç»§æ‰¿ç›¸å…³ pipeline é€‚é…å‰ç¼€è°ƒä¼˜å’Œéª¨å¹²ç½‘ç»œè‡ªé€‚åº”ï¼›æ£€ç´¢ä»»åŠ¡ä¸ºæ•ˆç‡ä»…æ›´æ–°ä¸æŸ¥è¯¢ç›¸å…³å‚æ•°ï¼›å›¾åƒæè¿°ä»»åŠ¡æ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„TTAæµç¨‹ï¼Œä¸”åº”ç”¨ä»»åŠ¡æ— å…³å®ç”¨æŠ€å·§ï¼ˆå¤šå¥–åŠ±æ¨¡å‹ã€æƒ…æ™¯å¼TTAã€å¢é‡å­¦ä¹ åŠ¨é‡ç¼“å†²ç­‰ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­é€šè¿‡å¤§é‡å®éªŒéªŒè¯RLCFçš„æœ‰æ•ˆæ€§ï¼Œåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€æ–‡æœ¬ - å›¾åƒæ£€ç´¢ã€å›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸­æå‡äº†ä¸åŒVLMsçš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæœ‰åŠ›è¯æ˜äº†RLCFåœ¨å¢å¼ºVLMsé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šå°†åé¦ˆæœºåˆ¶å¼•å…¥æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼Œä¸ºè§£å†³åˆ†å¸ƒåç§»ä¸‹æ¨¡å‹æ³›åŒ–é—®é¢˜æä¾›æ–°è§†è§’ï¼Œä¸”åˆ©ç”¨CLIPä½œä¸ºæ— æ ‡æ³¨ä¸‹çš„å¯é å¥–åŠ±æ¨¡å‹ï¼Œä¸ºæ— ç›‘ç£æˆ–é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„æ¨¡å‹ä¼˜åŒ–æä¾›æ€è·¯ã€‚
2. æ¡†æ¶å¤ç”¨ï¼šRLCFæ¡†æ¶çš„çµæ´»æ€§è®¾è®¡ï¼Œå±•ç¤ºäº†å¦‚ä½•æ‰“é€ é€šç”¨å‹çš„è·¨ä»»åŠ¡è‡ªé€‚åº”æ¡†æ¶ï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡æ‹“å±•æ–¹é¢æœ‰å€Ÿé‰´æ„ä¹‰ã€‚
3. ä»»åŠ¡å®šåˆ¶ï¼šé’ˆå¯¹ä¸åŒVLä»»åŠ¡å®šåˆ¶TTA pipelineçš„æ–¹å¼ï¼Œä¸ºåç»­ç ”ç©¶ä¸­æ ¹æ®ä»»åŠ¡ç‰¹æ€§è®¾è®¡é€‚é…æ–¹æ³•æä¾›äº†å‚è€ƒèŒƒå¼ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šé‡‡æ ·ã€å‚æ•°æ›´æ–°ç­–ç•¥åŠå®ç”¨æŠ€å·§è¿ç”¨ç­‰æ–¹é¢ã€‚

