# Paper List of Terms(reward model+data)
- [25/07] **Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities**  
[[Paper](http://arxiv.org/pdf/2507.13158v1)] [[Code/Page]()] [[TLDR/Notes](#inverse-reinforcement-learning-meets-large-language-model-post-training--basics--advances--and-opportunities)]

- [25/07] **Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**  
[[Paper](http://arxiv.org/pdf/2507.11155v1)] [[Code/Page]()] [[TLDR/Notes](#bridging-the-gap-in-vision-language-models-in-identifying-unsafe-concepts-across-modalities)]

- [25/07] **EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique**  
[[Paper](http://arxiv.org/pdf/2507.09374v1)] [[Code/Page]()] [[TLDR/Notes](#eduflow--advancing-mllms--problem-solving-proficiency-through-multi-stage--multi-perspective-critique)]

- [25/07] **One Token to Fool LLM-as-a-Judge**  
[[Paper](http://arxiv.org/pdf/2507.08794v1)] [[Code/Page](https://huggingface.co/sarosavo/Master-RM)] [[TLDR/Notes](#one-token-to-fool-llm-as-a-judge)]

- [25/07] **Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions**  
[[Paper](http://arxiv.org/pdf/2507.08068v1)] [[Code/Page]()] [[TLDR/Notes](#quantile-reward-policy-optimization--alignment-with-pointwise-regression-and-exact-partition-functions)]

- [25/07] **Why is Your Language Model a Poor Implicit Reward Model?**  
[[Paper](http://arxiv.org/pdf/2507.07981v1)] [[Code/Page]()] [[TLDR/Notes](#why-is-your-language-model-a-poor-implicit-reward-model-)]

- [25/07] **Bradley-Terry and Multi-Objective Reward Modeling Are Complementary**  
[[Paper](http://arxiv.org/pdf/2507.07375v1)] [[Code/Page]()] [[TLDR/Notes](#bradley-terry-and-multi-objective-reward-modeling-are-complementary)]

- [25/07] **Perception-Aware Policy Optimization for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2507.06448v2)] [[Code/Page](https://mikewangwzhl.github.io/PAPO.)] [[TLDR/Notes](#perception-aware-policy-optimization-for-multimodal-reasoning)]

- [25/07] **Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling**  
[[Paper](http://arxiv.org/pdf/2507.06419v1)] [[Code/Page]()] [[TLDR/Notes](#reward-models-can-improve-themselves--reward-guided-adversarial-failure-mode-discovery-for-robust-reward-modeling)]

- [25/07] **Prompt-Free Conditional Diffusion for Multi-object Image Augmentation**  
[[Paper](http://arxiv.org/pdf/2507.06146v1)] [[Code/Page](https://github.com/00why00/PFCD}{here}.)] [[TLDR/Notes](#prompt-free-conditional-diffusion-for-multi-object-image-augmentation)]

- [25/07] **Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS**  
[[Paper](http://arxiv.org/pdf/2507.05557v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts)]

- [25/07] **ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization**  
[[Paper](http://arxiv.org/pdf/2507.03069v1)] [[Code/Page]()] [[TLDR/Notes](#arf-rlhf--adaptive-reward-following-for-rlhf-through-emotion-driven-self-supervision-and-trace-biased-dynamic-optimization)]

- [25/07] **Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy**  
[[Paper](http://arxiv.org/pdf/2507.01352v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy)]

- [25/07] **SAFER: Probing Safety in Reward Models with Sparse Autoencoder**  
[[Paper](http://arxiv.org/pdf/2507.00665v1)] [[Code/Page](https://github.com/xzy-101/SAFER-code.)] [[TLDR/Notes](#safer--probing-safety-in-reward-models-with-sparse-autoencoder)]

- [25/06] **Generalist Reward Models: Found Inside Large Language Models**  
[[Paper](http://arxiv.org/pdf/2506.23235v1)] [[Code/Page]()] [[TLDR/Notes](#generalist-reward-models--found-inside-large-language-models)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **Listener-Rewarded Thinking in VLMs for Image Preferences**  
[[Paper](http://arxiv.org/pdf/2506.22832v2)] [[Code/Page](https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.)] [[TLDR/Notes](#listener-rewarded-thinking-in-vlms-for-image-preferences)]

- [25/06] **Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents**  
[[Paper](http://arxiv.org/pdf/2506.21252v1)] [[Code/Page]()] [[TLDR/Notes](#agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents)]

- [25/06] **Off-Policy Evaluation and Learning for the Future under Non-Stationarity**  
[[Paper](http://arxiv.org/pdf/2506.20417v1)] [[Code/Page]()] [[TLDR/Notes](#off-policy-evaluation-and-learning-for-the-future-under-non-stationarity)]

- [25/06] **Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations**  
[[Paper](http://arxiv.org/pdf/2506.20294v1)] [[Code/Page]()] [[TLDR/Notes](#ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.18841v1)] [[Code/Page](https://huggingface.co/THU-KEG/LongWriter-Zero-32B)] [[TLDR/Notes](#longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning)]

- [25/06] **RDPO: Real Data Preference Optimization for Physics Consistency Video Generation**  
[[Paper](http://arxiv.org/pdf/2506.18655v1)] [[Code/Page](https://wwenxu.github.io/RDPO/)] [[TLDR/Notes](#rdpo--real-data-preference-optimization-for-physics-consistency-video-generation)]

- [25/07] **Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization**  
[[Paper](http://arxiv.org/pdf/2507.01050v2)] [[Code/Page](https://github.com/allacnobug/Detoxification-of-Text.)] [[TLDR/Notes](#text-detoxification--data-efficiency--semantic-preservation-and-model-generalization)]

- [25/06] **Shrinking the Generation-Verification Gap with Weak Verifiers**  
[[Paper](http://arxiv.org/pdf/2506.18203v1)] [[Code/Page]()] [[TLDR/Notes](#shrinking-the-generation-verification-gap-with-weak-verifiers)]

- [25/06] **ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2506.16712v1)] [[Code/Page]()] [[TLDR/Notes](#reasongrm--enhancing-generative-reward-models-through-large-reasoning-models)]

- [25/06] **Robust Reward Modeling via Causal Rubrics**  
[[Paper](http://arxiv.org/pdf/2506.16507v1)] [[Code/Page]()] [[TLDR/Notes](#robust-reward-modeling-via-causal-rubrics)]

- [25/06] **Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples**  
[[Paper](http://arxiv.org/pdf/2506.16502v1)] [[Code/Page]()] [[TLDR/Notes](#relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples)]

- [25/06] **GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks**  
[[Paper](http://arxiv.org/pdf/2506.16114v1)] [[Code/Page]()] [[TLDR/Notes](#gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks)]

- [25/06] **GRAM: A Generative Foundation Reward Model for Reward Generalization**  
[[Paper](http://arxiv.org/pdf/2506.14175v2)] [[Code/Page]()] [[TLDR/Notes](#gram--a-generative-foundation-reward-model-for-reward-generalization)]

- [25/06] **VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training**  
[[Paper](http://arxiv.org/pdf/2506.13888v1)] [[Code/Page]()] [[TLDR/Notes](#vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **Personalized LLM Decoding via Contrasting Personal Preference**  
[[Paper](http://arxiv.org/pdf/2506.12109v1)] [[Code/Page]()] [[TLDR/Notes](#personalized-llm-decoding-via-contrasting-personal-preference)]

- [25/06] **Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11474v1)] [[Code/Page](https://med-prm.github.io/)] [[TLDR/Notes](#med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards)]

- [25/06] **Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards**  
[[Paper](http://arxiv.org/pdf/2506.11425v2)] [[Code/Page]()] [[TLDR/Notes](#agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards)]

- [25/06] **ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems**  
[[Paper](http://arxiv.org/pdf/2506.10955v1)] [[Code/Page]()] [[TLDR/Notes](#reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems)]

- [25/06] **Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.21560v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning)]

- [25/06] **DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision**  
[[Paper](http://arxiv.org/pdf/2506.09814v1)] [[Code/Page]()] [[TLDR/Notes](#dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO**  
[[Paper](http://arxiv.org/pdf/2506.08965v1)] [[Code/Page]()] [[TLDR/Notes](#gfriend--generative-few-shot-reward-inference-through-efficient-dpo)]

- [25/06] **Saffron-1: Safety Inference Scaling**  
[[Paper](http://arxiv.org/pdf/2506.06444v2)] [[Code/Page](https://github.com/q-rz/saffron)] [[TLDR/Notes](#saffron-1--safety-inference-scaling)]

- [25/06] **Preference Learning for AI Alignment: a Causal Perspective**  
[[Paper](http://arxiv.org/pdf/2506.05967v1)] [[Code/Page]()] [[TLDR/Notes](#preference-learning-for-ai-alignment--a-causal-perspective)]

- [25/06] **Customizing Speech Recognition Model with Large Language Model Feedback**  
[[Paper](http://arxiv.org/pdf/2506.11091v1)] [[Code/Page]()] [[TLDR/Notes](#customizing-speech-recognition-model-with-large-language-model-feedback)]

- [25/06] **Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models**  
[[Paper](http://arxiv.org/pdf/2506.05339v2)] [[Code/Page]()] [[TLDR/Notes](#flattery--fluff--and-fog--diagnosing-and-mitigating-idiosyncratic-biases-in-preference-models)]

- [25/06] **A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation via Learning to Search**  
[[Paper](http://arxiv.org/pdf/2506.05294v1)] [[Code/Page](https://github.com/arnavkj1995/SAILOR)] [[TLDR/Notes](#a-smooth-sea-never-made-a-skilled-$\texttt{sailor}$--robust-imitation-via-learning-to-search)]

- [25/06] **RewardAnything: Generalizable Principle-Following Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.03637v2)] [[Code/Page]()] [[TLDR/Notes](#rewardanything--generalizable-principle-following-reward-models)]

- [25/06] **DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models**  
[[Paper](http://arxiv.org/pdf/2506.03517v1)] [[Code/Page]()] [[TLDR/Notes](#densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models)]

- [25/06] **AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation**  
[[Paper](http://arxiv.org/pdf/2506.03122v1)] [[Code/Page]()] [[TLDR/Notes](#autocircuit-rl--reinforcement-learning-driven-llm-for-automated-circuit-topology-generation)]

- [25/06] **BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF**  
[[Paper](http://arxiv.org/pdf/2506.03234v1)] [[Code/Page]()] [[TLDR/Notes](#badreward--clean-label-poisoning-of-reward-models-in-text-to-image-rlhf)]

- [25/06] **Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences**  
[[Paper](http://arxiv.org/pdf/2506.02698v2)] [[Code/Page](https://jaydenlyh.github.io/SmPO-project-page/.)] [[TLDR/Notes](#smoothed-preference-optimization-via-renoise-inversion-for-aligning-diffusion-models-with-varied-human-preferences)]



# TLDR/Notes
## inverse-reinforcement-learning-meets-large-language-model-post-training--basics--advances--and-opportunities
### Abstract
In the era of Large Language Models (LLMs), alignment has emerged as a
fundamental yet challenging problem in the pursuit of more reliable,
controllable, and capable machine intelligence. The recent success of reasoning
models and conversational AI systems has underscored the critical role of
reinforcement learning (RL) in enhancing these systems, driving increased
research interest at the intersection of RL and LLM alignment. This paper
provides a comprehensive review of recent advances in LLM alignment through the
lens of inverse reinforcement learning (IRL), emphasizing the distinctions
between RL techniques employed in LLM alignment and those in conventional RL
tasks. In particular, we highlight the necessity of constructing neural reward
models from human data and discuss the formal and practical implications of
this paradigm shift. We begin by introducing fundamental concepts in RL to
provide a foundation for readers unfamiliar with the field. We then examine
recent advances in this research agenda, discussing key challenges and
opportunities in conducting IRL for LLM alignment. Beyond methodological
considerations, we explore practical aspects, including datasets, benchmarks,
evaluation metrics, infrastructure, and computationally efficient training and
inference techniques. Finally, we draw insights from the literature on
sparse-reward RL to identify open questions and potential research directions.
By synthesizing findings from diverse studies, we aim to provide a structured
and critical overview of the field, highlight unresolved challenges, and
outline promising future directions for improving LLM alignment through RL and
IRL techniques.
### 🌟 论文解读 | 大语言模型对齐中的逆强化学习：基础、进展与机遇

### 📌 背景痛点/本文动机
在大语言模型（LLM）时代，对齐（alignment）已成为追求更可靠、可控和有能力的机器智能过程中一个基础且具挑战性的问题。一方面，大规模数据驱动模型（如LLM）在多领域取得成功，但LLM存在无法自主持续改进等不足；另一方面，强化学习（RL）在众多领域展现超人类性能却面临系统透明性等挑战。鉴于RL和LLM在各自领域的成功，将二者结合极具前景。同时，当前在将RL扩展到更广泛LLM任务和应用中存在缺乏奖励信号、计算成本高、RL算法需适配LLM对齐任务特性等关键挑战，因此本文从逆强化学习（IRL）视角全面综述LLM对齐的最新进展，试图弥合IRL与LLM对齐间的差距以助力未来研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：以逆强化学习视角综述LLM对齐进展  
本文聚焦于从逆强化学习（IRL）的角度来审视大语言模型对齐的最新研究成果，突出LLM对齐中所用RL技术与传统RL任务中技术的区别，强调从人类数据构建神经奖励模型的必要性，并探讨这种范式转变在形式和实践上的影响，为该领域提供了全新的审视维度。  
💡 创新点2：全面覆盖多方面内容构建知识体系  
先为不熟悉RL领域的读者介绍RL基本概念（如马尔可夫决策过程等）奠定基础；接着审视该研究议程的最新进展，讨论为LLM对齐进行IRL的关键挑战和机遇；除方法考量外，还探索数据集、基准、评估指标、基础设施以及计算高效的训练和推理技术等实践方面；最后从稀疏奖励RL文献中汲取见解以确定开放性问题和潜在研究方向，全方位构建起LLM对齐与IRL结合领域的知识体系。  

### 📈 实验结果
文中未明确提及传统意义上的实验结果呈现（如对比实验数据等），主要是从综述角度梳理领域内相关工作、挑战与机遇等内容，通过对RL在对话AI（如RLHF提升LLM能力）、数学推理（如AlphaProof等在数学竞赛表现）等场景应用的分析，展现RL与LLM结合的潜力与现状，为后续研究提供参考依据。  

### 💬 可借鉴之处
1. 领域交叉视角：将逆强化学习与大语言模型对齐结合进行综述，为研究者提供了跨领域融合的思考角度，启发在AI不同子领域间寻找关联与创新点。  
2. 知识体系构建：从基础概念到前沿进展，再到实践层面（数据集、基础设施等）和未来方向，完整的知识脉络梳理有助于新手快速入门该领域，也让资深研究者全面把握领域现状与趋势。  
3. 挑战与机遇分析：对LLM对齐中RL应用面临的缺乏奖励信号、计算成本、算法适配等挑战的剖析，以及对潜在解决方向的探讨，为后续研究选题和技术突破提供了清晰的问题导向。

## bridging-the-gap-in-vision-language-models-in-identifying-unsafe-concepts-across-modalities
### Abstract
Vision-language models (VLMs) are increasingly applied to identify unsafe or
inappropriate images due to their internal ethical standards and powerful
reasoning abilities. However, it is still unclear whether they can recognize
various unsafe concepts when presented in different modalities, such as text
and images. To address this, we first compile the UnsafeConcepts dataset,
featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and
``Assaults,'' along with associated 1.5K images. We then conduct a systematic
evaluation of VLMs' perception (concept recognition) and alignment (ethical
reasoning) capabilities. We assess eight popular VLMs and find that, although
most VLMs accurately perceive unsafe concepts, they sometimes mistakenly
classify these concepts as safe. We also identify a consistent modality gap
among open-source VLMs in distinguishing between visual and textual unsafe
concepts. To bridge this gap, we introduce a simplified reinforcement learning
(RL)-based approach using proximal policy optimization (PPO) to strengthen the
ability to identify unsafe concepts from images. Our approach uses reward
scores based directly on VLM responses, bypassing the need for collecting
human-annotated preference data to train a new reward model. Experimental
results show that our approach effectively enhances VLM alignment on images
while preserving general capabilities. It outperforms baselines such as
supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope
our dataset, evaluation findings, and proposed alignment solution contribute to
the community's efforts in advancing safe VLMs.
### 🌟 论文解读 | 跨模态识别不安全概念：弥合视觉语言模型的能力鸿沟

### 📌 背景痛点/本文动机
随着视觉语言模型（VLMs）在内容审核等现实场景的广泛应用，识别不安全概念（如仇恨符号、暴力图像、露骨内容等）成为构建负责任AI的关键。然而，VLMs在文本和图像等不同模态下识别不安全概念时，是否存在**模态鸿沟**（即对相同有害内容，文本输入和图像输入下响应不一致）尚不明确；且若存在该鸿沟，如何在不影响模型通用能力的前提下弥合它，也是亟待解决的问题。此外，现有研究缺乏对VLMs识别各类不安全概念能力的系统评估。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建UnsafeConcepts数据集  
首次整合涵盖9大类别、75个不安全概念（如“纳粹万字符”“性骚扰”“袭击”）及1.5K关联图像的数据集，为评估VLMs识别不安全概念能力提供了细粒度标注的基准资源。区分“视觉不安全概念”（呈现概念的图像）与“文本不安全概念”（描述概念的文本），支撑跨模态对比分析。

💡 创新点2：系统评估VLMs的双核心能力  
从**感知（Perception）**和**对齐（Alignment）**两个维度评估8款主流VLMs：  
- 感知能力：测试模型检测图像中不安全概念的准确性（为每个图像设计单选择题，含1个正确选项和3个干扰项）；  
- 对齐能力：评估模型判断是否符合人类伦理标准（设计提示词，询问概念在“社交媒体展示”等安全场景下是否安全，并融入概念特定上下文线索分析 nuanced contexts 的影响）。  

💡 创新点3：简化RLHF方法弥合模态鸿沟  
提出基于近端策略优化（PPO）的简化强化学习方法，无需收集人工标注偏好数据训练新奖励模型，直接用响应分类器评估VLM回答的正确性并分配奖励分数，通过“生成（roll - out）- 评估 - 优化”三阶段迭代更新模型：  
- Roll - out：针对代表伦理标准的安全/不安全概念，采样VLM响应；  
- 评估：用分类器判断响应正确性并打赏；  
- 优化：结合奖励分数、熵奖励（鼓励探索）和KL散度（防止偏离原行为），通过PPO优化VLM。  


### 📈 实验结果
1. 感知与对齐能力存在落差：多数VLMs能较准确感知图像中不安全概念（如LLaVA - 7B感知准确率达0.93），但在安全场景下判断是否“安全/合适”时，对齐得分显著降低（LLaVA - 7B仅0.37），即常忽视图像的不安全属性；  
2. 模态鸿沟普遍存在：8款VLMs在区分视觉与文本不安全概念时，一致表现出跨模态响应不一致的问题；  
3. 简化RLHF方法更优：对比有监督微调（SFT）、直接偏好优化（DPO）等基线，该方法在增强图像上不安全概念对齐能力的同时，保留了模型通用能力，且在外部数据集上泛化性更强。  


### 💬 可借鉴之处
1. 数据集建设：UnsafeConcepts为安全AI领域提供了首个细粒度、多模态的不安全概念基准集，启发后续研究者构建更丰富的安全评估资源；  
2. 评估维度：从“感知（能不能识别）”到“对齐（是否符合伦理）”的双维度评估框架，为分析AI模型安全能力提供了更全面的视角；  
3. 轻量化强化学习：无需人工偏好数据的简化RLHF思路，为资源受限场景下优化模型安全对齐提供了高效方案，可推广到其他需伦理对齐但标注成本高的任务。

## eduflow--advancing-mllms--problem-solving-proficiency-through-multi-stage--multi-perspective-critique
### Abstract
Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.
### 🌟 论文解读 | EduFlow：多阶段多视角批判助力MLLMs攻克科学推理难题

### 📌 背景痛点/本文动机
在人工智能领域，多模态大语言模型（MLLMs）在语言类学科表现尚可，但在科学类任务（尤其是多步骤、可解释性推理任务）中表现糟糕。其存在三大核心问题：一是科学推理模式储备不足，预训练阶段STEM（科学、技术、工程、数学）类特定语料（如公式、符号逻辑、结构化解题步骤等）远少于开放域文本；二是多步骤推理时缺乏全局连贯性，推理错误不断累积；三是没有有效的自我反思和纠错机制，难以跳出记忆知识、整合新见解。2024年中国高考中LLMs在数学科目全军覆没等案例，也凸显了“学科失衡”问题在现代基准测试中普遍存在，这些都促使研究者思考如何提升MLLMs在科学推理任务中的能力，EduFlow框架应运而生。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出端到端框架EduFlow  
EduFlow是首个覆盖教育科学推理全流程的框架，包含数据选择、基于MCTS（蒙特卡洛树搜索）的推理轨迹构建、模型训练、输出优化等环节。它以EduPRM为核心，串联起“PRM引导的数据过滤（筛选需重构的低质量推理轨迹）、PRM引导的EduMCTS数据构建（借助EduPRM监督搜索生成优质轨迹）、基于PRM的Best - of - N选择（从候选输出选最优答案）”等关键阶段，为教育领域结构化推理监督提供了通用且可扩展的蓝图。

💡 创新点2：设计过程感知奖励模型EduPRM  
EduPRM突破传统PRM局限，能以分数、标签、理由对推理步骤进行批判。它通过课程学习，利用三种互补监督源训练：MCTS引导的推理轨迹（自动探索多样解题路径）、注入错误的批判（模拟学生常见误解）、师生对话（完善模糊推理步骤），以此捕捉多粒度教育推理模式，还能动态适配多阶段解题并在推理中迭代优化。

💡 创新点3：提出领域适配搜索框架EduMCTS  
针对MCTS应用于MLLMs时的低效搜索（无步骤指导易探索同质化低质路径）和高成本问题，EduMCTS结合actor模型生成与EduPRM引导的批判，迭代构建高质量推理路径。它融入逐步引导动作、步骤级奖励反馈、自我反思机制（促进反思性纠错），产出教学合理性强的推理轨迹。基于此构建的EduMCTS - 160K数据集，在科学领域比LLaVA - CoT成功率高18%。

### 📈 实验结果
大量实验验证了EduFlow的有效性：在不同规模Qwen模型家族上，对比基线、Edu - 1000K、CoT - 1000K等训练和推理策略，EduFlow相关方法（如EduMCTS - 160K结合+BoN(N = 8)）在逐步推理得分上有显著提升；在过程导向和结果导向指标上均实现+8.2%的一致改进，大幅缩小了LLMs与人类水平在复杂教育任务推理上的差距，同时保持计算效率。

### 💬 可借鉴之处
1. 全流程框架思路：EduFlow整合数据到输出全环节的做法，为特定领域（如教育、STEM）模型能力提升提供了“全栈式”优化参考，可启发研究者针对其他专业领域任务设计端到端方案。  
2. 奖励模型创新：EduPRM结合多监督源、课程学习以及丰富反馈形式（分数、标签、理由）的设计，为打造更智能的“过程评估”模型提供了范式，可用于需步骤级反馈的推理、决策类任务。  
3. 搜索框架适配：EduMCTS针对领域特性改造MCTS，融入引导与反思机制，证明了经典搜索算法在大模型时代结合领域知识和奖励模型能焕发新活力，为复杂任务推理路径探索提供了借鉴方向。  
4. 数据集构建：通过自一致性和拒绝采样构建EduMCTS - 160K高质量教育推理轨迹数据集，为领域内模型训练、评估提供优质数据资源的思路值得学习，尤其是在优质标注数据稀缺的领域。

## one-token-to-fool-llm-as-a-judge
### Abstract
Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.
### 🌟 论文解读 | 一个Token就能骗过LLM裁判？揭秘生成式奖励模型的脆弱性与应对

### 📌 背景痛点/本文动机
在强化学习结合可验证奖励（RLVR）的范式中，生成式奖励模型（也被称为“LLM作为裁判”）凭借大语言模型（LLM）强大的生成与泛化能力，逐渐取代僵化的规则式指标，用于评估答案质量。但研究发现，这类生成式奖励模型存在惊人的脆弱性：仅需一些表面操作（比如非文字符号“:”“.”，或者像“Thought process:”“Let’s solve this problem step by step.”这类推理开头），就能让模型给出错误的正向奖励。这种弱点在不同LLM、数据集和提示格式中广泛存在，对依赖生成式奖励模型的核心算法（如拒绝采样、偏好优化、RLVR等）构成严重威胁。因此，论文旨在揭示该问题并提出改进方法，推动更可靠的基于LLM的评估方式发展。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：发现“万能钥匙”漏洞  
论文首次明确指出，生成式奖励模型在RLVR场景下存在系统性脆弱性——仅含非文字符号或推理开头的“候选答案”，常能骗取正向奖励。这类能触发错误奖励的对抗式响应被称为“master keys（万能钥匙）”，且该问题在多模型、多数据集上普遍存在。  

💡 创新点2：系统性评估漏洞的普遍性  
团队用十种“万能钥匙”响应，在多模型（通用模型如Qwen2.5 - 72B、GPT - 4o，专用验证器如Omni - Judge）和多数据集（数学推理、通用领域等）上展开测试，验证了漏洞的广泛性；还分析了该现象的规模效应与生成新“万能钥匙”的技巧，同时证明推理时策略难以可靠防御这类攻击。  

💡 创新点3：数据增强+鲁棒奖励模型  
为缓解漏洞，提出简单有效的数据增强策略：构造“类对抗”响应（截断模型输出，保留仅做泛化铺垫、无实际解题的开头片段作为负样本）来扩充训练数据。基于此训练出通用领域的Master - RM奖励模型，大幅提升了对“万能钥匙”的鲁棒性。  


### 📈 实验结果
在五大推理基准测试（含GSM8K、MATH等数学推理数据集，及Multi - subject RLVR等通用领域数据集）中，传统LLM裁判面对“万能钥匙”攻击时，错误正例率（FPR）最高达80%；而新训练的Master - RM在所有场景下错误正例率接近0，鲁棒性显著超越同类模型。  


### 💬 可借鉴之处
1. 漏洞发现视角：提醒研究者与开发者重新审视“LLM作为裁判”的鲁棒性假设，在依赖生成式奖励模型的系统设计中，需优先考虑对抗式测试与鲁棒性验证；  
2. 数据增强思路：论文用“构造类对抗负样本”来增强模型鲁棒性的方法，为训练更可靠的奖励模型提供了轻量却有效的范式，可迁移到其他需抵御表面干扰的评估类模型训练中；  
3. 开源资源价值：发布的Master - RM模型与合成训练数据，为后续研究提供了直接可用的鲁棒奖励模型基线，降低了领域内重复造轮子的成本，推动相关方向快速迭代。

## quantile-reward-policy-optimization--alignment-with-pointwise-regression-and-exact-partition-functions
### Abstract
Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.
### 🌟 论文解读 | 弥合绝对奖励与离线策略鸿沟：QRPO开启大模型对齐新范式

### 📌 背景痛点/本文动机
大语言模型（LLM）对齐领域中，现有方法存在明显“割裂”：一方面，像PPO、GRPO这类**在线策略（on - policy）**算法虽能利用“逐点绝对奖励”（pointwise absolute rewards）优化模型，但需要在线采样，计算成本高且复杂；另一方面，DPO、REBEL等**策略拟合（policy fitting）**方法虽简单且支持离线数据，却只能依赖“相对奖励信号”（如偏好对），无法充分利用更易收集（如评分量表形式）、表达能力更强的绝对奖励（如强大的奖励模型输出、可验证奖励）。此外，相对奖励还可能因候选样本“双优”等情况提供次优信号，限制了方法的适用性。为弥合这一技术鸿沟，论文提出**分位数奖励策略优化（Quantile Reward Policy Optimization，QRPO）** 。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：攻克策略拟合中配分函数估计难题，实现绝对奖励下的闭式解拟合  
策略拟合方法此前因绝对奖励下配分函数（partition function）难以估计而受限。QRPO的关键洞见是**采用分位数奖励（quantile rewards）** ，让配分函数有了解析可解的表达式（推导得配分函数为β(exp(1/β)−1)）。这使得KL正则化的强化学习目标能通过简单的监督式回归，在单个样本的逐点绝对奖励信号下，拟合闭式最优策略，无需相对信号来抵消配分函数项，从理论层面突破了传统策略拟合只能依赖相对奖励的限制。同时，QRPO还可拓展为一个框架，在分位数奖励基础上施加额外变换后，配分函数仍保持可 tractable（易处理），灵活性强。  

💡 创新点2：计算资源可扩展的预计算范式，提升分位数奖励估计效能  
QRPO支持通过**增加预计算预算**来生成更多参考奖励，以此优化分位数奖励的估计质量，进而提升模型性能。这种“预计算 - 训练”的分离模式，为计算资源的规模化利用开辟了新维度——更多的预计算可转化为更精准的奖励估计，让方法在不同算力规模下都有性能提升空间。  

### 📈 实验结果
1. 对话与编码任务全面领先：在聊天（如基于 reward model 评分、AlpacaEval 2 基准）和编码（如 LeetCode 测试用例通过率）等任务中，QRPO 对比 DPO、REBEL、SimPO 等方法，在多数据集（如 Magpie - Air、UltraFeedback）和 8B 规模模型（如 Llama 8B）的超 200 组设置下，持续斩获最优表现。以 LeetCode 任务为例，Llama 8B 基础上，QRPO 能将测试用例平均通过率推至 32.7% ± 1.0，显著高于 DPO（30.2% ± 1.4）、REBEL（26.1% ± 1.8）等方法。  
2. 长度偏差缓解优势：相较于需将奖励转偏好的方法（如 DPO、SimPO），QRPO 直接使用鲁棒奖励训练，展现出更弱的长度偏差。SimPO 虽尝试用长度归一化缓解，但政策仍存在强长度偏向；而 QRPO 与 REBEL 则无此问题，验证了绝对奖励直接利用在偏差控制上的价值。  

### 💬 可借鉴之处
1. 技术范式创新：QRPO 为“离线策略 + 逐点绝对奖励”的大模型对齐提供了全新可行路径，打破了在线策略算法对绝对奖励利用的垄断，让简单高效的策略拟合方法能拥抱更丰富的奖励信号源，启发后续在奖励建模与策略优化结合上的探索。  
2. 预计算规模化思路：其“预计算参考奖励 - 训练时用分位数奖励”的模式，为大模型训练中计算资源的分层分配、预计算流水线构建提供了实践参考，助力行业在算力利用效率上的优化。  
3. 偏差与性能平衡：揭示了直接利用鲁棒绝对奖励在缓解长度偏差上的优势，为追求模型输出质量（如编码任务正确性）与公平性（无偏向性）的场景，提供了方法论层面的借鉴方向。

## why-is-your-language-model-a-poor-implicit-reward-model-
### Abstract
Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.
### 🌟 论文解读 | 语言模型为何成了“差劲”的隐式奖励模型？

### 📌 背景痛点/本文动机
奖励模型是语言模型后训练与推理流程的关键组件。现有研究表明，每个语言模型都能定义隐式奖励模型（IM - RM），且无需架构改动；同时还有显式奖励模型（EX - RM），它在语言模型隐藏表示上附加专用线性头。尽管二者训练数据、损失函数和基础语言模型几乎相同，仅奖励计算方式有别，但IM - RM泛化性（尤其分布外场景）远差于EX - RM，这种泛化差距成因不明。本文旨在探究不同奖励模型隐式偏差，揭示该差距根源。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：探究泛化差距替代假设并证伪
针对“IM - RM因兼具验证与生成角色，在生成比验证难的任务中泛化差”这一假设，通过理论证明学习IM - RM的验证能力无需学习生成能力，并在哈密顿回路验证任务实验中佐证该理论，挑战了这一直观认知。
💡 创新点2：分析学习动态揭示偏差本质
从理论上刻画EX - RM与IM - RM的学习动态（梯度训练中奖励的演变），发现EX - RM学习动态主要通过响应的隐藏表示依赖响应；而IM - RM对响应中特定 tokens 更敏感，提升某响应奖励可能不影响甚至降低语义相似但 tokens 不同响应的奖励，揭示出IM - RM更依赖表层 token 级线索这一关键差异。
💡 创新点3：理论与实验结合验证结论
理论层面，构造场景证明当隐藏表示结构良好时，IM - RM可能无法泛化到未见 tokens，而EX - RM可成功泛化；实验层面，在受控与真实场景下开展实验，如基于UltraFeedback训练并在分布内、token级分布偏移、领域偏移场景评估，进一步证实IM - RM因依赖token级线索泛化性差等结论。

### 📈 实验结果
在UltraFeedback数据集训练EX - RM与IM - RM，使用多种语言模型（如Gemma - 2 - 2B - IT、Qwen系列、Llama系列等），评估分布内（UltraFeedback测试集）、token级分布偏移（响应改写或翻译等变体）、领域偏移（数学与代码数据集）场景。结果显示：IM - RM在token级分布偏移下鲁棒性弱于EX - RM，但领域偏移下表现相当或更优；且IM - RM在分布内泛化也常差于EX - RM ，实验结果支撑了IM - RM更依赖token级线索导致泛化差异的结论。

### 💬 可借鉴之处
本文聚焦奖励模型设计中“奖励计算方式”这一看似微小的选择，却发现其对泛化行为影响重大。这启示研究者关注模型设计细节中隐含的偏差，后续可从理解不同奖励模型隐式偏差入手，针对性增强奖励模型鲁棒性；同时本文理论分析与实验结合的研究思路，也为探究模型内在机制类问题提供了范例，有助于推动奖励模型乃至更广泛的语言模型相关研究朝着更可解释、更鲁棒方向发展。

## bradley-terry-and-multi-objective-reward-modeling-are-complementary
### Abstract
Reward models trained on human preference data have demonstrated strong
effectiveness in aligning Large Language Models (LLMs) with human intent under
the framework of Reinforcement Learning from Human Feedback (RLHF). However,
RLHF remains vulnerable to reward hacking, where the policy exploits
imperfections in the reward function rather than genuinely learning the
intended behavior. Although significant efforts have been made to mitigate
reward hacking, they predominantly focus on and evaluate in-distribution
scenarios, where the training and testing data for the reward model share the
same distribution. In this paper, we empirically show that state-of-the-art
methods struggle in more challenging out-of-distribution (OOD) settings. We
further demonstrate that incorporating fine-grained multi-attribute scores
helps address this challenge. However, the limited availability of high-quality
data often leads to weak performance of multi-objective reward functions, which
can negatively impact overall performance and become the bottleneck. To address
this issue, we propose a unified reward modeling framework that jointly trains
Bradley--Terry (BT) single-objective and multi-objective regression-based
reward functions using a shared embedding space. We theoretically establish a
connection between the BT loss and the regression objective and highlight their
complementary benefits. Specifically, the regression task enhances the
single-objective reward function's ability to mitigate reward hacking in
challenging OOD settings, while BT-based training improves the scoring
capability of the multi-objective reward function, enabling a 7B model to
outperform a 70B baseline. Extensive experimental results demonstrate that our
framework significantly improves both the robustness and the scoring
performance of reward models.
### 🌟 论文解读 |  Bradley - Terry与多目标奖励建模：优势互补应对RLHF奖励黑客难题

### 📌 背景痛点/本文动机
在基于人类反馈的强化学习（RLHF）框架下，基于人类偏好数据训练的奖励模型能有效让大语言模型（LLMs）与人类意图对齐，但RLHF易受“奖励黑客”问题困扰，即策略利用奖励函数缺陷而非真正学习预期行为。过往缓解该问题的工作多聚焦分布内场景（奖励模型训练与测试数据分布相同），而在更具挑战的分布外（OOD）场景下表现不佳。同时，引入细粒度多属性分数虽有助于应对挑战，但高质量数据有限导致多目标奖励函数性能弱，成为瓶颈。此外，现有研究在OOD场景下奖励模型泛化能力的局限性未得到充分探索，多目标奖励模型（MORMs）因数据限制性能不如单目标奖励模型（SORMs），且两者独立使用存在计算昂贵和性能瓶颈等问题，这些都构成了本文研究的动机。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：揭示SOTA奖励模型在OOD场景的缺陷  
通过实验表明，当前最先进的方法在PPO和Best - of - N采样时，若所用prompt来自与训练数据不同分布的OOD场景，会出现奖励黑客问题，凸显了现有奖励模型在OOD场景下泛化能力的关键局限。

💡 创新点2：提出SMORM统一奖励建模框架  
提出联合单目标和多目标奖励模型（SMORM），该框架使用共享嵌入空间联合训练基于Bradley - Terry（BT）的单目标奖励函数和基于多目标回归的奖励函数。从理论上建立了BT损失和回归目标之间的联系，实现互补优势：多目标头训练优化嵌入空间，使表示能捕捉多属性质量差异，增强单目标头在OOD场景抗奖励黑客的鲁棒性；单目标头训练能修正嵌入空间中响应的定位，让多目标头在数据有限时也能有竞争力表现。且SMORM训练灵活，训练两个头的prompt - response对无需完全相同，只需单次前向传播共享骨干网络，解决了计算昂贵问题。

### 📈 实验结果
大量实验结果表明，该框架显著提升了奖励模型的鲁棒性和评分性能。例如，使用相同多目标数据集时，基于SMORM的7B模型能超越70B的基线模型；同时在应对OOD场景奖励黑客问题上，相比现有方法有明显改进，验证了多目标头对单目标头泛化能力的增强，以及单目标头对多目标头在有限数据下性能的提升等理论分析结论。

### 💬 可借鉴之处
1. 研究视角上，关注到OOD场景下奖励模型的性能问题，填补了过往研究在该场景下的空白，为后续奖励模型在更复杂场景的研究提供了方向参考。
2. 方法创新上，SMORM框架为结合单目标和多目标奖励建模提供了有效范式，其理论层面建立的BT损失与回归目标的联系，为奖励模型的联合训练提供了理论支撑，后续在处理多类型奖励函数结合、提升模型鲁棒性等方面可借鉴该联合训练思路。
3. 数据利用上，在高质量多属性数据有限的情况下，通过共享嵌入空间联合训练提升多目标奖励函数性能的方式，为解决数据瓶颈问题提供了创新思路，可应用于其他因数据限制导致模型性能受限的场景。

## perception-aware-policy-optimization-for-multimodal-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.
### 🌟 论文解读 | 多模态推理的感知感知策略优化：PAPO 如何突破视觉感知瓶颈？

### 📌 背景痛点/本文动机
大语言模型（LLMs）在纯文本领域的推理能力已通过带可验证奖励的强化学习（RLVR）得到有效增强，但现有 RLVR 设计和优化主要针对纯文本领域，在多模态推理任务中表现欠佳。当前多模态推理的主要误差来源是对视觉输入的感知问题：模型虽能完成逻辑或代数推理，但常因无法准确理解视觉输入（如空间关系、标签关联）导致错误。此前针对感知优化的方法要么依赖额外奖励模型或数据处理，要么将感知与推理生硬分离，存在计算开销大或适配性不足等问题。因此，如何设计更适配多模态领域的 RLVR 算法，成为关键研究问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 Perception - Aware Policy Optimization（PAPO）算法  
PAPO 是对 GRPO（Group Relative Policy Optimization）的简洁且高效的扩展，旨在让模型在学习推理的同时学习感知，且完全依赖内部监督信号，无需额外数据整理、外部奖励模型或专有模型。核心是引入**隐式感知损失（Implicit Perception Loss）**，以 KL 散度形式加入 GRPO 目标函数。通过对比模型在原始图像和“损坏图像（如随机遮盖图像块的版本）”下生成输出的概率差异，引导模型更依赖有效视觉信息推理。  

💡 创新点2：解决损失黑客（Loss Hacking）问题  
由于 KL 目标的无界性，若隐式感知损失系数过高，PAPO 可能过度优化 KL 项，导致奖励崩溃。为此，论文深入分析该问题并提出**双熵损失（Double Entropy Loss）**来缓解，保障训练稳定性。  


### 📈 实验结果
- 多模态基准测试：在 8 个多模态推理基准上，PAPO 相比 GRPO 平均提升 4.4%；在视觉依赖度高的任务中，提升幅度接近 8.0%。  
- 感知误差减少：人工分析 200 个错误案例显示，PAPO 使感知相关错误降低 30.5%，证明模型感知能力提升。  
- 收敛速度：PAPO 早期（约 25 步）就展现增益，收敛更快；结合移除参考 KL 等策略，在 3B 规模模型上最高可提升 11.2%。  


### 💬 可借鉴之处
1. 多模态任务优化思路：针对多模态场景特有问题（如视觉感知瓶颈），从算法层面融合感知与推理监督，为多模态强化学习提供新范式。  
2. 轻量高效设计：不依赖额外复杂组件（如外部奖励模型），仅通过修改目标函数引入感知监督，在工程实现上更具可操作性。  
3. 问题发现与解决：通过 error analysis 定位核心痛点（感知误差主导多模态推理错误），再针对性设计损失函数；同时发现并缓解“损失黑客”这类新问题，体现了从问题诊断到方法迭代的完整研究思路，为后续算法优化提供参考范式。

## reward-models-can-improve-themselves--reward-guided-adversarial-failure-mode-discovery-for-robust-reward-modeling
### Abstract
Reward modeling (RM), which captures human preferences to align large
language models (LLMs), is increasingly employed in tasks such as model
finetuning, response filtering, and ranking. However, due to the inherent
complexity of human preferences and the limited coverage of available datasets,
reward models often fail under distributional shifts or adversarial
perturbations. Existing approaches for identifying such failure modes typically
rely on prior knowledge about preference distributions or failure attributes,
limiting their practicality in real-world settings where such information is
unavailable. In this work, we propose a tractable, preference-distribution
agnostic method for discovering reward model failure modes via reward guided
controlled decoding. Building on this, we introduce REFORM, a self-improving
reward modeling framework that enhances robustness by using the reward model
itself to guide the generation of falsely scored responses. These adversarial
examples are then used to augment the training data and patch the reward
model's misaligned behavior. We evaluate REFORM on two widely used preference
datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate
that it significantly improves robustness without sacrificing reward quality.
Notably, REFORM preserves performance both in direct evaluation and in
downstream policy training, and further improves alignment quality by removing
spurious correlations.
### 🌟 论文解读 | 奖励模型也能自我提升？REFORM框架让奖励建模更鲁棒

### 📌 背景痛点/本文动机
奖励建模（RM）在大语言模型（LLMs）对齐人类偏好方面至关重要，广泛用于模型微调、响应过滤和排序等任务。然而，人类偏好的复杂性与数据集覆盖有限，导致奖励模型在分布偏移或对抗扰动下易失效，出现错误打分（如给偏好响应低奖励、给非偏好响应高奖励），引发过度优化或奖励黑客攻击等问题。现有识别失效模式的方法依赖偏好分布或失效属性的先验知识，在无此类信息的真实场景中实用性受限。因此，本文旨在探索**无需偏好分布先验、可处理地识别奖励模型失效模式，并利用这些模式让奖励模型自我提升鲁棒性**的方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无偏好先验的失效模式发现  
提出基于奖励引导的可控解码框架，无需偏好分布先验就能挖掘奖励模型的错误打分示例（即失效模式）。具体来说，失效模式需满足两个条件：一是响应在偏好分布下有明确类别（如明确是偏好或非偏好响应）；二是奖励模型打分与该类别矛盾（如偏好响应得低分、非偏好响应得高分）。为满足条件，先利用与目标偏好分布对齐/未对齐的策略生成反映已知偏好类别的示例，再通过奖励引导的可控解码，生成被奖励模型错误打分的响应，得到“类别一致但奖励不一致”的失效模式示例，且实证表明该方法比无模型先验的反事实基线更高效。  

💡 创新点2：基于失效模式的自我改进框架REFORM  
利用发现的失效模式增强奖励模型鲁棒性。具体是构建增强偏好数据集：筛选原始训练集中5%最具影响力的prompt - 响应对，生成其失效变体（如偏好响应变体得分低于对应非偏好响应、非偏好响应变体得分高于对应偏好响应），用这些针对性修正的失效示例微调奖励模型，无需大规模数据增强或手动重新标注，就能大幅提升鲁棒性。  

### 📈 实验结果
在两个广泛使用的偏好数据集Anthropic Helpful - Harmless（HH）和PKU Beavertails上评估REFORM：  
- 鲁棒性显著提升：能有效应对分布偏移等扰动，且不牺牲奖励质量；  
- 性能保留与提升：在直接评估和下游策略训练（如Best - of - N采样、PPO、DPO等对齐方法）中保留性能，还通过消除虚假相关性提升了对齐质量，在多样性、可读性和用户效用等生成质量维度表现良好。  

### 💬 可借鉴之处
- 方法普适性：提出的失效模式发现方法不依赖偏好分布先验，为不同场景下奖励模型的鲁棒性分析提供了通用思路；  
- 自我改进范式：展示了利用模型自身生成的对抗示例（失效模式）进行数据增强来改进模型的范式，可启发其他需鲁棒性提升的模型训练任务；  
- 下游影响考量：关注奖励模型鲁棒性对下游对齐任务的影响，验证了改进后模型在下游应用中的性能，为实际落地提供参考，避免鲁棒性提升以牺牲下游效果为代价。

## prompt-free-conditional-diffusion-for-multi-object-image-augmentation
### Abstract
Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.
```
### 🌟 论文解读 | 无需Prompt！多目标图像增强的条件扩散新框架

### 📌 背景痛点/本文动机
在计算机视觉任务中，扩散模型推动了数据集增强的发展，但生成多目标真实场景图像时，现有方法存在缺陷：要么完全依赖文本条件，导致生成对象与原始数据偏差；要么过度依赖原始图像，生成图像缺乏多样性，对下游任务帮助有限。同时，现有多目标图像生成方法还存在生成难度大、标签质量差、风格尺寸偏差、信息增量有限等问题。为解决这些问题，本文提出无Prompt的条件扩散框架用于多目标图像增强。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无Prompt的条件扩散框架与局部-全局语义融合策略
提出无Prompt的条件扩散框架，将文本条件改为局部-全局语义融合策略。利用预训练的CLIP模型从条件图像中分离提取整个图像及其局部裁剪内的语义知识，替代文本提取多目标信息并注入扩散模型，减少文本描述带来的类别偏差，实现从条件图像中恰当提取多目标信息用于图像生成。

💡 创新点2：基于奖励模型的计数损失设计
设计基于奖励模型的计数损失辅助模型训练。不同于逐像素约束，该损失显式限制生成图像中每个类别对象的数量，不对空间布局做约束，在弥合生成数据与原始数据数量偏差的同时，提升生成数据的多样性，让模型能生成同类别对象数量相当甚至更多且布局不同的高质量图像。

### 📈 实验结果
实验结果表明，所提方法在多个代表性的前沿基线方法中展现出优越性，在MS - COCO数据集上的多目标图像增强任务中，在下游任务表现和生成质量方面都达到了新的最优性能，同时还展示了强大的下游任务增益和域外泛化能力。

### 💬 可借鉴之处
1. 语义提取与注入方式：局部-全局语义融合策略为从图像中提取语义替代文本条件提供了新思路，在不需要文本描述时，可借鉴这种从图像自身提取多维度语义信息的方式用于生成任务。
2. 损失函数设计：基于奖励模型的计数损失跳出逐像素约束的传统思路，从对象数量维度约束生成，为提升生成多样性和控制生成数据与原始数据数量关系提供了新的损失函数设计方向，在需要控制生成对象数量或提升多样性的生成任务中可参考。
3. 框架创新：无Prompt的条件扩散框架针对多目标图像增强场景，为解决文本条件带来的偏差问题提供了创新架构，在类似的依赖生成模型做数据增强且对生成与原始数据偏差敏感的任务中，这种改变条件输入方式的思路值得借鉴。
```

## enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts
### Abstract
Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.
### 🌟 论文解读 | 分层检索增强MCTS，解锁大模型推理时缩放新姿势

### 📌 背景痛点/本文动机
大语言模型（LLMs）推理能力提升传统上依赖训练时大规模计算，而测试时缩放（Test - Time Scaling，TTS）作为互补范式，通过推理时分配额外计算资源增强推理能力。现有基于搜索的TTS方法（如MCTS结合过程奖励模型PRM）存在局限：依赖预训练信息易陷入局部最优或探索盲区，且仅靠PRM评估步骤难以捕捉全局策略和语义关系，导致奖励信号稀疏或次优，影响复杂推理任务效率与准确性。因此需要更有效通用的推理缩放方法，在无需大量额外训练下增强推理能力并提升鲁棒性与适应性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：双层次检索增强上下文学习（粗粒度层面）
提出深度逻辑检索（Deep Logical Retrieval），从复杂推理问题中提取抽象模板，检索相似问题 - 答案对。这些相似对为模型提供多样示例，助力模型捕捉问题结构的潜在模式与变异性，进而提升上下文学习效果，增强对未见过问题的适应性。

💡 创新点2：分层增强推理MCTS（细粒度层面）
在蒙特卡洛树搜索（MCTS）过程中，R2 - LLMs从外部数学问题数据集动态检索相关中间解决步骤，用相似先验知识丰富推理过程。结合这些检索步骤后，PRM能做出更具信息性和上下文一致性的评估，降低无效探索风险，同时该方法无缝整合上下文级推理增强与步骤级树搜索方法，利用PRM优化候选生成与决策以提升推理准确性。

### 📈 实验结果
在MATH500、GSM8K和OlympiadBench - TO数据集上，使用LLaMA - 3.1 - 8B模型时，与基线相比R2 - LLMs实现了显著相对提升，提升幅度最高达16%；在LLaMA 3.1 - 8B和Qwen 2 - 7B等策略模型上评估，也优于基于上下文学习（ICL）和基于树搜索的基线方法，证明了方法在复杂推理任务中的有效性。

### 💬 可借鉴之处
1. 分层检索增强思路：将检索在推理时的作用分层设计，粗粒度抓问题结构模式、细粒度补中间步骤知识，为多粒度利用外部知识辅助推理提供了参考范式。
2. 检索与MCTS结合：把外部检索引入MCTS过程来辅助PRM评估，为改进基于搜索的TTS方法中奖励信号不足、探索低效等问题提供了创新解法，后续可借鉴这种外部知识赋能搜索过程的思路拓展更多推理场景。
3. 无CoT蒸馏依赖：无需从更先进模型蒸馏获取思维链训练数据，降低了方法应用门槛，在资源有限或难获取高级模型蒸馏数据时，该轻量（相对）增强推理的方式值得参考。

## arf-rlhf--adaptive-reward-following-for-rlhf-through-emotion-driven-self-supervision-and-trace-biased-dynamic-optimization
### Abstract
With the rapid advancement of Reinforcement Learning from Human Feedback
(RLHF) and autoregressive transformers, state-of-the-art models such as
GPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and
personalization. However, most existing RLHF approaches (e.g., PPO, DPO) still
rely on a binary-preference (BT) paradigm, which, while reducing annotation
costs, still requires substantial human effort and captures only group-level
tendencies rather than individual preferences. To overcome these limitations,
we propose Adaptive Reward-Following (ARF), a self-assessment framework that
leverages a high-precision emotion analyzer achieving over 70% accuracy on
GoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback
into continuous preference scores. We further enrich and debias these signals
through lightweight data augmentations, including synonym replacement, random
trace truncation, and score bias annotation algorithm. A Dynamic Adapter
Preference Tracker continuously models evolving user tastes in real time,
enabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly
on these tracked rewards instead of coarse binary labels. Experiments on
Qwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate
that ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,
TB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF
presents a scalable, personalized, and cost-effective approach to RLHF LLMs
through autonomous reward modeling.
### 🌟 论文解读 | ARF-RLHF：用情感驱动自监督与轨迹偏置动态优化革新RLHF

### 📌 背景痛点/本文动机
随着强化学习从人类反馈（RLHF）和自回归Transformer的快速发展，像GPT - 4.0、DeepSeek R1、Llama 3.3等前沿模型愈发重视回答深度与个性化。然而，现有多数RLHF方法（如PPO、DPO）依赖二元偏好（BT）范式，虽降低了标注成本，但仍需大量人力，且仅能捕捉群体层面趋势而非个体偏好，还存在标注偏差、更新滞后等问题。为克服这些局限，论文提出ARF - RLHF框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：自适应奖励跟随（ARF）评分器
基于人类交流中隐含满意度信号这一观察，利用在GoEmotions、Sentiment140、DailyDialog等数据集上准确率超70%的高精度情感分析器，将自由形式的用户反馈转化为连续偏好分数。评分器基于轻量的RoBERTa - mini架构构建，平衡低延迟与强语义理解能力，实现从QA对的动态交互分析中自动进行偏好评分，替代传统BT - RLHF的二元比较分数。

💡 创新点2：增强型数据处理与动态偏好跟踪
通过轻量数据增强（同义词替换、随机轨迹截断、分数偏差标注算法）来丰富和去偏信号；借助带经验回放（ER）机制的动态适配器偏好跟踪器，结合软标签学习周期性更新评分器，实时建模用户变化的偏好，避免过拟合，为后续优化提供更优的奖励信号。

💡 创新点3：Trace Bias（TB）微调算法
无需依赖BT对数据，基于动态评分器反馈、随机路径截断和路径偏差校正提出新颖优化策略，直接在跟踪到的奖励上优化，而非粗糙的二元标签，且在理论上与PPO和DPO目标保持一致，实现稳定且有理论依据的微调。

### 📈 实验结果
在Qwen - 2/2.5、Gemma - 2、Llama - 3.2等模型及四个偏好领域的实验表明，ARF相对PPO提升3.3%，相对DPO提升7.6%，验证了方法的有效性与优越性。

### 💬 可借鉴之处
1. 情感驱动的自监督思路：利用用户交互中隐含的情感、满意度等信号来构建奖励模型，为减少人工标注依赖提供了新方向。
2. 轻量数据增强与动态跟踪结合：通过简单有效的数据增强手段丰富数据并去偏，搭配动态跟踪机制适应用户偏好变化，在数据处理和模型适应性提升上有借鉴价值。
3. 无二元标签依赖的优化算法：Trace Bias算法跳出传统RLHF依赖二元比较的局限，为LLM的RLHF优化开辟了更自主、可扩展的路径，在算法创新层面提供了参考。

## skywork-reward-v2--scaling-preference-data-curation-via-human-ai-synergy
### Abstract
Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.
### 🌟 论文解读 | Skywork-Reward-V2：人机协同解锁偏好数据规模与质量新高度

### 📌 背景痛点/本文动机
奖励模型（RMs）在基于人类反馈的强化学习（RLHF）中至关重要，但当前开源奖励模型在多数评估基准上表现不佳，难以捕捉复杂精妙的人类偏好。究其原因，现有偏好数据集存在范围狭窄、标签合成性强或质量控制不严谨等局限，即便采用先进训练技术也难有实质性提升。因此，提升偏好数据质量以推动开源奖励模型发展成为关键诉求。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建超大规模偏好数据集SynPref - 40M  
打造了包含4000万偏好对的大规模偏好数据集SynPref - 40M，为奖励模型训练提供了丰富的数据基础，这也是目前已知规模最大的精心整理偏好混合数据集。  

💡 创新点2：设计人机协同两阶段数据整理 pipeline  
第一阶段借助严格协议下的人工验证保障数据质量；第二阶段利用人类偏好引导的大语言模型（LLM）作为“裁判”实现规模化整理，同时结合奖励模型的迭代训练，持续纳入人工标签反馈并召回模型表现差的偏好数据以促进学习，最终得到2600万高质量偏好对用于模型训练。  

💡 创新点3：推出Skywork - Reward - V2系列奖励模型  
基于SynPref - 40M筛选出的偏好数据，训练出包含8个从0.6B到8B参数规模的Skywork - Reward - V2系列奖励模型，仅用Bradley - Terry目标函数训练却能在多基准展现卓越性能。  

### 📈 实验结果
在7个主要奖励模型基准测试中，Skywork - Reward - V2系列表现亮眼，8B规模模型在所有7个基准上大幅超越现有开源奖励模型；在人类偏好对齐、客观正确性、安全性、抗风格偏差、best - of - N缩放等多关键维度也展现出优越性能。消融实验表明，SynPref - 40M的成功既源于规模也得益于高质量，同时人机协同 pipeline 中人工标注、人类偏好引导的LLM标注及严谨标注协议都至关重要。  

### 💬 可借鉴之处
数据层面，大规模且高质量的偏好数据集对模型性能提升作用显著，SynPref - 40M的构建思路为数据驱动的模型优化提供范例；方法层面，人机协同的两阶段数据整理 pipeline 有效结合人类标注质量与AI可扩展性优势，为大规模数据高质量整理提供了可参考的流程框架；模型层面，Skywork - Reward - V2系列证明合理利用数据与训练策略，能在开源奖励模型领域实现性能突破，为后续奖励模型研发指明方向，凸显了挖掘现有偏好数据潜力与人机协同在提升数据质量上的价值。

## safer--probing-safety-in-reward-models-with-sparse-autoencoder
### Abstract
Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}
### 🌟 论文解读 | SAFER：用稀疏自编码器透视奖励模型的安全奥秘

### 📌 背景痛点/本文动机
大语言模型（LLMs）的广泛应用凸显了安全与可靠性方面的关键担忧，基于人类反馈的强化学习（RLHF）是让模型与人类价值观对齐的主流方法，而其核心的奖励模型却在很大程度上不透明。一方面，奖励模型仅输出标量分数，掩盖了背后的语义特征，降低了透明度、可靠性与安全性；另一方面，奖励模型对偏好数据的标注十分敏感，微小改动就可能大幅影响性能，但当前检测、解释和修正有噪声或有问题标注的方法还很有限。因此，理解奖励模型内部机制和偏好数据集至关重要，本文正是为解决奖励模型可解释性以及偏好数据对其影响的理解问题而展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：引入稀疏自编码器（SAEs）实现奖励模型的机制性解释  
利用稀疏自编码器这一机制可解释性方法，将奖励模型的激活分解为稀疏且可解释的特征，挖掘出驱动奖励预测的明确语义因素，通过识别与安全相关的特征大幅提升了奖励模型的透明度。SAEs旨在把语言模型激活表示为过完备基向量的稀疏组合，训练时最小化重建损失并对潜在向量施加稀疏约束，采用TopK SAE通过保留top K激活来增强稀疏性控制与特征可解释性。

💡 创新点2：提出SAFER框架实现偏好数据集的特征级分析与增强  
针对安全相关方面（因其实际重要性显著），先在安全导向的偏好数据集（SafeRLHF和WildGuardMix）上训练奖励模型，再在该奖励模型的隐藏状态激活上训练SAE以提取稀疏可解释特征；通过量化选中和拒绝响应间激活差异来确定特征显著性，进而实现对关键安全相关特征的隔离与解释。基于这些特征级信号，设计针对性的数据投毒和去噪策略，回答了偏好数据对奖励模型影响的问题，实现了基于安全相关性的靶向数据操作。

### 📈 实验结果
在数据投毒实验中，反转表现出最大安全相关特征激活差异的子集对，结果显示投毒操作能在极小数据改动下显著降低安全分数，且对模型通用能力（如聊天能力）几乎无影响；在数据去噪实验中，移除特征激活差异最小的对，去噪方法提升了奖励模型在安全评估上的性能。这表明SAFER能精准地降低或增强安全对齐度，同时不牺牲通用聊天性能。

### 💬 可借鉴之处
从方法层面，为奖励模型的解释、审计和优化提供了新途径，借助稀疏自编码器实现机制性解释的思路可迁移到其他模型可解释性任务中；从应用层面，针对偏好数据集的特征级探测策略，为高风险LLM对齐任务中数据的处理（投毒、去噪等）提供了靶向操作的范例，能启发后续在数据层面优化模型对齐与安全的工作；代码开源也为研究者复现和拓展相关研究提供了便利，推动该方向的发展。 

## generalist-reward-models--found-inside-large-language-models
### Abstract
The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.
### 🌟 论文解读 | 大语言模型中藏着通用奖励模型？LLM对齐新范式来了

### 📌 背景痛点/本文动机
大语言模型（LLM）对齐人类价值观（如帮助性、诚实性）是AI发展的核心挑战，主流的基于人类反馈的强化学习（RLHF）严重依赖用昂贵人类偏好数据训练的奖励模型（RM）。构建优质RM需大规模高质量人类偏好数据集，存在慢、贵、难扩展等问题。后续用AI反馈替代人类反馈的方法（如RLAIF、LLM-as-a-judge）又缺乏严谨理论基础，还易继承裁判模型的风格偏差与偏见。那么，高质量奖励信号是否必须外部获取？这成为关键问题，本文正是基于此展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：发现LLM中 latent 存在通用奖励模型  
论文发现，任何经标准下一个token预测训练的LLM中，天然潜伏着强大的通用奖励模型，将其命名为“内生奖励（endogenous reward）”。无需额外训练，就能从基础模型（预训练或有监督微调模型）中直接提取高质量奖励信号。  

💡 创新点2：理论层面建立与离线逆强化学习的联系  
从理论上证明，这种内生奖励等价于通过离线逆强化学习（IRL）学到的奖励函数。具体而言，LLM的logits可直接解释为软Q函数，借助逆软Bellman算子能从中恢复出奖励函数，为提取奖励函数提供了原理性方法，突破了过往启发式做法。  

💡 创新点3：证明基于内生奖励的RL有效性  
证明用该内生奖励进行后续强化学习后，得到的策略相比基础模型有更优的误差界。RL过程能修正标准模仿学习（下一个token预测）的复合误差，把性能差距从与任务时长相关的二次依赖（O(H²)）降到更优的线性依赖（O(H)）。这是首次从理论上证明LLM强化学习的有效性。  


### 📈 实验结果
大量实验验证了理论：提取内生奖励的方法不仅优于现有LLM-as-a-judge方法，还能超越在昂贵人类标注数据上显式训练的奖励模型。  

### 💬 可借鉴之处
论文表明奖励建模阶段可被一种原理性方法替代——提取预训练阶段已捕获的知识。这为LLM对齐以及多模态模型领域，开辟了更高效、强大且可扩展的新范式，后续在模型对齐、奖励函数设计等方向，都可借鉴这种“挖掘模型内在已有能力”的思路，减少对外部昂贵数据与额外训练的依赖。

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### 🌟 论文解读 | 用知识增强树搜索推理助力大模型攻克分子结构解析难题

### 📌 背景痛点/本文动机
分子结构解析是化学实验分析里的关键任务，要从核磁、红外等光谱数据推导分子结构，专业人员都得花10 - 15分钟分析单个分子，所以用大语言模型（LLM）自动化解析很有必要。但LLM在这任务上有挑战：一是对化学分子结构空间覆盖不足，像噻吩这类特殊杂环结构，LLM常因缺乏子结构知识误判；二是没法准确评估和修正推理过程，树搜索推理需要有效评估反馈，可LLM缺领域知识，做不好 reward model 角色。于是论文要解决这两个问题，提升LLM在分子结构解析的能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出K - MSE框架  
构建知识增强的分子结构解析推理框架K - MSE，把蒙特卡洛树搜索（MCTS）作为插件实现测试时的能力扩展，能适配任意LLM。借助MCTS平衡新解探索和已有解利用，还结合Self - Refine让LLM及时优化之前的解。

💡 创新点2：外部分子子结构知识库  
为弥补LLM化学结构空间覆盖不足，构建外部分子子结构知识库。子结构是化学空间基础元素，知识库通过自动化流程整合子结构和结构描述，给LLM补充领域知识，提升特殊结构推理准确性，减少 atypical 案例错误。

💡 创新点3：专属分子 - 光谱评分器  
设计分子 - 光谱评分器当 reward model，解决LLM解评估不准问题。评分器有分子编码器和光谱编码器，评估分子结构和光谱数据匹配度给奖励分。它还作为LLM和知识库间的检索器，用输入光谱查最相关子结构，减少子结构检索误差，增强推理稳定性。

### 📈 实验结果
在MolPuzzle基准测试上，K - MSE方法效果显著，对GPT - 4o - mini和GPT - 4o都带来超20%的性能提升，证明了框架在增强LLM分子结构解析能力上的有效性。

### 💬 可借鉴之处
1. 领域知识增强思路：面对专业领域任务，LLM通用知识不足时，构建领域子结构知识库补充，这种“外部知识 + LLM”模式可复用在其他专业领域（如生物、材料）任务。  
2. 推理过程评估优化：设计领域专属评分器做 reward model，结合树搜索框架优化推理，为需要深度推理、需评估反馈的复杂任务（如数学证明、代码调试）提供了“评分器 + 树搜索”的推理增强范式。  
3. 插件化框架设计：K - MSE作为插件适配任意LLM，这种解耦式设计方便技术落地，不同场景下可快速集成到现有LLM工作流里，降低技术迁移成本。

## listener-rewarded-thinking-in-vlms-for-image-preferences
### Abstract
Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.
### 🌟 论文解读 | 用Listener增强的RL，让视觉语言模型更懂人类图像偏好

### 📌 背景痛点/本文动机
在生成式建模领域，让视觉 - 语言模型（VLMs）精准捕捉人类视觉偏好是关键难题。现有奖励模型存在泛化能力不足问题，监督微调（SFT）易导致模型记忆训练数据，还需要复杂标注流程。强化学习（RL）里的Group Relative Policy Optimization（GRPO）虽能提升泛化性，但研究发现基于RL的偏好推理模型存在“听众分歧（listener disagreement）”问题：当模型推理轨迹和独立冻结的视觉 - 语言模型（“listener”）对同一输出的评估矛盾时，推理准确率大幅下降。所以，如何解决这种推理矛盾、提升模型与人类偏好的对齐度是本文动机所在。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：首次训练思维链风格推理模型预测人类对生成模型输出的视觉偏好
以往工作较少针对生成模型输出的人类视觉偏好来训练这种思维链推理模型，本文填补了这一空白，为视觉偏好预测提供新的模型训练思路。

💡 创新点2：识别并量化“听众分歧”这一RL视觉偏好建模的主要失效模式
通过分析发现，当模型预测和独立“listener”预测差异增大时，VLM准确率持续下降，将这种现象明确为关键问题并量化，为后续解决方法提供基础。

💡 创新点3：设计面向GRPO的listener - shaped软奖励机制
引入冻结的VLM“listener”，让其独立重新处理推理模型（reasoner）的思维链（排除最终答案token），输出对正确选择的校准置信分数，并整合到RL奖励信号中。这样既惩罚无法说服独立模型的解释，又无需额外人工标注就能提供密集、数据高效的监督，让推理模型不仅答案正确，推理过程也能被“listener”认可。

### 📈 实验结果
在ImageReward测试集上，方法达到67.4%的当前最优准确率；在大规模（120万投票）的Rapidata - HSP基准测试中，大幅超越强GRPO和SFT基线；同时减少了推理矛盾情况，且在分布外（OOD）数据集上表现出色，即便用少量偏好数据训练，也能让输出更校准、OOD鲁棒性更强。

### 💬 可借鉴之处
从方法创新角度，利用独立模型构建奖励机制来对齐推理轨迹和最终决策的思路，为解决模型推理一致性问题提供了新范式；从应用角度，证明listener增强的RL是VLMs中偏好对齐的有效实用工具，为下一代文本到图像、文本到视频系统提供了可扩展的偏好对齐解决方案，在工业界大规模生成模型偏好调优场景有借鉴价值；从问题发现角度，对“听众分歧”这种失效模式的识别和量化，让后续研究者能更关注模型推理过程的一致性问题，推动领域发展。

## agent-rewardbench--towards-a-unified-benchmark-for-reward-modeling-across-perception--planning--and-safety-in-real-world-multimodal-agents
### Abstract
As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.
### 🌟 论文解读 | Agent-RewardBench：面向真实世界多模态智能体的奖励建模统一基准

### 📌 背景痛点/本文动机
随着多模态大语言模型（MLLMs）不断发展，多模态智能体在网页导航、具身智能等真实世界任务中展现出潜力。但这些智能体因缺乏外部反馈，在自我修正与泛化能力上存在不足。利用奖励模型作为外部反馈是有前景的方向，然而目前缺乏针对智能体奖励模型选择的明确指导，也缺少专门面向智能体的奖励基准。因此，构建针对智能体的奖励基准迫在眉睫。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多维度与真实场景覆盖  
提出的Agent - RewardBench基准涵盖感知、规划、安全3个评估维度与7个真实世界智能体场景（如移动、网页、自动驾驶、Minecraft等）。在感知维度评估视觉理解与 grounding 的奖励能力；规划维度聚焦奖励模型对序列决策和任务分解的评估能力；安全维度考察在攻击和不安全环境场景下的奖励能力。

💡 创新点2：步骤级奖励评估  
不同于仅关注最终结果的评估，该基准支持在任务的单个步骤层面评估智能体能力，为规划过程中的性能提供更细致的视角，能更详细地反馈模型在每一步的奖励能力表现。

💡 创新点3：难度适配与高质量数据保障  
通过三种策略保障难度与数据质量：从10种不同的多模态模型（涵盖黑盒与白盒模型）采样以保证多样性；用小模型过滤数据来控制任务难度，避免过易或过难；人工验证数据以确保数据完整性与高质量。

### 📈 实验结果
实验表明现有先进多模态模型在该基准上表现有限：黑盒模型gemini - 1.5 - pro准确率仅61.6%，GPT - 4o为61.4%，Claude - 3.5 - Sonnet为57.9%，体现基准挑战性；像GPT - 4o这样较强的模型在安全奖励建模上准确率仅39.2%，说明智能体领域安全奖励建模仍不足；开源模型如Llama - 3.2 - 11B - Vision - Instruct在感知和规划上分别仅得53.5%和50.6%，凸显智能体奖励模型专项训练的必要性。

### 💬 可借鉴之处
1. 填补领域空白：Agent - RewardBench是首个评估多步骤智能体任务中模型奖励建模能力的基准，为从模仿学习向带反馈学习的过渡提供关键评估手段。 
2. 丰富评估维度与场景：涵盖多维度、多真实场景且采用多模型真实样本与人工验证保障数据质量，为后续奖励模型评估提供了全面且高质量的参考范式。 
3. 指导下游任务：展现出与下游任务的强关联性，说明准确的奖励建模对提升实际应用中搜索性能至关重要，为后续优化智能体性能指明方向。

## off-policy-evaluation-and-learning-for-the-future-under-non-stationarity
### Abstract
We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.
### 🌟 论文解读 | 非平稳环境下面向未来的离策略评估与学习

### 📌 背景痛点/本文动机
在推荐系统、精准医疗等决策问题中，离策略评估（OPE）用于利用历史日志数据评估新策略效果，但多数实际场景处于**非平稳环境**（如电商推荐中用户偏好、奖励随时间变化）。现有方法假设环境平稳或依赖严格奖励建模假设，难以准确估计未来策略价值——因为历史数据中没有未来环境的观测，直接套用传统OPE方法会引入显著偏差。例如电商推荐需用上个月旧策略数据估计下个月新策略价值，而用户行为的周度、季节等时间模式在历史与未来存在一致性却未被充分利用，这成为关键痛点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：定义未来离策略评估（F - OPE）问题并提出OPFV estimator  
首次形式化**未来离策略评估（F - OPE）**问题：在非平稳环境下，仅用历史日志数据估计未来任意时间点的策略价值。提出**Off - Policy Estimator for the Future Value (OPFV)**，其核心是利用时间序列中稳定的结构（如季节、周度、节假日效应）——这些效应在历史和未来数据中具有一致性。通过**新型重要性加权**，OPFV能无偏估计时间序列结构刻画的效应（称为时间序列特征），同时用回归模型处理剩余效应以平衡偏差和方差。理论分析证明了OPFV在特定条件下实现低偏差估计。

💡 创新点2：扩展OPFV实现未来离策略学习（F - OPL）  
基于OPFV estimator，进一步提出**新的策略梯度方法**，仅用历史数据主动学习未来的优质策略。该方法扩展OPFV以估计未来策略价值的策略梯度，从而指导策略优化，解决了非平稳环境下“用历史数据学未来策略”的难题。

💡 创新点3：数据驱动的时间序列特征选择  
分析时间序列特征选择对OPFV偏差 - 方差权衡的影响，提出**简单的数据驱动流程**选择时间序列特征，以优化OPFV的估计精度，让方法更具实用性。

### 📈 实验结果
在合成数据与真实推荐系统非平稳数据的实验中，OPFV在**估计未来策略价值**和**优化未来策略**两方面均显著优于现有方法。实验覆盖多种非平稳场景（如分布的平滑/突变变化），验证了OPFV在不同设定下的鲁棒性与有效性。

### 💬 可借鉴之处
1. 问题定义层面：将“时间戳”作为随机变量纳入建模，统一处理平滑与突变的非平稳性，为非平稳环境下的策略评估与学习提供了更普适的框架思路。  
2. 方法设计层面：挖掘时间序列中跨历史 - 未来的稳定结构（如周期效应），并用新型重要性加权和回归结合的方式利用这些结构，为“无未来观测时估计未来价值”提供了可复用的技术范式。  
3. 落地实践层面：数据驱动的特征选择流程与基于OPFV的策略梯度方法，为工业界（如推荐系统、广告投放）在非平稳场景下快速迭代策略提供了落地路径，减少在线实验成本与伦理风险。

## ctrl-z-sampling--diffusion-sampling-with-controlled-random-zigzag-explorations
### Abstract
Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.
### 🌟 论文解读 | 突破局部最优！Ctrl - Z Sampling：可控随机之字形探索的扩散采样

### 📌 背景痛点/本文动机
扩散模型在条件生成任务中表现出色，它通过迭代去噪将高斯噪声逐步转化为目标数据分布。但由于潜在空间复杂和初始化欠佳，扩散模型常收敛到局部最优——生成结果局部视觉连贯，却全局不一致或条件错位。以往方法如强化引导信号、操纵初始噪声分布等，在引导控制或逃逸局部最优的灵活性上存在不足，要么引导效果不佳，要么需大量候选状态评估，实用性受限。因此，需要一种能有效检测并逃逸局部最优的采样策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出可控随机之字形采样（Ctrl - Z Sampling）
该方法将条件扩散生成视为“爬山”过程，基于奖励模型检测潜在局部最优（依据预测分数的停滞情况）。一旦检测到，注入噪声并回退到更早、更具噪声的时间步，以此逃离当前优化平台，鼓励在潜在空间探索更优区域。

💡 创新点2：动态平衡 backward exploration 与 forward refinement
奖励模型评估候选状态，仅接受能带来更优未来预测的状态；若附近无改进，会继续回退到噪声更大的水平以增加逃逸可能性。这种可控随机之字形过程实现了前向优化与后向探索的动态交替，提升生成结果的条件对齐度与视觉质量，且模型无关，适配现有扩散框架。

### 📈 实验结果
在文本到图像基准测试中，Ctrl - Z Sampling 在多个指标上实现生成质量的显著提升，仅需约 7.6 倍函数评估（NFEs）增加。这表明其在推理时能以适度开销大幅改进生成效果，是昂贵测试时缩放方法的实用替代方案，尤其适用于本地推理场景。

### 💬 可借鉴之处
1. 从“爬山”视角分析扩散模型条件生成，为理解模型收敛到局部最优的问题提供新角度，启发后续对生成过程动力学的研究。
2. Ctrl - Z Sampling 的设计思路——通过奖励引导的随机探索、自适应噪声注入实现可控逃逸局部最优，可借鉴到其他需在复杂空间中优化、易陷局部最优的生成或优化任务中，如其他类型生成模型（变分自编码器等）的采样策略改进。
3. 模型无关性使其能无缝融入现有扩散框架，为工程实践中提升扩散模型生成质量提供即插即用的思路，无需重新训练模型，降低应用成本。

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### 🌟 论文解读 | ReasonFlux-PRM：面向大模型长思维链推理的轨迹感知型过程奖励模型

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的复杂推理场景（如数学解题）中，Process Reward Models（PRMs，过程奖励模型）是监督中间推理步骤的有力工具。不过现有PRMs存在明显局限：它们主要基于模型最终输出训练，难以对**轨迹 - 响应（trajectory - response）**这类新兴输出形式的中间推理轨迹进行鲁棒评估。像Deepseek - R1等前沿推理模型会生成“冗长、欠规整的中间思考轨迹 + 简洁最终响应”的轨迹 - 响应对，这类数据常被用于小模型蒸馏，但现有PRMs因与中间轨迹在结构、格式上不匹配，且训练时缺乏带奖励的轨迹 - 响应数据，在监督这类数据时效果不佳甚至会损害下游训练。所以，如何让PRMs既能监督最终响应，又能有效评估中间思考轨迹，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出轨迹感知的PRM——ReasonFlux - PRM  
ReasonFlux - PRM专为评估轨迹 - 响应型推理痕迹设计，融合了**步骤级（step - level）**和**轨迹级（trajectory - level）**监督。它在涵盖数学和科学推理的10k高质量轨迹 - 响应对 curated 数据集上训练，能为思考轨迹内的每个步骤提供细粒度奖励作为监督信号，让模型中间思考轨迹与最终响应更对齐，解决了现有PRMs对中间轨迹监督能力不足的问题。  

💡 创新点2：多场景适配的奖励监督  
ReasonFlux - PRM适配离线和在线多种场景：  
- 离线场景：为轨迹 - 响应对打分，筛选高质量数据，助力小模型下游有监督微调的训练数据精选；  
- 在线场景：融入GRPO等策略优化过程，为强化学习（RL）中的策略优化提供细粒度过程奖励；  
- 测试时缩放（test - time scaling）：通过奖励引导的Best - of - N策略，评估多个生成响应并选最优，提升推理性能。  


### 📈 实验结果
在AIME、MATH500、GPQA - Diamond等挑战性下游基准测试中，ReasonFlux - PRM展现出优异性能：  
- 数据选择方面：ReasonFlux - PRM - 7B比强基线（如Qwen2.5 - Math - PRM - 72B）和人工策划基线选出的数据集质量更高；  
- 性能提升方面：ReasonFlux - PRM - 7B在有监督微调中平均提升12.1%，强化学习中平均提升4.5%，测试时缩放中平均提升6.3%；  
- 资源友好型发布：还发布了ReasonFlux - PRM - 1.5B，适配资源受限场景与边缘部署。  


### 💬 可借鉴之处
1. 问题定义与分析角度：针对新兴的轨迹 - 响应蒸馏数据趋势，深入分析现有PRMs在监督中间轨迹时的问题（结构格式不匹配、训练数据缺失），这种从产业新数据形态反推技术痛点的思路，为后续研究锚定方向提供参考；  
2. 多粒度监督融合：将步骤级和轨迹级监督结合，为处理“长链条、多阶段”的推理类任务提供了细粒度奖励设计的范例，可迁移到代码生成、复杂决策等需分步评估的场景；  
3. 多场景工程落地：从离线数据筛选、在线RL优化到测试时增强，完整覆盖大模型训练 - 推理全流程的奖励监督，展示了技术方案在产业级落地中的多维度价值，为打造端到端的大模型推理增强管线提供了实践模板；  
4. 资源分层发布：同时提供7B和1.5B规模模型，兼顾高性能与资源受限场景，体现了技术普惠性，在实际业务中可根据算力、延迟等需求灵活选择，平衡效果与成本。  

## longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning
### Abstract
Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B
### 🌟 论文解读 | LongWriter-Zero：用强化学习突破超长文本生成难题

### 📌 背景痛点/本文动机
超长文本生成（如万字级报告、叙事创作等）是大语言模型（LLM）在实际场景中至关重要的能力，但现有技术面临两大核心挑战：一是模型生成长度存在上限，二是随着文本长度增加，内容质量（连贯性、一致性、结构合理性等）会显著下降。  

此前主流方案（如LongWriter）依赖**有监督微调（SFT）**，即在人工构造的“指令 - 长文本输出”配对数据上训练模型。但这种方式存在明显缺陷：  
- 构造高质量的合成SFT数据成本极高、难度大；  
- 合成数据往往缺乏连贯性与一致性，且风格单一、过度“人工化”；  
- SFT的最大似然目标无法显式优化全局层面的文本属性（如整体连贯性、格式一致性）。  

为突破这些限制，本文提出**完全从零开始、不依赖任何标注/合成数据**的强化学习（RL）方案，让LLM自主“进化”出超长高质量文本生成能力。  


### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于强化学习的无监督超长文本生成框架  
传统SFT依赖固定参考文本，而本文采用强化学习，让模型通过**奖励信号**优化长文本生成的全局目标（无需人工构造SFT数据集）。具体采用Group Relative Policy Optimization（GRPO）算法训练策略网络：从基础模型（如Qwen2.5 - 32B）出发，让模型在“写作过程中自主规划与迭代”，逐步掌握超长文本生成能力。  

💡 创新点2：多维度奖励模型设计（Reward Design）  
针对开放域文本生成的复杂性（主观性、多维度），设计**复合奖励函数**，整合多个专项奖励模型（RM），分别引导模型优化以下关键维度：  
- 长度控制（Length RM）：确保输出满足“超长”需求，同时避免无意义冗余；  
- 写作质量（Quality RM）：评估内容流畅度、逻辑性、专业性等；  
- 结构格式（Structure RM）：保障文本结构合理（如分章节、层次清晰）。  

💡 创新点3：测试时缩放（Test - time Scaling）与持续预训练（Continual Pretraining）  
- 测试时缩放：借鉴大模型在数学/代码任务中“长思维链（CoT）”的成功经验，探索在超长文本生成中引入长CoT，增强模型推理与规划能力；  
- 持续预训练：在长文本素材与推理数据上持续预训练，进一步提升RL训练后模型的性能上限。  


### 📈 实验结果
- 基准测试碾压传统SFT：基于Qwen2.5 - 32B训练的LongWriter - Zero，在WritingBench、Arena - Write等长文本写作基准测试中，**全面超越传统SFT方法**；  
- 超越千亿参数模型：在多项指标上击败DeepSeek R1、Qwen3 - 235B等百 billion + 规模的大模型，刷新SOTA；  
- 开源资源丰富：模型 checkpoint 和数据已开源（https://huggingface.co/THU - KEG/LongWriter - Zero - 32B），为社区提供了可复现、可扩展的基础。  


### 💬 可借鉴之处
1. 范式创新：证明强化学习可在“无标注/合成数据”场景下，激活LLM的超长文本生成能力，为大模型能力解锁提供了“非SFT”的新范式；  
2. 奖励工程：多维度复合奖励模型的设计思路，可迁移到其他开放域生成任务（如创意写作、多轮对话），用于刻画“主观性强、无明确ground - truth”场景下的质量评估；  
3. 训练策略：测试时缩放（长CoT）与持续预训练的组合，为提升大模型长文本推理、生成的上限提供了可复用的技术路线；  
4. 落地价值：针对真实世界“超长文本需求”（如报告撰写、法律文书、教育内容创作），提供了更优质的技术方案，推动LLM在专业领域的落地。  


LongWriter - Zero的工作不仅解决了超长文本生成的技术痛点，更展示了强化学习在大模型能力进化中的潜力——无需依赖大量人工标注，也能让模型“自主学习”复杂任务的完成能力。这为大模型研发范式、奖励机制设计等方向，都带来了极具启发性的参考。

## rdpo--real-data-preference-optimization-for-physics-consistency-video-generation
### Abstract
Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/
### 🌟 论文解读 | RDPO：从真实视频中提炼物理先验，革新视频生成的物理一致性

### 📌 背景痛点/本文动机
视频生成技术在视觉质量上取得了显著进展，但要忠实复现真实世界的物理规律仍颇具挑战。基于偏好的模型后训练虽有望提升物理一致性，却依赖昂贵的人工标注数据集或尚未成熟的奖励模型。一方面，构建能检测任意视频物理违规的奖励函数仍是开放难题；另一方面，大规模人工偏好数据集的制作成本高、耗时长，且人类判断存在主观性差异。因此，迫切需要一种高效、无需标注的偏好优化策略来针对性提升物理真实性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出无标注框架RDPO  
Real Data Preference Optimization（RDPO）是一个无需人工标注的偏好优化框架，旨在直接从真实世界视频中提炼物理先验。它绕开了传统DPO或RLHF对人类输入的依赖，以真实视频片段作为物理动态信息的“黄金标准”，探索利用真实世界视频固有物理先验的高效路径，而非对这些视频反复训练。  

💡 创新点2：自动构建偏好对的方法  
RDPO借助预训练生成器对真实视频序列进行反向采样，自动构建在物理正确性上具有统计区分度的偏好对。在反向采样过程中，通过选择性利用 latent 表示来构造视频，这些 latent 表示既源于真实数据（富含真实物理信息），又与模型固有生成分布保持对齐（避免过度改变模型视觉风格或产生分布外输出）。  

💡 创新点3：多阶段迭代训练机制  
采用多阶段迭代训练调度，引导生成器逐步更好地遵循物理定律。利用真实视频中探索到的动态信息，让生成视频在动作连贯性与物理真实性上得到显著提升。  


### 📈 实验结果
论文通过在多个基准测试与人类评估中验证，RDPO 在多个维度（如物理一致性、视频整体质量等）实现了提升，有力证明了其在改进不同基线模型物理一致性与视频质量方面的有效性；同时还对比分析了 RDPO 与依赖手动标注的传统 DPO 方法，探索了二者结合的潜在协同效益。

### 💬 可借鉴之处
1. 无标注范式的创新：为解决需大量人工标注的任务提供了思路，展示了如何从真实数据中自动挖掘监督信号，减少对人工标注的依赖。  
2. 物理先验的利用：在视频生成这类需遵循现实世界规律的任务中，提供了从真实数据提炼领域先验（如物理规律）的范例，可启发其他需结合现实知识的生成类任务（如模拟仿真、虚拟场景构建等）。  
3. 迭代训练与分布对齐：多阶段迭代训练调度以及 latent 空间对齐的思路，对平衡“引入新先验”与“保持模型原有能力/分布”具有参考价值，在模型微调、领域适配等场景中或可复用。  

## text-detoxification--data-efficiency--semantic-preservation-and-model-generalization
### Abstract
The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at: https://github.com/allacnobug/Detoxification-of-Text.
### 🌟 论文解读 | 文本去毒：数据高效、语义保留与模型泛化的新突破

### 📌 背景痛点/本文动机
随着在线媒体平台的快速发展，社交媒体上有毒内容（如侮辱、歧视、仇恨言论等）的广泛传播对网络环境和公共话语构成严重威胁，因此急需能在保留原始语义的同时有效去除毒性的去毒方法。然而现有方法存在诸多不足：一是难以同时实现强去毒性能、语义保留和对分布外数据的鲁棒性；二是依赖昂贵的人工标注平行语料且数据效率低。此外，以往基于较小模型（如T5、BART）的方法在处理语义复杂、风格多变的有毒内容时，语义理解和泛化能力有限，易出现分布外（OOD）场景下表现不佳的情况，且在去毒和语义保留间难以平衡；大语言模型（LLM）虽有强语义理解和泛化能力，但因与人类价值观对齐过程对有毒内容高度敏感，简单prompt工程或few - shots方法易拒生成或过度改变原意，而人工标注有毒内容成本高且数据质量存疑，直接监督微调（SFT） noisy数据易效果差且限制模型性能。基于此，本文探索利用LLM解决文本去毒问题，提出两阶段训练框架应对挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：两阶段训练框架实现数据高效去毒
首先使用小批量高质量、经过筛选的平行数据进行监督微调（SFT）来建立强初始化，此步骤能利用优质数据让模型先学习到去毒和语义保留的基础模式，避免在噪声数据上微调带来的“垃圾进垃圾出”问题；然后利用无标注的有毒输入和自定义奖励模型，通过Group Relative Policy Optimization（GRPO）训练LLM。该框架仅用20%的标注数据就能实现优异性能，大幅降低对昂贵人工标注的依赖。

💡 创新点2：平衡去毒性能与语义保留
首次同时优化去毒有效性和语义一致性。在训练过程中，奖励函数联合考虑语义相似性和去毒质量，引导模型在去除毒性的同时尽可能保留原始语义，解决了以往方法要么去毒好但语义丢失多、要么语义保留好但去毒不彻底的两难问题，在多基线对比中实现了当前最优性能。

💡 创新点3：引入GRPO增强OOD性能
首次将基于GRPO的强化学习引入有毒内容重写任务。利用强化学习以无标注方式让LLM与去毒目标对齐，提升了模型对多样且不断演变的有毒语言的泛化能力和鲁棒性，应对分布外数据时表现更强。

### 📈 实验结果
实验结果表明该方法有效缓解了先前工作面临的权衡问题，在提升泛化能力的同时，显著降低对标注数据的依赖，实现了当前最优性能。（文中未详细展开实验数据细节，但从摘要和引言可推断在去毒效果、语义保留度、OOD场景表现等方面均超越现有方法）

### 💬 可借鉴之处
1. 数据利用思路：在数据稀缺且标注昂贵的任务中，可借鉴先筛选高质量小数据监督微调初始化，再结合无标注数据和强化学习优化的思路，提升数据效率。
2. 多目标优化：面对需平衡多个目标（如去毒与语义保留）的任务，设计联合考虑各目标的奖励函数或优化目标，是实现多目标最优的有效方式。
3. 强化学习在LLM微调的应用：将强化学习引入LLM针对特定任务（如文本去毒）的微调，为提升模型泛化性和鲁棒性提供了新范式，可推广到其他需模型适应多变输入场景的NLP任务中。

## shrinking-the-generation-verification-gap-with-weak-verifiers
### Abstract
Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.
### 🌟 论文解读 | 用弱验证器缩小生成-验证差距：Weaver框架的创新之路

### 📌 背景痛点/本文动机
在部署语言模型（LM）时，验证模型响应的质量或正确性是核心挑战，这一问题在数据集整理、模型对齐和推理时决策等LM pipeline各环节都存在。借助完美验证器结合重复采样（生成多个候选响应后选最优）能大幅提升模型在数学、代码、推理等任务的能力，但完美验证器要么不可扩展（如人工验证），要么实用性有限（如Lean这类形式化证明工具）。而作为通用验证器的LM裁判和奖励模型，与“ oracle verifiers（完美准确的验证器）”仍存在显著性能差距，即“生成 - 验证差距”——模型能生成正确响应但无法被识别。同时，弱验证器（如LM裁判、奖励模型）存在分数噪声大、校准差、假阳性率高等问题，且整合弱验证器还面临 naive 聚合不足、有限标注数据下有效集成难、推理时部署验证成本高等挑战，因此本文旨在探索如何结合多个弱验证器来改进重复采样下的响应选择，缩小生成 - 验证差距。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Weaver框架聚合弱验证器
Weaver是一个无需在真实标签上进行有监督微调来聚合弱验证器的框架。首先发现，在有大量带标签训练数据时，学习加权验证器集合（利用验证器准确率差异）能比 naive 平均（假设验证器质量一致，易让低质量验证器主导致精度下降）最多高出11.2个百分点；当缺乏大量标注数据时，将弱监督（WS）适配到验证场景，解决输出不一致和低精度验证器问题，通过过滤无信息验证器、归一化验证器分数，并基于这些分数和未知真实标签构建 latent variable model 来估计验证器准确率作为集合权重。

💡 创新点2：解决弱验证器集成的多挑战
针对弱验证器集成的三大挑战逐一应对：对于 naive 聚合不足，用加权集合替代 naive 平均，利用验证器准确率差异提升性能；对于有限标注数据下有效集成难，适配弱监督技术，处理弱验证器输出格式不一致（如logits、二分类分数、Likert分数等）和低质量问题，借助数据集统计归一化输出和过滤特定验证器；对于推理时部署验证成本高，用Weaver的组合输出分数训练紧凑的400M跨编码器，在大幅降低计算成本同时保留高准确率。

### 📈 实验结果
在测试时重复采样场景（模型生成多个候选响应后选择其一）下评估，Weaver相比验证器分数无加权平均的重复采样性能提升17.1%，相比多数投票提升13.5%；对比LM的Pass@1（选第一个候选响应的性能），在推理和数学任务上，对8B模型性能提升17.9%，对70B模型提升14.5%；用Llama 3.3 70B Instruct作生成器、70B或更小的裁判和奖励模型作验证器集合时，能达到o3 - mini水平准确率（平均87.7%），该增益堪比GPT - 4o到o3 - mini的性能跃升（69.0% vs. 86.7%，后者需大量微调与后训练）；训练的400M跨编码器蒸馏模型保留了Weaver全精度的98.7%，同时将验证计算量降低达99.97%。

### 💬 可借鉴之处
1. 多弱验证器聚合思路：当面临多个有缺陷但互补的工具/模型时，可借鉴Weaver加权聚合并结合弱监督估计权重的方式，充分利用各工具优势，减少对大量标注数据依赖。
2. 性能 - 成本权衡：在追求模型性能提升同时关注推理成本，如Weaver通过蒸馏得到紧凑模型降低计算成本，这种在应用中平衡性能与资源消耗的思路值得借鉴。
3. 弱监督适配特定场景：针对自身任务场景中类似“弱验证器输出格式不一、质量参差”等问题，可参考Weaver利用数据集统计进行归一化、过滤等手段来适配弱监督技术，拓展弱监督应用边界。

## reasongrm--enhancing-generative-reward-models-through-large-reasoning-models
### Abstract
Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.
```
### 🌟 论文解读 | ReasonGRM：借大模型推理能力革新生成式奖励模型

### 📌 背景痛点/本文动机
大语言模型（LLMs）在理解、生成与决策上取得长足进步，但要让模型输出贴合人类价值观，奖励模型（RM）是关键。传统标量奖励模型（SRMs）把复杂人类偏好压缩成单一标量，易信息丢失、泛化性弱；新兴生成式奖励模型（GRMs）虽更灵活，但推理能力不足，常出现推理路径不完整或过度推测，导致任务中“幻觉”或关键信息缺失。因此，如何提升GRMs的推理质量以实现可靠偏好建模，成了核心问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ReasonGRM三阶段框架  
ReasonGRM分三步打造更优生成式奖励模型：  
- 阶段一（生成推理路径）：用Zero - RL生成简洁、以结果为导向的推理路径，减少关键信息遗漏风险；  
- 阶段二（筛选优质路径）：引入全新评估指标\( R^\star \)，依据生成可能性为推理路径打分，偏好“用最少探索达正确答案”的路径，削减训练中易引发幻觉的数据；  
- 阶段三（强化模型能力）：针对高难度示例用强化学习进一步精调模型，增强其偏好区分能力。  

💡 创新点2：定义\( R^\star \)评估指标解决数据质量难题  
\( R^\star \)结合“有效性（Validity，推理导向正确结果）”与“自一致性（Self - Consistency，推理逻辑连贯无冗余）”两大关键属性，通过生成可能性来评估推理路径，能从噪声候选集中自动选优质推理路径，破解复杂任务奖励模型训练的数据质量瓶颈。  


### 📈 实验结果
在RM - Bench、RewardBench、RMB三大公开基准测试中，ReasonGRM表现亮眼：平均超越此前最优GRMs 1.8%，在部分场景下比GPT - 4o等闭源模型领先达5.6%，还比主流SRMs平均高4.5%。实验不仅验证了方法有效性，消融实验也剖析了推理质量、\( R^\star \)过滤效果、各训练阶段对最终奖励模型的影响。  


### 💬 可借鉴之处
1. 重视推理质量在奖励模型中的价值：揭示了高质量推理路径（兼顾有效性与自一致性）对偏好建模的关键作用，为后续奖励模型设计指明“推理感知”方向；  
2. 创新评估与过滤机制：\( R^\star \)展示了如何用生成可能性量化推理质量，为处理噪声数据、构建优质训练集提供了可复用思路；  
3. 多阶段训练Pipeline：从生成到筛选再到强化学习的流程，为通用LLM向专精奖励模型转化提供了工程化参考范式；  
4. 全面实验验证：跨多个权威基准的测试+消融实验，是学术研究中验证方法普适性与模块价值的典范，值得借鉴以增强研究说服力。  
```

## robust-reward-modeling-via-causal-rubrics
### Abstract
Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.
### 🌟 论文解读 | 基于因果准则打造鲁棒奖励模型：Crome框架破解奖励黑客难题

### 📌 背景痛点/本文动机
在大语言模型（LLM）对齐人类反馈的过程中，奖励模型（RM）是核心环节，但当前奖励模型普遍面临 “奖励黑客” 问题。传统RM常把训练数据里的统计相关性错当成质量的真正因果驱动因素，比如会依据回复长度、格式这类表面或虚假属性来打分，而非事实性、相关性等真实质量维度。这是因为标准训练目标难以区分因果因素和虚假关联，最终导致RM脆弱、策略对齐失效。所以，如何让RM在未知虚假属性的情况下，只依托能获取的真实因果质量属性来实现鲁棒训练，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无虚假感知的因果框架  
提出基于因果模型的奖励模型训练框架Crome，无需预先指定或干预任何虚假属性，仅通过调用 “ oracle LLM ” 识别出的因果质量准则进行干预，就能引导RM学习，从机制上绕开了对虚假属性先验知识的依赖。  

💡 创新点2：基于因果属性的靶向反事实增强  
设计两类合成训练样本增强方式：  
- **因果增强（Causal Augmentations）**：生成在特定因果属性（如事实性）上有差异的样本对，让RM对真实质量变化的维度产生敏感性，精准捕捉因果维度的影响；  
- **中性增强（Neutral Augmentations）**：利用因果增强后的数据与原始偏好对，生成在虚假特征（如风格）上变化但因果内容保留的样本，并搭配平局标签，强制RM对虚假属性保持不变性。且整个过程无需显式知晓虚假因素，仅通过因果准则干预就可缓解对大量虚假关联的敏感。  

### 📈 实验结果
在RewardBench等多个基准测试中，Crome表现远超标准基线：  
- 在RewardBench上平均准确率提升最高达5.4%，其中安全（Safety）类别提升13.18%、推理（Reasoning）类别提升7.19%；  
- 在Best - of - N推理场景下，面对RewardBench、安全专项的WildGuardTest、推理专项的GSM8k等基准，随着N增大，Crome的RM在选择最优结果时持续领先基线，证明其在应对稀有（长尾）虚假因素时也具备鲁棒性。  

### 💬 可借鉴之处
1. 因果视角的鲁棒训练思路：将因果建模引入奖励模型训练，为解决“虚假关联干扰模型学习”这类普遍问题提供了新范式，可启发后续在其他需区分因果与相关场景的模型训练工作；  
2. 无先验的增强策略：展示了如何在不依赖虚假属性先验知识的情况下，仅通过对因果属性的干预来间接减少虚假关联影响，这种“绕开未知、强化已知因果”的思路在数据增强、鲁棒训练方向有推广价值；  
3. 多场景鲁棒性验证：在聊天、安全、推理等不同任务基准及Best - of - N这类实际推理场景下验证有效性，证明方法具备跨任务、跨场景的普适性，为工业级大模型对齐流程的鲁棒性优化提供了可落地参考。

## relic--enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples
### Abstract
Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.
### 🌟 论文解读 | RELIC：小样本示例助力低资源印度语言奖励模型泛化能力提升

### 📌 背景痛点/本文动机
奖励模型是大语言模型（LLMs）与人类偏好对齐的关键，但现有开源多语言奖励模型多基于高资源语言的偏好数据集训练，在低资源印度语言上奖励信号不可靠。收集低资源语言大规模高质量偏好数据成本极高，基于偏好的训练方法难以实施。同时研究发现开源多语言奖励模型在低资源印度语言中无法准确区分安全与不安全响应，如对低资源语言Bodo，奖励模型给不安全响应的分数反而高于安全响应，这对依赖这些低资源语言社区使用安全对齐的LLMs构成挑战，因此需要新方法提升低资源印度语言奖励模型性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出RELIC框架  
RELIC是针对低资源印度语言奖励建模的新型上下文学习框架。它训练检索器时采用 pairwise ranking 目标，从辅助高资源语言中选择上下文示例，这些示例能最有效地突出偏好响应和次偏好响应之间的区别，为奖励模型提供有判别性的上下文。

💡 创新点2：检索器训练与适用性  
RELIC使用 pairwise ranking loss（基于Thurstone、Bradley - Terry 相关理论）训练检索器，优化其选择最能区分正响应和负响应的上下文示例。且不需要访问配对偏好数据集，可应用于任何包含二元质量标签的低资源数据集，适用性广，还结合高资源语言辅助示例库，推理时能提供更丰富有判别性的上下文。

### 📈 实验结果
在 PKU - SafeRLHF、WebGPT、HH - RLHF 三个偏好数据集上，用最先进的开源奖励模型进行大量实验。结果显示RELIC显著提高低资源印度语言奖励模型准确率，持续超越现有示例选择方法。如在低资源印度语言Bodo上，用LLaMA - 3.2 - 3B奖励模型时，相比零样本提示和现有最先进示例选择方法，准确率分别提升12.81%和10.13%；在Santali语言上，基于LLAMA - 3.1 - 8B的奖励模型，RELIC比零样本提示准确率提升24.16%，比现有基于相关性的示例选择方法提升21.26%。

### 💬 可借鉴之处
1. 分析角度可借鉴：对低资源语言上开源多语言奖励模型泛化能力分析，揭示响应质量区分能力的关键缺口，为后续研究指明问题方向。
2. 方法创新可借鉴：RELIC框架针对低资源场景下数据稀缺问题，利用上下文学习和检索器结合，且基于奖励模型排序目标训练检索器的思路，为其他低资源语言任务或需要利用高资源辅助数据的任务提供了方法参考。
3. 实验设计可借鉴：在多个典型偏好数据集和开源奖励模型上验证方法有效性，这种全面的实验验证方式能有力支撑方法价值，值得相关研究实验设计学习。

## gflowgr--fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks
### Abstract
Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.
### 🌟 论文解读 | GFlowGR：用生成流网络优化生成式推荐框架的微调

### 📌 背景痛点/本文动机
生成式推荐（GR）框架结合物品 tokenizer 和大语言模型（LLM），在诸多场景取得成功，但目前研究多聚焦于优化物品 tokenizer 或 LLM 解码策略，对 LLM 适配推荐数据的**关键微调环节**探索不足。现有微调方法（如监督微调 SFT、直接偏好优化 DPO）存在“暴露偏差”问题：SFT 只关注数据集中正样本，既没引入负样本对比，也难探索潜在正样本；DPO 等强化学习微调方法虽能利用负样本，但依赖已收集数据，无法充分反映真实用户偏好，忽略了未观测到的潜在正样本。因此，亟需一种能探索潜在正样本、缓解暴露偏差的 LLM 微调方案。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：将生成式推荐建模为多步生成任务，提出 GFlowGR 框架  
把 GR 视为多步骤生成过程，基于生成流网络（GFlowNets）构建微调框架 GFlowGR。GFlowNets 能让 LLM 按与奖励分布成比例的概率生成 token 序列，借此识别潜在正样本、扩展推荐多样性，从生成概率层面缓解暴露偏差。  

💡 创新点2：设计自适应轨迹采样器与多维度奖励模型  
- 自适应轨迹采样器：融合传统推荐系统的协同知识，生成“从易到难”的增强轨迹，用课程式训练思路持续提供高质量训练数据；  
- 多维度奖励模型：综合考量增强信号、协同分数、语义相似度等，有效区分采样到的未观测轨迹中“用户可能偏好”的物品；  
- 置信加权机制：依据增强信号与协同分数的一致性，为每个样本分配置信权重，进一步缓解暴露偏差。  


### 📈 实验结果
论文在**两个真实世界数据集**、基于**两种不同 GR 骨干模型**开展大量实验。结果验证了 GFlowGR 在缓解暴露偏差、提升推荐效果上的有效性与鲁棒性，证明该框架能在不同数据和模型底座下稳定发挥作用。

### 💬 可借鉴之处
1. 跨领域融合思路：将生成流网络（GFlowNets）与推荐系统结合，为解决推荐场景中“暴露偏差”这类特有问题提供了新范式，启发后续跨领域技术迁移；  
2. 课程式采样与多维度奖励：自适应轨迹采样的“从易到难”课程训练、多维度奖励模型的构建方式，为处理“数据稀疏+偏好难建模”场景提供了可复用的设计思路；  
3. 问题建模视角：把推荐任务拆解为多步生成任务，重新定义推荐系统与生成式模型的交互方式，为推荐系统的“生成式转型”提供了方法论参考。  

## gram--a-generative-foundation-reward-model-for-reward-generalization
### Abstract
In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
### 🌟 论文解读 | GRAM：面向奖励泛化的生成式基础奖励模型

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的对齐工作中，奖励模型扮演着关键角色，但传统奖励模型多以判别式模型形式训练，且高度依赖带标签的人类偏好数据。这带来了两方面问题：一是强化学习算法复杂度与标注数据获取难度导致奖励模型应用成本高昂；二是现有训练方式对大量无标签数据利用不足，难以得到能灵活适配多任务的通用奖励模型。因此，本文旨在探索利用无标签和有标签数据训练奖励模型的方法，打造可泛化的基础奖励模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：两阶段训练的生成式奖励模型架构  
基于大语言模型的生成式能力，构建生成式奖励模型（GRAM），分两阶段训练。第一阶段在大规模无标签的输入 - 响应数据上进行无监督预训练，学习输入与响应间的对应关系，无需偏好标注数据，可规模化获取响应比较的通用知识；第二阶段利用人类偏好数据进行有监督微调，让模型学会预测两个响应间的偏好关系。最终得到的基础奖励模型能直接用于下游任务或仅用少量任务特定数据微调。

💡 创新点2：标签平滑下的损失函数统一视角  
引入标签平滑技术到奖励模型训练中，证明此时训练目标可转化为正则化的 pairwise ranking loss（Bradley - Terry 损失）形式。这一成果在一定程度上统一了生成式模型与判别式模型的训练目标视角，为奖励模型训练提供了新认知，且标签平滑对训练生成式奖励模型十分有益，提升了模型泛化性。

### 📈 实验结果
在响应排序、基于人类反馈的强化学习（RLHF）、任务适配等多任务场景下开展大量实验。结果显示，GRAM 在几乎无需或仅需少量微调时，在各任务上泛化性出色。例如，基于 LLaMA - 3.1 - 8B - Instruct 模型训练奖励模型时，在 RewardBench 平均准确率上，相比普通判别式和生成式奖励模型分别提升 11.0 和 5.1 个百分点，显著超越多个强基线模型。

### 💬 可借鉴之处
1. 数据利用思路：打破传统奖励模型对有标签数据的强依赖，示范了无标签数据在预训练阶段为模型注入通用知识的价值，为后续奖励模型甚至其他模型训练在数据利用上开辟了“无标签 + 有标签”结合的思路。
2. 模型架构创新：两阶段的生成式奖励模型架构为打造通用基础奖励模型提供了可行范式，展示了先无监督预训练再监督微调在奖励模型领域的有效性，可启发后续多模态、其他任务导向模型的架构设计。
3. 损失函数与正则化：标签平滑结合后对损失函数的分析与转化，为理解生成式和判别式模型在奖励建模中的联系提供新角度，也提醒开发者在模型训练中重视正则化技术对模型泛化等能力的提升作用。

## vl-genrm--enhancing-vision-language-verification-via-vision-experts-and-iterative-training
### Abstract
Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large
language models but remains underexplored for Vision-Language (VL) models. The
Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing
structured feedback, yet training effective VL-RMs faces two major challenges.
First, the bootstrapping dilemma arises as high-quality training data depends
on already strong VL models, creating a cycle where self-generated supervision
reinforces existing biases. Second, modality bias and negative example
amplification occur when VL models hallucinate incorrect visual attributes,
leading to flawed preference data that further misguides training. To address
these issues, we propose an iterative training framework leveraging vision
experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection
Sampling. Our approach refines preference datasets, enhances structured
critiques, and iteratively improves reasoning. Experiments across VL-RM
benchmarks demonstrate superior performance in hallucination detection and
multimodal reasoning, advancing VL model alignment with reinforcement learning.
### 🌟 论文解读 | VL-GenRM：借视觉专家与迭代训练，突破多模态奖励模型训练困境

### 📌 背景痛点/本文动机
在大语言模型领域，基于可验证奖励的强化微调（RFT）已取得进展，但在视觉-语言（VL）模型中仍待深入探索。视觉-语言奖励模型（VL-RM）是对齐VL模型的关键，能提供结构化反馈，然而训练高效VL-RM面临两大核心挑战：一是“自举困境”，高质量训练数据依赖强VL模型生成，易强化模型固有偏差；二是“模态偏差与负例放大”，VL模型对视觉属性的错误幻想会产生有缺陷的偏好数据，误导训练。为解决这些问题，论文提出创新训练框架VL-GenRM。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：借视觉专家自动构建偏好数据集  
利用视觉专家（如擅长目标检测、深度估计等的模型）生成大规模偏好数据集，提升VL-GenRM训练时的监督质量，打破“自举困境”中依赖自生成数据强化偏差的问题，为模型提供更可靠的训练依据。  

💡 创新点2：CoT增强VL-GenRM训练  
引入思维链（CoT）推理生成技术，为VL-GenRM训练提供系统性指导。通过结构化推理过程，增加数据中有效正确描述占比，缓解自生成数据的局限性，强化奖励建模的连贯性，让模型学习更合理的评估逻辑。  

💡 创新点3：基于边际拒绝采样的迭代自举  
通过对“正例与负例奖励信号边际”筛选出的优质推理依据，进行迭代微调，持续优化VL-GenRM的推理能力。让模型在多轮训练中逐步向更优输出适配，不断提升对视觉-语言场景的评估与推理水准。  


### 📈 实验结果
论文在VL-RM基准测试与Best-of-N采样等实验中验证方法有效性，在幻觉检测（识别VL模型错误幻想视觉属性等问题）与多模态推理任务上展现出更优性能，推动了VL模型借助强化学习实现更好对齐。  

### 💬 可借鉴之处
1. 跨模态领域数据增强思路：引入领域专家（如视觉专家）辅助构建训练数据，为突破“自举循环”提供了新范式，可推广到其他需多模态协作、依赖数据质量的任务。  
2. 结构化推理融入训练：借助CoT将模糊的评估转化为可解释的推理步骤，为提升模型可解释性与训练有效性提供了参考，在复杂任务型模型训练中具借鉴价值。  
3. 迭代式训练策略：基于奖励边际的采样与迭代微调，让模型能力逐步迭代提升，这种“小步快跑、数据择优”的训练逻辑，在强化学习与多轮优化场景中值得复用。  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### 🌟 论文解读 | 告别繁琐标注：GAN - RM 让奖励建模“以假乱真”

### 📌 背景痛点/本文动机
在视觉生成模型的训练后增强中，奖励模型至关重要。然而当前奖励建模方法存在诸多难题：一是构建奖励模型需大量人工标注偏好数据，收集成本高昂，且基于特定生成模型输出域标注的数据，在应用到不同输出域模型时存在域差距；二是为全面评估生成内容质量，需人工设计多种评估指标，既增加工程成本，又难在不同维度间取得最优平衡，还难保证与人类普遍偏好契合。因此，本文受生成对抗网络（GAN）中对抗训练启发，提出 GAN - RM 框架，旨在摆脱手动偏好标注和显式质量维度设计，高效构建奖励模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无需手动偏好标注，利用少量代理数据
GAN - RM 仅需少量（几百个）无标注的代表性样本（即偏好代理数据，Preference Proxy Data）作为外部数据。通过训练奖励模型区分偏好代理数据和生成模型输出，让模型学习评估生成样本。同时采用基于排名的自举策略，将 GAN - RM 在这些样本上的置信分数作为软标签，利用额外数据再训练 GAN - RM，使其更好捕捉潜在人类偏好。
💡 创新点2：支持多轮训练，迭代对齐偏好
GAN - RM 支持多轮训练后优化。每一轮中，将被识别为接近偏好代理数据的样本用于生成器的训练后优化，反过来再训练判别器以区分这些更难的样本。这种迭代的“以假乱真”过程能逐步让生成质量与偏好代理数据中的潜在人类偏好对齐。

### 📈 实验结果
实验表明，基于 GAN - RM 的方法在性能上可与依赖大量标注数据（如 Pickapic 的 100 万标注人类偏好数据）的方法（如相关对比方法）相当甚至超越。在图像质量实验设置中，GAN - RM 仅需 500 个偏好代理数据样本。除图像质量提升实验外，在图像安全和视频质量增强场景下的实验也凸显了 GAN - RM 框架在不同场景下的泛化能力，验证了其在测试时缩放（如 Best - of - N 样本过滤）、监督微调（SFT）和直接偏好优化（DPO）等训练后方法中的有效性。

### 💬 可借鉴之处
从方法创新角度，GAN - RM 为解决奖励建模中数据获取难、依赖特定域、人工设计维度难契合人类偏好等问题提供了新思路，其利用对抗训练和少量代理数据的方式，减少了对大规模人工标注的依赖，降低工程成本；从应用拓展角度，该框架在图像、视频等多场景的有效实验，为视觉生成模型在不同领域的训练后增强提供了可复用的奖励建模范式，后续在视觉生成相关任务中，若需构建奖励模型，可借鉴其利用少量代理数据和对抗训练的思路来降低成本与难度。

## personalized-llm-decoding-via-contrasting-personal-preference
### Abstract
As large language models (LLMs) are progressively deployed in various
real-world applications, personalization of LLMs has become increasingly
important. While various approaches to LLM personalization such as prompt-based
and training-based methods have been actively explored, the development of
effective decoding-time algorithms remains largely overlooked, despite their
demonstrated potential. In this paper, we propose CoPe (Contrasting Personal
Preference), a novel decoding-time approach applied after performing
parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is
to leverage reward-guided decoding specifically for personalization by
maximizing each user's implicit reward signal. We evaluate CoPe across five
open-ended personalized text generation tasks. Our empirical results
demonstrate that CoPe achieves strong performance, improving personalization by
an average of 10.57% in ROUGE-L, without relying on external reward models or
additional training procedures.
### 🌟 论文解读 | 解码阶段个性化LLM新范式：CoPe让模型更懂你

### 📌 背景痛点/本文动机
随着大语言模型（LLMs）在现实应用中广泛部署，LLM的个性化变得愈发重要。目前已有基于提示（prompt - based）和基于训练（training - based）的个性化方法，但解码阶段的有效算法开发却被忽视，尽管其有很大潜力。基于提示的方法缺乏对用户数据的直接学习，效果受限；基于训练的方法虽能更好捕捉用户偏好，但存在灾难性遗忘和计算成本增加等问题。所以本文旨在从解码阶段入手，提出新方法实现LLM的有效个性化。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出CoPe（Contrasting Personal Preference）解码阶段方法  
CoPe是在对用户特定数据进行参数高效微调（PEFT）后应用的解码阶段新方法。它属于奖励引导解码的一种，但无需外部奖励模型，而是利用PEFT调优模型和原始基础模型的似然来近似隐式用户奖励信号，将这种隐式奖励与对比解码目标相联系，实现奖励引导解码用于个性化，让生成文本更好地与用户偏好对齐。

💡 创新点2：增强PEFT捕捉隐式用户奖励  
通过Direct Preference Optimization（DPO）对比正面响应（用户提供）和负面响应（不太可能来自用户，如其他用户或合成的低隐式奖励输出）之间的隐式奖励。为避免依赖其他用户数据的隐私和实际挑战，用Best - of - N采样生成低隐式奖励的合成负面响应。这种训练方法不仅提升了PEFT的有效性，也为奖励引导解码提供更准确的隐式用户奖励建模，进而提升整体个性化效果。

### 📈 实验结果
在来自Language Model Personalization（LaMP）和LongLaMP基准的五个不同个性化开放式文本生成任务中评估CoPe。结果显示，与任务微调模型相比，CoPe在ROUGE - L上平均相对提升10.57%；与缺乏对比机制的简单个性化模型相比，在各任务中ROUGE - L平均提升5.67%。而且CoPe在不同规模和类型的最先进LLMs上泛化性良好，证明其隐式奖励最大化能进一步增强与个体用户偏好的对齐。

### 💬 可借鉴之处
1. 解码阶段个性化思路：开拓了LLM个性化在解码阶段的研究方向，展示了无需额外训练流程和外部奖励模型，仅在解码时利用模型自身似然对比实现个性化的可行性，为后续解码阶段个性化方法提供了新思路。
2. PEFT增强方式：利用DPO和合成负面样本增强PEFT捕捉用户隐式奖励的方式，为参数高效微调在个性化场景下的优化提供了可参考的技术路线，在处理用户数据隐私和避免依赖外部数据方面给出了创新解法。
3. 多任务多模型验证：在多个任务和不同LLM上验证有效性，这种全面的实验设计思路以及所展现出的泛化能力，为相关方法的实用性验证提供了范例，让该方法在实际落地到不同场景时更具可信度。

## med-prm--medical-reasoning-models-with-stepwise--guideline-verified-process-rewards
### Abstract
Large language models have shown promise in clinical decision making, but
current approaches struggle to localize and correct errors at specific steps of
the reasoning process. This limitation is critical in medicine, where
identifying and addressing reasoning errors is essential for accurate diagnosis
and effective patient care. We introduce Med-PRM, a process reward modeling
framework that leverages retrieval-augmented generation to verify each
reasoning step against established medical knowledge bases. By verifying
intermediate reasoning steps with evidence retrieved from clinical guidelines
and literature, our model can precisely assess the reasoning quality in a
fine-grained manner. Evaluations on five medical QA benchmarks and two
open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art
performance, with improving the performance of base models by up to 13.50%
using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by
integrating it in a plug-and-play fashion with strong policy models such as
Meerkat, achieving over 80\% accuracy on MedQA for the first time using
small-scale models of 8 billion parameters. Our code and data are available at:
https://med-prm.github.io/
### 🌟 论文解读 | Med - PRM：医疗推理模型的“步步为营”新范式

### 📌 背景痛点/本文动机
临床决策制定（CDM）是一个复杂多步骤过程，大语言模型（LLMs）虽在医疗应用有进展，但现有方法难在推理特定步骤定位和纠正错误，而医疗领域识别与解决推理错误对准确诊断和有效医疗至关重要。同时，过程奖励建模（PRM）应用于医疗有挑战：一是高质量步骤级监督获取成本高且费力，现有自动标注策略易低估合理但没导向正确结果的早期步骤；二是医疗推理需大量领域知识，仅靠语言模型参数难完全涵盖，训练奖励模型缺医疗上下文也不够。这些痛点推动了Med - PRM的提出。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Med - PRM框架
Med - PRM是检索增强的过程奖励建模框架，采用RAG - AS - A - JUDGE方式，基于临床问题和检索到的医疗文档对每个推理步骤做逐步评估。该评估相比训练时基于采样的自动标注方法，更贴近专家医师标注，在训练和推理阶段融入临床知识，能更精准评估中间推理步骤。
💡 创新点2：兼具通用性与高效性
Med - PRM展现出即插即用的通用性，可与Meerkat等强大策略模型结合。且训练成本高效，如针对花费约2万美元训练数据的UltraMedical模型，Med - PRM用花费不到20美元的精心策划数据集训练，仍能提升性能，体现成本效益与扩展性。

### 📈 实验结果
在五个医疗QA基准和两个开放式诊断任务评估中，Med - PRM实现了最先进性能，能将基础模型性能提升高达13.50%。与强策略模型结合后，用80亿参数小规模模型在MedQA首次实现超80%准确率，在七个医疗基准中六个实现SOTA，在MedQA（4选项）上用8B参数模型达到80.35%准确率，平均比现有PRM基线在七个医疗基准上高3.44%。

### 💬 可借鉴之处
从技术创新看，检索增强结合过程奖励建模用于垂直领域推理评估是很好思路，为领域特定的LLM优化提供方向；在工程实践上，展示了低成本数据训练高效模型辅助强模型提升性能的路径，为资源有限但需提升模型医疗推理能力的场景提供参考；从医疗AI发展角度，强调步骤级验证和医疗知识融入，让模型推理更透明可靠，为医疗AI贴近临床实际应用标准提供了实践范式，后续医疗或其他垂直领域的推理模型优化可借鉴其步骤验证、知识融合与成本控制等思路。

## agent-rlvr--training-software-engineering-agents-via-guidance-and-environment-rewards
### Abstract
Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too sparse for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.
### 🌟 论文解读 | Agent-RLVR：让大模型在软件工程任务中“拜师学艺”的RL框架

### 📌 背景痛点/本文动机
强化学习从可验证奖励（RLVR）在数学、竞赛编程等可验证领域提升大语言模型（LLM）推理能力表现出色，但在智能体环境（多步骤、复杂问题求解场景）中效果骤降。这类场景奖励稀疏，前沿LLM也易高失败率，传统RLVR难以有效训练。同时，智能体环境需多轮推理、与外部环境交互，训练复杂度高，为让RLVR在复杂真实场景（如软件工程）生效，催生了Agent - RLVR框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Agent - RLVR框架适配智能体场景  
借鉴人类教学法引入“agent guidance（智能体指导）”机制，利用从高层战略规划到错误与环境交互动态反馈等多样信息线索，引导智能体走向成功轨迹，像老师指导新人一样帮智能体在复杂解空间导航，还能通过环境探索促进自我提升。训练循环分三步：先让智能体无指导尝试生成初始轨迹，用单元测试验证并补充指导；再让智能体带指导重试；最后基于指导后轨迹奖励用RLVR更新策略。
💡 创新点2：构建软件工程领域专属数据集  
精心整理含817个训练环境的数据集，涵盖问题陈述、环境和指导信息，超越传统输入 - 输出对，捕捉带集成指导信号的完整编码环境，为训练软件工程智能体提供丰富资源。

### 📈 实验结果
在SWE - Bench Verified基准测试中，Agent - RLVR将Qwen - 2.5 - 72B - Instruct的pass@1性能从9.4%提升至22.4%；指导增强的RLVR数据用于测试时奖励模型训练，能进一步把pass@1推至27.8%；指导模型在pass@1（19.8%→22.4%）和pass@32（34.2%→38.4%）上都有提升，验证指导是关键组件，也体现方法在小数据集下提升智能体多步骤推理能力的高效性。

### 💬 可借鉴之处
1. 应对稀疏奖励场景时，引入类人类教学的指导机制是有效思路，为复杂多步骤推理任务中模型训练提供新范式参考。
2. 构建领域专属、含丰富环境与指导信息的数据集，能为特定领域智能体训练筑牢数据基础，这种“定制化 + 场景化”数据构建思维值得借鉴。
3. 展示了RLVR数据在奖励模型训练等方面的额外价值，启发后续探索不同模块间数据复用与协同增效，拓展技术应用边界。

## reguidance--a-simple-diffusion-wrapper-for-boosting-sample-quality-on-hard-inverse-problems
### Abstract
There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.
### 🌟 论文解读 | ReGuidance：为困难逆问题提升采样质量的简洁扩散包装器

### 📌 背景痛点/本文动机
近年来，利用预训练扩散模型作为先验来解决逆问题以及通过奖励模型引导扩散模型的研究十分活跃。像扩散后验采样（DPS）这类无训练方法虽提供了灵活的启发式算法，但当奖励信息不足时（如低信噪比的困难逆问题场景），这些技术会偏离数据流形，无法生成逼真输出。例如在大区域图像修复、高倍数超分辨率等硬逆问题中，现有先进基线方法表现不佳，既难保证采样来自合适的倾斜密度（后验分布），生成结果的真实感也很差。本文正是为解决这些问题，提出提升样本真实感与奖励表现的方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出ReGuidance方法  
ReGuidance是一个简洁的包装器算法，用于提升现有基于扩散的逆问题求解器的样本质量。它以用户选择的逆问题求解器生成的候选解$\hat{x}$为输入，分两步操作：第一步，从$\hat{x}$出发反向运行预训练扩散模型针对基础密度$q$的确定性采样器，提取与之关联的潜在变量$x^*$；第二步，以$x^*$为初始化，运行Diffusion Posterior Sampling（DPS）算法来生成新的重建结果$x_{\text{DPS}}$ 。  

💡 创新点2：理论保障  
从理论层面证明，在某些多模态数据分布下，ReGuidance能同时提升奖励并使候选解更接近数据流形。这是首次为DPS提供严格的算法保障，填补了该领域理论层面的部分空白，从数学角度支撑了方法的有效性。

### 📈 实验结果
在图像修复的传统困难逆任务（如大区域图像修复、高倍数超分辨率）中展开评估：当现有先进基线方法表现不佳时，在这些基线之上应用ReGuidance，能显著提升样本质量与测量一致性。以图像修复任务为例，对不同基线应用ReGuidance后，奖励和真实感指标都有持续且显著的提升；定性层面还能得到多样且逼真、与原始测量样本有意义区别的重建结果。 

### 💬 可借鉴之处
1. 方法设计简洁模块化：ReGuidance的两步操作思路清晰，易于理解和整合到现有基于扩散模型的逆问题求解流程中，为改进现有方法提供了轻量但有效的思路。  
2. 兼顾实证与理论：既通过实验验证在硬逆问题场景下对样本质量的提升，又给出理论证明支撑方法优势，这种从实践到理论的完整研究范式值得相关领域研究借鉴，帮助后续工作在方法创新时更注重理论与实证的结合。 
3. 针对硬逆问题场景：聚焦低信噪比等硬逆问题场景展开研究，为这类长期困扰的难题提供了新的解决方向，启发研究者关注更具挑战性的逆问题场景下的方法优化。 

## reinforcement-learning-fine-tuning-of-language-model-for-instruction-following-and-math-reasoning
### Abstract
This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.
### 🌟 论文解读 | 小模型也能打！强化学习微调让轻量级语言模型玩转指令遵循与数学推理

### 📌 背景痛点/本文动机
生成式语言模型在自然语言理解与生成领域取得了亮眼成果，但如何让小规模语言模型在指令遵循、数学推理等不同推理任务中表现出色仍是难题。同时，不同微调技术（尤其是强化学习类技术）间的性能对比也有待深入探索。为此，研究聚焦于轻量级开源模型Qwen2.5 - 0.5B Base，探索基于强化学习的微调方法在偏好对齐与特定领域适配方面的表现，以明确轻量级模型在类人对齐学习场景下的能力与局限。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多微调技术对比与RLOO reward model探索  
对比了监督微调（SFT）、直接偏好优化（DPO）、Reinforce Leave - One - Out（RLOO）三种微调技术。在RLOO中，还评估了DeBERTa、DistilBERT、Siamese DistilBERT等不同奖励模型，以此探究奖励模型对最终策略性能的影响。  
💡 创新点2：数学推理任务的数据增强与推理工具结合  
为提升模型数学推理能力，基于Countdown数据集构造含1600个样本的高质量合成数据集（借助GPT - 4o完成问题生成与答案验证）；同时采用best - of - N采样策略，结合外部验证器来提升模型预测的可靠性与正确性，探索微调与推理时工具结合的潜力。  

### 📈 实验结果
在指令遵循任务上，DPO相比SFT在全参数与LoRA配置下均能进一步提升效果；RLOO变体里，以DeBERTa为奖励模型的版本对齐分数最高。数学推理任务中，合成数据能小幅提升性能，而结合外部验证器的best - of - N采样带来显著增益，准确率超0.81，是SFT的两倍多。整体表明轻量级模型经有效微调与工具辅助可实现不错性能，奖励模型质量、采样响应多样性对RLOO很关键，外部验证器 + best - of - N采样为数学推理提准确率提供了低成本方案。

### 💬 可借鉴之处
对于想优化小模型下游任务性能的研究者与开发者，可借鉴多强化学习微调技术对比思路，明确不同技术在偏好对齐等场景的优劣；在特定领域（如数学推理）任务中，尝试合成数据增强与推理时工具（如外部验证器 + best - of - N采样）结合的方式，在计算资源受限下提升小模型表现；同时重视奖励模型选型与采样多样性等因素对RLOO类方法的影响，为轻量级语言模型适配多任务提供实践参考。

## dreamcs--geometry-aware-text-to-3d-generation-with-unpaired-3d-reward-supervision
### Abstract
While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.
### 🌟 论文解读 | DreamCS：无配对3D奖励监督下的几何感知文本到3D生成

### 📌 背景痛点/本文动机
文本到3D生成技术在数字漫画、游戏、电影和虚拟现实等领域展现出关键作用，但现有方法生成的3D资产往往难以契合人类偏好。当前3D内容的偏好对齐技术，通常依赖难收集的偏好配对多视角2D图像来训练2D奖励模型以指导3D生成，然而这种方式因固有2D偏差易产生几何伪影。同时，收集配对的偏好标记数据成本高、耗时且困难，且2D空间的反馈仅基于图像渲染而非3D结构，会导致如视角不一致、Janus脸问题等几何缺陷，限制了实际应用。因此，需要一种能提供几何级反馈且摆脱配对数据依赖的文本到3D生成框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建3D - MeshPref数据集
为解决配对偏好标记3D数据收集难的问题，构建了首个大规模无配对3D偏好数据集3D - MeshPref，包含14000 + 样本，每个样本有文本提示、3D资产及其偏好奖励分数。从Cap3D筛选多样高质量网格，用Llama - Mesh对几何保真度、语义对齐和结构合理性评分，再经人工验证优化分数，将高奖励3D资产选为偏好样本，低奖励选为非偏好样本。

💡 创新点2：提出RewardCS奖励模型
针对无配对数据训练3D奖励模型的难题，提出首个基于无配对数据训练的3D几何感知奖励模型RewardCS。引入基于柯西 - 施瓦茨（CS）散度的分布级训练目标，将偏好和非偏好资产的嵌入视为两个分布的样本，优化它们之间的CS散度，使模型给几何和语义更优的3D资产更高奖励。还证明了无配对数据上的CS散度与传统配对偏好监督渐近等价，为无配对奖励学习范式提供理论依据。

💡 创新点3：提出DreamCS框架
现有文本到3D框架缺乏几何级反馈原生支持，为此开发DreamCS，首个集成RewardCS到现有文本到3D管线的3D奖励引导框架。通过三项创新实现与隐式和显式3D表示无缝协作：可微分网格化使隐式场到端到端梯度流成为可能；自适应网格融合在不牺牲几何细节的情况下优化拓扑以适配RewardCS；渐进式奖励引导自动平衡粗结构细化和细粒度优化。

### 📈 实验结果
在GPTEval3D基准测试中，对于MVDream、Dreamfusion、Magic3D等单阶段和两阶段文本到3D生成管线，集成DreamCS后在几何对齐和网格质量方面比之前多视角引导方法取得更优结果。且3D奖励引导与现有2D方法互补，结合后能进一步提升性能。

### 💬 可借鉴之处
1. 数据集构建思路：面对数据收集难题时，可考虑构建大规模无配对且结合大语言模型初筛与人工精修的数据集，为模型训练提供高质量数据支撑。
2. 模型训练目标创新：在处理无配对数据场景时，借鉴基于分布级的训练目标（如CS散度）思路，摆脱配对比较依赖，拓展模型在无配对数据下的学习能力。
3. 框架集成创新：针对领域内现有框架短板（如缺乏几何级反馈支持），通过多项针对性创新（可微分网格化、自适应网格融合、渐进式奖励引导等）实现新组件与现有管线的无缝集成，为类似领域内框架升级提供参考范式。

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### 🌟 论文解读 | Athena：用数据高效的过程奖励模型提升多模态推理能力

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）和多模态大语言模型（MLLMs）在自然语言处理和多模态任务中取得显著进展，但解决复杂推理任务（如数学和多步骤推理）仍具挑战。为增强推理能力，测试时缩放（TTS）等方法被探索，其中过程奖励模型（PRMs）能为中间推理步骤提供细粒度反馈，性能更优且泛化性强。然而，PRMs 发展面临两大难题：一是获取带过程标签的高质量数据成本高（需大量人工标注或计算昂贵的自动化标注）；二是传统自动化标注（如蒙特卡洛估计）易产生噪声标签。本文旨在解决这些挑战，降低计算成本并减轻标签噪声问题，提升 PRMs 性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：利用强弱完成器预测一致性生成高质量过程标签  
传统蒙特卡洛等自动化标注方法易受完成器推理能力影响，标签有噪声且计算成本高。本文发现，强完成器即便中间步骤错误仍能得到正确答案，弱完成器则可能在中间步骤正确时也失败。基于此，提出用弱、强完成器预测一致性作为筛选可靠过程标签的标准，保留两者标签一致的步骤，减少完成器带来的偏差，提升标签质量。实验表明，约 5000 条高质量标签就能比传统方法约 30 万条大规模标注数据表现更优，且大幅降低数据合成和模型训练的计算成本。

💡 创新点2：提升 PRMs 性能的两大策略  
 - ORM 初始化：PRMs 通常基于预训练基础模型微调，而结果奖励模型（ORMs）在大规模响应级数据上训练，具备弱监督下评估中间步骤正确性的能力。因此用 ORMs 初始化 PRMs，将 ORMs 作为弱监督预训练，PRMs 再在高质量细粒度步骤数据上微调，显著提升性能。  
 - 负样本上采样：过程标签数据存在标签不平衡问题，通过对含负步骤标签的数据进行上采样，解决数据分布不均问题，优化模型训练。  

💡 创新点3：构建 Athena 系列模型并多场景验证  
基于上述方法构建结果奖励模型 Athena - ORM 和过程奖励模型 Athena - PRM，再利用 Athena - PRM 通过奖励排序微调得到 Athena - 7B。并在三个场景验证：测试时缩放（TTS）中对策略模型生成的多个输出排序；直接评估推理步骤正确性；奖励排序微调（用高奖励响应微调策略模型）。

### 📈 实验结果
- 测试时缩放场景：在 7 个多模态数学和推理基准测试中，用 Athena - PRM 配合不同规模（7B 到 72B）策略模型，推理能力显著提升。如用 Qwen2.5 - VL - 7B 作为策略模型时，在 WeMath 基准上零样本基线提升 10.2 分，在 MathVista 提升 7.1 分；在文本-only 数学基准用 Mistral - 8B 时提升 8.9 分。  
- 推理步骤正确性评估场景：在 VisualProcessBench 基准上，Athena - PRM 表现强劲，超越开源的 VisualPRM - 8B 等模型，F1 分数比之前最优结果高 3.9，展现准确评估推理步骤正确性的能力。  
- 奖励排序微调场景：基于 Qwen2.5 - VL - 7B 微调得到的 Athena - 7B，在 7 个数学和推理基准上大幅提升策略模型推理能力。  

### 💬 可借鉴之处
- 数据高效标注思路：利用多完成器预测一致性筛选标签，为解决需细粒度标注且标注成本高的任务提供了新范式，在减少数据量同时提升数据质量，实现数据高效利用。  
- 模型训练策略：ORM 初始化和负样本上采样策略，为提升奖励模型性能提供了可复用方法，可启发其他奖励模型或需细粒度反馈模型的训练优化。  
- 多场景验证模式：在测试时缩放、步骤评估、模型微调等多场景验证方法有效性，这种全面验证思路有助于更充分展示方法价值，为后续研究提供验证范式参考。

## gfriend--generative-few-shot-reward-inference-through-efficient-dpo
### Abstract
The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.
### 🌟 论文解读 | GFRIEND：小样本下高效DPO实现生成式奖励推理

### 📌 背景痛点/本文动机
大语言模型（LLMs）在自然语言处理任务中取得显著成果，但让模型与人类价值观和偏好对齐仍是挑战。基于人类反馈的强化学习（RLHF）是关键方法，其核心是用人类标注的偏好数据训练奖励模型。然而现有方法存在样本配对低效、偏好数据多样性有限等问题，在医疗、法律等专业领域，大规模偏好数据收集受限，低数据环境下训练奖励模型成难题，因此需要高效利用有限偏好数据的方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：高效数据增强  
引入思维链（Chain - of - Thought, CoT）采样机制生成多样且高质量的偏好数据，缓解数据稀疏性问题，提升低资源条件下奖励模型的鲁棒性。通过CoT采样挖掘丰富优质的偏好关系，为奖励模型训练提供更充足且优质的数据基础。

💡 创新点2：多水平偏好建模  
提出基于困惑度的评分方法来分配细致的偏好等级，相较于传统的二元配对方法，能实现更细粒度的奖励模型训练。利用该评分机制量化偏好程度，让模型捕捉样本间更细微的偏好差异。

💡 创新点3：优化偏好学习  
通过结合基于偏好差异的加权样本对来增强直接偏好优化（DPO）损失函数，在低数据设置下提升模型的泛化能力和稳定性。依据偏好差异对样本对加权，确保训练时更关注有代表性的数据，优化奖励模型训练过程。

### 📈 实验结果
在多个基准测试中评估该框架，与传统奖励建模方法对比。实验证实数据增强提高了偏好建模准确性，多水平偏好评分有效。消融实验揭示各组件的贡献，结果表明在低资源设置下性能提升显著，能与在大规模数据集上训练的模型性能相当，凸显了数据高效策略在推进RLHF优化方面的潜力。

### 💬 可借鉴之处
在数据利用层面，其高效数据增强思路为低资源场景下的数据扩充提供了新方式，借助CoT采样生成优质数据的方法可被借鉴到其他需要数据增强的任务中；在模型训练层面，多水平偏好建模和优化损失函数的思路，为提升模型对细粒度差异的捕捉能力、增强模型在低数据下的泛化性提供了参考，对于资源受限但需高精度模型训练的场景有很好的借鉴价值，比如小众领域的AI应用开发等。

## saffron-1--safety-inference-scaling
### Abstract
Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .
### 🌟 论文解读 | Saffron-1：安全推理缩放，筑牢大模型安全防线

### 📌 背景痛点/本文动机
大语言模型（LLMs）快速发展与广泛应用带来了新安全风险，有害输出在实际应用中后果严重。现有安全保障研究多聚焦训练阶段对齐以植入安全行为，但近期研究暴露其易受越狱攻击的弱点。同时，推理缩放大幅提升了LLM推理能力，却在安全保障领域未被探索。为填补这一空白，本文探索推理缩放作为应对新威胁、建立更强安全保障的新方向，核心问题是“推理缩放在LLM安全保障中能有多强？” 且现有先进推理缩放方法用于安全保障时，缩放效率甚至不如最基础的Best - of - N采样，原因是存在“探索 - 效率困境”——频繁调用过程奖励模型（PRM）带来高计算开销，探索越多奖励模型调用越多，缩放效率越差。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出SAFFRON推理缩放范式
为解决“探索 - 效率困境”，提出首个针对LLM安全保障的推理缩放范式SAFFRON。核心是引入多分支奖励模型（MRM）替代过程奖励模型（PRM），大幅减少奖励模型调用次数。如传统PRM在树搜索步骤中需对每个候选next - token对应的序列多次调用PRM，而MRM只需对当前序列调用一次，就能得到各next - token对应的奖励值，减少计算开销。

💡 创新点2：MRM训练与辅助策略
 - 提出MRM的部分监督训练目标，用于训练多分支奖励模型，使其能有效为多个候选分支提供奖励评估。
 - 引入保守探索约束，防止分布外探索，确保模型探索在安全相关的合理范围内，避免因无意义探索增加开销与风险。
 - 设计基于Trie的键值（KV）缓存策略，在树搜索过程中实现序列间的KV缓存共享，提升计算效率。

### 📈 实验结果
大量实验验证了方法的有效性。在安全保障场景下，现有先进推理缩放方法（如DeAL（MCTS）、Rebase（Beam Search））缩放效率不如基础的Best - of - N采样；而提出的Saffron - 1方法显著优于Best - of - N，即便Best - of - N配备了本文的Trie - based KV缓存，Saffron - 1仍有大幅领先，在应对高难度越狱攻击时表现出色。

### 💬 可借鉴之处
 - 思路创新：将推理缩放引入安全保障领域，为LLM安全研究开辟新方向，启发研究者关注推理阶段的安全强化。
 - 技术贡献：提出的SAFFRON范式及MRM、训练目标、探索约束、缓存策略等技术手段，为解决安全推理中的效率与探索平衡问题提供了可行方案，后续研究可在此基础上优化或拓展应用。
 - 资源开放：公开了训练好的多分支奖励模型Saffron - 1和token级安全奖励数据集Safety4M，为该领域后续研究提供了数据和模型基础，加速LLM安全研究进程。

## preference-learning-for-ai-alignment--a-causal-perspective
### Abstract
Reward modelling from preference data is a crucial step in aligning large
language models (LLMs) with human values, requiring robust generalisation to
novel prompt-response pairs. In this work, we propose to frame this problem in
a causal paradigm, providing the rich toolbox of causality to identify the
persistent challenges, such as causal misidentification, preference
heterogeneity, and confounding due to user-specific factors. Inheriting from
the literature of causal inference, we identify key assumptions necessary for
reliable generalisation and contrast them with common data collection
practices. We illustrate failure modes of naive reward models and demonstrate
how causally-inspired approaches can improve model robustness. Finally, we
outline desiderata for future research and practices, advocating targeted
interventions to address inherent limitations of observational data.
### 🌟 论文解读 | 从因果视角看AI对齐中的偏好学习：挑战与新方向

### 📌 背景痛点/本文动机
大语言模型（LLMs）对齐人类价值观的关键环节是基于偏好数据的奖励建模，其需要对新的提示 - 响应对实现鲁棒泛化。但当前简单依赖基于观测数据集拟合的回归模型，存在泛化能力不足的问题，因为这类模型易学习虚假相关性而非影响用户偏好的真实因素。同时，成对偏好数据集收集受LLM采样分布、响应潜在特征和用户特定上下文等因素影响，使现有方法的鲁棒性和可靠性存疑。因此，论文主张从因果范式重新审视偏好学习问题，借助因果工具解决因果误识别、偏好异质性和用户特定因素混淆等挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：因果范式下的偏好学习框架构建  
将偏好学习置于因果范式中，把观测到的（提示、响应、偏好标签）元组视为分配给人类标注者的“处理（treatment）”，偏好标签视为“结果（outcome）”，引入潜在结果框架到Bradley - Terry - Luce（BTL）奖励模型中，定义潜在奖励概念，建立潜在奖励差异与预期潜在结果的直接联系，以此明确偏好学习中因果关系的形式化表达，为分析泛化性等问题提供基础。  
💡 创新点2：关键假设识别与实践对比  
从因果推断文献出发，识别奖励模型可靠泛化所需的关键假设，并将这些假设与常见的数据收集实践相对比。通过这种方式，揭示现有数据收集和模型训练中可能存在的与因果假设相悖之处，为改进方法指明方向。  
💡 创新点3：因果启发方法提升模型鲁棒性  
阐述朴素奖励模型在因果假设被违反时的失败模式，如因用户特定目标导致的混淆等（论文首次明确识别和解决该问题）。同时展示因果表示学习等因果启发方法如何改善模型鲁棒性，为构建更可靠的奖励模型提供技术路径。

### 📈 实验结果
论文通过实例和真实世界实验，展现了朴素模型在因果假设被违反时（如用户特定目标引发的混淆情况）的失效模式，同时验证了因果启发方法在提升模型对未见文本和场景泛化能力、增强鲁棒性方面的效果，直观地呈现出因果视角下偏好学习方法改进的必要性与有效性。

### 💬 可借鉴之处
论文为AI对齐中偏好学习的未来研究和实践勾勒了愿景，倡导有针对性的干预措施以解决观测数据的固有局限性。在方法层面，引入因果框架为处理偏好学习中的复杂因素（如用户异质性、虚假关联等）提供了新的分析维度与工具；在数据收集和模型设计层面，识别关键假设并对比实践，能指导研究者优化数据采集流程、改进模型结构以满足可靠泛化要求；在问题认知层面，让学界更清晰认识到当前偏好学习面临的根本挑战（如因果误识别等），推动领域向更鲁棒的技术方向发展。

## customizing-speech-recognition-model-with-large-language-model-feedback
### Abstract
Automatic speech recognition (ASR) systems have achieved strong performance
on general transcription tasks. However, they continue to struggle with
recognizing rare named entities and adapting to domain mismatches. In contrast,
large language models (LLMs), trained on massive internet-scale datasets, are
often more effective across a wide range of domains. In this work, we propose a
reinforcement learning based approach for unsupervised domain adaptation,
leveraging unlabeled data to enhance transcription quality, particularly the
named entities affected by domain mismatch, through feedback from a LLM. Given
contextual information, our framework employs a LLM as the reward model to
score the hypotheses from the ASR model. These scores serve as reward signals
to fine-tune the ASR model via reinforcement learning. Our method achieves a
21\% improvement on entity word error rate over conventional self-training
methods.
### 🌟 论文解读 | 用大语言模型反馈定制语音识别模型：领域适配新范式

### 📌 背景痛点/本文动机
自动语音识别（ASR）系统在通用转录任务上已取得不错成绩，但面对罕见命名实体识别与领域不匹配场景时表现不佳。传统解决领域适配的自训练方法依赖伪标签置信度分数，而这些分数在新领域中可靠性不足。同时，大语言模型（LLM）经大规模互联网数据训练，具备跨领域泛化能力，为ASR在领域适配难题上提供了新突破口。因此，本文旨在利用LLM反馈，通过强化学习实现ASR的无监督领域适配，提升领域不匹配场景下的转录质量，尤其是命名实体的识别效果。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于强化学习的无监督领域适配框架  
提出一套结合LLM反馈的强化学习（RL）流程来实现ASR无监督领域适配。框架分为数据收集、生成奖励、模型微调三步：先让预训练ASR模型为目标域音频生成候选假设；再结合上下文信息（如领域元数据等），用LLM计算各假设的奖励分数；最后用RL算法依据奖励微调ASR模型，摆脱了传统自训练对不可靠置信度分数的依赖。  

💡 创新点2：利用LLM构建奖励函数  
不单独训练ASR专用奖励模型，而是借助LLM对语言和上下文的隐式理解来生成奖励。将领域上下文信息整合为提示词输入LLM，通过计算假设在LLM下的对数概率和（结合ASR模型自身分数，通过参数λ控制权重）作为奖励信号，让LLM的跨域泛化能力为ASR领域适配提供指导。  

💡 创新点3：RL算法驱动ASR微调  
探索多种强化学习算法用于ASR模型微调，利用LLM给出的奖励信号，让ASR模型在无监督场景下朝着更契合目标领域转录质量的方向更新，区别于传统仅依赖词错误率（WER）或人工反馈设计奖励的思路，把LLM反馈深度融入训练过程。  

### 📈 实验结果
实验聚焦领域不匹配场景下命名实体等转录质量提升。对比传统自训练领域适配方法，本文方法在实体词错误率（EWER）上实现了高达21%的相对提升，有力证明了LLM反馈在引导ASR模型适配过程中的有效性，验证了基于RL和LLM反馈的无监督领域适配框架的价值。  

### 💬 可借鉴之处
1. 跨模态协作思路：展示了ASR与LLM结合的新路径，不再局限于LLM在ASR后处理（重排序、纠错）环节，而是让LLM深度参与训练阶段，为多模态模型协作提供了新范式参考。  
2. 无监督适配方案：针对领域适配中标签成本高的问题，提供了利用无标签数据+LLM反馈+强化学习的高效无监督适配方案，在低资源、领域差异大的场景下有推广潜力。  
3. 奖励设计创新：借助LLM天然的语言理解和跨域能力构建奖励函数，避免了传统自训练置信度分数不可靠的缺陷，为后续模型评估与奖励机制设计打开新思路。

## flattery--fluff--and-fog--diagnosing-and-mitigating-idiosyncratic-biases-in-preference-models
### Abstract
Language models serve as proxies for human preference judgements in alignment
and evaluation, yet they exhibit systematic miscalibration, prioritizing
superficial patterns over substantive qualities. This bias manifests as
overreliance on features like length, structure, and style, leading to issues
like reward hacking and unreliable evaluations. Evidence suggests these biases
originate in artifacts in human training data. In this work, we systematically
investigate the relationship between training data biases and preference model
miscalibration across five idiosyncratic features of language model
generations: length, structure, jargon, sycophancy and vagueness. Using
controlled counterfactual pairs, we first quantify the extent to which
preference models favor responses with magnified biases (skew), finding this
preference occurs in >60% of instances, and model preferences show high
miscalibration (~40%) compared to human preferences. Notably, bias features
only show mild negative correlations to human preference labels (mean r_human =
-0.12) but show moderately strong positive correlations with labels from a
strong reward model (mean r_model = +0.36), suggesting that models may overrely
on spurious cues. To mitigate these issues, we propose a simple post-training
method based on counterfactual data augmentation (CDA) using synthesized
contrastive examples. Finetuning models with CDA reduces average miscalibration
from 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%,
while maintaining overall RewardBench performance, showing that targeted
debiasing is effective for building reliable preference models.
### 🌟 论文解读 | 偏好模型中“奉承、冗长与模糊”：诊断并缓解特殊偏差

### 📌 背景痛点/本文动机
语言模型常被用作人类偏好判断的代理，在对齐和评估任务中发挥作用，但存在系统性校准偏差，会优先关注长度、结构、风格等表面模式而非实质质量，引发奖励黑客、评估不可靠等问题。已有证据表明这些偏差源于人类训练数据中的伪影，而现有研究多孤立记录单个偏差，缺乏对训练数据伪影如何在多偏差维度转化为模型校准偏差的量化分析。因此，本文旨在系统探究训练数据偏差与偏好模型校准偏差的关系。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：系统研究多偏差维度与模型校准偏差关系  
聚焦语言模型生成文本中常见的五个特殊偏差特征：长度（冗长性）、结构（如列表格式）、行话（过度技术化语言）、奉承（过度迎合用户）和模糊性（缺乏特异性）。通过构建反事实响应对（在保留其他有意义特征的同时放大目标偏差特征），量化偏好模型对有偏差响应的偏好倾斜度（skew）以及模型与人类偏好的校准偏差率（miscalibration rate）。  

💡 创新点2：提出基于反事实数据增强（CDA）的后训练去偏方法  
为缓解偏差问题，提出利用合成对比示例的反事实数据增强方法。针对每个偏差特征，在现有偏好数据集上扩充“翻转对”（即明确让被扰动后放大偏差的响应成为不被偏好的样本），用这类数据微调奖励模型，以惩罚有偏差的偏好。  

### 📈 实验结果
1. 偏好倾斜与校准偏差层面：模型对有偏差响应存在显著偏好（如对结构化响应偏好达89.5%、对冗长响应偏好达60.1%），模糊性和行话偏差对应的校准偏差率超50%；整体上，模型偏好与人类多数偏好在39.4%的评估中存在冲突，校准偏差高。  
2. 训练数据关联层面：分析主流奖励模型训练数据，发现人类选择和拒绝的响应中偏差存在明显不平衡，且偏差特征与人类偏好弱相关（如长度与人类偏好相关系数r=-0.09）却与训练后模型偏好强相关（r=0.67），说明标准RLHF pipeline会无意放大数据伪影为错位偏好信号。  
3. 去偏效果层面：用CDA微调模型后，平均校准偏差从39.4%降至32.5%，平均绝对偏好倾斜差从20.5%降至10.0%，同时在RewardBench上保持竞争力，证明针对性去偏对构建可靠偏好模型有效。  

### 💬 可借鉴之处
1. 多维度偏差分析思路：从长度、结构等多个实际易出现的偏差维度切入，系统量化模型行为与人类偏好的差异，为理解偏好模型偏差提供了全面视角，后续研究可借鉴这种多维度拆解分析的方式。  
2. 反事实数据增强去偏：提出的CDA方法简单且有效，通过构造对比示例扩充数据来引导模型修正偏差，为后训练阶段优化模型偏好判断提供了可行范式，在处理类似“模型过度依赖表面特征”问题时可参考该数据增强+微调的思路。  
3. 训练数据与模型偏差关联分析：揭示训练数据中偏差伪影如何影响模型，强调了数据层面分析对理解和改进模型行为的重要性，提醒后续在构建偏好模型数据时需关注偏差平衡与特征相关性。

## a-smooth-sea-never-made-a-skilled-$\texttt{sailor}$--robust-imitation-via-learning-to-search
### Abstract
The fundamental limitation of the behavioral cloning (BC) approach to
imitation learning is that it only teaches an agent what the expert did at
states the expert visited. This means that when a BC agent makes a mistake
which takes them out of the support of the demonstrations, they often don't
know how to recover from it. In this sense, BC is akin to giving the agent the
fish -- giving them dense supervision across a narrow set of states -- rather
than teaching them to fish: to be able to reason independently about achieving
the expert's outcome even when faced with unseen situations at test-time. In
response, we explore learning to search (L2S) from expert demonstrations, i.e.
learning the components required to, at test time, plan to match expert
outcomes, even after making a mistake. These include (1) a world model and (2)
a reward model. We carefully ablate the set of algorithmic and design decisions
required to combine these and other components for stable and
sample/interaction-efficient learning of recovery behavior without additional
human corrections. Across a dozen visual manipulation tasks from three
benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs
state-of-the-art Diffusion Policies trained via BC on the same data.
Furthermore, scaling up the amount of demonstrations used for BC by
5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can
identify nuanced failures and is robust to reward hacking. Our code is
available at https://github.com/arnavkj1995/SAILOR .
### 🌟 论文解读 | 从“授鱼”到“授渔”：SAILOR让智能体学会从错误中自主恢复

### 📌 背景痛点/本文动机
在模仿学习领域，行为克隆（Behavioral Cloning，BC）是常用方法，它让智能体学习专家在访问过的状态下的行为。但BC存在根本局限：一旦智能体犯错脱离演示数据的“支持域”（即遇到训练时没见过的状态），就不知如何恢复。这就像只给智能体“鱼”（在有限状态下密集监督），而非教它“捕鱼”（面对 unseen 情况时自主推理达成目标）。  

语言建模领域的经验也表明，仅靠“缩放版BC”（如单纯增大数据量训练更强大模型）不够，还需交互式学习（如RLHF）。机器人学习同理，即便用大量数据和强表达力模型，智能体仍可能因自身动作陷入未知状态，此时需自主恢复能力。而传统依赖人类在线纠错的方式难以规模化，因此本文希望让智能体在无额外人类反馈下，学会从自身错误中恢复。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出“学习搜索（Learning to Search, L2S）”范式  
不再只从专家演示学策略，而是学习**世界模型（World Model, WM）**和**奖励模型（Reward Model, RM）**。世界模型用于预测动作后果，奖励模型用于评估结果优劣。结合规划算法后，测试时智能体能基于学到的 WM 和 RM，推理如何从基础策略（如Diffusion Policy）的错误中恢复，实现“从专家演示中学习搜索”，而非单纯模仿。  

💡 创新点2：SAILOR架构实现L2S  
针对长 horizon 视觉操作任务，实例化 L2S 范式：用基础Diffusion Policies、Dreamer World Models 和 MPPI Planner 构建名为 SAILOR（Searching Across Imagined Latents Online for Recovery）的复合架构。它学习“残差规划器”，测试时执行局部搜索来修正基础策略的错误。  

💡 创新点3：关键设计决策的消融与验证  
为让组件稳定且样本高效地学习，验证了三类关键设计：  
- “热启动”混合世界模型训练阶段；  
- 在线微调混合世界模型；  
- 周期性将学习到的搜索算法蒸馏到基础策略（类似 expert iteration 或 Guided Policy Search）。  


### 📈 实验结果
1. 性能超越SOTA：在三个基准的十余项视觉操作任务中，SAILOR 持续超越同数据下用 BC 训练的最先进 Diffusion Policies。即便把 BC 用的演示数据量扩大 5 - 10 倍，仍存在性能差距。  
2. 交互效率更高：比传统无模型逆 RL 方法（如用 DPPO 更新策略参数）的交互效率显著更高。  
3. 鲁棒性与失效识别：学到的奖励模型能识别长 horizon 操作任务不同阶段的细微失效；整个 SAILOR 架构对“奖励黑客攻击（reward hacking）”鲁棒。  


### 💬 可借鉴之处
1. 范式层面：从“授鱼”到“授渔”的思路。在模仿学习中，不应仅聚焦“复刻专家行为”，更要赋予智能体自主推理、从错误恢复的能力，这为解决 BC 固有缺陷提供了新方向。  
2. 技术实现：对世界模型、奖励模型与规划算法的结合方式，以及热启动、在线微调、蒸馏等工程化技巧的验证，为后续构建鲁棒模仿学习系统提供了可复用的设计参考。  
3. 实验视角：在多任务、多数据规模下对比 SOTA 方法，并验证鲁棒性等属性，这种全面的实验设计思路值得借鉴，能更充分地展现方法价值。

## rewardanything--generalizable-principle-following-reward-models
### Abstract
Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.
### 🌟 论文解读 | RewardAnything：让奖励模型突破固定偏好，灵活遵循自然语言原则

### 📌 背景痛点/本文动机
大语言模型（LLMs）对齐人类偏好是关键挑战，奖励模型（RMs）作为RLHF等对齐技术的核心，当前训练方式存在两大瓶颈：一是泛化与适应性受限，依赖固定偏好数据集训练的RMs难以适配不同真实场景（如客服需简洁、科研助手需详细），重新收集数据再训练成本高；二是隐式偏好学习带来偏差与可解释性难题，现有RMs从人类标注偏好数据学习，多仅保留结果监督，易让模型通过虚假关联推断隐式价值，导致奖励偏差且可解释性差。为解决这些问题，论文提出转向**遵循原则的奖励模型**，让RMs能基于动态自然语言原则调整奖励标准。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出“遵循原则的奖励模型”概念范式  
明确RMs应像LLMs遵循指令一样，理解并遵循动态提供的自然语言奖励原则描述，无需为每个偏好场景训练新模型，使RMs成为跨多样偏好场景泛化的灵活工具，为构建适配多变人类偏好的AI系统筑牢能力基础。

💡 创新点2：构建RABench基准测试集  
打造全面的RMs基准RABench，聚焦评估RMs对新颖自然语言原则的泛化能力，覆盖多领域、暴露当前RMs局限，为衡量“遵循原则”能力进展提供标尺，填补领域内系统评估空白。

💡 创新点3：研发RewardAnything模型  
设计并训练RewardAnything这一生成式RM，结合GRPO与Group Relative Preference Learning技术，让模型能在推理时高效解读、应用多样偏好原则，实现对响应组的高效排序与打分，无需任务特定重训就能保持高质量偏好判断，还能高效支撑PPO、GRPO等RL训练。

### 📈 实验结果
在传统RM基准测试中，仅通过指定清晰原则，RewardAnything就能取得SOTA性能；在RABench上，展现出无需重训就能适配新原则的卓越能力；此外，案例研究验证其能仅以自然语言原则为指导自动高效对齐LLM，在细微安全、帮助性、响应质量等维度实现显著提升，有力证明“遵循原则”范式在LLM高效灵活对齐上的价值。

### 💬 可借鉴之处
1. 范式创新层面：将“遵循指令”思路迁移到奖励模型，提出“遵循自然语言原则”的新范式，为打破RMs固定偏好束缚、适配多样真实场景提供方向，启发后续对齐技术从“静态偏好拟合”转向“动态原则遵循”。  
2. 基准构建层面：RABench为评估RMs泛化到新原则的能力提供了标准化工具，推动领域内对RMs“灵活性”“通用性”的量化研究，后续可基于此持续迭代优化模型。  
3. 模型设计层面：RewardAnything结合特定训练方法实现推理时原则遵循，其技术路线（如GRPO结合、生成式RM设计）为打造灵活奖励模型提供了可参考的工程实践，助力后续高效RLHF流程搭建。

## densedpo--fine-grained-temporal-preference-optimization-for-video-diffusion-models
### Abstract
Direct Preference Optimization (DPO) has recently been applied as a
post-training technique for text-to-video diffusion models. To obtain training
data, annotators are asked to provide preferences between two videos generated
from independent noise. However, this approach prohibits fine-grained
comparisons, and we point out that it biases the annotators towards low-motion
clips as they often contain fewer visual artifacts. In this work, we introduce
DenseDPO, a method that addresses these shortcomings by making three
contributions. First, we create each video pair for DPO by denoising corrupted
copies of a ground truth video. This results in aligned pairs with similar
motion structures while differing in local details, effectively neutralizing
the motion bias. Second, we leverage the resulting temporal alignment to label
preferences on short segments rather than entire clips, yielding a denser and
more precise learning signal. With only one-third of the labeled data, DenseDPO
greatly improves motion generation over vanilla DPO, while matching it in text
alignment, visual quality, and temporal consistency. Finally, we show that
DenseDPO unlocks automatic preference annotation using off-the-shelf Vision
Language Models (VLMs): GPT accurately predicts segment-level preferences
similar to task-specifically fine-tuned video reward models, and DenseDPO
trained on these labels achieves performance close to using human labels.
### 🌟 论文解读 | DenseDPO：为视频扩散模型带来细粒度时序偏好优化

### 📌 背景痛点/本文动机
在文本到视频生成领域，扩散模型取得了不少进展，但现有视频生成器在时间一致性、视觉保真度和提示对齐等方面仍有不足。Direct Preference Optimization（DPO）技术被用于文本到视频扩散模型的后训练，但传统DPO方法存在缺陷：一是训练数据由独立噪声生成的视频对构建，难以进行细粒度比较；二是标注者易偏向低运动片段（这类片段视觉伪影少），导致模型在训练中强化对慢动作内容的偏向，抑制生成动态丰富视频的能力。为解决这些问题，论文提出DenseDPO方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：改进DPO视频对构建方式  
传统DPO用独立噪声生成视频对，而DenseDPO通过对真实视频的损坏副本去噪来创建视频对。这样得到的视频对具有相似运动结构但局部细节不同，能有效消除运动偏向，让视频对在语义和运动轨迹层面保持高一致性，减少无关关联干扰。  

💡 创新点2：细粒度时序偏好标注  
借助上述构建方式带来的时间对齐特性，DenseDPO不再对整个视频片段标注偏好，而是对短片段（如1秒切片）标注。这种方式能产生更密集、精确的学习信号，即便只用三分之一标注数据，也能在运动生成上远超 vanilla DPO，同时在文本对齐、视觉质量和时间一致性上与vanilla DPO持平。  

💡 创新点3：解锁基于通用视觉语言模型的自动偏好标注  
DenseDPO展示了利用现成视觉语言模型（VLMs）实现自动偏好标注的可能性。实验表明，GPT能精准预测片段级偏好，效果接近为特定任务微调的视频奖励模型；用这些自动标注数据训练的DenseDPO，性能接近使用人类标注数据的版本。  

### 📈 实验结果
在运动生成方面，DenseDPO仅用三分之一标注数据就大幅超越vanilla DPO；在文本对齐、视觉质量和时间一致性等指标上，能与vanilla DPO匹配。此外，基于通用VLMs的自动标注实验中，模型在片段级偏好预测表现良好，用自动标注训练的DenseDPO性能接近人类标注训练的模型，验证了自动标注方案的可行性。  

### 💬 可借鉴之处
1. 数据构建思路：当需减少数据偏差时，可参考“基于真实样本变体构建对比数据”的思路，让对比样本在关键结构（如视频运动结构）上保持一致，只在细节区分，以削弱无关偏向。  
2. 细粒度监督：对于有时间、空间等维度的生成任务（如视频、长文本等），可尝试将整体监督拆解为细粒度单元（如视频短片段、文本段落）的监督，提升学习信号密度与精度。  
3. 自动标注探索：利用现有通用大模型（如VLMs）做领域内偏好等信号的自动标注，能降低人力成本，为数据稀缺场景提供解决方案，该思路可迁移到其他需人工标注反馈的任务中。

## autocircuit-rl--reinforcement-learning-driven-llm-for-automated-circuit-topology-generation
### Abstract
Analog circuit topology synthesis is integral to Electronic Design Automation
(EDA), enabling the automated creation of circuit structures tailored to
specific design requirements. However, the vast design search space and strict
constraint adherence make efficient synthesis challenging. Leveraging the
versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel
reinforcement learning (RL)-based framework for automated analog circuit
synthesis. The framework operates in two phases: instruction tuning, where an
LLM learns to generate circuit topologies from structured prompts encoding
design constraints, and RL refinement, which further improves the
instruction-tuned model using reward models that evaluate validity, efficiency,
and output voltage. The refined model is then used directly to generate
topologies that satisfy the design constraints. Empirical results show that
AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by
~14% compared to the best baselines, while reducing duplicate generation rates
by ~38%. It achieves over 60% success in synthesizing valid circuits with
limited training data, demonstrating strong generalization. These findings
highlight the framework's effectiveness in scaling to complex circuits while
maintaining efficiency and constraint adherence, marking a significant
advancement in AI-driven circuit design.
### 🌟 论文解读 | AUTOCIRCUIT - RL：用强化学习驱动大模型，革新模拟电路拓扑生成

### 📌 背景痛点/本文动机
模拟电路拓扑合成是电子设计自动化（EDA）的关键部分，能依据特定设计需求自动生成电路结构。然而，设计搜索空间极为庞大，且要严格遵循约束条件，这让高效合成电路变得极具挑战。传统方法中，基于搜索的AI算法在扩展性、效率和适应性方面存在不足；生成式AI方法里的大语言模型（LLM）相关研究，在规模、灵活性与迭代优化上也有局限，像处理复杂拓扑、多目标约束优化等场景时表现不佳。在此背景下，论文提出AUTOCIRCUIT - RL框架来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：两阶段训练框架
AUTOCIRCUIT - RL采用两阶段训练模式。第一阶段是指令调优，让LLM从编码设计约束的结构化提示中学习生成电路拓扑，使模型初步具备根据约束生成拓扑的能力；第二阶段是强化学习（RL）优化，利用评估有效性、效率和输出电压的奖励模型，对经过指令调优的模型进一步改进，让模型生成的电路在满足约束的同时性能更优，且RL优化仅在训练时进行，推理阶段不涉及。

💡 创新点2：聚焦约束驱动与多目标优化
该框架以约束驱动设计为目标，针对模拟电路合成打造RL框架。在优化过程中兼顾有效性、效率、输出电压等多目标，借助AI奖励模型引导模型迭代，实现了在拓扑和性能指标上的同时优化，突破了以往方法在多目标约束下的局限，能更好地应对实际设计中复杂的约束要求。

### 📈 实验结果
实验结果显示，与最优基线相比，AUTOCIRCUIT - RL生成有效电路的比例提升约12%，效率提升约14%，重复生成率降低约38%；在训练数据有限的情况下，合成有效电路的成功率超过60%，展现出强泛化能力；还能支持多达10个组件的电路生成，在少样本微调下可泛化到6 - 10个组件的电路场景，体现了良好的扩展性与适应性。

### 💬 可借鉴之处
从方法设计角度，两阶段训练结合指令调优与RL优化的思路，为利用大模型处理复杂任务、实现多目标优化提供了参考范式，可迁移到其他需约束驱动和迭代优化的生成类任务中；从应用角度，该框架在电路设计领域成功突破传统方法局限，证明了AI驱动电路设计在扩展性、效率和约束遵循上的潜力，为EDA领域乃至其他工程设计领域借助大模型与强化学习技术实现自动化、智能化设计提供了优秀范例，启发研究者思考如何将大模型与强化学习结合，解决专业领域中搜索空间大、约束复杂的设计问题。

## badreward--clean-label-poisoning-of-reward-models-in-text-to-image-rlhf
### Abstract
Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.
### 🌟 论文解读 | BadReward：多模态文本到图像RLHF中奖励模型的“清洁标签投毒”攻击

### 📌 背景痛点/本文动机
文本到图像（T2I）模型近年发展迅猛，强化学习从人类反馈（RLHF）在让模型与人类偏好对齐上至关重要。但RLHF的反馈机制也给攻击者留下可乘之机。此前针对单模态（文本）的投毒技术存在依赖脏标签或易被检测等问题，且多模态T2I领域的RLHF阶段投毒研究较少。同时，现有奖励投毒方法常需控制偏好标注流程，在现实场景难实现。因此，本文聚焦多模态RLHF中奖励模型，提出更隐蔽、无需控制标注流程的投毒攻击方法BadReward，探索多模态系统下RLHF面临的新威胁。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出BadReward清洁标签投毒攻击  
针对多模态RLHF中的奖励模型，设计无需控制偏好标注流程的投毒攻击。区别于依赖脏标签或明显对抗内容的传统方式，BadReward用“清洁标签”（不篡改偏好标签本身）实现投毒，提升攻击隐蔽性与现实威胁性，能在注入少量自然样例时误导奖励模型，进而让T2I模型生成有害或不当输出。  

💡 创新点2：视觉特征碰撞策略  
通过在嵌入空间中诱导视觉上矛盾的偏好数据实例间发生特征碰撞，来破坏奖励模型训练。不直接修改偏好标签，而是操纵特征表示，以此腐蚀奖励信号，让攻击更隐蔽且具实用性，绕开对标注流程的控制需求，增强攻击可行性。  


### 📈 实验结果
在Stable Diffusion v1.4和SD Turbo等主流T2I模型上开展大量实验，验证BadReward能持续引导模型针对特定概念生成不当输出（如偏见、暴力图像等），展现了在不同模型架构与设置下的有效性、隐蔽性与可迁移性，有力证明多模态RLHF场景下该攻击带来的威胁。

### 💬 可借鉴之处
从安全研究角度，本文首次深入多模态T2I的RLHF阶段奖励模型投毒，提出的BadReward为该领域安全威胁分析提供新视角与方法参考，推动对多模态系统RLHF安全的研究；从防御角度，凸显多模态RLHF面临的放大威胁，为后续研发鲁棒防御机制敲响警钟，指引研究者关注奖励模型层面的隐蔽攻击并设计对应防护策略；从方法创新看，视觉特征碰撞思路为投毒攻击在多模态场景的隐蔽实施提供了新范式，启发后续在特征层面做更精细的攻击或防御探索。 

## smoothed-preference-optimization-via-renoise-inversion-for-aligning-diffusion-models-with-varied-human-preferences
### Abstract
Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation
models with human preferences using pairwise preference data. Although
substantial resources are expended in collecting and labeling datasets, a
critical aspect is often neglected: \textit{preferences vary across individuals
and should be represented with more granularity.} To address this, we propose
SmPO-Diffusion, a novel method for modeling preference distributions to improve
the DPO objective, along with a numerical upper bound estimation for the
diffusion optimization objective. First, we introduce a smoothed preference
distribution to replace the original binary distribution. We employ a reward
model to simulate human preferences and apply preference likelihood averaging
to improve the DPO loss, such that the loss function approaches zero when
preferences are similar. Furthermore, we utilize an inversion technique to
simulate the trajectory preference distribution of the diffusion model,
enabling more accurate alignment with the optimization objective. Our approach
effectively mitigates issues of excessive optimization and objective
misalignment present in existing methods through straightforward modifications.
Our SmPO-Diffusion achieves state-of-the-art performance in preference
evaluation, outperforming baselines across metrics with lower training costs.
The project page is https://jaydenlyh.github.io/SmPO-project-page/.
### 🌟 论文解读 | 用平滑偏好优化与ReNoise逆变换，让扩散模型精准对齐多样人类偏好

### 📌 背景痛点/本文动机
文本到图像（T2I）的扩散模型虽已广泛流行，但在文本渲染、空间布局、光照等方面仍存不足。当前基于人类反馈对齐模型的研究，一方面收集带偏好标注数据成本高且默认二元偏好分布，忽略了人类偏好的个体差异与细粒度；另一方面现有方法在建模偏好和优化目标估计上存在偏差，易导致过度优化、目标错位等问题。因此，论文旨在提出更适配人类多样偏好的扩散模型对齐方法，解决上述痛点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：平滑偏好建模（Smoothed Preference Modeling）  
考虑到“一千个人眼中有一千个哈姆雷特”式的人类偏好差异，论文用**平滑偏好分布**替代传统二元偏好分布。借助奖励模型自动生成平滑偏好，把偏好的平均似然融入DPO损失函数，当偏好相似时损失趋近于0，既减轻标签偏差，又降低人工标注成本、提升方法适应性，让模型更贴合真实多样的人类偏好。  

💡 创新点2：ReNoise逆变换优化（Optimization via ReNoise Inversion）  
为精准估计扩散模型的轨迹偏好分布，论文采用ReNoise逆变换方法来估计采样轨迹，替代过往基于前向过程的估计方式。这有效解决了优化目标与期望结果错位问题，让模型微调更高效，使扩散模型在优化时能更精准对齐人类偏好目标。  

### 📈 实验结果
论文将提出的SmPO - Diffusion与主流基线方法在人类偏好对齐任务上对比。在多个人类偏好评估指标上，SmPO - Diffusion表现超越现有方法；图像生成质量（如空间布局、文本渲染、光照等维度）有显著提升（文中图1、3、4展示示例）；训练效率也大幅提高，比Diffusion - KTO快26倍（表1数据支撑），实现了性能与效率的双重优势。  

### 💬 可借鉴之处
1. 偏好建模思路：面对人类偏好的多样性与模糊性，用“平滑化”思路替代非黑即白的二元假设，为处理带主观倾向的任务提供了更柔性的建模范式，可迁移到其他需考虑用户偏好差异的生成或对齐任务。  
2. 优化目标估计：通过逆变换方式改进扩散模型轨迹估计，启发我们在处理生成模型优化时，要关注过程层面的精准度，从采样轨迹等更细粒度环节优化对齐效果。  
3. 高效训练实践：在提升性能同时兼顾训练效率，为工业界落地需快速迭代的场景提供了参考，证明了方法在实用性上的潜力。

