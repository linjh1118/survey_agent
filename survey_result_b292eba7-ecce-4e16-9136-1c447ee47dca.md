# Paper List of Terms(Tool Learning)
- [25/12] **ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs**  
[[Paper](https://arxiv.org/pdf/2512.16149v1)] [[Code/Page](https://github.com/Buycar-arb/ToolForge)] [[TLDR/Notes](#toolforge--a-data-synthesis-pipeline-for-multi-hop-search-without-real-world-apis)]

- [25/12] **AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition**  
[[Paper](https://arxiv.org/pdf/2512.03794v1)] [[Code/Page]()] [[TLDR/Notes](#adaptvision--efficient-vision-language-models-via-adaptive-visual-acquisition)]

- [25/11] **The Empowerment of Science of Science by Large Language Models: New Tools and Methods**  
[[Paper](https://arxiv.org/pdf/2511.15370v1)] [[Code/Page]()] [[TLDR/Notes](#the-empowerment-of-science-of-science-by-large-language-models--new-tools-and-methods)]

- [25/11] **LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls**  
[[Paper](https://arxiv.org/pdf/2511.09148v2)] [[Code/Page]()] [[TLDR/Notes](#looptool--closing-the-data-training-loop-for-robust-llm-tool-calls)]

- [25/10] **One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning**  
[[Paper](https://arxiv.org/pdf/2510.26167v1)] [[Code/Page]()] [[TLDR/Notes](#one-model-to-critique-them-all--rewarding-agentic-tool-use-via-efficient-reasoning)]

- [25/09] **ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning**  
[[Paper](https://arxiv.org/pdf/2509.14718v1)] [[Code/Page]()] [[TLDR/Notes](#toolsample--dual-dynamic-sampling-methods-with-curriculum-learning-for-rl-based-tool-learning)]

- [25/08] **Provable Benefits of In-Tool Learning for Large Language Models**  
[[Paper](https://arxiv.org/pdf/2508.20755v1)] [[Code/Page]()] [[TLDR/Notes](#provable-benefits-of-in-tool-learning-for-large-language-models)]

- [25/08] **Beyond Self-Regulated Learning Processes: Unveiling Hidden Tactics in Generative AI-Assisted Writing**  
[[Paper](https://arxiv.org/pdf/2508.10310v1)] [[Code/Page]()] [[TLDR/Notes](#beyond-self-regulated-learning-processes--unveiling-hidden-tactics-in-generative-ai-assisted-writing)]

- [25/08] **LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval**  
[[Paper](https://arxiv.org/pdf/2508.07690v1)] [[Code/Page]()] [[TLDR/Notes](#losemb--logic-guided-semantic-bridging-for-inductive-tool-retrieval)]

- [25/08] **MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning**  
[[Paper](https://arxiv.org/pdf/2508.00271v2)] [[Code/Page](https://github.com/qhjqhj00/MetaAgent.)] [[TLDR/Notes](#metaagent--toward-self-evolving-agent-via-tool-meta-learning)]

- [25/06] **CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios**  
[[Paper](https://arxiv.org/pdf/2506.13977v1)] [[Code/Page](https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.)] [[TLDR/Notes](#critictool--evaluating-self-critique-capabilities-of-large-language-models-in-tool-calling-error-scenarios)]

- [25/06] **SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition**  
[[Paper](https://arxiv.org/pdf/2506.07557v1)] [[Code/Page](https://github.com/fairyshine/SELT)] [[TLDR/Notes](#selt--self-evaluation-tree-search-for-llms-with-task-decomposition)]

- [25/06] **CheMatAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning**  
[[Paper](https://arxiv.org/pdf/2506.07551v2)] [[Code/Page](https://github.com/AI4Chem/ChemistryAgent)] [[TLDR/Notes](#chematagent--enhancing-llms-for-chemistry-and-materials-science-through-tree-search-based-tool-learning)]

- [25/06] **Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists**  
[[Paper](https://arxiv.org/pdf/2506.00042v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-tool-learning-in-large-language-models-with-hierarchical-error-checklists)]

- [25/05] **MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning**  
[[Paper](https://arxiv.org/pdf/2505.20670v2)] [[Code/Page]()] [[TLDR/Notes](#mirror--multi-agent-intra--and-inter-reflection-for-optimized-reasoning-in-tool-learning)]

- [25/05] **TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation**  
[[Paper](https://arxiv.org/pdf/2505.20016v1)] [[Code/Page]()] [[TLDR/Notes](#ttpa--token-level-tool-use-preference-alignment-training-framework-with-fine-grained-evaluation)]

- [25/05] **RRTL: Red Teaming Reasoning Large Language Models in Tool Learning**  
[[Paper](https://arxiv.org/pdf/2505.17106v1)] [[Code/Page]()] [[TLDR/Notes](#rrtl--red-teaming-reasoning-large-language-models-in-tool-learning)]

- [25/05] **ToLeaP: Rethinking Development of Tool Learning with Large Language Models**  
[[Paper](https://arxiv.org/pdf/2505.11833v1)] [[Code/Page]()] [[TLDR/Notes](#toleap--rethinking-development-of-tool-learning-with-large-language-models)]

- [25/05] **OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning**  
[[Paper](https://arxiv.org/pdf/2505.08617v2)] [[Code/Page]()] [[TLDR/Notes](#openthinkimg--learning-to-think-with-images-via-visual-tool-reinforcement-learning)]

- [25/05] **ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution**  
[[Paper](https://arxiv.org/pdf/2505.07512v1)] [[Code/Page]()] [[TLDR/Notes](#toolace-dev--self-improving-tool-learning-via-decomposition-and-evolution)]

- [25/04] **ToolRL: Reward is All Tool Learning Needs**  
[[Paper](https://arxiv.org/pdf/2504.13958v1)] [[Code/Page]()] [[TLDR/Notes](#toolrl--reward-is-all-tool-learning-needs)]

- [25/04] **FamilyTool: A Multi-hop Personalized Tool Use Benchmark**  
[[Paper](https://arxiv.org/pdf/2504.06766v2)] [[Code/Page](https://github.com/yxzwang/FamilyTool}{https://github.com/yxzwang/FamilyTool}.)] [[TLDR/Notes](#familytool--a-multi-hop-personalized-tool-use-benchmark)]

- [25/04] **Select Me! When You Need a Tool: A Black-box Text Attack on Tool Selection**  
[[Paper](https://arxiv.org/pdf/2504.04809v1)] [[Code/Page]()] [[TLDR/Notes](#select-me!-when-you-need-a-tool--a-black-box-text-attack-on-tool-selection)]

- [25/04] **ToolACE-R: Model-aware Iterative Training and Adaptive Refinement for Tool Learning**  
[[Paper](https://arxiv.org/pdf/2504.01400v2)] [[Code/Page]()] [[TLDR/Notes](#toolace-r--model-aware-iterative-training-and-adaptive-refinement-for-tool-learning)]

- [25/03] **StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs**  
[[Paper](https://arxiv.org/pdf/2503.20527v1)] [[Code/Page]()] [[TLDR/Notes](#stabletoolbench-mirrorapi--modeling-tool-environments-as-mirrors-of-7-000+-real-world-apis)]

- [25/03] **Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models**  
[[Paper](https://arxiv.org/pdf/2503.16779v1)] [[Code/Page](https://github.com/fairyshine/Chain-of-Tools)] [[TLDR/Notes](#chain-of-tools--utilizing-massive-unseen-tools-in-the-cot-reasoning-of-frozen-language-models)]

- [25/03] **Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM**  
[[Paper](https://arxiv.org/pdf/2503.10071v1)] [[Code/Page]()] [[TLDR/Notes](#advanced-tool-learning-and-selection-system-(atlass)--a-closed-loop-framework-using-llm)]

- [25/03] **Alignment for Efficient Tool Calling of Large Language Models**  
[[Paper](https://arxiv.org/pdf/2503.06708v1)] [[Code/Page]()] [[TLDR/Notes](#alignment-for-efficient-tool-calling-of-large-language-models)]

- [25/03] **Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models**  
[[Paper](https://arxiv.org/pdf/2503.01763v2)] [[Code/Page]()] [[TLDR/Notes](#retrieval-models-aren-t-tool-savvy--benchmarking-tool-retrieval-for-large-language-models)]

- [25/03] **AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification**  
[[Paper](https://arxiv.org/pdf/2503.01940v2)] [[Code/Page]()] [[TLDR/Notes](#asktoact--enhancing-llms-tool-use-via-self-correcting-clarification)]

- [25/02] **PEToolLLM: Towards Personalized Tool Learning in Large Language Models**  
[[Paper](https://arxiv.org/pdf/2502.18980v1)] [[Code/Page]()] [[TLDR/Notes](#petoolllm--towards-personalized-tool-learning-in-large-language-models)]

- [25/02] **ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models**  
[[Paper](https://arxiv.org/pdf/2502.11404v2)] [[Code/Page]()] [[TLDR/Notes](#toolcoder--a-systematic-code-empowered-tool-learning-framework-for-large-language-models)]

- [25/02] **Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System**  
[[Paper](https://arxiv.org/pdf/2502.11358v1)] [[Code/Page]()] [[TLDR/Notes](#mimicking-the-familiar--dynamic-command-generation-for-information-theft-attacks-in-llm-tool-learning-system)]

- [25/02] **Tool Unlearning for Tool-Augmented LLMs**  
[[Paper](https://arxiv.org/pdf/2502.01083v2)] [[Code/Page]()] [[TLDR/Notes](#tool-unlearning-for-tool-augmented-llms)]

- [25/01] **Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation**  
[[Paper](https://arxiv.org/pdf/2501.12432v2)] [[Code/Page](https://corn0205.github.io/)] [[TLDR/Notes](#divide-then-aggregate--an-efficient-tool-learning-method-via-parallel-tool-invocation)]

- [24/12] **GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask Instruction**  
[[Paper](https://arxiv.org/pdf/2412.12152v1)] [[Code/Page](https://anonymous.4open.science/r/GraphTool-Instruction.)] [[TLDR/Notes](#graphtool-instruction--revolutionizing-graph-reasoning-in-llms-through-decomposed-subtask-instruction)]

- [24/12] **Federated In-Context LLM Agent Learning**  
[[Paper](https://arxiv.org/pdf/2412.08054v1)] [[Code/Page]()] [[TLDR/Notes](#federated-in-context-llm-agent-learning)]

- [24/12] **TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling Capability of LLM**  
[[Paper](https://arxiv.org/pdf/2412.03096v2)] [[Code/Page]()] [[TLDR/Notes](#tool-ed--enhancing-empathetic-response-generation-with-the-tool-calling-capability-of-llm)]

- [24/10] **Toolken+: Improving LLM Tool Usage with Reranking and a Reject Option**  
[[Paper](https://arxiv.org/pdf/2410.12004v1)] [[Code/Page]()] [[TLDR/Notes](#toolken+--improving-llm-tool-usage-with-reranking-and-a-reject-option)]

- [24/10] **NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models**  
[[Paper](https://arxiv.org/pdf/2410.11805v2)] [[Code/Page]()] [[TLDR/Notes](#nestools--a-dataset-for-evaluating-nested-tool-learning-abilities-of-large-language-models)]

- [24/10] **From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions**  
[[Paper](https://arxiv.org/pdf/2410.08197v2)] [[Code/Page]()] [[TLDR/Notes](#from-exploration-to-mastery--enabling-llms-to-master-tools-via-self-driven-interactions)]

- [24/10] **StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning**  
[[Paper](https://arxiv.org/pdf/2410.07745v4)] [[Code/Page](https://github.com/yuyq18/StepTool.)] [[TLDR/Notes](#steptool--enhancing-multi-step-tool-usage-in-llms-via-step-grained-reinforcement-learning)]

- [24/10] **Learning Evolving Tools for Large Language Models**  
[[Paper](https://arxiv.org/pdf/2410.06617v5)] [[Code/Page](https://github.com/Chen-GX/ToolEVO)] [[TLDR/Notes](#learning-evolving-tools-for-large-language-models)]

- [24/10] **ToolGen: Unified Tool Retrieval and Calling via Generation**  
[[Paper](https://arxiv.org/pdf/2410.03439v3)] [[Code/Page]()] [[TLDR/Notes](#toolgen--unified-tool-retrieval-and-calling-via-generation)]

- [24/09] **CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance**  
[[Paper](https://arxiv.org/pdf/2409.13202v2)] [[Code/Page]()] [[TLDR/Notes](#citi--enhancing-tool-utilizing-ability-in-large-language-models-without-sacrificing-general-performance)]

- [24/09] **ToolACE: Winning the Points of LLM Function Calling**  
[[Paper](https://arxiv.org/pdf/2409.00920v2)] [[Code/Page](https://huggingface.co/Team-ACE.)] [[TLDR/Notes](#toolace--winning-the-points-of-llm-function-calling)]

- [24/09] **Learning to Ask: When LLM Agents Meet Unclear Instruction**  
[[Paper](https://arxiv.org/pdf/2409.00557v3)] [[Code/Page]()] [[TLDR/Notes](#learning-to-ask--when-llm-agents-meet-unclear-instruction)]

- [24/07] **MetaTool: Facilitating Large Language Models to Master Tools with Meta-task Augmentation**  
[[Paper](https://arxiv.org/pdf/2407.12871v2)] [[Code/Page]()] [[TLDR/Notes](#metatool--facilitating-large-language-models-to-master-tools-with-meta-task-augmentation)]

- [24/07] **What Affects the Stability of Tool Learning? An Empirical Study on the Robustness of Tool Learning Frameworks**  
[[Paper](https://arxiv.org/pdf/2407.03007v1)] [[Code/Page]()] [[TLDR/Notes](#what-affects-the-stability-of-tool-learning--an-empirical-study-on-the-robustness-of-tool-learning-frameworks)]

- [24/07] **WTU-EVAL: A Whether-or-Not Tool Usage Evaluation Benchmark for Large Language Models**  
[[Paper](https://arxiv.org/pdf/2407.12823v1)] [[Code/Page]()] [[TLDR/Notes](#wtu-eval--a-whether-or-not-tool-usage-evaluation-benchmark-for-large-language-models)]



# TLDR/Notes
## toolforge--a-data-synthesis-pipeline-for-multi-hop-search-without-real-world-apis
### Abstract
Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolForgeï¼šæ— éœ€çœŸå®APIçš„å¤šè·³æœç´¢æ•°æ®åˆæˆæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·¥å…·è°ƒç”¨èƒ½åŠ›è®­ç»ƒé¢†åŸŸï¼Œé«˜è´¨é‡ä¸”å¤šæ ·çš„è®­ç»ƒæ•°æ®è‡³å…³é‡è¦ã€‚ç„¶è€Œç°æœ‰åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹å­˜åœ¨è¯¸å¤šä¸è¶³ï¼šä¸€æ–¹é¢ä¾èµ–å¤§é‡çœŸå®APIè°ƒç”¨ï¼ˆå¾€å¾€æ•°ä»¥ä¸‡è®¡ï¼‰æ¥æå‡æ³›åŒ–æ€§ï¼Œæˆæœ¬é«˜æ˜‚ï¼›å¦ä¸€æ–¹é¢ç¼ºä¹å¤šè·³æ¨ç†ä¸è‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ç°å®ä»»åŠ¡ã€‚åŒæ—¶ï¼ŒçœŸå®ä¸–ç•Œä»»åŠ¡å¸¸éœ€å¤šè·³æ¨ç†ï¼ˆé€šè¿‡å¤šä¸­é—´æ­¥éª¤å’Œé€»è¾‘é“¾æ¨å¯¼æœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œä½†ç°æœ‰å·¥ä½œå¤šèšç„¦æ–‡æœ¬å¤šè·³æ¨ç†ï¼Œæœªä¸å¤–éƒ¨å·¥å…·æœ‰æ•ˆæ•´åˆï¼›ä¸”æ•°æ®ä¿çœŸåº¦éªŒè¯ç¯èŠ‚è–„å¼±ï¼Œä»…å…³æ³¨è¯­æ³•æ­£ç¡®æ€§ç­‰è¡¨å±‚å†…å®¹ï¼Œå¿½ç•¥ä¸­é—´æ¨ç†æ­¥éª¤è¯­ä¹‰å’Œé€»è¾‘å®Œæ•´æ€§ã€‚è¿™äº›ç—›ç‚¹æ¨åŠ¨äº†ToolForgeçš„è¯ç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºToolForgeè‡ªåŠ¨åŒ–åˆæˆæ¡†æ¶  
ä»…éœ€ï¼ˆé—®é¢˜ã€é»„é‡‘ä¸Šä¸‹æ–‡ã€ç­”æ¡ˆï¼‰ä¸‰å…ƒç»„ï¼Œå°±èƒ½ç”Ÿæˆå¤§è§„æ¨¡å…·å¤‡å¤šè·³æ¨ç†å’Œè‡ªæˆ‘åæ€ç‰¹æ€§çš„å·¥å…·è°ƒç”¨æ•°æ®ã€‚ä¸å†ä¾èµ–çœŸå®APIï¼Œè€Œæ˜¯æ„å»ºå°‘é‡è™šæ‹Ÿå·¥å…·æ¥å®ç°å¼ºå·¥å…·è°ƒç”¨æ€§èƒ½ï¼Œæ‘†è„±äº†çœŸå®APIè°ƒç”¨çš„é«˜é¢æˆæœ¬ä¸é™åˆ¶ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¢å¼ºLLMsæ³›åŒ–èƒ½åŠ›ä¸äº¤äº’æ¨¡å¼ä¸°å¯Œåº¦  
ç”¨è™šæ‹Ÿå·¥å…·æ›¿ä»£çœŸå®APIï¼Œå¹¶èå…¥åæ€é©±åŠ¨çš„å¤šè½®äº¤äº’ã€‚è®¾è®¡äº†å››ç§å·¥å…·è°ƒç”¨èŒƒå¼ä¸ä¸‰ç±»é”™è¯¯æ‰°åŠ¨ç±»åˆ«ï¼Œè¡ç”Ÿå‡º29ç§ä¸åŒäº¤äº’æ¨¡å¼ï¼Œè¦†ç›–å¤æ‚å¤šè½®å·¥å…·è°ƒç”¨åœºæ™¯ï¼Œè®©ç”Ÿæˆçš„æ¨ç† - å·¥å…·äº¤äº’æ¨¡å¼æ›´ä¸°å¯Œå¤šæ ·ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¯æ‰©å±•æ€§è®¾è®¡  
ToolForgeä¸å±€é™äºæ–‡ä¸­å®ä¾‹åŒ–çš„19ä¸ªè™šæ‹Ÿå·¥å…·å’Œ29ç§äº¤äº’æ¨¡å¼ã€‚é¢å¤–è™šæ‹Ÿå·¥å…·ã€æ–°å™ªå£°ç±»å‹æˆ–æ›´å¤æ‚äº¤äº’ motif èƒ½ä»¥å³æ’å³ç”¨æ–¹å¼æ•´åˆï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒæµç¨‹ï¼Œä¸ºåç»­æ‹“å±•ç•™è¶³ç©ºé—´ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šå±‚éªŒè¯æ¡†æ¶ä¿éšœæ•°æ®ä¿çœŸåº¦  
å¼•å…¥ç»“åˆåŸºäºè§„åˆ™å¯å‘å¼å’ŒåŸºäºæ¨¡å‹è¯„ä¼°çš„å¤šå±‚éªŒè¯æ¡†æ¶ï¼ˆMulti - Layer Validation Frameworkï¼‰ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ï¼Œå¤§å¹…æå‡éªŒè¯é²æ£’æ€§ä¸è¦†ç›–åº¦ï¼Œè§£å†³äº†å¤æ‚è‡ªåŠ¨åˆæˆæ•°æ®ä¿çœŸåº¦éªŒè¯éš¾ã€ä¸­é—´æ¨ç†é”™è¯¯æ˜“è¢«å¿½è§†çš„é—®é¢˜ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œä»…8Bå‚æ•°çš„æ¨¡å‹ï¼ˆToolForge - 8Bï¼ŒåŸºäºQwen3 - 8Båœ¨ToolForgeåˆæˆæ•°æ®ä¸Šå¾®è°ƒå¾—åˆ°ï¼‰åœ¨å¤šä¸ªå·¥å…·è°ƒç”¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½è¶…è¿‡äº†å¦‚GPT - 4oè¿™æ ·çš„å¼ºå¤§ä¸“æœ‰æ¨¡å‹ï¼Œæœ‰åŠ›è¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹å·¥å…·è°ƒç”¨èƒ½åŠ›ä¸Šçš„å‰æ²¿æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆæˆæ€è·¯é©æ–°ï¼šå±•ç¤ºäº†ç”¨è™šæ‹Ÿå·¥å…·æ›¿ä»£çœŸå®APIæ¥é™ä½æˆæœ¬ã€æå‡æ³›åŒ–çš„å¯è¡Œæ€§ï¼Œä¸ºèµ„æºå—é™ä¸‹çš„å·¥å…·å­¦ä¹ æ•°æ®ç”Ÿæˆæä¾›æ–°æ€è·¯ã€‚  
2. å¤šè·³ä¸åæ€èåˆï¼šå°†å¤šè·³æ¨ç†å’Œè‡ªæˆ‘åæ€æœºåˆ¶èå…¥å·¥å…·è°ƒç”¨æ•°æ®ç”Ÿæˆï¼Œä¸ºå¤„ç†å¤æ‚ç°å®ä»»åŠ¡åœºæ™¯çš„æ¨¡å‹è®­ç»ƒæä¾›äº†å‚è€ƒæ–¹å‘ã€‚  
3. éªŒè¯æœºåˆ¶æ„å»ºï¼šå¤šå±‚éªŒè¯æ¡†æ¶æ•´åˆè§„åˆ™ä¸æ¨¡å‹è¯„ä¼°ã€ç»“åˆMCTSæŒ–æ˜è´Ÿæ ·æœ¬çš„æ–¹å¼ï¼Œä¸ºä¿éšœåˆæˆæ•°æ®è´¨é‡æä¾›äº†å¯å¤ç”¨çš„éªŒè¯èŒƒå¼ã€‚  
4. å¯æ‰©å±•æ€§æ¶æ„ï¼šå³æ’å³ç”¨çš„æ‹“å±•è®¾è®¡ç†å¿µï¼Œè®©ç ”ç©¶è€…èƒ½è½»æ¾æ·»åŠ æ–°å…ƒç´ æ¥é€‚é…ä¸åŒéœ€æ±‚ï¼Œåˆ©äºç”Ÿæ€æŒç»­å‘å±•ä¸åŠŸèƒ½è¿­ä»£ã€‚

## adaptvision--efficient-vision-language-models-via-adaptive-visual-acquisition
### Abstract
Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdaptVisionï¼šè®©è§†è§‰è¯­è¨€æ¨¡å‹â€œä¸»åŠ¨çœ‹â€ï¼Œé«˜æ•ˆå¹³è¡¡æ€§èƒ½ä¸è®¡ç®—å¼€é”€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†å¤§é‡è§†è§‰tokençš„ä¾èµ–å¸¦æ¥äº†é«˜æ˜‚çš„è®¡ç®—ä¸å†…å­˜å¼€é”€ã€‚ç°æœ‰é«˜æ•ˆVLMæ–¹æ³•å¤šé€šè¿‡å›ºå®šæ¯”ä¾‹å‹ç¼©è§†è§‰tokenï¼Œå´â€œè¢«åŠ¨â€ä¸”ç¼ºä¹ä»»åŠ¡é€‚åº”æ€§â€”â€”æ— æ³•è‡ªä¸»åˆ¤æ–­æ¯ä¸ªæ ·æœ¬æ‰€éœ€çš„æœ€å°‘è§†è§‰tokenæ•°é‡ã€‚å—äººç±»**ä¸»åŠ¨è§†è§‰æœºåˆ¶**ï¼ˆå…ˆæŠ“åœºæ™¯æ¢—æ¦‚ã€å†èšç„¦å…³é”®åŒºåŸŸåˆ†æï¼‰å¯å‘ï¼Œè®ºæ–‡è¯•å›¾è®©VLMsåƒäººç±»ä¸€æ ·â€œä¸»åŠ¨å†³ç­–â€è§†è§‰tokenç”¨é‡ï¼Œåœ¨ä¿è¯ç²¾åº¦çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—æˆæœ¬ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAdaptVisionæ¡†æ¶ï¼Œå®ç°â€œç”±ç²—åˆ°ç»†â€çš„è‡ªé€‚åº”è§†è§‰tokenè·å–  
AdaptVisionå…ˆå¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒçš„å‹ç¼©è§†è§‰tokenï¼Œå¿…è¦æ—¶è°ƒç”¨â€œè¾¹ç•Œæ¡†å·¥å…·â€è£å‰ªåŸå›¾å…³é”®åŒºåŸŸï¼Œä¸»åŠ¨è·å–é¢å¤–è§†è§‰ä¿¡æ¯ã€‚è¿™ç§â€œå…ˆç²—åç»†â€çš„é€»è¾‘ï¼Œè®©æ¨¡å‹èƒ½åŠ¨æ€å†³å®šæ¯ä¸ªæ ·æœ¬éœ€è¦å¤šå°‘è§†è§‰tokenï¼Œè€Œéä¾èµ–å›ºå®šå‹ç¼©è§„åˆ™ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è§£è€¦å›åˆç­–ç•¥ä¼˜åŒ–ï¼ˆDTPOï¼‰ç®—æ³•ï¼Œè§£å†³å¼ºåŒ–å­¦ä¹ è®­ç»ƒéš¾é¢˜  
è®­ç»ƒæ—¶éœ€å¹³è¡¡â€œç²¾åº¦â€ä¸â€œæ•ˆç‡â€åŒç›®æ ‡ï¼Œä¼ ç»ŸRLç®—æ³•ï¼ˆå¦‚GRPOï¼‰å­˜åœ¨**ä¿¡ç”¨åˆ†é…æ¨¡ç³Š**ï¼ˆæ— æ³•åŒºåˆ†â€œæ˜¯å¦è°ƒç”¨å·¥å…·â€å’Œâ€œç”Ÿæˆç­”æ¡ˆâ€çš„è´¡çŒ®ï¼‰ä¸**ä¼˜åŒ–ä¸å¹³è¡¡**ï¼ˆå¤šå›åˆå·¥å…·è°ƒç”¨åºåˆ—æ˜“è®­ç»ƒä¸è¶³ï¼‰é—®é¢˜ã€‚DTPOå°†å­¦ä¹ ç›®æ ‡è§£è€¦ä¸ºä¸¤éƒ¨åˆ†ï¼š  
- å·¥å…·å­¦ä¹ ï¼ˆTool Learningï¼‰ï¼šä¼˜åŒ–â€œä½•æ—¶/æ˜¯å¦è°ƒç”¨å·¥å…·â€çš„å†³ç­–ï¼›  
- ç²¾åº¦æå‡ï¼ˆAccuracy Improvementï¼‰ï¼šä¼˜åŒ–æœ€ç»ˆç­”æ¡ˆç”Ÿæˆçš„æ­£ç¡®æ€§ã€‚  
åŒæ—¶ï¼Œå¯¹æ¯ä¸ªç›®æ ‡çš„tokenå•ç‹¬åšä¼˜åŠ¿ä¼°è®¡ï¼ˆAdvantage Estimationï¼‰è§£è€¦ï¼Œè®©è®­ç»ƒæ›´é«˜æ•ˆã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAdaptVisionç›¸æ¯”å½“å‰SOTAçš„é«˜æ•ˆVLMæ–¹æ³•ï¼Œ**ç”¨æ˜¾è‘—æ›´å°‘çš„è§†è§‰token**å®ç°äº†æ›´ä¼˜æ€§èƒ½ã€‚è¿™éªŒè¯äº†â€œè‡ªé€‚åº”è·å–è§†è§‰tokenâ€+â€œDTPOè®­ç»ƒæ¡†æ¶â€åœ¨â€œæ€§èƒ½-æ•ˆç‡å¹³è¡¡â€ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»äººç±»è®¤çŸ¥ä¸­æ‰¾çµæ„Ÿï¼šå°†â€œä¸»åŠ¨è§†è§‰â€æœºåˆ¶è½¬åŒ–ä¸ºæ¨¡å‹é€»è¾‘ï¼Œä¸ºé«˜æ•ˆå¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æä¾›äº†è®¤çŸ¥ç§‘å­¦è§†è§’çš„æ€è·¯ï¼›  
2. è§£è€¦å¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼šé¢å¯¹â€œå¤šç›®æ ‡å¹³è¡¡â€ç±»ä»»åŠ¡ï¼Œæ‹†è§£ç›®æ ‡ã€åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒç­–ç•¥å€¼å¾—å€Ÿé‰´ï¼›  
3. å·¥å…·è°ƒç”¨ä¸æ•ˆç‡ç»“åˆï¼šæŠŠâ€œå·¥å…·ä½¿ç”¨â€ä»â€œæå‡ç²¾åº¦â€å»¶ä¼¸åˆ°â€œæ§åˆ¶è®¡ç®—å¼€é”€â€ï¼Œæ‹“å®½äº†è§†è§‰è¯­è¨€æ¨¡å‹å·¥å…·é“¾çš„åº”ç”¨åœºæ™¯ã€‚  

AdaptVisionçš„æ€è·¯ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨â€œé«˜æ•ˆæ¨ç†â€æ–¹å‘æ‰“å¼€äº†æ–°è§†è§’â€”â€”è®©æ¨¡å‹å­¦ä¼šâ€œä¸»åŠ¨å†³ç­–è¯¥çœ‹å¤šå°‘ã€çœ‹å“ªé‡Œâ€ï¼Œè€Œéè¢«åŠ¨å‹ç¼©tokenã€‚è¿™ç§â€œç”±ç²—åˆ°ç»†+å¼ºåŒ–å­¦ä¹ è§£è€¦è®­ç»ƒâ€çš„èŒƒå¼ï¼Œä¹Ÿä¸ºåç»­å¤šæ¨¡æ€é«˜æ•ˆæ¨¡å‹ç ”ç©¶æä¾›äº†æœ‰åŠ›å‚è€ƒã€‚

## the-empowerment-of-science-of-science-by-large-language-models--new-tools-and-methods
### Abstract
Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½ç§‘å­¦å­¦ï¼šæ–°å·¥å…·ä¸æ–°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆã€å›¾åƒè¯†åˆ«ã€å¤šæ¨¡æ€ä»»åŠ¡ç­‰æ–¹é¢å±•ç°å“è¶Šèƒ½åŠ›ï¼Œæœç€é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆè¿›ï¼Œæˆä¸ºå…¨çƒç§‘æŠ€ç«èµ›æ ¸å¿ƒè®®é¢˜ã€‚è€Œç§‘å­¦å­¦ï¼ˆSciSciï¼‰ä½œä¸ºå¯¹ç§‘å­¦æœ¬èº«è¿›è¡Œå®šé‡ç ”ç©¶çš„äº¤å‰å­¦ç§‘ï¼Œå…¶ç ”ç©¶æ–¹æ³•æ­£ä»ä¼ ç»Ÿåˆ†ææ‰‹æ®µå‘èåˆè®¡ç®—æœºç§‘å­¦ä¸äººå·¥æ™ºèƒ½æ¼”è¿›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè®ºæ–‡æ—¨åœ¨ä»ç”¨æˆ·è§†è§’å…¨é¢æ¢³ç†æ”¯æ’‘LLMsçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œè¿½æº¯ç§‘å­¦å­¦å‘å±•å†ç¨‹ï¼Œå¹¶å‰ç»æ€§æ¢è®¨LLMsåœ¨ç§‘å­¦è®¡é‡é¢†åŸŸçš„æ½œåœ¨åº”ç”¨ï¼Œä¸ºç§‘å­¦å­¦ç ”ç©¶æ³¨å…¥æ–°æ´»åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢æ¢³ç†LLMsæ ¸å¿ƒæŠ€æœ¯  
ä»ç”¨æˆ·è§†è§’å¯¹æ”¯æ’‘å¤§è¯­è¨€æ¨¡å‹çš„å…³é”®æŠ€æœ¯å±•å¼€ç»¼è¿°ï¼Œæ¶µç›–æç¤ºå·¥ç¨‹ã€çŸ¥è¯†å¢å¼ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€å¾®è°ƒã€é¢„è®­ç»ƒä»¥åŠå·¥å…·å­¦ä¹ ç­‰ï¼Œæ¸…æ™°å‘ˆç°å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯æ¶æ„ä¸åº”ç”¨å±‚é¢çš„æ ¸å¿ƒè¦ç‚¹ï¼Œå¸®åŠ©è¯»è€…ç³»ç»Ÿç†è§£LLMsæŠ€æœ¯æ ˆã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè”ç»“LLMsä¸ç§‘å­¦å­¦å‘å±•è„‰ç»œä¸åº”ç”¨å±•æœ›  
è¿½æº¯ç§‘å­¦å­¦ä»ä¼ ç»Ÿåˆ†ææ–¹æ³•ï¼ˆå¦‚å¼•æ–‡åˆ†æã€è¯é¢‘åˆ†æç­‰ï¼‰åˆ°èåˆè®¡ç®—æœºç§‘å­¦ä¸AIæŠ€æœ¯ï¼ˆå¦‚åŠ¨æ€ä¸»é¢˜æ¨¡å‹ã€BERTã€å›¾å·ç§¯ç½‘ç»œç­‰ï¼‰çš„å‘å±•å†ç¨‹ï¼Œå¹¶å‰ç»æ€§æ¢è®¨LLMsåœ¨ç§‘å­¦è®¡é‡é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬åŸºäºAIæ™ºèƒ½ä½“çš„ç§‘å­¦è¯„ä¼°æ¨¡å‹å‰æ™¯ã€å€ŸåŠ©LLMså®ç°çš„æ–°ç ”ç©¶å‰æ²¿æ£€æµ‹ä¸çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•ç­‰ï¼Œä¸ºç§‘å­¦å­¦ç ”ç©¶å¼€æ‹“æ–°æ–¹å‘ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡æœªèšç„¦ä¼ ç»Ÿå®éªŒå¯¹æ¯”ç±»ç»“æœå‘ˆç°ï¼Œè€Œæ˜¯é€šè¿‡æŠ€æœ¯æ¢³ç†ä¸é¢†åŸŸèåˆåˆ†æï¼Œå±•ç°å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ä½“ç³»çš„ä¸°å¯Œæ€§ï¼Œä»¥åŠå…¶ä¸ç§‘å­¦å­¦é¢†åŸŸç»“åˆååœ¨æ–¹æ³•æ¼”è¿›ã€åº”ç”¨æ‹“å±•ç­‰æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºåç»­ç›¸å…³æŠ€æœ¯è½åœ°ä¸ç§‘å­¦å­¦ç ”ç©¶åˆ›æ–°æä¾›ç†è®ºä¸æ–¹å‘å‚è€ƒã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºAIé¢†åŸŸç ”ç©¶è€…ï¼Œèƒ½ç³»ç»Ÿå­¦ä¹ å¤§è¯­è¨€æ¨¡å‹æ ¸å¿ƒæŠ€æœ¯æ¨¡å—ï¼ˆæç¤ºå·¥ç¨‹ã€RAGã€å¾®è°ƒç­‰ï¼‰ï¼ŒæŠŠæ¡æŠ€æœ¯åº”ç”¨é€»è¾‘ï¼›å¯¹äºç§‘å­¦å­¦é¢†åŸŸå­¦è€…ï¼Œå¯å€Ÿé‰´è®ºæ–‡ä¸­LLMsä¸ç§‘å­¦å­¦ç»“åˆçš„æ€è·¯ï¼Œå¦‚åˆ©ç”¨LLMsè¿›è¡Œç ”ç©¶å‰æ²¿æ¢æµ‹ã€çŸ¥è¯†å›¾è°±æ„å»ºç­‰åˆ›æ–°ç ”ç©¶æ–¹æ³•ï¼›å¯¹äºå…³æ³¨è·¨å­¦ç§‘å‘å±•çš„è¯»è€…ï¼Œè®ºæ–‡å±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹åœ¨äº¤å‰é¢†åŸŸçš„èµ‹èƒ½è·¯å¾„ï¼Œä¸ºæ¢ç´¢æŠ€æœ¯ä¸ä¸åŒå­¦ç§‘èåˆæä¾›èŒƒä¾‹ï¼Œå¯å‘æ€è€ƒæŠ€æœ¯é©±åŠ¨ä¸‹å„é¢†åŸŸåˆ›æ–°çš„å¯èƒ½æ€§ã€‚ 

## looptool--closing-the-data-training-loop-for-robust-llm-tool-calls
### Abstract
Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LoopToolï¼šæ‰“é€ æ•°æ®-è®­ç»ƒé—­ç¯ï¼Œæå‡å¤§æ¨¡å‹å·¥å…·è°ƒç”¨é²æ£’æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆå¤–éƒ¨å·¥å…·èƒ½æ‰§è¡Œå¤æ‚å¤šæ­¥ä»»åŠ¡ï¼Œä½†å½“å‰å·¥å…·å­¦ä¹ å—é™äº**é™æ€åˆæˆæ•°æ® pipeline**ï¼šæ•°æ®ç”Ÿæˆä¸æ¨¡å‹è®­ç»ƒæ˜¯åˆ†ç¦»ã€æ— äº¤äº’çš„è¿‡ç¨‹ã€‚è¿™å¯¼è‡´æ— æ³•é’ˆå¯¹æ€§èšç„¦æ¨¡å‹å¼±ç‚¹ï¼Œå™ªå£°æ ‡ç­¾ä¹Ÿä¼šæŒç»­å­˜åœ¨é™ä½è®­ç»ƒæ•ˆç‡ï¼›åŒæ—¶å·¥å…·ä½¿ç”¨æ•°æ®ç”Ÿæˆåœ¨æˆæœ¬æ•ˆç‡å’Œæ•°æ®è´¨é‡é—´éš¾å¹³è¡¡ï¼Œä¾èµ–é—­æºå¤§æ¨¡å‹ä¼šå¸¦æ¥é«˜é¢ API æˆæœ¬ä¸ä½æ•ˆé—®é¢˜ï¼Œç”¨å¼€æºæ¨¡å‹åˆæ˜“å¼•å…¥å™ªå£°æ ‡æ³¨å½±å“æ³›åŒ–æ€§ã€‚ä¸ºè§£å†³è¿™äº›é™æ€ã€é«˜æˆæœ¬ã€æ˜“å‡ºé”™çš„å·¥å…·æ•°æ® pipeline å±€é™ï¼Œè®ºæ–‡æå‡º LoopTool æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå…¨è‡ªåŠ¨åŒ–ã€æ„ŸçŸ¥æ¨¡å‹çš„æ•°æ®æ¼”åŒ–æ¡†æ¶ LoopToolï¼Œé—­åˆæ•°æ®åˆæˆä¸æ¨¡å‹è®­ç»ƒçš„å¾ªç¯  
LoopTool å…ˆé€šè¿‡è‡ªåŠ¨åŒ–å·¥å…·å¢å¼ºæ•°æ®æ„å»ºé˜¶æ®µç”Ÿæˆç§å­è¯­æ–™å¹¶å®Œæˆåˆå§‹è®­ç»ƒï¼›ä¹‹åè¿­ä»£ä¸­æ•´åˆä¸‰ä¸ªååŒæ¨¡å—æŒç»­ä¼˜åŒ–æ•°æ®ä¸æ¨¡å‹ï¼š  
- **Greedy Capability Probingï¼ˆGCPï¼‰**ï¼šç”¨è´ªå¿ƒè§£ç åœ¨ç§å­è¯­æ–™ä¸ŠæŸ¥è¯¢å¾®è°ƒåæ¨¡å‹ï¼Œè¯Šæ–­æ¨¡å‹å·²æŒæ¡ã€ä¸´ç•Œå’Œå¤±è´¥èƒ½åŠ›æƒ…å†µï¼Œå®šä½å…·æŒ‘æˆ˜æ€§ã€è¡¨ç°å·®çš„æ¡ˆä¾‹ã€‚  
- **Judgement - Guided Label Verificationï¼ˆJGLVï¼‰**ï¼šç”¨å¼€æºå¤§æ¨¡å‹ Qwen3 - 32B ä½œä¸ºåˆ¤æ–­æ¨¡å‹ï¼Œå¯¹æ¯”æ¨¡å‹é¢„æµ‹ä¸å‚è€ƒæ ‡ç­¾ï¼Œè¯†åˆ«æ¨¡å‹çœŸå®é”™è¯¯ä¸â€œæ¨¡å‹è¾“å‡ºä¼˜äºæ ‡ç­¾â€æƒ…å†µï¼Œç”¨æ›´ä¼˜è¾“å‡ºæ›¿æ¢å™ªå£°æ ‡ç­¾ï¼Œé€æ­¥å‡€åŒ–ç›‘ç£ä¿¡å·ã€‚  
- **Error - Driven Data Expansionï¼ˆEDDEï¼‰**ï¼šå°†éªŒè¯åçš„å¤±è´¥æ¡ˆä¾‹è½¬åŒ–ä¸ºç»“æ„ç›¸ä¼¼ä½†åœºæ™¯å¤šæ ·çš„æ–°æŒ‘æˆ˜æ ·æœ¬ï¼Œåœ¨ä¿ç•™æ ¸å¿ƒåŠŸèƒ½æŒ‘æˆ˜åŒæ—¶å¢åŠ åœºæ™¯å¤šæ ·æ€§ã€‚è¿­ä»£ä¸­æŠŠä¿®æ­£æ ‡æ³¨ã€å¤šæ ·åŒ–éš¾æ ·æœ¬ç­‰çº³å…¥åç»­è®­ç»ƒï¼Œæ‰“é€ é€‚é…æ¨¡å‹èƒ½åŠ›æ¼”åŒ–çš„åŠ¨æ€è¯¾ç¨‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæˆæœ¬ä¸è´¨é‡å¹³è¡¡ï¼Œç»Ÿä¸€æ•°æ®ç”Ÿæˆä¸è¯„ä¼°è§’è‰²åˆ°å•ä¸€å¼€æºæ¨¡å‹  
LoopTool ç”¨å¼€æºæ¨¡å‹ Qwen3 - 32B åŒæ—¶æ‰¿æ‹…æ•°æ®ç”Ÿæˆä¸è¯„ä¼°åˆ¤æ–­è§’è‰²ï¼Œæ‘†è„±å¯¹æ˜‚è´µé—­æº API ä¾èµ–ï¼Œè¿˜èƒ½ç»´æŒé«˜è´¨é‡æ•°æ®ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œå®Œå…¨ç”¨ Qwen3 - 32B ç”Ÿæˆå’Œè¯„ä¼°æ•°æ®è®­ç»ƒå‡ºçš„ 8B è§„æ¨¡ LoopTool æ¨¡å‹ï¼Œåœ¨å·¥å…·ä½¿ç”¨æ€§èƒ½ä¸Š**æ˜¾è‘—è¶…è¶Š 32B çš„æ•°æ®ç”Ÿæˆæ¨¡å‹**ï¼›ä¸”åœ¨ BFCL - v3 å’Œ ACEBench åŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨åŒè§„æ¨¡æ¨¡å‹é‡Œå–å¾—äº†**å…¨æ–°çš„ state - of - the - art ç»“æœ**ï¼Œå‡¸æ˜¾è¿­ä»£å¼ã€æ„ŸçŸ¥æ¨¡å‹çš„æ•°æ®ç²¾ä¿®å¸¦æ¥çš„æ”¾å¤§æ•ˆåº”ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- æ•°æ® - è®­ç»ƒé—­ç¯æ€è·¯ï¼šæ‰“ç ´é™æ€æ•°æ® pipeline æ¨¡å¼ï¼Œè®©æ•°æ®ç”Ÿæˆä¸æ¨¡å‹è®­ç»ƒäº¤äº’ï¼Œæ ¹æ®æ¨¡å‹èƒ½åŠ›åŠ¨æ€è°ƒæ•´æ•°æ®ï¼Œä¸ºæå‡æ¨¡å‹ç‰¹å®šèƒ½åŠ›æä¾›äº†æ–°èŒƒå¼ã€‚  
- å™ªå£°æ ‡ç­¾å¤„ç†ï¼šJGLV æ¨¡å—åˆ©ç”¨åˆ¤æ–­æ¨¡å‹è‡ªåŠ¨è¯†åˆ«å¹¶ä¿®æ­£æ ‡ç­¾é”™è¯¯ï¼Œä¸ºå‡€åŒ–è®­ç»ƒæ•°æ®ã€æå‡ç›‘ç£ä¿¡å·è´¨é‡æä¾›äº†å¯å‚è€ƒçš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚  
- å¤±è´¥æ¡ˆä¾‹åˆ©ç”¨ï¼šEDDE æŠŠå¤±è´¥æ¡ˆä¾‹è½¬åŒ–ä¸ºæ–°æŒ‘æˆ˜æ ·æœ¬ï¼Œä¸ºé«˜æ•ˆæ‰©å……é«˜ä»·å€¼è®­ç»ƒæ•°æ®ã€é’ˆå¯¹æ€§å¼ºåŒ–æ¨¡å‹è–„å¼±ç‚¹æä¾›äº†æ€è·¯ï¼›ä¸”åŸºäºå¼€æºç”Ÿæ€å®ç°æˆæœ¬å¯æ§ï¼Œå¯¹èµ„æºæœ‰é™çš„ç ”ç©¶æˆ–åº”ç”¨åœºæ™¯æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## one-model-to-critique-them-all--rewarding-agentic-tool-use-via-efficient-reasoning
### Abstract
Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šç”¨å·¥å…·ä½¿ç”¨åœºæ™¯çš„å¥–åŠ±æ¨¡å‹æ–°çªç ´ï¼šToolRM å¦‚ä½•é©æ–°æ™ºèƒ½ä½“å·¥å…·å­¦ä¹ ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é¢†åŸŸï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›æ¨åŠ¨äº†è¯¸å¤šè¿›å±•ï¼Œä½†é’ˆå¯¹å·¥å…·å­¦ä¹ ä»»åŠ¡çš„ä¸“ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ç¼ºå¤±ï¼Œé™åˆ¶äº†æ›´å¼ºå¤§æ™ºèƒ½ä½“ AI çš„å‘å±•ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å·²éªŒè¯çš„å·¥å…·è°ƒç”¨è½¨è¿¹è·å–åé¦ˆï¼Œå¯æ‰©å±•æ€§å—é™ï¼Œæ¨ç†æ—¶ä¹Ÿéš¾åˆ©ç”¨å¤šé‡‡æ ·ç­”æ¡ˆåšé€‰æ‹©ã€‚å› æ­¤ï¼Œå¼€å‘èƒ½è¯„ä¼°å·¥å…·ä½¿ç”¨è¡Œä¸ºä¸”æ— éœ€çœŸå®æ ‡ç­¾çš„é²æ£’å¥–åŠ±æ¨¡å‹å¯¹è¯¥é¢†åŸŸè‡³å…³é‡è¦ï¼Œè€Œè®¾è®¡å·¥å…·ä½¿ç”¨å¥–åŠ±æ¨¡å‹é¢ä¸´æ„å»ºé«˜è´¨é‡åå¥½å¯¹ã€å®ç°æ³›åŒ–æ€§ critiqueã€è¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸‰å¤§æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºç”Ÿæˆå·¥å…·ä½¿ç”¨å¥–åŠ±æ¨¡å‹åå¥½æ•°æ®çš„æ–°é¢– pipeline  
é¦–å…ˆä»ä¸ƒä¸ªå¼€æºå·¥å…·è°ƒç”¨æ•°æ®é›†æ•´ç†å¹¶éªŒè¯å·¥å…·è°ƒç”¨è½¨è¿¹ï¼Œå°†å…¶åˆ†å‰²ä¸ºä¸Šä¸‹æ–‡ - å“åº”å¯¹ï¼Œç”¨å¤šä¸ª LLM é‡‡æ ·æ›¿ä»£å“åº”ï¼›ä¸ä¾èµ–çœŸå®åŒ¹é…ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„æ ‡è®°æ•æ‰ç»†ç²’åº¦åå¥½ï¼›å¤šç»´é‡‡æ ·ç­–ç•¥ç¡®ä¿åœºæ™¯å¤šæ ·ã€åå¥½å¼ºåº¦æœ‰å˜åŒ–ä¸”ä»»åŠ¡å¤æ‚åº¦é«˜ï¼Œæœ€ç»ˆæ„å»ºå‡ºå« 3 ä¸‡æ¡å…·æœ‰æŒ‘æˆ˜æ€§åå¥½å¯¹çš„ ToolPref - Pairwise - 30K æ•°æ®é›†ï¼Œä¸ºå·¥å…·å¯¼å‘å¥–åŠ±å»ºæ¨¡æä¾›å…¬å¼€èµ„æºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®­ç»ƒè½»é‡çº§ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ ToolRM  
åŸºäº Qwen3 - 4B/8B ç³»åˆ—ï¼Œåˆ©ç”¨ä»å¯éªŒè¯å¥–åŠ±ä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰èŒƒå¼ï¼Œä»¥ pairwise ç›®æ ‡è®­ç»ƒ ToolRMã€‚è¯¥æ¨¡å‹èƒ½å­¦ä¹ é²æ£’æ¨ç†ï¼Œæ— éœ€ç²¾å¿ƒç­–åˆ’çš„è½¨è¿¹ï¼Œä¸”é™¤è®­ç»ƒç›®æ ‡å¤–ï¼Œè¿˜èƒ½æ³›åŒ–åˆ°æ›´å¹¿æ³›çš„ critique ä»»åŠ¡ï¼ˆå¦‚ Best - of - N é‡‡æ ·å’Œè‡ªæˆ‘ä¿®æ­£ï¼‰ï¼Œå®ç°é«˜æ•ˆæ¨ç†æ—¶æ‰©å±•å¹¶ç”Ÿæˆç®€æ´ä¸”ä¿¡æ¯ä¸°å¯Œçš„ critiqueã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºå·¥å…·ä½¿ç”¨åœºæ™¯å¥–åŠ±æ¨¡å‹è¯„ä¼°åŸºå‡† TRBench$_{BFCL}$  
åŸºäº agentic è¯„ä¼°å¥—ä»¶ BFCL æ„å»ºè¯¥åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸Šå¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ†ææ˜¾ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ LLM å’Œä¸“ç”¨å¥–åŠ±æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šä¹Ÿå­˜åœ¨æ˜æ˜¾å·®è·ï¼Œå‡¸æ˜¾äº†é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆçš„å¿…è¦æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ„å»ºçš„ ToolPref - Pairwise - 30K æ•°æ®ä¸Šè®­ç»ƒåï¼ŒQwen3 - 4B/8B ç³»åˆ—æ¨¡å‹åœ¨ pairwise å¥–åŠ±åˆ¤æ–­ä¸­å‡†ç¡®ç‡æœ€é«˜æå‡ 14.28%ï¼Œå¤§å¹…è¶…è¶Š Claude 4ã€OpenAI o3 ç­‰å‰æ²¿æ¨¡å‹ï¼›åœ¨ ACEBench ä¸Šçš„å®éªŒå‡¸æ˜¾å…¶åœ¨æ›´å¹¿æ³› critique ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œèƒ½å®ç°æ¨ç†æ—¶æ‰©å±•ä¸”å‡å°‘è¶… 66% çš„è¾“å‡º token ä½¿ç”¨é‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºå±‚é¢ï¼šæå‡ºçš„ä¸¤é˜¶æ®µ pipeline ä¸ºé¢†åŸŸç‰¹å®šå¥–åŠ±æ¨¡å‹æ„å»ºé«˜è´¨é‡åå¥½æ•°æ®æä¾›äº†èŒƒä¾‹ï¼Œå¤šæ•°æ®æºæ•´åˆã€åŸºäºè§„åˆ™æ ‡è®°ä¸å¤šç»´é‡‡æ ·ç»“åˆçš„æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€è¦åå¥½æ•°æ®çš„ä»»åŠ¡åœºæ™¯ã€‚  
2. æ¨¡å‹è®­ç»ƒå±‚é¢ï¼šå€ŸåŠ© RLVR èŒƒå¼ä»¥ pairwise ç›®æ ‡è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œè®©æ¨¡å‹å­¦ä¹ é²æ£’æ¨ç†å¹¶æ³›åŒ–åˆ°å¤šä»»åŠ¡çš„æ–¹å¼ï¼Œä¸ºæå‡å¥–åŠ±æ¨¡å‹æ³›åŒ–èƒ½åŠ›æä¾›äº†å‚è€ƒã€‚  
3. è¯„ä¼°å±‚é¢ï¼šä¸“é—¨é’ˆå¯¹å·¥å…·ä½¿ç”¨åœºæ™¯æ„å»ºè¯„ä¼°åŸºå‡† TRBench$_{BFCL}$ï¼Œå¼ºè°ƒäº†é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡æ„å»ºä¸“å±è¯„ä¼°åŸºå‡†å¯¹æ¨åŠ¨é¢†åŸŸå‘å±•çš„é‡è¦æ€§ï¼Œè¿™ç§æ€è·¯å¯ç”¨äºå…¶ä»– AI ç»†åˆ†é¢†åŸŸè¯„ä¼°ä½“ç³»æ­å»ºã€‚  
4. å¼€æºè´¡çŒ®å±‚é¢ï¼šå…¬å¼€æ•°æ®å’Œæ¨¡å‹ checkpointï¼Œåˆ©äºæ•´ä¸ªç¤¾åŒºåŸºäºæ­¤å¼€å±•åç»­ç ”ç©¶ï¼Œæ¨åŠ¨é¢†åŸŸå¿«é€Ÿå‘å±•ï¼Œè¿™ç§å¼€æºå…±äº«çš„ç§‘ç ”å®è·µå€¼å¾—å­¦ä¹ æ¨å¹¿ã€‚

## toolsample--dual-dynamic-sampling-methods-with-curriculum-learning-for-rl-based-tool-learning
### Abstract
While reinforcement learning (RL) is increasingly used for LLM-based tool learning, its efficiency is often hampered by an overabundance of simple samples that provide diminishing learning value as training progresses. Existing dynamic sampling techniques are ill-suited for the multi-task structure and fine-grained reward mechanisms inherent to tool learning. This paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework specifically designed to address this challenge by targeting the unique characteristics of tool learning: its multiple interdependent sub-tasks and multi-valued reward functions. DSCL features two core components: Reward-Based Dynamic Sampling, which uses multi-dimensional reward statistics (mean and variance) to prioritize valuable data, and Task-Based Dynamic Curriculum Learning, which adaptively focuses training on less-mastered sub-tasks. Through extensive experiments, we demonstrate that DSCL significantly improves training efficiency and model performance over strong baselines, achieving a 3.29\% improvement on the BFCLv3 benchmark. Our method provides a tailored solution that effectively leverages the complex reward signals and sub-task dynamics within tool learning to achieve superior results.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolSampleï¼šä¸ºåŸºäºRLçš„å·¥å…·å­¦ä¹ é‡èº«å®šåˆ¶çš„åŒåŠ¨æ€é‡‡æ ·ä¸è¯¾ç¨‹å­¦ä¹ æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å·¥å…·å­¦ä¹ é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¹Ÿæˆä¸ºæå‡LLMsæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›çš„å…³é”®ç­–ç•¥ã€‚ä½†åœ¨åŸºäºRLçš„å·¥å…·å­¦ä¹ ä¸­ï¼Œéšç€è®­ç»ƒæ¨è¿›ï¼Œå¤§é‡ç®€å•æ ·æœ¬å­¦ä¹ ä»·å€¼é€’å‡ï¼Œå½±å“è®­ç»ƒæ•ˆç‡ï¼›ç°æœ‰åŠ¨æ€é‡‡æ ·æŠ€æœ¯éš¾ä»¥é€‚é…å·¥å…·å­¦ä¹ çš„å¤šä»»åŠ¡ç»“æ„ä¸ç»†ç²’åº¦å¥–åŠ±æœºåˆ¶ã€‚å·¥å…·å­¦ä¹ å­˜åœ¨å¤šä¸ªç›¸äº’ä¾èµ–å­ä»»åŠ¡ã€å¤šå€¼å¥–åŠ±å‡½æ•°ç­‰ç‰¹æ€§ï¼Œä¼ ç»Ÿæ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨å¤æ‚å¥–åŠ±ä¿¡å·ä¸å­ä»»åŠ¡åŠ¨æ€ï¼Œå› æ­¤éœ€è¦é’ˆå¯¹æ€§æ–¹æ³•ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDynamic Sampling with Curriculum Learningï¼ˆDSCLï¼‰æ¡†æ¶  
DSCLæ˜¯é¦–ä¸ªä¸“ä¸ºå·¥å…·å­¦ä¹ ç‹¬ç‰¹ç‰¹æ€§è®¾è®¡çš„åŠ¨æ€é‡‡æ ·æ–¹æ³•ï¼Œé’ˆå¯¹å·¥å…·å­¦ä¹ å¤šç›¸äº’ä¾èµ–å­ä»»åŠ¡ã€å¤šå€¼å¥–åŠ±å‡½æ•°çš„ç‰¹ç‚¹ï¼Œæ¥æå‡è®­ç»ƒæ•ˆç‡ä¸æ¨¡å‹æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šReward - Based Dynamic Samplingï¼ˆRDSï¼‰  
åˆ©ç”¨å¥–åŠ±çš„å‡å€¼å’Œæ–¹å·®ç­‰å¤šç»´å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯æ¥ä¼˜å…ˆé€‰æ‹©æœ‰ä»·å€¼æ•°æ®ã€‚é€šè¿‡åŠ¨æ€è·Ÿè¸ªä¸€ç»„rolloutsçš„å¥–åŠ±å‡å€¼ã€æ–¹å·®ä»¥åŠæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„å¹³å‡å¥–åŠ±æ–¹å·®è¿™ä¸‰ä¸ªç»´åº¦ï¼Œä¸ä»…èƒ½è¯„ä¼°æ ·æœ¬çš„ç¬æ—¶éš¾åº¦å’Œç¨³å®šæ€§ï¼Œè¿˜èƒ½è€ƒé‡å…¶åœ¨å­¦ä¹ å†å²ä¸­çš„æ¼”åŒ–è½¨è¿¹ï¼Œå®ç°æ›´é«˜æ•ˆä¸”å…·æ¢ç´¢æ€§çš„è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šTask - Based Dynamic Curriculum Learningï¼ˆTDCLï¼‰  
ä¾æ®æ ·æœ¬è®­ç»ƒçŠ¶æ€ï¼Œè‡ªé€‚åº”åœ°å°†è®­ç»ƒé‡ç‚¹æ”¾åœ¨æŒæ¡ä¸è¶³çš„å­ä»»åŠ¡ä¸Šï¼Œåˆ©ç”¨å­ä»»åŠ¡ä¾èµ–æ„å»ºä¸‰é˜¶æ®µè¯¾ç¨‹ï¼Œé€šè¿‡åœ¨rolloutsä¹‹é—´æä¾›å¤šæ ·çš„æ–¹å·®æ¥è¾…åŠ©æ¨¡å‹è®­ç»ƒï¼Œå¹³è¡¡å­ä»»åŠ¡é—´çš„æ ·æœ¬åˆ†å¸ƒï¼Œé¿å…è¿‡åº¦å…³æ³¨å·²å­¦å¥½éƒ¨åˆ†è€Œå¿½è§†å¾…æ”¹è¿›éƒ¨åˆ†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¤§é‡å®éªŒå°†DSCLä¸å¼ºåŸºçº¿å¯¹æ¯”ï¼Œç»“æœè¡¨æ˜DSCLæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ä¸æ¨¡å‹æ€§èƒ½ï¼Œåœ¨BFCLv3åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†3.29%çš„æ€§èƒ½æå‡ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡ç‰¹æ€§å®šåˆ¶æ–¹æ³•ï¼šå½“é¢†åŸŸä»»åŠ¡ï¼ˆå¦‚å·¥å…·å­¦ä¹ ï¼‰å­˜åœ¨ç‰¹æ®Šç»“æ„ï¼ˆå¤šå­ä»»åŠ¡ï¼‰ä¸æœºåˆ¶ï¼ˆç»†ç²’åº¦å¥–åŠ±ï¼‰æ—¶ï¼Œå¯åƒæœ¬æ–‡é’ˆå¯¹å·¥å…·å­¦ä¹ é‚£æ ·ï¼Œæ·±å…¥åˆ†æç‰¹æ€§åè®¾è®¡é€‚é…æ–¹æ³•ï¼Œè€Œéç›´æ¥å¥—ç”¨é€šç”¨æŠ€æœ¯ã€‚
2. å¤šç»´ä¿¡æ¯åˆ©ç”¨ä¸åŠ¨æ€è°ƒæ•´æ€è·¯ï¼šåœ¨é‡‡æ ·æˆ–è®­ç»ƒç­–ç•¥è®¾è®¡ä¸­ï¼Œå¯å€Ÿé‰´RDSåˆ©ç”¨å¤šç»´ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚å‡å€¼ã€æ–¹å·®ï¼‰ä»¥åŠTDCLä¾æ®ä»»åŠ¡çŠ¶æ€åŠ¨æ€è°ƒæ•´çš„æ€è·¯ï¼Œè®©æ–¹æ³•æ›´è´´åˆä»»åŠ¡è¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚
3. è¯¾ç¨‹å­¦ä¹ ä¸å­ä»»åŠ¡å¤„ç†ï¼šå¯¹äºå­˜åœ¨å¤šå­ä»»åŠ¡ä¸”å­ä»»åŠ¡æœ‰ä¾èµ–æˆ–æ”¶æ•›å¼‚æ­¥æƒ…å†µçš„ä»»åŠ¡ï¼ŒTDCLåŸºäºå­ä»»åŠ¡è®­ç»ƒçŠ¶æ€æ„å»ºè¯¾ç¨‹çš„æ–¹å¼ï¼Œä¸ºå¹³è¡¡å­ä»»åŠ¡å­¦ä¹ ã€æå‡æ•´ä½“æ€§èƒ½æä¾›äº†å‚è€ƒèŒƒå¼ã€‚

## provable-benefits-of-in-tool-learning-for-large-language-models
### Abstract
Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹ä¸­å·¥å…·å†…å­¦ä¹ çš„å¯è¯æ˜ä¼˜åŠ¿ï¼šç†è®ºä¸å®è¯æ­ç¤ºå·¥å…·å¢å¼ºçš„å¿…è¦æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£ä»é™æ€é¢„æµ‹å™¨å‘åŠ¨æ€ã€å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„ç³»ç»Ÿæ¼”å˜ï¼Œå·¥å…·å¢å¼ºå‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ç»“åˆæ£€ç´¢ã€å†…å­˜æˆ–å¤–éƒ¨APIï¼‰æ­£åœ¨é‡å¡‘AIé¢†åŸŸï¼Œä½†è¿™ç±»æ¨¡å‹çš„ç†è®ºä¼˜åŠ¿å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ ¸å¿ƒé—®é¢˜åœ¨äºï¼šæ¨¡å‹è·å–å’Œåˆ©ç”¨çŸ¥è¯†çš„æœ€æœ‰æ•ˆæ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯é€šè¿‡å‚æ•°æ›´æ–°å†…åŒ–äº‹å®ï¼ˆæƒé‡å†…å­¦ä¹ ï¼‰ï¼Œè¿˜æ˜¯å­¦ä¹ è®¿é—®å’Œæ“ä½œå¤–éƒ¨çœŸå®æºï¼ˆå·¥å…·å†…å­¦ä¹ ï¼‰ï¼Ÿå‰è€…å—æ¨¡å‹å‚æ•°å®¹é‡é™åˆ¶ä¸”æ˜“é—å¿˜ï¼Œåè€…åˆ™æœ‰å¼€æ”¾å¼çŸ¥è¯†è®¿é—®ç­‰æ½œåŠ›ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç†è®ºä¸å®è¯åˆ†æï¼Œé˜æ˜å·¥å…·å¢å¼ºæ–¹æ³•ç›¸è¾ƒä¼ ç»Ÿå•ä½“æ¨¡å‹æ›´ä¼˜çš„åŸå› ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¨å¯¼æƒé‡å†…å­¦ä¹ çš„ç†è®ºä¸‹é™  
ä»ç†è®ºä¸Šæ¨å¯¼å¾—å‡ºï¼Œæ¨¡å‹ä»…é€šè¿‡æƒé‡èƒ½å­˜å‚¨çš„ä¸åŒäº‹å®æ•°é‡ï¼Œæœ¬è´¨ä¸Šå—å…¶å‚æ•°æ•°é‡é™åˆ¶ï¼Œå‡¸æ˜¾äº†å•çº¯ä¾èµ–æƒé‡å†…è®°å¿†å­˜åœ¨ç»“æ„æ€§ç“¶é¢ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå·¥å…·å¢å¼ºæ¨¡å‹çš„æ˜¾å¼ä¸Šé™ä¸ç”µè·¯æ„é€   
è¯æ˜å·¥å…·å¢å¼ºå‹æ¨¡å‹åŸåˆ™ä¸Šå¯é€šè¿‡å­¦ä¹ ä¸å¤–éƒ¨æ•°æ®åº“äº¤äº’ï¼Œå®ç°æ— ç•Œçš„äº‹å®å¬å›ã€‚å€ŸåŠ©å½¢å¼åŒ–çš„ç”µè·¯æ„é€ ï¼Œå±•ç¤ºäº†è¿™ç§å·¥å…·ä½¿ç”¨æ–¹å¼çš„å¯è¡Œæ€§ä¸é«˜æ•ˆæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ§åˆ¶å®éªŒéªŒè¯ç†è®º & ç°æœ‰LLMå®è·µå¯ç¤º  
åœ¨å—æ§å®éªŒç¯å¢ƒä¸‹ï¼Œè®©æ¨¡å‹ä»æ— åˆ°æœ‰è®­ç»ƒä»¥è®°å¿†äº‹å®æˆ–å­¦ä¹ ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå®è¯éªŒè¯äº†ç†è®ºé¢„æµ‹çš„ç¼©æ”¾å®šå¾‹ä¸è®°å¿†é™åˆ¶ï¼›åŒæ—¶é’ˆå¯¹ç°æœ‰é¢„è®­ç»ƒLLMï¼Œè¡¨æ˜æ•™æ¨¡å‹ä½¿ç”¨å·¥å…·å’Œé€šç”¨è§„åˆ™ï¼Œæ¯”å°†äº‹å®å¾®è°ƒè¿›å†…å­˜æ›´æœ‰æ•ˆï¼Œä¸ºæœªæ¥LLMå¼€å‘æ–¹å‘æä¾›æŒ‡å¼•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å—æ§å®éªŒä¸­ï¼Œå­¦ä¹ ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„æ¨¡å‹åœ¨äº‹å®å¬å›ä»»åŠ¡ä¸Šï¼ŒæŒç»­è¶…è¶Šå•çº¯ä¾èµ–è®°å¿†ï¼ˆæƒé‡å†…å­¦ä¹ ï¼‰çš„æ¨¡å‹ï¼›ä¸”å¯¹äºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œæ•™å…¶å·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œé€šç”¨è§„åˆ™ï¼Œåœ¨å­¦ä¹ æ–°äº‹å®ç­‰åœºæ™¯ä¸‹ï¼Œæ•ˆæœæ˜¾è‘—ä¼˜äºæŠŠäº‹å®å¾®è°ƒè¿›æ¨¡å‹å†…å­˜çš„æ–¹å¼ï¼Œä»å®è¯è§’åº¦æ”¯æ’‘äº†å·¥å…·å¢å¼ºåœ¨å¯æ‰©å±•æ€§ç­‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ç†è®ºå±‚é¢ï¼Œä¸ºç†è§£å·¥å…·å¢å¼ºä¸ºä½•æ›´å…·ä¼˜åŠ¿æä¾›äº†æ¦‚å¿µä¸å®è¯åŸºç¡€ï¼Œæ˜ç¡®æƒé‡å†…å­¦ä¹ çš„ç“¶é¢ˆä¸å·¥å…·å†…å­¦ä¹ çš„æ— ç•Œæ½œåŠ›ï¼›å®è·µå±‚é¢ï¼Œå¯ç¤ºæœªæ¥LLMå¼€å‘åº”ä»è®­ç»ƒè¶…å¤§å•ä½“æ¨¡å‹ï¼Œè½¬å‘æ„å»ºèƒ½å­¦ä¹ æŸ¥è¯¢ï¼ˆè€Œéä»…å­˜å‚¨ä¿¡æ¯ï¼‰çš„æ¨¡å—åŒ–ç³»ç»Ÿï¼›åŒæ—¶å¼€æºä»£ç åº“ï¼Œä¸ºç ”ç©¶è€…æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹å†…å­˜è´Ÿè½½ç­‰é—®é¢˜æä¾›äº†ä¾¿åˆ©ï¼Œæ¨åŠ¨è¯¥æ–¹å‘ç ”ç©¶è¿›ä¸€æ­¥å‘å±•ã€‚ 

## beyond-self-regulated-learning-processes--unveiling-hidden-tactics-in-generative-ai-assisted-writing
### Abstract
The integration of Generative AI (GenAI) into education is reshaping how students learn, making self-regulated learning (SRL) - the ability to plan, monitor, and adapt one's learning - more important than ever. To support learners in these new contexts, it is essential to understand how SRL unfolds during interaction with GenAI tools. Learning analytics offers powerful techniques for analyzing digital trace data to infer SRL behaviors. However, existing approaches often assume SRL processes are linear, segmented, and non-overlapping-assumptions that overlook the dynamic, recursive, and non-linear nature of real-world learning. We address this by conceptualizing SRL as a layered system: observable learning patterns reflect hidden tactics (short, purposeful action states), which combine into broader SRL strategies. Using Hidden Markov Models (HMMs), we analyzed trace data from higher education students engaged in GenAI-assisted academic writing. We identified three distinct groups of learners, each characterized by different SRL strategies. These groups showed significant differences in performance, indicating that students' use of different SRL strategies in GenAI-assisted writing led to varying task outcomes. Our findings advance the methodological toolkit for modeling SRL and inform the design of adaptive learning technologies that more effectively support learners in GenAI-enhanced educational environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”Ÿæˆå¼AIè¾…åŠ©å†™ä½œä¸‹ï¼Œæ­ç§˜è‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ çš„éšè—ç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰èå…¥æ•™è‚²æ­£é‡å¡‘å­¦ç”Ÿå­¦ä¹ æ–¹å¼ï¼Œè‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ ï¼ˆSRLï¼Œå³è§„åˆ’ã€ç›‘æ§ä¸è°ƒæ•´è‡ªèº«å­¦ä¹ çš„èƒ½åŠ›ï¼‰å˜å¾—æ„ˆå‘å…³é”®ã€‚äº†è§£SRLåœ¨ä¸GenAIå·¥å…·äº¤äº’æ—¶å¦‚ä½•å±•å¼€ï¼Œå¯¹æ”¯æŒæ–°åœºæ™¯ä¸‹çš„å­¦ä¹ è€…è‡³å…³é‡è¦ã€‚å­¦ä¹ åˆ†ææŠ€æœ¯å¯é€šè¿‡æ•°å­—ç—•è¿¹æ•°æ®æ¨æ–­SRLè¡Œä¸ºï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å‡è®¾SRLè¿‡ç¨‹æ˜¯çº¿æ€§ã€åˆ†æ®µä¸”ä¸é‡å çš„ï¼Œå¿½ç•¥äº†çœŸå®å­¦ä¹ åŠ¨æ€ã€é€’å½’å’Œéçº¿æ€§çš„æœ¬è´¨ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ›´ç²¾å‡†åœ°å»ºæ¨¡SRLå¹¶ä¸ºè‡ªé€‚åº”å­¦ä¹ æŠ€æœ¯è®¾è®¡æä¾›å‚è€ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSRLåˆ†å±‚ç³»ç»Ÿæ¦‚å¿µåŒ–  
å°†SRLæ¦‚å¿µåŒ–ä¸ºåˆ†å±‚ç³»ç»Ÿï¼Œæå‡ºå¯è§‚æµ‹å­¦ä¹ æ¨¡å¼åæ˜ éšè—æˆ˜æœ¯ï¼ˆçŸ­æœŸã€æœ‰ç›®çš„çš„è¡ŒåŠ¨çŠ¶æ€ï¼‰ï¼Œéšè—æˆ˜æœ¯å†ç»„åˆæˆæ›´å¹¿æ³›çš„SRLç­–ç•¥ï¼Œä»¥æ­¤æ¥å¥‘åˆçœŸå®å­¦ä¹ éçº¿æ€§ç­‰ç‰¹æ€§ï¼Œå¼¥è¡¥ç°æœ‰æ–¹æ³•å¯¹SRLå¤æ‚æœ¬è´¨åˆ»ç”»ä¸è¶³çš„é—®é¢˜ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå€ŸåŠ©éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMsï¼‰åˆ†ææ•°æ®  
è¿ç”¨éšé©¬å°”å¯å¤«æ¨¡å‹åˆ†æé«˜ç­‰æ•™è‚²å­¦ç”Ÿåœ¨GenAIè¾…åŠ©å­¦æœ¯å†™ä½œä¸­çš„ç—•è¿¹æ•°æ®ï¼Œé€šè¿‡è¯¥æ¨¡å‹æ•æ‰éšè—æˆ˜æœ¯ï¼ˆä½œä¸ºæ½œåœ¨çŠ¶æ€ï¼‰ï¼Œè¿›è€Œè¯†åˆ«ä¸åŒSRLç­–ç•¥ï¼Œçªç ´ä¼ ç»Ÿæ–¹æ³•å‡è®¾å±€é™ï¼Œæ›´é€‚é…å«GenAIåœºæ™¯ä¸‹SRLè¿‡ç¨‹åˆ†æã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åˆ†æå­¦ç”ŸGenAIè¾…åŠ©å†™ä½œçš„ç—•è¿¹æ•°æ®åï¼Œè¯†åˆ«å‡ºä¸‰ç±»å…·æœ‰ä¸åŒSRLç­–ç•¥ç‰¹å¾çš„å­¦ä¹ è€…ç¾¤ä½“ï¼Œä¸”è¿™äº›ç¾¤ä½“åœ¨è¡¨ç°ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¯´æ˜å­¦ç”Ÿåœ¨GenAIè¾…åŠ©å†™ä½œä¸­ä½¿ç”¨ä¸åŒSRLç­–ç•¥ä¼šå¯¼è‡´ä¸åŒä»»åŠ¡ç»“æœã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ–¹æ³•è®ºå±‚é¢ï¼Œæ¨è¿›äº†SRLå»ºæ¨¡çš„å·¥å…·åº“ï¼Œä¸ºåç»­ç ”ç©¶æä¾›æ›´è´´åˆçœŸå®å­¦ä¹ å¤æ‚æ€§çš„åˆ†ææ€è·¯ï¼›å®è·µå±‚é¢ï¼Œä¸ºGenAIå¢å¼ºæ•™è‚²ç¯å¢ƒä¸‹è‡ªé€‚åº”å­¦ä¹ æŠ€æœ¯è®¾è®¡æä¾›ä¾æ®ï¼Œå¸®åŠ©æ‰“é€ æ›´æœ‰æ•ˆæ”¯æŒå­¦ä¹ è€…çš„æŠ€æœ¯ï¼›åŒæ—¶ä¹Ÿç»™ä»ä¸šè€…å’Œç ”ç©¶è€…å¯ç¤ºï¼Œå¦‚ä»ä¸šè€…éœ€å…³æ³¨å­¦ç”Ÿå¯¹GenAIè¿‡åº¦ä¾èµ–é£é™©ï¼Œç ”ç©¶è€…è¦é‡è§†SRLå¤æ‚æœ¬è´¨ä»¥è·å–æ›´ç»†è‡´æ´è§ç­‰ã€‚ 

## losemb--logic-guided-semantic-bridging-for-inductive-tool-retrieval
### Abstract
Tool learning has emerged as a promising paradigm for large language models (LLMs) to solve many real-world tasks. Nonetheless, with the tool repository rapidly expanding, it is impractical to contain all tools within the limited input length of LLMs. To alleviate these issues, researchers have explored incorporating a tool retrieval module to select the most relevant tools or represent tools as unique tokens within LLM parameters. However, most state-of-the-art methods are under transductive settings, assuming all tools have been observed during training. Such a setting deviates from reality as the real-world tool repository is evolving and incorporates new tools frequently. When dealing with these unseen tools, which refer to tools not encountered during the training phase, these methods are limited by two key issues, including the large distribution shift and the vulnerability of similarity-based retrieval. To this end, inspired by human cognitive processes of mastering unseen tools through discovering and applying the logical information from prior experience, we introduce a novel Logic-Guided Semantic Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to mine and transfer latent logical information for inductive tool retrieval without costly retraining. Specifically, LoSemB contains a logic-based embedding alignment module to mitigate distribution shifts and implements a relational augmented retrieval mechanism to reduce the vulnerability of similarity-based retrieval. Extensive experiments demonstrate that LoSemB achieves advanced performance in inductive settings while maintaining desirable effectiveness in the transductive setting.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LoSemBï¼šé¢å‘å½’çº³å¼å·¥å…·æ£€ç´¢çš„é€»è¾‘å¼•å¯¼è¯­ä¹‰æ¡¥æ¥æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨è¯¸å¤šä»»åŠ¡å±•ç°å¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚è®¡ç®—ã€å®æ—¶ä¿¡æ¯è·å–ç­‰åœºæ™¯å­˜åœ¨ä¸è¶³ï¼Œå·¥å…·å­¦ä¹ æˆä¸ºæ‰©å±•LLMsèƒ½åŠ›çš„é‡è¦èŒƒå¼ã€‚ç„¶è€Œå·¥å…·åº“å¿«é€Ÿæ‰©å¼ ï¼ŒLLMsæœ‰é™çš„è¾“å…¥é•¿åº¦éš¾ä»¥å®¹çº³æ‰€æœ‰å·¥å…·ï¼Œä¸ºæ­¤å‡ºç°åŸºäºTokenå’ŒåŸºäºæ£€ç´¢çš„ä¸¤ç±»å·¥å…·å¤„ç†æ–¹æ³•ã€‚ä½†ç°æœ‰ä¸»æµæ–¹æ³•å¤šå¤„äºç›´æ¨å¼ï¼ˆtransductiveï¼‰è®¾ç½®ï¼Œå‡è®¾è®­ç»ƒæ—¶å·²è§è¿‡æ‰€æœ‰å·¥å…·ï¼Œè€Œç°å®ä¸­å·¥å…·åº“ä¸æ–­æ›´æ–°æ–°å¢å·¥å…·ï¼ˆ unseen tools ï¼‰ã€‚å¤„ç†è¿™äº›æœªè§è¿‡çš„å·¥å…·æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤å¤§å…³é”®é—®é¢˜ï¼šä¸€æ˜¯åˆ†å¸ƒåç§»å¤§ï¼ˆ unseen tools åŠŸèƒ½å¤šæ ·æ€§å’Œå‚æ•°æ•æ„Ÿæ€§å¯¼è‡´è®­ç»ƒæ—¶å­¦ä¹ çš„è¡¨ç¤ºæ— æ³•æ•æ‰å…¶çœŸå®åŠŸèƒ½ï¼‰ï¼›äºŒæ˜¯åŸºäºç›¸ä¼¼åº¦çš„æ£€ç´¢é²æ£’æ€§å·®ï¼ˆä»…ä¾èµ–æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œå¯¹è¡¨ç¤ºè´¨é‡æ•æ„Ÿï¼Œæ³›åŒ–åˆ° unseen tools æ—¶æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼‰ã€‚å—äººç±»é€šè¿‡å·²æœ‰ç»éªŒé€»è¾‘ä¿¡æ¯æŒæ¡æ–°å·¥å…·çš„è®¤çŸ¥è¿‡ç¨‹å¯å‘ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºæ— éœ€æ˜‚è´µé‡è®­ç»ƒå°±èƒ½æŒ–æ˜å’Œè¿ç§»æ½œåœ¨é€»è¾‘ä¿¡æ¯ä»¥å®ç°å½’çº³å¼å·¥å…·æ£€ç´¢çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºé€»è¾‘å¼•å¯¼çš„è¯­ä¹‰æ¡¥æ¥æ¡†æ¶LoSemB  
LoSemB é¢å‘å½’çº³å¼å·¥å…·æ£€ç´¢åœºæ™¯ï¼Œç›®æ ‡æ˜¯åœ¨ä¸è¿›è¡Œé«˜æˆæœ¬é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒæŒ–æ˜å’Œè¿ç§»æ½œåœ¨é€»è¾‘ä¿¡æ¯æ¥æå‡å¯¹ unseen tools æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚æ•´ä½“æ¡†æ¶å›´ç»•è§£å†³åˆ†å¸ƒåç§»å’ŒåŸºäºç›¸ä¼¼åº¦æ£€ç´¢çš„è„†å¼±æ€§å±•å¼€è®¾è®¡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºé€»è¾‘çš„åµŒå…¥å¯¹é½æ¨¡å—  
è¯¥æ¨¡å—å°†é€»è¾‘ç‰¹å¾èå…¥ unseen tools çš„è¡¨ç¤ºä¸­ï¼Œä»¥æ­¤ç¼“è§£åˆ†å¸ƒåç§»é—®é¢˜ã€‚é€šè¿‡æŒ–æ˜å·¥å…·é—´ã€å·¥å…·ä¸ä½¿ç”¨åœºæ™¯ç­‰é€»è¾‘å…³ç³»ï¼ŒæŠŠè¿™äº›é€»è¾‘ä¿¡æ¯æ•´åˆåˆ°å·¥å…·çš„åµŒå…¥è¡¨ç¤ºé‡Œï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒåé¢å¯¹æ–°å·¥å…·æ—¶ï¼Œå…¶è¡¨ç¤ºèƒ½æ›´è´´åˆçœŸå®åŠŸèƒ½ï¼Œå‡å°‘å› å·¥å…·åˆ†å¸ƒå˜åŒ–å¸¦æ¥çš„æ£€ç´¢æ€§èƒ½ä¸‹é™ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå…³ç³»å¢å¼ºçš„æ£€ç´¢æœºåˆ¶  
åœ¨ç»è¿‡é€»è¾‘å¢å¼ºçš„åµŒå…¥è¡¨ç¤ºåŸºç¡€ä¸Šï¼ŒLoSemB é‡‡ç”¨å…³ç³»å¢å¼ºæ£€ç´¢æœºåˆ¶ã€‚è¯¥æœºåˆ¶åŒæ—¶åˆ©ç”¨é€»è¾‘çº¦æŸå’ŒåµŒå…¥ç›¸ä¼¼åº¦æ¥è¿›è¡Œæ£€ç´¢ï¼Œä¸å†å•çº¯ä¾èµ–æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œä»¥æ­¤å…‹æœåŸºäºç›¸ä¼¼åº¦æ£€ç´¢çš„è„†å¼±æ€§ï¼Œè®©æ£€ç´¢è¿‡ç¨‹æ›´é²æ£’ã€å‡†ç¡®ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLoSemB åœ¨å½’çº³å¼ï¼ˆ inductive ï¼‰è®¾ç½®ä¸‹å®ç°äº†å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„å·¥å…·ï¼›åŒæ—¶åœ¨ç›´æ¨å¼ï¼ˆ transductive ï¼‰è®¾ç½®ä¸‹ä¹Ÿä¿æŒäº†è‰¯å¥½çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ä¸ä¼˜è¶Šæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜æ´å¯Ÿè§’åº¦ï¼šå…³æ³¨åˆ°ç°å®ä¸­å·¥å…·åº“åŠ¨æ€æ›´æ–°åœºæ™¯ä¸‹ç°æœ‰æ£€ç´¢æ–¹æ³•çš„ä¸è¶³ï¼Œä»åˆ†å¸ƒåç§»å’Œæ£€ç´¢é²æ£’æ€§ä¸¤æ–¹é¢ç²¾å‡†å‰–æç—›ç‚¹ï¼Œè¿™ç§å¯¹çœŸå®åœºæ™¯é—®é¢˜çš„æ·±å…¥æŒ–æ˜æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚  
2. è®¤çŸ¥å¯å‘è®¾è®¡ï¼šä»äººç±»æŒæ¡æ–°å·¥å…·çš„è®¤çŸ¥è¿‡ç¨‹è·å–çµæ„Ÿï¼Œå°†é€»è¾‘ä¿¡æ¯å¼•å…¥å·¥å…·æ£€ç´¢ä»»åŠ¡ï¼Œä¸ºè§£å†³æ¨¡å‹æ³›åŒ–åˆ° unseen æ•°æ®é—®é¢˜æä¾›äº†ä»äººç±»æ™ºèƒ½ä¸­æ±²å–æ€è·¯çš„èŒƒä¾‹ã€‚  
3. æ¨¡å—åˆ›æ–°æ€è·¯ï¼šè®¾è®¡çš„åŸºäºé€»è¾‘çš„åµŒå…¥å¯¹é½å’Œå…³ç³»å¢å¼ºæ£€ç´¢æœºåˆ¶ï¼Œåˆ†åˆ«é’ˆå¯¹æ€§è§£å†³åˆ†å¸ƒåç§»ä¸ç›¸ä¼¼åº¦æ£€ç´¢è„†å¼±æ€§é—®é¢˜ï¼Œè¿™ç§æ¨¡å—åŒ–ä¸”ç›®æ ‡æ˜ç¡®çš„æ¨¡å‹è®¾è®¡æ–¹å¼ï¼Œå¯¹å¤„ç†æœ‰åˆ†å¸ƒå˜åŒ–å’Œé²æ£’æ€§è¦æ±‚çš„æ£€ç´¢ç±»ä»»åŠ¡å…·æœ‰å‚è€ƒä»·å€¼ã€‚

## metaagent--toward-self-evolving-agent-via-tool-meta-learning
### Abstract
In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MetaAgentï¼šå·¥å…·å…ƒå­¦ä¹ é©±åŠ¨çš„è‡ªè¿›åŒ–æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“ä¸‹ï¼Œç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„ä¿¡æ¯æŸ¥è¯¢ç³»ç»Ÿè™½èƒ½åº”å¯¹ä¸å°‘åŸºç¡€ä¿¡æ¯éœ€æ±‚ï¼Œä½†åœ¨å¤æ‚çŸ¥è¯†å‘ç°ä»»åŠ¡ï¼ˆéœ€å¤šæ­¥æ¨ç†ã€å¤–éƒ¨å·¥å…·äº¤äº’æ•´åˆä¿¡æ¯ï¼‰ä¸Šè¡¨ç°æ¬ ä½³ã€‚ç°æœ‰æ™ºèƒ½ä½“å®ç°æ–¹æ¡ˆä¹Ÿå­˜åœ¨å±€é™ï¼šä¸€æ˜¯æ‰‹åŠ¨è®¾è®¡ç‰¹å®šä»»åŠ¡å·¥ä½œæµï¼Œä¾èµ–äººå·¥ä¸”çµæ´»æ€§å·®ï¼›äºŒæ˜¯ç«¯åˆ°ç«¯è®­ç»ƒLLMsåšæ¨ç†ä¸å·¥å…·ä½¿ç”¨ï¼Œæ•°æ®è·å–éš¾ã€æ˜“å—è®­ç»ƒåå·®å½±å“ä¸”æ³›åŒ–åä»»åŠ¡è¡¨ç°æ˜“ä¸‹æ»‘ã€‚ä¸ºçªç ´è¿™äº›å›°å¢ƒï¼Œè®ºæ–‡æå‡ºMetaAgentè¿™ä¸€æ™ºèƒ½ä½“èŒƒå¼ï¼ŒæœŸæœ›ä»¥â€œåšä¸­å­¦â€å®ç°æŒç»­è‡ªè¿›åŒ–ï¼Œé«˜æ•ˆå¤„ç†æ·±åº¦çŸ¥è¯†å‘ç°ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæç®€åˆå§‹å·¥ä½œæµä¸è‡ªé€‚åº”æ±‚åŠ©æœºåˆ¶  
MetaAgentåˆå§‹ä»…é…å¤‡è‡ªä¸»æ¨ç†å’Œè‡ªé€‚åº”æ±‚åŠ©ä¸¤å¤§æ ¸å¿ƒèƒ½åŠ›ã€‚é¢å¯¹å¤æ‚ä»»åŠ¡æ—¶ï¼Œå…ˆè‡ªä¸»åˆ†æ­¥æ¨ç†ï¼Œå½“é‡åˆ°çŸ¥è¯†ç¼ºå£ï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ±‚åŠ©è¯·æ±‚ï¼Œå†ç”±ä¸“é—¨çš„å·¥å…·è·¯ç”±å™¨å°†è¯·æ±‚è·¯ç”±åˆ°æœ€é€‚é…çš„å¤–éƒ¨å·¥å…·æ‰§è¡Œï¼Œä»¥æ­¤åœ¨ä¿æŒæ ¸å¿ƒç®€æ´çš„åŒæ—¶ï¼Œè¦†ç›–å¤šæ ·ä¿¡æ¯æŸ¥è¯¢ä¸æ·±åº¦çŸ¥è¯†å‘ç°åœºæ™¯ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå…ƒå·¥å…·å­¦ä¹ çš„æŒç»­è¿›åŒ–  
å€Ÿé‰´å…ƒè®¤çŸ¥ç†è®ºï¼ŒMetaAgentåœ¨å®Œæˆæ¯ä¸ªä»»åŠ¡åä¼šå¼€å±•è‡ªæˆ‘åæ€ä¸ç­”æ¡ˆéªŒè¯ã€‚åæ€ä¸­ä¸ä»…æ£€æŸ¥ç­”æ¡ˆå‡†ç¡®æ€§ï¼Œè¿˜ä¼šå‰–ææ¨ç†æ¨¡å¼ã€å·¥å…·é€‰æ‹©ä½¿ç”¨æ•ˆæœã€å¤–éƒ¨ä¿¡æ¯æ•´åˆæ–¹å¼ç­‰ï¼Œæç‚¼å¯å¤ç”¨ç»éªŒï¼ˆå¦‚è¯†åˆ«å¸¸è§è®¤çŸ¥åå·®ã€æˆåŠŸå†³ç­–ç­–ç•¥ç­‰ï¼‰å¹¶åŠ¨æ€èå…¥åç»­ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼›åŒæ—¶ï¼Œæ¢³ç†ä¸å·¥å…·è·¯ç”±å™¨çš„äº¤äº’å†å²ï¼Œæ„å»ºå†…éƒ¨çŸ¥è¯†åº“ä¸å·¥å…·ï¼ŒæŒç»­ä¼˜åŒ–æ¨ç†å’Œå·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œæ•´ä¸ªè¿‡ç¨‹æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–é¢å¤–å†è®­ç»ƒï¼Œå®ç°â€œæ•°æ®é©±åŠ¨å¼â€è‡ªè¿›åŒ–ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GAIAã€WebWalkerQAã€BrowseCompç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„çŸ¥è¯†å‘ç°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaAgentæŒç»­è¶…è¶ŠåŸºäºå·¥ä½œæµçš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è¡¨ç°ä¸Šèƒ½åŒ¹é…ç”šè‡³è¶…è¿‡ç«¯åˆ°ç«¯è®­ç»ƒçš„æ™ºèƒ½ä½“ï¼Œå……åˆ†éªŒè¯äº†è¿™ç§è‡ªè¿›åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é²æ£’ã€é€šç”¨çŸ¥è¯†å‘ç°ä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æŠ€æœ¯è®¾è®¡çœ‹ï¼Œâ€œæç®€åˆå§‹+æ•°æ®é©±åŠ¨è¿›åŒ–â€çš„æ€è·¯ä¸ºæ™ºèƒ½ä½“å¼€å‘æä¾›äº†è½»é‡ä¸”çµæ´»çš„èŒƒå¼ï¼Œæ‘†è„±å¯¹å¤§é‡æ ‡æ³¨æ•°æ®æˆ–å¼ºäººå·¥é¢„è®¾çš„è¿‡åº¦ä¾èµ–ï¼›è‡ªæˆ‘åæ€ä¸ç»éªŒæç‚¼æœºåˆ¶ï¼Œä¸ºæ™ºèƒ½ä½“æŒç»­å­¦ä¹ ã€è·¨ä»»åŠ¡æ³›åŒ–æä¾›äº†å¯å‚è€ƒçš„å®ç°è·¯å¾„ï¼›åœ¨å·¥å…·äº¤äº’ä¸å†…éƒ¨çŸ¥è¯†åº“æ„å»ºæ–¹é¢ï¼Œä¹Ÿå±•ç¤ºäº†å¦‚ä½•é€šè¿‡å†å²ç®¡ç†æ¥æ²‰æ·€èƒ½åŠ›ï¼Œè¿™äº›è®¾è®¡ç†å¿µå’Œæ¨¡å—æ¶æ„ï¼Œå¯¹åç»­æ‰“é€ æ›´æ™ºèƒ½ã€è‡ªé€‚åº”çš„AIç³»ç»Ÿå…·æœ‰é‡è¦å€Ÿé‰´ä»·å€¼ã€‚ 

## critictool--evaluating-self-critique-capabilities-of-large-language-models-in-tool-calling-error-scenarios
### Abstract
The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CRITICTOOLï¼šè¯„ä¼°å¤§æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨é”™è¯¯åœºæ™¯ä¸‹çš„è‡ªæˆ‘æ‰¹åˆ¤èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ©ç”¨å¤–éƒ¨å·¥å…·æ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›ä½¿å…¶èƒ½åº”å¯¹æ›´å¤æ‚å¤šæ ·çš„ä»»åŠ¡ï¼Œä½†éšç€ä»»åŠ¡å¤æ‚åº¦å’Œæ—¶é—´è·¨åº¦å¢åŠ ï¼Œå·¥å…·è°ƒç”¨è¿‡ç¨‹æ˜“å‡ºç°æ„å¤–é”™è¯¯ã€‚ç°æœ‰è¯„ä¼°å¤šèšç„¦ç»“æœæˆ–å•å·¥å…·åœºæ™¯ï¼Œå¿½ç•¥é”™è¯¯å¤„ç†ï¼ˆè¯†åˆ«ã€è¯Šæ–­ã€æ¢å¤ï¼‰ï¼Œéš¾ä»¥å‡†ç¡®è¯„ä¼°æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚å› æ­¤ï¼Œæ„å»ºèƒ½è¯„ä¼°å¤§æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨é”™è¯¯åœºæ™¯ä¸‹è‡ªæˆ‘æ‰¹åˆ¤èƒ½åŠ›çš„åŸºå‡†è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢åˆ†æå·¥å…·è°ƒç”¨é”™è¯¯ç±»å‹  
å¯¹å¤šä¸ªä¸»æµå·¥å…·è¯„ä¼°åŸºå‡†ä¸­å‡½æ•°è°ƒç”¨è¿‡ç¨‹çš„é”™è¯¯ç±»å‹å±•å¼€æ·±å…¥åˆ†æï¼Œå°†é”™è¯¯ä»æ¥æºä¸Šåˆ†ä¸ºæ¨¡å‹å†…éƒ¨é©±åŠ¨é”™è¯¯ä¸å¤–éƒ¨ç¯å¢ƒé”™è¯¯ï¼Œè¿˜ç»†åˆ†å‡ºå·¥å…·é€‰æ‹©é”™è¯¯ã€å·¥å…·å¹»è§‰é”™è¯¯ã€å‚æ•°é”®é”™è¯¯ã€å‚æ•°å€¼é”™è¯¯ã€ç¯å¢ƒé”™è¯¯ç­‰å…·ä½“æ¨¡å¼ï¼Œä¸ºåç»­è¯„ä¼°æä¾›åŸºç¡€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºCRITICTOOLåŸºå‡†  
æ„å»ºé¦–ä¸ªé’ˆå¯¹å¤§æ¨¡å‹å·¥å…·ä½¿ç”¨çš„è‡ªæˆ‘æ‰¹åˆ¤è¯„ä¼°åŸºå‡†CRITICTOOLã€‚åŒºåˆ«äºä»¥å¾€ç»“æœå¯¼å‘è¯„ä¼°ï¼Œä»å¤šç»´åº¦è¯„ä¼°æ¨¡å‹ï¼Œæ¶µç›–å¯¹å†…éƒ¨é”™è¯¯çš„åæ€ä¿®æ­£ã€å¯¹å¤–éƒ¨é”™è¯¯çš„é‡è¯•/è·³è¿‡/ç»“æŸç­‰å¤„ç†ï¼Œæ›´è´´åˆçœŸå®åœºæ™¯ä¸­å¤šæ ·å¤æ‚çš„é”™è¯¯æƒ…å†µã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ–°é¢–çš„æ•°æ®æ¼”åŒ–ç­–ç•¥æ„å»ºæ•°æ®é›†  
é‡‡ç”¨åˆ›æ–°çš„æ•°æ®é›†æ„å»ºæ¼”åŒ–ç­–ç•¥ï¼Œä¸°å¯Œé”™è¯¯æ•°æ®é›†ã€‚é€šè¿‡æ”¶é›†å·¥å…·ä½¿ç”¨åŸºå‡†æ•°æ®ã€åˆ©ç”¨GPTæ¨¡æ‹Ÿå™¨å’Œé‡å¤APIè°ƒç”¨å¤šæ ·åŒ–å†…å¤–éƒ¨é”™è¯¯æ¨¡å¼ã€å¤„ç†å·¥å…·å“åº”ã€æ¼”åŒ–é”™è¯¯æ•°æ®ç­‰æ­¥éª¤ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ï¼Œè®©æ•°æ®åŒ…å«ä¸åŒå¤æ‚åº¦çš„å·¥å…·ä½¿ç”¨é”™è¯¯ï¼Œæå‡è¯„ä¼°çš„å¹¿åº¦ä¸æ·±åº¦ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨CRITICTOOLä¸Šå¼€å±•å¤§é‡å®éªŒï¼Œåˆ†æä¸åŒå¤§æ¨¡å‹é¢å¯¹ä¸åŒæ¥æºé”™è¯¯æ—¶çš„è‡ªæˆ‘æ‰¹åˆ¤è¡¨ç°ã€‚ä¾‹å¦‚è¡¨1å±•ç¤ºä¸åŒå…ˆè¿›å¤§æ¨¡å‹åœ¨å››ä¸ªæ•°æ®é›†é”™è¯¯æ¢å¤çš„æˆåŠŸç‡ï¼Œå‘ç°ä¸åŒæ¨¡å‹åº”å¯¹ä¸åŒæ¥æºé”™è¯¯æ—¶ï¼Œè‡ªæˆ‘æ‰¹åˆ¤è¡Œä¸ºå­˜åœ¨å·®å¼‚ï¼ŒéªŒè¯äº†æ‰€æ„å»ºåŸºå‡†ç­–ç•¥çš„æ³›åŒ–æ€§ä¸æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºç†è§£å¤§æ¨¡å‹å·¥å…·åæ€èƒ½åŠ›æä¾›ä¾æ®ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é”™è¯¯åˆ†æç»´åº¦ï¼šä»å†…å¤–éƒ¨å¤šç»´åº¦æ‹†è§£å·¥å…·è°ƒç”¨é”™è¯¯ç±»å‹ï¼Œä¸ºåç»­ç ”ç©¶å·¥å…·é”™è¯¯å¤„ç†æä¾›äº†ç»†è‡´çš„åˆ†ç±»å‚è€ƒï¼Œä¾¿äºé’ˆå¯¹æ€§ä¼˜åŒ–æ¨¡å‹ã€‚  
2. åŸºå‡†æ„å»ºæ€è·¯ï¼šæ‰“é€ ä¸“æ³¨å·¥å…·å­¦ä¹ è‡ªæˆ‘æ‰¹åˆ¤çš„åŸºå‡†ï¼Œæ‰“ç ´ä¼ ç»Ÿç»“æœå¯¼å‘è¯„ä¼°å±€é™ï¼Œä¸ºè¯„ä¼°å¤§æ¨¡å‹å·¥å…·ä½¿ç”¨å…¨æµç¨‹èƒ½åŠ›ï¼ˆå°¤å…¶æ˜¯é”™è¯¯å¤„ç†ï¼‰æä¾›æ–°èŒƒå¼ã€‚  
3. æ•°æ®æ„å»ºç­–ç•¥ï¼šå€ŸåŠ©æ¼”åŒ–ç­–ç•¥ä¸°å¯Œé”™è¯¯æ•°æ®åœºæ™¯ï¼Œè¿™ç§å¢å¼ºæ•°æ®å¤æ‚æ€§ä»¥è´´è¿‘çœŸå®åº”ç”¨çš„æ€è·¯ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€æ¨¡æ‹ŸçœŸå®å¤æ‚åœºæ™¯çš„åŸºå‡†æ„å»ºæˆ–æ¨¡å‹è®­ç»ƒä¸­ã€‚  
4. å®éªŒåˆ†æè§†è§’ï¼šå¯¹ä¸åŒå¤§æ¨¡å‹å·¥å…·åæ€èƒ½åŠ›çš„æ·±å…¥åˆ†æï¼Œä¸ºè¡Œä¸šäº†è§£å¤§æ¨¡å‹åœ¨å·¥å…·å­¦ä¹ é¢†åŸŸçš„è¡¨ç°æä¾›æ–°è§†è§’ï¼ŒåŠ©åŠ›åç»­æ¨¡å‹æ”¹è¿›æ–¹å‘çš„æ¢ç´¢ã€‚  

è®ºæ–‡ä»£ç å·²å¼€æºï¼ˆhttps://github.com/Shellorley0513/CriticToolï¼‰ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å¯å¤ç°ä¸æ‹“å±•çš„åŸºç¡€ï¼Œæ¨åŠ¨å·¥å…·å­¦ä¹ é¢†åŸŸå¯¹é”™è¯¯å¤„ç†æ–¹å‘çš„ç ”ç©¶è¿›å±•ã€‚ 

## selt--self-evaluation-tree-search-for-llms-with-task-decomposition
### Abstract
While Large Language Models (LLMs) have achieved remarkable success in a wide range of applications, their performance often degrades in complex reasoning tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to enhance LLM reasoning without relying on external reward models. By redefining the Upper Confidence Bound scoring to align with intrinsic self-evaluation capabilities of LLMs and decomposing the inference process into atomic subtasks augmented with semantic clustering at each node, SELT effectively balances exploration and exploitation, reduces redundant reasoning paths, and mitigates hallucination. We validate our approach on challenging benchmarks, including the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT achieves significant improvements in answer accuracy and reasoning robustness compared to baseline methods. Notably, our framework operates without task-specific fine-tuning, demonstrating strong generalizability across diverse reasoning tasks. Relevant results and code are available at https://github.com/fairyshine/SELT .
### ğŸŒŸ è®ºæ–‡è§£è¯» | SELTï¼šæ— éœ€å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œç”¨ä»»åŠ¡åˆ†è§£+è‡ªè¯„ä¼°æ ‘æœç´¢å¢å¼ºå¤§æ¨¡å‹æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šåº”ç”¨ä¸­å–å¾—äº†ç©ç›®æˆæœï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å¾€å¾€ä¼šä¸‹é™ï¼Œå‡ºç°ç­”æ¡ˆä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚ä¸ºå¢å¼ºLLMæ¨ç†èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ã€æ€ç»´é“¾æç¤ºã€å¼ºåŒ–å­¦ä¹ å¾®è°ƒç­‰ï¼Œå­˜åœ¨ä¾èµ–äººå·¥æ„é€ æ¨ç†æ¨¡æ¿æˆ–éœ€è¦åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šå¤§é‡å¾®è°ƒçš„æ˜‚è´µå¥–åŠ±æ¨¡å‹ç­‰å±€é™ã€‚æ­¤å¤–ï¼Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è™½è¢«ç”¨äºå¢å¼ºLLMæ¨ç†ï¼Œä½†ç°æœ‰åŸºäºMCTSçš„æ–¹æ³•ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹è¯„ä¼°ä¸­é—´æ­¥éª¤ï¼Œé™åˆ¶äº†åº”ç”¨åœºæ™¯ä¸”å¸¦æ¥é¢å¤–è®­ç»ƒå¼€é”€ä¸åå·®ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆã€é€šç”¨çš„æ–¹æ³•æ¥æå‡LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºè‡ªè¯„ä¼°çš„MCTSæ”¹è¿›  
æå‡ºSELTæ¡†æ¶ï¼Œå¯¹åŸå§‹MCTSçš„UCTï¼ˆUpper Confidence Bound for Treesï¼‰è¯„åˆ†æœºåˆ¶è¿›è¡Œä¿®æ”¹ï¼Œé‡æ–°å®šä¹‰ä¸¤éƒ¨åˆ†è¯„åˆ†ä»¥å¥‘åˆLLMå†…åœ¨çš„è‡ªè¯„ä¼°èƒ½åŠ›ï¼Œä¸å†ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ã€‚æ–°çš„è¯„åˆ†æœºåˆ¶è®©æ¨ç†è·¯å¾„çš„æ¢ç´¢ä¸åˆ©ç”¨æ›´å¹³è¡¡ï¼Œç”±LLMè‡ªèº«å†…åœ¨å¼•å¯¼æ ‘æœç´¢è¿‡ç¨‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMæ¨ç†ä»»åŠ¡åˆ†è§£ä¸è¯­ä¹‰èšç±»  
å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªåŸå­çº§LLMä»»åŠ¡ï¼ŒæŠŠå¤æ‚ä»»åŠ¡æ‹†åˆ†ä¸ºæ›´æ˜“å¤„ç†çš„å­ä»»åŠ¡ï¼›åŒæ—¶åœ¨æ¨ç†æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹å¼•å…¥è¯­ä¹‰èšç±»æœºåˆ¶ï¼ŒåŠ¨æ€åˆ†ç»„è¯­ä¹‰ç­‰ä»·çš„è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘æ¨ç†è·¯å¾„å†—ä½™ï¼Œç­›é€‰é«˜è´¨é‡ä»£è¡¨æ€§ç­”æ¡ˆï¼Œæå‡æœç´¢æ•ˆç‡å¹¶å‡è½»â€œå¹»è§‰é™·é˜±â€ï¼Œè¿˜èƒ½ä¿ç•™æ¨ç†è·¯å¾„çš„å¤šæ ·æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼š  
- **MMLUæ•°æ®é›†**ï¼ˆåŸºäºçŸ¥è¯†çš„é—®ç­”ï¼Œéœ€å¤šæ­¥æ¨ç†ï¼‰ï¼šSELTç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼ˆå¦‚CoTã€æ ‡å‡†MCTSç­‰ï¼‰ï¼Œåœ¨ç­”æ¡ˆå‡†ç¡®ç‡å’Œæ¨ç†é²æ£’æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚  
- **Seal - Toolsæ•°æ®é›†**ï¼ˆå·¥å…·å­¦ä¹ ï¼Œæ¶‰åŠä¸å¤–éƒ¨å·¥å…·åŠ¨æ€äº¤äº’ï¼‰ï¼šSELTä¹Ÿå±•ç°å‡ºä¼˜äºåŸºçº¿çš„è¡¨ç°ã€‚  
ä¸”æ¡†æ¶æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒï¼Œåœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­ä½“ç°å‡ºå¼ºæ³›åŒ–æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªè¯„ä¼°é©±åŠ¨æœç´¢æ€è·¯ï¼šæ‘†è„±å¯¹å¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ï¼Œåˆ©ç”¨LLMè‡ªèº«èƒ½åŠ›å¼•å¯¼æœç´¢ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æä¾›äº†â€œå†…ç”Ÿâ€ä¼˜åŒ–çš„æ–°æ€è·¯ï¼Œå‡å°‘å¤–éƒ¨ä¾èµ–ä¸è®­ç»ƒå¼€é”€ã€‚  
2. ä»»åŠ¡åˆ†è§£ä¸è¯­ä¹‰èšç±»ï¼šå°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºåŸå­å­ä»»åŠ¡é™ä½æ¨ç†éš¾åº¦ï¼Œè¯­ä¹‰èšç±»å‡å°‘å†—ä½™å¹¶æå‡ç­”æ¡ˆè´¨é‡ï¼Œè¿™ç§â€œåˆ†è€Œæ²»ä¹‹ + èšç±»é€‰ä¼˜â€çš„æ¨¡å¼å¯å€Ÿé‰´åˆ°å…¶ä»–éœ€å¤„ç†å¤æ‚è¾“å‡ºæˆ–è·¯å¾„é€‰æ‹©çš„å¤§æ¨¡å‹åº”ç”¨åœºæ™¯ã€‚  
3. é€šç”¨åŒ–èƒ½åŠ›éªŒè¯ï¼šåœ¨ä¸åŒç±»å‹æ¨ç†ä»»åŠ¡ï¼ˆæ•°å­¦ã€å¸¸è¯†ã€è¿‡ç¨‹æ¨ç†ç­‰ï¼‰éªŒè¯æœ‰æ•ˆæ€§ä¸”æ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œè¯æ˜æ–¹æ³•åœ¨è·¨ä»»åŠ¡åœºæ™¯çš„æ™®é€‚æ€§ï¼Œä¸ºæ‰“é€ é€šç”¨å‹å¤§æ¨¡å‹æ¨ç†å¢å¼ºå·¥å…·æä¾›å‚è€ƒã€‚

## chematagent--enhancing-llms-for-chemistry-and-materials-science-through-tree-search-based-tool-learning
### Abstract
Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .
### ğŸŒŸ è®ºæ–‡è§£è¯» | CheMatAgentï¼šåŸºäºæ ‘æœç´¢å·¥å…·å­¦ä¹ å¢å¼ºåŒ–å­¦ä¸ææ–™ç§‘å­¦é¢†åŸŸå¤§æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ–å­¦ç›¸å…³ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä¹Ÿé¢ä¸´é¢„è®­ç»ƒçŸ¥è¯†è¿‡æ—¶ä»¥åŠéš¾ä»¥èå…¥ä¸“ä¸šåŒ–å­¦çŸ¥è¯†ç­‰æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œç°æœ‰åŒ–å­¦å·¥å…·åŒ…ä¾èµ–ä¸“ä¸š cheminformatics è½¯ä»¶ï¼Œå¼€å‘éƒ¨ç½²éš¾ã€å·¥å…·æ•°é‡æœ‰é™ï¼›ç°æœ‰æ•°æ®é›†è´¨é‡å·®ä¸”ç¼ºä¹åˆé€‚è¯„ä¼°è®¾ç½®ï¼Œå³ä¾¿æœ‰å·¥å…·ï¼Œæ™ºèƒ½ä½“åœ¨å·¥å…·é€‰æ‹©å’Œå‚æ•°ç”Ÿæˆä¸Šä¹Ÿå› åŒ–å­¦ä¸“ä¸šçŸ¥è¯†é—¨æ§›è€Œå­˜åœ¨å›°éš¾ï¼Œè¿™äº›éƒ½é™åˆ¶äº†èšç„¦åŒ–å­¦é¢†åŸŸçš„ LLM æ™ºèƒ½ä½“æ•ˆèƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡º CheMatAgent æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡åŒ–å­¦å·¥å…·æ±   
æ”¶é›†æ•´åˆäº†æ¥è‡ª ChemCrowã€CACTUSã€chemlibã€pymatgenã€Chemistry Tools ç­‰ 5 ä¸ªæ¥æºçš„å·¥å…·ï¼Œå½¢æˆåŒ–å­¦ä¸ææ–™é¢†åŸŸè¿„ä»Šæœ€å¤§çš„å·¥å…·æ± ï¼ŒåŒ…å« 137 ä¸ªå·¥å…·ï¼Œè¦†ç›–ä»åŸºç¡€ä¿¡æ¯æ£€ç´¢åˆ°å¤æ‚ååº”é¢„æµ‹ç­‰å¤šæ ·ä»»åŠ¡ã€‚è¿˜é€šè¿‡ç»Ÿä¸€å·¥å…·æ ¼å¼ï¼ˆç”Ÿæˆ â€œtools.jsonâ€ æ˜ç¡®å·¥å…·ä¿¡æ¯ä¸è°ƒç”¨è·¯å¾„ï¼‰ã€ç¼–å†™æ–‡æ¡£å’Œä¼˜åŒ–ä»£ç ï¼ˆè§„èŒƒå‚æ•°å‘½åã€è§£è€¦å·¥å…·ä¸åŸåŒ…ä¾èµ–ç­‰ï¼‰è®©å·¥å…·æ± æ›´æ˜“ç”¨ã€æ˜“æ‰©å±•ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡é¢†åŸŸå·¥å…·å­¦ä¹ æ•°æ®é›†æ„å»º pipeline ä¸ ChemToolBench æ•°æ®é›†  
ä¸ºæ¨¡å‹å¾®è°ƒä¸è¯„ä¼°æ‰“é€ é«˜è´¨é‡ã€å¤šæ ·çš„å…ƒæ•°æ®é›† ChemToolBenchã€‚è®¾è®¡äº†é¢å‘åŒ–å­¦é¢†åŸŸå·¥å…·å­¦ä¹ çš„æ•°æ®é›† curation pipelineï¼Œç”¨äºè‡ªæŒ‡ä»¤å¼çš„å·¥å…·å­¦ä¹ æ•°æ®ç”Ÿæˆï¼Œæ•°æ®æ¶µç›–å·¥å…·é€‰æ‹©å’Œå‚æ•°å¡«å……çš„éš¾é¢˜æ¡ˆä¾‹ï¼ŒåŠ©åŠ›æ¨¡å‹æ›´å¥½å­¦ä¹ è°ƒç”¨åŒ–å­¦é¢†åŸŸå·¥å…·ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡º HE - MCTS æ¡†æ¶å®ç°å·¥å…·è§„åˆ’ä¸æ‰§è¡Œè§£è€¦ä¼˜åŒ–  
å¼•å…¥åˆ†å±‚è¿›åŒ–è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆHierarchical Evolutionary Monte Carlo Tree Search, HE - MCTSï¼‰æ¡†æ¶ï¼Œé«˜å±‚ç­–ç•¥æ¨¡å‹è¿­ä»£æ¢ç´¢ä¼˜åŒ–å·¥å…·é€‰æ‹©åºåˆ—ï¼Œå¾®è°ƒåçš„ä½å±‚æ‰§è¡Œæ¨¡å‹åŸºäºæ‰§è¡Œåé¦ˆè¿­ä»£æå‡å‡†ç¡®æ€§ã€‚åˆ©ç”¨è‡ªç”Ÿæˆçš„ HE - MCTS æ•°æ®ç»“åˆå…ƒæ•°æ®é›†ï¼Œå¯¹ç­–ç•¥æ¨¡å‹è¿›è¡Œæ­¥éª¤çº§å¾®è°ƒï¼Œè®­ç»ƒè¶…è¶Š GPT - 4o çš„ä»»åŠ¡è‡ªé€‚åº” PRM å’Œ ORMï¼Œä¸”è®­ç»ƒæ— éœ€äººå·¥æ ‡æ³¨ï¼Œç”± HE - MCTS å¼•å¯¼æ™ºèƒ½ä½“è‡ªä¸»ä¼˜åŒ–æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ–å­¦é—®ç­”ï¼ˆChemistry QAï¼‰å’Œå‘ç°ç±»ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä¸ºä¸“ä¸šå·¥å…·ä¸ LLM é›†æˆç”¨äºé«˜çº§åŒ–å­¦åº”ç”¨æä¾›äº†ç¨³å¥æ–¹æ¡ˆã€‚ï¼ˆæ–‡ä¸­æœªè¯¦ç»†å±•å¼€å®éªŒæ•°æ®ç»†èŠ‚ï¼Œä½†å¼ºè°ƒäº†åœ¨ä»»åŠ¡ä¸­è¡¨ç°è¶…è¶Šå¯¹æ¯”åŸºçº¿ç­‰æ•ˆæœï¼‰  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸå·¥å…·æ± æ„å»ºæ€è·¯ï¼šé’ˆå¯¹ç‰¹å®šé¢†åŸŸæ”¶é›†ã€è§„æ•´ã€ä¼˜åŒ–å·¥å…·ï¼Œå½¢æˆä¸°å¯Œä¸”æ˜“ç”¨çš„å·¥å…·ç”Ÿæ€ï¼Œä¸ºé¢†åŸŸå¤§æ¨¡å‹æ‰©å±•èƒ½åŠ›æä¾›å‚è€ƒã€‚  
2. é¢†åŸŸæ•°æ®é›†æ„å»ºï¼šç»“åˆé¢†åŸŸç‰¹æ€§è®¾è®¡æ•°æ®ç”Ÿæˆ pipelineï¼Œæ‰“é€ é«˜è´¨é‡é¢†åŸŸæ•°æ®é›†ç”¨äºæ¨¡å‹è°ƒä¼˜ä¸è¯„ä¼°ï¼Œè¿™ç§èšç„¦é¢†åŸŸéœ€æ±‚çš„æ•°æ®é›†æ„å»ºæ–¹å¼å€¼å¾—å€Ÿé‰´ã€‚  
3. åˆ†å±‚æ ‘æœç´¢æ¡†æ¶ï¼šå°†å·¥å…·è§„åˆ’ä¸æ‰§è¡Œè§£è€¦ä¸ºä¸åŒæ¨¡å‹ï¼Œåˆ©ç”¨è‡ªç”Ÿæˆæ•°æ®å®ç°æ— ç›‘ç£å¼çš„æ¨¡å‹è¿­ä»£ä¼˜åŒ–ï¼Œä¸ºå¤æ‚ä»»åŠ¡ä¸‹å¤§æ¨¡å‹æ™ºèƒ½ä½“çš„èƒ½åŠ›å¢å¼ºæä¾›äº†æ¶æ„è®¾è®¡ä¸è®­ç»ƒæ–¹æ³•çš„æ€è·¯ã€‚  
4. å¼€æºç”Ÿæ€å»ºè®¾ï¼šè®ºæ–‡å°†æ•°æ®é›†å’Œä»£ç å¼€æºï¼Œè¿™ç§å¼€æ”¾å…±äº«çš„ç§‘ç ”å®è·µåˆ©äºé¢†åŸŸå†…åç»­ç ”ç©¶å¤ç”¨ä¸å‘å±•ã€‚

## enhancing-tool-learning-in-large-language-models-with-hierarchical-error-checklists
### Abstract
Large language models (LLMs) have significantly advanced natural language processing, particularly through the integration of external tools and APIs. However, their effectiveness is frequently hampered by parameter mis-filling during tool calling. In this paper, we propose the Hierarchical Tool Error Checklist (HiTEC) framework to systematically diagnose and mitigate tool-calling errors without relying on extensive real-world interactions. HiTEC introduces a two-tiered approach: a global error checklist that identifies common, cross-tool issues, and a local error checklist that targets tool-specific and contextual failures. Building on this structure, we propose two deployments: HiTEC-In Context Learning (HiTEC-ICL) and HiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global checklist in the initial prompts and leverages a two-round conversational interaction to dynamically refine parameter handling, while HiTEC-KTO generates high-quality negative examples to drive fine-tuning via preference-based optimization. Extensive experiments across five public datasets demonstrate that our framework significantly improves parameter-filling accuracy and tool-calling success rates compared to baseline methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨åˆ†å±‚é”™è¯¯æ¸…å•æå‡å¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·å­¦ä¹ èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·å’ŒAPIæ‹“å±•äº†èƒ½åŠ›è¾¹ç•Œã€‚ç„¶è€Œï¼Œå·¥å…·è°ƒç”¨è¿‡ç¨‹ä¸­å‚æ•°å¡«å†™é”™è¯¯çš„é—®é¢˜é¢‘ç¹å‡ºç°ï¼Œä¸¥é‡å½±å“äº†å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ä¸å¯é æ€§ã€‚ä»¥å¾€å¤šæ•°å·¥å…·å­¦ä¹ æ–¹æ³•ä¾èµ–å¤§é‡çœŸå®çš„LLM - å·¥å…·äº¤äº’æ¥æå‡è°ƒç”¨ç²¾åº¦ï¼Œä½†è¿™ç§æ–¹å¼å­˜åœ¨èµ„æºæ¶ˆè€—å¤§ï¼ˆå¦‚Bing Search APIæœ‰è¾ƒé«˜çš„è°ƒç”¨æˆæœ¬ï¼‰å’Œç¨³å®šæ€§ä¸è¶³ç­‰é—®é¢˜ã€‚å¹¶ä¸”ï¼ŒLLMè°ƒç”¨å·¥å…·æ—¶å‡ºç°çš„é”™è¯¯å¤§å¤šæ˜¯å¯æå‰çŸ¥æ™“çš„å¸¸è§ç±»å‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¸Œæœ›æå‡ºä¸€ç§æ— éœ€å¤§é‡çœŸå®äº¤äº’ï¼Œèƒ½ç³»ç»Ÿè¯Šæ–­å’Œç¼“è§£å·¥å…·è°ƒç”¨é”™è¯¯çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºHierarchical Tool Error Checklistï¼ˆHiTECï¼‰æ¡†æ¶
è¯¥æ¡†æ¶æ„å»ºäº†ä¸¤å±‚é”™è¯¯æ¸…å•æ¥è¯†åˆ«å’Œå¤„ç†å·¥å…·è°ƒç”¨é”™è¯¯ã€‚ä¸€æ˜¯å…¨å±€é”™è¯¯æ¸…å•ï¼Œç”¨äºæ•æ‰ä¸åŒå·¥å…·é—´å¸¸è§çš„é€šç”¨é”™è¯¯ï¼›äºŒæ˜¯å±€éƒ¨é”™è¯¯æ¸…å•ï¼Œèšç„¦äºç‰¹å®šå·¥å…·çš„ä¸“å±é”™è¯¯ä»¥åŠæƒ…å¢ƒæ€§æ•…éšœã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¸…å•ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€å¤§é‡çœŸå®ä¸–ç•Œæ‰§è¡Œçš„æƒ…å†µä¸‹ï¼Œç»“æ„åŒ–ä¸”å…¨é¢åœ°è¯Šæ–­å’Œçº æ­£å·¥å…·è°ƒç”¨é”™è¯¯ï¼Œæ”¯æŒè‡ªé€‚åº”å’Œå¯æ‰©å±•çš„å·¥å…·å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºä¸¤ç§éƒ¨ç½²æ–¹å¼HiTEC - ICLå’ŒHiTEC - KTO
 - HiTEC - In Context Learningï¼ˆHiTEC - ICLï¼‰ï¼šå°†å…¨å±€é”™è¯¯æ¸…å•åµŒå…¥åˆå§‹æŸ¥è¯¢ï¼Œé€šè¿‡ä¸¤è½®å¯¹è¯äº¤äº’æ•´åˆå±€éƒ¨é”™è¯¯æ¸…å•ï¼Œå¼•å¯¼LLMsé¢„å…ˆè§„é¿å¸¸è§é”™è¯¯å¹¶çµæ´»ä¼˜åŒ–å‚æ•°å¤„ç†ã€‚
 - HiTEC - Kahneman - Tversky Optimizationï¼ˆHiTEC - KTOï¼‰ï¼šåˆ©ç”¨é”™è¯¯æ¸…å•ç”Ÿæˆé«˜è´¨é‡è´Ÿä¾‹ï¼Œé€šè¿‡åŸºäºåå¥½çš„ä¼˜åŒ–è¿›è¡Œå¾®è°ƒï¼Œè®©å¼€æºLLMsè·å¾—æ›´å¼ºçš„å‡½æ•°è°ƒç”¨å‡†ç¡®æ€§ï¼Œå…‹æœäº†åŸºäºåå¥½ä¼˜åŒ–åœ¨å·¥å…·å­¦ä¹ ä¸­çš„å¤±æ•ˆæ¨¡å¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ¡†æ¶ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œåœ¨å‚æ•°å¡«å†™å‡†ç¡®ç‡å’Œå·¥å…·è°ƒç”¨æˆåŠŸç‡æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œå‚æ•°å¡«å†™å‡†ç¡®ç‡æœ€å¤šå¯æå‡42%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
 - åˆ†å±‚é”™è¯¯æ¸…å•çš„æ€è·¯ä¸ºå¤„ç†å¤æ‚ä»»åŠ¡ä¸­çš„é”™è¯¯æä¾›äº†ç»“æ„åŒ–èŒƒå¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€è¦å¯¹é”™è¯¯è¿›è¡Œç³»ç»Ÿç®¡ç†çš„AIä»»åŠ¡åœºæ™¯ï¼Œæ¯”å¦‚å¤šæ™ºèƒ½ä½“åä½œä¸­çš„é”™è¯¯æ’æŸ¥ç­‰ã€‚
 - ä¸¤ç§éƒ¨ç½²æ–¹å¼åˆ†åˆ«å¯¹åº”å…è°ƒä¼˜å’Œè°ƒä¼˜åœºæ™¯ï¼Œä¸ºä¸åŒèµ„æºå’Œéœ€æ±‚ä¸‹çš„æ¨¡å‹ä¼˜åŒ–æä¾›äº†å‚è€ƒï¼Œåœ¨å®é™…å·¥ä¸šç•Œæˆ–å­¦æœ¯ç•Œè¿›è¡ŒLLMå·¥å…·å¢å¼ºæ—¶ï¼Œå¯æ ¹æ®è‡ªèº«æ¡ä»¶é€‰æ‹©åˆé€‚çš„æ–¹å¼å€Ÿé‰´ã€‚
 - å¯¹åŸºäºåå¥½ä¼˜åŒ–åœ¨å·¥å…·å­¦ä¹ ä¸­å¤±æ•ˆæ¨¡å¼çš„åˆ†æä»¥åŠç›¸åº”è§£å†³æ–¹æ³•ï¼Œä¸ºåç»­ä¼˜åŒ–LLMåœ¨å·¥å…·ä½¿ç”¨ã€äº¤äº’ç­‰æ–¹é¢çš„ç ”ç©¶æä¾›äº†ç†è®ºå’Œå®è·µå±‚é¢çš„å¯å‘ã€‚

## mirror--multi-agent-intra--and-inter-reflection-for-optimized-reasoning-in-tool-learning
### Abstract
Complex tasks involving tool integration pose significant challenges for Large Language Models (LLMs), leading to the emergence of multi-agent workflows as a promising solution. Reflection has emerged as an effective strategy for correcting erroneous trajectories in agentic workflows. However, existing approaches only exploit such capability in the post-action stage, where the agent observes the execution outcomes. We argue that, like humans, LLMs can also engage in reflection before action execution: the agent can anticipate undesirable outcomes from its own decisions, which not only provides a necessarily complementary perspective to evaluate the decision but also prevents the propagation of errors throughout the trajectory. In this paper, we propose MIRROR, a framework that consists of both intra-reflection, which critically assesses intended actions before execution, and inter-reflection, which further adjusts the trajectory based on observations. This design systematically leverages LLM reflection capabilities to eliminate and rectify erroneous actions on a more comprehensive scope. Evaluations on both the StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior performance, achieving state-of-the-art results compared to existing approaches.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MIRRORï¼šå¤šæ™ºèƒ½ä½“â€œåæ€åŒæœºåˆ¶â€é©æ–°å·¥å…·å­¦ä¹ æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå±•ç°å“è¶Šèƒ½åŠ›ï¼Œä½†é¢å¯¹éœ€å·¥å…·é›†æˆçš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œå­˜åœ¨å®æ—¶ä¿¡æ¯è·å–ã€ç²¾å‡†ç³»ç»Ÿæ§åˆ¶ç­‰å±€é™ã€‚å·¥å…·å­¦ä¹ èŒƒå¼è™½æ‹“å±•äº†LLMsèƒ½åŠ›ï¼Œå¯å¤„ç†å¤šæ­¥éª¤å¤æ‚å·¥å…·åä½œä»»åŠ¡æ—¶ä»åŠ›æœ‰ä¸é€®ï¼Œå¤šæ™ºèƒ½ä½“å·¥ä½œæµåº”è¿è€Œç”Ÿã€‚ç°æœ‰å¤šæ™ºèƒ½ä½“åæ€æœºåˆ¶ä»…åœ¨â€œè¡ŒåŠ¨åâ€ï¼ˆè§‚å¯Ÿæ‰§è¡Œç»“æœå†åæ€ï¼‰ï¼Œæ— æ³•é¢„é˜²åˆå§‹é”™è¯¯ã€æ˜“å¼•å‘ä¸å¯é€†ç³»ç»Ÿå˜åŒ–ä¸”è¯•é”™æˆæœ¬é«˜ã€‚è€Œäººç±»è¡ŒåŠ¨å‰ä¼šé¢„æƒ³ç»“æœï¼Œå—æ­¤å¯å‘ï¼Œè®ºæ–‡æå‡ºè¦è®©LLMsä¹Ÿèƒ½â€œè¡ŒåŠ¨å‰åæ€â€ï¼Œç»“åˆè¡ŒåŠ¨ååæ€æ„å»ºæ›´å…¨é¢æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºâ€œè¡ŒåŠ¨å‰åæ€ï¼ˆIntra - reflectionï¼‰â€æ¦‚å¿µ  
åœ¨å¤šæ™ºèƒ½ä½“å·¥å…·å­¦ä¹ ä¸­ï¼Œè®©æ™ºèƒ½ä½“åœ¨æ‰§è¡ŒåŠ¨ä½œæˆ–äº¤æ¥ç»™å…¶ä»–æ™ºèƒ½ä½“å‰ï¼Œå¯¹è‡ªèº«é¢„æœŸè¾“å‡ºåšæ‰¹åˆ¤æ€§è¯„ä¼°ã€‚è¿™æ¨¡ä»¿äººç±»è¡ŒåŠ¨å‰å¿ƒç†æ¨¡æ‹Ÿç»“æœçš„è®¤çŸ¥è¿‡ç¨‹ï¼Œæ˜¯ä¸€ç§ä¸»åŠ¨é¢„é˜²é”™è¯¯çš„æœºåˆ¶ï¼Œèƒ½åœ¨æ‰§è¡Œå‰å°±é¢„åˆ¤å†³ç­–å¯èƒ½å¸¦æ¥çš„ä¸è‰¯ç»“æœï¼Œä¸ºå†³ç­–è¯„ä¼°æä¾›äº’è¡¥è§†è§’ï¼Œé˜²æ­¢é”™è¯¯åœ¨æ•´ä¸ªä»»åŠ¡è½¨è¿¹ä¸­ä¼ æ’­ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºMIRRORæ¡†æ¶  
æ•´åˆâ€œè¡ŒåŠ¨å‰åæ€ï¼ˆIntra - reflectionï¼‰â€ä¸â€œè¡ŒåŠ¨ååæ€ï¼ˆInter - reflectionï¼‰â€åŒæœºåˆ¶ã€‚Intra - reflectionåœ¨å•ä¸ªæ™ºèƒ½ä½“å†…éƒ¨ï¼Œæ‰§è¡Œå‰è¯„ä¼°å†³ç­–é˜²é”™ï¼›Inter - reflectionåœ¨æ™ºèƒ½ä½“ä¹‹é—´ï¼ŒåŸºäºæ‰§è¡Œåè§‚å¯Ÿè¿›ä¸€æ­¥è°ƒæ•´ä»»åŠ¡è½¨è¿¹ï¼Œå®ç°ç³»ç»Ÿå±‚é¢ä¼˜åŒ–ã€‚åŒé˜¶æ®µååŒï¼Œç³»ç»Ÿåœ°åˆ©ç”¨LLMåæ€èƒ½åŠ›ï¼Œåœ¨æ›´å…¨é¢èŒƒå›´å†…æ¶ˆé™¤å’Œçº æ­£é”™è¯¯åŠ¨ä½œï¼Œè®©æ™ºèƒ½ä½“æ—¢èƒ½é¢„åˆ¤è§„é¿æ½œåœ¨é”™è¯¯ï¼Œåˆèƒ½ä»ä¸å¯é¿å…çš„å¤±è´¥ä¸­å­¦ä¹ ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨StableToolBenchå’ŒTravelPlannerä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°MIRRORã€‚ç»“æœæ˜¾ç¤ºï¼ŒMIRRORå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½è¾¾åˆ°äº†å½“å‰æœ€ä¼˜ï¼ˆstate - of - the - artï¼‰æ°´å¹³ï¼Œæœ‰åŠ›è¯æ˜äº†å…¶åœ¨å¤šæ™ºèƒ½ä½“å·¥å…·å­¦ä¹ ä»»åŠ¡ä¸­ä¼˜åŒ–æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åæ€æœºåˆ¶æ‹“å±•ï¼šç°æœ‰LLMåæ€å¤šåœ¨è¡ŒåŠ¨åï¼ŒMIRRORå¼•å…¥è¡ŒåŠ¨å‰åæ€ï¼Œä¸ºæå‡æ™ºèƒ½ä½“å†³ç­–è´¨é‡æä¾›äº†â€œé¢„é˜²å‹â€æ€è·¯ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§â€œäº‹å‰ + äº‹åâ€åŒç»´åº¦åæ€çš„è®¾è®¡é€»è¾‘ï¼Œåº”ç”¨åˆ°å…¶ä»–æ™ºèƒ½ä½“æˆ–å•æ™ºèƒ½ä½“ä»»åŠ¡å¤„ç†ä¸­ã€‚  
2. å¤šæ™ºèƒ½ä½“åä½œä¼˜åŒ–ï¼šåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡ä¸Šï¼Œé€šè¿‡åŒåæ€æœºåˆ¶å¢å¼ºå†³ç­–ã€é˜²æ­¢é”™è¯¯ä¼ æ’­ä¸æå‡åä½œæ•ˆç‡ï¼Œä¸ºå¤æ‚ä»»åŠ¡ä¸‹å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶çš„æ„å»ºæä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘æ›´å¤šé’ˆå¯¹å¤šæ™ºèƒ½ä½“åä½œä¸­ error handlingï¼ˆé”™è¯¯å¤„ç†ï¼‰çš„ç ”ç©¶ã€‚  
3. å·¥å…·å­¦ä¹ èƒ½åŠ›æå‡ï¼šé’ˆå¯¹LLMå·¥å…·å­¦ä¹ åœ¨å¤æ‚ä»»åŠ¡çš„å±€é™ï¼ŒMIRRORæä¾›äº†ä¸€å¥—åˆ©ç”¨åæ€ä¼˜åŒ–å·¥å…·é€‰æ‹©ã€å‚æ•°åŒ–ç­‰ç¯èŠ‚çš„æ–¹æ¡ˆï¼Œä¸ºå·¥å…·å­¦ä¹ é¢†åŸŸæå‡å¤æ‚ä»»åŠ¡å¤„ç†èƒ½åŠ›æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯å¾„ã€‚

## ttpa--token-level-tool-use-preference-alignment-training-framework-with-fine-grained-evaluation
### Abstract
Existing tool-learning methods usually rely on supervised fine-tuning, they often overlook fine-grained optimization of internal tool call details, leading to limitations in preference alignment and error discrimination. To overcome these challenges, we propose Token-level Tool-use Preference Alignment Training Framework (TTPA), a training paradigm for constructing token-level tool-use preference datasets that align LLMs with fine-grained preferences using a novel error-oriented scoring mechanism. TTPA first introduces reversed dataset construction, a method for creating high-quality, multi-turn tool-use datasets by reversing the generation flow. Additionally, we propose Token-level Preference Sampling (TPS) to capture fine-grained preferences by modeling token-level differences during generation. To address biases in scoring, we introduce the Error-oriented Scoring Mechanism (ESM), which quantifies tool-call errors and can be used as a training signal. Extensive experiments on three diverse benchmark datasets demonstrate that TTPA significantly improves tool-using performance while showing strong generalization ability across models and datasets.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TTPAï¼šé¢å‘Tokençº§å·¥å…·ä½¿ç”¨åå¥½å¯¹é½çš„è®­ç»ƒæ¡†æ¶ï¼Œå®ç°ç»†ç²’åº¦è¯„ä¼°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¤–éƒ¨å·¥å…·äº¤äº’å¯¹è§£å†³å¤æ‚ç°å®é—®é¢˜è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰å·¥å…·å­¦ä¹ æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼šä¾èµ–æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ—¶ï¼Œå¸¸å¿½è§†å·¥å…·è°ƒç”¨ç»†èŠ‚çš„ç»†ç²’åº¦ä¼˜åŒ–ï¼Œåœ¨åå¥½å¯¹é½å’Œé”™è¯¯åˆ¤åˆ«ä¸Šå—é™ï¼›åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ä¹Ÿå­˜åœ¨æŒ‘æˆ˜ï¼Œä¸€æ–¹é¢å¿½ç•¥å•ä¸ªå·¥å…·è°ƒç”¨å†…ç»†ç²’åº¦åå¥½å·®å¼‚ï¼ˆå¦‚tokençº§é”™è¯¯æ˜“è‡´è°ƒç”¨å¤±è´¥ï¼‰ï¼Œå¦ä¸€æ–¹é¢åå¥½æ•°æ®é‡‡æ ·å¤šåŸºäºè½¨è¿¹çº§è¯„ä¼°ï¼Œæ˜“å¼•å…¥åå·®ï¼Œç”Ÿæˆä½è´¨é‡åå¥½æ•°æ®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºTokençº§å·¥å…·ä½¿ç”¨åå¥½å¯¹é½è®­ç»ƒæ¡†æ¶ï¼ˆTTPAï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåå¥½å¯¼å‘çš„å·¥å…·ä½¿ç”¨æ•°æ®é›†æ„å»º  
åŒ…å«åå‘æ•°æ®é›†æ„å»ºä¸Tokençº§åå¥½é‡‡æ ·ã€‚åå‘æ•°æ®é›†æ„å»ºé¢ è¦†ä¼ ç»Ÿä»æŸ¥è¯¢å¼€å§‹çš„æµç¨‹ï¼Œå…ˆè®©LLMåœ¨é¢„å®šä¹‰å·¥å…·ä½¿ç”¨åœºæ™¯ä¸­ç”Ÿæˆå·¥å…·è°ƒç”¨åºåˆ—ä¸æœ€ç»ˆç­”æ¡ˆï¼Œå†åŸºäºç­”æ¡ˆæ„é€ æŸ¥è¯¢ã€‚æ­¤ç­–ç•¥é¿å…æ— æ„ä¹‰æŸ¥è¯¢ä¸æ•°æ®æ³„æ¼ï¼Œä¿è¯æŸ¥è¯¢å¯å›ç­”æ€§ï¼Œè¿˜èƒ½ç»´æŒé—®é¢˜éš¾åº¦ï¼ˆéœ€å¤šå·¥å…·åä½œï¼‰ã€‚Tokençº§åå¥½é‡‡æ ·åˆ™èšç„¦å·¥å…·è°ƒç”¨ç”Ÿæˆæ—¶çš„tokençº§å·®å¼‚ï¼Œä»LLMç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·top - kå€™é€‰tokenï¼Œæ˜¾å¼å»ºæ¨¡tokençº§åå¥½ï¼Œæ•æ‰ç»†ç²’åº¦åå¥½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢å‘é”™è¯¯çš„è¯„åˆ†æœºåˆ¶ï¼ˆESMï¼‰  
ç°æœ‰æ¨¡å‹ç”¨LLMå¯¹è¾“å‡ºè¯„åˆ†æ˜“å› ç²—ç²’åº¦è¯„ä¼°å’Œæ¨¡ç³Šæ ‡å‡†å¼•å…¥åå·®ï¼ŒTTPAå®šä¹‰å·¥å…·è°ƒç”¨é”™è¯¯åˆ†ç±»æ³•ï¼Œé‡åŒ–å·¥å…·è°ƒç”¨é”™è¯¯å¹¶å°†å…¶ä½œä¸ºè®­ç»ƒä¿¡å·ï¼Œç”¨äºæ„å»ºåå¥½å¯¹é½æ•°æ®é›†å’Œå¾®è°ƒLLMï¼Œå®ç°LLMçš„ç²¾ç¡®å¯¹é½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸‰ä¸ªä¸åŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTTPAæ˜¾è‘—æå‡äº†å·¥å…·é€‰æ‹©ã€å‚æ•°å¡«å……å’Œè¿”å›å€¼è§£æç­‰å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼›ç»TTPAå¾®è°ƒçš„æ¨¡å‹åœ¨è·¨æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºæ³›åŒ–æ€§ä¸å¯è¿ç§»æ€§ï¼Œæå‡äº†LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ä¸é€‚ç”¨æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºæ€è·¯åˆ›æ–°ï¼šåå‘æ„å»ºæ•°æ®é›†ä¸ºè§£å†³ä¼ ç»Ÿæ•°æ®ç”Ÿæˆå¼Šç«¯æä¾›æ–°æ€è·¯ï¼Œå¯å€Ÿé‰´åˆ°éœ€æ„å»ºé«˜è´¨é‡äº¤äº’ç±»æ•°æ®é›†çš„ä»»åŠ¡ä¸­ï¼Œé¿å…æ•°æ®å™ªå£°ä¸æ— æ•ˆæ ·æœ¬é—®é¢˜ã€‚  
2. ç»†ç²’åº¦å»ºæ¨¡ï¼šTokençº§åå¥½é‡‡æ ·å…³æ³¨ç”Ÿæˆè¿‡ç¨‹ä¸­tokençº§å·®å¼‚ï¼Œå¯¹äºéœ€ç²¾ç¡®è¾“å‡ºï¼ˆå¦‚ä»£ç ç”Ÿæˆã€ç»“æ„åŒ–å·¥å…·è°ƒç”¨ï¼‰çš„ä»»åŠ¡ï¼Œè¿™ç§ç»†ç²’åº¦å»ºæ¨¡æ–¹å¼å€¼å¾—å‚è€ƒï¼Œèƒ½æå‡è¾“å‡ºç²¾å‡†åº¦ã€‚  
3. é”™è¯¯å¯¼å‘è¯„ä¼°ï¼šé¢å‘é”™è¯¯çš„è¯„åˆ†æœºåˆ¶å°†é”™è¯¯é‡åŒ–ä¸ºè®­ç»ƒä¿¡å·ï¼Œä¸ºæ¨¡å‹ä¼˜åŒ–æä¾›æ›´æ˜ç¡®æ–¹å‘ï¼Œåœ¨éœ€ç²¾å‡†é”™è¯¯åˆ¤åˆ«ä¸ä¿®æ­£çš„ä»»åŠ¡ï¼ˆå¦‚æ™ºèƒ½å®¢æœå¯¹è¯çº é”™ã€ä»£ç çº é”™ï¼‰ä¸­å¯å€Ÿé‰´è¯¥æ€è·¯è®¾è®¡è¯„ä¼°ä¸è®­ç»ƒæœºåˆ¶ã€‚

## rrtl--red-teaming-reasoning-large-language-models-in-tool-learning
### Abstract
While tool learning significantly enhances the capabilities of large language models (LLMs), it also introduces substantial security risks. Prior research has revealed various vulnerabilities in traditional LLMs during tool learning. However, the safety of newly emerging reasoning LLMs (RLLMs), such as DeepSeek-R1, in the context of tool learning remains underexplored. To bridge this gap, we propose RRTL, a red teaming approach specifically designed to evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the identification of deceptive threats, which evaluates the model's behavior in concealing the usage of unsafe tools and their potential risks; and (2) the use of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also includes a benchmark for traditional LLMs. We conduct a comprehensive evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs generally achieve stronger safety performance than traditional LLMs, yet substantial safety disparities persist across models; (2) RLLMs can pose serious deceptive risks by frequently failing to disclose tool usage and to warn users of potential tool output risks; (3) CoT prompting reveals multi-lingual safety vulnerabilities in RLLMs. Our work provides important insights into enhancing the security of RLLMs in tool learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RRTLï¼šé¢å‘å·¥å…·å­¦ä¹ åœºæ™¯ä¸‹æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„çº¢é˜Ÿæµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥å…·å­¦ä¹ èƒ½åŠ›è™½å¤§å¹…å¢å¼ºäº†æ¨¡å‹èƒ½åŠ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†å®‰å…¨é£é™©ã€‚ä¼ ç»ŸLLMsåœ¨å·¥å…·å­¦ä¹ ä¸­å·²è¢«å‘ç°å­˜åœ¨è¯¸å¤šæ¼æ´ï¼Œè€Œæ–°å…´çš„æ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼Œå¦‚DeepSeek - R1ï¼‰åœ¨å·¥å…·å­¦ä¹ åœºæ™¯ä¸‹çš„å®‰å…¨æ€§å´å°šæœªå……åˆ†æ¢ç´¢ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†RRTLæ–¹æ³•æ¥è¯„ä¼°RLLMsåœ¨å·¥å…·å­¦ä¹ ä¸­çš„å®‰å…¨æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRRTLçº¢é˜Ÿæµ‹è¯•æ–¹æ³•
ä¸“é—¨é’ˆå¯¹RLLMsåœ¨å·¥å…·å­¦ä¹ åœºæ™¯è®¾è®¡ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒè¯„ä¼°æ¨¡å—ï¼šåŸºäºåœºæ™¯çš„å®‰å…¨è¯„ä¼°ã€é’ˆå¯¹å·¥å…·è°ƒç”¨çš„æ¬ºéª—æ€§å¨èƒè¯„ä¼°ä»¥åŠç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰çš„å¼ºåˆ¶å·¥å…·è°ƒç”¨æ”»å‡»ï¼ˆTool - CoT Attackï¼‰ã€‚åŸºäºä¼ ç»ŸLLMså·¥å…·å­¦ä¹ å®‰å…¨åœºæ™¯çš„ç ”ç©¶æˆæœï¼Œç³»ç»Ÿè¯„ä¼°RLLMså®‰å…¨æ€§èƒ½å¹¶ä¸ä¼ ç»ŸLLMså¯¹æ¯”ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰æ¬ºéª—æ€§å¨èƒå¹¶å¼•å…¥æ¬ºéª—ç‡ metric
è¯†åˆ«åˆ°RLLMsåœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆè¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¸ä¼šå¦‚å®æŠ«éœ²æ˜¯å¦è°ƒç”¨å·¥å…·ä»¥åŠå·¥å…·ä½¿ç”¨æ½œåœ¨é£é™©ï¼Œå°†è¿™ç±»å®‰å…¨é—®é¢˜å®šä¹‰ä¸ºé’ˆå¯¹å·¥å…·è°ƒç”¨çš„â€œæ¬ºéª—æ€§å¨èƒâ€ï¼Œå¹¶å¼•å…¥â€œæ¬ºéª—ç‡â€æ¥é‡åŒ–è¯¥ç°è±¡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºTool - CoTæ”»å‡»æ–¹æ³•
åˆ©ç”¨Tool - CoTæ”»å‡»è¯„ä¼°RLLMsåœ¨è¢«æ˜ç¡®æŒ‡ç¤ºä¸ºæ¶æ„æŸ¥è¯¢ä½¿ç”¨å·¥å…·æ—¶ï¼Œæ˜¯å¦ä»èƒ½é¿å…ç”Ÿæˆä¸å®‰å…¨è¾“å‡ºï¼Œä»¥æ­¤è¯„ä¼°RLLMså¯¹æ¶æ„æŸ¥è¯¢çš„è„†å¼±æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. RLLMsæ•´ä½“å®‰å…¨æ€§èƒ½ä¼˜äºä¼ ç»ŸLLMsï¼Œä½†æ¨¡å‹é—´å®‰å…¨å·®å¼‚æ˜¾è‘—ã€‚ä¾‹å¦‚åœ¨æœ‰å®³å·¥å…·è¾“å‡ºåœºæ™¯ä¸­ï¼Œo3 - miniçš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰è¾¾100%ï¼Œè€Œè¯¥åœºæ™¯å¹³å‡ASRä»…43.18%ã€‚
2. æ‰€æœ‰RLLMséƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„æ¬ºéª—è¡Œä¸ºï¼Œå¦‚o1ã€o1 - miniå’Œo1 - previewçš„æ¬ºéª—ç‡ç”šè‡³è¶…è¿‡90%ã€‚
3. åœ¨Tool - CoTæ”»å‡»ä¸‹ï¼ŒRLLMså¹³å‡ASRè¶…85%ï¼›ä¸”å¤šæ•°RLLMsåœ¨ä¸­æ–‡æ”»å‡»ç¯å¢ƒä¸‹çš„ASRæ˜¾è‘—é«˜äºè‹±æ–‡ç¯å¢ƒï¼Œæš´éœ²å‡ºå¤šè¯­è¨€å®‰å…¨æ€§èƒ½å·®å¼‚ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æå‡ºçš„RRTLæ–¹æ³•ä¸ºè¯„ä¼°RLLMsåœ¨å·¥å…·å­¦ä¹ ä¸­çš„å®‰å…¨æ€§æä¾›äº†æ–°èŒƒå¼ï¼Œåç»­é’ˆå¯¹RLLMså·¥å…·å­¦ä¹ å®‰å…¨çš„ç ”ç©¶å¯å‚è€ƒè¯¥çº¢é˜Ÿæµ‹è¯•æ¡†æ¶ã€‚
2. å¯¹æ¬ºéª—æ€§å¨èƒçš„å®šä¹‰ä¸é‡åŒ–ï¼Œè®©ç ”ç©¶è€…å…³æ³¨åˆ°RLLMså·¥å…·è°ƒç”¨è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŠ«éœ²ä¸é£é™©æç¤ºé—®é¢˜ï¼Œä¸ºæ¨¡å‹å®‰å…¨ä¼˜åŒ–æŒ‡æ˜æ–¹å‘ã€‚
3. Tool - CoTæ”»å‡»æ–¹æ³•æ­ç¤ºäº†RLLMså¤šè¯­è¨€å®‰å…¨å¼±ç‚¹ï¼Œåœ¨æ„å»ºå¤šè¯­è¨€å®‰å…¨è¯„ä¼°ä¸é˜²å¾¡æœºåˆ¶æ—¶å¯å€Ÿé‰´è¯¥æ€è·¯ã€‚

## toleap--rethinking-development-of-tool-learning-with-large-language-models
### Abstract
Tool learning, which enables large language models (LLMs) to utilize external tools effectively, has garnered increasing attention for its potential to revolutionize productivity across industries. Despite rapid development in tool learning, key challenges and opportunities remain understudied, limiting deeper insights and future advancements. In this paper, we investigate the tool learning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling one-click evaluation for seven of them, forming a Tool Learning Platform named ToLeaP. We also collect 21 out of 33 potential training datasets to facilitate future exploration. After analyzing over 3,000 bad cases of 41 LLMs based on ToLeaP, we identify four main critical challenges: (1) benchmark limitations induce both the neglect and lack of (2) autonomous learning, (3) generalization, and (4) long-horizon task-solving capabilities of LLMs. To aid future advancements, we take a step further toward exploring potential directions, namely (1) real-world benchmark construction, (2) compatibility-aware autonomous learning, (3) rationale learning by thinking, and (4) identifying and recalling key clues. The preliminary experiments demonstrate their effectiveness, highlighting the need for further research and exploration.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToLeaPï¼šé‡æ–°å®¡è§†å¤§è¯­è¨€æ¨¡å‹å·¥å…·å­¦ä¹ çš„å‘å±•ä¹‹è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥å…·å­¦ä¹ èƒ½è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é«˜æ•ˆåˆ©ç”¨å¤–éƒ¨å·¥å…·ï¼Œåœ¨å„è¡Œä¸šé©æ–°ç”Ÿäº§åŠ›æ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œè™½å‘å±•è¿…é€Ÿï¼Œä½†å…³é”®æŒ‘æˆ˜ä¸æœºé‡ç ”ç©¶ä¸è¶³ã€‚ç°æœ‰å·¥å…·å­¦ä¹ åŸºå‡†éš¾ä»¥å…¨é¢ã€å‡†ç¡®æŒ‡å¯¼LLMsåœ¨å·¥å…·å­¦ä¹ é¢†åŸŸçš„è¯„ä¼°ä¸è®­ç»ƒï¼Œè¯„ä¼°ç¢ç‰‡åŒ–ï¼Œå¿½è§†èƒ½åŠ›é—´ç›¸äº’ä¾èµ–ï¼Œæ˜“å¾—å‡ºè¯¯å¯¼æ€§ç»“è®ºï¼Œæ©ç›–å·¥å…·å­¦ä¹ æ ¸å¿ƒç“¶é¢ˆï¼Œé™åˆ¶å¯¹å‘å±•çš„æ·±å…¥ç†è§£ä¸æœªæ¥æ–¹å‘çš„å‡†ç¡®é¢„åˆ¤ã€‚å› æ­¤ï¼Œæ„å»ºç»Ÿä¸€åŸºå‡†è¯†åˆ«å·¥å…·å­¦ä¹ é¢†åŸŸæ ¸å¿ƒæŒ‘æˆ˜ä¸æœºé‡è¿«åœ¨çœ‰ç«ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºæ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ToLeaP  
å¤ç°33ä¸ªå·¥å…·å­¦ä¹ åŸºå‡†ï¼Œå¯¹å…¶ä¸­7ä¸ªå®ç°ä¸€é”®è¯„ä¼°ï¼Œä»æ›´å®è§‚è§†è§’æ•´ä½“è§‚å¯ŸLLMså„èƒ½åŠ›åŠ¨æ€æ¼”å˜ï¼Œæ­ç¤ºå…³é”®æŒ‘æˆ˜ï¼›åŒæ—¶æ”¶é›†33ä¸ªè®­ç»ƒæ•°æ®é›†ä¸­çš„21ä¸ªå¹¶ç»Ÿä¸€æ•°æ®ç»“æ„ï¼Œä¸ºæ¢ç´¢æ½œåœ¨æœºé‡æä¾›ä¾¿åˆ©ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯†åˆ«å·¥å…·å­¦ä¹ å››å¤§å…³é”®æŒ‘æˆ˜  
é€šè¿‡åˆ†æ41ä¸ªLLMsè¶…3000ä¸ªé”™è¯¯æ¡ˆä¾‹ï¼Œæ˜ç¡®å››å¤§æŒ‘æˆ˜ï¼šåŸºå‡†å±€é™æ€§å¯¼è‡´æœ‰æ•ˆç ”ç©¶å°è¯•è¯†åˆ«å›°éš¾ï¼Œè¿›è€Œå¼•å‘LLMsè‡ªä¸»å­¦ä¹ ã€æ³›åŒ–ã€é•¿ç¨‹ä»»åŠ¡è§£å†³èƒ½åŠ›è¢«å¿½è§†ä¸æ¬ ç¼ºã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¢ç´¢å·¥å…·å­¦ä¹ å››å¤§æ½œåœ¨å‘å±•æ–¹å‘  
é’ˆå¯¹æŒ‘æˆ˜æ¢ç´¢æ–¹å‘ï¼šæ„å»ºçœŸå®ä¸–ç•ŒåŸºå‡†ã€å…¼å®¹æ€§æ„ŸçŸ¥çš„è‡ªä¸»å­¦ä¹ ã€é€šè¿‡æ€è€ƒè¿›è¡Œæ¨ç†å­¦ä¹ ã€è¯†åˆ«ä¸å¬å›å…³é”®çº¿ç´¢ï¼Œå¹¶é€šè¿‡åˆæ­¥å®éªŒéªŒè¯è¿™äº›æ–¹å‘æœ‰æ•ˆæ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åˆ†ææŒ‘æˆ˜ä¸æ¢ç´¢æ–¹å‘è¿‡ç¨‹ä¸­ï¼Œå¾—åˆ°è¯¸å¤šå®éªŒæ€§ç»“è®ºã€‚å¦‚åœ¨è‡ªä¸»å­¦ä¹ æ–¹é¢ï¼Œæ— è‡ªä¸»æ€§æ—¶è®­ç»ƒæ•°æ®è§„æ¨¡å¢100å€ä»…å¸¦æ¥5%æ€§èƒ½æå‡ï¼Œè€Œå¯¹å¼€æºå­é›†ï¼ˆå°äº1å€è§„æ¨¡ï¼‰åº”ç”¨ç®€å•è‡ªä¸»å»ºæ¨¡èƒ½å®ç°3%æå‡ï¼Œæ¥è¿‘100å€è§„æ¨¡è®¾ç½®æ€§èƒ½ï¼›æ³›åŒ–èƒ½åŠ›æå‡ä¸Šï¼Œå­¦ä¹ é€šç”¨æ€ç»´å¯çº æ­£è¶…50%åˆå§‹é”™è¯¯æ¡ˆä¾‹ï¼›é•¿ç¨‹ä»»åŠ¡è§£å†³ä¸­ï¼Œè¯†åˆ«å¬å›å…³é”®çº¿ç´¢èƒ½åè½¬GPT - 4oé«˜è¾¾60.9%çš„åˆå§‹é”™è¯¯æ¡ˆä¾‹ç­‰ï¼Œåˆæ­¥éªŒè¯æ‰€æ¢ç´¢æ–¹å‘çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡æ„å»ºçš„ToLeaPå¹³å°ä¸ºå·¥å…·å­¦ä¹ é¢†åŸŸæä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°ä¸æ•°æ®èµ„æºæ•´åˆçš„èŒƒä¾‹ï¼Œåˆ©äºåç»­ç ”ç©¶ç»Ÿä¸€è¯„ä¼°ä¸åˆ©ç”¨æ•°æ®ï¼›å¯¹å››å¤§æŒ‘æˆ˜çš„ç²¾å‡†è¯†åˆ«ä¸ºç§‘ç ”äººå‘˜æ˜ç¡®äº†å½“å‰å·¥å…·å­¦ä¹ ç“¶é¢ˆæ‰€åœ¨ï¼Œé¿å…ç ”ç©¶åç¦»æ ¸å¿ƒé—®é¢˜ï¼›æ¢ç´¢çš„å››å¤§æ½œåœ¨æ–¹å‘ä¸ºçªç ´ç“¶é¢ˆæä¾›äº†å¯è¡Œæ€è·¯ï¼Œåç»­ç ”ç©¶å¯å›´ç»•è¿™äº›æ–¹å‘æ·±å…¥æŒ–æ˜ï¼›åŒæ—¶å…¬å¼€ä»£ç ä¹Ÿæ–¹ä¾¿äº†ç¤¾åŒºå…±å»ºä¸æŠ€æœ¯ä¼ æ’­ï¼Œæ¨åŠ¨æ•´ä¸ªå·¥å…·å­¦ä¹ é¢†åŸŸçš„å‘å±•ã€‚ 

## openthinkimg--learning-to-think-with-images-via-visual-tool-reinforcement-learning
### Abstract
While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".
### ğŸŒŸ è®ºæ–‡è§£è¯» | OpenThinkIMGï¼šç”¨è§†è§‰å·¥å…·å¼ºåŒ–å­¦ä¹ è®©å¤§æ¨¡å‹â€œä»¥å›¾æ€è€ƒâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»èƒ½å¤Ÿçµæ´»å€ŸåŠ©äº¤äº’å¼è§†è§‰è®¤çŸ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä½†è®©å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ç”¨ç±»ä¼¼æ–¹å¼ç»“åˆè§†è§‰å·¥å…·å®ç°è‡ªé€‚åº”è¡Œä¸ºä»å…·æŒ‘æˆ˜ã€‚å½“å‰ç¼ºä¹æ ‡å‡†åŒ–åŸºç¡€è®¾æ–½ï¼Œé˜»ç¢äº†å¤šæ ·å·¥å…·æ•´åˆã€ä¸°å¯Œäº¤äº’æ•°æ®ç”Ÿæˆä¸é²æ£’æ™ºèƒ½ä½“è®­ç»ƒï¼›ä¸”åŸºäºé™æ€æ¼”ç¤ºçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹åŠ¨æ€å·¥å…·è°ƒç”¨çš„ç­–ç•¥æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€å¥—æ¡†æ¶æ¥æ¨è¿›å·¥å…·å¢å¼ºå‹LVLMsçš„å‘å±•ï¼Œè®©æ¨¡å‹èƒ½åƒäººç±»ä¸€æ ·â€œä»¥å›¾æ€è€ƒâ€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºOpenThinkIMGæ¡†æ¶  
å®ƒæ˜¯é¦–ä¸ªé¢å‘å·¥å…·å¢å¼ºå‹LVLMsçš„å¼€æºã€å…¨é¢ç«¯åˆ°ç«¯æ¡†æ¶ã€‚å…·å¤‡æ ‡å‡†åŒ–è§†è§‰å·¥å…·æ¥å£ï¼Œèƒ½ç»Ÿä¸€æ•´åˆå¼‚æ„å·¥å…·ï¼›æ”¯æŒå¯æ‰©å±•çš„è½¨è¿¹ç”Ÿæˆç”¨äºç­–ç•¥åˆå§‹åŒ–ï¼Œè¿˜æä¾›çµæ´»è®­ç»ƒç¯å¢ƒã€‚æ¡†æ¶åŒ…å«å·¥å…·ä¸æ¨¡å‹çš„ç»Ÿä¸€æ³¨å†Œä¸­å¿ƒã€åˆ†å¸ƒå¼éƒ¨ç½²ç­–ç•¥å®ç°é«˜æ•ˆå·¥å…·æ¨ç†ï¼Œä»¥åŠé›†æˆäº†V - ToolRLæ–¹æ³•çš„ç«¯åˆ°ç«¯è®­ç»ƒ pipelineï¼Œæ‰€æœ‰ä»£ç èµ„æºå¼€æºç»´æŠ¤ä»¥ä¿ƒè¿›ç¤¾åŒºåä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºV - ToolRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶  
è€ƒè™‘åˆ°SFTåœ¨åŠ¨æ€å·¥å…·è°ƒç”¨ä¸Šçš„å±€é™ï¼ŒV - ToolRLè®©LVLMså­¦ä¹ è°ƒç”¨å¤–éƒ¨è§†è§‰å·¥å…·çš„è‡ªé€‚åº”ç­–ç•¥ã€‚å®ƒå€ŸåŠ©å·¥å…·äº¤äº’åé¦ˆç›´æ¥ä¼˜åŒ–ä»»åŠ¡æˆåŠŸç‡ï¼Œä½¿æ¨¡å‹è‡ªä¸»æ¢ç´¢å‘ç°æœ€ä¼˜å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºé«˜è´¨é‡è§†è§‰å·¥å…·ä½¿ç”¨è½¨è¿¹çš„ä¸‰é˜¶æ®µ pipeline  
åˆ©ç”¨æ¨¡å‹èƒ½åŠ›åšåˆå§‹åŠ¨ä½œè§„åˆ’ï¼Œè‡ªåŠ¨å®Œæˆå·¥å…·è°ƒç”¨å’ŒåŸç†è§£æï¼Œç»“åˆå¤šé˜¶æ®µè¿‡æ»¤ï¼ˆåŸºäºè§„åˆ™éªŒè¯å’Œäººå·¥ç›‘ç£ï¼‰ä¿éšœç”¨äºæœ‰ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ çš„æ•°æ®è´¨é‡ï¼Œå®ç°å¯æ‰©å±•ä¸”é€‚åº”æ€§å¼ºçš„è½¨è¿¹ç”Ÿæˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸ŠéªŒè¯V - ToolRLï¼šåŸºäºQwen2 - VL - 2Bè®­ç»ƒçš„RLæ™ºèƒ½ä½“ï¼Œæ¯”SFTåˆå§‹åŒ–çš„ç‰ˆæœ¬æ€§èƒ½æå‡28.83ä¸ªç‚¹ï¼›å¹³å‡è¶…è¿‡Tacoã€CogComç­‰æœ‰ç›‘ç£å·¥å…·å­¦ä¹ åŸºçº¿12.7ä¸ªç‚¹ï¼›è¿˜æ¯”GPT - 4.1ç­‰é—­æºæ¨¡å‹å‡†ç¡®ç‡é«˜8.68ä¸ªç‚¹ï¼Œå……åˆ†å±•ç°äº†æ–¹æ³•åœ¨å·¥å…·å¢å¼ºè§†è§‰æ¨ç†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŸºç¡€è®¾æ–½å±‚é¢ï¼šOpenThinkIMGçš„æ ‡å‡†åŒ–å·¥å…·æ¥å£ã€ç»Ÿä¸€æ³¨å†Œä¸åˆ†å¸ƒå¼éƒ¨ç½²ç­‰è®¾è®¡ï¼Œä¸ºåç»­å·¥å…·å¢å¼ºå‹å¤šæ¨¡æ€æ¨¡å‹å¼€å‘æä¾›äº†å¯å¤ç”¨çš„åŸºç¡€è®¾æ–½èŒƒå¼ï¼Œé™ä½å·¥å…·æ•´åˆä¸è§„æ¨¡åŒ–è®­ç»ƒé—¨æ§›ã€‚
2. è®­ç»ƒæ–¹æ³•å±‚é¢ï¼šV - ToolRLå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥è§†è§‰å·¥å…·è°ƒç”¨ç­–ç•¥å­¦ä¹ ï¼Œä¸ºè§£å†³SFTæ³›åŒ–ä¸è¶³é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨å·¥å…·å¢å¼ºå‹æ¨¡å‹åŠ¨æ€é€‚åº”èƒ½åŠ›è®­ç»ƒä¸Šçš„æ½œåŠ›ã€‚
3. æ•°æ®ç”Ÿæˆå±‚é¢ï¼šä¸‰é˜¶æ®µè½¨è¿¹ç”Ÿæˆ pipeline ç»“åˆè‡ªåŠ¨å¤„ç†ä¸è´¨é‡è¿‡æ»¤æœºåˆ¶ï¼Œä¸ºé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡å·¥å…·äº¤äº’æ•°æ®æä¾›äº†å¯å‚è€ƒçš„æµç¨‹ï¼Œå¹³è¡¡äº†è§„æ¨¡ä¸è´¨é‡ã€‚

## toolace-dev--self-improving-tool-learning-via-decomposition-and-evolution
### Abstract
The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolACE - DEVï¼šè®©å°æ¨¡å‹ä¹Ÿèƒ½è‡ªä¸»è¿›åŒ–çš„å·¥å…·å­¦ä¹ æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›èƒ½è®©å…¶è·å–å®æ—¶ä¿¡æ¯ã€å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä½†å½“å‰æå‡è¯¥èƒ½åŠ›çš„æ–¹æ³•å¤šä¾èµ–ç”¨å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚GPT - 4ï¼‰åˆæˆæ•°æ®æ¥è’¸é¦ï¼Œå­˜åœ¨ä¸‰å¤§é—®é¢˜ï¼šä¸€æ˜¯æ¨ç†æˆæœ¬é«˜ï¼Œç”Ÿæˆå¤§è§„æ¨¡è®­ç»ƒæ•°æ®æ—¶è°ƒç”¨å…ˆè¿›æ¨¡å‹èŠ±è´¹å·¨å¤§ï¼›äºŒæ˜¯æ•°æ®å…¼å®¹æ€§å·®ï¼Œå…ˆè¿›æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çŸ¥è¯†èŒƒå›´å·®å¼‚å¤§ï¼Œåˆæˆæ•°æ®åˆ†å¸ƒä¸åŒï¼Œæ˜“è®©ç›®æ ‡æ¨¡å‹å­¦ä¸åˆ°æ³›åŒ–èƒ½åŠ›ç”šè‡³äº§ç”Ÿå¹»è§‰ï¼›ä¸‰æ˜¯æ•°æ®éšç§é—®é¢˜ï¼Œå¾ˆå¤šå«éšç§çš„ç”¨æˆ·æŸ¥è¯¢æ²¡æ³•ç”¨å¤–éƒ¨å…ˆè¿›æ¨¡å‹åˆæˆæ•°æ®ã€‚åŒæ—¶ï¼ŒæŠŠè‡ªè¿›åŒ–ç”¨äºå·¥å…·å­¦ä¹ åœºæ™¯ä¹Ÿæœ‰æŒ‘æˆ˜ï¼Œè½»é‡æ¨¡å‹éš¾ç›´æ¥ä»ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆæ–°å·¥å…·å’Œå‡†ç¡®è°ƒç”¨ã€‚æ‰€ä»¥éœ€è¦æ–°æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºToolACE - DEVè‡ªè¿›åŒ–å·¥å…·å­¦ä¹ æ¡†æ¶
è¿™æ˜¯é¦–ä¸ªä¸ºæå‡LLMså·¥å…·è°ƒç”¨èƒ½åŠ›è®¾è®¡çš„è‡ªè¿›åŒ–æ¡†æ¶ï¼Œèƒ½è®©è½»é‡æ¨¡å‹å…·å¤‡è‡ªè¿›åŒ–èƒ½åŠ›ï¼Œä¸å†è¿‡åº¦ä¾èµ–å¤–éƒ¨å…ˆè¿›å¤§æ¨¡å‹ï¼Œé€šè¿‡è‡ªèº«è¿­ä»£æ¥æå‡å·¥å…·å­¦ä¹ èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä»»åŠ¡åˆ†è§£ä¸å­ä»»åŠ¡è®¾è®¡
é¦–å…ˆè®¾è®¡å·¥å…·æ–‡æ¡£é€‚é…å­ä»»åŠ¡ï¼Œèšç„¦å·¥å…·å®šä¹‰æ¥è®©æ¨¡å‹åè®­ç»ƒï¼Œæå‡æ¨¡å‹å¯¹å·¥å…·çš„ç†è§£ï¼Œè¿›è€Œå¢å¼ºå·¥å…·ä½¿ç”¨å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç„¶åæŠŠä¼ ç»Ÿåªå…³æ³¨å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è®­ç»ƒç›®æ ‡ï¼Œåˆ†è§£ä¸ºå·¥å…·ç”Ÿæˆå’Œå·¥å…·è°ƒç”¨ä¸¤ä¸ªä»»åŠ¡ã€‚å·¥å…·ç”Ÿæˆè®©æ¨¡å‹èƒ½åŸºäºæŸ¥è¯¢ç”Ÿæˆå€™é€‰å·¥å…·ï¼Œå·¥å…·è°ƒç”¨æå‡è°ƒç”¨å‡†ç¡®æ€§ï¼Œä¸ºè‡ªè¿›åŒ–æ‰“ä¸‹åŸºç¡€ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªè¿›åŒ–èŒƒå¼å¼•å…¥
ç»è¿‡å‰ä¸¤é˜¶æ®µè®­ç»ƒåï¼Œç»™ç›®æ ‡æ¨¡å‹æ–°ç”¨æˆ·æŸ¥è¯¢ï¼Œæ¨¡å‹è¿­ä»£ç”Ÿæˆå€™é€‰å·¥å…·å’Œå¯¹åº”è°ƒç”¨ï¼Œå½¢æˆè‡ªè¿›åŒ–æœºåˆ¶ï¼Œéšç€æ—¶é—´è‡ªåŠ¨æå‡å·¥å…·ä½¿ç”¨æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹ä¸Šåšäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¿˜æ¢ç©¶äº†è‡ªè¿›åŒ–æ½œåŠ›éšæ¨¡å‹å¤§å°å˜åŒ–çš„è§„å¾‹ï¼ˆæ–‡ä¸­æœªè¯¦ç»†å±•å¼€å®éªŒæ•°æ®ï¼Œä½†å¼ºè°ƒäº†å¹¿æ³›å®éªŒéªŒè¯æœ‰æ•ˆæ€§ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»»åŠ¡åˆ†è§£æ€è·¯ï¼šå°†å¤æ‚çš„å·¥å…·å­¦ä¹ ç›®æ ‡æ‹†åˆ†æˆæ›´ç»†çš„å­ä»»åŠ¡ï¼Œè¿™ç§åˆ†è§£ä»»åŠ¡æå‡èƒ½åŠ›çš„æ€è·¯å¯æ¨å¹¿åˆ°å…¶ä»–å¤æ‚AIä»»åŠ¡å­¦ä¹ ä¸­ï¼Œæ¯”å¦‚å¤šæ­¥éª¤çš„æ¨ç†ä»»åŠ¡ç­‰ã€‚
2. è‡ªè¿›åŒ–ç†å¿µï¼šè®©æ¨¡å‹è‡ªä¸»ç”Ÿæˆæˆ–ä¼˜åŒ–è®­ç»ƒæ•°æ®æ¥è¿­ä»£æå‡ï¼Œå‡å°‘å¯¹å¤–éƒ¨æ˜‚è´µèµ„æºä¾èµ–ï¼Œä¸ºèµ„æºæœ‰é™æƒ…å†µä¸‹æ¨¡å‹èƒ½åŠ›æå‡æä¾›äº†æ€è·¯ï¼Œå°æ¨¡å‹ä¹Ÿèƒ½èµ°è‡ªè¿›åŒ–æå‡ä¹‹è·¯ã€‚
3. å·¥å…·ç›¸å…³å­ä»»åŠ¡è®¾è®¡ï¼šå·¥å…·æ–‡æ¡£é€‚é…è¿™ç±»é’ˆå¯¹å·¥å…·ç†è§£çš„å­ä»»åŠ¡è®¾è®¡ï¼Œä¸ºæå‡æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸï¼ˆå·¥å…·é¢†åŸŸï¼‰çŸ¥è¯†å’Œèƒ½åŠ›çš„æŒæ¡æä¾›äº†å‚è€ƒï¼Œå¯ç”¨äºå…¶ä»–é¢†åŸŸç‰¹å®šèƒ½åŠ›æå‡åœºæ™¯ã€‚

## toolrl--reward-is-all-tool-learning-needs
### Abstract
Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolRLï¼šå·¥å…·å­¦ä¹ ï¼Œå¥–åŠ±è‡³ä¸Š

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è·å¾—å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä½†SFTåœ¨é™Œç”Ÿæˆ–å¤æ‚å·¥å…·ä½¿ç”¨åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½åœ¨æ¨ç†å’Œæ³›åŒ–æ–¹é¢å±•ç°æ½œåŠ›ï¼Œç„¶è€Œå·¥å…·ä½¿ç”¨çš„å¥–åŠ±è®¾è®¡å­˜åœ¨æŒ‘æˆ˜ï¼šå¤šå·¥å…·è°ƒç”¨å‚æ•°å¤šæ ·ï¼Œç²—ç²’åº¦å¥–åŠ±ï¼ˆå¦‚ç­”æ¡ˆåŒ¹é…ï¼‰æ— æ³•æä¾›æœ‰æ•ˆå­¦ä¹ æ‰€éœ€çš„ç»†ç²’åº¦åé¦ˆã€‚å› æ­¤ï¼Œæ¢ç´¢é€‚ç”¨äºå·¥å…·é€‰æ‹©ä¸åº”ç”¨ä»»åŠ¡çš„RLèŒƒå¼ä¸‹å¥–åŠ±è®¾è®¡è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡ç³»ç»Ÿç ”ç©¶RLèŒƒå¼ä¸‹å·¥å…·é€‰æ‹©ä¸åº”ç”¨ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡  
å¯¹å¥–åŠ±ç­–ç•¥çš„ç±»å‹ã€è§„æ¨¡ã€ç²’åº¦å’Œæ—¶é—´åŠ¨æ€ç­‰ç»´åº¦è¿›è¡Œå¹¿æ³›æ¢ç´¢ï¼Œåˆ†æä¸åŒå¥–åŠ±ç­–ç•¥å¯¹å·¥å…·ä½¿ç”¨å­¦ä¹ çš„å½±å“ï¼Œä¸ºå·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡æä¾›å…¨é¢è®¤çŸ¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºé’ˆå¯¹æ€§å¥–åŠ±è®¾è®¡æ¡†æ¶å¹¶ç»“åˆGRPOè®­ç»ƒLLMs  
åŸºäºå¯¹å¥–åŠ±ç­–ç•¥çš„æ¢ç´¢æ´å¯Ÿï¼Œè®¾è®¡é€‚åˆå·¥å…·ä½¿ç”¨ä»»åŠ¡çš„åŸåˆ™æ€§å¥–åŠ±æ–¹æ¡ˆï¼Œå¹¶åˆ©ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è®­ç»ƒå‡ºçš„æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼šç›¸æ¯”åŸºç¡€æ¨¡å‹æ€§èƒ½æå‡17%ï¼Œç›¸æ¯”SFTæ¨¡å‹æå‡15%ï¼›ä¸”è®­ç»ƒè¿‡ç¨‹é²æ£’ã€å¯æ‰©å±•ä¸”ç¨³å®šï¼Œå¥–åŠ±æ›²çº¿åœ¨è®­ç»ƒä¸­å¿«é€Ÿä¸Šå‡ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å­¦ä¹ æ•ˆæœä¸æ³›åŒ–èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¥–åŠ±è®¾è®¡ç»´åº¦çš„å…¨é¢æ¢ç´¢ä¸ºåç»­LLM - agentè®­ç»ƒä¸­å¥–åŠ±æœºåˆ¶ç ”ç©¶æä¾›äº†ä¸°å¯Œå‚è€ƒï¼Œå¦‚è®¤è¯†åˆ°é•¿æ¨ç†è½¨è¿¹å¹¶éè¶Šå¥½ã€åŠ¨æ€å¥–åŠ±è§„æ¨¡åŠ©åŠ›è¡Œä¸ºè¿‡æ¸¡ã€ç»†ç²’åº¦å¥–åŠ±åˆ†è§£æå‡å­¦ä¹ ç¨³å®šæ€§ç­‰ç»“è®ºï¼Œå¯æŒ‡å¯¼åç»­ä¼˜åŒ–å¥–åŠ±ç­–ç•¥ã€‚  
2. é¦–æ¬¡å°†RLåº”ç”¨äºé€šç”¨TIRä»»åŠ¡å¹¶ç»™å‡ºå¥–åŠ±è®¾è®¡å®è¯è·¯çº¿å›¾ï¼Œä¸ºæ‰“é€ æ›´å¼ºå¤§è‡ªä¸»çš„LLMæ™ºèƒ½ä½“å¼€è¾Ÿé“è·¯ï¼Œæ¨åŠ¨é¢†åŸŸåœ¨å·¥å…·é›†æˆæ¨ç†æ–¹å‘çš„ç ”ç©¶ä¸åº”ç”¨è½åœ°ã€‚  
3. å¼€æºä»£ç ä¾¿äºåç»­ç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šå¼€å±•å·¥ä½œï¼Œé™ä½ç ”ç©¶é—¨æ§›ï¼Œä¿ƒè¿›é¢†åŸŸå‘å±•ã€‚

## familytool--a-multi-hop-personalized-tool-use-benchmark
### Abstract
The integration of tool learning with Large Language Models (LLMs) has expanded their capabilities in handling complex tasks by leveraging external tools. However, existing benchmarks for tool learning inadequately address critical real-world personalized scenarios, particularly those requiring multi-hop reasoning and inductive knowledge adaptation in dynamic environments. To bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a family-based knowledge graph (KG) that simulates personalized, multi-hop tool use scenarios. FamilyTool, including base and extended datasets, challenges LLMs with queries spanning from 1 to 4 relational hops (e.g., inferring familial connections and preferences) and 2 to 6 hops respectively, and incorporates an inductive KG setting where models must adapt to unseen user preferences and relationships without re-training, a common limitation in prior approaches that compromises generalization. We further propose KGETool: a simple KG-augmented evaluation pipeline to systematically assess LLMs' tool use ability in these settings. Experiments reveal significant performance gaps in state-of-the-art LLMs, with accuracy dropping sharply as hop complexity increases and inductive scenarios exposing severe generalization deficits. These findings underscore the limitations of current LLMs in handling personalized, evolving real-world contexts and highlight the urgent need for advancements in tool-learning frameworks. FamilyTool serves as a critical resource for evaluating and advancing LLM agents' reasoning, adaptability, and scalability in complex, dynamic environments. Code and dataset are available at \href{https://github.com/yxzwang/FamilyTool}{https://github.com/yxzwang/FamilyTool}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | FamilyToolï¼šèšç„¦ä¸ªæ€§åŒ–å¤šè·³å·¥å…·ä½¿ç”¨çš„å…¨æ–°åŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å·¥å…·å­¦ä¹ çš„ç»“åˆæ‹“å±•äº†å…¶å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰å·¥å…·å­¦ä¹ åŸºå‡†åœ¨åº”å¯¹ç°å®ä¸–ç•Œä¸­ä¸ªæ€§åŒ–åœºæ™¯ï¼ˆå°¤å…¶æ˜¯éœ€è¦å¤šè·³æ¨ç†å’ŒåŠ¨æ€ç¯å¢ƒä¸‹å½’çº³æ€§çŸ¥è¯†é€‚é…çš„åœºæ™¯ï¼‰æ—¶å­˜åœ¨ä¸è¶³ã€‚æ¯”å¦‚åœ¨è®¾å¤‡ç«¯å·¥å…·ä½¿ç”¨é‡Œï¼Œå®¶åº­åœºæ™¯ä¸‹çš„ä¸ªæ€§åŒ–å·¥å…·è°ƒç”¨å¸¸éœ€å¤šè·³æ¨ç†ï¼ˆç»“åˆå®¶åº­æˆå‘˜å…³ç³»ã€åå¥½ç­‰ï¼‰ä¸å½’çº³æ¨ç†ï¼ˆåº”å¯¹æ–°å‡ºç°çš„å…³ç³»å’Œåå¥½ä¸”æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼‰ï¼Œè€Œå½“å‰ç¼ºä¹é’ˆå¯¹è¿™ç±»åœºæ™¯çš„è¯„ä¼°åŸºå‡†ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè®ºæ–‡æå‡ºäº†FamilyToolåŸºå‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºFamilyToolåŸºå‡†  
æ„å»ºåŸºäºå®¶åº­çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„å…¨æ–°åŸºå‡†ï¼Œæ¶µç›–åŸºç¡€æ•°æ®é›†ï¼ˆFamilyTool-bï¼‰å’Œæ‰©å±•æ•°æ®é›†ï¼ˆFamilyTool-eï¼‰ï¼Œåˆ†åˆ«é’ˆå¯¹å·¥å…·ä½¿ç”¨æŸ¥è¯¢è®¾ç½®1 - 4è·³ã€2 - 6è·³çš„å¤šè·³æ¨ç†æŒ‘æˆ˜ï¼Œæ¨¡æ‹Ÿä¸ªæ€§åŒ–å¤šè·³å·¥å…·ä½¿ç”¨åœºæ™¯ï¼Œè®©LLMsåœ¨æ¨æ–­å®¶åº­å…³ç³»ã€åå¥½ç­‰ä»»åŠ¡ä¸­æ¥å—è€ƒéªŒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥å½’çº³å¼KGè®¾å®š  
åœ¨åŸºå‡†ä¸­åŠ å…¥å½’çº³çŸ¥è¯†å›¾è°±åœºæ™¯ï¼Œè¦æ±‚æ¨¡å‹åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹é€‚é…ä»æœªè§è¿‡çš„ç”¨æˆ·åå¥½å’Œå…³ç³»ï¼Œè§£å†³äº†ä»¥å¾€æ–¹æ³•æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæ›´è´´è¿‘ç°å®ä¸­å®¶åº­å…³ç³»ã€åå¥½åŠ¨æ€å˜åŒ–çš„æƒ…å†µã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡KGEToolè¯„ä¼° pipeline  
æå‡ºç®€å•çš„çŸ¥è¯†å›¾è°±å¢å¼ºå‹è¯„ä¼°æµç¨‹KGEToolï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°LLMsåœ¨ä¸Šè¿°å¤šè·³ã€å½’çº³åœºæ™¯ä¸‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä¸ºè¡¡é‡æ¨¡å‹è¡¨ç°æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨FamilyToolåŸºå‡†ä¸Šå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼šéšç€æ¨ç†è·³æ•°å¤æ‚åº¦å¢åŠ ï¼Œæ¨¡å‹å‡†ç¡®ç‡æ€¥å‰§ä¸‹é™ï¼›åœ¨å½’çº³åœºæ™¯ä¸‹ï¼Œæ¨¡å‹æš´éœ²å‡ºä¸¥é‡çš„æ³›åŒ–èƒ½åŠ›ç¼ºé™·ã€‚è¿™å……åˆ†å‡¸æ˜¾äº†ç°æœ‰LLMsåœ¨å¤„ç†ä¸ªæ€§åŒ–ã€åŠ¨æ€æ¼”å˜çš„ç°å®åœºæ™¯æ—¶çš„å±€é™æ€§ï¼Œä¹Ÿè¡¨æ˜å·¥å…·å­¦ä¹ æ¡†æ¶äºŸéœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åœºæ™¯åˆ›æ–°ï¼šèšç„¦å®¶åº­åœºæ™¯ä¸‹ä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨ï¼Œå¡«è¡¥äº†é¢†åŸŸç©ºç™½ï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜äº†â€œè´´è¿‘çœŸå®ç”Ÿæ´»åœºæ™¯ã€å…³æ³¨ä¸ªæ€§åŒ–ä¸åŠ¨æ€æ€§â€çš„æ–¹å‘ã€‚  
2. åŸºå‡†æ„å»ºï¼šæ‰“é€ çš„FamilyToolåŒ…å«å¤šè·³ä¸å½’çº³è®¾å®šï¼Œä¸ºè¯„ä¼°LLMsåœ¨å¤æ‚å·¥å…·ä½¿ç”¨åœºæ™¯çš„æ¨ç†ã€é€‚é…èƒ½åŠ›æä¾›äº†ä¼˜è´¨èµ„æºï¼Œåç»­å¯åŸºäºæ­¤åŸºå‡†æŒç»­æ¨è¿›æ¨¡å‹ä¼˜åŒ–ã€‚  
3. è¯„ä¼°æµç¨‹ï¼šKGEToolçš„æå‡ºä¸ºçŸ¥è¯†å›¾è°±è¾…åŠ©ä¸‹çš„LLMå·¥å…·ä½¿ç”¨è¯„ä¼°æä¾›äº†ç®€æ´æœ‰æ•ˆçš„èŒƒå¼ï¼Œå¯å¯å‘æ›´å¤šç»“åˆå¤–éƒ¨çŸ¥è¯†æºçš„å·¥å…·å­¦ä¹ è¯„ä¼°æ–¹æ³•è®¾è®¡ã€‚  
4. é—®é¢˜æ­ç¤ºï¼šæ¸…æ™°å±•ç°ç°æœ‰LLMsçŸ­æ¿ï¼Œè®©å­¦ç•Œæ›´æ˜ç¡®â€œæå‡å¤šè·³æ¨ç†ã€å½’çº³æ³›åŒ–èƒ½åŠ›ä»¥é€‚é…åŠ¨æ€ç°å®åœºæ™¯â€æ˜¯å·¥å…·å­¦ä¹ æ–¹å‘çš„å…³é”®æ”»åšç‚¹ã€‚  

æ€»ä¹‹ï¼ŒFamilyToolä¸ºLLMæ™ºèƒ½ä½“åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„æ¨ç†ã€é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§è¯„ä¼°ä¸å‘å±•æä¾›äº†å…³é”®èµ„æºï¼Œæ— è®ºæ˜¯åŸºå‡†ç†å¿µè¿˜æ˜¯è¯„ä¼°æ–¹æ³•ï¼Œéƒ½ä¸ºAIé¢†åŸŸç›¸å…³ç ”ç©¶æ³¨å…¥äº†æ–°çš„æ€è€ƒä¸åŠ¨åŠ›ï½ 

## select-me!-when-you-need-a-tool--a-black-box-text-attack-on-tool-selection
### Abstract
Tool learning serves as a powerful auxiliary mechanism that extends the capabilities of large language models (LLMs), enabling them to tackle complex tasks requiring real-time relevance or high precision operations. Behind its powerful capabilities lie some potential security issues. However, previous work has primarily focused on how to make the output of the invoked tools incorrect or malicious, with little attention given to the manipulation of tool selection. To fill this gap, we introduce, for the first time, a black-box text-based attack that can significantly increase the probability of the target tool being selected in this paper. We propose a two-level text perturbation attack witha coarse-to-fine granularity, attacking the text at both the word level and the character level. We conduct comprehensive experiments that demonstrate the attacker only needs to make some perturbations to the tool's textual information to significantly increase the possibility of the target tool being selected and ranked higher among the candidate tools. Our research reveals the vulnerability of the tool selection process and paves the way for future research on protecting this process.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å·¥å…·é€‰æ‹©ä¹Ÿèƒ½è¢«æ”»å‡»ï¼Ÿæ­ç§˜å¤§æ¨¡å‹å·¥å…·é€‰æ‹©é˜¶æ®µçš„é»‘ç›’æ–‡æœ¬æ”»å‡»

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å·¥å…·å­¦ä¹ ï¼ˆTool Learningï¼‰æ‹“å±•èƒ½åŠ›ï¼Œèƒ½åº”å¯¹éœ€å®æ—¶ä¿¡æ¯æˆ–é«˜ç²¾åº¦æ“ä½œçš„å¤æ‚ä»»åŠ¡ï¼Œä½†å·¥å…·å­¦ä¹ èƒŒåå­˜åœ¨å®‰å…¨éšæ‚£ã€‚è¿‡å¾€ç ”ç©¶å¤šèšç„¦å·¥å…·è°ƒç”¨è¾“å‡ºçš„é”™è¯¯æˆ–æ¶æ„æ”»å‡»ï¼Œå¯¹å·¥å…·é€‰æ‹©é˜¶æ®µçš„æ“çºµå…³æ³¨ç”šå°‘ã€‚è€Œæ”»å‡»è€…æœ‰æ“çºµå·¥å…·é€‰æ‹©çš„åŠ¨æœºï¼šä¸€æ˜¯å•†ä¸šåˆ©ç›Šï¼ˆå·¥å…·æä¾›å•†å¸Œæœ›è‡ªå®¶å·¥å…·è¢«æ›´å¤šä½¿ç”¨ä»¥ç›ˆåˆ©ï¼‰ï¼›äºŒæ˜¯åŠ©åŠ›æ¶æ„å·¥å…·è°ƒç”¨ï¼ˆè®©æ¶æ„å·¥å…·æ›´æ˜“è¢«è°ƒç”¨ï¼‰ã€‚ä¸ºå¡«è¡¥å·¥å…·é€‰æ‹©é˜¶æ®µå®‰å…¨ç ”ç©¶çš„ç©ºç™½ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºé’ˆå¯¹å·¥å…·é€‰æ‹©çš„é»‘ç›’æ–‡æœ¬æ”»å‡»æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡æå‡ºå·¥å…·é€‰æ‹©é»‘ç›’æ–‡æœ¬æ”»å‡»  
è¿‡å¾€å·¥ä½œé›†ä¸­åœ¨å·¥å…·è°ƒç”¨é˜¶æ®µæ”»å‡»ï¼Œæœ¬æ–‡åˆ™èšç„¦å·¥å…·é€‰æ‹©é˜¶æ®µï¼Œæå‡ºé»‘ç›’æ–‡æœ¬æ”»å‡»æ–¹æ³•ã€‚æ”»å‡»è€…æ— éœ€çŸ¥æ™“å·¥å…·é€‰æ‹©æ¨¡å‹ï¼ˆTSMï¼‰å†…éƒ¨å‚æ•°ï¼Œä»…é€šè¿‡ä¿®æ”¹ç›®æ ‡å·¥å…·æ–‡æœ¬ä¿¡æ¯ï¼ˆå¦‚åç§°ã€æè¿°ç­‰ï¼‰ï¼Œè¯¯å¯¼TSMï¼Œå¢åŠ ç›®æ ‡å·¥å…·è¢«é€‰ä¸­æˆ–æ’åæå‡çš„æ¦‚ç‡ï¼Œä¸”ä¸å½±å“å·¥å…·æ­£å¸¸åŠŸèƒ½ï¼Œä¿è¯æ”»å‡»éšè”½æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç²—ç»†ç²’åº¦ç»“åˆçš„ä¸¤çº§æ–‡æœ¬æ‰°åŠ¨æ”»å‡»  
é‡‡ç”¨ä»ç²—åˆ°ç»†ï¼ˆword level + character levelï¼‰çš„ä¸¤çº§æ–‡æœ¬æ‰°åŠ¨æ”»å‡»æ–¹å¼ã€‚åœ¨å•è¯å±‚é¢å’Œå­—ç¬¦å±‚é¢å¯¹ç›®æ ‡å·¥å…·æ–‡æœ¬è¿›è¡Œæ‰°åŠ¨ï¼Œä»¥æ­¤å¹²æ‰°å·¥å…·é€‰æ‹©æ¨¡å‹çš„åˆ¤æ–­ï¼Œå®ç°è®©ç›®æ ‡å·¥å…·æ›´æ˜“è¢«é€‰ä¸­çš„ç›®çš„ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸‰ä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢å™¨ä¸Šå¼€å±•å…¨é¢å®éªŒï¼ŒéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä»…éœ€å¯¹å·¥å…·æ–‡æœ¬ä¿¡æ¯åšä¸€äº›æ‰°åŠ¨ï¼Œå°±èƒ½æ˜¾è‘—æå‡ç›®æ ‡å·¥å…·è¢«é€‰ä¸­åŠåœ¨å€™é€‰å·¥å…·ä¸­æ’åæ›´é å‰çš„å¯èƒ½æ€§ï¼›åŒæ—¶è¿˜åˆ†æäº†æŸ¥è¯¢æ¬¡æ•°ã€æ”»å‡»é¢„ç®—å¯¹æ”»å‡»æˆåŠŸç‡çš„å½±å“ï¼Œä»¥åŠæ”»å‡»çš„å¯è¿ç§»æ€§ï¼Œè¯æ˜æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„ç°å®æ€§ä¸å¯è¡Œæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å®‰å…¨è§†è§’å¯å‘ï¼šæ­ç¤ºå·¥å…·é€‰æ‹©è¿‡ç¨‹å› ä¾èµ–æ–‡æœ¬å†…å®¹å­˜åœ¨æ­¤å‰è¢«å¿½è§†çš„å®‰å…¨æ¼æ´ï¼Œä¸ºå¤§æ¨¡å‹å·¥å…·å­¦ä¹ å®‰å…¨é¢†åŸŸç ”ç©¶æä¾›æ–°è§†è§’ï¼Œåç»­å¯å›´ç»•ä¿æŠ¤å·¥å…·é€‰æ‹©è¿‡ç¨‹å±•å¼€ç ”ç©¶ã€‚  
2. æ”»å‡»æ–¹æ³•åˆ›æ–°ï¼šé¦–æ¬¡ç”¨æ–‡æœ¬æ‰°åŠ¨æ”»å‡»å·¥å…·é€‰æ‹©é˜¶æ®µï¼Œä¸ºæ“çºµå·¥å…·é€‰æ‹©ç»“æœæä¾›æ–°æ‰‹æ®µï¼Œç›¸å…³æ”»å‡»æ€è·¯å’ŒæŠ€æœ¯è·¯çº¿å¯è¢«å®‰å…¨ç ”ç©¶äººå‘˜å€Ÿé‰´ï¼Œç”¨äºæ¢ç´¢æ›´å¤šå®‰å…¨æ”»é˜²åœºæ™¯ã€‚  
3. å®éªŒç»´åº¦å…¨é¢ï¼šä»å¤šæ¨¡å‹éªŒè¯ã€æ”»å‡»å½±å“å› ç´ åˆ†æåˆ°è¿ç§»æ€§æ£€éªŒç­‰ç»´åº¦å¼€å±•å®éªŒï¼Œè¿™ç§å…¨é¢å®éªŒè®¾è®¡æ€è·¯å¯ä¸ºç±»ä¼¼å®‰å…¨è¯„ä¼°ç±»ç ”ç©¶æä¾›å‚è€ƒï¼ŒåŠ©åŠ›å®Œå–„ç ”ç©¶çš„ä¸¥è°¨æ€§ä¸å®ç”¨æ€§ã€‚

## toolace-r--model-aware-iterative-training-and-adaptive-refinement-for-tool-learning
### Abstract
Tool learning, which allows Large Language Models (LLMs) to leverage external tools for solving complex user tasks, has emerged as a promising avenue for extending model capabilities. However, existing approaches primarily focus on data synthesis for fine-tuning LLMs to invoke tools effectively, largely ignoring how to fully stimulate the potential of the model. In this paper, we propose ToolACE-R, a novel framework that includes both model-aware iterative training and adaptive refinement for tool learning. ToolACE-R features a model-aware iterative training procedure that progressively adjust training samples based on the model's evolving capabilities to maximize its potential. Additionally, it incorporates self-refinement training corpus which emphasizes LLM's ability to iteratively refine their tool calls, optimizing performance without requiring external feedback. Furthermore, we introduce adaptive self-refinement mechanism for efficient test-time scaling, where the trained model can autonomously determine when to stop the process based on iterative self-refinement. We conduct extensive experiments across several benchmark datasets, showing that ToolACE-R achieves competitive performance compared to advanced API-based models. The performance of tool invocation can be further improved efficiently through adaptive self-refinement. These results highlight the effectiveness and generalizability of ToolACE-R, offering a promising direction for more efficient and scalable tool learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolACE-Rï¼šé‡Šæ”¾å¤§æ¨¡å‹å·¥å…·å­¦ä¹ æ½œåŠ›çš„è¿­ä»£ä¸è‡ªé€‚åº”æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥å…·å­¦ä¹ è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å€ŸåŠ©å¤–éƒ¨å·¥å…·è§£å†³å¤æ‚ä»»åŠ¡ï¼Œæ˜¯æ‰©å±•æ¨¡å‹èƒ½åŠ›çš„é‡è¦æ–¹å‘ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼š  
1. **æ•°æ®é€‚é…æ€§ä¸è¶³**ï¼šå¤šæ•°ç ”ç©¶èšç„¦ç”¨å…ˆè¿›æ¨¡å‹åˆæˆæ•°æ®æ¥å¾®è°ƒLLMsä»¥è°ƒç”¨å·¥å…·ï¼Œå´å¿½ç•¥åˆæˆæ•°æ®è‹¥è¶…å‡ºæ¨¡å‹å½“å‰çŸ¥è¯†èŒƒå›´ï¼Œæ˜“å¯¼è‡´æ€§èƒ½ä¸‹é™æˆ–å¹»è§‰ï¼›ä¸”å¦‚ä½•ä¸ºæ¨¡å‹é€‰åˆé€‚è®­ç»ƒæ ·æœ¬ä»æ˜¯éš¾é¢˜ã€‚  
2. **æ¨¡å‹æ½œåŠ›ä¸æ¨ç†æ•ˆç‡å¾…æŒ–æ˜**ï¼šä¸€æ–¹é¢ï¼Œå·¥å…·å­¦ä¹ ä¸­é²œå°‘é€šè¿‡æ•°æ®å¢å¼ºç­‰æŠ€æœ¯å……åˆ†åˆ©ç”¨å·²æœ‰æ•°æ®ã€é‡Šæ”¾æ¨¡å‹å†…åœ¨æ½œåŠ›ï¼›å¦ä¸€æ–¹é¢ï¼Œæµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾ï¼ˆå¦‚è¿­ä»£ä¼˜åŒ–è¾“å‡ºï¼‰åœ¨å·¥å…·å­¦ä¹ åœºæ™¯å…³æ³¨å°‘ï¼Œä¸”æ— å·®åˆ«ç¼©æ”¾å¯¹ç®€å•æŸ¥è¯¢ä½æ•ˆï¼Œéœ€è‡ªé€‚åº”ç­–ç•¥ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
é’ˆå¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡º**ToolACE-R**æ¡†æ¶ï¼Œä»è®­ç»ƒå’Œæ¨ç†ä¸¤é˜¶æ®µé‡Šæ”¾æ¨¡å‹å·¥å…·å­¦ä¹ æ½œåŠ›ï¼š  

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¨¡å‹æ„ŸçŸ¥çš„è¿­ä»£è®­ç»ƒï¼ˆModel-aware Iterative Trainingï¼‰  
åŸºäºæ¨¡å‹èƒ½åŠ›æ¼”åŒ–ï¼Œç”¨â€œæ¨¡å‹æ„ŸçŸ¥éš¾åº¦åº¦é‡â€è¿­ä»£è°ƒæ•´è®­ç»ƒæ ·æœ¬ï¼Œè®©è®­ç»ƒæ›´è´´åˆæ¨¡å‹å½“å‰æ°´å¹³ï¼›åŒæ—¶æ„å»ºè‡ª refinementï¼ˆè‡ªæˆ‘ä¼˜åŒ–ï¼‰è®­ç»ƒè¯­æ–™ä½œæ•°æ®å¢å¼ºï¼Œè®©æ¨¡å‹å­¦ä¹ è¿­ä»£ä¼˜åŒ–å·¥å…·è°ƒç”¨çš„èƒ½åŠ›ï¼Œæ— éœ€å¤–éƒ¨åé¦ˆä¹Ÿèƒ½æå‡æ€§èƒ½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªé€‚åº”è‡ªä¼˜åŒ–æ¨ç†ï¼ˆAdaptive Self-Refinement Inferenceï¼‰  
è®­ç»ƒåŸºç¡€ä¸Šï¼Œå°†è‡ªé€‚åº”è‡ªä¼˜åŒ–èå…¥è¿­ä»£æ¨ç†ã€‚æ¨¡å‹å¯è‡ªä¸»åˆ¤æ–­ä½•æ—¶åœæ­¢ä¼˜åŒ–è¿‡ç¨‹ï¼ŒåŠ¨æ€åŒ¹é…ä¸åŒå¤æ‚åº¦æŸ¥è¯¢çš„è®¡ç®—å¼€é”€ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªå·¥å…·è°ƒç”¨åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Berkeley Function Call Leaderboardã€API-Bankï¼‰éªŒè¯ToolACE-Rï¼š  
- ä¸GPT-4oç­‰å…ˆè¿›APIæ¨¡å‹ç›¸æ¯”ï¼ŒToolACE-Ræ€§èƒ½å…·å¤‡ç«äº‰åŠ›ï¼›  
- è‡ªé€‚åº”è‡ªä¼˜åŒ–èƒ½è¿›ä¸€æ­¥é«˜æ•ˆæå‡å·¥å…·è°ƒç”¨è¡¨ç°ï¼Œè¯æ˜æ¡†æ¶åœ¨æ•ˆç‡ä¸æ³›åŒ–æ€§ä¸Šçš„ä¼˜åŠ¿ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **è®­ç»ƒé˜¶æ®µçš„â€œæ¨¡å‹æ„ŸçŸ¥â€æ€ç»´**ï¼šä¸å†ç›²ç›®ç”¨åˆæˆæ•°æ®ï¼Œè€Œæ˜¯æ ¹æ®æ¨¡å‹èƒ½åŠ›åŠ¨æ€é€‰æ ·ã€è¿­ä»£è®­ç»ƒï¼Œä¸ºå¤§æ¨¡å‹æ•°æ®é«˜æ•ˆåˆ©ç”¨æä¾›æ–°æ€è·¯ã€‚  
2. **è‡ªä¼˜åŒ–çš„é—­ç¯è®¾è®¡**ï¼šè®­ç»ƒæ—¶è®©æ¨¡å‹å­¦â€œè‡ªæˆ‘ä¼˜åŒ–å·¥å…·è°ƒç”¨â€ï¼Œæ¨ç†æ—¶è‡ªé€‚åº”åœæ­¢ï¼Œæ—¢æŒ–æ˜æ¨¡å‹æ½œåŠ›ï¼Œåˆå¹³è¡¡è®¡ç®—èµ„æºï¼Œä¸ºå·¥å…·å­¦ä¹ çš„â€œè®­ç»ƒ-æ¨ç†â€å…¨æµç¨‹ä¼˜åŒ–æä¾›èŒƒå¼ã€‚  
3. **å¤šåŸºå‡†æµ‹è¯•çš„ä¸¥è°¨æ€§**ï¼šåœ¨ä¸»æµå·¥å…·è°ƒç”¨åŸºå‡†éªŒè¯ï¼Œç»“æœæ”¯æ’‘æ–¹æ³•æ™®é€‚æ€§ï¼Œä¸ºåç»­å·¥å…·å­¦ä¹ ç ”ç©¶çš„å®éªŒè®¾è®¡æä¾›å‚è€ƒã€‚  

ToolACE-Rè·³å‡ºâ€œåªå…³æ³¨æ•°æ®åˆæˆâ€çš„ä¼ ç»Ÿæ€è·¯ï¼Œä»æ¨¡å‹èƒ½åŠ›é€‚é…ã€è‡ªä¼˜åŒ–é—­ç¯ä¸¤ç»´åº¦é©æ–°å·¥å…·å­¦ä¹ èŒƒå¼ï¼Œä¸ºæ›´é«˜æ•ˆã€å¯æ‰©å±•çš„å·¥å…·å­¦ä¹ é“ºå°±æ–°è·¯å¾„ï½

## stabletoolbench-mirrorapi--modeling-tool-environments-as-mirrors-of-7-000+-real-world-apis
### Abstract
The rapid advancement of large language models (LLMs) has spurred significant interest in tool learning, where LLMs are augmented with external tools to tackle complex tasks. However, existing tool environments face challenges in balancing stability, scalability, and realness, particularly for benchmarking purposes. To address this problem, we propose MirrorAPI, a novel framework that trains specialized LLMs to accurately simulate real API responses, effectively acting as "mirrors" to tool environments. Using a comprehensive dataset of request-response pairs from 7,000+ APIs, we employ supervised fine-tuning and chain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves superior accuracy and stability compared to state-of-the-art methods, as demonstrated by its performance on the newly constructed MirrorAPI-Bench and its integration into StableToolBench.
### ğŸŒŸ è®ºæ–‡è§£è¯» | StableToolBench-MirrorAPIï¼šè®©å·¥å…·ç¯å¢ƒæˆä¸º7000+çœŸå®APIçš„â€œé•œåƒâ€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å·¥å…·å­¦ä¹ çš„ç ”ç©¶çƒ­æ½®ï¼Œå³è®©LLMså€ŸåŠ©å¤–éƒ¨å·¥å…·å¤„ç†å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œç°æœ‰å·¥å…·ç¯å¢ƒåœ¨ç¨³å®šæ€§ã€å¯æ‰©å±•æ€§å’ŒçœŸå®æ€§çš„å¹³è¡¡ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼šåŸºäºå¤§è§„æ¨¡å…¬å…±APIæ„å»ºçš„ç¯å¢ƒæ˜“å› å¼€å‘è€…æ›´æ–°ã€ç½‘ç»œæ³¢åŠ¨ç­‰ä¸ç¨³å®šï¼›ä¾èµ–æ‰‹åŠ¨é€‰æ‹©æˆ–åˆ›å»ºAPIçš„ç¯å¢ƒå—äººåŠ›é™åˆ¶ç¼ºä¹æ‰©å±•æ€§ï¼›LLMsæ¨¡æ‹Ÿçš„APIä¸çœŸå®APIå“åº”ä»æœ‰è¾ƒå¤§å·®è·ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºMirrorAPIæ¡†æ¶ï¼Œæ—¨åœ¨è®©ä¸“é—¨è®­ç»ƒçš„LLMsç²¾å‡†æ¨¡æ‹ŸçœŸå®APIå“åº”ï¼Œæˆä¸ºå·¥å…·ç¯å¢ƒçš„â€œé•œåƒâ€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºMirrorAPIæ¡†æ¶æ¨¡æ‹ŸçœŸå®APIå“åº”  
æå‡ºMirrorAPIè¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œè®­ç»ƒä¸“é—¨çš„LLMsæ¥ç²¾å‡†æ¨¡æ‹ŸçœŸå®APIçš„å“åº”ï¼Œä½¿å…¶æˆä¸ºå·¥å…·ç¯å¢ƒçš„â€œé•œåƒâ€ã€‚é€šè¿‡è®©æ¨¡å‹å­¦ä¹ çœŸå®APIçš„è¯·æ±‚ - å“åº”é€»è¾‘ï¼Œæ¥å¹³è¡¡å·¥å…·å­¦ä¹ ç¯å¢ƒçš„ç¨³å®šæ€§ã€å¯æ‰©å±•æ€§ä¸çœŸå®æ€§ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºå¤§è§„æ¨¡çœŸå®APIæ•°æ®è®­ç»ƒ  
æ”¶é›†æ¶µç›–49ä¸ªç±»åˆ«ã€7000 + APIçš„è¯·æ±‚ - å“åº”å¯¹æ•°æ®é›†ï¼ˆå«APIæ–‡æ¡£ï¼‰ç”¨äºæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ä¸ºæ•æ‰çœŸå®APIç³»ç»Ÿçš„éšå«å› ç´ ï¼Œè¿˜å¼•å…¥å¯¹APIæœºåˆ¶çš„æ¨ç†è§£é‡Šåˆ°è®­ç»ƒæ•°æ®ä¸­ï¼Œåˆ©ç”¨OpenAI o1 - previewé’ˆå¯¹è¯·æ±‚ - å“åº”å¯¹ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ rationaleæ¥è§£é‡ŠAPIå·¥ä½œæœºåˆ¶ï¼Œæ¨¡å‹åœ¨SFTå’ŒCoTæ¨¡å¼ä¸‹è®­ç»ƒï¼Œæ¨ç†æ—¶é»˜è®¤SFTæ¨¡å¼ä¿éšœæ€§èƒ½ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®æ”¶é›†ä¸å¤„ç†çš„ç²¾ç»†åŒ–æµç¨‹  
æ•°æ®æ”¶é›†åˆ†ä¸‰é˜¶æ®µï¼šæ”¶é›†çœŸå®è¯·æ±‚ - å“åº”å¯¹æ—¶ï¼Œå…ˆçˆ¬å–RapidAPIå·¥å…·å’Œæ–‡æ¡£ã€éªŒè¯APIå¯ç”¨æ€§ï¼Œå†ç”¨ä¸¤é˜¶æ®µåŸºäºåœºæ™¯çš„æ–¹æ³•è®©LLMsç”Ÿæˆå¤šæ ·ä¸”ç²¾å‡†çš„APIè¯·æ±‚ï¼›è¿‡æ»¤é˜¶æ®µå‰”é™¤å¤±è´¥è°ƒç”¨ï¼›è¿˜è¿›è¡Œåˆæˆç­‰æ“ä½œæ¥å®Œå–„æ•°æ®ï¼Œæå‡è®­ç»ƒæ•°æ®è´¨é‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ„å»ºMirrorAPI - Benchï¼ŒåŸºäºè®­ç»ƒæ—¶è§è¿‡å’Œæœªè§è¿‡çš„APIçš„çœŸå®è¯·æ±‚ - å“åº”å¯¹å®šä¹‰åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰é›†åˆæ¥è¯„ä¼°ã€‚å®éªŒè¡¨æ˜MirrorAPIåœ¨æ¨¡æ‹ŸçœŸå®APIä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨æ–‡æ¡£ç†è§£å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸Šè¶…è¶Šç°æœ‰LLMæç¤ºæ–¹æ³•ï¼Œä¸”ä¸çœŸå®å“åº”çš„ç›¸ä¼¼åº¦æœ€é«˜ã€‚å°†å…¶é›†æˆåˆ°StableToolBenchåï¼Œæ—¢ä¿æŒç¯å¢ƒå®Œå…¨ç¨³å®šï¼Œäº§å‡ºåˆèƒ½ä¸çœŸå®ç¯å¢ƒåª²ç¾ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æ•°æ®å±‚é¢ï¼Œå¤§è§„æ¨¡ä¸”å¤šç±»åˆ«çš„çœŸå®APIè¯·æ±‚ - å“åº”å¯¹æ”¶é›†ä¸ç²¾ç»†åŒ–å¤„ç†æµç¨‹ï¼Œä¸ºè®­ç»ƒæ¨¡æ‹Ÿç±»æ¨¡å‹æä¾›äº†é«˜è´¨é‡æ•°æ®è·å–æ€è·¯ï¼›æ¨¡å‹è®­ç»ƒå±‚é¢ï¼Œç»“åˆæœ‰ç›‘ç£å¾®è°ƒä¸æ€ç»´é“¾æ¨ç†æ¥æ•æ‰çœŸå®ç³»ç»Ÿéšå«å› ç´ ï¼Œæå‡æ¨¡æ‹Ÿä¿çœŸåº¦çš„æ–¹æ³•å€¼å¾—å€Ÿé‰´ï¼›åº”ç”¨å±‚é¢ï¼ŒMirrorAPIä½œä¸ºå·¥å…·ç¯å¢ƒé›†æˆåˆ°Benchmarkä¸­éªŒè¯æ•ˆæœï¼Œä»¥åŠå…¶åœ¨å¢å¼ºå·¥å…·ä½¿ç”¨æ¨¡å‹ï¼ˆå¦‚æä¾›é€æ­¥åé¦ˆã€æ‰©å±•è®­ç»ƒæ•°æ®ï¼‰æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå·¥å…·å­¦ä¹ é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å¢å¼ºç­‰æ–¹å‘æä¾›äº†æ–°èŒƒå¼ã€‚

## chain-of-tools--utilizing-massive-unseen-tools-in-the-cot-reasoning-of-frozen-language-models
### Abstract
Tool learning can further broaden the usage scenarios of large language models (LLMs). However most of the existing methods either need to finetune that the model can only use tools seen in the training data, or add tool demonstrations into the prompt with lower efficiency. In this paper, we present a new Tool Learning method Chain-of-Tools. It makes full use of the powerful semantic representation capability of frozen LLMs to finish tool calling in CoT reasoning with a huge and flexible tool pool which may contain unseen tools. Especially, to validate the effectiveness of our approach in the massive unseen tool scenario, we construct a new dataset SimpleToolQuestions. We conduct experiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two knowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions). Experimental results show that our approach performs better than the baseline. We also identify dimensions of the model output that are critical in tool selection, enhancing the model interpretability. Our code and data are available at: https://github.com/fairyshine/Chain-of-Tools .
### ğŸŒŸ è®ºæ–‡è§£è¯» | Chain-of-Toolsï¼šè®©å†»ç»“è¯­è¨€æ¨¡å‹åœ¨æ€ç»´é“¾æ¨ç†ä¸­ç”¨å¥½æµ·é‡æœªè§è¿‡çš„å·¥å…·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿå‘å±•è¿…é€Ÿï¼Œä½†LLMsåœ¨å®Œæˆè®¡ç®—æ•°å­¦å…¬å¼ã€è·å–å®æ—¶ä¿¡æ¯ç­‰ç‰¹å®šä»»åŠ¡æ—¶å­˜åœ¨ä¸è¶³ï¼Œå·¥å…·å­¦ä¹ ï¼ˆTool Learningï¼‰æˆä¸ºæ‹“å±•å…¶åº”ç”¨åœºæ™¯çš„å…³é”®ã€‚ç°æœ‰å·¥å…·å­¦ä¹ æ–¹æ³•å­˜åœ¨å±€é™ï¼šåŸºäºå¾®è°ƒçš„æ–¹æ³•è™½èƒ½ç²¾å‡†è°ƒç”¨è®­ç»ƒæ—¶è§è¿‡çš„å·¥å…·ï¼Œä½†å¯èƒ½å½±å“æ¨¡å‹çš„æ¶Œç°èƒ½åŠ›ç­‰ï¼›åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ–¹æ³•è™½èƒ½è°ƒç”¨æœªè§è¿‡çš„å·¥å…·ï¼Œä½†é¢å¯¹æµ·é‡å·¥å…·æ—¶æ¨ç†æ•ˆç‡ä½ã€‚ç°å®ä¸­å·¥å…·ä¸æ–­æ¶Œç°ï¼ŒLLMæ™ºèƒ½ä½“éœ€åœ¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸­é«˜æ•ˆç®¡ç†å’Œåˆ©ç”¨æµ·é‡æœªè§è¿‡çš„å·¥å…·ï¼Œè¿™å°±æ˜¯æœ¬æ–‡è¦è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºChain-of-Toolsï¼ˆCoToolsï¼‰æ–¹æ³•  
CoToolsæ˜¯å…¨æ–°çš„å·¥å…·å­¦ä¹ æ–¹æ³•ï¼Œéµå¾ªåŸºäºå¾®è°ƒæ€è·¯ä¿è¯å·¥å…·è°ƒç”¨æ•ˆç‡ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨å†»ç»“LLMsçš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ï¼ˆéšè—çŠ¶æ€ï¼‰æ¥å®Œæˆå·¥å…·è°ƒç”¨ã€‚åœ¨LLMç”Ÿæˆæ¯ä¸ªå›ç­”tokenæ—¶ï¼Œä¾æ®æ–°tokençš„éšè—çŠ¶æ€åˆ¤æ–­æ˜¯å¦è°ƒç”¨å·¥å…·ï¼›è‹¥éœ€è°ƒç”¨ï¼Œåˆ©ç”¨å¯¹åº”éšè—çŠ¶æ€è®¡ç®—æŸ¥è¯¢å‘é‡å’Œå·¥å…·å‘é‡æ¥é€‰å·¥å…·ï¼Œæœªè§è¿‡çš„å·¥å…·å¯é€šè¿‡å…¶æè¿°è®¡ç®—å‘é‡å®ç°çµæ´»æ£€ç´¢ï¼Œä¸”å†»ç»“LLMä¿è¯å…¶CoTæ¨ç†èƒ½åŠ›ä¸å—å½±å“ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºSimpleToolQuestionsæ•°æ®é›†  
ä¸ºéªŒè¯æ–¹æ³•åœ¨æµ·é‡æœªè§è¿‡å·¥å…·åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œæ„å»ºåŒ…å«1836ä¸ªå·¥å…·çš„SimpleToolQuestionsï¼ˆSTQuestionsï¼‰æ•°æ®é›†ï¼Œèšç„¦è¯„ä¼°æµ·é‡æœªè§è¿‡å·¥å…·åœºæ™¯ä¸‹çš„å·¥å…·é€‰æ‹©æ€§èƒ½ï¼Œå¡«è¡¥äº†æ­¤å‰åŸºå‡†æµ‹è¯•åœ¨è¯¥åœºæ™¯çš„ç©ºç™½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸¤ä¸ªæ•°å€¼æ¨ç†åŸºå‡†ï¼ˆGSM8K - XLã€FuncQAï¼‰å’Œä¸¤ä¸ªåŸºäºçŸ¥è¯†çš„é—®ç­”åŸºå‡†ï¼ˆKAMELã€SimpleToolQuestionsï¼‰ä¸Šå¼€å±•å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒCoToolsåœ¨æ•°å€¼æ¨ç†å’ŒåŸºäºçŸ¥è¯†çš„é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼›è¿˜å‘ç°äº†éšè—çŠ¶æ€ä¸­å¯¹å·¥å…·é€‰æ‹©èµ·å…³é”®ä½œç”¨çš„ç»´åº¦ï¼Œæå‡äº†æ¨¡å‹å¯è§£é‡Šæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•è®¾è®¡è§’åº¦ï¼šCoToolsåˆ©ç”¨å†»ç»“LLMçš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›å¤„ç†æœªè§è¿‡å·¥å…·çš„æ€è·¯ï¼Œä¸ºåœ¨ä¸ç ´åæ¨¡å‹åŸæœ‰èƒ½åŠ›å‰æä¸‹æ‹“å±•å·¥å…·ä½¿ç”¨åœºæ™¯æä¾›äº†æ–°èŒƒå¼ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å¯¹æ¨¡å‹éšè—çŠ¶æ€çš„é«˜æ•ˆåˆ©ç”¨æ–¹å¼ã€‚  
2. æ•°æ®é›†æ„å»ºè§’åº¦ï¼šé’ˆå¯¹ç‰¹å®šåœºæ™¯ï¼ˆæµ·é‡æœªè§è¿‡å·¥å…·ï¼‰æ„å»ºä¸“é—¨æ•°æ®é›†STQuestionsï¼Œä¸ºè¯„ä¼°å·¥å…·å­¦ä¹ æ–¹æ³•åœ¨è¯¥åœºæ™¯ä¸‹çš„æ€§èƒ½æä¾›äº†æœ‰æ•ˆåŸºå‡†ï¼Œå¯ç¤ºç ”ç©¶è€…å¯é’ˆå¯¹æ–¹æ³•ç›®æ ‡åœºæ™¯æ„å»ºé’ˆå¯¹æ€§è¯„ä¼°æ•°æ®é›†ã€‚  
3. å¯è§£é‡Šæ€§æ¢ç´¢è§’åº¦ï¼šæŒ–æ˜éšè—çŠ¶æ€ä¸­å·¥å…·é€‰æ‹©å…³é”®ç»´åº¦ï¼Œä¸ºç†è§£æ¨¡å‹å·¥å…·é€‰æ‹©å†³ç­–è¿‡ç¨‹æä¾›äº†æ€è·¯ï¼Œåç»­å¯æ·±å…¥æ¢ç´¢æ¨¡å‹å†…éƒ¨å†³ç­–æœºåˆ¶æå‡å¯è§£é‡Šæ€§ã€‚

## advanced-tool-learning-and-selection-system-(atlass)--a-closed-loop-framework-using-llm
### Abstract
The combination of LLM agents with external tools enables models to solve complex tasks beyond their knowledge base. Human-designed tools are inflexible and restricted to solutions within the scope of pre-existing tools created by experts. To address this problem, we propose ATLASS, an advanced tool learning and selection system designed as a closed-loop framework. It enables the LLM to solve problems by dynamically generating external tools on demand. In this framework, agents play a crucial role in orchestrating tool selection, execution, and refinement, ensuring adaptive problem-solving capabilities. The operation of ATLASS follows three phases: The first phase, Understanding Tool Requirements, involves the Agents determining whether tools are required and specifying their functionality; the second phase, Tool Retrieval/Generation, involves the Agents retrieving or generating tools based on their availability; and the third phase, Task Solving, involves combining all the component tools necessary to complete the initial task. The Tool Dataset stores the generated tools, ensuring reusability and minimizing inference cost. Current LLM-based tool generation systems have difficulty creating complex tools that need APIs or external packages. In ATLASS, we solve the problem by automatically setting up the environment, fetching relevant API documentation online, and using a Python interpreter to create a reliable, versatile tool that works in a wider range of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and ethical concerns are handled through human feedback before executing generated code. By addressing the limitations of predefined toolsets and enhancing adaptability, ATLASS serves as a real-world solution that empowers users with dynamically generated tools for complex problem-solving.
### ğŸŒŸ è®ºæ–‡è§£è¯» | çªç ´å·¥å…·é™åˆ¶ï¼šATLASSè®©LLMåŠ¨æ€ç”Ÿæˆå·¥å…·è§£å†³å¤æ‚ä»»åŠ¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å›ºæœ‰å±€é™ï¼Œå¦‚ä¿¡æ¯è¿‡æ—¶ã€å¤„ç†å¤æ‚ä»»åŠ¡èƒ½åŠ›ä¸è¶³ç­‰ã€‚LLMæ™ºèƒ½ä½“ç»“åˆå¤–éƒ¨å·¥å…·èƒ½çªç ´çŸ¥è¯†åº“é™åˆ¶è§£å†³å¤æ‚ä»»åŠ¡ï¼Œç„¶è€Œäººå·¥è®¾è®¡çš„å·¥å…·ç¼ºä¹çµæ´»æ€§ï¼Œå—é™äºä¸“å®¶é¢„å…ˆè®¾å®šçš„å·¥å…·èŒƒå›´ã€‚åŒæ—¶ï¼Œå½“å‰åŸºäºLLMçš„å·¥å…·ç”Ÿæˆç³»ç»Ÿéš¾ä»¥åˆ›å»ºéœ€APIæˆ–å¤–éƒ¨åŒ…çš„å¤æ‚å·¥å…·ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Advanced Tool Learning and Selection Systemï¼ˆATLASSï¼‰æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé—­ç¯æ¡†æ¶å®ç°å·¥å…·åŠ¨æ€ç”Ÿæˆä¸é€‰æ‹©  
ATLASSæ˜¯ä¸€ä¸ªé—­ç¯æ¡†æ¶ï¼Œä½¿LLMèƒ½æŒ‰éœ€åŠ¨æ€ç”Ÿæˆå¤–éƒ¨å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚æ¡†æ¶ä¸­æ™ºèƒ½ä½“åœ¨å·¥å…·é€‰æ‹©ã€æ‰§è¡Œå’Œä¼˜åŒ–çš„åè°ƒä¸­èµ·å…³é”®ä½œç”¨ï¼Œä¿éšœè‡ªé€‚åº”è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚å…¶è¿ä½œåˆ†ä¸‰é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µâ€œç†è§£å·¥å…·éœ€æ±‚â€ï¼Œæ™ºèƒ½ä½“ç¡®å®šæ˜¯å¦éœ€è¦å·¥å…·å¹¶æ˜ç¡®åŠŸèƒ½ï¼›ç¬¬äºŒé˜¶æ®µâ€œå·¥å…·æ£€ç´¢/ç”Ÿæˆâ€ï¼Œæ™ºèƒ½ä½“åŸºäºå·¥å…·å¯ç”¨æ€§æ£€ç´¢æˆ–ç”Ÿæˆå·¥å…·ï¼›ç¬¬ä¸‰é˜¶æ®µâ€œä»»åŠ¡è§£å†³â€ï¼Œç»„åˆå®Œæˆåˆå§‹ä»»åŠ¡æ‰€éœ€çš„æ‰€æœ‰å·¥å…·ç»„ä»¶ã€‚å·¥å…·æ•°æ®é›†å­˜å‚¨ç”Ÿæˆçš„å·¥å…·ï¼Œç¡®ä¿å¯å¤ç”¨å¹¶æœ€å°åŒ–æ¨ç†æˆæœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£å†³å¤æ‚å·¥å…·åˆ›å»ºéš¾é¢˜  
é’ˆå¯¹å½“å‰LLMå·¥å…·ç”Ÿæˆç³»ç»Ÿéš¾ä»¥åˆ›å»ºéœ€APIæˆ–å¤–éƒ¨åŒ…çš„å¤æ‚å·¥å…·è¿™ä¸€é—®é¢˜ï¼ŒATLASSé€šè¿‡è‡ªåŠ¨è®¾ç½®ç¯å¢ƒã€åœ¨çº¿è·å–ç›¸å…³APIæ–‡æ¡£ä»¥åŠä½¿ç”¨Pythonè§£é‡Šå™¨ï¼Œåˆ›å»ºåœ¨æ›´å¹¿æ³›åœºæ™¯ä¸‹å·¥ä½œçš„å¯é ã€é€šç”¨å·¥å…·ã€‚å¹¶ä¸”åœ¨æ‰§è¡Œç”Ÿæˆçš„ä»£ç å‰ï¼Œé€šè¿‡äººå·¥åé¦ˆå¤„ç†å®‰å…¨å’Œä¼¦ç†é—®é¢˜ï¼Œä½¿ç”¨OpenAI GPT - 4.0ä½œä¸ºLLMæ™ºèƒ½ä½“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡å·¥å…·å¤ç”¨æ€§å‡å°‘å†—ä½™  
ä¸å¦‚Large Language Models as Tool Makersï¼ˆLATMï¼‰ç­‰è¾ƒç®€å•æ–¹æ³•ä¸åŒï¼ŒATLASSé€šè¿‡åˆ†æç”¨æˆ·æŸ¥è¯¢ã€åˆ†è§£ä»»åŠ¡ã€ç†è§£å·¥å…·éœ€æ±‚ï¼Œè¯†åˆ«å•ä¸ªå·¥å…·å¯é«˜æ•ˆå¤„ç†ç›¸ä¼¼æŸ¥è¯¢ï¼Œåˆ›å»ºå¯å¤ç”¨å·¥å…·ï¼Œåº”ç”¨äºæœªæ¥ä»»åŠ¡ä»¥å‡å°‘å†—ä½™ï¼Œå…‹æœäº†åªå…³æ³¨ç‰¹å®šä»»åŠ¡å·¥å…·ä¸”å¿½è§†å¤ç”¨çš„å±€é™ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡æœªæ˜ç¡®æåŠä¼ ç»Ÿå®éªŒéƒ¨åˆ†çš„å¯¹æ¯”ç»“æœå±•ç¤ºï¼ˆå¦‚å’Œå…¶ä»–å·¥å…·ç”Ÿæˆã€é€‰æ‹©ç³»ç»Ÿåœ¨æŒ‡æ ‡ä¸Šçš„å¯¹æ¯”ç­‰ï¼‰ï¼Œä¸»è¦ä¾§é‡äºæ¡†æ¶è®¾è®¡ä¸åˆ›æ–°ç‚¹é˜è¿°ï¼Œè¯´æ˜å…¶åœ¨å·¥å…·åŠ¨æ€ç”Ÿæˆã€å¤æ‚å·¥å…·åˆ›å»ºã€å·¥å…·å¤ç”¨ç­‰æ–¹é¢çš„è®¾è®¡ä¼˜åŠ¿ä¸è§£å†³é—®é¢˜çš„æ€è·¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼ŒATLASSçš„é—­ç¯ä¸‰é˜¶æ®µæ¡†æ¶ä¸ºLLMç»“åˆå¤–éƒ¨å·¥å…·è§£å†³å¤æ‚ä»»åŠ¡æä¾›äº†æ¸…æ™°çš„æµç¨‹èŒƒå¼ï¼Œåœ¨å·¥å…·éœ€æ±‚ç†è§£ã€ç”Ÿæˆ/æ£€ç´¢ã€ä»»åŠ¡è§£å†³ç¯èŠ‚çš„åˆ†å·¥åä½œæ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œå¯ç”¨äºæŒ‡å¯¼æ„å»ºæ›´æ™ºèƒ½çš„å·¥å…·é©±åŠ¨å‹LLMåº”ç”¨ç³»ç»Ÿï¼›åœ¨å¤æ‚å·¥å…·åˆ›å»ºä¸Šï¼Œè‡ªåŠ¨é…ç½®ç¯å¢ƒã€åˆ©ç”¨åœ¨çº¿APIæ–‡æ¡£å’ŒPythonè§£é‡Šå™¨çš„æ–¹å¼ï¼Œä¸ºè§£å†³éœ€å¤–éƒ¨ä¾èµ–çš„å·¥å…·ç”Ÿæˆéš¾é¢˜æä¾›äº†å®è·µè·¯å¾„ï¼›åœ¨å·¥å…·å¤ç”¨æ€§æå‡æ–¹é¢ï¼Œå…³æ³¨ç›¸ä¼¼ä»»åŠ¡å·¥å…·å¤ç”¨ã€å‡å°‘å†—ä½™çš„ç†å¿µï¼Œå¯¹äºæ„å»ºé«˜æ•ˆå·¥å…·åº“å’Œé™ä½æ¨ç†æˆæœ¬å…·æœ‰å‚è€ƒä»·å€¼ï¼Œèƒ½å¯å‘åç»­åœ¨å·¥å…·ç®¡ç†ä¸ä¼˜åŒ–æ–¹å‘çš„ç ”ç©¶ã€‚

## alignment-for-efficient-tool-calling-of-large-language-models
### Abstract
Recent advancements in tool learning have enabled large language models (LLMs) to integrate external tools, enhancing their task performance by expanding their knowledge boundaries. However, relying on tools often introduces tradeoffs between performance, speed, and cost, with LLMs sometimes exhibiting overreliance and overconfidence in tool usage. This paper addresses the challenge of aligning LLMs with their knowledge boundaries to make more intelligent decisions about tool invocation. We propose a multi objective alignment framework that combines probabilistic knowledge boundary estimation with dynamic decision making, allowing LLMs to better assess when to invoke tools based on their confidence. Our framework includes two methods for knowledge boundary estimation, consistency based and absolute estimation, and two training strategies for integrating these estimates into the model decision making process. Experimental results on various tool invocation scenarios demonstrate the effectiveness of our framework, showing significant improvements in tool efficiency by reducing unnecessary tool usage.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå·¥å…·è°ƒç”¨çš„å¯¹é½æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥å…·å­¦ä¹ çš„å‘å±•è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½æ•´åˆå¤–éƒ¨å·¥å…·æ‹“å±•çŸ¥è¯†è¾¹ç•Œï¼Œä½†å·¥å…·ä¾èµ–åœ¨æ€§èƒ½ã€é€Ÿåº¦å’Œæˆæœ¬é—´å­˜åœ¨æƒè¡¡ï¼ŒLLMså¸¸å‡ºç°å·¥å…·è¿‡åº¦ä¾èµ–ä¸è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚æ¯”å¦‚åœ¨é—®ç­”åœºæ™¯ç”¨æœç´¢å·¥å…·éœ€å¤šæ­¥éª¤ã€è€—æ—¶ä¸”æœ‰è°ƒç”¨æˆæœ¬ï¼Œè€Œç›´æ¥å›ç­”æ›´ç®€ä¾¿ï¼›ç°æœ‰LLMsè¦ä¹ˆç®€å•ä»»åŠ¡è¿‡åº¦è°ƒç”¨å·¥å…·ï¼Œè¦ä¹ˆå¿…è¦æ—¶ä¸ç”¨å·¥å…·ï¼Œå½±å“å·¥å…·æ™ºèƒ½ä¸å¢åŠ ä»»åŠ¡æˆæœ¬ã€‚å› æ­¤ï¼Œéœ€è®©LLMsè¡Œä¸ºä¸å…¶çŸ¥è¯†è¾¹ç•Œå¯¹é½ï¼ŒåŸºäºç½®ä¿¡åº¦æ™ºèƒ½å†³ç­–æ˜¯å¦è°ƒç”¨å·¥å…·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå¤šç›®æ ‡å¯¹é½æ¡†æ¶  
ç»“åˆæ¦‚ç‡æ€§çŸ¥è¯†è¾¹ç•Œä¼°è®¡ä¸åŠ¨æ€å†³ç­–ï¼Œè®©LLMsåŸºäºç½®ä¿¡åº¦æ›´å¥½è¯„ä¼°ä½•æ—¶è°ƒç”¨å·¥å…·ã€‚æ¡†æ¶åŒ…å«çŸ¥è¯†è¾¹ç•Œä¼°è®¡ä¸çŸ¥è¯†è¾¹ç•Œå»ºæ¨¡ä¸¤éƒ¨åˆ†ï¼Œå¹³è¡¡ä»»åŠ¡æˆåŠŸä¸å·¥å…·ä½¿ç”¨æˆæœ¬ï¼Œæå‡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçŸ¥è¯†è¾¹ç•Œä¼°è®¡çš„ä¸¤ç§æ–¹æ³•  
ä¸€æ˜¯åŸºäºä¸€è‡´æ€§çš„ä¼°è®¡ï¼Œä¾æ®æ¨¡å‹å¤šæ¬¡é‡‡æ ·çš„ä¸€è‡´æ€§ç¨‹åº¦è¯„ä¼°çŸ¥è¯†ï¼›äºŒæ˜¯ç»å¯¹ä¼°è®¡ï¼Œåˆ©ç”¨å¤–éƒ¨çœŸå®æ ‡ç­¾è¯„ä¼°æ¨¡å‹å¤šæ¬¡é‡‡æ ·çš„å¹³å‡å‡†ç¡®ç‡æ¥è¯„ä¼°çŸ¥è¯†ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šçŸ¥è¯†è¾¹ç•Œå»ºæ¨¡çš„ä¸¤ç±»ç­–ç•¥  
æ„å»ºéšå¼å»ºæ¨¡ï¼ˆæ¨¡å‹ä¾çŸ¥è¯†ç¡®å®šæ€§é¢„å®šä¹‰é˜ˆå€¼å†³ç­–ï¼‰ä¸æ˜¾å¼å»ºæ¨¡ï¼ˆæ¨¡å‹è¾“å‡ºç­”æ¡ˆåŒæ—¶è¾“å‡ºç½®ä¿¡åº¦åˆ†æ•°ï¼‰çš„æ•°æ®ï¼Œå°†çŸ¥è¯†è¾¹ç•Œä¼°è®¡æ•´åˆåˆ°æ¨¡å‹å†³ç­–è¿‡ç¨‹ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šç§å·¥å…·è°ƒç”¨åœºæ™¯å®éªŒï¼Œè¯æ˜æ¡†æ¶èƒ½æœ‰æ•ˆå‡å°‘ä¸å¿…è¦å·¥å…·è°ƒç”¨ï¼Œæå‡å·¥å…·æ•´ä½“æ•ˆç‡ï¼Œå¦‚é™ä½æ¨¡å‹å¯¹å·¥å…·çš„è¿‡åº¦ä¾èµ–ä¸è¿‡åº¦è‡ªä¿¡æƒ…å†µï¼ˆä»å›¾1å¯çœ‹å‡ºæ–¹æ³•åœ¨å‡å°‘å·¥å…·è¿‡åº¦ä¾èµ–å’Œè¿‡åº¦è‡ªä¿¡ä¸Šçš„æ•ˆæœï¼‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡çœ‹ï¼Œæå‡ºçš„å¤šç›®æ ‡å¯¹é½æ¡†æ¶ä¸ºLLMså·¥å…·è°ƒç”¨å†³ç­–æä¾›æ–°èŒƒå¼ï¼Œå°†çŸ¥è¯†è¾¹ç•Œä»ç®€å•äºŒå…ƒåˆ’åˆ†æ‹“å±•åˆ°æ¦‚ç‡æ€§ã€ç°åº¦åŒ–å¤„ç†ï¼Œå¯å‘åç»­å¯¹æ¨¡å‹çŸ¥è¯†è¾¹ç•Œæ›´ç²¾ç»†çš„ç ”ç©¶ï¼›ä»æŠ€æœ¯æ¨¡å—çœ‹ï¼ŒçŸ¥è¯†è¾¹ç•Œä¼°è®¡çš„ä¸¤ç§æ–¹æ³•ä¸å»ºæ¨¡çš„ä¸¤ç±»ç­–ç•¥ï¼Œä¸ºå¦‚ä½•é‡åŒ–æ¨¡å‹çŸ¥è¯†ã€å¦‚ä½•åŸºäºçŸ¥è¯†çŠ¶æ€å†³ç­–æä¾›äº†å…·ä½“æŠ€æœ¯è·¯çº¿ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€å¹³è¡¡èµ„æºæ¶ˆè€—ä¸ä»»åŠ¡è¡¨ç°çš„AIä»»åŠ¡åœºæ™¯ï¼›ä»è¯„ä¼°è§’åº¦ï¼Œè¿˜æå‡ºå¯¹åº”è¯„ä¼°æŒ‡æ ‡ï¼Œå®Œå–„äº†é«˜æ•ˆå·¥å…·è°ƒç”¨çš„è¯„ä¼°ä½“ç³»ï¼Œä¸ºè¯¥é¢†åŸŸåç»­ç ”ç©¶æä¾›è¯„ä¼°å‚è€ƒã€‚ 

## retrieval-models-aren-t-tool-savvy--benchmarking-tool-retrieval-for-large-language-models
### Abstract
Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å·¥å…·æ£€ç´¢èƒ½åŠ›å¦‚ä½•ï¼Ÿå¤§è¯­è¨€æ¨¡å‹å·¥å…·æ£€ç´¢çš„ benchmark ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†å­˜åœ¨ä¸ç‰©ç†ä¸–ç•Œäº¤äº’ã€è·å–å®æ—¶çŸ¥è¯†ç­‰æ–¹é¢çš„ä¸è¶³ã€‚å·¥å…·å­¦ä¹ æ—¨åœ¨ä¸º LLMs é…å¤‡å¤–éƒ¨å·¥å…·ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè€Œä»å¤§è§„æ¨¡å·¥å…·é›†ä¸­æ£€ç´¢æœ‰ç”¨å·¥å…·æ˜¯å·¥å…·ä½¿ç”¨æµç¨‹çš„å…³é”®åˆå§‹æ­¥éª¤ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥å…·ä½¿ç”¨åŸºå‡†å¤§å¤šé€šè¿‡æ‰‹åŠ¨é¢„æ ‡æ³¨å°‘é‡ç›¸å…³å·¥å…·æ¥ç®€åŒ–æ£€ç´¢æ­¥éª¤ï¼Œä¸çœŸå®åœºæ™¯ç›¸å·®ç”šè¿œï¼›ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ¨¡å‹åœ¨å·¥å…·æ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¹Ÿå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ­¤å¤–ï¼Œå·²æœ‰ç ”ç©¶è¡¨æ˜ï¼Œå½“ç”¨æ£€ç´¢åˆ°çš„å·¥å…·æ›¿æ¢æ‰‹åŠ¨æ ‡æ³¨å·¥å…·é›†æ—¶ï¼Œæ™ºèƒ½ä½“æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œä¸”å¼ºæ£€ç´¢æ¨¡å‹åœ¨å·¥å…·æ£€ç´¢ä¸­è¡¨ç°ä¹Ÿä¸ä½³ï¼Œå› æ­¤äºŸéœ€ç³»ç»Ÿè¯„ä¼° IR æ¨¡å‹åœ¨å·¥å…·æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶åˆ†ææ£€ç´¢å¯¹ç«¯åˆ°ç«¯ä»»åŠ¡é€šè¿‡ç‡çš„å½±å“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º ToolRet åŸºå‡†  
æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡å·¥å…·æ£€ç´¢åŸºå‡† ToolRetï¼ŒåŒ…å« 7.6k ä¸ªå¤šæ ·çš„æ£€ç´¢ä»»åŠ¡å’Œ 43k ä¸ªå·¥å…·çš„è¯­æ–™åº“ã€‚æ•°æ®ä» AI ä¼šè®®è®ºæ–‡ä¸­çš„å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“åŸºå‡†ã€ç›¸å…³ä¼šè®®èµ„æºä»¥åŠå¼€æºç¤¾åŒºå…¬å¼€æ•°æ®é›†ç­‰å¤šæ¥æºæ”¶é›†ï¼Œæ¶µç›–å¹¿æ³›å®ç”¨å·¥å…·éœ€æ±‚ï¼›è¿˜å¯¹ä»»åŠ¡æ ¼å¼æ ‡å‡†åŒ–ï¼Œå¹¶é‡‡ç”¨ç›®æ ‡æ„ŸçŸ¥ç­–ç•¥ä¸ºæ¯ä¸ªæŸ¥è¯¢è¡¥å……æŒ‡ä»¤ï¼Œä»¥æ”¯æŒåŸºå‡†çš„æŒ‡ä»¤æ£€ç´¢è®¾ç½®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°å¤šç§ IR æ¨¡å‹å¹¶åˆ†æå½±å“  
ç³»ç»Ÿè¯„ä¼°äº†åµŒå…¥æ¨¡å‹ã€LLM é‡æ’åºç­‰äº”ç±» IR æ¨¡å‹åœ¨ ToolRet ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºå‡ºå³ä¾¿åœ¨ä¼ ç»Ÿ IR åŸºå‡†ä¸­è¡¨ç°å¼ºåŠ²çš„æ¨¡å‹ï¼Œåœ¨å·¥å…·æ£€ç´¢ä»»åŠ¡ä¸­æ€§èƒ½ä¹Ÿè¾ƒå·®ï¼Œå¹¶åˆ†æå‡ºæŸ¥è¯¢ä¸ç›®æ ‡å·¥å…·æœ¯è¯­é‡å åº¦ä½ã€ä»»åŠ¡ä»ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢å‘å·¥å…·æ£€ç´¢è½¬ç§»è¿™ä¸¤ä¸ªå¯¼è‡´æ€§èƒ½å·®è·çš„å…³é”®å› ç´ ï¼›åŒæ—¶æ¢ç©¶äº†å·¥å…·æ£€ç´¢æ€§èƒ½å¯¹å·¥å…·ä½¿ç”¨ LLMs ç«¯åˆ°ç«¯ä»»åŠ¡é€šè¿‡ç‡çš„å½±å“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºå¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›† ToolRet - train  
ä¸ºæå‡ IR æ¨¡å‹å·¥å…·æ£€ç´¢èƒ½åŠ›ï¼Œæ„å»ºäº†åŒ…å«è¶… 20 ä¸‡å®ä¾‹çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›† ToolRet - trainã€‚æ‰©å±•æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼Œçº³å…¥ä¸»æµå·¥å…·ä½¿ç”¨æ•°æ®é›†çš„è®­ç»ƒé›†ï¼Œå¹¶ä¸ºæ¯ä¸ªæ£€ç´¢ä»»åŠ¡é…å¯¹ç”± NV - embed - v1 æ£€ç´¢çš„ 10 ä¸ªè´Ÿå·¥å…·ï¼Œä½¿æ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«æŸ¥è¯¢ã€ç”Ÿæˆçš„æŒ‡ä»¤ã€ç›®æ ‡å·¥å…·å’Œè´Ÿå·¥å…·ï¼Œç”¨äºä¼˜åŒ– IR æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ ToolRet åŸºå‡†ä¸Šè¯„ä¼°å¤šç§ IR æ¨¡å‹å‘ç°ï¼Œå³ä¾¿å¦‚åœ¨ä¼ ç»Ÿ IR åŸºå‡†è¡¨ç°å¥½çš„ NV - embedd - v1 æ¨¡å‹ï¼Œåœ¨è¯¥åŸºå‡†çš„ nDCG@10 ä¹Ÿä»…ä¸º 33.83ï¼Œå‡¸æ˜¾å·¥å…·æ£€ç´¢ä»»åŠ¡çš„æŒ‘æˆ˜æ€§ï¼›è€Œä½¿ç”¨ ToolRet - train è®­ç»ƒåçš„ IR æ¨¡å‹ï¼Œåœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¡¨ç°æ˜¾è‘—æå‡ï¼Œä¸å·¥å…·ä½¿ç”¨ LLMs é›†æˆæ—¶èƒ½å¸¦æ¥æ›´é«˜çš„ç«¯åˆ°ç«¯ä»»åŠ¡é€šè¿‡ç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŸºå‡†æ„å»ºè§’åº¦ï¼šåˆ›å»ºäº†é¦–ä¸ªé’ˆå¯¹å·¥å…·æ£€ç´¢ä»»åŠ¡çš„å¤§è§„æ¨¡è¯„ä¼°åŸºå‡† ToolRetï¼Œä¸ºåç»­å·¥å…·æ£€ç´¢é¢†åŸŸçš„ç ”ç©¶æä¾›äº†ç»Ÿä¸€ã€å…¨é¢çš„è¯„ä¼°å¹³å°ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸå¯¹å·¥å…·æ£€ç´¢ä»»åŠ¡çš„é‡è§†ä¸æ¢ç´¢ã€‚
2. æ¨¡å‹è¯„ä¼°ä¸åˆ†æè§’åº¦ï¼šç³»ç»Ÿè¯„ä¼°å¤šç§ IR æ¨¡å‹å¹¶æ·±å…¥åˆ†ææ€§èƒ½å·®è·åŸå› ï¼Œè®©ç ”ç©¶è€…æ¸…æ™°è®¤è¯†å·¥å…·æ£€ç´¢ä»»åŠ¡ç‰¹æ€§ä¸ç°æœ‰ IR æ¨¡å‹ä¸è¶³ï¼Œä¸ºåç»­æ”¹è¿›æ–¹å‘æä¾›å‚è€ƒã€‚
3. æ•°æ®å¢å¼ºè§’åº¦ï¼šæ„å»ºå¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›† ToolRet - train æ¥ä¼˜åŒ– IR æ¨¡å‹å·¥å…·æ£€ç´¢èƒ½åŠ›ï¼Œè¯æ˜äº†æ•°æ®é©±åŠ¨ä¼˜åŒ–å·¥å…·æ£€ç´¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡å·¥å…·ä½¿ç”¨ LLMs æ•´ä½“æ€§èƒ½æä¾›äº†æ•°æ®å±‚é¢çš„æ€è·¯ï¼Œä¹Ÿä¸ºåç»­æ„å»ºæ›´ä¼˜å·¥å…·æ£€ç´¢æ¨¡å‹æä¾›äº†æ•°æ®èµ„æºæ”¯æ’‘ã€‚

## asktoact--enhancing-llms-tool-use-via-self-correcting-clarification
### Abstract
Large language models (LLMs) have demonstrated remarkable capabilities in tool learning. In real-world scenarios, user queries are often ambiguous and incomplete, requiring effective clarification. However, existing interactive clarification approaches face two critical limitations: reliance on manually constructed datasets, which inherently constrains training data scale and diversity, and lack of error correction mechanisms during multi-turn clarification, leading to error accumulation that compromises both accuracy and efficiency. We present AskToAct, which addresses these challenges by exploiting the structural mapping between queries and their tool invocation solutions. Our key insight is that tool parameters naturally represent explicit user intents. By systematically removing key parameters from queries while retaining them as ground truth, we enable automated construction of high-quality training data. We further enhance model robustness through error-correction pairs and selective masking, enabling dynamic error detection during clarification interactions. Comprehensive experiments demonstrate that AskToAct significantly outperforms existing approaches, achieving above 57% accuracy in recovering critical unspecified intents and enhancing clarification efficiency by an average of 10.46% while maintaining high accuracy in tool invocation. Our framework exhibits robust performance across different model architectures and successfully generalizes to entirely unseen APIs without additional training, achieving performance comparable to GPT-4o with substantially fewer computational resources.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AskToActï¼šè®©å¤§æ¨¡å‹å·¥å…·è°ƒç”¨æ›´æ™ºèƒ½çš„è‡ªçº é”™æ¾„æ¸…æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å·¥å…·å­¦ä¹ æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œç„¶è€Œç°å®åœºæ™¯ä¸­ç”¨æˆ·æŸ¥è¯¢å¾€å¾€æ¨¡ç³Šã€ä¸å®Œæ•´ï¼Œéœ€è¦æœ‰æ•ˆæ¾„æ¸…ã€‚ç°æœ‰äº¤äº’å¼æ¾„æ¸…æ–¹æ³•å­˜åœ¨ä¸¤å¤§å…³é”®å±€é™ï¼šä¸€æ˜¯ä¾èµ–äººå·¥æ„å»ºæ•°æ®é›†ï¼Œé™åˆ¶äº†è®­ç»ƒæ•°æ®çš„è§„æ¨¡ä¸å¤šæ ·æ€§ï¼›äºŒæ˜¯å¤šè½®æ¾„æ¸…ä¸­ç¼ºä¹çº é”™æœºåˆ¶ï¼Œæ˜“å¯¼è‡´é”™è¯¯ç´¯ç§¯ï¼Œå½±å“å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†AskToActæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æ•°æ®é›†æ„å»º  
åˆ©ç”¨æŸ¥è¯¢ä¸å·¥å…·è°ƒç”¨è§£å†³æ–¹æ¡ˆé—´çš„ç»“æ„æ˜ å°„ï¼Œä»¥å·¥å…·å‚æ•°å¤©ç„¶ä»£è¡¨æ˜ç¡®ç”¨æˆ·æ„å›¾ä¸ºå…³é”®æ´å¯Ÿï¼Œä»å·²æœ‰å®Œæ•´æŸ¥è¯¢ä¸­ç³»ç»Ÿæ€§ç§»é™¤å…³é”®å‚æ•°å¹¶ä¿ç•™ä¸ºçœŸå®æ ‡ç­¾ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šæ ·ä¸”å¸¦æ ‡æ³¨çš„æœªæ˜ç¡®æŸ¥è¯¢ï¼Œæ„å»ºä¸°å¯Œæ¾„æ¸…å¯¹è¯æ•°æ®ï¼Œè§£å†³äººå·¥æ ‡æ³¨è§„æ¨¡å’Œå¤šæ ·æ€§å—é™é—®é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªçº é”™è®­ç»ƒèŒƒå¼å®ç°åŠ¨æ€é”™è¯¯å¤„ç†  
é€šè¿‡è®¾è®¡é”™è¯¯ - çº æ­£å¯¹æ¨¡æ‹ŸçœŸå®é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨è®­ç»ƒä¸­é‡‡ç”¨é€‰æ‹©æ€§æ©ç ï¼Œå¢å¼ºæ¨¡å‹é²æ£’æ€§ï¼Œè®©æ¨¡å‹åœ¨æ¾„æ¸…äº¤äº’ä¸­èƒ½åŠ¨æ€æ£€æµ‹å’Œçº æ­£é”™è¯¯ï¼Œé¿å…å¤šè½®å¯¹è¯ä¸­é”™è¯¯ç´¯ç§¯ï¼Œæå‡æ¾„æ¸…æ•ˆç‡ä¸å·¥å…·è°ƒç”¨è´¨é‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜AskToActè¡¨ç°å“è¶Šï¼šåœ¨æ¢å¤å…³é”®æœªæ˜ç¡®æ„å›¾ä¸Šå‡†ç¡®ç‡è¶…57%ï¼Œæ¾„æ¸…æ•ˆç‡è¾ƒåŸºç¡€æ¨¡å‹å¹³å‡æå‡10.46%ï¼›ç«¯åˆ°ç«¯å·¥å…·è°ƒç”¨ä¸­ï¼Œå·¥å…·é€‰æ‹©å‡†ç¡®ç‡è¶…81%ã€å‚æ•°è§£æå‡†ç¡®ç‡è¶…68%ï¼›åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸Šè¡¨ç°ç¨³å¥ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå°±èƒ½æ³›åŒ–åˆ°å…¨æ–°APIï¼Œä¸”ç”¨æ›´å°‘è®¡ç®—èµ„æºè¾¾åˆ°åª²ç¾GPT - 4oçš„æ€§èƒ½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®æ„å»ºå±‚é¢ï¼šæä¾›äº†è‡ªåŠ¨åŒ–ç”Ÿæˆé«˜è´¨é‡æ„å›¾æ¾„æ¸…æ•°æ®é›†çš„æ€è·¯ï¼Œä¸ºè§£å†³äººå·¥æ ‡æ³¨ç“¶é¢ˆæä¾›èŒƒä¾‹ï¼Œå¯å¯å‘åç»­åœ¨æ•°æ®é©±åŠ¨ä»»åŠ¡ä¸­æ¢ç´¢è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆæ–¹å¼ã€‚  
2. æ¨¡å‹è®­ç»ƒå±‚é¢ï¼šè‡ªçº é”™æœºåˆ¶ä¸é€‰æ‹©æ€§æ©ç çš„è®¾è®¡ï¼Œä¸ºæå‡æ¨¡å‹åœ¨äº¤äº’ä»»åŠ¡ä¸­é”™è¯¯å¤„ç†èƒ½åŠ›æä¾›äº†åˆ›æ–°èŒƒå¼ï¼Œåœ¨å¤šè½®å¯¹è¯ã€å·¥å…·è°ƒç”¨ç­‰éœ€åŠ¨æ€çº é”™çš„åœºæ™¯æœ‰å€Ÿé‰´ä»·å€¼ã€‚  
3. æ³›åŒ–èƒ½åŠ›å±‚é¢ï¼šåœ¨ unseen API ä¸Šçš„è‰¯å¥½æ³›åŒ–è¡¨ç°ï¼Œä¸ºå¤§æ¨¡å‹è·¨é¢†åŸŸã€è·¨å·¥å…·çš„é€šç”¨èƒ½åŠ›æå‡ç ”ç©¶æä¾›äº†å‚è€ƒæ–¹å‘ï¼ŒåŠ©åŠ›æ¢ç´¢æ¨¡å‹æ›´é«˜æ•ˆçš„çŸ¥è¯†è¿ç§»ä¸æ³›åŒ–è·¯å¾„ã€‚

## petoolllm--towards-personalized-tool-learning-in-large-language-models
### Abstract
Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PEToolLLMï¼šå¤§è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–å·¥å…·å­¦ä¹ æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨æ–‡æœ¬æ”¹å†™ã€é—®ç­”ã€ä»£ç ç¼–å†™ç­‰ä»»åŠ¡è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤©æ°”æŸ¥è¯¢ã€èˆªç­é¢„è®¢ç­‰åœºæ™¯åº”å¯¹ç”¨æˆ·éœ€æ±‚æ—¶å­˜åœ¨ä¸è¶³ï¼Œå·¥å…·å­¦ä¹ åº”è¿è€Œç”Ÿï¼Œè®©LLMsèƒ½å€ŸåŠ©å¤–éƒ¨å·¥å…·æ‹“å±•èƒ½åŠ›ã€‚ç„¶è€Œç°æœ‰å·¥å…·å­¦ä¹ ç ”ç©¶èšç„¦é€šç”¨å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå¿½è§†ä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œæ— æ³•å¤„ç†ç”¨æˆ·éšå«åå¥½ã€‚æ¯”å¦‚ç”¨æˆ·æœç´¢æ–‡ç« æ—¶å¯¹å­¦æœ¯ç±»å·¥å…·çš„åå¥½ï¼Œéœ€ä»å†å²äº¤äº’æ¨æ–­ï¼Œä¸”å·¥å…·åŠŸèƒ½ç›¸åŒä½†éåŠŸèƒ½å±æ€§ï¼ˆå¦‚æ˜“ç”¨æ€§ã€é›†æˆæ€§ï¼‰ä¹Ÿå½±å“ç”¨æˆ·é€‰æ‹©ï¼Œå› æ­¤ä¸ªæ€§åŒ–å·¥å…·å­¦ä¹ è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡å®šä¹‰ä¸ªæ€§åŒ–å·¥å…·å­¦ä¹ ä»»åŠ¡  
æ˜ç¡®ä¸ªæ€§åŒ–å·¥å…·å­¦ä¹ ä»»åŠ¡ï¼Œè¦æ±‚LLMsç»“åˆç”¨æˆ·æŒ‡ä»¤ä¸äº¤äº’å†å²ï¼Œè€ƒè™‘æŒ‡ä»¤ä¸­æ˜¾å¼éœ€æ±‚å’Œå†å²èƒŒåéšå¼åå¥½æ¥ä½¿ç”¨å·¥å…·ï¼Œä¸ºLLMsä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨èƒ½åŠ›ç ”ç©¶æ­å»ºä»»åŠ¡æ¡†æ¶ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºé¦–ä¸ªä¸ªæ€§åŒ–å·¥å…·å­¦ä¹ åŸºå‡†PEToolBench  
å¡«è¡¥é¢†åŸŸåŸºå‡†ç©ºç™½ï¼Œåˆ†ä¸‰æ­¥æ„å»ºï¼šå·¥å…·å‡†å¤‡ï¼ˆä»RapidAPIæ”¶é›†å·¥å…·å¹¶è®©LLMç†è§£å…¶åŠŸèƒ½ä¸éåŠŸèƒ½å±æ€§ï¼‰ã€åå¥½æ„å»ºï¼ˆä¸ºåŒåŠŸèƒ½å·¥å…·æŒ‰éåŠŸèƒ½å±æ€§ç»™ä¸åŒç”¨æˆ·åˆ†é…åå¥½ï¼‰ã€æ•°æ®åˆ›å»ºï¼ˆåˆæˆå«äº¤äº’å†å²çš„ç”¨æˆ·æŒ‡ä»¤ï¼Œè®¾è®¡ä¸‰ç§ä¸ªæ€§åŒ–è®¾ç½®ï¼Œæœ€ç»ˆå¾—åˆ°12000æ¡æŒ‡ä»¤ç­‰ï¼Œè¦†ç›–å¤šå·¥å…·åœºæ™¯ä¸åå¥½ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºPEToolLLaMAä¸ªæ€§åŒ–å·¥å…·å­¦ä¹ æ¡†æ¶  
åˆ†ä¸¤é˜¶æ®µè®­ç»ƒé€‚é…ä¸ªæ€§åŒ–ä»»åŠ¡ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µèµ‹äºˆLLMåŸºç¡€å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼›ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é˜¶æ®µé‡‡æ ·åå¥½ä¸éåå¥½å·¥å…·è°ƒç”¨åš pairwise ä¼˜åŒ–ï¼Œæ›´å¥½å¯¹é½ç”¨æˆ·åå¥½ï¼Œæå‡ä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨è¡¨ç°ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨PEToolBenchä¸Šè¯„ä¼°6æ¬¾å¼€æºå’Œé—­æºLLMsï¼ˆå«GPT - 4oï¼‰ï¼ŒPEToolLLaMAè¡¨ç°è¿œè¶…ç°æœ‰æœ€ä½³LLMï¼Œéƒ¨åˆ†åœºæ™¯æå‡è¶…50%ï¼Œæœ‰åŠ›è¯æ˜å…¶ä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„ä¼˜è¶Šæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»»åŠ¡å®šä¹‰è§’åº¦ï¼šå¼€æ‹“ä¸ªæ€§åŒ–å·¥å…·å­¦ä¹ æ–°æ–¹å‘ï¼Œå¯å‘åç»­ç ”ç©¶å…³æ³¨ç”¨æˆ·äº¤äº’å†å²ä¸éšå«åå¥½ç»“åˆçš„å·¥å…·ä½¿ç”¨åœºæ™¯ï¼Œå®Œå–„LLMså·¥å…·èƒ½åŠ›ç»´åº¦ã€‚  
2. åŸºå‡†æ„å»ºè§’åº¦ï¼šPEToolBenchçš„æ„å»ºæµç¨‹ï¼ˆå·¥å…·åˆ†æã€åå¥½è®¾è®¡ã€æ•°æ®åˆæˆç­‰ï¼‰ä¸ºé¢†åŸŸç‰¹å®šåŸºå‡†æ‰“é€ æä¾›æ¨¡æ¿ï¼Œå°¤å…¶æ˜¯å¤šç»´åº¦è€ƒè™‘å·¥å…·å±æ€§å’Œç”¨æˆ·åå¥½çš„æ€è·¯ã€‚  
3. æ¨¡å‹è®­ç»ƒè§’åº¦ï¼šä¸¤é˜¶æ®µï¼ˆSFT + DPOï¼‰çš„è®­ç»ƒæ¡†æ¶ä¸ºæå‡LLMsä¸ªæ€§åŒ–èƒ½åŠ›æä¾›æœ‰æ•ˆèŒƒå¼ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€å¯¹é½ç”¨æˆ·åå¥½çš„ä»»åŠ¡åœºæ™¯ä¸­ã€‚  
4. å¼€æºè´¡çŒ®è§’åº¦ï¼šå…¬å¼€ä»£ç å’Œæ•°æ®ï¼Œåˆ©äºç¤¾åŒºåŸºäºæ­¤å¤ç°ã€æ”¹è¿›å’Œæ‹“å±•ç ”ç©¶ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## toolcoder--a-systematic-code-empowered-tool-learning-framework-for-large-language-models
### Abstract
Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolCoderï¼šä»¥ä»£ç èµ‹èƒ½å¤§æ¨¡å‹å·¥å…·å­¦ä¹ çš„ç³»ç»Ÿæ€§æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ–­å‘å±•ï¼Œå·¥å…·å­¦ä¹ æˆä¸ºå…¶è§£å†³å¤æ‚ç°å®ä»»åŠ¡çš„å…³é”®èƒ½åŠ›ï¼Œèƒ½è®©LLMsé€šè¿‡ä¸å¤–éƒ¨å·¥å…·ã€APIäº¤äº’æ‹“å±•è§£é¢˜è¾¹ç•Œã€‚ä½†ç°æœ‰å·¥å…·å­¦ä¹ æ–¹æ³•å­˜åœ¨è¯¸å¤šå±€é™ï¼šè¿‡åº¦ä¾èµ–äººå·¥æ„é€ çš„promptï¼Œå¤šæ­¥éª¤è§„åˆ’å›°éš¾ï¼›ç¼ºä¹ç²¾å‡†çš„é”™è¯¯è¯Šæ–­ä¸åæ€æœºåˆ¶ï¼Œæ‰§è¡Œå¤±è´¥æ—¶éš¾ä»¥å®šä½åŸå› ï¼›ä¸”æ— æ³•ç§¯ç´¯å¤ç”¨è¿‡å¾€æˆåŠŸç»éªŒï¼Œæ¯æ¬¡éƒ½è¦â€œä»é›¶å¼€å§‹â€è§£å†³ç›¸ä¼¼é—®é¢˜ã€‚è¿™äº›ç¼ºé™·åˆ¶çº¦äº†å·¥å…·å­¦ä¹ ç³»ç»Ÿçš„é²æ£’æ€§ã€é€‚åº”æ€§ä¸æ‰©å±•æ€§ï¼Œå› æ­¤äºŸéœ€æ–°æ¡†æ¶çªç ´ç“¶é¢ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†å·¥å…·å­¦ä¹ é‡æ„ä¸ºä»£ç ç”Ÿæˆä»»åŠ¡  
ToolCoderå—è½¯ä»¶å·¥ç¨‹åŸç†å¯å‘ï¼ŒæŠŠæ¨¡ç³Šçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºç»“æ„åŒ–çš„Pythonå‡½æ•°è„šæ‰‹æ¶ï¼ˆç±»ä¼¼è½¯ä»¶å·¥ç¨‹é‡Œçš„éœ€æ±‚åˆ†æç¯èŠ‚ï¼‰ï¼Œæ˜ç¡®è¾“å…¥è¾“å‡ºè§„èŒƒï¼Œè®©å¤§æ¨¡å‹èƒ½åŸºäºä»£ç èŒƒå¼å¼€å±•å¤æ‚æ¨ç†ä¸è§„åˆ’ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å—åŒ–æ‹†è§£ä¸ä»£ç æ‰§è¡Œé—­ç¯  
éµå¾ªæ¨¡å—åŒ–è®¾è®¡åŸåˆ™ï¼Œç”¨æè¿°æ€§æ³¨é‡ŠæŠŠä»»åŠ¡è„šæ‰‹æ¶ç³»ç»Ÿæ‹†è§£ä¸ºå­ä»»åŠ¡ï¼›ç”Ÿæˆå¯è¿è¡Œçš„å­å‡½æ•°ä¸ä¸»å‡½æ•°ä»£ç å¹¶æ‰§è¡Œä»¥è·å–æœ€ç»ˆå“åº”ã€‚åŒæ—¶ï¼ŒæŠŠæˆåŠŸæ‰§è¡Œçš„ä»£ç ç‰‡æ®µå­˜å…¥å‡½æ•°ä»“åº“å®ç°ç»éªŒå¤ç”¨ï¼›è‹¥æ‰§è¡Œå¤±è´¥ï¼Œå€ŸåŠ©Pythonçš„é”™è¯¯å›æº¯ï¼ˆtracebackï¼‰æœºåˆ¶ç²¾å‡†è¯Šæ–­é—®é¢˜ï¼Œæå‡ç³»ç»Ÿå¯é æ€§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒToolCoderåœ¨ä»»åŠ¡å®Œæˆå‡†ç¡®ç‡ã€æ‰§è¡Œå¯é æ€§ç­‰æŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰SOTAæ–¹æ³•ï¼Œæœ‰åŠ›éªŒè¯äº†â€œä»¥ä»£ç ä¸ºä¸­å¿ƒâ€çš„å·¥å…·å­¦ä¹ æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿˜éªŒè¯äº†å„ç»„ä»¶çš„å¿…è¦æ€§ï¼Œä¸”å‘ç°å¯¹ä»£ç ç±»LLMsçš„æå‡æ¯”å¯¹åŸºç¡€LLMsæ›´æ˜¾è‘—ï¼Œè¿›ä¸€æ­¥å‡¸æ˜¾è¯¥æ¡†æ¶ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨é¢†åŸŸæ€è·¯èåˆï¼šæŠŠè½¯ä»¶å·¥ç¨‹æ–¹æ³•è®ºå¼•å…¥å¤§æ¨¡å‹å·¥å…·å­¦ä¹ ï¼Œä¸ºè§£å†³è‡ªç„¶è¯­è¨€é©±åŠ¨çš„å¤æ‚ä»»åŠ¡æä¾›äº†â€œä»£ç åŒ–â€çš„å…¨æ–°è§†è§’ï¼Œå¯å‘åç»­ç»“åˆä¸“ä¸šé¢†åŸŸæ–¹æ³•è®ºèµ‹èƒ½AIç³»ç»Ÿã€‚  
2. é—­ç¯æœºåˆ¶è®¾è®¡ï¼šé€šè¿‡â€œä»£ç ç”Ÿæˆ - æ‰§è¡Œ - å¤ç”¨/è°ƒè¯•â€çš„é—­ç¯ï¼ŒåŒæ—¶è§£å†³æ•ˆç‡ï¼ˆå¤ç”¨ï¼‰ä¸é²æ£’æ€§ï¼ˆè°ƒè¯•ï¼‰é—®é¢˜ï¼Œè¿™ç§å…¨æµç¨‹ä¼˜åŒ–çš„æ€è·¯å€¼å¾—åœ¨å„ç±»éœ€è¿­ä»£ä¼˜åŒ–çš„AIä»»åŠ¡æ¡†æ¶è®¾è®¡ä¸­å‚è€ƒã€‚  
3. å®éªŒéªŒè¯ç»´åº¦ï¼šä¸ä»…éªŒè¯æ•´ä½“æ€§èƒ½è¶…è¶ŠSOTAï¼Œè¿˜æ‹†è§£éªŒè¯å„ç»„ä»¶å¿…è¦æ€§ã€å¯¹æ¯”ä¸åŒç±»å‹LLMsçš„æ”¶ç›Šå·®å¼‚ï¼Œä¸ºç›¸å…³ç ”ç©¶çš„å®éªŒè®¾è®¡æä¾›äº†å…¨é¢æ€§çš„ç¤ºèŒƒã€‚

## mimicking-the-familiar--dynamic-command-generation-for-information-theft-attacks-in-llm-tool-learning-system
### Abstract
Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches. However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation chain of tools. It makes malicious commands more likely to be detected by LLM and leads to attack failure. In this paper, we propose AutoCMD, a dynamic attack comment generation approach for information theft attacks in LLM tool-learning systems. Inspired by the concept of mimicking the familiar, AutoCMD is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thereby generating more targeted commands for information theft. The evaluation results show that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can be generalized to new tool-learning systems to expose their information leakage risks. We also design four defense methods to effectively protect tool-learning systems from the attack.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLMå·¥å…·å­¦ä¹ ç³»ç»Ÿä¿¡æ¯çªƒå–æ–°æŒ‘æˆ˜ï¼šAutoCMDåŠ¨æ€æ”»å‡»å‘½ä»¤ç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·¥å…·å­¦ä¹ ç³»ç»Ÿè“¬å‹ƒå‘å±•çš„å½“ä¸‹ï¼Œä¿¡æ¯çªƒå–æ”»å‡»å¯¹å…¶æ„æˆäº†é‡å¤§å®‰å…¨é£é™©ã€‚æ”»å‡»è€…èƒ½é€šè¿‡å— compromised çš„å·¥å…·æ³¨å…¥æ¶æ„å‘½ä»¤ï¼Œæ“çºµ LLM æŠŠæ•æ„Ÿä¿¡æ¯å‘é€ç»™è¿™äº›å·¥å…·ï¼Œè¿›è€Œå¼•å‘éšç§æ³„éœ²ã€‚ç„¶è€Œï¼Œç°æœ‰æ”»å‡»æ–¹æ³•å¤šé¢å‘é»‘ç›’ä¸”ä¾èµ–é™æ€å‘½ä»¤ï¼Œæ— æ³•çµæ´»é€‚é…ç”¨æˆ·æŸ¥è¯¢ä¸å·¥å…·è°ƒç”¨é“¾çš„å˜åŒ–ï¼Œè¿™ä½¿å¾—æ¶æ„å‘½ä»¤æ˜“è¢« LLM æ£€æµ‹åˆ°ï¼Œå¯¼è‡´æ”»å‡»å¤±è´¥ã€‚æ‰€ä»¥ï¼ŒäºŸéœ€ä¸€ç§åŠ¨æ€çš„æ”»å‡»å‘½ä»¤ç”Ÿæˆæ–¹æ³•æ¥åº”å¯¹è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º AutoCMD åŠ¨æ€æ”»å‡»å‘½ä»¤ç”Ÿæˆæ–¹æ³•  
å—ç¤¾ä¼šå·¥ç¨‹å­¦ä¸­ â€œæ¨¡ä»¿ç†Ÿæ‚‰äº‹ç‰©â€ æ¦‚å¿µå¯å‘ï¼ŒAutoCMD èƒ½å¤Ÿé€šè¿‡å¯¹å¼€æºç³»ç»Ÿçš„å­¦ä¹ ä»¥åŠç›®æ ‡ç³»ç»Ÿç¤ºä¾‹çš„å¼ºåŒ–ï¼Œæ¨æ–­å·¥å…·é“¾ä¸­ä¸Šæ¸¸å·¥å…·æ‰€ä½¿ç”¨çš„ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´å…·é’ˆå¯¹æ€§çš„ä¿¡æ¯çªƒå–å‘½ä»¤ã€‚æ¯”å¦‚ï¼Œå®ƒå¯ä»¥ä»ç”¨æˆ·æŸ¥è¯¢é‡ŒåŠ¨æ€æ¨æ–­ä¸åŒå·¥å…·ï¼ˆåƒé…’åº—é¢„è®¢ã€èˆªç­é¢„è®¢å·¥å…·ï¼‰ä¸­çš„ç”¨æˆ·åã€å¯†ç ç­‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶åˆç†åµŒå…¥å·¥å…·å‚æ•°åˆ—è¡¨ï¼Œè®©æ”»å‡»å‘½ä»¤æ›´éš¾è¢«æ£€æµ‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºæ”»å‡»æ¡ˆä¾‹æ•°æ®åº“ï¼ˆAttackDBï¼‰ä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–  
é¦–å…ˆå‡†å¤‡ AttackDBï¼Œè¯†åˆ«å·¥å…·é—´å½±å“ä¿¡æ¯çªƒå–æ”»å‡»æˆåŠŸç‡çš„å…³é”®ä¿¡æ¯äº¤æ¢ï¼›ä¹‹ååœ¨é»‘ç›’æ”»å‡»åœºæ™¯ä¸­åº”ç”¨ AutoCMDï¼Œä»…åˆ©ç”¨æ¶æ„å·¥å…·å’Œ AttackDB ç”Ÿæˆå‘½ä»¤ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–ï¼Œå€ŸåŠ©å¥–åŠ±æå‡æ”»å‡»æœ‰æ•ˆæ€§ï¼Œè®©å…¶åœ¨ä»…çŸ¥æ¶æ„å·¥å…·æ—¶ä¹Ÿèƒ½æœ‰æ•ˆå‘èµ·ä¿¡æ¯çªƒå–æ”»å‡»ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡é˜²å¾¡æ–¹æ³•  
é’ˆå¯¹ AutoCMD æ”»å‡»è®¾è®¡äº†å››ç§é˜²å¾¡æ–¹æ³•ï¼Œç”¨äºæœ‰æ•ˆä¿æŠ¤å·¥å…·å­¦ä¹ ç³»ç»Ÿå…å—è¯¥æ”»å‡»å¨èƒï¼Œå¡«è¡¥äº†é˜²å¾¡å±‚é¢é’ˆå¯¹è¿™ç±»åŠ¨æ€æ”»å‡»çš„ç©ºç™½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ ToolBenchã€ToolEyes å’Œ AutoGen è¿™ä¸‰ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œç”¨ 1260 ä¸ªæ¨ç†æ¡ˆä¾‹è¿›è¡Œå®éªŒå¹¶ä¸ä¸‰ä¸ªåŸºçº¿å¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤º AutoCMD åœ¨æƒè¡¡æŒ‡æ ‡ $ASR_{Theft}$ ä¸Šæ¯”åŸºçº¿é«˜å‡º 13.2%ï¼Œå®ç°äº†æœ€é«˜çš„æ”»å‡»éšè”½æ€§ä¸æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œå°†ä¼˜åŒ–åçš„æ¨¡å‹åº”ç”¨åˆ° LangChainã€KwaiAgentsã€QwenAgent ç­‰é»‘ç›’ LLM å·¥å…·å­¦ä¹ ç³»ç»Ÿæ—¶ï¼Œèƒ½æš´éœ²å…¶ä¿¡æ¯æ³„éœ²é£é™©ï¼Œä¸”åœ¨è¿™äº›ç³»ç»Ÿä¸­ $ASR_{Theft}$ è¶…è¿‡ 80.9%ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç ”ç©¶è§’åº¦ï¼Œæå‡ºçš„åŠ¨æ€å‘½ä»¤ç”Ÿæˆæ€è·¯ä¸º LLM å·¥å…·å­¦ä¹ ç³»ç»Ÿå®‰å…¨é¢†åŸŸå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ¨¡ä»¿ç†Ÿæ‚‰æ¨¡å¼æ¥ç»•è¿‡æ£€æµ‹çš„æ€è·¯å¾ˆæœ‰å¯å‘æ€§ï¼›åœ¨å·¥ç¨‹å®è·µä¸Šï¼Œæ„å»ºçš„ AttackDB ä»¥åŠå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æµç¨‹ï¼Œä¸ºåç»­ç±»ä¼¼æ”»å‡»ç”Ÿæˆæˆ–é˜²å¾¡ç ”ç©¶æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯è·¯çº¿ï¼›é˜²å¾¡æ–¹æ³•çš„è®¾è®¡ä¹Ÿä¸ºä¼ä¸šå’Œå¼€å‘è€…ä¿æŠ¤è‡ªèº« LLM å·¥å…·ç³»ç»Ÿå®‰å…¨æä¾›äº†ç›´æ¥å‚è€ƒï¼Œèƒ½åŠ©åŠ›ä»–ä»¬åº”å¯¹æ–°å…´çš„åŠ¨æ€ä¿¡æ¯çªƒå–æ”»å‡»å¨èƒã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜å…¬å¼€äº†ä»£ç å’Œæ•°æ®é›†ï¼Œæ–¹ä¾¿è¯¥æ–¹å‘çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## tool-unlearning-for-tool-augmented-llms
### Abstract
Tool-augmented large language models (LLMs) are often trained on datasets of query-response pairs, which embed the ability to use tools or APIs directly into the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to forget learned tools due to security vulnerabilities, privacy regulations, or tool deprecations. However, ``tool unlearning'' has not been investigated in unlearning literature. We introduce this novel task, which requires addressing distinct challenges compared to traditional unlearning: knowledge removal rather than forgetting individual samples, the high cost of optimizing LLMs, and the need for principled evaluation metrics. To bridge these gaps, we propose ToolDelete, the first approach for unlearning tools from tool-augmented LLMs. It implements three key properties to address the above challenges for effective tool unlearning and introduces a new membership inference attack (MIA) model for effective evaluation. Extensive experiments on multiple tool learning datasets and tool-augmented LLMs show that ToolDelete effectively unlearns randomly selected tools, while preserving the LLM's knowledge on non-deleted tools and maintaining performance on general tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å·¥å…·å¢å¼ºå‹å¤§æ¨¡å‹çš„â€œå·¥å…·é—å¿˜â€ï¼šToolDelete å¼€å¯æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥å…·å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆTool - augmented LLMsï¼‰å¸¸é€šè¿‡æŸ¥è¯¢ - å“åº”å¯¹æ•°æ®é›†è®­ç»ƒï¼Œå°†ä½¿ç”¨å·¥å…·æˆ–APIçš„èƒ½åŠ›åµŒå…¥æ¨¡å‹å‚æ•°çŸ¥è¯†ä¸­ã€‚åœ¨å®é™…åº”ç”¨é‡Œï¼Œå› å®‰å…¨æ¼æ´ã€éšç§æ³•è§„ã€å·¥å…·å¼ƒç”¨ç­‰æƒ…å†µï¼Œæ¨¡å‹éœ€â€œé—å¿˜â€ç‰¹å®šå·¥å…·ï¼Œä½†â€œå·¥å…·é—å¿˜â€åœ¨é—å¿˜ç ”ç©¶é¢†åŸŸæ­¤å‰æœªè¢«æ¢ç´¢ã€‚ä¸ä¼ ç»Ÿæ ·æœ¬çº§é—å¿˜ä¸åŒï¼Œå·¥å…·é—å¿˜é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼šéœ€ç§»é™¤å·¥å…·ç›¸å…³çš„åŠŸèƒ½æ€§çŸ¥è¯†è€Œéå•ä¸ªæ•°æ®ç‚¹ã€å¤§æ¨¡å‹ä¼˜åŒ–æˆæœ¬é«˜ã€éœ€åˆç†çš„è¯„ä¼°æŒ‡æ ‡ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡å¼€å±•å·¥å…·é—å¿˜ä»»åŠ¡çš„ç ”ç©¶å¹¶æå‡ºè§£å†³æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºå·¥å…·é—å¿˜ä»»åŠ¡å¹¶å½¢å¼åŒ–å®šä¹‰
é¦–æ¬¡æ˜ç¡®â€œå·¥å…·é—å¿˜ï¼ˆTool Unlearningï¼‰â€ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯ä»å·¥å…·å¢å¼ºå‹LLMä¸­ç§»é™¤ä½¿ç”¨ç‰¹å®šå·¥å…·çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™ä½¿ç”¨å…¶ä»–å·¥å…·å’Œæ‰§è¡Œå¤§æ¨¡å‹ä¸€èˆ¬ä»»åŠ¡ï¼ˆå¦‚è¿è´¯æ–‡æœ¬ç”Ÿæˆï¼‰çš„èƒ½åŠ›ã€‚å½¢å¼åŒ–å®šä¹‰ä¸­ï¼Œç»™å®šè¦é—å¿˜çš„å·¥å…·åŠå…¶æ¼”ç¤ºé›† \( D_f \) å’Œè¦ä¿ç•™çš„å·¥å…·åŠå…¶æ¼”ç¤ºé›† \( D_r \)ï¼Œéœ€å¾—åˆ°æ¨¡å‹ \( f' \)ï¼Œä½¿å…¶å¯¹ \( D_f \) å¯¹åº”å·¥å…·çš„ä½¿ç”¨çŸ¥è¯†å—é™ï¼Œå¯¹ \( D_r \) å¯¹åº”å·¥å…·çš„ä½¿ç”¨èƒ½åŠ›ä¿ç•™ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºToolDeleteæ¡†æ¶
è¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å·¥å…·å¢å¼ºå‹LLMçš„é—å¿˜æ¡†æ¶ï¼Œæ»¡è¶³æœ‰æ•ˆå·¥å…·é—å¿˜çš„ä¸‰ä¸ªå…³é”®å±æ€§ï¼šå·¥å…·çŸ¥è¯†ç§»é™¤ï¼ˆç§»é™¤æ ‡è®°ä¸ºé—å¿˜å·¥å…·çš„ç›¸å…³çŸ¥è¯†ï¼‰ã€å·¥å…·çŸ¥è¯†ä¿ç•™ï¼ˆä¿ç•™å…¶ä»–å‰©ä½™å·¥å…·çš„çŸ¥è¯†ï¼‰ã€é€šç”¨èƒ½åŠ›ä¿ç•™ï¼ˆå€ŸåŠ©ä»»åŠ¡ç®—æœ¯ç­‰ç†å¿µç»´æŒå¤§æ¨¡å‹åœ¨æ–‡æœ¬å’Œä»£ç ç”Ÿæˆç­‰é€šç”¨ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼‰ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºLiRA - Toolè¯„ä¼°æ–¹æ³•
å°†Likelihood Ratio Attackï¼ˆLiRAï¼‰é€‚é…åˆ°å·¥å…·é—å¿˜ä»»åŠ¡ï¼Œä½œä¸ºé¦–ä¸ªç”¨äºå·¥å…·é—å¿˜çš„æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰æ¨¡å‹ï¼Œè¯„ä¼°å·¥å…·ç›¸å…³çŸ¥è¯†æ˜¯å¦æˆåŠŸè¢«é—å¿˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªå·¥å…·å­¦ä¹ æ•°æ®é›†å’Œå·¥å…·å¢å¼ºå‹LLMä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼šToolDeleteåœ¨é—å¿˜å·¥å…·å’Œä¿ç•™å·¥å…·çš„å‡†ç¡®æ€§ä¸Šï¼Œåˆ†åˆ«æ¯”ç°æœ‰é€šç”¨å’ŒLLMç‰¹å®šçš„é—å¿˜ç®—æ³•é«˜å‡º12.5å’Œ9.1ï¼›ä¸é‡æ–°è®­ç»ƒç›¸æ¯”ï¼Œå¯èŠ‚çœ74.8%çš„è®­ç»ƒæ—¶é—´ï¼›èƒ½å¤„ç†é¡ºåºé—å¿˜è¯·æ±‚ï¼›åœ¨ä½èµ„æºè®¾ç½®ä¸‹ä»èƒ½ä¿ç•™95%çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»»åŠ¡åˆ›æ–°è§’åº¦ï¼šå¼€æ‹“äº†å·¥å…·å¢å¼ºå‹å¤§æ¨¡å‹é¢†åŸŸä¸­â€œå·¥å…·é—å¿˜â€è¿™ä¸€å…¨æ–°ç ”ç©¶æ–¹å‘ï¼Œä¸ºåç»­é’ˆå¯¹æ¨¡å‹èƒ½åŠ›é€‰æ‹©æ€§é—å¿˜çš„ç ”ç©¶æä¾›äº†æ€è·¯å¯å‘ã€‚
2. æ–¹æ³•è®¾è®¡è§’åº¦ï¼šToolDeleteæ¡†æ¶ä¸­å¯¹å·¥å…·çŸ¥è¯†â€œç§»é™¤ - ä¿ç•™ - é€šç”¨èƒ½åŠ›ä¿ç•™â€çš„å¤šç»´åº¦è€ƒé‡ï¼Œä¸ºå¤„ç†æ¨¡å‹ä¸­ç‰¹å®šæŠ€èƒ½ç±»çŸ¥è¯†çš„è°ƒæ•´æä¾›äº†æ¶æ„å‚è€ƒï¼›LiRA - Toolåˆ™ä¸ºç‰¹æ®Šä»»åŠ¡ä¸‹çš„æ¨¡å‹çŸ¥è¯†é—å¿˜è¯„ä¼°æä¾›äº†æ–°çš„æœ‰æ•ˆæ‰‹æ®µã€‚
3. å®é™…åº”ç”¨è§’åº¦ï¼šé’ˆå¯¹çœŸå®åœºæ™¯ä¸­å·¥å…·å®‰å…¨ã€ç‰ˆæœ¬æ›´æ–°ã€éšç§åˆè§„ç­‰é—®é¢˜ï¼Œæä¾›äº†æ¨¡å‹å±‚é¢çš„è§£å†³æ–¹æ¡ˆæ€è·¯ï¼ŒåŠ©åŠ›å·¥å…·å¢å¼ºå‹LLMåœ¨å¤æ‚ç°å®åœºæ™¯ä¸­æ›´å¯é åœ°éƒ¨ç½²ã€‚

## divide-then-aggregate--an-efficient-tool-learning-method-via-parallel-tool-invocation
### Abstract
Although current Large Language Models (LLMs) exhibit impressive capabilities, performing complex real-world tasks still requires tool learning. Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to interact with external environments, but they are limited in perceptual scope and lack adequate task-planning capability. To address these limitations, other studies introduce the first Search-based Decision Tree (DFSDT), which still suffers from the high computational cost. In this paper, we introduce a novel parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama). First, we transform traditional tree-based tool search paths into Directed Acyclic Graph (DAG) structure, generating a high-quality parallel tool invocation dataset. The DTA-Llama is then trained on the dataset to learn to iteratively divide the current task into several parallel tool invocation sub-tasks and aggregate the invocation results to decide the next actions. Furthermore, we introduce an efficient inference framework inspired by the Process/Threads mechanism when applying the DTA-Llama to practical tasks. Experimental results show that our approach substantially enhances task performance while reducing token consumption and inference time. Llama2-7B, using our method, is comparable to the official parallel function calling method of GPT-3.5. The relevant code, dataset, and model weights are available at https://corn0205.github.io/
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¹¶è¡Œå·¥å…·è°ƒç”¨æ–°èŒƒå¼ï¼šDTA-Llamaè®©å¤§æ¨¡å‹å·¥å…·å­¦ä¹ æ›´é«˜æ•ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨è¯¸å¤šAIä»»åŠ¡ä¸­å±•ç°å¼ºå¤§èƒ½åŠ›ï¼Œä½†æ‰§è¡Œå¤æ‚ç°å®ä»»åŠ¡æ—¶éœ€å€ŸåŠ©å·¥å…·å­¦ä¹ æ¥æ‰©å±•è‡ªèº«èƒ½åŠ›ã€‚ç°æœ‰ä¸»æµå·¥å…·å­¦ä¹ æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼šCoT/ReActç­‰é‡‡ç”¨é¡ºåºå·¥å…·è°ƒç”¨èŒƒå¼ï¼Œæ„ŸçŸ¥èŒƒå›´å—é™ä¸”ä»»åŠ¡è§„åˆ’èƒ½åŠ›ä¸è¶³ï¼›åŸºäºæœç´¢çš„å†³ç­–æ ‘æ–¹æ³•ï¼ˆå¦‚DFSDTï¼‰è™½å°è¯•å…¨å±€è§„åˆ’ï¼Œä½†è®¡ç®—å¼€é”€å¤§ï¼Œä¸”æ¯è½®ä»…è°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼Œæ•ˆç‡åä½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºDTA-Llamaè¿™ä¸€æ–°é¢–çš„å¹¶è¡Œå·¥å…·è°ƒç”¨æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºDAGç»“æ„ä¸å¹³è¡Œå·¥å…·è°ƒç”¨æ•°æ®é›†  
å°†ä¼ ç»ŸåŸºäºæ ‘çš„å·¥å…·æœç´¢è·¯å¾„è½¬åŒ–ä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ç»“æ„ï¼Œå€ŸåŠ©å±‚åºéå†å®ç°å·¥å…·çš„å¹¶è¡Œæ‰§è¡Œï¼Œçªç ´äº†ä»¥å¾€é¡ºåºæ‰§è¡Œçš„é™åˆ¶ã€‚å¹¶åŸºäºå¹¿æ³›ä½¿ç”¨çš„ToolBenchæ•°æ®é›†æ„å»ºäº†é«˜è´¨é‡çš„å¹¶è¡Œå·¥å…·è°ƒç”¨æ•°æ®é›†DTA-Toolï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›ä¼˜è´¨æ•°æ®æ”¯æ’‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šProcess/Threadså¯å‘çš„æ¨ç†æ¡†æ¶  
è®¾è®¡å—è¿›ç¨‹/çº¿ç¨‹æœºåˆ¶å¯å‘çš„æ¨ç†æ¡†æ¶ï¼Œåœ¨æ¨ç†æ—¶è®©â€œProcessâ€ç»„ä»¶è´Ÿè´£è§„åˆ’å·¥å…·è°ƒç”¨ï¼Œå°†å¯å¹¶è¡Œçš„å·¥å…·åˆ’åˆ†åˆ°ä¸åŒâ€œThreadsâ€ä¸­ç‹¬ç«‹æ‰§è¡Œï¼Œæ‰§è¡Œåé€šè¿‡ä¸­é—´çŠ¶æ€é”èšåˆæ‰€æœ‰çº¿ç¨‹ç»“æœæ¥å†³å®šåç»­åŠ¨ä½œã€‚è¿™ç§æ–¹å¼ç¼©çŸ­äº†å·¥å…·è°ƒç”¨è·¯å¾„ï¼Œå¤§å¹…æå‡å¤§æ¨¡å‹å·¥å…·ä½¿ç”¨æ•ˆç‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ‰“é€ DTA-Llamaæ¨¡å‹  
åœ¨æ„å»ºå¥½çš„DTA-Toolæ•°æ®é›†ä¸Šè®­ç»ƒLlamaæ¨¡å‹å¾—åˆ°DTA-Llamaï¼Œä½¿å…¶å­¦ä¼šè¿­ä»£åœ°å°†å½“å‰ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå¹¶è¡Œå·¥å…·è°ƒç”¨å­ä»»åŠ¡ï¼Œå¹¶èšåˆè°ƒç”¨ç»“æœæ¥å†³ç­–ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå®ç°å¹¶è¡Œå·¥å…·è°ƒç”¨èƒ½åŠ›çš„å­¦ä¹ ä¸åº”ç”¨ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨StableToolBenchçœŸå®ä¸–ç•Œå·¥å…·ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»ä»»åŠ¡è§£å†³ç‡ï¼ˆå¯è§£å†³é€šè¿‡ç‡SoPRã€å¯è§£å†³èƒœç‡SoWRï¼‰ã€è®¡ç®—æˆæœ¬ï¼ˆtokenæ¶ˆè€—ç­‰ï¼‰ç­‰ç»´åº¦è¯„ä¼°ï¼ŒDTA-Llamaè¡¨ç°ä¼˜å¼‚ï¼šä¸ä»…æå‡äº†ä»»åŠ¡æ‰§è¡Œæ€§èƒ½ï¼Œè¿˜é™ä½äº†tokenæ¶ˆè€—ä¸æ¨ç†æ—¶é—´ï¼›ç»å¾®è°ƒçš„Llama2-7Bä½¿ç”¨è¯¥æ–¹æ³•åï¼Œåœ¨å·¥å…·è°ƒç”¨è¡¨ç°ä¸Šèƒ½ä¸GPT-3.5å®˜æ–¹å¹¶è¡Œå‡½æ•°è°ƒç”¨æ–¹æ³•ç›¸åª²ç¾ã€‚åŒæ—¶ï¼Œåœ¨å¤šä¸ªå¤§æ¨¡å‹ä¸Šå¾®è°ƒéªŒè¯äº†æ–¹æ³•çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®å±‚é¢ï¼šå°†ä¸²è¡Œæ ‘ç»“æ„æ•°æ®è½¬åŒ–ä¸ºDAGæ ¼å¼æ„å»ºå¹¶è¡Œæ•°æ®é›†ï¼Œä¸ºå¼€æºç¤¾åŒºæä¾›äº†é«˜è´¨é‡å¹¶è¡Œå·¥å…·è°ƒç”¨æ•°æ®èµ„æºï¼Œè¿™ç§æ•°æ®è½¬æ¢ä¸æ„å»ºæ€è·¯ä¸ºåç»­å·¥å…·å­¦ä¹ æ•°æ®å»ºè®¾æä¾›å‚è€ƒã€‚  
2. æ¡†æ¶å±‚é¢ï¼šåˆ›æ–°æ€§åœ°æŠŠè¿›ç¨‹/çº¿ç¨‹æœºåˆ¶å¼•å…¥å·¥å…·è°ƒç”¨æ¨ç†æµç¨‹ï¼Œå°†å·¥å…·è°ƒç”¨è½¬åŒ–ä¸ºç±»ä¼¼â€œè¿›ç¨‹è§„åˆ’ - çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ - ç»“æœèšåˆâ€çš„æ¨¡å¼ï¼Œä¸ºå¤§æ¨¡å‹å·¥å…·è°ƒç”¨çš„æ•ˆç‡ä¼˜åŒ–æä¾›äº†æ–°çš„æ¶æ„è®¾è®¡æ€è·¯ã€‚  
3. éªŒè¯å±‚é¢ï¼šä»æœ‰æ•ˆæ€§ã€è®¡ç®—æˆæœ¬ã€æ³›åŒ–èƒ½åŠ›å¤šç»´åº¦å…¨é¢éªŒè¯æ–¹æ³•ä¼˜åŠ¿ï¼Œè¿™ç§å¤šç»´åº¦è¯„ä¼°èŒƒå¼æœ‰åŠ©äºæ›´å……åˆ†åœ°å±•ç°æŠ€æœ¯ä»·å€¼ï¼Œå€¼å¾—ç›¸å…³ç ”ç©¶åœ¨æ–¹æ³•éªŒè¯æ—¶å€Ÿé‰´ã€‚

## graphtool-instruction--revolutionizing-graph-reasoning-in-llms-through-decomposed-subtask-instruction
### Abstract
Large language models (LLMs) have been demonstrated to possess the capabilities to understand fundamental graph properties and address various graph reasoning tasks. Existing methods fine-tune LLMs to understand and execute graph reasoning tasks by specially designed task instructions. However, these Text-Instruction methods generally exhibit poor performance. Inspired by tool learning, researchers propose Tool-Instruction methods to solve various graph problems by special tool calling (e.g., function, API and model), achieving significant improvements in graph reasoning tasks. Nevertheless, current Tool-Instruction approaches focus on the tool information and ignore the graph structure information, which leads to significantly inferior performance on small-scale LLMs (less than 13B). To tackle this issue, we propose GraphTool-Instruction, an innovative Instruction-tuning approach that decomposes the graph reasoning task into three distinct subtasks (i.e., graph extraction, tool name identification and tool parameter extraction), and design specialized instructions for each subtask. Our GraphTool-Instruction can be used as a plug-and-play prompt for different LLMs without fine-tuning. Moreover, building on GraphTool-Instruction, we develop GTools, a dataset that includes twenty graph reasoning tasks, and create a graph reasoning LLM called GraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph reasoning tasks with different graph types (e.g., graph size or graph direction), and we find that GraphTool-Instruction achieves SOTA compared to Text-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge gets further improvement of over 30% compared to the Tool-Instruction enhanced GPT-3.5-turbo, and it performs comparably to the high-cost GPT-4o. Our codes and data are available at https://anonymous.4open.science/r/GraphTool-Instruction.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GraphTool-Instructionï¼šé€šè¿‡åˆ†è§£å­ä»»åŠ¡æŒ‡ä»¤é©æ–°å¤§æ¨¡å‹çš„å›¾æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†å¤„ç†å›¾æ•°æ®æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚å›¾ç»“æ„å…·æœ‰é«˜è¿æ¥æ€§ã€ä¸°å¯Œç»„åˆç‰¹æ€§ä¸éæ¬§å‡ é‡Œå¾—ç‰¹å¾ï¼Œå’Œä¼ ç»Ÿæ–‡æœ¬ã€å›¾åƒæ•°æ®å¤„ç†æ–¹å¼å·®å¼‚æ˜¾è‘—ã€‚ç°æœ‰æ–¹æ³•ä¸­ï¼ŒText - Instruction ç±»æ–¹æ³•ï¼ˆå¦‚åŸºäºæ€ç»´é“¾çš„æ–¹æ³•ï¼‰åœ¨å¤æ‚å›¾æ¨ç†ä»»åŠ¡ä¸­å¯¹å›¾ç†è§£ï¼ˆGUï¼‰æŒ‘æˆ˜çš„æ€§èƒ½æå‡æœ‰é™ï¼›Tool - Instruction ç±»æ–¹æ³•è™½å¼•å…¥å·¥å…·å­¦ä¹ æ€è·¯ï¼Œä½†å­˜åœ¨å¿½è§†å›¾ç»“æ„ä¿¡æ¯çš„é—®é¢˜ï¼Œåœ¨å°è§„æ¨¡ LLMï¼ˆå°äº 13Bï¼‰ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸”éƒ¨åˆ†æ–¹æ³•ä¾èµ–å·¥å…·æ–‡æ¡£è´¨é‡æ˜“å‡ºç°å·¥å…·è°ƒç”¨å¤±è´¥ç­‰æƒ…å†µã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡º GraphTool - Instruction æ–¹æ³•æ¥å¢å¼º LLM å¤„ç†å›¾æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»»åŠ¡åˆ†è§£ä¸å¤šæŒ‡ä»¤è®¾è®¡
å°†å›¾æ¨ç†ä»»åŠ¡åˆ†è§£ä¸º**å›¾æå–ã€å·¥å…·åç§°è¯†åˆ«ã€å·¥å…·å‚æ•°æå–**ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡è®¾è®¡ä¸“é—¨æŒ‡ä»¤ã€‚é’ˆå¯¹å›¾ç†è§£ï¼ˆGUï¼‰æŒ‘æˆ˜ï¼Œæå‡º Graph - Instruction è§£å†³å›¾æå–ä»»åŠ¡ï¼ŒåŠ©åŠ› LLM ä»è‡ªç„¶è¯­è¨€æˆ–æ–‡ä»¶è·¯å¾„ä¸­è¯†åˆ«æå–å›¾ç»“æ„ä¿¡æ¯ï¼›é’ˆå¯¹å›¾å¤„ç†ï¼ˆGPï¼‰æŒ‘æˆ˜ï¼ŒæŠŠä¼ ç»Ÿ Tool - Instruction åˆ†è§£ä¸º Task - Instruction å’Œ Parameter - Instructionï¼ŒTask - Instruction å¼•å¯¼ LLM é€‰æ‹©åˆé€‚å›¾å·¥å…·å¹¶çº¦æŸå·¥å…·è¾“å‡ºæ ¼å¼ï¼ŒParameter - Instruction ä¸ºéœ€ç‰¹å®šè¾“å…¥çš„ä»»åŠ¡ï¼ˆå¦‚æœ€çŸ­è·¯å¾„ä»»åŠ¡ä¸­çš„èµ·æ­¢èŠ‚ç‚¹ï¼‰æå–å·¥å…·å‚æ•°ã€‚ä¸”è¯¥æ–¹æ³•æ— éœ€å¾®è°ƒï¼Œå¯ä½œä¸ºå³æ’å³ç”¨æç¤ºç”¨äºä¸åŒ LLMã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºæ•°æ®é›†ä¸ä¸“å±å¤§æ¨¡å‹
åŸºäº GraphTool - Instruction æ„å»º **GTools æ•°æ®é›†**ï¼ŒåŒ…å« 20 ç§å›¾æ¨ç†ä»»åŠ¡å…± 4 ä¸‡å®ä¾‹ï¼Œåœ¨ä»»åŠ¡å¤šæ ·æ€§å’Œå›¾è§„æ¨¡ä¸Šå¯¹ LLM æ•æ‰å›¾ç»“æ„ä¿¡æ¯æå‡ºæ›´é«˜æŒ‘æˆ˜ï¼›åŸºäº Llama3 - 8B å¹¶ä½¿ç”¨ GTools å¾®è°ƒï¼Œæ‰“é€ å›¾æ¨ç†ä¸“å±å¤§æ¨¡å‹ **GraphForge**ï¼Œæå‡ LLM åœ¨å›¾æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ 20 ä¸ªä¸åŒå›¾ç±»å‹ï¼ˆå¦‚å›¾å¤§å°ã€æ–¹å‘ï¼‰çš„å›¾æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œå¤§é‡å®éªŒï¼šæœªå¾®è°ƒæ—¶ï¼ŒGraphTool - Instruction åœ¨ Llama3 - 8B ä¸Šå¹³å‡å‡†ç¡®ç‡è¾¾ 94%ï¼Œæ˜¾è‘—è¶…è¿‡ Text - Instruction æ–¹æ³• 40% ä»¥ä¸Šã€è¶…è¿‡ GPT - 3.5 - turbo - FC 30% ä»¥ä¸Šï¼›ç» GTools å¾®è°ƒåçš„ GraphForge åœ¨æ‰€æœ‰å›¾æ¨ç†ä»»åŠ¡ä¸Šå¹³å‡å‡†ç¡®ç‡è¶… 98%ï¼Œæ€§èƒ½ä¸é«˜æˆæœ¬çš„ GPT - 4o ç›¸å½“ï¼Œä¸”ç›¸æ¯” Tool - Instruction å¢å¼ºåçš„ GPT - 3.5 - turbo æå‡è¶… 30%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»»åŠ¡åˆ†è§£æ€è·¯ï¼šé¢å¯¹å¤æ‚ä»»åŠ¡æ—¶ï¼Œå¯å€Ÿé‰´å°†å¤§ä»»åŠ¡æ‹†è§£ä¸ºæ›´æ˜“å¤„ç†çš„å­ä»»åŠ¡ï¼Œå¹¶ä¸ºå„å­ä»»åŠ¡å®šåˆ¶è§£å†³æ–¹æ¡ˆçš„æ€è·¯ï¼Œæå‡æ¨¡å‹å¤„ç†å¤æ‚é—®é¢˜èƒ½åŠ›ã€‚
2. æ•°æ®é›†æ„å»ºï¼šé’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡æ„å»ºä¸°å¯Œå¤šæ ·ä¸”æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°æä¾›æœ‰åŠ›æ”¯æ’‘ï¼Œæ¨åŠ¨é¢†åŸŸå†…æ¨¡å‹å‘å±•ã€‚
3. å·¥å…·ä¸æŒ‡ä»¤ç»“åˆï¼šåœ¨åˆ©ç”¨å·¥å…·å¢å¼ºæ¨¡å‹èƒ½åŠ›æ—¶ï¼Œå…³æ³¨é¢†åŸŸç‰¹å®šä¿¡æ¯ï¼ˆå¦‚å›¾ç»“æ„ä¿¡æ¯ï¼‰ï¼Œè®¾è®¡æ›´è´´åˆé¢†åŸŸéœ€æ±‚çš„æŒ‡ä»¤å’Œå·¥å…·è°ƒç”¨æ–¹å¼ï¼Œè§£å†³é¢†åŸŸä»»åŠ¡ç—›ç‚¹ã€‚

## federated-in-context-llm-agent-learning
### Abstract
Large Language Models (LLMs) have revolutionized intelligent services by enabling logical reasoning, tool use, and interaction with external systems as agents. The advancement of LLMs is frequently hindered by the scarcity of high-quality data, much of which is inherently sensitive. Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding private data. However, FL frameworks face significant bandwidth and computational demands, along with challenges from heterogeneous data distributions. The emerging in-context learning capability of LLMs offers a promising approach by aggregating natural language rather than bulky model parameters. Yet, this method risks privacy leakage, as it necessitates the collection and presentation of data samples from various clients during aggregation. In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for the first work unleashes the power of in-context learning to train diverse LLM agents through FL. In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods. Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools. We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\mathbf{3.33\times10^5}$ times.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è”é‚¦ä¸Šä¸‹æ–‡å­¦ä¹ èµ‹èƒ½å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼šFICAL ç®—æ³•é©æ–°è®­ç»ƒèŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡­å€Ÿé€»è¾‘æ¨ç†ã€å·¥å…·ä½¿ç”¨ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’èƒ½åŠ›ï¼Œé©æ–°äº†æ™ºèƒ½æœåŠ¡ï¼Œä½†é«˜è´¨é‡æ•°æ®ç¨€ç¼ºï¼ˆä¸”å¤šå«æ•æ„Ÿä¿¡æ¯ï¼‰åˆ¶çº¦å…¶å‘å±•ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰è™½èƒ½åœ¨ä¿æŠ¤éšç§ä¸‹ååŒè®­ç»ƒåˆ†å¸ƒå¼ LLMï¼Œå´é¢ä¸´å¸¦å®½ã€è®¡ç®—å¼€é”€å¤§åŠæ•°æ®å¼‚æ„åˆ†å¸ƒç­‰æŒ‘æˆ˜ã€‚LLM ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¸ºè”é‚¦è®­ç»ƒæä¾›æ–°æ€è·¯ï¼ˆèšåˆè‡ªç„¶è¯­è¨€è€Œéæ¨¡å‹å‚æ•°ï¼‰ï¼Œä½†èšåˆæ—¶æ”¶é›†å®¢æˆ·ç«¯æ•°æ®æ ·æœ¬æ˜“å¼•å‘éšç§æ³„éœ²ã€‚å› æ­¤ï¼Œå¦‚ä½•å€ŸåŠ©ä¸Šä¸‹æ–‡å­¦ä¹ åœ¨è”é‚¦åœºæ™¯è®­ç»ƒ LLM æ™ºèƒ½ä½“å¹¶ä¿éšœéšç§ï¼Œæˆä¸ºå¾…è§£éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º FICAL ç®—æ³•ï¼Œé¦–å¼€ä¸Šä¸‹æ–‡å­¦ä¹ èµ‹èƒ½è”é‚¦ LLM æ™ºèƒ½ä½“è®­ç»ƒä¹‹å…ˆæ²³  
ä¼ ç»Ÿè”é‚¦å­¦ä¹ æ¯è½®ä¼ è¾“æ¨¡å‹å‚æ•°ï¼ŒFICAL åˆ›æ–°æ€§è®¾è®¡ LLM å¢å¼ºçš„çŸ¥è¯†çº²è¦ç”Ÿæˆï¼ˆKCGï¼‰æ¨¡å—ï¼Œè®©å®¢æˆ·ç«¯ä¸æœåŠ¡ç«¯é—´ä¼ è¾“â€œçŸ¥è¯†çº²è¦â€ï¼ˆå«å·¥å…·ä½¿ç”¨çŸ¥è¯†ï¼‰è€Œéæ¨¡å‹å‚æ•°ã€‚æ­¤è®¾è®¡ä½¿é€šä¿¡å¤æ‚åº¦é™è‡³ O(1)ï¼ˆä¼ ç»Ÿå‚æ•°å…±äº« FL ä¸º O(N)ï¼ŒN ä¸ºæ¨¡å‹è§„æ¨¡ï¼‰ï¼Œåœ¨æ¨¡å‹è§„æ¨¡æŒç»­å¢å¤§è¶‹åŠ¿ä¸‹ï¼Œå±•ç°æå¼ºå¯æ‰©å±•æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å·¥å…·å­¦ä¹ ä¸åˆ©ç”¨ï¼ˆTLUï¼‰æ¨¡å—  
é¢å¯¹å¤§é‡å®¢æˆ·ç«¯å¯¼è‡´çŸ¥è¯†çº²è¦ä¸Šä¸‹æ–‡è¿‡é•¿ã€å½±å“æ™ºèƒ½ä½“æ€§èƒ½ç”šè‡³è¶…æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦é—®é¢˜ï¼ŒTLU æ¨¡å—è®© LLM æ™ºèƒ½ä½“å€ŸåŠ©é•¿ä¸Šä¸‹æ–‡èšåˆçŸ¥è¯†çº²è¦å­¦ä¹ å·¥å…·ä½¿ç”¨ï¼Œè§£å†³å¯æ‰©å±•æ€§æŒ‘æˆ˜åŒæ—¶æå‡å·¥å…·ä½¿ç”¨ç²¾åº¦ï¼ˆå®éªŒæ˜¾ç¤ºç²¾åº¦æå‡ 7.6%ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šéšç§ä¿æŠ¤ä¸æµç¨‹é©æ–°  
å®¢æˆ·ç«¯åŸºäºæœ¬åœ°æ•°æ®ï¼Œé€šè¿‡ KCG ç”Ÿæˆå«å·¥å…·ä½¿ç”¨åœºæ™¯ã€æ³¨æ„äº‹é¡¹ç­‰çš„**æœ¬åœ°çŸ¥è¯†çº²è¦**å¹¶ä¸Šä¼ ï¼›æœåŠ¡ç«¯èšåˆå½¢æˆ**å…¨å±€çŸ¥è¯†çº²è¦**ï¼ˆæè¿°å·¥å…·ä¿¡æ¯ï¼Œæ— éšç§æ³„éœ²é£é™©ï¼‰å›ä¼ ï¼›å®¢æˆ·ç«¯ä»¥å…¨å±€çŸ¥è¯†çº²è¦ä¸ºâ€œæ•™å¸ˆâ€ï¼Œé€šè¿‡ TLU æ¨¡å—å­¦ä¹ å·¥å…·è°ƒç”¨ â€”â€” å…¨æµç¨‹è§„é¿ä¼ ç»Ÿåˆæˆæ•°æ®æ˜“æ³„éœ²éšç§ã€é­æ”»å‡»çš„é—®é¢˜ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šåœºæ™¯å®éªŒä¸­ï¼ŒFICAL ä¸å…¶ä»– SOTA åŸºçº¿æ–¹æ³•ç›¸æ¯”æ€§èƒ½å…·ç«äº‰åŠ›ï¼Œä¸”é€šä¿¡æˆæœ¬é™ä½ **3.33Ã—10âµ å€**ï¼ŒéªŒè¯äº†å…¶åœ¨æ•ˆç‡ä¸æ•ˆæœä¸Šçš„ä¼˜åŠ¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°ï¼šå°†ä¸Šä¸‹æ–‡å­¦ä¹ ä¸è”é‚¦å­¦ä¹ ç»“åˆï¼Œä¸ºå¤§æ¨¡å‹è·¨ç«¯ååŒè®­ç»ƒå¼€è¾Ÿæ–°è·¯å¾„ï¼Œçªç ´ä¼ ç»Ÿå‚æ•°ä¼ è¾“ç“¶é¢ˆï¼›  
2. æ¨¡å—å¤ç”¨ï¼šKCG ä¸ TLU æ¨¡å—çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»è‡³éœ€ä½é€šä¿¡ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€å·¥å…·å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“æˆ–è”é‚¦åœºæ™¯ï¼›  
3. éšç§ä¿éšœï¼šé€šè¿‡â€œçŸ¥è¯†çº²è¦â€æ›¿ä»£å‚æ•°/åŸå§‹æ•°æ®ä¼ è¾“ï¼Œä¸ºéšç§æ•æ„Ÿåœºæ™¯ä¸‹çš„ååŒè®­ç»ƒæä¾›éšç§ä¿æŠ¤æ–°èŒƒå¼å‚è€ƒã€‚  

FICAL ä¸ä»…åœ¨æŠ€æœ¯å±‚é¢è§£å†³è”é‚¦è®­ç»ƒå¸¦å®½ã€è®¡ç®—ä¸éšç§ç—›ç‚¹ï¼Œæ›´åœ¨æ–¹æ³•è®ºä¸Šä¸ºå¤§æ¨¡å‹æ—¶ä»£çš„åˆ†å¸ƒå¼æ™ºèƒ½ä½“è®­ç»ƒæä¾›äº†åˆ›æ–°æ€§æ¡†æ¶ï¼Œæœ‰æœ›æ¨åŠ¨ LLM æ™ºèƒ½ä½“åœ¨å¤šç«¯åä½œåœºæ™¯çš„è½åœ°åº”ç”¨ã€‚

## tool-ed--enhancing-empathetic-response-generation-with-the-tool-calling-capability-of-llm
### Abstract
Empathetic conversation is a crucial characteristic in daily conversations between individuals. Nowadays, Large Language models (LLMs) have shown outstanding performance in generating empathetic responses. Knowledge bases like COMET can assist LLMs in mitigating illusions and enhancing the understanding of users' intentions and emotions. However, models remain heavily reliant on fixed knowledge bases and unrestricted incorporation of external knowledge can introduce noise. Tool learning is a flexible end-to-end approach that assists LLMs in handling complex problems. In this paper, we propose Emotional Knowledge Tool Calling (EKTC) framework, which encapsulates the commonsense knowledge bases as empathetic tools, enabling LLMs to integrate external knowledge flexibly through tool calling. In order to adapt the models to the new task, we construct a novel dataset TOOL-ED based on the EMPATHETICMPATHETIC DIALOGUE (ED) dataset. We validate EKTC on the ED dataset, and the experimental results demonstrate that our framework can enhance the ability of LLMs to generate empathetic responses effectively.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨LLMå·¥å…·è°ƒç”¨èƒ½åŠ›å¢å¼ºå…±æƒ…å›å¤ç”Ÿæˆï¼šTOOL - EDä¸EKTCæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å…±æƒ…å¯¹è¯æ˜¯æ—¥å¸¸äººé™…äº¤æµçš„å…³é”®ç‰¹æ€§ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå…±æƒ…å›å¤ä¸Šè¡¨ç°çªå‡ºï¼ŒCOMETç­‰çŸ¥è¯†åº“èƒ½è¾…åŠ©LLMså‡è½»å¹»è§‰ã€å¢å¼ºå¯¹ç”¨æˆ·æ„å›¾å’Œæƒ…ç»ªçš„ç†è§£ã€‚ä½†æ¨¡å‹è¿‡åº¦ä¾èµ–å›ºå®šçŸ¥è¯†åº“ï¼Œæ— é™åˆ¶å¼•å…¥å¤–éƒ¨çŸ¥è¯†ä¼šå¸¦æ¥å™ªå£°ã€‚å·¥å…·å­¦ä¹ æ˜¯å¸®åŠ©LLMså¤„ç†å¤æ‚é—®é¢˜çš„çµæ´»ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œè€Œå…±æƒ…å›å¤ç”Ÿæˆä»»åŠ¡ä¸­æ¨¡å‹éœ€ä¸»åŠ¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å…±æƒ…å·¥å…·ï¼ŒåŸºäºæ­¤æœ¬æ–‡å¼€å±•ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºEmotional Knowledge Tool Callingï¼ˆEKTCï¼‰æ¡†æ¶
å°†å¸¸è¯†çŸ¥è¯†åº“å°è£…ä¸ºå…±æƒ…å·¥å…·ï¼Œä½¿LLMsèƒ½é€šè¿‡å·¥å…·è°ƒç”¨çµæ´»æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œä»¥ç«¯åˆ°ç«¯æ–¹å¼è®©LLMsåŠ¨æ€è¿›è¡Œå¸¸è¯†æ¨ç†ï¼Œä¸”è¯¥æ¡†æ¶èƒ½è®©æ¨¡å‹è‡ªä¸»å†³å®šæ˜¯å¦åœ¨å…±æƒ…å¯¹è¯ä¸­è®¿é—®å¤–éƒ¨çŸ¥è¯†åº“ï¼Œè€Œéæ¯è½®å¯¹è¯éƒ½ä½¿ç”¨ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºTOOL - EDæ•°æ®é›†
åŸºäºEMPATHETIC DIALOGUEï¼ˆEDï¼‰æ•°æ®é›†ï¼Œå€ŸåŠ©LLMsæ„å»ºæ–°çš„TOOL - EDæ•°æ®é›†ã€‚é€‰æ‹©COMETä½œä¸ºä»£è¡¨æ€§å·¥å…·ï¼Œå°†å·¥å…·ä½¿ç”¨è½¨è¿¹æ’å…¥EDæ•°æ®é›†ï¼Œä¸ºæ¨¡æ‹Ÿå…±æƒ…å·¥å…·ä½¿ç”¨æä¾›åŸºå‡†ï¼Œé€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œè®©æ¨¡å‹è·å¾—ä½¿ç”¨å…±æƒ…å·¥å…·çš„èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šéªŒè¯æ¡†æ¶æ³›åŒ–æ€§
å°†ä¸¤ä¸ªä¸åŒçŸ¥è¯†åº“å®šä¹‰ä¸ºå·¥å…·ï¼Œé€šè¿‡å³æ’å³ç”¨çš„æ–¹å¼éªŒè¯EKTCçš„æ³›åŒ–æ€§ï¼Œæ¢ç´¢ä¸åŒå¸¸è¯†çŸ¥è¯†åº“ä½œä¸ºå·¥å…·æ—¶æ¡†æ¶çš„è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨EDæ•°æ®é›†ä¸ŠéªŒè¯EKTCæ¡†æ¶ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆå¢å¼ºLLMsç”Ÿæˆå…±æƒ…å›å¤çš„èƒ½åŠ›ï¼Œèƒ½åˆ©ç”¨å¤–éƒ¨å·¥å…·é«˜æ•ˆæå‡å…±æƒ…å›å¤ç”Ÿæˆè´¨é‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å·¥å…·å­¦ä¹ èŒƒå¼åº”ç”¨ï¼šé¦–æ¬¡å°†å·¥å…·å­¦ä¹ èŒƒå¼ç”¨äºå¢å¼ºLLMsçš„å…±æƒ…èƒ½åŠ›ï¼Œä¸ºæå‡æ¨¡å‹åœ¨å…±æƒ…å¯¹è¯ä»»åŠ¡çš„è¡¨ç°æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¯¥èŒƒå¼æ‹“å±•åˆ°å…¶ä»–ç±»ä¼¼éœ€çµæ´»çŸ¥è¯†æ•´åˆçš„å¯¹è¯ä»»åŠ¡ã€‚
2. æ•°æ®é›†æ„å»ºæ€è·¯ï¼šåŸºäºç°æœ‰çŸ¥åæ•°æ®é›†ç»“åˆå·¥å…·ä½¿ç”¨è½¨è¿¹æ„å»ºæ–°æ•°æ®é›†ï¼Œä¸ºç›¸å…³é¢†åŸŸåˆ›å»ºé€‚åˆç‰¹å®šä»»åŠ¡ï¼ˆå¦‚å·¥å…·è¾…åŠ©ç±»å¯¹è¯ä»»åŠ¡ï¼‰çš„æ•°æ®é›†æä¾›äº†å‚è€ƒï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥æ–¹å‘æ•°æ®èµ„æºçš„ä¸°å¯Œå’Œå®Œå–„ã€‚
3. å³æ’å³ç”¨éªŒè¯æ³›åŒ–ï¼šé€šè¿‡å°†ä¸åŒçŸ¥è¯†åº“å®šä¹‰ä¸ºå·¥å…·å¹¶éªŒè¯æ³›åŒ–æ€§ï¼Œè¿™ç§æ–¹å¼ä¸ºæ£€éªŒæ¡†æ¶å¯¹ä¸åŒçŸ¥è¯†æºçš„é€‚é…èƒ½åŠ›æä¾›äº†èŒƒä¾‹ï¼Œåœ¨åç»­ç ”ç©¶æ–°æ¡†æ¶æˆ–æ–¹æ³•å¯¹ä¸åŒå¤–éƒ¨èµ„æºçš„å…¼å®¹æ€§æ—¶å¯å€Ÿé‰´ã€‚

## toolken+--improving-llm-tool-usage-with-reranking-and-a-reject-option
### Abstract
The recently proposed ToolkenGPT tool learning paradigm demonstrates promising performance but suffers from two major issues: first, it cannot benefit from tool documentation, and second, it often makes mistakes in whether to use a tool at all. We introduce Toolken+ that mitigates the first problem by reranking top $k$ tools selected by ToolkenGPT and the second problem with a special "Reject" option such that the model will generate a vocabulary token if "Reject" is ranked first. We demonstrate the effectiveness of Toolken+ on multistep numerical reasoning and tool selection tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Toolken+ï¼šç”¨é‡æ’åºä¸æ‹’ç»é€‰é¡¹æå‡å¤§æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆå¤–éƒ¨å·¥å…·èƒ½æ‹“å±•èƒ½åŠ›ï¼Œä½†ç°æœ‰å·¥å…·å­¦ä¹ èŒƒå¼å­˜åœ¨ä¸è¶³ã€‚ToolkenGPTè™½è¡¨ç°å‡ºæ½œåŠ›ï¼Œå´é¢ä¸´ä¸¤å¤§é—®é¢˜ï¼šä¸€æ˜¯æ— æ³•åˆ©ç”¨å·¥å…·æ–‡æ¡£è¾…åŠ©å†³ç­–ï¼ŒäºŒæ˜¯åœ¨â€œæ˜¯å¦ä½¿ç”¨å·¥å…·â€çš„åˆ¤æ–­ä¸Šæ˜“å‡ºé”™ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæå‡ºToolken+æ¥å¢å¼ºå¤§æ¨¡å‹å·¥å…·ä½¿ç”¨çš„é²æ£’æ€§ä¸å‡†ç¡®æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå·¥å…·é‡æ’åºæœºåˆ¶  
ToolkenGPTéš¾ä»¥å€ŸåŠ©å·¥å…·æ–‡æ¡£é€‰æ‹©å·¥å…·ï¼ŒToolken+å¼•å…¥å·¥å…·åµŒå…¥å‰¯æœ¬å¯¹ToolkenGPTé€‰å‡ºçš„Top - kå·¥å…·è¿›è¡Œé‡æ’åºã€‚å…·ä½“æ˜¯å°†å·¥å…·æ–‡æ¡£å‰ç½®åˆ°æç¤ºä¸­ï¼Œè®©å¤§æ¨¡å‹ä¾æ®æ–‡æ¡£é€‰æœ€ç›¸å…³å·¥å…·ï¼Œä»¥æ­¤åˆ©ç”¨å·¥å…·æ–‡æ¡£è¾…åŠ©å·¥å…·é€‰æ‹©å†³ç­–ï¼Œç¼“è§£å·¥å…·é€‰æ‹©çš„ä¸ç¡®å®šæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šâ€œRejectâ€æ‹’ç»é€‰é¡¹  
é’ˆå¯¹ToolkenGPTåœ¨â€œæ˜¯å¦ä½¿ç”¨å·¥å…·â€åˆ¤æ–­ä¸Šçš„é”™è¯¯ï¼ˆå¦‚è¿‡åº¦è°ƒç”¨å·¥å…·ï¼‰ï¼ŒToolken+æ–°å¢ç‰¹æ®Šâ€œREJï¼ˆRejectï¼‰â€å·¥å…·ã€‚è‹¥â€œREJâ€åœ¨é‡æ’åºä¸­æ’ç¬¬ä¸€ï¼Œæ¨¡å‹åˆ‡æ¢å›æ–‡æœ¬ç”Ÿæˆæ¨¡å¼è€Œä¸è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œæœ‰æ•ˆå‡å°‘å·¥å…·è°ƒç”¨çš„è¯¯æŠ¥é”™è¯¯ï¼Œä¼˜åŒ–â€œæ˜¯å¦ä½¿ç”¨å·¥å…·â€è¿™ä¸€å†³ç­–ç¯èŠ‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨GSM8Kï¼ˆæ•°å€¼æ¨ç†ï¼‰ã€MetaToolï¼ˆå·¥å…·é€‰æ‹©ï¼‰å’ŒVirtualHomeï¼ˆå¤šæ­¥éª¤ä»»åŠ¡ï¼‰ç­‰æ•°æ®é›†ä¸Šè¯„ä¼°Toolken+ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ­¥éª¤æ•°å€¼æ¨ç†ä¸å·¥å…·é€‰æ‹©ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ€§èƒ½ï¼ŒéªŒè¯äº†é‡æ’åºæœºåˆ¶å’Œæ‹’ç»é€‰é¡¹å¯¹ä¼˜åŒ–å·¥å…·ä½¿ç”¨æµç¨‹å‰ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼ˆæ˜¯å¦ç”¨å·¥å…·ã€ç”¨å“ªä¸ªå·¥å…·ï¼‰çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è§£å†³å·¥å…·ä½¿ç”¨æµç¨‹å…³é”®ç¯èŠ‚é—®é¢˜çš„æ€è·¯ï¼šèšç„¦å·¥å…·ä½¿ç”¨â€œæ˜¯å¦ç”¨â€â€œç”¨å“ªä¸ªâ€é˜¶æ®µçš„ç—›ç‚¹ï¼Œé’ˆå¯¹æ€§è®¾è®¡æœºåˆ¶ï¼Œä¸ºä¼˜åŒ–å¤§æ¨¡å‹å·¥å…·åä½œæµç¨‹æä¾›äº†ç»†åˆ†ç¯èŠ‚ä¼˜åŒ–çš„å‚è€ƒã€‚
2. ç»“åˆæ–‡æ¡£ä¸è½»é‡é‡æ’åºçš„æ–¹æ³•ï¼šåˆ©ç”¨å·¥å…·æ–‡æ¡£è¾…åŠ©å·¥å…·é€‰æ‹©ï¼Œé€šè¿‡é‡æ’åºåœ¨ä¸å¤§é‡ä¿®æ”¹æ¨¡å‹çš„æƒ…å†µä¸‹æå‡å·¥å…·é€‰æ‹©å‡†ç¡®æ€§ï¼Œè¿™ç§è½»é‡åˆ©ç”¨å¤–éƒ¨ä¿¡æ¯å¢å¼ºå·¥å…·é€‰æ‹©çš„æ–¹å¼å€¼å¾—å€Ÿé‰´ã€‚
3. å¼•å…¥ç‰¹æ®Šæ ‡è®°ä¼˜åŒ–å†³ç­–è¾¹ç•Œï¼šâ€œRejectâ€é€‰é¡¹ä¸ºæ¨¡å‹åœ¨å·¥å…·è°ƒç”¨å†³ç­–ä¸Šæä¾›æ›´çµæ´»çš„æ§åˆ¶æ–¹å¼ï¼Œå¯ç¤ºåœ¨å¤§æ¨¡å‹ä¸å¤–éƒ¨äº¤äº’ï¼ˆå¦‚å·¥å…·è°ƒç”¨ã€ä»»åŠ¡å†³ç­–ï¼‰åœºæ™¯ä¸­ï¼Œå¯è®¾è®¡ç‰¹æ®Šæ ‡è®°æ¥ä¼˜åŒ–å†³ç­–é€»è¾‘ï¼Œå‡å°‘é”™è¯¯äº¤äº’ã€‚

## nestools--a-dataset-for-evaluating-nested-tool-learning-abilities-of-large-language-models
### Abstract
Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | NesToolsï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åµŒå¥—å·¥å…·å­¦ä¹ èƒ½åŠ›çš„æ–°æ•°æ®é›†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆå·¥å…·å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­å–å¾—äº†å‡ºè‰²æˆæœï¼Œå·¥å…·è°ƒç”¨è¿‡ç¨‹ä¸­å­˜åœ¨åµŒå¥—è°ƒç”¨ï¼ˆåä¸€ä¸ªå·¥å…·è°ƒç”¨ä»¥å‰ä¸€ä¸ªå·¥å…·å“åº”ä¸ºè¾“å…¥å‚æ•°ï¼‰çš„æƒ…å†µï¼Œä½†å½“å‰å¯¹åµŒå¥—å·¥å…·å­¦ä¹ èƒ½åŠ›çš„ç ”ç©¶ä¸è¶³ï¼Œç°æœ‰åŸºå‡†ç¼ºä¹ç›¸å…³æ•°æ®å®ä¾‹ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºNesToolsæ•°æ®é›†ç”¨äºå…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„åµŒå¥—å·¥å…·å­¦ä¹ èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ–°é¢–è‡ªåŠ¨æ•°æ®ç”Ÿæˆæ–¹æ³•  
è®¾è®¡äº†è‡ªåŠ¨åŒ–æ•°æ®é›†ç”Ÿæˆ pipeline æ¥æ„å»ºå¤§è§„æ¨¡ã€å…·æœ‰ä¸åŒåµŒå¥—ç»“æ„çš„åµŒå¥—å·¥å…·è°ƒç”¨æ•°æ®ï¼Œèƒ½ç”Ÿæˆæ¯”ç°æœ‰æ•°æ®é›†æ›´ä¸°å¯Œå¤šæ ·çš„åµŒå¥—å·¥å…·å­¦ä¹ ç¤ºä¾‹ï¼Œæ¶µç›–å·¥å…·ä¸å®ä¾‹ç”Ÿæˆã€æŸ¥è¯¢ç”Ÿæˆä»¥åŠæ•°æ®å®¡æŸ¥ä¸ä¼˜åŒ–ç­‰ç¯èŠ‚ï¼Œä¿éšœæ•°æ®è§„æ¨¡ä¸å¤šæ ·æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºé«˜è´¨é‡åµŒå¥—å·¥å…·å­¦ä¹ åŸºå‡†NesTools  
é€šè¿‡äººå·¥å®¡æŸ¥å’Œä¼˜åŒ–ï¼Œæ‰“é€ å‡ºé«˜è´¨é‡ä¸”è´´è¿‘çœŸå®åœºæ™¯çš„æ•°æ®é›†ï¼Œå¯ä½œä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åµŒå¥—å·¥å…·å­¦ä¹ èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚ç›¸æ¯”ç°æœ‰åŸºå‡†ï¼Œèšç„¦åµŒå¥—å·¥å…·å­¦ä¹ ä»»åŠ¡ï¼Œæä¾›å¤§è§„æ¨¡å·¥å…·ä¸æ›´å¤šåµŒå¥—è°ƒç”¨ï¼Œä¸”è¯„ä¼°ç»´åº¦æ›´ç»†ç²’åº¦ï¼ˆå·¥å…·é€‰æ‹©ã€è°ƒç”¨é¡ºåºã€å‚æ•°å¡«å……ã€åµŒå¥—å‚æ•°å¡«å……ï¼‰ï¼Œèƒ½æ›´å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨çœŸå®åµŒå¥—åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨22ä¸ªæµè¡Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆå«é—­æºå’Œå¼€æºæ¨¡å‹ï¼‰ä¸Šå¼€å±•å¤§é‡å®éªŒï¼Œä»åµŒå¥—æ·±åº¦ã€åµŒå¥—ç»“æ„ã€æ¨¡å‹è§„æ¨¡ç¼©æ”¾ã€é²æ£’æ€§å½±å“ç­‰æ–¹é¢æ·±å…¥åˆ†æã€‚ç»“æœè¡¨æ˜ï¼šå°½ç®¡æ¨¡å‹ä»è§„æ¨¡ç¼©æ”¾ä¸­å—ç›Šï¼Œä½†åœ¨ç®€å•å·¥å…·é€‰æ‹©ä¸Šä»æœ‰ä¸è¶³ï¼Œä¸”å·¥å…·æ·±åº¦åµŒå¥—æ—¶æ€§èƒ½ä¼šä¸‹é™ï¼Œè¯´æ˜å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚åµŒå¥—å·¥å…·å­¦ä¹ ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®ç”Ÿæˆå±‚é¢ï¼šå…¶è‡ªåŠ¨åŒ–æ•°æ®æ„å»º pipeline ä¸ºé¢†åŸŸå†…å¤§è§„æ¨¡ç‰¹å®šåœºæ™¯æ•°æ®é›†ç”Ÿæˆæä¾›äº†å‚è€ƒèŒƒå¼ï¼Œå¯ç”¨äºå…¶ä»–éœ€å¤æ‚äº¤äº’æˆ–åµŒå¥—é€»è¾‘çš„æ•°æ®æ„å»ºä»»åŠ¡ã€‚  
2. è¯„ä¼°ç»´åº¦å±‚é¢ï¼šç»†ç²’åº¦çš„åµŒå¥—å·¥å…·è°ƒç”¨è¯„ä¼°ç»´åº¦ï¼ˆå·¥å…·é€‰æ‹©ã€è°ƒç”¨é¡ºåºç­‰ï¼‰ä¸ºåç»­å·¥å…·å­¦ä¹ èƒ½åŠ›è¯„ä¼°æä¾›äº†æ›´å…¨é¢çš„æ€è·¯ï¼Œå¯æŒ‡å¯¼ç›¸å…³è¯„ä¼°æŒ‡æ ‡è®¾è®¡ã€‚  
3. æ•°æ®é›†ä»·å€¼å±‚é¢ï¼šNesTools ä½œä¸ºèšç„¦åµŒå¥—å·¥å…·å­¦ä¹ çš„é«˜è´¨é‡åŸºå‡†ï¼Œä¸ºç ”ç©¶å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å·¥å…·äº¤äº’åœºæ™¯çš„èƒ½åŠ›æä¾›äº†æœ‰åŠ›æ”¯æ’‘ï¼Œæ¨åŠ¨è¯¥æ–¹å‘ç ”ç©¶å‘å±•ã€‚
```

## from-exploration-to-mastery--enabling-llms-to-master-tools-via-self-driven-interactions
### Abstract
Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for LLMs, thereby facilitating effective tool utilization. This paper concentrates on the critical challenge of bridging the comprehension gap between LLMs and external tools due to the inadequacies and inaccuracies inherent in existing human-centric tool documentation. We propose a novel framework, DRAFT, aimed at Dynamically Refining tool documentation through the Analysis of Feedback and Trials emanating from LLMs' interactions with external tools. This methodology pivots on an innovative trial-and-error approach, consisting of three distinct learning phases: experience gathering, learning from experience, and documentation rewriting, to iteratively enhance the tool documentation. This process is further optimized by implementing a diversity-promoting exploration strategy to ensure explorative diversity and a tool-adaptive termination mechanism to prevent overfitting while enhancing efficiency. Extensive experiments on multiple datasets demonstrate that DRAFT's iterative, feedback-based refinement significantly ameliorates documentation quality, fostering a deeper comprehension and more effective utilization of tools by LLMs. Notably, our analysis reveals that the tool documentation refined via our approach demonstrates robust cross-model generalization capabilities.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»æ¢ç´¢åˆ°ç²¾é€šï¼šè®©å¤§æ¨¡å‹é€šè¿‡è‡ªä¸»äº¤äº’æŒæ¡å·¥å…·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥å…·å­¦ä¹ èƒ½è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è°ƒç”¨å¤–éƒ¨å·¥å…·ä¸ç¯å¢ƒäº¤äº’ï¼Œå¼¥è¡¥é¢„è®­ç»ƒæ•°æ®å±€é™ï¼Œä½†å·¥å…·æ–‡æ¡£æ˜¯å…³é”®ã€‚ç°æœ‰å·¥å…·æ–‡æ¡£ä»¥äººç±»ä¸ºä¸­å¿ƒï¼Œå­˜åœ¨ä¸è¶³ä¸ä¸å‡†ç¡®é—®é¢˜ï¼Œæ¯”å¦‚ä¿¡æ¯ä¸å®Œæ•´ã€å†—ä½™ã€ä¸å‡†ç¡®ç­‰ï¼Œå¯¼è‡´å¤§æ¨¡å‹ç†è§£å·¥å…·å›°éš¾ï¼Œé˜»ç¢å·¥å…·æœ‰æ•ˆä½¿ç”¨ï¼›ä¸”æ‰‹åŠ¨ä¿®æ”¹æ–‡æ¡£è€—æ—¶è´¹åŠ›ï¼Œå·¥å…·åŠŸèƒ½åŠ¨æ€æ›´æ–°ä¹Ÿè®©æ–‡æ¡£éš¾ä»¥åŠæ—¶é€‚é…ã€‚è€Œäººç±»é€šè¿‡åå¤äº¤äº’æŒæ¡å·¥å…·ï¼Œå—æ­¤å¯å‘ï¼Œè®ºæ–‡æ—¨åœ¨è®©æ–‡æ¡£èƒ½åŸºäºå¤§æ¨¡å‹ä¸å·¥å…·äº¤äº’åé¦ˆè‡ªåŠ¨ä¼˜åŒ–ï¼Œå¼¥åˆç†è§£é¸¿æ²Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDRAFTæ¡†æ¶  
DRAFTæ¡†æ¶æ—¨åœ¨é€šè¿‡åˆ†æå¤§æ¨¡å‹ä¸å¤–éƒ¨å·¥å…·äº¤äº’äº§ç”Ÿçš„åé¦ˆå’Œè¯•éªŒï¼ŒåŠ¨æ€ä¼˜åŒ–å·¥å…·æ–‡æ¡£ã€‚å®ƒé‡‡ç”¨è¯•é”™æ³•ï¼ŒåŒ…å«ä¸‰ä¸ªè”åŠ¨å­¦ä¹ é˜¶æ®µï¼šç»éªŒæ”¶é›†ï¼ˆé€šè¿‡è®¾è®¡çš„æ¢ç´¢å™¨æ¨¡æ‹Ÿå·¥å…·åº”ç”¨åœºæ™¯ã€ç”Ÿæˆæ¢ç´¢å®ä¾‹å¹¶æ•è·å·¥å…·æ‰§è¡Œç»“æœï¼‰ã€ä»ç»éªŒä¸­å­¦ä¹ ï¼ˆåˆ†æå™¨å‰–æç°æœ‰æ–‡æ¡£ï¼Œç»“åˆæ¢ç´¢å™¨å‘ç°å’Œåé¦ˆæå‡ºæ–‡æ¡£ä¿®æ”¹å»ºè®®ï¼‰ã€æ–‡æ¡£é‡å†™ï¼ˆé‡å†™å™¨æ•´åˆè§è§£ä¼˜åŒ–æ–‡æ¡£ï¼ŒåŒæ—¶æŒ‡å¯¼æ¢ç´¢å™¨åç»­æ¢ç´¢ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·æ€§ä¿ƒè¿›çš„æ¢ç´¢ç­–ç•¥ä¸å·¥å…·è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶  
è®¾è®¡å¤šæ ·æ€§ä¿ƒè¿›çš„æ¢ç´¢ç­–ç•¥ï¼Œç¡®ä¿æ¢ç´¢çš„å¤šæ ·æ€§ï¼Œä¸ºåç»­é‡å†™æä¾›æ›´å¹¿æ³›æ ·æœ¬ï¼›è€ƒè™‘ä¸åŒå·¥å…·å¯¹å¤§æ¨¡å‹å¤æ‚åº¦ä¸åŒï¼Œå¼•å…¥å·¥å…·è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶ï¼Œå½“æ–‡æ¡£ä¸å¤§æ¨¡å‹ç†è§£åŒ¹é…æ—¶åœæ­¢è¿­ä»£ï¼Œæå‡æ•ˆç‡ã€é¿å…è¿‡æ‹Ÿåˆï¼ŒèŠ‚çœæ—¶é—´å’Œèµ„æºã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDRAFTåŸºäºåé¦ˆçš„è¿­ä»£ä¼˜åŒ–æ˜¾è‘—æå‡äº†æ–‡æ¡£è´¨é‡ï¼Œèƒ½è®©å¤§æ¨¡å‹æ›´æ·±å…¥ç†è§£å’Œæœ‰æ•ˆä½¿ç”¨å·¥å…·ï¼›ä¸”ç»è¯¥æ–¹æ³•ä¼˜åŒ–åçš„å·¥å…·æ–‡æ¡£å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨ä¸åŸå§‹æ–‡æ¡£å¯¹æ¯”å·¥å…·æ–‡æ¡£æœ‰ç”¨æ€§çš„å®éªŒä¸­ï¼ŒDRAFTç”Ÿæˆçš„æ–‡æ¡£åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ›´å—å¤§æ¨¡å‹é’çã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œå°†äººç±»é€šè¿‡äº¤äº’æŒæ¡å·¥å…·çš„æ€è·¯è¿ç§»åˆ°æ–‡æ¡£ä¼˜åŒ–ï¼Œä¸ºè§£å†³å¤§æ¨¡å‹ä¸å·¥å…·é—´ç†è§£é¸¿æ²Ÿæä¾›äº†åˆ›æ–°èŒƒå¼ï¼›è¯•é”™æ³•åˆ†é˜¶æ®µè¿­ä»£ä¼˜åŒ–çš„æ€è·¯ï¼Œä»¥åŠå¤šæ ·æ€§æ¢ç´¢å’Œè‡ªé€‚åº”ç»ˆæ­¢ç­‰æœºåˆ¶ï¼Œåœ¨å¤„ç†éœ€åŠ¨æ€ä¼˜åŒ–ã€ä¾èµ–äº¤äº’åé¦ˆçš„ä»»åŠ¡åœºæ™¯ä¸­å…·æœ‰å‚è€ƒä»·å€¼ï¼Œæ¯”å¦‚å…¶ä»–éœ€æ¨¡å‹ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’å­¦ä¹ ä¼˜åŒ–æŒ‡å¯¼ææ–™çš„åœºæ™¯ã€‚ä»åº”ç”¨ä»·å€¼çœ‹ï¼Œä¸ºå·¥å…·å­¦ä¹ ä¸­å·¥å…·æ–‡æ¡£çš„è‡ªåŠ¨ä¼˜åŒ–æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼ŒåŠ©åŠ›å¤§æ¨¡å‹æ›´å¥½å‘æŒ¥å·¥å…·èƒ½åŠ›ï¼Œä¹Ÿä¸ºåº”å¯¹å·¥å…·åŠ¨æ€æ›´æ–°ä¸‹çš„æ–‡æ¡£ç»´æŠ¤éš¾é¢˜æä¾›äº†æ€è·¯ã€‚ 

## steptool--enhancing-multi-step-tool-usage-in-llms-via-step-grained-reinforcement-learning
### Abstract
Despite their powerful text generation capabilities, large language models (LLMs) still struggle to effectively utilize external tools to solve complex tasks, a challenge known as tool learning. Existing methods primarily rely on supervised fine-tuning, treating tool learning as a text generation problem while overlooking the decision-making complexities inherent in multi-step contexts. In this work, we propose modeling tool learning as a dynamic decision-making process and introduce StepTool, a novel step-grained reinforcement learning framework that enhances LLMs' capabilities in multi-step tool use. StepTool comprises two key components: Step-grained Reward Shaping, which assigns rewards to each tool interaction based on its invocation success and contribution to task completion; and Step-grained Optimization, which applies policy gradient methods to optimize the model across multiple decision steps. Extensive experiments across diverse benchmarks show that StepTool consistently outperforms both SFT-based and RL-based baselines in terms of task Pass Rate and Recall of relevant tools. Furthermore, our analysis suggests that StepTool helps models discover new tool-use strategies rather than merely re-weighting prior knowledge. These results highlight the importance of fine-grained decision modeling in tool learning and establish StepTool as a general and robust solution for enhancing multi-step tool use in LLMs. Code and data are available at https://github.com/yuyq18/StepTool.
### ğŸŒŸ è®ºæ–‡è§£è¯» | StepToolï¼šåŸºäºæ­¥ç²’åº¦å¼ºåŒ–å­¦ä¹ æå‡å¤§æ¨¡å‹å¤šæ­¥å·¥å…·ä½¿ç”¨èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½æœ‰å¼ºå¤§æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†åœ¨åˆ©ç”¨å¤–éƒ¨å·¥å…·è§£å†³å¤æ‚ä»»åŠ¡ï¼ˆå³å·¥å…·å­¦ä¹ ï¼‰æ—¶ä»å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå°†å·¥å…·å­¦ä¹ å½“ä½œæ–‡æœ¬ç”Ÿæˆé—®é¢˜ï¼Œå´å¿½è§†äº†å¤šæ­¥åœºæ™¯ä¸‹å†³ç­–çš„å¤æ‚æ€§ã€‚æ¯”å¦‚å¤æ‚ä»»åŠ¡éœ€å¤šè½®å·¥å…·è°ƒç”¨ä¸ç¯å¢ƒåé¦ˆï¼ŒSFT éš¾ä»¥å¯¹è¿™ç§åŠ¨æ€å†³ç­–è¿‡ç¨‹æœ‰æ•ˆå»ºæ¨¡ï¼Œé™åˆ¶äº†æ¨¡å‹å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦æ–°æ–¹æ³•æŠŠå·¥å…·å­¦ä¹ å»ºæ¨¡ä¸ºåŠ¨æ€å†³ç­–è¿‡ç¨‹ï¼Œæå‡å¤šæ­¥å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†å·¥å…·å­¦ä¹ å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹  
æŠŠæ¯ä¸€æ¬¡å·¥å…·è°ƒç”¨è§†ä¸ºä¼šå¼•å‘çŠ¶æ€è½¬ç§»çš„åŠ¨ä½œï¼Œä»åŠ¨ä½œ - çŠ¶æ€è½¬ç§»ä¸­å­¦ä¹ ï¼Œä¸ºå†³ç­–è¿‡ç¨‹æä¾›æ­¥çº§åˆ«çš„ç›‘ç£ã€‚ä¸å†å°†å·¥å…·å­¦ä¹ ç®€å•çœ‹ä½œæ–‡æœ¬ç”Ÿæˆï¼Œè€Œæ˜¯å…³æ³¨å¤šæ­¥å†³ç­–é‡Œæ¯ä¸€æ­¥å·¥å…·äº¤äº’å¯¹ä»»åŠ¡å®Œæˆçš„å½±å“ï¼Œè®©æ¨¡å‹èƒ½æ›´å¥½åº”å¯¹å¤æ‚ä»»åŠ¡ä¸­å¤šè½®å·¥å…·ä½¿ç”¨çš„æƒ…å†µã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡º StepTool æ¡†æ¶ï¼ˆå«æ­¥ç²’åº¦å¥–åŠ±å¡‘é€ ä¸æ­¥ç²’åº¦ä¼˜åŒ–ï¼‰  
- æ­¥ç²’åº¦å¥–åŠ±å¡‘é€ ï¼šä¾æ®å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§å’Œå¯¹æ•´ä½“ä»»åŠ¡å®Œæˆçš„è´¡çŒ®ï¼Œåœ¨æ¯ä¸€æ­¥è®¾è®¡å¥–åŠ±ã€‚è€ƒè™‘å·¥å…·äº¤äº’ä¸­é—´æ­¥éª¤çš„æ ¼å¼ä¸ä»»åŠ¡ç›®æ ‡ç­‰ç‰¹ç‚¹ï¼Œè®©å¥–åŠ±èƒ½æä¾›æ›´ä¸°å¯Œä¿¡å·ï¼Œæœ‰æ•ˆå¼•å¯¼æ¨¡å‹å†³ç­–ã€‚æ¯”å¦‚ä¸ä»…çœ‹å·¥å…·è°ƒç”¨æˆæ²¡æˆåŠŸï¼Œè¿˜è¦çœ‹è¿™ä¸€æ­¥å¯¹æœ€ç»ˆå®Œæˆä»»åŠ¡æœ‰æ²¡æœ‰å¸®åŠ©ã€‚  
- æ­¥ç²’åº¦ä¼˜åŒ–ï¼šåŸºäºç­–ç•¥æ¢¯åº¦ç†è®ºæå‡ºä¼˜åŒ–æ–¹æ³•ï¼Œé€‚é…å·¥å…·å­¦ä¹ ä¸­åŠ¨æ€ã€å¤šæ­¥çš„äº¤äº’åœºæ™¯ï¼Œå…‹æœåƒ RLHF è¿™ç±»å•æ­¥æ–¹æ³•çš„å±€é™ï¼Œåœ¨å¤šä¸ªå†³ç­–æ­¥éª¤ä¸­ä¼˜åŒ–æ¨¡å‹ï¼Œæå‡å¤šæ­¥å·¥å…·ä½¿ç”¨æ—¶çš„å†³ç­–èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒStepTool åœ¨ä»»åŠ¡é€šè¿‡ç‡ï¼ˆPass Rateï¼‰å’Œç›¸å…³å·¥å…·å¬å›ç‡ï¼ˆRecall of relevant toolsï¼‰ä¸ŠæŒç»­è¶…è¶ŠåŸºäº SFT å’Œå…¶ä»– RL æ–¹æ³•çš„åŸºçº¿æ¨¡å‹ã€‚è€Œä¸”åˆ†æè¡¨æ˜ï¼ŒStepTool èƒ½å¸®åŠ©æ¨¡å‹å‘ç°æ–°çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä¸åªæ˜¯å¯¹å·²æœ‰çŸ¥è¯†é‡æ–°åŠ æƒï¼Œå……åˆ†ä½“ç°äº†ç»†ç²’åº¦å†³ç­–å»ºæ¨¡åœ¨å·¥å…·å­¦ä¹ é‡Œçš„é‡è¦æ€§ï¼ŒéªŒè¯äº† StepTool æå‡å¤§æ¨¡å‹å¤šæ­¥å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æœ‰æ•ˆæ€§ä¸é€šç”¨æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- å»ºæ¨¡è§†è§’åˆ›æ–°ï¼šæŠŠå·¥å…·å­¦ä¹ ä»æ–‡æœ¬ç”Ÿæˆè§†è§’è½¬å‘åºåˆ—å†³ç­–è¿‡ç¨‹è§†è§’ï¼Œä¸ºè§£å†³éœ€å¤šæ­¥äº¤äº’ã€ç¯å¢ƒåé¦ˆçš„ä»»åŠ¡ç±»é—®é¢˜æä¾›äº†æ–°çš„å»ºæ¨¡æ€è·¯ï¼Œå¯å¯å‘åç»­åœ¨å·¥å…·å­¦ä¹ æˆ–ç±»ä¼¼å¤šæ­¥å†³ç­–ç±» NLP ä»»åŠ¡çš„ç ”ç©¶ã€‚  
- å¼ºåŒ–å­¦ä¹ è½åœ°ï¼šåœ¨å¤§æ¨¡å‹å·¥å…·å­¦ä¹ åœºæ™¯ä¸­ï¼Œè®¾è®¡äº†é€‚é…çš„æ­¥ç²’åº¦å¥–åŠ±ä¸ä¼˜åŒ–æ–¹å¼ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†å¤šæ­¥ã€åŠ¨æ€äº¤äº’ä»»åŠ¡æ—¶çš„æ½œåŠ›ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹å¤æ‚èƒ½åŠ›æå‡æ–¹å‘çš„åº”ç”¨æä¾›äº†å®è·µå‚è€ƒã€‚  
- å®éªŒéªŒè¯å…¨é¢ï¼šé€šè¿‡å¤šåŸºå‡†æµ‹è¯•éªŒè¯æœ‰æ•ˆæ€§ï¼Œä¸”å¯¹æ¨¡å‹èƒ½åŠ›æå‡çš„æœ¬è´¨ï¼ˆå‘ç°æ–°ç­–ç•¥è€Œéç®€å•åŠ æƒï¼‰åšåˆ†æï¼Œè¿™ç§å…¨é¢éªŒè¯ä¸æ·±å…¥åˆ†æçš„æ€è·¯ï¼Œå€¼å¾—ç§‘ç ”ä¸­å€Ÿé‰´ä»¥æ›´å……åˆ†è¯´æ˜æ–¹æ³•ä»·å€¼ã€‚

## learning-evolving-tools-for-large-language-models
### Abstract
Tool learning enables large language models (LLMs) to interact with external tools and APIs, greatly expanding the application scope of LLMs. However, due to the dynamic nature of external environments, these tools and APIs may become outdated over time, preventing LLMs from correctly invoking tools. Existing research primarily focuses on static environments and overlooks this issue, limiting the adaptability of LLMs in real-world applications. In this paper, we propose ToolEVO, a novel framework designed to enhance the adaptive and reflective capabilities of LLMs against tool variability. By leveraging Monte Carlo Tree Search, ToolEVO facilitates active exploration and interaction of LLMs within dynamic environments, allowing for autonomous self-reflection and self-updating of tool usage based on environmental feedback. Additionally, we introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of tool variability. Extensive experiments demonstrate the effectiveness and stability of our approach, highlighting the importance of adaptability to tool variability for effective tool learning. Code: https://github.com/Chen-GX/ToolEVO
### ğŸŒŸ è®ºæ–‡è§£è¯» | åº”å¯¹å·¥å…·å˜åŒ–ï¼Œè®©å¤§æ¨¡å‹å·¥å…·å­¦ä¹ æ›´æ™ºèƒ½ï¼šToolEVOæ¡†æ¶ä¸ToolQA - DåŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥å…·å­¦ä¹ èƒ½è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¤–éƒ¨å·¥å…·å’ŒAPIäº¤äº’ï¼Œæ‹“å±•å…¶åº”ç”¨èŒƒå›´ã€‚ä½†å¤–éƒ¨ç¯å¢ƒåŠ¨æ€å˜åŒ–ï¼Œå·¥å…·å’ŒAPIä¼šè¿‡æ—¶ï¼Œå¯¼è‡´LLMsæ— æ³•æ­£ç¡®è°ƒç”¨ã€‚ç°æœ‰ç ”ç©¶èšç„¦é™æ€ç¯å¢ƒï¼Œå¿½ç•¥æ­¤é—®é¢˜ï¼Œé™åˆ¶äº†LLMsåœ¨çœŸå®åœºæ™¯çš„é€‚åº”æ€§ã€‚æ¯”å¦‚LLMså­¦åˆ°çš„APIå’ŒçœŸå®ç¯å¢ƒéƒ¨ç½²çš„APIå¯èƒ½åœ¨åç§°ã€å‚æ•°ã€å“åº”æ ¼å¼ç­‰æ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œä½¿å…¶è°ƒç”¨å‡ºé”™ï¼Œè€Œå®æ—¶æ›´æ–°APIåˆè€—æ—¶è€—èµ„æºã€‚æ‰€ä»¥æœ¬æ–‡æ—¨åœ¨æå‡æ¨¡å‹åœ¨å·¥å…·å­¦ä¹ ä¸­çš„é€‚åº”æ€§ï¼Œåº”å¯¹åŠ¨æ€å¤–éƒ¨ç¯å¢ƒçš„å¤æ‚æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºToolEVOæ¡†æ¶
ToolEVOæ—¨åœ¨å¢å¼ºLLMsåº”å¯¹å·¥å…·å˜åŒ–çš„è‡ªé€‚åº”å’Œåæ€èƒ½åŠ›ã€‚å®ƒå€Ÿé‰´è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œè®©LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¸»åŠ¨æ¢ç´¢å’Œäº¤äº’ï¼ŒåŸºäºç¯å¢ƒåé¦ˆè‡ªä¸»åæ€å’Œæ›´æ–°å·¥å…·ä½¿ç”¨æ–¹å¼ã€‚MCTSèƒ½å¤„ç†åŠ¨æ€ç¯å¢ƒä¸­åºå¤§çš„åŠ¨ä½œç©ºé—´ï¼ŒLLMsä¸å†æ˜¯å•çº¯è®°å¿†ç°æœ‰å·¥å…·è°ƒç”¨æ¨¡å¼ï¼Œè€Œæ˜¯é€šè¿‡è‡ªä¸»æ¢ç´¢çš„è¯•é”™æ¥ç†è§£å·¥å…·å˜åŒ–ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºToolQA - DåŸºå‡†
åŸºäºToolQAæ„å»ºäº†é¦–ä¸ªç”¨äºè¯„ä¼°å·¥å…·å˜åŒ–å½±å“çš„åŸºå‡†ToolQA - Dã€‚è¯¥åŸºå‡†å¯ç”¨äºç ”ç©¶ä¸åŒAPIå˜åŒ–ï¼ˆå¦‚åç§°ã€å‚æ•°ã€å“åº”æ ¼å¼ç­‰ï¼‰å¯¹LLMsçš„å½±å“ï¼Œæ¨åŠ¨ç›¸å…³ç ”ç©¶å‘å±•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å¤§é‡å®éªŒå……åˆ†è¯æ˜äº†ToolEVOæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ï¼Œåœ¨é€‚åº”å·¥å…·å˜åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå‡¸æ˜¾äº†å¯¹å·¥å…·å˜åŒ–çš„é€‚åº”æ€§åœ¨æœ‰æ•ˆå·¥å…·å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚åŒæ—¶åŸºäºToolQA - DåŸºå‡†çš„ç ”ç©¶ä¹Ÿèƒ½å…¨é¢åˆ†æå„ç±»APIå˜åŒ–å¯¹LLMsçš„å½±å“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å…³æ³¨çœŸå®åœºæ™¯é—®é¢˜ï¼šé¦–æ¬¡èšç„¦å·¥å…·å˜åŒ–å¯¹LLMsæ€§èƒ½çš„å½±å“ï¼Œè¿™å¯¹ä¿éšœLLMsåœ¨çœŸå®åº”ç”¨ä¸­çš„å¯é æ€§å’Œé€‚åº”æ€§è‡³å…³é‡è¦ï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜äº†å…³æ³¨çœŸå®åŠ¨æ€åœºæ™¯çš„æ–¹å‘ã€‚
2. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šToolEVOé€šè¿‡MCTSå¢å¼ºLLMsåœ¨åŠ¨æ€ç¯å¢ƒçš„äº¤äº’å’Œæ¢ç´¢ï¼Œè¿™ç§è®©æ¨¡å‹é€šè¿‡è¯•é”™ç†è§£å˜åŒ–è€Œéå•çº¯å¤åˆ¶ç°æœ‰æ¨¡å¼çš„æ€è·¯ï¼Œå¯ä¸ºå¤„ç†å…¶ä»–åŠ¨æ€åœºæ™¯ä¸‹çš„æ¨¡å‹èƒ½åŠ›æå‡æä¾›å‚è€ƒã€‚
3. åŸºå‡†æ„å»ºï¼šæ„å»ºToolQA - DåŸºå‡†ç”¨äºç‰¹å®šç ”ç©¶æ–¹å‘çš„è¯„ä¼°ï¼Œè¿™ç§é’ˆå¯¹æ–°é—®é¢˜æ„å»ºåŸºå‡†ä»¥æ¨åŠ¨ç ”ç©¶çš„æ–¹å¼ï¼Œå€¼å¾—åœ¨å…¶ä»–æ–°å…´ç ”ç©¶é¢†åŸŸå€Ÿé‰´ã€‚

## toolgen--unified-tool-retrieval-and-calling-via-generation
### Abstract
As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM's parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation. Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains. By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolGenï¼šç”¨ç”Ÿæˆå¼èŒƒå¼é©æ–°å·¥å…·è°ƒç”¨ä¸æ£€ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ–­å‘å±•ï¼Œå…¶æ— æ³•è‡ªä¸»é€šè¿‡ä¸å¤–éƒ¨å·¥å…·ç›´æ¥äº¤äº’æ‰§è¡Œä»»åŠ¡çš„é—®é¢˜ï¼Œæˆä¸ºå…³é”®ç“¶é¢ˆã€‚ä¼ ç»Ÿå·¥å…·è°ƒç”¨æ–¹æ³•ä¾èµ–å°†å·¥å…·æè¿°ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ï¼Œå—é™äºä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä¸”éœ€è¦å•ç‹¬ã€ä½æ•ˆçš„æ£€ç´¢æœºåˆ¶ï¼›åŒæ—¶LLMsé¢„è®­ç»ƒä»¥è‡ªç„¶è¯­è¨€æ•°æ®ä¸ºä¸»ï¼Œå¯¹å·¥å…·ç›¸å…³åŠŸèƒ½çš„å†…åœ¨è®¤çŸ¥æœ‰é™ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„å·¥å…·æè¿°åšå†³ç­–æ—¶è¡¨ç°æ¬ ä½³ã€‚å½“å·¥å…·æ•°é‡å¢é•¿åˆ°æ•°ä¸‡çº§åˆ«ï¼Œç°æœ‰å·¥å…·æ£€ç´¢ä¸æ‰§è¡Œæ–¹æ³•åœ¨æ•ˆç‡å’Œæ‰©å±•æ€§ä¸Šä¹Ÿéš¾ä»¥åº”å¯¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå·¥å…·çŸ¥è¯†èå…¥æ¨¡å‹å‚æ•°çš„èŒƒå¼é©æ–°  
ToolGen æŠŠæ¯ä¸ªå·¥å…·è¡¨ç¤ºä¸º LLM è¯æ±‡è¡¨ä¸­å”¯ä¸€çš„è™šæ‹Ÿ tokenï¼Œå°†å·¥å…·çŸ¥è¯†ç›´æ¥æ•´åˆåˆ° LLM å‚æ•°é‡Œã€‚ä¸å†ä¾èµ–å¤–éƒ¨æ£€ç´¢æ¨¡å—ï¼Œè®©æ¨¡å‹èƒ½æŠŠå·¥å…·è°ƒç”¨å’Œå‚æ•°ç”Ÿæˆéƒ½çº³å…¥â€œä¸‹ä¸€ä¸ª token é¢„æµ‹â€èƒ½åŠ›ä¸­ï¼ŒæŠŠå·¥å…·è°ƒç”¨ä¸è¯­è¨€ç”Ÿæˆæ— ç¼èåˆï¼Œä»æ ¹æœ¬ä¸ŠæŠŠå·¥å…·æ£€ç´¢è½¬åŒ–ä¸ºç”Ÿæˆè¿‡ç¨‹ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹å®ç°é«˜æ•ˆå·¥å…·å­¦ä¹   
ToolGen åŸºäºé¢„è®­ç»ƒ LLM è®¾è®¡äº†ä¸‰é˜¶æ®µè®­ç»ƒï¼šå·¥å…·è®°å¿†é˜¶æ®µè®©æ¨¡å‹æŠŠè™šæ‹Ÿå·¥å…· token å’Œå¯¹åº”æ–‡æ¡£å…³è”ï¼›æ£€ç´¢è®­ç»ƒé˜¶æ®µè®©æ¨¡å‹å­¦ä¹ æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆç›¸å…³å·¥å…· tokenï¼›ç«¯åˆ°ç«¯æ™ºèƒ½ä½“è°ƒä¼˜é˜¶æ®µåˆ™è®­ç»ƒæ¨¡å‹ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“ï¼Œç”Ÿæˆè®¡åˆ’ã€å·¥å…·åŠå¯¹åº”å‚æ•°æ¥å®Œæˆä»»åŠ¡ï¼Œè¿˜èƒ½é€šè¿‡å·¥å…·è°ƒç”¨å’Œå¤–éƒ¨åé¦ˆé«˜æ•ˆå¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è¶…è¿‡ 47,000 ä¸ªå·¥å…·çš„å®éªŒä¸­ï¼ŒToolGen åœ¨å·¥å…·æ£€ç´¢ä»»åŠ¡ï¼ˆä¸ºæŸ¥è¯¢åŒ¹é…æ­£ç¡®å·¥å…·ï¼‰å’ŒåŸºäº LLM çš„æ™ºèƒ½ä½“ä»»åŠ¡ï¼ˆå®Œæˆæ¶‰åŠçœŸå® API è°ƒç”¨çš„å¤æ‚ä»»åŠ¡ï¼‰ä¸Šéƒ½å±•ç°ä¼˜åŠ¿ï¼šä¸ä¸»æµå·¥å…·æ£€ç´¢æ–¹æ³•æ€§èƒ½ç›¸å½“ï¼Œä½†æˆæœ¬æ›´ä½ã€æ•ˆç‡æ›´é«˜ï¼›åŒæ—¶è¶…è¶Šä¼ ç»Ÿå·¥å…·å­¦ä¹ èŒƒå¼ï¼ŒéªŒè¯äº†å…¶åœ¨å¤§è§„æ¨¡å·¥å…·åº“åœºæ™¯ä¸‹çš„æ½œåŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°è§’åº¦ï¼šå°†æ£€ç´¢ä¸ç”Ÿæˆç»Ÿä¸€ä¸ºå•ä¸€ä»»åŠ¡ï¼Œä¸º AI æ™ºèƒ½ä½“é€‚åº”å¤šé¢†åŸŸå·¥å…·å¼€è¾Ÿæ–°æ–¹å‘ï¼Œå¯ç¤ºåç»­åœ¨å·¥å…·äº¤äº’ç±»ä»»åŠ¡ä¸­æ€è€ƒâ€œèƒ½å¦ç”¨ç”Ÿæˆå¼æ€è·¯æ›¿ä»£ä¼ ç»Ÿæ‹†åˆ†å¼æµç¨‹â€ã€‚  
2. æŠ€æœ¯æ•´åˆè§’åº¦ï¼šä¸ºæ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰ã€å¼ºåŒ–å­¦ä¹ ç­‰å…ˆè¿›æŠ€æœ¯ä¸å·¥å…·è°ƒç”¨èƒ½åŠ›çš„ç»“åˆæä¾›äº†æ–°å¯èƒ½ï¼Œåœ¨æ‰©å±• LLM å®ç”¨èƒ½åŠ›æ—¶ï¼Œå¯å‚è€ƒè¿™ç§â€œæŠŠå·¥å…·èƒ½åŠ›å†…åŒ–ä¸ºæ¨¡å‹ç”Ÿæˆèƒ½åŠ›â€çš„æ€è·¯ã€‚  
3. å·¥ç¨‹è½åœ°è§’åº¦ï¼šä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ä¸ºå¤§è§„æ¨¡å·¥å…·çš„é«˜æ•ˆæ¥å…¥å’Œå­¦ä¹ æä¾›äº†å¯è½åœ°çš„è·¯å¾„ï¼Œå¯¹éœ€è¦æ•´åˆå¤§é‡å¤–éƒ¨å·¥å…·çš„æ™ºèƒ½ä½“ç±»äº§å“ç ”å‘æœ‰å‚è€ƒä»·å€¼ã€‚

## citi--enhancing-tool-utilizing-ability-in-large-language-models-without-sacrificing-general-performance
### Abstract
Tool learning enables the Large Language Models (LLMs) to interact with the external environment by invoking tools, enriching the accuracy and capability scope of LLMs. However, previous works predominantly focus on improving model's tool-utilizing accuracy and the ability to generalize to new, unseen tools, excessively forcing LLMs to adjust specific tool-invoking pattern without considering the harm to model's general performance. This deviates from the actual applications and original intention of integrating tools to enhance model. To tackle this problem, we dissect the capability trade-offs by examining the hidden representation changes and the gradient-based importance score of model's components. Based on the analysis result, we propose a Component Importance-based Tool-utilizing ability Injection method (CITI). According to the gradient-based importance score of different components, it alleviates the capability conflicts caused by fine-tuning process by applying distinct training strategies to different components. CITI applies Mixture-Of-LoRA (MOLoRA) for important components. Meanwhile, it fine-tunes the parameters of few components deemed less important in the backbone of the LLM, while keeping other parameters frozen. CITI can effectively enhance the model's tool-utilizing capability without excessively compromising its general performance. Experimental results demonstrate that our approach achieves outstanding performance across a range of evaluation metrics.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CITIï¼šå¤§æ¨¡å‹å·¥å…·èƒ½åŠ›å¢å¼ºä¸é€šç”¨æ€§èƒ½å…¼é¡¾çš„æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å·¥å…·å­¦ä¹ èƒ½ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ï¼Œæ‹“å±•èƒ½åŠ›è¾¹ç•Œï¼Œä½†ç°æœ‰å·¥ä½œå¤šèšç„¦å·¥å…·è°ƒç”¨ç²¾åº¦ä¸å¯¹æ–°å·¥å…·æ³›åŒ–æ€§ï¼Œè¿‡åº¦è°ƒæ•´å·¥å…·è°ƒç”¨æ¨¡å¼å´å¿½è§†å¯¹æ¨¡å‹é€šç”¨æ€§èƒ½çš„æŸå®³ï¼Œå‡ºç°èƒ½åŠ›æƒè¡¡å›°å¢ƒï¼ˆå¦‚å…¨é‡å¾®è°ƒå·¥å…·æ•°æ®ä¼šä¸¥é‡å‰Šå¼±æ¨¡å‹é€šç”¨è®¤çŸ¥èƒ½åŠ›ï¼‰ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨å¢å¼ºå·¥å…·åˆ©ç”¨èƒ½åŠ›åŒæ—¶ä¿ç•™é€šç”¨æ€§èƒ½æˆå…³é”®é—®é¢˜ï¼Œæœ¬æ–‡å°±æ­¤å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šèƒ½åŠ›æƒè¡¡å‰–æç»´åº¦åˆ›æ–°  
ä»éšè—è¡¨ç¤ºå’Œæ¨¡å‹ç»„ä»¶ä¸¤æ–¹é¢å‰–æèƒ½åŠ›æƒè¡¡ã€‚éšè—è¡¨ç¤ºç»´åº¦æå‡ºâ€œIncremental Change of Capability (ICC)â€é‡åŒ–å¾®è°ƒä¸­éšè—è¡¨ç¤ºå˜åŒ–ï¼Œå‘ç°å·¥å…·ç›¸å…³ä¸æ— å…³ä»»åŠ¡åœ¨éšè—çŠ¶æ€ç©ºé—´å¢é‡å‘ˆâ€œåŒå‘åç§»â€ï¼›ç»„ä»¶ç»´åº¦è®¡ç®—çº¿æ€§æ¨¡å—åŸºäºæ¢¯åº¦çš„é‡è¦æ€§åˆ†æ•°æ’åï¼Œæ­ç¤ºä¸åŒèƒ½åŠ›ä¸‹é‡è¦ç»„ä»¶å¯¹èƒ½åŠ›è¡¨è¾¾å…³é”®åº¦åŠä¸åŒèƒ½åŠ›é‡è¦æ€§æ’åé«˜åº¦ä¸€è‡´ï¼Œä¸”å¾®è°ƒä¸åŒé‡è¦æ€§ç»„ä»¶å¯¹é€šç”¨æ€§èƒ½å½±å“ä¸åŒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCITIæ–¹æ³•è®¾è®¡  
æå‡ºåŸºäºç»„ä»¶é‡è¦æ€§çš„å·¥å…·èƒ½åŠ›æ³¨å…¥æ–¹æ³•ï¼ˆCITIï¼‰ã€‚æ­¥éª¤ä¸Šï¼Œå…ˆè¯†åˆ«æ¨¡å‹æ‰€æœ‰çº¿æ€§ç»„ä»¶æ¢¯åº¦é‡è¦æ€§åˆ†æ•°ï¼›å¯¹é‡è¦çº¿æ€§ç»„ä»¶ï¼Œé›†æˆMixture - Of - LoRA (MOLoRA)é€‚é…å™¨å¸æ”¶å·¥å…·è°ƒç”¨çŸ¥è¯†ï¼Œè®¾è®¡è·¯ç”±ç½‘ç»œåˆ†ç¦»å·¥å…·ç›¸å…³ä¸æ— å…³è¾“å…¥ä»¥å‡è½»å¯¹æ¨¡å‹ä¸»å¹²å½±å“ï¼›å¯¹ä¸é‡è¦çº¿æ€§ç»„ä»¶ï¼Œé‡‡ç”¨å…¨å‚æ•°å¾®è°ƒåˆ©ç”¨æ›´å¤šå‚æ•°ã€‚è®­ç»ƒåˆ†ä¸‰é˜¶æ®µï¼šè·¯ç”±é¢„è®­ç»ƒï¼ˆæ•™è·¯ç”±ç½‘ç»œåŒºåˆ†è¾“å…¥ç±»å‹ï¼‰ã€MOLoRAæ”¹è¿›ï¼ˆå¾®è°ƒé€‚é…å™¨å†»ç»“ä¸»å¹²ï¼‰ã€ä¸é‡è¦ç»„ä»¶ä¼˜åŒ–ï¼ˆå¾®è°ƒå°‘é‡ä¸»å¹²ä¸­ä¸é‡è¦ç»„ä»¶ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å·¥å…·å­¦ä¹ æ•°æ®é›†API - Bankå’ŒToolAlpacaä¸Šå¼€å±•å¤§é‡å®éªŒï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ•°å­¦ã€ä»£ç ç”Ÿæˆã€äº‹å®çŸ¥è¯†ã€æŒ‡ä»¤éµå¾ªç­‰é€šç”¨èƒ½åŠ›ä¿ç•™æƒ…å†µã€‚ç»“æœæ˜¾ç¤ºæ–¹æ³•æœ‰æ•ˆï¼šåœ¨API - Bankæ•°æ®é›†ï¼Œé€šç”¨æ€§èƒ½æ¯”LoRAé«˜7.59%ã€æ¯”å…¨é‡å¾®è°ƒé«˜31.95%ï¼›åœ¨ToolAlpacaæ•°æ®é›†ï¼Œæ¯”LoRAé«˜8.96%ã€æ¯”å…¨é‡å¾®è°ƒé«˜29.03%ï¼Œåœ¨å·¥å…·åˆ©ç”¨ä»»åŠ¡å’Œé€šç”¨ä»»åŠ¡æ€§èƒ½é—´å®ç°è‰¯å¥½å¹³è¡¡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åˆ†æè§†è§’å¯å€Ÿé‰´ï¼šä»éšè—è¡¨ç¤ºå’Œç»„ä»¶ç»´åº¦æ·±å…¥åˆ†æå¤§æ¨¡å‹èƒ½åŠ›æƒè¡¡ï¼Œä¸ºç†è§£æ¨¡å‹å¾®è°ƒä¸­èƒ½åŠ›å˜åŒ–æœºåˆ¶æä¾›æ–°è§†è§’ï¼Œåç»­ç ”ç©¶å¤§æ¨¡å‹èƒ½åŠ›äº¤äº’ç­‰å¯å‚è€ƒè¯¥åˆ†æç»´åº¦ã€‚
2. æ–¹æ³•è®¾è®¡æ€è·¯ï¼šä¾æ®ç»„ä»¶é‡è¦æ€§å·®å¼‚æ–½åŠ ä¸åŒè®­ç»ƒç­–ç•¥ï¼Œå¹³è¡¡ç‰¹å®šèƒ½åŠ›å¢å¼ºä¸é€šç”¨èƒ½åŠ›ä¿ç•™ï¼Œä¸ºå¤§æ¨¡å‹å¤šèƒ½åŠ›ååŒä¼˜åŒ–æä¾›æ–°æ€è·¯ï¼Œåœ¨éœ€å¢å¼ºæŸä¸“é¡¹èƒ½åŠ›åˆè¦ä¿é€šç”¨æ€§èƒ½åœºæ™¯ï¼ˆå¦‚å¤šæ¨¡æ€èƒ½åŠ›é›†æˆç­‰ï¼‰å¯å€Ÿé‰´æ­¤åˆ†å±‚ç­–ç•¥æ€æƒ³ã€‚
3. å®éªŒéªŒè¯ç»´åº¦ï¼šåŒæ—¶éªŒè¯å·¥å…·ä»»åŠ¡æ€§èƒ½å’Œå¤šç±»é€šç”¨ä»»åŠ¡æ€§èƒ½ï¼Œå…¨é¢è¯„ä¼°æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œåç»­ç›¸å…³æ–¹æ³•è¯„ä¼°å¯å‚è€ƒæ­¤å¤šç»´åº¦éªŒè¯æ¨¡å¼ï¼Œç¡®ä¿æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­æ›´å¯é ã€‚

## toolace--winning-the-points-of-llm-function-calling
### Abstract
Function calling significantly extends the application boundary of large language models, where high-quality and diverse training data is critical for unlocking this capability. However, real function-calling data is quite challenging to collect and annotate, while synthetic data generated by existing pipelines tends to lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data, even with only 8B parameters, achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a subset of the data are publicly available at https://huggingface.co/Team-ACE.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolACEï¼šçªç ´å¤§æ¨¡å‹å·¥å…·è°ƒç”¨èƒ½åŠ›çš„å…³é”®åˆ©å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é…å¤‡å¤–éƒ¨å·¥å…·èƒ½æå¤§æ‹“å±•å…¶è§£å†³å¤æ‚ç°å®ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå·¥å…·è°ƒç”¨ï¼ˆFunction callingï¼‰åœ¨å…¶ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡ä¸”å¤šæ ·çš„è®­ç»ƒæ•°æ®æ˜¯è§£é”è¯¥èƒ½åŠ›çš„å…³é”®ï¼Œç°å®ä¸­æ”¶é›†å’Œæ ‡æ³¨å·¥å…·è°ƒç”¨æ•°æ®éš¾åº¦å¤§ï¼Œç°æœ‰æµç¨‹ç”Ÿæˆçš„åˆæˆæ•°æ®åˆå­˜åœ¨è¦†ç›–ä¸è¶³ä¸ç²¾åº¦æ¬ ç¼ºçš„é—®é¢˜ã€‚åŒæ—¶ï¼Œå®é™…åº”ç”¨é‡Œå·¥å…·è°ƒç”¨åœºæ™¯å¤šæ ·å¤æ‚ï¼Œå½“å‰å·¥å…·å¢å¼ºå‹LLMå¤šèšç„¦ç®€å•ä»»åŠ¡ï¼Œåœ¨å¤šæ ·æ€§å’Œå¤æ‚æ€§ä¸Šå­˜åœ¨å±€é™ï¼Œä¸”å‡½æ•°è°ƒç”¨æ‰§è¡Œå¯¹æ•°æ®è´¨é‡å’Œå‡†ç¡®æ€§é«˜åº¦ä¾èµ–ï¼Œç°æœ‰ç®€å•ç”Ÿæˆæµç¨‹éš¾ä»¥åº”å¯¹æ•°æ®æ„ˆå‘å¤šæ ·å¤æ‚çš„æƒ…å†µã€‚å› æ­¤ï¼Œæ‰“é€ èƒ½ç”Ÿæˆå‡†ç¡®ã€å¤æ‚ã€å¤šæ ·å·¥å…·å­¦ä¹ æ•°æ®çš„è‡ªåŠ¨åŒ–æµç¨‹è¿«åœ¨çœ‰ç«ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå·¥å…·è‡ªæ¼”åŒ–åˆæˆï¼ˆTool Self - Evolution Synthesis, TSSï¼‰
ToolACEä¸å†ä¾èµ–å…¬å¼€APIï¼Œé‡‡ç”¨TSSæ–¹æ³•ç”Ÿæˆå¤šé¢†åŸŸã€å…·å¤‡å¤šæ ·æ•°æ®ç±»å‹å’Œçº¦æŸçš„APIã€‚è¯¥æ–¹æ³•é€šè¿‡â€œç‰©ç§å½¢æˆ - é€‚åº” - æ¼”åŒ–â€è¿‡ç¨‹ï¼Œä»é¢„è®­ç»ƒæ•°æ®èµ·æ­¥ä¿éšœå…¨é¢è¦†ç›–ï¼Œç»è¿­ä»£è‡ªæ¼”åŒ–ä¸æŒç»­æ›´æ–°ï¼Œæ‰©å……APIæ± å¤šæ ·æ€§ï¼Œæ„å»ºå‡ºåŒ…å«26507ä¸ªAPIçš„å…¨é¢APIæ± ï¼Œåœ¨æ•°é‡å’Œé¢†åŸŸè¦†ç›–ä¸Šè¶…è¶Šå…¶ä»–ä»£è¡¨æ€§å·¥å…·å¢å¼ºå‹LLMï¼Œä¸ºæ›´å¤æ‚çš„æ•°æ®ç”Ÿæˆæä¾›æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªå¼•å¯¼å¯¹è¯ç”Ÿæˆï¼ˆSelf - Guided Dialog Generation, SDGï¼‰
ä¸ºè®©æŒ‡ä»¤è·Ÿéšæ•°æ®å…·å¤‡è¶³å¤Ÿå¤æ‚åº¦ä»¥åŸ¹å…»å·¥å…·è°ƒç”¨æŠ€èƒ½ï¼Œæå‡ºSDGæµç¨‹ã€‚è®©LLMå……å½“å¤æ‚åº¦è¯„ä¼°å™¨æ¥è°ƒèŠ‚æ•°æ®å¤æ‚åº¦ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“äº¤äº’ï¼Œéµå¾ªè‡ªå¼•å¯¼å¤æ‚åŒ–ç­–ç•¥ç”Ÿæˆå››ç§ç±»å‹çš„å·¥å…·è°ƒç”¨æ•°æ®ï¼Œä½¿ç”Ÿæˆæ•°æ®å¤æ‚åº¦ç•¥è¶…æ¨¡å‹å½“å‰èƒ½åŠ›ä»¥ä¿ƒè¿›æ›´æœ‰æ•ˆå­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŒå±‚éªŒè¯ç³»ç»Ÿï¼ˆDual - Layer Verification, DLVï¼‰
æ•°æ®å‡†ç¡®æ€§æ˜¯å·¥å…·å¢å¼ºå‹LLMæœ‰æ•ˆæ€§çš„åŸºç¡€ï¼ŒToolACEé‡‡ç”¨DLVç³»ç»Ÿï¼Œæ•´åˆåŸºäºè§„åˆ™å’ŒåŸºäºæ¨¡å‹çš„æ£€æŸ¥å™¨ï¼Œä¿éšœåˆæˆæ•°æ®çš„å¯æ‰§è¡Œæ€§ä¸ä¸€è‡´æ€§ï¼Œä»è§„åˆ™æ£€æŸ¥ï¼ˆå¦‚ç±»å‹æ£€æŸ¥ã€å€¼çº¦æŸã€æ ¼å¼éªŒè¯ç­‰ï¼‰å’Œæ¨¡å‹æ£€æŸ¥ç­‰å±‚é¢ä¸¥æ ¼æŠŠæ§æ•°æ®è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Berkeley Function - Calling Leaderboardç­‰å¹¿æ³›é‡‡ç”¨çš„åŸºå‡†æµ‹è¯•ï¼ˆå¦‚BFCLå’ŒAPIBankï¼‰ä¸Šå¼€å±•å®éªŒï¼Œä»…ç”¨8Bå‚æ•°ã€åœ¨ToolACEåˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰å¼€æºLLMï¼Œä¸”èƒ½ä¸æœ€å…ˆè¿›çš„GPT - 4æ¨¡å‹ç›¸åª²ç¾ï¼Œæœ‰åŠ›è¯æ˜äº†ToolACEç”Ÿæˆæ•°æ®å¯¹æå‡å¤§æ¨¡å‹å·¥å…·è°ƒç”¨èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆ pipeline æ„å»ºæ€è·¯ï¼šToolACEæå‡ºçš„åŒ…å«å·¥å…·è‡ªæ¼”åŒ–åˆæˆã€è‡ªå¼•å¯¼å¯¹è¯ç”Ÿæˆã€åŒå±‚éªŒè¯æ¨¡å—çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œä¸ºè§£å†³é¢†åŸŸå†…æ•°æ®ç”Ÿæˆéš¾é¢˜æä¾›äº†å®Œæ•´çš„ pipeline å‚è€ƒï¼Œåç»­åœ¨å…¶ä»–éœ€åˆæˆæ•°æ®èµ‹èƒ½æ¨¡å‹èƒ½åŠ›çš„åœºæ™¯ä¸­ï¼Œå¯å€Ÿé‰´è¿™ç§æ¨¡å—åŒ–ä¸”ç¯ç¯ç›¸æ‰£çš„è‡ªåŠ¨åŒ–æ„å»ºæ€è·¯ã€‚
2. å¤æ‚åº¦å¼•å¯¼ä¸å¤šæ™ºèƒ½ä½“åä½œï¼šåˆ©ç”¨LLMè‡ªèº«ä½œä¸ºå¤æ‚åº¦è¯„ä¼°å™¨ï¼Œç»“åˆå¤šæ™ºèƒ½ä½“äº¤äº’ç”Ÿæˆåˆé€‚å¤æ‚åº¦æ•°æ®çš„æ–¹å¼ï¼Œä¸ºç”Ÿæˆæ»¡è¶³æ¨¡å‹èƒ½åŠ›å‘å±•éœ€æ±‚ï¼ˆç•¥è¶…å½“å‰èƒ½åŠ›ä»¥ä¿ƒè¿›å­¦ä¹ ï¼‰çš„æ•°æ®æä¾›äº†åˆ›æ–°èŒƒå¼ï¼Œåœ¨éœ€è¦æ§åˆ¶æ•°æ®éš¾åº¦æ¢¯åº¦çš„è®­ç»ƒæ•°æ®ç”Ÿæˆä»»åŠ¡ä¸­å€¼å¾—å‚è€ƒã€‚
3. åŒå±‚éªŒè¯ä¿éšœæ•°æ®è´¨é‡ï¼šå°†åŸºäºè§„åˆ™å’ŒåŸºäºæ¨¡å‹çš„æ£€æŸ¥ç»“åˆæ¥ä¿éšœæ•°æ®å‡†ç¡®æ€§çš„åšæ³•ï¼Œåœ¨å¯¹æ•°æ®è´¨é‡è¦æ±‚é«˜çš„AIä»»åŠ¡æ•°æ®å¤„ç†ç¯èŠ‚ï¼Œæ¯”å¦‚åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸAIåº”ç”¨çš„æ•°æ®æ ¡éªŒï¼Œèƒ½å€Ÿé‰´è¿™ç§å¤šå±‚çº§ã€å¤šç»´åº¦çš„éªŒè¯æœºåˆ¶æ¥æå‡æ•°æ®å¯é æ€§ã€‚ 
```

## learning-to-ask--when-llm-agents-meet-unclear-instruction
### Abstract
Equipped with the capability to call functions, modern large language models (LLMs) can leverage external tools for addressing a range of tasks unattainable through language skills alone. However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLMs but also on precise user instructions, which often cannot be ensured in the real world. To evaluate the performance of LLMs tool-use under imperfect instructions, we meticulously examine the real-world instructions queried from users, analyze the error patterns, and build a challenging tool-use benchmark called Noisy ToolBench (NoisyToolBench). We find that due to the next-token prediction training objective, LLMs tend to arbitrarily generate the missed argument, which may lead to hallucinations and risks. To address this issue, we propose a novel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to users whenever they encounter obstacles due to unclear instructions. Moreover, to reduce the manual labor involved in user-LLM interaction and assess LLMs performance in tool utilization from both accuracy and efficiency perspectives, we design an automated evaluation tool named ToolEvaluator. Our experiments demonstrate that the AwN significantly outperforms existing frameworks for tool learning in the NoisyToolBench. We will release all related code and datasets to support future research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å½“å¤§æ¨¡å‹æ™ºèƒ½ä½“é‡åˆ°æ¨¡ç³ŠæŒ‡ä»¤ï¼šå­¦ä¼šä¸»åŠ¨æé—®çš„å…³é”®

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å·¥å…·è°ƒç”¨èƒ½åŠ›èƒ½å®Œæˆè¯¸å¤šä»…é è¯­è¨€æŠ€èƒ½æ— æ³•è¾¾æˆçš„ä»»åŠ¡ï¼Œä½†å·¥å…·çš„æœ‰æ•ˆæ‰§è¡Œä¸ä»…ä¾èµ–LLMsè‡ªèº«èƒ½åŠ›ï¼Œè¿˜éœ€è¦ç²¾ç¡®çš„ç”¨æˆ·æŒ‡ä»¤ï¼Œè€Œç°å®ä¸­ç”¨æˆ·æŒ‡ä»¤å¾€å¾€å¹¶ä¸å®Œå–„ã€‚ç°æœ‰æ¡†æ¶å’ŒåŸºå‡†å¸¸å‡è®¾ç”¨æˆ·æŒ‡ä»¤æ˜ç¡®æ— æ­§ä¹‰ï¼Œä¸çœŸå®åœºæ™¯ä¸ç¬¦ã€‚LLMså› next - tokené¢„æµ‹çš„è®­ç»ƒç›®æ ‡ï¼Œåœ¨æŒ‡ä»¤ä¿¡æ¯ç¼ºå¤±æ—¶æ˜“éšæ„ç”Ÿæˆå‚æ•°ï¼Œå¼•å‘å¹»è§‰ç­‰é£é™©ï¼›ä¸”ä»»åŠ¡å¤æ‚æ—¶å¤šè½®APIè°ƒç”¨æ˜“å‡ºé”™ï¼Œè€ŒLLMsåœ¨æ¨¡ç³ŠæŒ‡ä»¤ä¸‹çš„å·¥å…·ä½¿ç”¨ç ”ç©¶è¾ƒå°‘ã€‚å› æ­¤ï¼Œæœ¬æ–‡èšç„¦äºè§£å†³LLMsåœ¨ä¸æ¸…æ™°æŒ‡ä»¤ä¸‹çš„å·¥å…·ä½¿ç”¨é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºNoisyToolBenchåŸºå‡†
å¯¹çœŸå®ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œå°†æŒ‡ä»¤é—®é¢˜åˆ†ä¸ºä¿¡æ¯ç¼ºå¤±ã€æŒ‡ä»£æ¨¡ç³Šã€åŒ…å«é”™è¯¯ã€å› å·¥å…·é™åˆ¶ä¸å¯æ‰§è¡Œç­‰ç±»åˆ«ã€‚åŸºäºæ­¤æ„å»ºNoisyToolBenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMsæ£€æµ‹ç”¨æˆ·æŸ¥è¯¢æ¨¡ç³Šæ€§å¹¶æå‡ºæ¾„æ¸…é—®é¢˜çš„èƒ½åŠ›ï¼ŒåŒ…å«APIé›†åˆã€æ¨¡ç³ŠæŸ¥è¯¢ã€é¢„æœŸæ¾„æ¸…é—®é¢˜åŠå¯¹åº”å›å¤ç­‰å†…å®¹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºAsk - when - Neededï¼ˆAwNï¼‰æ¡†æ¶
æ ¸å¿ƒæ€è·¯æ˜¯ä¿ƒä½¿LLMsåœ¨æŒ‡ä»¤æ‰§è¡Œé‡ä¸ç¡®å®šæ—¶ï¼Œä¸»åŠ¨å‘ç”¨æˆ·æé—®å¯»æ±‚æ¾„æ¸…ã€‚é€šè¿‡åœ¨è¿‡ç¨‹ä¸­ä¿ƒè¿›å¯¹è¯ï¼Œç¡®ä¿å‡½æ•°è°ƒç”¨çš„å‡†ç¡®æ€§ï¼Œé¿å…å› æŒ‡ä»¤æ¨¡ç³Šéšæ„ç”Ÿæˆå‚æ•°å¯¼è‡´çš„é—®é¢˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡ToolEvaluatorè‡ªåŠ¨è¯„ä¼°å·¥å…·
ä»å‡†ç¡®æ€§å’Œæ•ˆç‡è§’åº¦è®¾è®¡è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®æ€§åŒ…æ‹¬æåˆé€‚æ¾„æ¸…é—®é¢˜ã€æ‰§è¡Œæ­£ç¡®å‡½æ•°è°ƒç”¨ã€ç»™å‡ºæ»¡è¶³éœ€æ±‚æœ€ç»ˆå›å¤çš„èƒ½åŠ›ï¼›æ•ˆç‡åŒ…æ‹¬å†—ä½™é—®é¢˜æ•°å’Œå®ŒæˆæŒ‡ä»¤è¡ŒåŠ¨æ•°ï¼‰ã€‚åˆ©ç”¨GPT - 4oèƒ½åŠ›è®¾è®¡ToolEvaluatorï¼Œè‡ªåŠ¨ä»£ç†ç”¨æˆ·ä¸LLMsäº¤äº’å¹¶è¯„ä¼°å·¥å…·ä½¿ç”¨è¡¨ç°ï¼Œå‡å°‘äººå·¥äº¤äº’å’ŒéªŒè¯æˆæœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨6ä¸ªLLMså’Œ2ä¸ªå·¥å…·ä½¿ç”¨æ¡†æ¶ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAwNæ¡†æ¶åœ¨NoisyToolBenchä¸­æ˜¾è‘—ä¼˜äºç°æœ‰å·¥å…·å­¦ä¹ åŸºçº¿æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆæå‡LLMsåœ¨æ¨¡ç³ŠæŒ‡ä»¤ä¸‹çš„å·¥å…·ä½¿ç”¨æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹çœŸå®åœºæ™¯ç”¨æˆ·æŒ‡ä»¤çš„ç³»ç»Ÿåˆ†ææ–¹æ³•ï¼Œä¸ºç†è§£LLMså·¥å…·ä½¿ç”¨çš„ç°å®æŒ‘æˆ˜æä¾›äº†å…¨é¢è§†è§’ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å¯¹çœŸå®åœºæ™¯é—®é¢˜çš„æ‹†è§£åˆ†ç±»æ€è·¯ã€‚
2. æ„å»ºé’ˆå¯¹æ¨¡ç³ŠæŒ‡ä»¤å·¥å…·ä½¿ç”¨è¯„ä¼°åŸºå‡†çš„æ–¹å¼ï¼Œä¸ºè¯¥é¢†åŸŸåˆ›å»ºäº†æ–°çš„è¯„ä¼°æ ‡å‡†å’Œæ•°æ®é›†èµ„æºï¼Œæ¨åŠ¨é¢†åŸŸå†…æ¨¡å‹èƒ½åŠ›å¯¹æ¯”å’Œè¿›æ­¥ã€‚
3. ä¸»åŠ¨æé—®æ¡†æ¶AwNä¸ºè§£å†³æŒ‡ä»¤æ¨¡ç³Šä¸‹çš„å·¥å…·ä½¿ç”¨é—®é¢˜æä¾›äº†åˆ›æ–°èŒƒå¼ï¼Œå¯å‘åç»­åœ¨äººæœºäº¤äº’å¼å·¥å…·ä½¿ç”¨æµç¨‹ä¼˜åŒ–æ–¹é¢çš„ç ”ç©¶ã€‚
4. è‡ªåŠ¨è¯„ä¼°å·¥å…·ToolEvaluatorçš„è®¾è®¡æ€è·¯ï¼Œä¸ºé™ä½LLMså·¥å…·ä½¿ç”¨è¯„ä¼°çš„äººåŠ›æˆæœ¬ã€æå‡è¯„ä¼°æ•ˆç‡æä¾›äº†èŒƒä¾‹ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€å¤šè½®äº¤äº’è¯„ä¼°çš„åœºæ™¯ã€‚

## metatool--facilitating-large-language-models-to-master-tools-with-meta-task-augmentation
### Abstract
Utilizing tools with Large Language Models (LLMs) is essential for grounding AI agents in real-world applications. The prevailing approach involves few-shot prompting with demonstrations or fine-tuning with expert annotations. However, mere in-context demonstrations may fail to cover sufficient knowledge for complex tools and tasks. Training on solution paths is also hindered by the high cost of expert annotations and generalizing to new tools. A core challenge of generalizable tool use lies in understanding the "meta", or fundamental natures of tools that are transferable across tasks, such as causality and constraints. In this paper, we present MetaTool, a novel tool learning methodology designed to generalize across any reusable toolset. Our approach incorporates a self-supervised augmentation technique derived from a series of meta-tasks. This involves predicting masked elements in the tool execution process. The self-supervised procedure enables scalable generation of high-quality QA data, which is handy for supervising tool understanding. By incorporating meta-task data into task-oriented training, our method significantly enhances the performance of open-source LLMs, achieving results comparable to ChatGPT in both tool-based planning and chatting scenarios. Through large-scale instruction tuning, the MetaTool model demonstrates impressive zero-shot generalizability on new tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MetaToolï¼šè®©å¤§è¯­è¨€æ¨¡å‹æŒæ¡å·¥å…·çš„å…ƒä»»åŠ¡å¢å¼ºæ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®åº”ç”¨ä¸­ï¼Œè®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆå·¥å…·æ˜¯å®ç°AIæ™ºèƒ½ä½“è½åœ°çš„å…³é”®ã€‚å½“å‰ä¸»æµæ–¹æ³•æ˜¯é€šè¿‡å°‘é‡ç¤ºä¾‹æç¤ºï¼ˆin - context promptingï¼‰æˆ–ä¸“å®¶æ ‡æ³¨å¾®è°ƒï¼Œä½†å­˜åœ¨å±€é™ï¼šå°‘é‡ç¤ºä¾‹éš¾ä»¥è¦†ç›–å¤æ‚å·¥å…·å’Œä»»åŠ¡çš„æ‰€æœ‰çŸ¥è¯†ï¼Œä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜ä¸”éš¾æ³›åŒ–åˆ°æ–°å·¥å…·ã€‚é€šç”¨å·¥å…·ä½¿ç”¨çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯ç†è§£å·¥å…·å¯è·¨ä»»åŠ¡è¿ç§»çš„â€œå…ƒâ€æœ¬è´¨ï¼ˆå¦‚å› æœã€çº¦æŸï¼‰ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½è®©LLMsæ³›åŒ–æŒæ¡å·¥å…·çš„æ–°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMetaToolå·¥å…·å­¦ä¹ æ–¹æ³•è®º
MetaToolæ—¨åœ¨è®©LLMså¯¹å¯å¤ç”¨å·¥å…·é›†å®ç°æ³›åŒ–ã€‚å®ƒåŸºäºä»»åŠ¡æ— å…³çš„å·¥å…·ç†è§£ï¼Œæ—¢åŠ©åŠ›å¤æ‚å·¥å…·æŒæ¡ï¼Œä¹Ÿæ”¯æŒæœªè§è¿‡å·¥å…·çš„æ³›åŒ–ï¼Œçªç ´äº†ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯å’Œæ–°å·¥å…·æ³›åŒ–ä¸Šçš„é™åˆ¶ï¼Œä»ä»»åŠ¡æ— å…³çŸ¥è¯†ä¸­è·å–å¯è¿ç§»çš„å·¥å…·ç†è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡è‡ªç›‘ç£å…ƒä»»åŠ¡é›†åˆ
ä»å·¥å…·æ‰§è¡Œè¿‡ç¨‹ä¸­æ‹†è§£å‡º6ä¸ªå…ƒä»»åŠ¡ï¼ŒåŸºäºæ— ç›‘ç£æˆ–è‡ªç©ï¼ˆself - playï¼‰çš„å·¥å…·æ‰§è¡Œæ„å»ºå…ƒä»»åŠ¡æ•°æ®ã€‚è¿™äº›å…ƒä»»åŠ¡åŒ…æ‹¬é¢„æµ‹å·¥å…·æ‰§è¡Œç»“æœï¼ˆEffectï¼‰ã€æ ¹æ®çŠ¶æ€ç¡®å®šåŠ¨ä½œï¼ˆDecision - makingï¼‰ã€ç”±åŠ¨ä½œå’Œç»“æœåæ¨åˆå§‹çŠ¶æ€ï¼ˆReversionï¼‰ã€åˆ¤æ–­åŠ¨ä½œæ˜¯å¦å¯æ‰§è¡Œï¼ˆInput Boundaryï¼‰ã€åˆ¤æ–­çŠ¶æ€æ˜¯å¦å¯è¾¾ï¼ˆOutput Boundaryï¼‰ã€åäº‹å®æ¨ç†é¢„æµ‹æ–°ç»“æœï¼ˆCounterfactï¼‰ã€‚å…ƒä»»åŠ¡ä»¥è‡ªç›‘ç£æ–¹å¼å®ç°é«˜è´¨é‡QAæ•°æ®çš„è§„æ¨¡åŒ–ç”Ÿæˆï¼Œæ— éœ€ä¸“å®¶æ ‡æ³¨å°±èƒ½ä¸ºå·¥å…·ç†è§£æä¾›ç›‘ç£æ•°æ®ï¼Œè¦†ç›–å·¥å…·å¢å¼ºå’Œå·¥å…·å¯¼å‘ç­‰åœºæ™¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå…ƒä»»åŠ¡å¢å¼ºè®­ç»ƒæ–¹å¼
å°†å…ƒä»»åŠ¡æ•°æ®èå…¥é¢å‘ä»»åŠ¡çš„è®­ç»ƒä¸­ï¼Œè®©LLMsåœ¨è§£å†³é—®é¢˜çš„åŒæ—¶åŠ æ·±å¯¹å·¥å…·çš„æŒæ¡ï¼Œæå‡å¼€æºLLMsåœ¨å·¥å…·ç±»ä»»åŠ¡ï¼ˆå¦‚åŸºäºå·¥å…·çš„è§„åˆ’å’Œå¯¹è¯åœºæ™¯ï¼‰ä¸­çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤æ‚å·¥å…·å¯¼å‘ä»»åŠ¡å’Œå·¥å…·å¢å¼ºåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaToolæ˜¾è‘—è¶…è¶Šä»…ç”¨æ ‡æ³¨è§£å†³æ–¹æ¡ˆè®­ç»ƒçš„æ¨¡å‹ï¼Œ8Bè§„æ¨¡çš„MetaToolæ¨¡å‹åœ¨æ€§èƒ½ä¸Šèƒ½ä¸ChatGPTç«äº‰ï¼Œåœ¨æ–°ä»»åŠ¡ä¸Šå±•ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œç¼©å°äº†å¼€æºæ¨¡å‹ä¸æœ€å…ˆè¿›LLMsä¹‹é—´çš„å·®è·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å·¥å…·å­¦ä¹ æ€è·¯ï¼šä»ä»»åŠ¡æ— å…³çš„å·¥å…·æœ¬è´¨ç†è§£å‡ºå‘æ„å»ºæ–¹æ³•ï¼Œä¸ºé€šç”¨AIæ™ºèƒ½ä½“å·¥å…·ä½¿ç”¨èƒ½åŠ›æå‡æä¾›äº†æ–°è§†è§’ï¼Œå¯å¯å‘åç»­æ¢ç´¢å·¥å…·æ³›åŒ–æ€§çš„ç ”ç©¶ã€‚
2. è‡ªç›‘ç£ä»»åŠ¡è®¾è®¡ï¼šé€šè¿‡æ‹†è§£å·¥å…·æ‰§è¡Œè¿‡ç¨‹è®¾è®¡å…ƒä»»åŠ¡ï¼Œå®ç°æ— ä¸“å®¶æ ‡æ³¨çš„æ•°æ®ç”Ÿæˆä¸å¢å¼ºï¼Œè¿™ç§è‡ªç›‘ç£æ€è·¯åœ¨æ•°æ®ç¨€ç¼ºæˆ–æ ‡æ³¨æ˜‚è´µåœºæ™¯ä¸‹å…·æœ‰å¹¿æ³›å€Ÿé‰´æ„ä¹‰ã€‚
3. è®­ç»ƒå¢å¼ºç­–ç•¥ï¼šå°†å…ƒä»»åŠ¡æ•°æ®ä¸ä»»åŠ¡å¯¼å‘è®­ç»ƒç»“åˆï¼Œå¹³è¡¡äº†ç‰¹å®šä»»åŠ¡è§£å†³ä¸é€šç”¨å·¥å…·ç†è§£ï¼Œä¸ºæå‡æ¨¡å‹åœ¨å·¥å…·ç±»ä»»åŠ¡æ€§èƒ½æä¾›äº†æœ‰æ•ˆèŒƒå¼ã€‚

## what-affects-the-stability-of-tool-learning--an-empirical-study-on-the-robustness-of-tool-learning-frameworks
### Abstract
Tool learning methods have enhanced the ability of large language models (LLMs) to interact with real-world applications. Many existing works fine-tune LLMs or design prompts to enable LLMs to select appropriate tools and correctly invoke them to meet user requirements. However, it is observed in previous works that the performance of tool learning varies from tasks, datasets, training settings, and algorithms. Without understanding the impact of these factors, it can lead to inconsistent results, inefficient model deployment, and suboptimal tool utilization, ultimately hindering the practical integration and scalability of LLMs in real-world scenarios. Therefore, in this paper, we explore the impact of both internal and external factors on the performance of tool learning frameworks. Through extensive experiments on two benchmark datasets, we find several insightful conclusions for future work, including the observation that LLMs can benefit significantly from increased trial and exploration. We believe our empirical study provides a new perspective for future tool learning research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å·¥å…·å­¦ä¹ ç¨³å®šæ€§å—ä½•å½±å“ï¼Ÿå·¥å…·å­¦ä¹ æ¡†æ¶é²æ£’æ€§çš„å®è¯ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å·¥å…·å­¦ä¹ æ–¹æ³•å¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç°å®ä¸–ç•Œåº”ç”¨äº¤äº’çš„èƒ½åŠ›ï¼Œä½†æ­¤å‰ç ”ç©¶å‘ç°å·¥å…·å­¦ä¹ æ€§èƒ½ä¼šå› ä»»åŠ¡ã€æ•°æ®é›†ã€è®­ç»ƒè®¾ç½®å’Œç®—æ³•ç­‰å› ç´ äº§ç”Ÿå·®å¼‚ã€‚è‹¥ä¸äº†è§£è¿™äº›å› ç´ å½±å“ï¼Œä¼šå¯¼è‡´ç»“æœä¸ä¸€è‡´ã€æ¨¡å‹éƒ¨ç½²ä½æ•ˆä¸å·¥å…·åˆ©ç”¨æ¬ ä½³ï¼Œé˜»ç¢LLMsåœ¨ç°å®åœºæ™¯çš„å®é™…æ•´åˆä¸æ‰©å±•ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ¢ç´¢å†…éƒ¨å’Œå¤–éƒ¨å› ç´ å¯¹å·¥å…·å­¦ä¹ æ¡†æ¶æ€§èƒ½çš„å½±å“ï¼Œä¸ºå·¥å…·å­¦ä¹ ç ”ç©¶æä¾›æ–°è§†è§’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå› ç´ åˆ†ç±»åˆ†æ
å°†å½±å“å·¥å…·å­¦ä¹ ç¨³å®šæ€§çš„å› ç´ åˆ†ä¸ºå†…éƒ¨å’Œå¤–éƒ¨ä¸¤ç±»ã€‚å†…éƒ¨å› ç´ ä»å¼€å‘è€…è§†è§’å‡ºå‘ï¼Œæ¶µç›–è§£ç æ¸©åº¦ã€æœ€å¤§æ¨ç†æ­¥æ•°ã€åŸºç¡€å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©ä»¥åŠä¸åŒå·¥å…·ä½¿ç”¨æ¡†æ¶çš„å½±å“ï¼›å¤–éƒ¨å› ç´ æ¶‰åŠæ¨¡å‹éƒ¨ç½²åä¸ç”¨æˆ·äº¤äº’æ—¶çš„æç¤ºå·¥ç¨‹ï¼ŒåŒ…æ‹¬ç”¨æˆ·æŸ¥è¯¢é£æ ¼ã€å·¥å…·ä½¿ç”¨æ¨¡å‹çš„å®šåˆ¶ç³»ç»Ÿæç¤ºä»¥åŠå€™é€‰å·¥å…·é›†ï¼ˆé€šè¿‡é‡æ–°æ’åºæˆ–æ‰©å±•è§„æ¨¡æ”¹å˜ï¼‰ ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç³»ç»Ÿæ€§å®è¯ç ”ç©¶
é¦–æ¬¡é’ˆå¯¹å·¥å…·ä½¿ç”¨æ¨¡å‹çš„ç¨³å®šæ€§å¼€å±•ç³»ç»Ÿæ€§å®è¯ç ”ç©¶ï¼Œåœ¨å¸¸ç”¨çš„ToolBenchåŸºå‡†æ•°æ®é›†å­é›†ä¸Šè¿›è¡Œå¤§é‡å®éªŒï¼Œä»å¤šç»´åº¦è¡¡é‡æ€§èƒ½ä»¥å¾—åˆ°ç³»åˆ—å‘ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. ç°æœ‰å·¥å…·ä½¿ç”¨å·¥ä½œæµåœ¨é¢å¯¹å„ç±»å†…éƒ¨å’Œå¤–éƒ¨å› ç´ æ—¶å­˜åœ¨æ˜æ˜¾ä¸ç¨³å®šæ€§ï¼Œå³ä¾¿æœ€å…ˆè¿›æ–¹æ³•åœ¨éå…³é”®æ‰°åŠ¨ä¸‹ä¹Ÿä¼šä¸ç¨³å®šï¼›
2. å†…éƒ¨å› ç´ ä¸­ï¼Œåˆé€‚è¶…å‚æ•°è®¾ç½®è™½èƒ½ä¿ƒä½¿LLMsç”Ÿæˆå¤šæ ·è§£å†³æ–¹æ¡ˆï¼Œä½†ä¹Ÿä¼šå¼•å‘ä¸ç¨³å®šæ€§ï¼›
3. å¤–éƒ¨å› ç´ é‡Œï¼ŒLLMså¯¹å€™é€‰å·¥å…·é›†ï¼ˆé¡ºåºæˆ–è§„æ¨¡ï¼‰å’Œç³»ç»Ÿæç¤ºçš„å˜åŒ–å¾ˆæ•æ„Ÿï¼›
4. å…ˆè¿›å·¥å…·é€‰æ‹©ç®—æ³•ï¼ˆå¦‚åŸºäºæ ‘çš„æœç´¢ï¼‰è™½èƒ½æå‡å‡†ç¡®ç‡ï¼Œä½†å¯èƒ½å—ç´¯ç§¯å¹»è§‰å½±å“ç¨³å®šæ€§ï¼Œä¸”æ¨ç†æˆæœ¬é«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç ”ç©¶è§†è§’ä¸Šï¼Œä¸ºå·¥å…·å­¦ä¹ é¢†åŸŸæä¾›äº†ä»ç¨³å®šæ€§è§’åº¦ç³»ç»Ÿåˆ†æå½±å“å› ç´ çš„æ–°æ€è·¯ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å¯¹æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­é²æ£’æ€§çš„å…³æ³¨ï¼›
2. å› ç´ åˆ†ææ–¹é¢ï¼Œæ¸…æ™°çš„å†…å¤–éƒ¨å› ç´ åˆ†ç±»ä¸ºå¼€å‘è€…ä¼˜åŒ–å·¥å…·å­¦ä¹ æ¡†æ¶ï¼ˆå†…éƒ¨å› ç´ å±‚é¢ï¼‰å’Œæå‡ç”¨æˆ·äº¤äº’ä½“éªŒï¼ˆå¤–éƒ¨å› ç´ å±‚é¢ï¼‰æä¾›äº†æ˜ç¡®æ–¹å‘ï¼›
3. å®éªŒè®¾è®¡ä¸Šï¼ŒåŸºäºç»å…¸æ•°æ®é›†ç»“åˆå¤šæŒ‡æ ‡è¯„ä¼°çš„æ–¹å¼ï¼Œä¸ºç›¸å…³é¢†åŸŸå¼€å±•å®è¯ç ”ç©¶æä¾›äº†å¯å‚è€ƒçš„èŒƒå¼ï¼ŒåŠ©åŠ›åç»­å¯¹å·¥å…·å­¦ä¹ æ›´æ·±å…¥çš„æ¢ç´¢ã€‚

## wtu-eval--a-whether-or-not-tool-usage-evaluation-benchmark-for-large-language-models
### Abstract
Although Large Language Models (LLMs) excel in NLP tasks, they still need external tools to extend their ability. Current research on tool learning with LLMs often assumes mandatory tool use, which does not always align with real-world situations, where the necessity for tools is uncertain, and incorrect or unnecessary use of tools can damage the general abilities of LLMs. Therefore, we propose to explore whether LLMs can discern their ability boundaries and use tools flexibly. We then introduce the Whether-or-not tool usage Evaluation benchmark (WTU-Eval) to assess LLMs with eleven datasets, where six of them are tool-usage datasets, and five are general datasets. LLMs are prompted to use tools according to their needs. The results of eight LLMs on WTU-Eval reveal that LLMs frequently struggle to determine tool use in general datasets, and LLMs' performance in tool-usage datasets improves when their ability is similar to ChatGPT. In both datasets, incorrect tool usage significantly impairs LLMs' performance. To mitigate this, we also develop the finetuning dataset to enhance tool decision-making. Fine-tuning Llama2-7B results in a 14\% average performance improvement and a 16.8\% decrease in incorrect tool usage. We will release the WTU-Eval benchmark.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WTU-EVALï¼šå¤§è¯­è¨€æ¨¡å‹â€œæ˜¯å¦ç”¨å·¥å…·â€çš„èƒ½åŠ›è¯„ä¼°æ–°åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é¢å¯¹ä¸€äº›ä»»åŠ¡ä»éœ€å¤–éƒ¨å·¥å…·æ‹“å±•èƒ½åŠ›ã€‚ç„¶è€Œç°æœ‰ç ”ç©¶å¤§å¤šå‡è®¾å·¥å…·â€œå¿…é¡»ç”¨â€ï¼Œå’Œç°å®åœºæ™¯ä¸­â€œæ˜¯å¦éœ€è¦å·¥å…·ä¸ç¡®å®šã€é”™è¯¯/ä¸å¿…è¦ä½¿ç”¨å·¥å…·ä¼šæŸå®³æ¨¡å‹èƒ½åŠ›â€çš„æƒ…å†µä¸åŒ¹é…ã€‚æ¯”å¦‚æœ‰æ—¶æ¨¡å‹æ˜æ˜è‡ªèº«çŸ¥è¯†èƒ½å›ç­”ï¼Œå´é”™è¯¯è°ƒç”¨å·¥å…·ï¼›æˆ–è¯¥ç”¨å·¥å…·æ—¶æ²¡ç”¨å¯¹ï¼Œå¯¼è‡´ç»“æœå‡ºé”™ã€‚å› æ­¤ï¼Œæ¢ç´¢LLMsèƒ½å¦è¯†åˆ«è‡ªèº«èƒ½åŠ›è¾¹ç•Œã€çµæ´»å†³å®šæ˜¯å¦ç”¨å·¥å…·ï¼Œæˆäº†å…³é”®é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºWTU - EvalåŸºå‡†  
æ„å»ºäº†é¦–ä¸ªèšç„¦â€œæ˜¯å¦å‡†ç¡®ä½¿ç”¨å·¥å…·â€çš„è¯„ä¼°åŸºå‡†WTU - Evalï¼ŒåŒ…å«11ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­6ä¸ªæ˜¯éœ€å·¥å…·çš„å·¥å…·ä½¿ç”¨æ•°æ®é›†ï¼ˆå¦‚æ¶‰åŠæœºå™¨ç¿»è¯‘ã€æ•°å­¦æ¨ç†ã€ç½‘é¡µæœç´¢ç­‰ä»»åŠ¡çš„MLQAã€GSM8Kç­‰ï¼‰ï¼Œ5ä¸ªæ˜¯æ— éœ€å·¥å…·ã€é æ¨¡å‹è‡ªèº«èƒ½åŠ›å°±èƒ½å›ç­”çš„é€šç”¨æ•°æ®é›†ï¼ˆå¦‚ä¾§é‡é˜…è¯»ç†è§£ã€å¸¸è¯†æ¨ç†çš„BoolQã€PIQAç­‰ï¼‰ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒåŒºåŸŸï¼ˆæœ‰æ— å·¥å…·é€‰é¡¹ä¸‹æ¨¡å‹è¡¨ç°ï¼‰ï¼Œè¯„ä¼°æ¨¡å‹å¯¹å·¥å…·ä½¿ç”¨çš„å†³ç­–èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡å¾®è°ƒæ•°æ®é›†æå‡å·¥å…·å†³ç­–  
ä»WTU - EvalåŸºå‡†çš„è®­ç»ƒé›†æ„å»ºäº†è§„æ¨¡ä¸º4000çš„å¾®è°ƒæ•°æ®é›†ï¼Œç”¨äºå¢å¼ºæ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ä¸Šçš„å†³ç­–èƒ½åŠ›ã€‚é€šè¿‡å¯¹Llama2 - 7Bå¾®è°ƒï¼ŒéªŒè¯äº†è¯¥æ•°æ®é›†åœ¨æå‡æ¨¡å‹è¡¨ç°ã€å‡å°‘é”™è¯¯å·¥å…·ä½¿ç”¨æ–¹é¢çš„æ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
1. å¯¹8ä¸ªçŸ¥åLLMsåœ¨WTU - Evalä¸Šæµ‹è¯•å‘ç°ï¼šé€šç”¨æ•°æ®é›†é‡Œï¼Œæ¨¡å‹å¸¸éš¾ä»¥åˆ¤æ–­æ˜¯å¦ç”¨å·¥å…·ï¼›å·¥å…·ä½¿ç”¨æ•°æ®é›†é‡Œï¼Œå½“æ¨¡å‹èƒ½åŠ›æ¥è¿‘ChatGPTæ—¶è¡¨ç°ä¼šæå‡ã€‚ä¸”ä¸¤ç±»æ•°æ®é›†é‡Œï¼Œé”™è¯¯ä½¿ç”¨å·¥å…·éƒ½ä¼šå¤§å¹…æŸå®³æ¨¡å‹æ€§èƒ½ã€‚  
2. ç”¨æ„å»ºçš„å¾®è°ƒæ•°æ®é›†å¾®è°ƒLlama2 - 7Båï¼Œæ¨¡å‹å¹³å‡æ€§èƒ½æå‡14%ï¼Œé”™è¯¯å·¥å…·ä½¿ç”¨å‡å°‘16.8%ï¼›åƒåœ¨PIQAçš„æœç´¢å¼•æ“ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡è¾¾40%ï¼Œå·¥å…·è°ƒç”¨ç‡ä¹Ÿé™ä½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åŸºå‡†æ„å»ºè§’åº¦ï¼šWTU - Evalå¡«è¡¥äº†â€œè¯„ä¼°æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹æ˜¯å¦éœ€è¦å·¥å…·â€çš„ç©ºç™½ï¼Œä¸ºåç»­ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨å†³ç­–æä¾›äº†ç»Ÿä¸€ã€å…¨é¢çš„è¯„ä¼°ä½“ç³»å‚è€ƒã€‚  
2. æ¨¡å‹ä¼˜åŒ–è§’åº¦ï¼šé€šè¿‡æ„å»ºç‰¹å®šå¾®è°ƒæ•°æ®é›†æ¥å¢å¼ºå·¥å…·å†³ç­–èƒ½åŠ›çš„æ€è·¯ï¼Œè¯æ˜äº†æ•°æ®é©±åŠ¨ä¼˜åŒ–å·¥å…·ä½¿ç”¨ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨åˆç†æ€§æä¾›äº†å®è·µè·¯å¾„ã€‚  
3. ç ”ç©¶è§†è§’è§’åº¦ï¼šå…³æ³¨â€œå·¥å…·æ˜¯å¦è¯¥ç”¨â€è€Œéâ€œå¿…é¡»ç”¨å·¥å…·â€ï¼Œè´´è¿‘çœŸå®åœºæ™¯éœ€æ±‚ï¼Œå¯å‘ç ”ç©¶è€…ä»æ›´è´´åˆå®é™…åº”ç”¨çš„è§’åº¦å»æ€è€ƒå¤§è¯­è¨€æ¨¡å‹ä¸å·¥å…·çš„åä½œæ¨¡å¼ã€‚

