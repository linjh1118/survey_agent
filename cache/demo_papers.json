{
  "2508.00414": {
    "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training",
    "summary": "```\n## 🌟 论文解读 | Cognitive Kernel - Pro：开启开源智能体研究新征程\n\n## 📌 背景痛点/本文动机\n通用人工智能体正逐渐成为下一代人工智能的基础框架，具备复杂推理、网页交互、编码和自主研究等能力。然而，当前的智能体系统要么是闭源的，要么严重依赖各种付费API和专有工具，这限制了研究社区的可访问性和可重复性。为了解决这一问题，论文提出了Cognitive Kernel - Pro，一个完全开源且（尽可能）免费的多模块智能体框架，旨在推动先进人工智能体的开发和评估的民主化。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：开源多模块智能体框架\n提出Cognitive Kernel - Pro框架，采用两层多模块架构，由负责任务分解、子任务委派和信息聚合等的主智能体，以及解决主智能体分配的子任务的多个子智能体组成。主智能体和子智能体都继承自同一基类，输入为任务字符串，输出为响应字符串，中间动作以Python代码形式执行。...",
    "content_hash": "9370b6a80bf29afdba7d26f275029ca3",
    "cached_at": "2025-11-04T10:06:13.493929",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2510.24702": {
    "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents",
    "summary": "```\n## 🌟 论文解读 | ADP：开启大语言模型智能体训练标准化新时代\n\n## 📌 背景痛点/本文动机\n大语言模型（LLMs）的预训练得益于丰富的互联网规模数据，但后训练阶段获取高质量特定任务数据面临挑战，尤其是在智能体应用场景中。智能体需采取顺序行动并与世界迭代交互，构建此类场景的数据集需记录和构建智能体行为轨迹，比收集静态输入 - 输出对困难得多。尽管已有多种创建智能体数据集的方法且数据集涵盖广泛任务，但大规模监督微调（SFT）在学术研究中仍较为罕见。原因并非缺乏数据，而是现有数据集格式和表示不一致，碎片化严重，难以有效组合、共享和利用。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：提出智能体数据协议（ADP）\nADP是一种轻量级表示语言，作为不同格式的智能体数据集与统一的下游智能体训练管道之间的“中间语言”。它以Pydantic模式实现，表达对应常见智能体用例（如通信、浏览、编码和各种工具调用）的行动和观察，并通过严格的自动验证维持高数据质量。...",
    "content_hash": "2b20574572e4662a59b584fcc58d5099",
    "cached_at": "2025-11-04T10:06:11.195056",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2505.15859": {
    "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
    "summary": "```\n## 🌟 论文解读 | AutoData：开启自动网络数据收集新时代\n\n## 📌 背景痛点/本文动机\n数据是现代以数据为中心的智能系统的驱动力，高质量的网络源数据集需求日益增长。万维网成为大规模数据获取的默认来源，已有网络源数据推动了多领域研究。然而，传统网络数据收集方法存在显著局限：基于包装器的方法适应性和可重复性差；基于大语言模型（LLM）的方法计算和财务成本高。此外，缺乏用于评估开放网络数据收集任务模型性能的基准数据集，现有相关基准数据集多基于静态和存档网页，无法测试开放网络数据收集。因此，需要构建一个端到端的全自动开放网络数据收集系统，以兼顾覆盖范围、准确性和效率。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：新颖的多智能体系统\n开发了全自动多智能体系统AutoData，由八个专业智能体和新颖的有向超图缓存系统（OHCache）组成。AutoData在中央任务管理器（MGR）下协调研究和开发两个专业智能体小组。研究小组的智能体根据输入指令浏览网页生成开发蓝图，开发小组则将蓝图转化为可执行代码并运行获取所需数据集。...",
    "content_hash": "44e108ddd331fe1884d37bc6e16261c3",
    "cached_at": "2025-11-04T10:06:08.008609",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2510.01179": {
    "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments",
    "summary": "```\n## 🌟 论文解读 | TOUCAN：开启工具 - 代理数据新时代\n\n## 📌 背景痛点/本文动机\n大语言模型（LLM）代理正迅速成为跨领域自动化任务的强大系统，但开源社区的发展受到缺乏高质量、许可宽松的工具 - 代理训练数据的限制。现有数据集在多样性、真实性和复杂性方面往往存在局限，特别是在多工具和多轮交互方面。目前急需能够涵盖生产环境中工具 - 代理完整交互范围的高质量数据集。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：构建大规模真实数据集\nTOUCAN是目前最大的公开可用工具 - 代理数据集，包含从近500个真实世界的模型上下文协议（MCP）中合成的150万条轨迹。与先前依赖模拟或有限工具集的方法不同，TOUCAN利用具有2000多种工具的真实MCP环境，生成涵盖并行和多步骤工具调用以及多轮对话的多样化、现实且具有挑战性的任务。...",
    "content_hash": "4179f2d7470adab8a7c7e75f9cb4527d",
    "cached_at": "2025-11-04T10:06:05.281385",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2409.00920": {
    "title": "ToolACE: Winning the Points of LLM Function Calling",
    "summary": "```\n## 🌟 论文解读 | ToolACE：解锁大语言模型函数调用新高度\n\n## 📌 背景痛点/本文动机\n大语言模型（LLMs）中函数调用极大地拓展了其应用边界，高质量且多样的训练数据对于解锁这一能力至关重要。然而，收集和标注真实的函数调用数据颇具挑战，现有的合成数据生成管道产生的数据往往缺乏覆盖范围和准确性。当前工具增强的LLMs主要聚焦于简单的函数调用任务，多样性和复杂性有限，且依赖现有公共API进行任务构建，限制了零样本能力和对复杂场景（如依赖或多轮交互）的适用性。因此，需要一种新的方法来生成准确、多样且复杂的函数调用数据。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：Tool Self - Evolution Synthesis（TSS）模块\n提出一种工具自我进化合成方法，通过物种形成、适应和进化三个步骤，生成具有多种数据类型和约束的API定义。该方法不依赖公共API，从预训练数据出发，通过迭代的自我进化和持续更新，扩展API池的多样性，建立了一个包含26,507个多样API的全面API池，在数量和领域覆盖上超越其他代表性工具增强LLMs。...",
    "content_hash": "47323504d5d1dbd3adf6e15355e697c4",
    "cached_at": "2025-11-04T10:06:03.831078",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2504.03601": {
    "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
    "summary": "```\n## 🌟 论文解读 | APIGen - MT：开启多轮对话数据生成新征程\n\n## 📌 背景痛点/本文动机\n随着大语言模型（LLM）代理在各行业需求的增长，其角色已从简单聊天机器人拓展到能执行现实任务的智能体。然而，训练有效的多轮交互 AI 代理需要高质量数据来捕捉真实的人机动态，但此类数据在公共预训练语料库中稀缺，且手动收集和标注成本高昂、耗时。现有方法如 APIGen 主要关注单轮交互，无法体现现实中多轮交互的复杂性，其他涉及多轮方面的方法又缺乏人机交互，高质量多轮轨迹的验证和合成仍是难题，这严重阻碍了代理能力的提升。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：提出 APIGen - MT 代理数据合成管道，利用环境执行反馈和评审委员会确保生成的多轮代理数据的高质量。\n💡 创新点2：开发两阶段框架。...",
    "content_hash": "f3ca8388387176eaf1a3474f81309ded",
    "cached_at": "2025-11-04T10:06:02.570593",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2502.16863": {
    "title": "Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment",
    "summary": "```\n## 🌟 论文解读 | 利用大语言模型实现高效且可解释的多智能体功劳分配\n\n## 📌 背景痛点/本文动机\n在自动驾驶车辆协调到太空装配等众多实际场景中，学习协作行为对于使机器人实现共同目标至关重要，多智能体强化学习（MARL）中的集中训练 - 分散执行（CTDE）范式常被用于学习这种协作行为。然而，在CTDE的中央训练阶段，一个关键挑战是如何分离每个策略变化的影响，并评估每个智能体对全局任务整体成功或失败的贡献，即 “功劳分配” 问题。传统上，环境仅根据智能体是否实现共享目标提供集体奖励，CTDE训练算法需从单一奖励中确定每个智能体的贡献并更新策略。该问题一直未得到很好解决，现有方法存在诸多局限，如反馈质量低、行动影响力低以及处理复杂交互困难等。同时，人类手动检查智能体行为往往能产生比现有方法更好的功劳评估，且近期研究表明大语言模型（LLMs）在许多模式识别任务中展现出人类水平的性能，基于此，论文作者希望利用LLMs来解决多智能体功劳分配问题。...",
    "content_hash": "24e27194a1000a89c41a22b41de0a93e",
    "cached_at": "2025-11-04T10:05:58.137115",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2410.18447": {
    "title": "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis",
    "summary": "```\n## 🌟 论文解读 | ToolFlow：开启大语言模型工具调用新境界\n\n## 📌 背景痛点/本文动机\n大语言模型（LLMs）的工具调用能力提升常采用监督微调（SFT）方法，其训练数据多为合成所得。当前数据合成过程通常是采样一组工具、基于工具制定需求并生成调用语句。然而，随机采样的工具缺乏相关性，难以组合，降低了数据多样性；同时，现有工作忽视了对话轮次间的连贯性，导致合成数据与现实场景存在差距。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：基于图的采样策略（Graph - based Sampling strategy）\n考虑参数或返回值相似的工具为相关工具，构建工具图，图中节点代表工具，边表示工具对之间的相关性。采样工具时，从工具图中随机选择子图，使采样工具更易有效交互，便于生成复杂需求，提升合成工具调用需求的多样性和复杂性。\n💡 创新点2：规划生成策略（Planned - Generation strategy）\n在合成对话前，让LLM基于选定的工具子集创建计划，该计划勾勒出用户在对话每一轮需提出的请求。...",
    "content_hash": "be929926eceb5304504a42a677efa941",
    "cached_at": "2025-11-04T10:05:57.166273",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2510.16872": {
    "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
    "summary": "```\n## 🌟 论文解读 | DeepAnalyze：开启自主数据科学新时代\n\n## 📌 背景痛点/本文动机\n自主数据科学旨在实现从原始数据源到分析师级深度研究报告的自动化，这一直是数据科学界长期追求的核心目标。然而，该过程涉及数据准备、分析、建模、可视化和报告生成等一系列复杂且相互依赖的任务，实现起来颇具挑战。尽管强大的大语言模型（LLMs）的出现使其变得可行，但LLMs在协调复杂的多阶段数据科学流程以及处理各种结构化数据方面仍存在困难。近期基于工作流的数据代理在特定数据任务上虽有不错表现，但因依赖预定义工作流，在实现完全自主的数据科学方面存在根本局限。\n\n## 🚀 核心方法\n💡 创新点1：提出DeepAnalyze - 8B\n这是首个专为自主数据科学设计的代理型大语言模型，能够自动完成从数据源到分析师级深度研究报告的端到端流程。\n\n💡 创新点2：提出基于课程的代理训练范式\n该范式模仿人类数据科学家的学习轨迹，使大语言模型能够在现实世界环境中逐步获取并整合多种能力，以应对高复杂性的数据科学任务。...",
    "content_hash": "a084acccca6764c7a5098f4f5f115525",
    "cached_at": "2025-11-04T10:05:49.937679",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250529110941-khvtx"
    }
  },
  "2509.09675": {
    "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models",
    "summary": "## 🌟 论文解读 | 用“好奇心”驱动大模型强化学习探索：CDE框架破解RLVR探索不足难题\n\n## 📌 背景痛点/本文动机\n大语言模型（LLMs）在数学、编码等领域推理能力进步显著，但如何高效引导高质量思维链（CoT）推理仍是挑战。基于可验证奖励的强化学习（RLVR）是提升LLMs推理能力的有力范式，然而现有RLVR方法存在探索不足问题，易导致过早收敛与熵坍缩（模型过度偏向“利用”已知策略，而非充分探索环境找更优解）。传统强化学习探索策略（如熵奖励、ϵ - greedy）在LLMs场景下要么理论次优、要么效果存疑；基于计数的探索方法（如UCB类）因计算量大、依赖复杂状态 - 动作表示，在长思维链LLMs推理中也不实用。此外，直接将计数探索方法用于RLVR时，因思维链轨迹难用固定嵌入刻画，多数响应会坍缩到相同哈希网格，削弱探索效果。因此，需为LLMs设计高效可扩展的探索方法。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：提出好奇心驱动探索（CDE）框架  \n利用模型内在“好奇心”引导探索，将actor和critic的信号结合来形式化好奇心。...",
    "content_hash": "633f86c7c4ea23ab7a8d31db9e7a3170",
    "cached_at": "2025-10-24T14:59:59.240724",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.10396": {
    "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
    "summary": "## 🌟 论文解读 | 利用修复引导策略优化，解锁扩散大语言模型新可能\n\n## 📌 背景痛点/本文动机\n掩码扩散大语言模型（dLLMs）作为自回归大语言模型的有前景替代方案，性能有竞争力且支持如修复（inpainting）这类独特生成能力。但大语言模型结合强化学习（RL）时面临探索难题：奖励信号稀疏，且模型没找到正确解时会造成样本浪费。虽这低效性普遍影响大语言模型，可dLLMs的修复能力提供了独特机会——其修复能力能引导探索。所以本文探索如何让修复为dLLMs的RL算法设计提供思路，解决强化学习中探索不足等问题。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：提出IGPO（Inpainting Guided Policy Optimization）框架。该RL框架在在线采样期间战略性插入部分真实推理轨迹，不同于直接给全解，修复在保留自生成推理的同时，将探索引向有前景的轨迹空间，搭建起监督微调与强化学习间的桥梁。\n💡 创新点2：针对基于分组的优化方法（如GRPO），IGPO能恢复有意义梯度并提升样本效率。因为在这些方法中探索失败会导致优势和梯度为零，IGPO解决了该问题。...",
    "content_hash": "cbb2fa91d0b373f940501fc11d3e0d56",
    "cached_at": "2025-10-24T14:59:51.105687",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.10423": {
    "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning",
    "summary": "## 🌟 论文解读 | 用互信息追踪强化学习中的策略一致性\n\n## 📌 背景痛点/本文动机\n在现实环境中部署强化学习（RL）智能体时，会面临传感器故障、执行器磨损和环境变化等导致的性能下降问题，然而RL智能体本身缺乏检测和诊断这些故障的内在机制。为了解决这一问题，本文提出了一个信息论框架，旨在揭示RL的基本动态并提供部署时异常诊断的实用方法。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：基于信息论分析RL学习过程特征  \n通过对机器人控制任务中状态 - 动作互信息模式的分析，发现成功学习存在特征性信息特征。在状态熵增长的情况下，状态和动作之间的互信息从0.84稳定增长到2.83比特（增长238%），这表明智能体对任务相关模式发展出了越来越有选择性的注意力。  \n💡 创新点2：揭示联合互信息的变化规律  \n发现状态、动作和下一个状态的联合互信息MI(S,A;S')遵循倒U型曲线，在学习早期达到峰值，之后随着智能体专业化（从广泛探索过渡到高效利用）而下降。...",
    "content_hash": "1af4632b419d2d776dd0b480d90cc794",
    "cached_at": "2025-10-24T14:59:49.975798",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.15194": {
    "title": "Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation",
    "summary": "## 🌟 论文解读 | 无标签进化语言模型：多数驱动选择，新颖性促进变异\n\n## 📌 背景痛点/本文动机\n大语言模型（LLMs）在基于可验证奖励的强化学习（RLVR）下训练取得进展，但现实部署需要模型在无标签或无外部评判下自我改进。现有自我改进方法依赖自我确认信号（如置信度、熵、一致性）生成奖励，这会使模型趋向过度自信、多数偏好的解决方案，引发“熵坍缩”，降低pass@n和推理复杂度。同时，在无标签设置中平衡探索与利用的困境严峻，依赖内部信号的学习过程会主动降低奖励信号质量，形成退化反馈循环，导致策略坍缩到低熵状态，推理多样性下降等问题。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：提出EVOL - RL框架，借鉴进化中选择与变异平衡的原理。将无标签学习构建为进化系统，把多样性坍缩诊断为过早收敛问题，用选择与变异平衡的核心进化原理解决。EVOL - RL保留多数投票答案作为稳定性锚点，同时添加新颖性感知奖励，依据每个采样解决方案的推理与其他同时生成响应的差异程度（推理轨迹的语义相似性）来评分。\n💡 创新点2：设计实用的新颖性感知奖励，补充多数选择，实现稳定的无标签改进。...",
    "content_hash": "360bc470be3a0f1d4f3d432dc0d2f234",
    "cached_at": "2025-10-24T14:59:41.832934",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.12108": {
    "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models",
    "summary": "```\n## 🌟 论文解读 | 大语言模型文本分类新范式：GTA框架融合监督微调与强化学习优势\n\n## 📌 背景痛点/本文动机\n在自然语言处理的文本分类任务中，纯强化学习（RL）微调方法存在探索效率低、收敛慢的问题；而监督微调（SFT）虽训练高效，但性能上限有限且理论基础不如RL扎实。同时，思维链（CoT）提示技术虽能提升推理任务表现，却需大量人工标注推理链，成本高且易受偏差影响。为平衡效率与能力，本文提出Guess - Think - Answer（GTA）框架，旨在统一训练范式中结合SFT的效率与RL的能力增益。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：提出GTA框架重构推理流程  \n将推理过程结构化分为三个阶段，首先生成直观的初步猜测（Guess），该阶段通过交叉熵损失监督优化；接着基于初步猜测和输入问题进行显式推理（Think）；最后整合推理结果生成精炼的最终答案（Answer），RL奖励同时塑造最终输出与整个GTA结构的格式。\n\n💡 创新点2：单阶段统一SFT与RL训练  \n在同一训练流程中无缝整合SFT与RL。...",
    "content_hash": "c970f6eccf42035cc20d1d9899cf0899",
    "cached_at": "2025-10-24T14:59:39.467383",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.20265": {
    "title": "Failure Modes of Maximum Entropy RLHF",
    "summary": "## 🌟 论文解读 | 揭秘最大熵RLHF的失效模式：SimPO理论溯源与在线RLHF挑战\n\n## 📌 背景痛点/本文动机\n在AI系统与人类价值观对齐的研究中，强化学习从人类反馈（RLHF）是主流方法，但传统RLHF pipeline（监督微调、奖励建模、强化学习优化）存在计算量大、操作复杂等问题，推动了直接对齐算法（如DPO、SimPO）的探索。SimPO作为无参考模型的方法在离线偏好优化中表现出色，却缺乏理论基础；同时，基于最大熵强化学习在离线场景助力SimPO的表现，引发了其在在线RLHF场景是否有效的疑问，这构成了本文研究的动机——探究SimPO的理论根基及最大熵RL在在线RLHF的表现与挑战。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：SimPO的理论溯源——最大熵强化学习视角  \n本文证明Simple Preference Optimization（SimPO）可被推导为**带长度归一化温度的最大熵强化学习**形式，为这一无参考模型的方法提供了理论基础。...",
    "content_hash": "7dcf8b61bb18ca9f54c8879ee1211348",
    "cached_at": "2025-10-24T14:59:38.749453",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.15498": {
    "title": "Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers",
    "summary": "## 🌟 论文解读 | 为在线决策Transformer注入认知灵感：EWA-VQ-ODT提升长期决策效率\n\n## 📌 背景痛点/本文动机\n在强化学习（RL）领域，Transformer架构因能通过自注意力建模轨迹，为序列决策提供了新思路。决策Transformer（DT）将RL转化为监督序列建模，但受限于离线数据且缺乏探索；在线决策Transformer（ODT）通过对策略轨迹的熵正则化训练解决了部分问题，成为Soft Actor - Critic等传统依赖自举目标和奖励塑造方法的稳定替代。然而，ODT采用的标准注意力缺乏对特定动作结果的显式记忆，在学习长期动作有效性时效率低下。因此，本文受认知模型（如Experience - Weighted Attraction，EWA）启发，旨在提升ODT对动作长期效果的学习能力。...",
    "content_hash": "6ec7d9ca13c96c3951f54ecd70f54fbf",
    "cached_at": "2025-10-24T14:59:38.689315",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.21044": {
    "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
    "summary": "## 🌟 论文解读 | RL微调如何重塑大语言模型内部电路？\n\n## 📌 背景痛点/本文动机\n大语言模型（LLMs）通过大规模预训练积累大量先验知识，还能经有监督微调（SFT）或基于强化学习（RL）的后训练进一步增强能力。已有不少证据表明RL微调能让LLMs能力超越仅SFT的情况，但RL微调为何能增强不同固有特性LLMs能力的底层机制却研究不足。所以本文受边缘归因修补（EAP）相关工作启发，探究LLMs在RL微调前后的内部差异，以揭示RL微调提升模型能力的内在原因。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：借鉴边缘归因修补（EAP）方法探究内部差异\n借助EAP这一工具，对多个模型家族在RL微调前后的内部情况展开分析，以此来挖掘RL微调给模型内部电路带来的变化规律，为理解RL微调作用机制提供技术手段支撑。\n💡 创新点2：聚焦在线RL后训练的影响分析维度\n从激活强度和激活模式多样性这两个关键维度，分析在线RL后训练对模型内部的影响，清晰界定出RL微调带来的如激活强度整体提升、激活模式更具多样性等核心变化表现，从而剖析RL如何重塑模型信息流。...",
    "content_hash": "44e8d2a0abce3d0a0d9b0a079f8e1db1",
    "cached_at": "2025-10-24T14:59:26.467830",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.21010": {
    "title": "ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning",
    "summary": "## 🌟 论文解读 | ExMolRL：多目标强化学习驱动的表型-靶点联合从头分子生成\n\n## 📌 背景痛点/本文动机\n在人工智能驱动的药物设计领域，生成高质量候选分子一直是核心挑战。当前基于表型和基于靶点的策略各有局限：基于表型的策略实验成本高，而基于靶点的策略易忽视系统级的细胞响应。为了弥补这一差距，本文提出ExMolRL框架，旨在协同整合表型和靶点特异性线索来进行从头分子生成。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：表型引导的生成器预训练与微调  \n首先在大量药物诱导的转录组谱上对表型引导的生成器进行预训练，之后通过多目标强化学习进行微调，让模型能充分利用表型层面的信息。  \n\n💡 创新点2：多目标强化学习的奖励函数设计  \n奖励函数融合了对接亲和力和类药性分数，同时加入排序损失、先验似然正则化和熵最大化。多目标强化学习引导模型生成同时具备高效力、多样性且与指定表型效应一致的化学型分子。  \n\n## 📈 实验结果\n在多个特征明确的靶点上，大量实验表明ExMolRL的性能优于当前最先进的基于表型和基于靶点的模型。...",
    "content_hash": "f6363c5ff86f2065406613b94f373c37",
    "cached_at": "2025-10-24T14:59:25.138670",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.21124": {
    "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
    "summary": "## 🌟 论文解读 | 用多样化思维链模式拓展大模型推理潜力\n\n## 📌 背景痛点/本文动机\n在面向复杂数学推理的大模型研究中，强化学习（RL）推动了不少进展，训练中期融入长思维链（CoT）数据也被证实能提升推理深度。但当前方法存在对CoT数据不加区分使用的问题，“哪种数据类型最能有效增强模型推理能力”成了待解的关键问题。为此，论文首次定义大模型的“推理潜力”（即正确回答问题所需独立尝试次数的倒数，与最终模型性能强相关），并探索用富含高价值推理模式的多样化数据来拓展这一潜力。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：定义推理潜力并抽象原子推理模式  \n首次提出“推理潜力”概念，将其定义为正确回答问题所需独立尝试次数的倒数，且该指标和模型最终性能强关联。同时从思维链序列里抽象出“原子推理模式”——这类模式具备通用性与归纳能力，并用它们构建富含高价值推理模式的核心参考集。  \n\n💡 创新点2：双粒度数据选择算法  \n提出结合推理模式链与token熵的双粒度算法，从数据池中高效筛选与核心集匹配的高价值思维链数据（CoTP），让模型能更有效地学习掌握推理能力。...",
    "content_hash": "c447ccf8286b7ddd465986bbbd84bd20",
    "cached_at": "2025-10-24T14:59:22.943095",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  },
  "2509.21482": {
    "title": "Learning to Reason with Mixture of Tokens",
    "summary": "## 🌟 论文解读 | 用混合token学习推理：突破大模型推理能力提升瓶颈\n\n## 📌 背景痛点/本文动机\n强化学习结合可验证奖励（RLVR）是提升大语言模型（LLM）推理能力的主流方法。当前多数方法遵循Group Relative Policy Optimization的变体，在每个推理步骤采样离散token，丢弃了模型候选token概率分布中的丰富分布信息。在非强化学习场景利用这些分布信息已被证明有效，但现有RLVR方法因未利用该信息，不必要地限制了推理搜索空间。为解决此局限，本文探索RLVR中的混合token生成（MoT - G）。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：提出统一框架泛化现有MoT - G方法 \n构建了能泛化现有MoT - G方法的统一框架，涵盖现有无训练方法（这类方法将混合嵌入构建为token嵌入的加权和），把RLVR扩展到在连续混合空间直接生成思维链。...",
    "content_hash": "a04897eee6dae8c97f35ba3c3aaf2870",
    "cached_at": "2025-10-24T14:59:20.450489",
    "model_info": {
      "provider": "doubao",
      "model_name": "ep-20250526175303-cv654"
    }
  }
}