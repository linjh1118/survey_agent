# Paper List of Terms(RL+entropy)
- [25/08] **ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents**  
[[Paper](http://arxiv.org/pdf/2508.14040v1)] [[Code/Page]()] [[TLDR/Notes](#computerrl--scaling-end-to-end-online-reinforcement-learning-for-computer-use-agents)]

- [25/08] **Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation**  
[[Paper](http://arxiv.org/pdf/2508.14143v1)] [[Code/Page]()] [[TLDR/Notes](#beyond-turing--memory-amortized-inference-as-a-foundation-for-cognitive-computation)]

- [25/08] **From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR**  
[[Paper](http://arxiv.org/pdf/2508.07534v2)] [[Code/Page]()] [[TLDR/Notes](#from-trial-and-error-to-improvement--a-systematic-analysis-of-llm-exploration-mechanisms-in-rlvr)]

- [25/08] **AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance**  
[[Paper](http://arxiv.org/pdf/2508.06944v2)] [[Code/Page](https://github.com/hlxtsyj/AMFT.)] [[TLDR/Notes](#amft--aligning-llm-reasoners-by-meta-learning-the-optimal-imitation-exploration-balance)]

- [25/08] **GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy**  
[[Paper](http://arxiv.org/pdf/2508.04349v4)] [[Code/Page]()] [[TLDR/Notes](#gtpo-and-grpo-s--token-and-sequence-level-reward-shaping-with-policy-entropy)]

- [25/08] **Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following**  
[[Paper](http://arxiv.org/pdf/2508.03178v1)] [[Code/Page]()] [[TLDR/Notes](#light-if--endowing-llms-with-generalizable-reasoning-via-preview-and-self-checking-for-complex-instruction-following)]

- [25/08] **Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2508.02260v1)] [[Code/Page]()] [[TLDR/Notes](#decomposing-the-entropy-performance-exchange--the-missing-keys-to-unlocking-effective-reinforcement-learning)]

- [25/07] **MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning**  
[[Paper](http://arxiv.org/pdf/2507.20278v1)] [[Code/Page]()] [[TLDR/Notes](#mol-rl--distilling-multi-step-environmental-feedback-into-llms-for-feedback-independent-reasoning)]

- [25/07] **The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2507.20150v1)] [[Code/Page]()] [[TLDR/Notes](#the-policy-cliff--a-theoretical-analysis-of-reward-policy-maps-in-large-language-models)]

- [25/07] **Agentic Reinforced Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2507.19849v1)] [[Code/Page](https://github.com/dongguanting/ARPO)] [[TLDR/Notes](#agentic-reinforced-policy-optimization)]

- [25/07] **UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities**  
[[Paper](http://arxiv.org/pdf/2507.19766v1)] [[Code/Page]()] [[TLDR/Notes](#ulorl-an-ultra-long-output-reinforcement-learning-approach-for-advancing-large-language-models--reasoning-abilities)]

- [25/07] **Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2507.14748v1)] [[Code/Page]()] [[TLDR/Notes](#skill-learning-via-policy-diversity-yields-identifiable-representations-for-reinforcement-learning)]

- [25/07] **Perception-Aware Policy Optimization for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2507.06448v4)] [[Code/Page](https://mikewangwzhl.github.io/PAPO.)] [[TLDR/Notes](#perception-aware-policy-optimization-for-multimodal-reasoning)]

- [25/07] **Skywork-R1V3 Technical Report**  
[[Paper](http://arxiv.org/pdf/2507.06167v3)] [[Code/Page]()] [[TLDR/Notes](#skywork-r1v3-technical-report)]

- [25/07] **Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2507.01551v2)] [[Code/Page]()] [[TLDR/Notes](#self-guided-process-reward-optimization-with-redefined-step-wise-advantage-for-process-reinforcement-learning)]

- [25/07] **Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems**  
[[Paper](http://arxiv.org/pdf/2507.00358v2)] [[Code/Page]()] [[TLDR/Notes](#data-driven-exploration-for-a-class-of-continuous-time-indefinite-linear--quadratic-reinforcement-learning-problems)]

- [25/06] **EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework**  
[[Paper](http://arxiv.org/pdf/2506.22200v3)] [[Code/Page](https://github.com/597358816/EFRame.)] [[TLDR/Notes](#eframe--deeper-reasoning-via-exploration-filter-replay-reinforcement-learning-framework)]

- [25/06] **APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2506.21655v1)] [[Code/Page](https://github.com/Indolent-Kawhi/View-R1.)] [[TLDR/Notes](#apo--enhancing-reasoning-ability-of-mllms-via-asymmetric-policy-optimization)]

- [25/06] **SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.19767v1)] [[Code/Page]()] [[TLDR/Notes](#srft--a-single-stage-method-with-supervised-and-reinforcement-fine-tuning-for-reasoning)]

- [25/06] **Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning**  
[[Paper](http://arxiv.org/pdf/2506.18330v2)] [[Code/Page](https://github.com/netease-youdao/Confucius3-Math.)] [[TLDR/Notes](#confucius3-math--a-lightweight-high-performance-reasoning-llm-for-chinese-k-12-mathematics-learning)]

- [25/06] **AdapThink: Adaptive Thinking Preferences for Reasoning Language Model**  
[[Paper](http://arxiv.org/pdf/2506.18237v1)] [[Code/Page]()] [[TLDR/Notes](#adapthink--adaptive-thinking-preferences-for-reasoning-language-model)]

- [25/06] **Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs**  
[[Paper](http://arxiv.org/pdf/2506.14758v3)] [[Code/Page]()] [[TLDR/Notes](#reasoning-with-exploration--an-entropy-perspective-on-reinforcement-learning-for-llms)]

- [25/06] **Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs**  
[[Paper](http://arxiv.org/pdf/2506.14731v2)] [[Code/Page]()] [[TLDR/Notes](#ring-lite--scalable-reasoning-via-c3po-stabilized-reinforcement-learning-for-llms)]

- [25/06] **Unsupervised Skill Discovery through Skill Regions Differentiation**  
[[Paper](http://arxiv.org/pdf/2506.14420v1)] [[Code/Page]()] [[TLDR/Notes](#unsupervised-skill-discovery-through-skill-regions-differentiation)]

- [25/06] **StaQ it! Growing neural networks for Policy Mirror Descent**  
[[Paper](http://arxiv.org/pdf/2506.13862v1)] [[Code/Page]()] [[TLDR/Notes](#staq-it!-growing-neural-networks-for-policy-mirror-descent)]

- [25/06] **AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy**  
[[Paper](http://arxiv.org/pdf/2506.13284v1)] [[Code/Page](https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B)] [[TLDR/Notes](#acereason-nemotron-1-1--advancing-math-and-code-reasoning-through-sft-and-rl-synergy)]

- [25/06] **Research on Optimal Control Problem Based on Reinforcement Learning under Knightian Uncertainty**  
[[Paper](http://arxiv.org/pdf/2506.13207v1)] [[Code/Page]()] [[TLDR/Notes](#research-on-optimal-control-problem-based-on-reinforcement-learning-under-knightian-uncertainty)]

- [25/06] **DR-SAC: Distributionally Robust Soft Actor-Critic for Reinforcement Learning under Uncertainty**  
[[Paper](http://arxiv.org/pdf/2506.12622v1)] [[Code/Page]()] [[TLDR/Notes](#dr-sac--distributionally-robust-soft-actor-critic-for-reinforcement-learning-under-uncertainty)]

- [25/06] **Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization**  
[[Paper](http://arxiv.org/pdf/2506.10871v1)] [[Code/Page]()] [[TLDR/Notes](#viability-of-future-actions--robust-safety-in-reinforcement-learning-via-entropy-regularization)]

- [25/06] **Exploration by Random Reward Perturbation**  
[[Paper](http://arxiv.org/pdf/2506.08737v1)] [[Code/Page]()] [[TLDR/Notes](#exploration-by-random-reward-perturbation)]

- [25/06] **State Entropy Regularization for Robust Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.07085v2)] [[Code/Page]()] [[TLDR/Notes](#state-entropy-regularization-for-robust-reinforcement-learning)]

- [25/06] **When Maximum Entropy Misleads Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2506.05615v1)] [[Code/Page]()] [[TLDR/Notes](#when-maximum-entropy-misleads-policy-optimization)]

- [25/06] **Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic**  
[[Paper](http://arxiv.org/pdf/2506.05445v1)] [[Code/Page]()] [[TLDR/Notes](#causal-policy-learning-in-reinforcement-learning--backdoor-adjusted-soft-actor-critic)]

- [25/05] **Reinforcing Video Reasoning with Focused Thinking**  
[[Paper](http://arxiv.org/pdf/2505.24718v3)] [[Code/Page](https://github.com/longmalongma/TW-GRPO}.)] [[TLDR/Notes](#reinforcing-video-reasoning-with-focused-thinking)]

- [25/05] **The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.24630v1)] [[Code/Page]()] [[TLDR/Notes](#the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models)]

- [25/05] **On-Policy RL with Optimal Reward Baseline**  
[[Paper](http://arxiv.org/pdf/2505.23585v2)] [[Code/Page](https://verl.readthedocs.io/en/latest/algo/opo.html.)] [[TLDR/Notes](#on-policy-rl-with-optimal-reward-baseline)]

- [25/05] **Enhanced DACER Algorithm with High Diffusion Efficiency**  
[[Paper](http://arxiv.org/pdf/2505.23426v1)] [[Code/Page]()] [[TLDR/Notes](#enhanced-dacer-algorithm-with-high-diffusion-efficiency)]

- [25/05] **Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners**  
[[Paper](http://arxiv.org/pdf/2505.23150v1)] [[Code/Page]()] [[TLDR/Notes](#bigger--regularized--categorical--high-capacity-value-functions-are-efficient-multi-task-learners)]

- [25/05] **Maximizing Confidence Alone Improves Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.22660v4)] [[Code/Page]()] [[TLDR/Notes](#maximizing-confidence-alone-improves-reasoning)]

- [25/05] **The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models**  
[[Paper](http://arxiv.org/pdf/2505.22617v1)] [[Code/Page]()] [[TLDR/Notes](#the-entropy-mechanism-of-reinforcement-learning-for-reasoning-language-models)]

- [25/05] **Skywork Open Reasoner 1 Technical Report**  
[[Paper](http://arxiv.org/pdf/2505.22312v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-open-reasoner-1-technical-report)]

- [25/05] **GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.18763v2)] [[Code/Page]()] [[TLDR/Notes](#genpo--generative-diffusion-models-meet-on-policy-reinforcement-learning)]

- [25/05] **Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs**  
[[Paper](http://arxiv.org/pdf/2505.18573v1)] [[Code/Page](https://github.com/LiaoMengqi/E3-RL4LLMs)] [[TLDR/Notes](#enhancing-efficiency-and-exploration-in-reinforcement-learning-for-llms)]

- [25/05] **PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.17714v1)] [[Code/Page]()] [[TLDR/Notes](#ppo-br--dual-signal-entropy-reward-adaptation-for-trust-region-policy-optimization)]

- [25/05] **Enter the Void - Planning to Seek Entropy When Reward is Scarce**  
[[Paper](http://arxiv.org/pdf/2505.16787v2)] [[Code/Page]()] [[TLDR/Notes](#enter-the-void---planning-to-seek-entropy-when-reward-is-scarce)]

- [25/05] **The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.15134v1)] [[Code/Page]()] [[TLDR/Notes](#the-unreasonable-effectiveness-of-entropy-minimization-in-llm-reasoning)]

- [25/05] **AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum**  
[[Paper](http://arxiv.org/pdf/2505.14264v1)] [[Code/Page]()] [[TLDR/Notes](#aapo--enhance-the-reasoning-capabilities-of-llms-with-advantage-momentum)]

- [25/05] **DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization**  
[[Paper](http://arxiv.org/pdf/2505.12366v2)] [[Code/Page]()] [[TLDR/Notes](#disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization)]

- [25/05] **Preference Optimization for Combinatorial Optimization Problems**  
[[Paper](http://arxiv.org/pdf/2505.08735v1)] [[Code/Page]()] [[TLDR/Notes](#preference-optimization-for-combinatorial-optimization-problems)]

- [25/05] **Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.07815v2)] [[Code/Page]()] [[TLDR/Notes](#imagine--verify--execute--memory-guided-agentic-exploration-with-vision-language-models)]



# TLDR/Notes
## computerrl--scaling-end-to-end-online-reinforcement-learning-for-computer-use-agents
### Abstract
We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)
### 🌟 论文解读 | ComputerRL：推动桌面智能代理端到端在线强化学习规模化

### 📌 背景痛点/本文动机
大语言模型（LLMs）推动人工智能能力拓展，基于LLM的GUI代理在桌面设备自主执行复杂任务受关注，但让代理在真实场景长期自主运行仍存挑战。一是GUI为人类交互设计，代理模拟人类动作困难；二是行为克隆等主流方法在可扩展性与有效性上受限，手动标注繁琐、模型蒸馏受限于教师模型；三是强化学习（RL）虽有潜力但因计算复杂和方法挑战，在桌面自动化任务中大规模应用受限，如环境复杂、收敛慢等。因此，需新框架推动桌面级规划、推理与设备操作发展。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：API - GUI交互范式
现有GUI代理依赖类人GUI交互操作设备存在困难，纯API操作也有实现复杂、适应性受限等问题。为此提出API - GUI范式，整合大规模自动构建的API生态系统与传统GUI操作，从以人为中心转向以机器为导向的交互框架，解决人类设计界面与智能代理能力不匹配问题，提升任务操作效率与泛化性。

💡 创新点2：大规模分布式RL训练基础设施
环境可扩展性限制大规模代理训练，通过重构基于Docker和gRPC协议的虚拟机集群，搭建分布式训练基础设施，支持数千并行环境，实现空前可扩展性且与AgentBench兼容，突破RL在计算机使用代理训练中规模实验的瓶颈，结合AgentRL框架实现高效异步训练，加速训练过程。

💡 创新点3：Entropulse训练策略
长时间RL训练存在熵坍缩和KL散度累积等停滞与收敛问题，提出Entropulse，通过在RL和周期性监督微调（SFT）阶段之间策略性交替，系统解决这些挑战，维持探索能力并确保性能持续提升。

### 📈 实验结果
在OSWorld基准测试中，基于ComputerRL训练的AutoGLM - OS - 9B取得48.1%的最新准确率，超过OpenAI CUA o3（42.9%）、UI - TARS - 1.5（42.5%）、Anthropic Claude Sonnet 4（30.7%）等其他先进模型；且Entropulse方法相比传统方法，训练奖励更高，提升了学习效率与最终性能。

### 💬 可借鉴之处
在交互范式上，整合不同操作方式构建更适配智能代理的交互模式，为跨界面交互类任务提供思路；在训练基础设施方面，利用容器和分布式协议搭建大规模并行环境，对需大规模实验支撑的研究有参考价值；在训练策略上，针对强化学习长期训练问题，通过阶段交替方式缓解训练困境，为解决类似模型训练收敛与性能停滞问题提供了创新方法借鉴。

## beyond-turing--memory-amortized-inference-as-a-foundation-for-cognitive-computation
### Abstract
Intelligence is fundamentally non-ergodic: it emerges not from uniform
sampling or optimization from scratch, but from the structured reuse of prior
inference trajectories. We introduce Memory-Amortized Inference (MAI) as a
formal framework in which cognition is modeled as inference over latent cycles
in memory, rather than recomputation through gradient descent. MAI systems
encode inductive biases via structural reuse, minimizing entropy and enabling
context-aware, structure-preserving inference. This approach reframes cognitive
systems not as ergodic samplers, but as navigators over constrained latent
manifolds, guided by persistent topological memory. Through the lens of
delta-homology, we show that MAI provides a principled foundation for
Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a
local inference operator over cycle-consistent memory states. Furthermore, we
establish a time-reversal duality between MAI and reinforcement learning:
whereas RL propagates value forward from reward, MAI reconstructs latent causes
backward from memory. This inversion paves a path toward energy-efficient
inference and addresses the computational bottlenecks facing modern AI. MAI
thus offers a unified, biologically grounded theory of intelligence based on
structure, reuse, and memory. We also briefly discuss the profound implications
of MAI for achieving artificial general intelligence (AGI).
### 🌟 论文解读 | 超越图灵：记忆摊销推理（MAI）作为认知计算的基石

### 📌 背景痛点/本文动机
在自然与人工智能系统中，智能本质上具有非遍历性，它并非源于均匀采样或从头开始的优化，而是来自对先前推理轨迹的结构化复用。传统的诸如强化学习、贝叶斯学习等遍历性模型，将智能视为随机游走或全局函数拟合，未充分考虑记忆、目标和上下文塑造的结构化、路径依赖行为，且现代AI面临计算瓶颈与能效问题。同时，智能具有时间不可逆性，知识积累等引入时间箭头，违背遍历动力学假设，因此需要新框架来捕捉这一核心不对称性，理解记忆、推理与智能行为的关系。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出记忆摊销推理（MAI）框架  
将认知建模为对记忆中潜在循环的推理，而非通过梯度下降重新计算。MAI系统通过结构化复用编码归纳偏差，最小化熵，实现情境感知、结构保持的推理。智能系统不再是遍历性采样器，而是在持久拓扑记忆引导下，在受限潜在流形上的导航器，推理基于记忆中结构化循环的复用，把记忆从被动存储转为主动摊销推理的基础，让预测和重建从潜在循环而非参数优化中产生。  

💡 创新点2：基于delta - 同调为Mountcastle通用皮层算法提供原理性基础  
借助delta - 同调，将每个皮层柱建模为对循环一致记忆状态的局部推理算子，皮层连接把这些算子粘合为全脑范围的记忆 - 推理层。delta - 同调捕捉了局部循环在分层尺度上的持续、交互和稳定，为跨模态推理、感觉运动泛化和认知表征的鲁棒性提供原理解释。  

💡 创新点3：建立MAI与强化学习（RL）的时间反转对偶性  
RL从奖励向前传播价值，而MAI从记忆向后重建潜在原因。这种反转为高能效推理铺路，解决现代AI的计算瓶颈。MAI将学习重构为构建和稳定可复用拓扑算子等的过程，而非从数据拟合全局函数，还为记忆与决策制定搭建理论桥梁，用结构感知复用替代能源密集型迭代。  

### 📈 实验结果
论文未提及传统意义上的实验结果呈现（如对比实验指标等），主要从理论层面论证MAI的合理性、与生物理论（如Mountcastle通用皮层算法）的契合度、与RL的对偶关系等，通过理论推导、概念类比（如遍历与非遍历推理轨迹对比、delta - 同调类比等）来支撑MAI框架的创新性与价值。  

### 💬 可借鉴之处
1. 认知建模视角：启发重新审视智能系统，将其视为在受限流形上基于记忆循环推理的导航器，而非简单的遍历采样器，为认知科学领域建模智能行为提供新范式。  
2. 人工智能方向：针对现代AI计算与能效瓶颈，MAI的结构感知复用思路，为打造生物合理、高能效的人工智能，尤其是迈向通用人工智能（AGI）提供了基于记忆而非暴力计算的路径参考。  
3. 跨学科理论融合：展示了将拓扑学（如delta - 同调）、神经科学（Mountcastle通用皮层算法）与计算机科学（强化学习等）理论融合的范例，为跨学科研究智能提供方法论借鉴，推动不同领域理论在智能研究中的交叉应用。

## from-trial-and-error-to-improvement--a-systematic-analysis-of-llm-exploration-mechanisms-in-rlvr
### Abstract
Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.


## amft--aligning-llm-reasoners-by-meta-learning-the-optimal-imitation-exploration-balance
### Abstract
Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.


## gtpo-and-grpo-s--token-and-sequence-level-reward-shaping-with-policy-entropy
### Abstract
Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.
### 🌟 论文解读 | 用策略熵重塑奖励，突破长链推理瓶颈：GTPO与GRPO - S

### 📌 背景痛点/本文动机
近年来，强化学习（RL）推动大语言模型（LLM）推理能力进步，但像Group Relative Policy Optimization（GRPO）这类算法存在粗粒度信用分配问题——对序列中所有token施加统一奖励，在长链推理任务（如多步骤数学证明）中弊端明显。比如50步数学证明前49步正确、最后一步错误就给全序列零奖励，让正确步骤和错误步骤受同等惩罚，稀疏粗糙的反馈信号成了LLM长链推理能力提升的瓶颈。同时，现有模型对齐算法从复杂高成本向简单高效演进，GRPO虽简化流程却在信用分配精度上妥协，因此本文希望通过动态熵加权解决该问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Group Token Policy Optimization（GTPO）
核心是为每个token分配熵加权奖励实现细粒度信用分配。依据是正确推理序列中，模型策略熵高的token位置常对应推理关键决策点或不确定性时刻（如选择应用哪个数学定理），用token级熵动态加权奖励，能让策略梯度更新更聚焦对最终结果关键的时刻，在GRPO框架内首次实现真正的token级信用分配。

💡 创新点2：提出Sequence - Level Group Relative Policy Optimization（GRPO - S）
这是轻量级序列级方案，基于序列的平均token熵为每个序列分配熵加权奖励，在性能和计算效率间取得良好平衡。同时，在定义GTPO和GRPO - S目标函数时，提供多种选项以平衡训练效率和性能的权衡。此外，还从方差减少角度对目标函数设计进行理论分析，论证其收敛性。

### 📈 实验结果
在数学推理和代码生成基准测试中，GTPO和GRPO - S显著超越强基线（包括GRPO和DAPO等），验证了基于熵的信用分配方法的有效性，证明熵加权机制是性能提升的关键驱动力。

### 💬 可借鉴之处
1. 问题洞察角度：关注到现有算法在长链推理场景下信用分配的缺陷，将其定位为能力提升瓶颈，这种精准找问题的思路值得借鉴，在优化模型时要先深入分析现有方案短板。
2. 创新思路：利用推理中token的熵（不确定性）作为信用分配启发式信息，把模型内在不确定性转化为优化信号，为强化学习在LLM推理优化中的奖励设计提供了新视角——可关注模型生成过程中的内在特征（如熵）来细化奖励机制。
3. 方法拓展性：提出的GTPO和GRPO - S在GRPO基础上做不同粒度（token和序列级）的改进，且给出目标函数多选项平衡效率与性能，这种从不同层级优化且预留灵活性的设计思路，对后续算法优化有参考价值，比如在其他基于RL的模型优化任务中，可思考如何从不同粒度、不同内在特征维度去改进奖励或损失函数。

## light-if--endowing-llms-with-generalizable-reasoning-via-preview-and-self-checking-for-complex-instruction-following
### Abstract
While advancements in the reasoning abilities of LLMs have significantly
enhanced their performance in solving mathematical problems, coding tasks, and
general puzzles, their effectiveness in accurately adhering to instructions
remains inconsistent, particularly with more complex directives. Our
investigation identifies lazy reasoning during the thinking stage as the
primary factor contributing to poor instruction adherence. To mitigate this
issue, we propose a comprehensive framework designed to enable rigorous
reasoning processes involving preview and self-checking, essential for
satisfying strict instruction constraints. Specifically, we first generate
instructions with complex constraints and apply a filtering process to obtain
valid prompts, resulting in three distinct prompt datasets categorized as hard,
easy, and pass. Then, we employ rejection sampling on the pass prompts to
curate a small yet high-quality dataset, enabling a cold-start initialization
of the model and facilitating its adaptation to effective reasoning patterns.
Subsequently, we employ an entropy-preserving supervised fine-tuning
(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)
reinforcement learning guided by rule-based dense rewards. This approach
encourages the model to transform its reasoning mechanism, ultimately fostering
generalizable reasoning abilities that encompass preview and self-checking.
Extensive experiments conducted on instruction-following benchmarks demonstrate
remarkable performance improvements across various model scales. Notably, our
Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1
and closed-source models like Doubao-1.6.
### 🌟 论文解读 | Light-IF：让大模型在复杂指令遵循中具备可泛化推理能力

### 📌 背景痛点/本文动机
大语言模型（LLMs）在数学问题、编码任务等方面推理能力进步显著，但在准确遵循复杂指令时表现仍不一致。研究发现思考阶段的“惰性推理”是指令遵循效果差的主因，即模型面对复杂指令时仅简单重复指令，未真正检查是否合规。同时，现有提升复杂指令遵循能力的方法依赖大量有监督数据，数据收集难度大。此外，指令遵循是大模型从单纯下一个token预测器向实用可靠助手转变的关键能力，若模型无法准确遵循指令，在医疗、自动驾驶等真实领域应用会受限，所以提升复杂指令遵循能力至关重要。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建分层指令数据集  
先生成带复杂约束的指令，经筛选得到“难（hard）”“易（easy）”“通过（pass）”三类不同难度的有效提示数据集，为后续训练提供基础数据支撑。  

💡 创新点2：冷启动数据集构建  
对“pass”类提示采用拒绝采样，精心打造小而优质的数据集，实现模型冷启动初始化，助力模型适应有效推理模式，解决了依赖大规模数据的痛点，用小数据开启有效训练。  

💡 创新点3：结合Entropy - SFT与TEA - RL的训练策略  
采用保留熵的监督微调（Entropy - preserving supervised fine - tuning，Entropy - SFT）策略，搭配基于规则密集奖励引导的token级熵自适应强化学习（token - wise entropy - adaptive reinforcement learning，TEA - RL）。这种组合促使模型转变推理机制，培养出包含“预览（preview）”与“自检查（self - checking）”的可泛化推理能力，从训练策略层面推动模型推理能力升级。  

### 📈 实验结果
在指令遵循基准测试中开展大量实验，不同规模模型都有显著性能提升。其中Light - IF - 32B模型表现突出，超过了像DeepSeek - R1这样更大的开源模型以及Doubao - 1.6这类闭源模型，有力证明了方法的有效性。  

### 💬 可借鉴之处
1. 数据分层与筛选思路：在构建训练数据时，通过合理分层和筛选得到不同难度数据，能针对性地为模型训练提供梯度式数据资源，这种数据处理方式可推广到其他需要分难度训练的任务场景。  
2. 冷启动小数据训练：利用拒绝采样构建小而优的冷启动数据集，为数据稀缺场景下的模型初始化提供了新思路，避免过度依赖大规模数据收集。  
3. 训练策略组合创新：将保留熵的监督微调与token级自适应强化学习结合，为模型推理机制转变提供了有效技术路径，在提升模型特定能力（如指令遵循）的训练策略设计上具有参考价值，可启发后续针对不同能力提升的训练方法创新。

## decomposing-the-entropy-performance-exchange--the-missing-keys-to-unlocking-effective-reinforcement-learning
### Abstract
Recently, reinforcement learning with verifiable rewards (RLVR) has been
widely used for enhancing the reasoning abilities of large language models
(LLMs). A core challenge in RLVR involves managing the exchange between entropy
and performance of policies. Despite the importance of this exchange, a
fine-grained understanding of when and how this exchange operates most
effectively remains limited. To bridge this gap, we conduct a systematic
empirical analysis of the entropy-performance exchange mechanism of RLVR across
different levels of granularity. Specifically, we first divide the training
process into two distinct stages based on entropy dynamics, i.e., rising stage
and plateau stage, and then systematically investigate how this mechanism
varies across stage-level, instance-level, and token-level granularitiess. Our
analysis reveals that, in the rising stage, entropy reduction in negative
samples facilitates the learning of effective reasoning patterns, which in turn
drives rapid performance gains. Moreover, in the plateau stage, learning
efficiency strongly correlates with high-entropy tokens present in
low-perplexity samples and those located at the end of sequences. Motivated by
these findings, we propose two methods that dynamically adjust the reward
signal using perplexity and positional information to focus RL updates on
tokens that exhibit high learning potential, achieving improvements compared to
the baseline methods on various LLMs.
### 🌟 论文解读 | 拆解熵与性能交换：解锁高效强化学习的关键密码

### 📌 背景痛点/本文动机
强化学习结合可验证奖励（RLVR）在提升大语言模型（LLMs）推理能力方面应用广泛，但其中策略的熵与性能之间的交换机制缺乏细粒度理解。现有研究对熵 - 性能权衡的分析粒度较粗，将RLVR训练视为单一过程，未深入探究训练过程中熵动态与模型性能的交互。为填补这一空白，论文对RLVR中熵 - 性能交换机制在不同粒度下展开系统实证分析。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：分阶段分析训练过程
将RLVR训练过程依据熵动态分为上升阶段和平台阶段。在上升阶段，负样本的熵减有助于模型建立有效推理模式；平台阶段则聚焦高熵 tokens 实现缓慢但稳定的性能提升，明确不同阶段性能提升机制差异。
💡 创新点2：多粒度机制探究
从阶段、实例、token 三个粒度系统研究熵 - 性能交换机制。实例粒度下发现熵显著变化的 tokens 多来自低困惑度响应；token 粒度下揭示响应开头高熵 tokens 助力探索、结尾 tokens 承载任务特定信息辅助最终决策，明确不同 tokens 学习潜力差异。
💡 创新点3：基于发现的奖励塑造方法
依据上述实证发现，提出两种奖励塑造方法。利用困惑度和位置信息动态调整 token 优势，引导模型更新聚焦高学习潜力 tokens，释放RLVR潜力。

### 📈 实验结果
论文通过对不同粒度下熵 - 性能交换机制的分析，验证了各阶段、实例、token 层面的现象。基于提出的奖励塑造方法，在多种LLMs和推理基准测试中，相比基线方法实现了性能提升，证明了方法的有效性。

### 💬 可借鉴之处
论文采用多粒度分析思路，为复杂训练过程机制研究提供了范式，可借鉴于其他涉及动态过程与性能关联的研究；提出的基于困惑度和位置信息的奖励塑造方法，为优化强化学习在大模型推理能力提升中的应用提供了新的调节思路，启发后续针对模型不同部分、不同阶段设计更精细的训练策略。

## mol-rl--distilling-multi-step-environmental-feedback-into-llms-for-feedback-independent-reasoning
### Abstract
Large language models (LLMs) face significant challenges in effectively
leveraging sequential environmental feedback (EF) signals, such as natural
language evaluations, for feedback-independent chain-of-thought (CoT)
reasoning. Existing approaches either convert EF into scalar rewards, losing
rich contextual information, or employ refinement datasets, failing to exploit
the multi-step and discrete nature of EF interactions. To address these
limitations, we propose MoL-RL, a novel training paradigm that integrates
multi-step EF signals into LLMs through a dual-objective optimization
framework. Our method combines MoL (Mixture-of-Losses) continual training,
which decouples domain-specific EF signals (optimized via cross-entropy loss)
and general language capabilities (preserved via Kullback-Leibler divergence),
with GRPO-based post-training to distill sequential EF interactions into
single-step inferences. This synergy enables robust feedback-independent
reasoning without relying on external feedback loops. Experimental results on
mathematical reasoning (MATH-500, AIME24/AIME25) and code generation
(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art
performance with the Qwen3-8B model, while maintaining strong generalization
across model scales (Qwen3-4B). This work provides a promising approach for
leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities
in diverse domains.
### 🌟 论文解读 | MoL - RL：将多步环境反馈注入大模型，实现无反馈独立推理

### 📌 背景痛点/本文动机
大语言模型（LLMs）在有效利用顺序环境反馈（EF）信号进行无反馈的思维链（CoT）推理时面临重大挑战。现有方法要么将EF转化为标量奖励，丢失丰富上下文信息；要么采用精炼数据集，未能利用EF交互的多步和离散特性。为解决这些局限，本文提出MoL - RL训练范式。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出MoL - RL训练范式，通过双目标优化框架整合多步EF信号到LLMs中。采用MoL（Mixture - of - Losses）持续训练，解耦特定领域EF信号（通过交叉熵损失优化）和通用语言能力（通过Kullback - Leibler散度保留），防止通用能力灾难性遗忘同时吸收特定领域上下文知识。
💡 创新点2：结合基于GRPO的后训练，将顺序EF交互提炼为单步推理。在持续预训练后，实施Group Relative Policy Optimization（GRPO）强化学习算法，协同激活模型固有通用能力和压缩后的EF知识，诱导出无反馈的推理能力。

### 📈 实验结果
在数学推理（MATH - 500、AIME24/AIME25）和代码生成（CodeAgent - Test）基准测试中，MoL - RL在Qwen3 - 8B模型上实现了最先进的性能，同时在模型规模（Qwen3 - 4B）上保持强泛化性。消融研究表明，通过MoL持续训练对EF的顺序整合显著增强了模型对RL的响应性，MoL训练步骤与RL阶段奖励改进潜力呈正相关。

### 💬 可借鉴之处
本文提出的MoL - RL为智能体 - 环境系统中的持续学习建立了可推广的范式，为利用多步文本反馈增强LLMs在不同领域的推理能力提供了有前景的方法，在处理多步交互反馈、防止模型能力遗忘以及强化学习与大模型结合提升推理能力等方面都提供了新的思路和实践方向，可用于指导后续大模型在复杂反馈场景下的训练优化工作。

## the-policy-cliff--a-theoretical-analysis-of-reward-policy-maps-in-large-language-models
### Abstract
Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.
### 🌟 论文解读 | 大语言模型中奖励-策略映射的理论分析：揭秘“策略悬崖”

### 📌 背景痛点/本文动机
强化学习（RL）在塑造大语言与推理模型（LLMs/LRMs）行为方面至关重要，但常产生脆弱且不稳定的策略，引发如虚假推理、欺骗性对齐、指令不服从等问题，损害模型可信性与安全性。目前这些问题缺乏统一理论解释，多靠临时启发式方法应对。因此，论文旨在构建严谨数学框架分析从奖励函数到最优策略映射的稳定性，推进策略稳定性分析从经验启发式向原理性理论发展，为设计更安全可信AI系统提供洞见。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建奖励 - 策略映射稳定性分析框架  
提出严谨数学框架分析奖励函数到最优策略映射的稳定性，指出策略脆弱性常源于最优动作不唯一（推理任务中多有效轨迹时常见情况），以此统一解释诸多看似不同的失败案例，将其重构为奖励优化（奖励可能不完整或含噪声，尤其动作退化时）的合理结果。  

💡 创新点2：从单奖励到多奖励RL的拓展分析  
将分析从基础单奖励场景拓展到更现实的多领域多奖励RL场景，揭示稳定性由“有效奖励”聚合机制主导，阐释不同领域下多奖励如何影响策略稳定性。  

💡 创新点3：熵正则化对策略稳定性的作用证明  
证明熵正则化能以增加随机性为代价恢复策略稳定性，为缓解策略不稳定提供理论依据与方法方向。  

### 📈 实验结果
通过多奖励RL中的扰动实验验证框架有效性，且该框架能统一解释如欺骗性推理、指令遵循权衡、RLHF诱导诡辩等近期实证发现，在理论与实践结合层面展现说服力，支撑了从单奖励到多奖励场景下对策略稳定性相关现象的分析与解释。  

### 💬 可借鉴之处
论文为大语言模型强化学习训练中策略稳定性问题提供了原理性理论基础，让开发者理解策略脆弱、不稳定根源（如动作退化下的奖励优化），可指导设计更合理奖励函数（如利用“有效奖励”聚合、添加熵正则化等）来缓解虚假推理、指令不服从等问题；同时启发后续研究从理论层面深入探索大模型强化学习对齐与推理能力训练的稳定性机制，推动AI系统向更安全可信方向发展，减少依赖临时启发式调参的现状，走向更具原则性的设计路径。 

## agentic-reinforced-policy-optimization
### Abstract
Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO
### 🌟 论文解读 | 面向多轮工具交互的智能体强化策略优化：ARPO 如何突破 LLM 训练瓶颈？

### 📌 背景痛点/本文动机
近年来，基于可验证奖励的大规模强化学习（RLVR）在单轮推理任务中展现出释放大语言模型（LLM）潜力的能力。但在真实开放的推理场景中，LLM 需兼具长程推理能力与多轮工具交互能力，而现有强化学习（RL）算法难以平衡这两者。当前智能体强化学习多采用轨迹级算法（如 GRPO、DAPO），这类方法侧重完整工具使用轨迹采样并基于最终输出给奖励，却忽视了 LLM 与工具环境多轮交互中的细粒度行为探索。此外，研究发现 LLM 在每次工具调用反馈后生成的初始 token 熵值会显著升高（即不确定性大增），但现有轨迹级方法未充分利用这一特性去探索工具使用后的高不确定性步骤行为，限制了 LLM 智能体潜力的发挥。因此，亟需设计适配智能体 - 环境交互特性的 RL 算法来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点 1：基于熵的自适应 rollout 机制  
ARPO 提出一种融合全局与局部采样视角的熵基自适应 rollout 机制。在 rollout 阶段，LLM 先进行多次全局采样并记录各样本初始熵分布；每次工具调用后，监测实时 token 熵变并将其作为分支依据，若熵变超预设阈值，模型执行额外局部采样以探索更多样化的工具集成推理行为。该机制让 ARPO 在平衡全局与步骤级工具使用行为学习的同时，有效扩展原始采样空间，针对性探索工具使用后高不确定性步骤的行为。  

💡 创新点 2：优势归因估计（Advantage Attribution Estimation）  
为充分利用自适应采样优势，ARPO 引入优势归因估计。具体探索了硬、软优势设置，给同一源推理路径上的 token 分配共享优势值，分支路径上的 token 分配不同优势值，助力 LLM 在内化逐步工具使用交互中的优势差异，进而优化策略学习。  

💡 创新点 3：理论与实践结合的算法验证  
除了启发式动机，论文还从理论层面论证了 ARPO 在 LLM 智能体训练中应用的合理性，为算法有效性提供更坚实支撑，区别于仅靠实验验证的多数工作。  


### 📈 实验结果
论文在计算推理、知识推理、深度搜索三大领域的 13 个具有挑战性的基准测试集上开展实验。结果显示：  
- ARPO 在智能体训练中持续超越传统轨迹级 RL 算法；  
- 仅用现有轨迹级 RL 方法一半的工具调用预算，就能实现性能提升，在精度与效率间达成更优平衡；  
- 进一步的扩展性分析验证了 ARPO 以可扩展方式增强 LLM 智能体推理能力的潜力。  


### 💬 可借鉴之处
1. 行为量化视角：通过量化 LLM 智能体推理时的 token 熵变，揭示轨迹级 RL 算法在对齐 LLM 智能体上的固有局限，为后续算法改进提供了基于“熵 - 不确定性”关联的分析思路，启发研究者关注细粒度行为特征对训练的影响。  
2. 自适应采样范式：ARPO 的熵基自适应 rollout 机制展示了如何利用模型行为中的动态特征（如熵变）来设计自适应采样策略，为处理具有环境反馈不确定性的多轮交互任务提供了可参考的采样优化范式。  
3. 优势内化思路：优势归因估计机制为多步骤、多分支的交互场景下，如何让模型学习步骤间优势差异提供了方法参考，可迁移到需逐步决策、多轮交互的智能体训练任务中。  
4. 效率 - 性能平衡：在降低工具使用预算同时提升性能，证明了算法在资源受限场景下的实用性，为工业界落地 LLM 智能体训练提供了高效方案借鉴方向。  

## ulorl-an-ultra-long-output-reinforcement-learning-approach-for-advancing-large-language-models--reasoning-abilities
### Abstract
Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.
### 🌟 论文解读 | UloRL：突破超长输出瓶颈，强化大模型推理能力

### 📌 背景痛点/本文动机
大语言模型（LLMs）推理能力的提升常依赖带可验证奖励的强化学习（RLVR），但传统强化学习（RL）框架在处理超长输出时存在两大痛点：一是长序列长度的长尾分布，导致训练时需等批次所有样本解码完成，长尾样本易成为训练瓶颈，效率低下；二是训练中出现的熵坍缩问题，模型多样性过早流失，性能受限。为解决这些挑战，腾讯混元团队提出面向超长输出的强化学习方法UloRL，助力大模型推理能力进阶。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：分段滚动输出（Segment Rollout）  
将超长输出的解码过程拆分为多个短片段，每一步仅解码更短的片段。已完成解码的样本可立即进入经验池用于训练，未完成的后续迭代继续解码。此方式规避了长尾样本带来的不必要延迟，加速训练同时高效利用计算资源。还配套提出Segment - Aware Importance Sampling（SAIS）和Pesudo On - Policy Importance Sampling（POIS），适配分段滚动场景下的重要性采样机制，保障训练动态稳定。  

💡 创新点2：已掌握正 tokens 动态掩码（DMMPTs）  
熵坍缩源于模型对已掌握正 tokens（MPTs，即模型能高置信度预测的 tokens）过拟合。DMMPTs 依据模型当前熵值自适应控制 MPTs 训练：当熵低于阈值，掩码 MPTs 使其不参与训练；否则全量 token 参与。该策略既不引入额外优化目标，也不依赖重要性采样，规避了现有方法局限，实验表明能让模型训练中熵稳定。  

💡 创新点3：生成式验证器与数据处理  
引入生成式验证器模型提升 RL 训练奖励计算准确性，相较传统基于规则的方法，它借生成能力判断预测与参考答案等价性。同时重视数据清洗与转换，如过滤噪声数据、处理多子问题题目、规范格式与简化参考答等，保障奖励信号质量。  

### 📈 实验结果
在 Qwen3 - 30B - A3B 模型上验证方法有效性：  
- 训练效率：采用分段滚动的 RL 训练速度提升 2.06 倍；  
- 推理性能：128k token 输出的 RL 训练后，AIME2025 任务上性能从 70.9% 提升至 85.1%，BeyondAIME 从 50.7% 提升至 61.9%，甚至超过 Qwen3 - 235B - A22B 模型（AIME2025 为 81.5%、BeyondAIME 为 59.0%），涨幅显著。  

### 💬 可借鉴之处
1. 超长序列训练优化：分段处理思路为长文本生成类任务的 RL 训练提供了高效执行范式，可迁移到需长输出的代码生成、数学推理等场景；  
2. 熵坍缩解决：DMMPTs 从 token 选择角度动态调控训练，不额外增加优化目标的思路，为平衡模型探索与利用、避免过拟合提供新视角；  
3. 奖励机制完善：生成式验证器结合数据治理的方式，对依赖奖励信号的 RL 训练（如 RLHF 等）中提升奖励准确性有参考价值；  
4. 开源生态：团队计划开源代码与模型，利于社区基于此进一步探索超长输出下大模型能力提升，推动领域发展。

## skill-learning-via-policy-diversity-yields-identifiable-representations-for-reinforcement-learning
### Abstract
Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.
### 🌟 论文解读 | 强化学习中基于策略多样性的技能学习如何实现可识别表征

### 📌 背景痛点/本文动机
强化学习（RL）领域面临稀疏奖励下学习、环境探索及奖励函数设计等挑战，自监督特征学习与预训练方法常依赖互信息技能学习（MISL）这类信息论原理，但MISL中表征和互信息参数化的理论作用尚未明晰。同时，自监督学习（SSL）与MISL的关联为探究该问题提供了路径，而可识别性结果对部分可观测马尔可夫决策过程（POMDP）中从观测推断真实状态至关重要，因此本文从可识别表征学习视角探究MISL，聚焦对比后继特征（CSF）方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：揭示MISL成功的原理  
通过分析对比后继特征（CSF）方法，阐释MISL的成功源于**多样策略下的互信息估计与内积模型参数化的相互作用**。证明了CSF能在特征内积参数化和判别性技能多样性的条件下，以线性变换的精度恢复环境的真实特征，给出了强化学习表征学习领域首个可识别性保证，解释了为何之前一些方法的目标和参数化表现更优。  
💡 创新点2：量化多样策略并指明方法局限与实践建议  
借鉴独立成分分析（ICA）和因果表征学习（CRL）中的多样性与变异性假设，**形式化定义了多样策略**，分析特征维度和技能空间覆盖的作用，指出如最大熵策略在技能学习中并非最优等先前方法的局限，进而给出实践层面的建议。  

### 📈 实验结果
在MuJoCo和DeepMind Control环境中，针对基于状态和像素的场景验证理论主张，展示了CSF能够在理论上从状态和像素中恢复真实特征，实证了方法在特征可识别性方面的有效性。  

### 💬 可借鉴之处
从理论层面，为理解MISL方法中互信息目标、熵正则化的影响等提供了新视角，帮助解释不同设计选择的优劣；从实践层面，明确了多样策略的定义及特征、技能空间等要素的作用，为后续强化学习中自监督表征学习方法设计提供了指导，比如在技能发现和探索类任务中如何设计策略与模型参数化方式等。 

## perception-aware-policy-optimization-for-multimodal-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose PAPO, a novel policy
gradient algorithm that encourages the model to learn to perceive while
learning to reason. Specifically, we introduce the Implicit Perception Loss in
the form of a KL divergence term, which can be seamlessly plugged into
mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely
on additional data curation, reward models, or stronger teacher models. To
further enhance the training stability of PAPO, we introduce the Double Entropy
Loss, which effectively regularizes the new KL objective without compromising
performance. Despite its simplicity, PAPO yields significant overall
improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements
are more pronounced, approaching 8.0%-19.1%, on tasks with high vision
dependency. We also observe a substantial reduction of 30.5% in perception
errors, indicating improved perceptual capabilities with PAPO. Overall, our
work introduces a deeper integration of perception-aware supervision into core
learning objectives and lays the groundwork for a new RL framework that
encourages visually grounded reasoning. Code and data will be made publicly
available for research purposes. Project page:
https://mikewangwzhl.github.io/PAPO.
### 🌟 论文解读 | 感知感知策略优化助力多模态推理新突破

### 📌 背景痛点/本文动机
大语言模型（LLMs）领域中，基于可验证奖励的强化学习（RLVR）在赋予模型强大多步推理能力上成效显著，但现有设计和优化多针对纯文本领域，应用于多模态推理任务时表现欠佳。当前多模态推理中，视觉输入感知是主要错误来源之一。此前多数针对多模态推理的RLVR相关工作，或聚焦数据与rollout质量、奖励设计，或算法修改照搬纯文本模型经验，未从多模态视角重新思考。为解决这些问题，本文提出PAPO算法提升多模态推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出PAPO算法
PAPO是GRPO的创新性扩展，旨在让模型在学习推理同时学习感知，且完全依赖内部监督信号，无需额外数据整理、外部奖励模型或专有模型。引入以KL散度形式呈现的隐式感知损失（Implicit Perception Loss）到GRPO目标中，通过计算模型在原始图像和被破坏图像（如随机遮盖部分图像块得到的图像）条件下输出的KL散度来实现，以此激励模型利用有信息的视觉内容生成响应。

💡 创新点2：引入双熵损失（Double Entropy Loss）
由于KL目标的无界性，若隐式感知损失系数设置过高，PAPO可能过度优化KLprcp项引发“目标黑客攻击”问题，导致奖励骤降。为此引入双熵损失，在不影响性能前提下有效正则化新的KL目标，缓解上述问题，提升训练稳定性。

### 📈 实验结果
在八个多模态推理基准测试中，PAPO相比GRPO实现持续改进，平均提升4.4%；在高度依赖视觉基础且输入问题视觉线索少的任务中，提升效果更显著，达8.0%；结合移除参考KL时，在3B规模模型上改进可达11.2%；感知相关错误减少30.5%；且PAPO收敛更快，早期约25步左右就开始有性能增益。

### 💬 可借鉴之处
从多模态视角重新思考强化学习算法设计，为解决多模态推理中感知瓶颈提供新思路；提出的隐式感知损失无需额外复杂组件，简单有效易整合到主流RLVR算法（如GRPO、DAPO）中；针对KL目标潜在问题提出的双熵损失，为处理类似无界目标优化时的训练稳定性问题提供了参考方法；整体工作将感知感知监督更深入地融入RLVR学习目标，为鼓励视觉基础推理的新强化学习框架奠定基础，后续在多模态模型优化等方向可借鉴其思路来提升模型对视觉输入的有效利用与推理能力。 

## skywork-r1v3-technical-report
### Abstract
We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.
### 🌟 论文解读 | Skywork-R1V3：开源多模态推理的突破性进展

### 📌 背景痛点/本文动机
视觉-语言模型（VLMs）作为通用人工智能的关键范式，在多领域取得进展，但与闭源模型相比，开源VLMs在复杂视觉推理等任务上存在差距，且多模态数据异质性带来训练推理瓶颈，视觉-语言标注稀缺不均也加剧了开源与闭源VLMs的能力鸿沟。同时，现有开源VLMs推理能力和结构化思维不足，因此推进开源VLMs在复杂视觉推理任务的发展至关重要，本文旨在通过创新方法提升开源VLMs性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：精巧的RL后训练框架  
提出精心设计的后训练强化学习（RL）框架，无需额外持续预训练，就能有效激活和增强模型推理能力，实现从纯文本大语言模型（LLMs）到视觉任务的推理技能迁移，为模型推理能力提升提供有力支撑。  

💡 创新点2：关键推理token熵的能力指标  
引入独特的推理能力指标——关键推理token的熵，在RL训练阶段，通过监测关键推理起始点的熵值，区分真正有推理能力和仅模仿推理模式的模型，该指标与验证集实际推理性能强相关，为RL训练中高质量检查点选择提供高效方法。  

💡 创新点3：揭示connector模块核心作用  
在强化学习阶段进一步明确VLMs中connector模块对维持跨模态对齐的关键作用，还发现RL后仅对connector调优是在不损害推理能力前提下重新平衡模型知识分布的有效策略，深入探究了多模态推理中跨模态对齐的实现机制。  

💡 创新点4：多方面训练策略探索  
涵盖冷启动微调（利用早期Skywork R1V2构建冷启动数据集，将语言推理模型推理模式迁移到视觉-语言模型）、课程学习在强化学习中的应用、强化学习技巧（如Clip - Higher和Dynamic Sampling）复现等多方面训练策略探索，完善了VLMs后训练方法体系。  

### 📈 实验结果
Skywork - R1V3在MMMU基准测试中取得76.0%的当前最优结果，性能匹配入门级人类专家能力；其38B参数模型借助RL驱动的后训练方法，能与顶级闭源VLMs媲美；且成功将数学推理迁移到其他学科相关推理任务，充分验证了模型在多模态推理任务上的强大性能与迁移能力。  

### 💬 可借鉴之处
1. 后训练框架设计：其RL后训练框架无需额外预训练就能激活推理能力的思路，为提升模型能力提供了轻量高效的路径参考，可用于其他模型推理能力强化场景。  
2. 能力评估指标：关键推理token熵这一创新指标，为模型训练中评估推理能力、选择优质检查点提供了新的有效维度，在模型训练过程监控与优化方面具有借鉴价值。  
3. 模块作用探究：对connector模块在跨模态对齐中作用的深入研究，以及后续调优策略，为多模态模型中模块功能挖掘与优化提供了实践案例，助力多模态模型架构设计与优化。  
4. 多策略融合训练：冷启动微调、课程学习结合强化学习等多策略在VLMs后训练中的应用实践，为模型训练流程设计提供了丰富的策略组合参考，有助于提升模型训练效果与泛化能力。  
5. 推理行为分析：对视觉推理模型中记忆与泛化平衡、推理时快思考慢思考、思考token预算有效性、推理迁移 pipeline 中幻觉等方面的分析，为深入理解多模态模型推理机制、优化推理过程提供了研究方向与思路借鉴。

## self-guided-process-reward-optimization-with-redefined-step-wise-advantage-for-process-reinforcement-learning
### Abstract
Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.
### 🌟 论文解读 | 无额外开销！SPRO革新过程强化学习的奖励与优势估计

### 📌 背景痛点/本文动机
过程强化学习（PRL）在提升大语言模型（LLMs）推理能力上潜力巨大，但现有方法存在两大关键问题：一是引入额外过程奖励模型会带来高昂计算开销；二是缺乏统一的过程级优势估计理论框架。当前基于辅助过程奖励模型（PRMs）的方法，还存在训练难（人工标注或自动标注都难满足需求）、计算成本高（额外模型占显存影响训练效率）、在线强化学习中难扩展等缺陷。同时，像PRIME这类依赖辅助奖励模型的方法，不仅有计算开销，优势估计还易有偏差。因此，亟需一种既高效又能准确估计过程优势的方法，本文提出SPRO来解决这些痛点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：过程奖励自引导，摆脱额外模型依赖  
理论上证明过程奖励可从策略模型本身内在推导得出，无需额外训练过程奖励模型。这既免去了PRMs训练难、计算成本高的问题，又保留了结果监督RL算法（如GRPO）的简洁性与可扩展性，对工业落地友好。

💡 创新点2：重定义逐步优势，精准估计过程优势  
引入**累积过程奖励（CPR）**和**掩码逐步优势（MSA）**来重新定义逐步优势。CPR隐式聚合前缀序列中所有先前步骤的过程奖励，作为过程奖励的替代，能在每个时间步更准确估计期望回报；MSA则在共享提示采样组内严格执行每一步的比较，实现合理的逐步动作优势估计，让优势估计更贴合基于优势的策略梯度框架，减少偏差。

### 📈 实验结果
实验表明SPRO相比基线方法（如 vanilla GRPO）优势显著：训练效率提升3.4倍，测试准确率提升17.5%；训练全程能维持稳定且较高的策略熵，保证充分探索，同时平均响应长度减少约1/3，避免奖励黑客问题；且和GRPO这类结果监督RL方法相比，无额外计算开销，利于工业场景实施。

### 💬 可借鉴之处
1. 思路创新：打破“过程奖励必须依赖额外模型”的思维定式，证明从策略模型自身推导过程奖励的可行性，为后续过程强化学习算法设计开辟新方向。  
2. 优势估计范式：通过CPR和MSA重新定义逐步优势，提供了更严谨的过程级优势估计理论框架，后续研究在处理序列逐步奖励与优势时可参考该范式。  
3. 工业落地友好：在保证性能提升的同时控制计算开销，为工业界将过程强化学习用于提升大模型推理能力提供了高效且实用的方案参考。

## data-driven-exploration-for-a-class-of-continuous-time-indefinite-linear--quadratic-reinforcement-learning-problems
### Abstract
We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.
### 🌟 论文解读 | 连续时间不定线性二次强化学习的自适应数据驱动探索

### 📌 背景痛点/本文动机
线性 - 二次（LQ）控制是最优控制理论的基石，但传统研究多基于模型已知的范式。在实际应用中，模型参数往往难以完全知晓，基于模型的方法存在参数估计难、对参数敏感等问题。强化学习（RL）为未知模型参数的随机控制提供了解决思路，然而现有针对连续时间RL的探索策略多采用固定或确定性探索调度，存在需大量手动调参、缺乏适应性、收敛慢等缺陷。同时，不定LQ控制因目标函数中控制项不定，需特殊处理，传统基于模型方法依赖复杂的广义Riccati方程，而RL有望绕开此难题。本文针对连续时间随机不定LQ控制问题，旨在提出更高效的自适应探索机制。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出无模型、数据驱动的探索机制  
针对连续时间随机不定LQ控制问题，设计了能自适应调整评论家（critic）的熵正则化和行动者（actor）的策略方差的探索机制，不同于以往固定或确定性探索调度，该机制依据智能体学习进程动态调整，减少调参需求且提升学习效率。  

💡 创新点2：理论保证与 regret 界分析  
在灵活的自适应探索下，仍实现了与该类LQ问题已知最佳无模型结果匹配的次线性 regret 界（$O_p(N^{3/4})$），同时证明了自适应探索和策略参数的几乎必然收敛性，且无需非零初始状态假设，完全从数据中学习。  

### 📈 实验结果
通过数值实验验证理论结果，与非自适应无模型及基于模型的方法对比，自适应探索加速了收敛过程，在 regret 性能上表现更优。即便理论上与先前固定探索调度方法的 regret 界同阶，但数值实验显示本文自适应探索策略始终更出色，能实现更低的 regret。  

### 💬 可借鉴之处
对于连续时间RL尤其是LQ控制场景，提供了自适应探索的新思路，展示了数据驱动调整探索参数以提升效率的可行性；在理论分析上，为自适应探索机制下的收敛性和 regret 界分析提供了范例，可指导后续相关连续时间RL问题的理论研究；绕开复杂广义Riccati方程的RL解决不定LQ控制问题的思路，为模型未知时的不定控制问题提供了新范式，在工程、经济等依赖LQ控制的领域有潜在应用参考价值。

## eframe--deeper-reasoning-via-exploration-filter-replay-reinforcement-learning-framework
### Abstract
Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filter-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame not only enables
fine-grained categorization of training samples for deeper insight into their
contributions, but also introduces an efficient and precise mechanism for
entropy control, which is critical for balancing exploration and convergence in
RL training. Our code is available at https://github.com/597358816/EFRame.
### 🌟 论文解读 | EFRame：探索-过滤-回放框架助力大模型实现更深度推理

### 📌 背景痛点/本文动机
强化学习（RL）在大语言模型（LLMs）训练中作用关键，能增强模型推理能力。Group Relative Policy Optimization（GRPO）作为PPO高效变体，虽降低计算成本，但在复杂推理任务中存在探索能力有限、样本效率低和训练不稳定等问题，限制了其性能。比如RLVR依赖模型自身pass@k输出提取奖励信号，存在奖励信号消失导致模型难从数据中提取有用知识、探索受限等情况；GRPO也因这些问题在后期训练易不稳定。为解决这些局限，本文提出EFRame框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：探索 - 过滤 - 回放一体化框架设计
EFRame从三个关键维度系统增强GRPO。一是额外rollouts探索，针对常规rollouts难处理的有挑战prompt，新增rollout阶段进行针对性探索，生成少量高质量正样本和大量低质量负样本；二是在线过滤机制，丢弃低质量样本以降低训练方差、提升效率；三是经验回放，将高优势样本存入回放缓冲区并在训练中复用，放大其影响并增强模型稳定性，构建从探索到收敛的完整稳定学习周期。

💡 创新点2：样本类型感知训练
对训练样本进行有原则的分类，深入理解不同样本在学习中的独特作用。明确高优势、低概率正样本对触发探索和解锁深层知识至关重要，而低优势负样本易导致过早收敛到次优解需抑制，实现对样本贡献的细粒度认知。

💡 创新点3：精准高效的熵控制
通过联合调整额外rollout温度和回放样本数量实现熵控制，相比传统基于奖励的熵正则化，提供更灵活稳定的熵调节，支撑高效探索与稳定收敛。

### 📈 实验结果
在多种推理基准测试中验证了EFRame的有效性，如在具挑战性的Geometry3K基准上，比GRPO性能提升36.5%；在多模态和数学推理等广泛基准测试中也持续取得增益，展现出强泛化性与鲁棒性，证明其不仅提升训练鲁棒性和效率，还能让模型获得在原始GRPO下难以实现的更深度推理能力。

### 💬 可借鉴之处
1. 框架设计思路：针对已有方法局限，从探索、过滤、回放多维度系统构建框架解决问题，为强化学习中处理复杂任务时的框架设计提供了多维度协同优化的思路，可借鉴这种系统性增强已有算法的方式应对算法瓶颈。
2. 样本管理与分析：对训练样本进行细粒度分类并分析不同样本作用，这种关注样本在学习中角色的思路，有助于在其他模型训练任务中更精准利用数据，提升训练效果。
3. 熵控制方式：创新的熵控制机制，脱离传统仅基于奖励的熵正则化思路，通过联合调整参数实现更灵活稳定调节，为强化学习中平衡探索与收敛的熵控制提供了新的实用方法参考。 
4. 代码开源：提供了代码仓库（https://github.com/597358816/EFRame），方便研究者复现实验、学习方法细节并在此基础上进行改进拓展，利于相关领域研究的推进与交流。

## apo--enhancing-reasoning-ability-of-mllms-via-asymmetric-policy-optimization
### Abstract
Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.
```
### 🌟 论文解读 | APO：用非对称策略优化提升多模态大模型推理能力

### 📌 背景痛点/本文动机
多模态大语言模型（MLLMs）虽能整合多种模态数据，但在复杂推理任务上表现欠佳。强化学习（RL）虽能增强大语言模型（LLMs）推理能力，可应用到MLLMs时却面临难题：一是在通用任务上性能下降，二是易产生“过度思考”式的冗长推理。同时，KL惩罚与“过度思考”对MLLMs的RL训练影响机制尚不清晰。基于此，论文探索如何解决这些问题以提升MLLMs推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出非对称策略优化（APO）框架  
将采样得到的响应分为正样本和负样本两组，分别针对性优化。对正样本，设计难度自适应散度塑造（DADS）；对负样本，提出次优轨迹复杂度正则化（STCR），双管齐下提升MLLMs推理能力。  

💡 创新点2：Difficulty - Adaptive Divergence Shaping（DADS）  
针对正样本，依据样本难度动态调整KL散度权重。此方法避免策略熵骤降，提升训练稳定性，更高效利用样本，同时保留模型已有知识。解决了“加KL惩罚保通用能力但牺牲推理，去KL惩罚提推理但丢通用”的两难问题，实现推理与通用能力平衡。  

💡 创新点3：Suboptimal Trajectory Complexity Regularization（STCR）  
针对负样本，惩罚过长响应以缓解“过度思考”。让模型生成更简洁推理的同时，保留探索能力，减少推理中冗余重复信息干扰，避免错误推导。  

### 📈 实验结果
以Qwen2.5 - VL - 3B为基础模型，训练得到View - R1 - 3B。在推理基准测试中，比基础模型平均提升7%，还能超越7 - 11B规模的更大MLLMs。更关键的是，其他推理调优的MLLMs在通用任务上常性能下降，而View - R1 - 3B在通用任务上也保持性能提升，泛化能力更强，验证了DADS和STCR技术在推进MLLMs复杂多模态推理上的有效性与广泛适用性。  

### 💬 可借鉴之处
1. 问题分析角度：深入分析KL惩罚和“过度思考”对MLLMs RL训练的影响，为后续优化指明方向，这种对训练中关键因素的剖析思路值得借鉴。  
2. 样本分组优化：将样本分正负组分别设计优化策略，针对性解决不同问题，为处理复杂任务时的模型训练提供“分而治之”的思路。  
3. 动态与正则化技巧：DADS的动态权重调整、STCR的复杂度正则化，在平衡模型能力（如推理与通用、探索与简洁）方面提供了具体技术手段，可迁移到其他模型训练场景解决类似权衡问题。
```

## srft--a-single-stage-method-with-supervised-and-reinforcement-fine-tuning-for-reasoning
### Abstract
Large language models (LLMs) have achieved remarkable progress in reasoning
tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) remains a fundamental challenge. Through
comprehensive analysis of token distributions, learning dynamics, and
integration mechanisms from entropy-based perspectives, we reveal key
differences between these paradigms: SFT induces coarse-grained global changes
to LLM policy distributions, while RL performs fine-grained selective
optimizations, with entropy serving as a critical indicator of training
effectiveness. Building on these observations, we propose Supervised
Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both
fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach
simultaneously applies SFT and RL to directly optimize the LLM using
demonstrations and self-exploration rollouts rather than through two-stage
sequential methods. Extensive experiments show that SRFT achieves 59.1% average
accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning
benchmarks and 10.9% on three out-of-distribution benchmarks.
### 🌟 论文解读 | SRFT：单阶段融合监督与强化微调，突破大模型推理瓶颈

### 📌 背景痛点/本文动机
大语言模型（LLMs）在推理任务中取得显著进展，但如何**最优整合监督微调（SFT）与强化学习（RL）**仍是核心挑战。传统做法将SFT和RL作为独立、顺序的阶段：SFT虽能让模型学习指令遵循，但易导致模式记忆甚至过拟合，缺乏真正推理能力；RL虽擅长探索与奖励优化，却存在样本低效、大空间探索困难或模式坍塌等问题。此外，简单的“先SFT再RL”两阶段模式，难以平衡知识蒸馏与策略优化，要么整合不足引发误差传播，要么过度依赖演示导致探索受限。因此，亟需一种能单阶段统一SFT与RL、兼顾演示学习与策略探索的新方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：从熵视角剖析SFT与RL的本质差异  
通过对token分布、学习动态和整合机制的熵基分析，揭示两大范式核心区别：**SFT对LLM策略分布产生“粗粒度全局变化”**，侧重拟合演示数据；**RL则实现“细粒度选择性优化”**，聚焦策略邻域的探索。而“熵”可作为训练有效性的关键指标，反映训练过程中分布的不确定性与探索程度。  

💡 创新点2：提出单阶段方法SRFT（Supervised Reinforcement Fine-Tuning）  
SRFT通过**熵感知加权机制**统一SFT与RL：不再分阶段，而是同时应用SFT与RL，利用演示数据和自探索rollouts直接优化LLM。对LLM策略生成的样本，依奖励正负采用不同RL损失；对演示数据集样本，同时施加SFT与RL目标。这种设计让模型在多粒度下稳定学习演示，又能桥接SFT（知识传递）与RL（策略探索）的互补优势。  


### 📈 实验结果
在5个数学推理基准测试和3个分布外（OOD）基准测试上，基于Qwen - 2.5 - Math - 7B的SRFT平均准确率达59.1%：  
- 数学推理任务中，比无RL的方法高出9.0%；  
- OOD任务中，相对零RL基线提升10.9%；  
- 与其他用演示数据的方法相比，泛化能力平均提升超4.7%。  


### 💬 可借鉴之处
1. **分析视角创新**：从“熵”切入分析SFT与RL对策略分布、学习动态的影响，为理解两大微调范式提供了新的理论透镜，后续研究可借鉴这类“量化+机制解析”的分析思路。  
2. **单阶段范式突破**：打破“先SFT后RL”的固有思维，证明单阶段融合能更高效优化推理能力，为大模型微调流程设计提供了范式级启发。  
3. **工程落地价值**：SRFT在数学推理与OOD任务的显著增益，验证了其在复杂推理、泛化场景的实用性，对需强推理能力的LLM下游应用（如数学解题、逻辑推理类AI产品）有直接参考价值。

## confucius3-math--a-lightweight-high-performance-reasoning-llm-for-chinese-k-12-mathematics-learning
### Abstract
We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.
### 🌟 论文解读 | 网易有道开源Confucius3-Math：为中国K-12数学教育打造轻量高性能推理大模型

### 📌 背景痛点/本文动机
在大语言模型（LLM）推理能力成为研究热点的当下，OpenAI的o1等模型虽展现强大推理性能但未开源技术细节，且推理时计算（TTC）在LLM场景落地有挑战；同时，教育领域对模型准确性要求高，现有强推理LLM在K - 12任务表现不佳，且高性能LLM开发部署成本高加剧教育资源数字鸿沟。在此背景下，网易有道团队聚焦中国K - 12数学教育领域，旨在打造低成本、高性能、开源的推理大模型，助力教育普惠。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：Targeted Entropy Regularization（靶向熵正则化）
提出了一种全新的熵正则化方式，在强化学习（RL）训练过程中，通过该正则化手段能显著稳定训练过程，让模型训练更平稳，为模型性能提升提供支撑。

💡 创新点2：Recent Sample Recovery（近期样本恢复）
设计了新颖的数据调度策略，借助该策略可以更好地利用训练数据，提升数据使用效率，让模型在训练时能更充分地从数据中学习有效信息，进而助力模型性能提升。

💡 创新点3：Policy - Specific Hardness Weighting（特定策略难度加权）
改进了组相对优势估计器，通过考虑策略相关的难度加权，在RL训练中进一步优化训练效果，对提升模型性能起到积极作用。

此外，整体采用基于大规模强化学习的后训练方式，让模型能贴合国家课程标准，低成本地解决中国K - 12主流数学问题。

### 📈 实验结果
Confucius3 - Math在多个数学推理任务基准测试中表现卓越，达成SOTA性能。在如CK12 - MATH、GAOKAO - Bench (Math)、MathBench (K12)等一系列基准测试里，对比DeepSeek - R1、Qwen3 - 14B等模型，在很多任务上准确率领先，例如在部分测试中准确率超过90%甚至更高，且在目标领域（中国K - 12数学学习）表现突出，还能在单消费级GPU上高效运行，推理性能约为DeepSeek - R1的15倍，训练成本仅26000美元，有力证明了在特定领域低成本构建强推理模型的可行性。

### 💬 可借鉴之处
1. 领域聚焦与普惠思路：聚焦K - 12教育这类特定领域打造模型，关注教育普惠，为利用AI缩小教育资源差距提供了实践范例，启示后续研究可瞄准垂直领域做深度优化。
2. 技术创新复用：文中提出的Targeted Entropy Regularization、Recent Sample Recovery、Policy - Specific Hardness Weighting这三项技术创新，在强化学习训练大模型尤其是领域推理模型时，为稳定训练、提升数据效率和模型性能提供了新的技术思路和方法参考，后续相关方向研究可借鉴这些创新点来优化训练流程。
3. 低成本实践路径：展示了基于合适基础模型、适量高质量数据和良好训练方案，通过RL后训练在轻量模型中激发强推理能力的路径，证明了低成本构建特定领域强推理模型的可行性，为资源有限但想做领域模型的团队提供了实践参考。
4. 开源生态贡献：开源模型与代码，邀请社区参与，这种开放协作的模式有助于推动整个LLM推理模型在教育等领域的应用发展，为行业开源生态建设提供了积极范例。

## adapthink--adaptive-thinking-preferences-for-reasoning-language-model
### Abstract
Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.
### 🌟 论文解读 | AdapThink：让推理语言模型拥有自适应“思考偏好”

### 📌 背景痛点/本文动机
基于强化学习（RL）的后训练极大提升了语言模型的复杂推理能力，催生了精细的自我反思过程（即“慢思考”范式）。但该范式存在推理效率难题：模型面对简单问题可能过度计算，面对复杂问题却可能过早切换推理思路。以往机制依赖静态长度预算或预定义规则，缺乏对问题复杂度变化和模型能力演进的适应性。因此，需要一种自适应后训练框架，在维持推理性能的同时提升效率，AdapThink 应运而生。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：组相对奖励函数  
设计了一种新颖的组相对奖励函数来调整模型当前推理偏好。该函数借助生成响应的组内准确率，让模型学习确定合适的反思偏好；同时通过统计训练样本组中关键过渡词数量，定量衡量推理效率，无需依赖固定长度偏好，而是基于模型置信度和响应特征动态调整与反思相关的过渡词偏好。  

💡 创新点2：多样性感知采样机制  
提出多样性感知采样机制以平衡训练样本组的解题准确率与推理多样性。先对推理实例过采样，再用精心定义的多样性指标评估实例的最终答案和中间步骤，最后通过多样性感知下采样来筛选和提升用于 RL 后训练的实例整体质量。  


### 📈 实验结果
在多个数学推理数据集上，使用 DeepSeek 蒸馏模型进行实验。结果表明，AdapThink 在使模型具备自适应推理模式、缓解推理低效问题上展现出优势。例如，对上下文长度限制仅 2K token 的 DeepSeek 蒸馏 Qwen 模型进行后训练后，在 8K token 限制下测试时，相比多个长度控制基线方法表现更优，证明了其在打造强大且高效的思维链（CoT）模型上的有效性。

### 💬 可借鉴之处
1. 自适应思路：打破静态规则束缚，从模型置信度、响应特征等维度动态调整推理偏好，为处理“模型能力演进下的推理效率”问题提供了自适应思路参考。  
2. 组相对与多样性机制：组相对奖励函数从群体样本角度优化推理偏好，多样性感知采样兼顾准确率与过程多样性，这类从“群体层面 + 多维度平衡”的设计思路，可迁移到需考虑样本多样性、动态调整策略的其他 RL 后训练或生成任务场景中。  
3. 小模型高效推理：在小参数规模模型（如实验中 DeepSeek 蒸馏模型）上验证了提升推理效率与性能的可行性，为资源受限场景下打造高效推理模型提供了实践范例。

## reasoning-with-exploration--an-entropy-perspective-on-reinforcement-learning-for-llms
### Abstract
Balancing exploration and exploitation is a central goal in reinforcement
learning (RL). Despite recent advances in enhancing large language model (LLM)
reasoning, most methods lean toward exploitation, and increasingly encounter
performance plateaus. In this work, we revisit entropy -- a signal of
exploration in RL -- and examine its relationship to exploratory reasoning in
LLMs. Through empirical analysis, we uncover positive correlations between
high-entropy regions and three types of exploratory reasoning actions: (1)
pivotal tokens that determine or connect logical steps, (2) reflective actions
such as self-verification and correction, and (3) rare behaviors under-explored
by the base LLMs. Motivated by this, we introduce a minimal modification to
standard RL with only one line of code: augmenting the advantage function with
an entropy-based term. Unlike traditional maximum-entropy methods which
encourage exploration by promoting uncertainty, we encourage exploration by
promoting longer and deeper reasoning chains. Notably, our method achieves
significant gains on the Pass@K metric -- an upper-bound estimator of LLM
reasoning capabilities -- even when evaluated with extremely large K values,
pushing the boundaries of LLM reasoning.
### 🌟 论文解读 | 从熵视角探索大模型推理：强化学习中平衡探索与利用的新路径

### 📌 背景痛点/本文动机
在强化学习（RL）里，平衡探索（exploration）与利用（exploitation）是核心目标。当下提升大语言模型（LLM）推理能力的方法大多偏向“利用”，容易陷入性能瓶颈——模型会收敛到狭窄且过度优化的行为，失去探索其他策略的动力，进而削弱多步推理能力。传统RL中，“熵”是衡量探索的关键指标（反映策略动作分布的不确定性），受此启发，论文探究了熵与LLM探索性推理的关联，试图用熵来引导LLM更高效地探索推理路径。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：揭示熵与探索性推理的强关联  
通过实证分析发现，高熵区域和三类探索性推理行为高度相关：  
- 决定或连接逻辑步骤的**关键token**（如first、because、however等逻辑连接词）；  
- 自我验证、纠错等**反思性动作**；  
- 基座LLM中未充分探索的**稀有行为**。  
这些发现证明“熵”可作为识别LLM探索性推理行为的有效信号。  

💡 创新点2：极简RL改进：熵辅助优势函数  
基于上述发现，论文对标准RL仅做“一行代码级”的极简修改——**在优势函数中加入基于熵的项**。和传统“最大熵方法靠提升不确定性来鼓励探索”不同，该方法通过促进“更长、更深的推理链”来鼓励探索。具体来说，给PPO/GRPO等算法的优势函数引入“截断且梯度分离的熵项”：截断保证熵项不会主导或反转原始优势的符号，梯度分离则保留原始优化方向。这样既放大了不确定性下的探索性推理行为，又维持了原策略梯度流；且熵项会随置信度自然衰减，训练前期鼓励探索、后期避免过度探索。  


### 📈 实验结果
在主流RLVR算法（GRPO、PPO）上验证方法有效性：  
- 增强探索性推理行为：关键token使用、反思动作等显著增加；  
- 生成更长更具探索性的回复，且不提升重复率，支持连贯多步推理；  
- Pass@1精度在不同基准上持续提升；  
- 评估“Pass@K（推理能力的上界估计指标）”时，即便K极大，方法仍实现显著增益，突破了LLM推理的边界（如图1所示，熵辅助的PPO/GRPO在大K值下Pass@K表现远超基线）。  


### 💬 可借鉴之处
1. **视角创新**：把传统RL中“熵衡量探索”的思路迁移到LLM推理场景，实证分析熵与推理行为的关联，为后续研究提供了“用RL中探索指标赋能LLM推理”的新视角；  
2. **极简有效**：仅需修改优势函数这一“一行代码级”改动，就能无缝融入现有RLVR训练 pipeline，工程落地成本低、易复现；  
3. **指标拓展**：用Pass@K评估推理能力上界，为衡量LLM复杂推理提供了更具挑战性的测试维度，也验证了方法在“极限探索推理”场景的潜力。  


这篇论文从“熵”这一经典RL概念切入，为LLM推理的“探索-利用”平衡难题提供了简洁且有效的解法，无论是理论关联的挖掘还是工程实现的轻量化，都为大模型推理能力提升开辟了新方向~

## ring-lite--scalable-reasoning-via-c3po-stabilized-reinforcement-learning-for-llms
### Abstract
We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model
optimized via reinforcement learning (RL) to achieve efficient and robust
reasoning capabilities. Built upon the publicly available Ling-lite model, a
16.8 billion parameter model with 2.75 billion activated parameters, our
approach matches the performance of state-of-the-art (SOTA) small-scale
reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,
GPQA-Diamond) while activating only one-third of the parameters required by
comparable models. To accomplish this, we introduce a joint training pipeline
integrating distillation with RL, revealing undocumented challenges in MoE RL
training. First, we identify optimization instability during RL training, and
we propose Constrained Contextual Computation Policy Optimization(C3PO), a
novel approach that enhances training stability and improves computational
throughput via algorithm-system co-design methodology. Second, we empirically
demonstrate that selecting distillation checkpoints based on entropy loss for
RL training, rather than validation metrics, yields superior
performance-efficiency trade-offs in subsequent RL training. Finally, we
develop a two-stage training paradigm to harmonize multi-domain data
integration, addressing domain conflicts that arise in training with mixed
dataset. We will release the model, dataset, and code.
### 🌟 论文解读 | Ring - lite：基于C3PO强化学习的高效可扩展推理MoE大模型

### 📌 背景痛点/本文动机
大模型在复杂推理任务上展现出潜力，但像OpenAI - O1系列等模型训练细节未公开，形成知识鸿沟。现有研究多聚焦特定领域（如数学或代码生成）且在密集模型架构，对混合专家（MoE）范式关注少，MoE架构下RL训练存在稳定性难题（如梯度同步、参数更新冲突致训练不稳定），限制MoE优势发挥。同时，多领域数据训练易现冲突，缺乏有效应对方法。为此，团队推出Ring - lite来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：开源多领域MoE推理模型及全流程  
首次开源涵盖基础设施、训练方法和数据集的多领域MoE推理模型，详细公开包括Long - CoT监督微调（SFT）和推理特定强化学习（RL）的透明训练 pipeline，为研究社区提供全面资源。

💡 创新点2：提出C3PO解决训练不稳定  
识别推理模型训练不稳定问题后，提出Constrained Contextual Computation Policy Optimization（C3PO）框架。通过固定训练token大小（预算）消除响应长度差异，选择高熵基模型稳定学习动态；该框架基于算法 - 工程协同设计范式整合强化学习方法，保障长期训练稳定同时提升计算效率。

💡 创新点3：两阶段训练应对多领域数据冲突  
在数学、代码和科学等多领域发现域间数据冲突，引入能力整合方法（阶段式训练和平衡数据混合）来解决多领域数据训练时的冲突问题，实现跨域能力融合。

💡 创新点4：基于熵损失选蒸馏 checkpoint  
实证表明，RL训练时依据熵损失而非验证指标选择蒸馏检查点，在后续RL训练中能产生更优的性能 - 效率权衡。

### 📈 实验结果
Ring - lite（总参数168亿，激活参数27.5亿）在多个基准测试中表现优异：数学竞赛类基准AIME2024和AIME2025分别获76.61%和69.11%；代码竞赛基准LiveCodeBench和Codeforces分别获60.66%和86.45%；研究生级科学QA基准GPQA - diamond获61.05%。在性能上匹配或超越10B参数内的密集模型，平均表现超过Qwen3 - 8B，逼近其他顶级推理模型，且仅激活可比模型三分之一参数就能达到SOTA小尺度推理模型性能。

### 💬 可借鉴之处
1. 开源生态构建：为AI研究社区提供从模型到数据、代码的全栈开源资源，降低多领域推理研究门槛，推动领域发展。
2. 训练稳定性优化：C3PO框架为MoE架构下RL训练稳定性提供新思路，算法 - 系统协同设计思路可借鉴到其他大模型训练优化场景。
3. 多域数据处理：两阶段训练和基于熵损失选 checkpoint 等方法，为多领域数据融合训练、蒸馏与RL结合等提供实践参考，助力解决复杂场景下模型训练难题。

## unsupervised-skill-discovery-through-skill-regions-differentiation
### Abstract
Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors
that can accelerate the learning of downstream tasks. Previous methods
typically focus on entropy-based exploration or empowerment-driven skill
learning. However, entropy-based exploration struggles in large-scale state
spaces (e.g., images), and empowerment-based methods with Mutual Information
(MI) estimations have limitations in state exploration. To address these
challenges, we propose a novel skill discovery objective that maximizes the
deviation of the state density of one skill from the explored regions of other
skills, encouraging inter-skill state diversity similar to the initial MI
objective. For state-density estimation, we construct a novel conditional
autoencoder with soft modularization for different skill policies in
high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we
formulate an intrinsic reward based on the learned autoencoder that resembles
count-based exploration in a compact latent space. Through extensive
experiments in challenging state and image-based tasks, we find our method
learns meaningful skills and achieves superior performance in various
downstream tasks.
### 🌟 论文解读 | 无监督技能发现新突破：基于技能区域分化的方法

### 📌 背景痛点/本文动机
强化学习（RL）在诸多领域取得成功，但传统RL依赖精心设计的奖励函数，设计成本高且泛化性有限。无监督强化学习（Unsupervised RL）旨在无外部奖励时学习有用行为，以快速适应新任务。现有方法分基于赋能（empowerment）的技能发现和纯探索方法：基于赋能方法在大规模状态空间（如图像）中状态覆盖有限；纯探索易产生无意义行为，且互信息（MI）和熵估计在大规模空间难扩展。为解决这些问题，本文提出新方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于技能状态密度偏差的技能发现目标  
提出最大化单个技能状态密度与其他技能探索区域偏差的目标（ISD3），鼓励技能间状态多样性，类似初始MI目标，让各技能探索区域尽可能不同，学习到有区分度的技能。  

💡 创新点2：带软模块化的条件自动编码器用于状态密度估计  
在高维空间为不同技能策略构建条件自动编码器，采用软模块化，使技能条件网络按技能决定的路由网络成为共享模块的加权组合，稳定估计差异明显技能的状态密度。  

💡 创新点3：基于自动编码器的技能内探索内在奖励  
利用学习到的自动编码器潜在空间，构造类似表格型计数探索的内在奖励，激励单个技能内的探索，且可扩展到大规模问题，理论上与表格型高效计数探索相关。  


### 📈 实验结果
在Maze、基于状态的无监督强化学习基准（URLB）和具挑战性的基于图像的URLB环境中开展大量实验，结果表明该方法（SD3）能学习到具探索性和多样性的技能，在各类下游任务中实现了最先进性能，且在基于图像的URLB任务中展现出可扩展性。

### 💬 可借鉴之处
1. 从状态密度偏差角度设计技能发现目标，为学习不同状态占用的多样技能提供直接思路，启发后续无监督RL中技能分化方向的研究。  
2. 带软模块化的条件自动编码器设计，为高维空间多技能状态密度稳定估计提供新范式，可借鉴到需对多类别/多策略进行密度估计的场景。  
3. 基于自动编码器潜在空间构造内在奖励，将表格型计数探索思路拓展到高维连续空间，为解决大规模环境下的探索问题提供参考。  

## staq-it!-growing-neural-networks-for-policy-mirror-descent
### Abstract
In Reinforcement Learning (RL), regularization has emerged as a popular tool
both in theory and practice, typically based either on an entropy bonus or a
Kullback-Leibler divergence that constrains successive policies. In practice,
these approaches have been shown to improve exploration, robustness and
stability, giving rise to popular Deep RL algorithms such as SAC and TRPO.
Policy Mirror Descent (PMD) is a theoretical framework that solves this general
regularized policy optimization problem, however the closed-form solution
involves the sum of all past Q-functions, which is intractable in practice. We
propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions
in memory, and show that for finite and large enough $M$, a convergent
algorithm can be derived, introducing no error in the policy update, unlike
prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong
theoretical guarantees and is competitive with deep RL baselines, while
exhibiting less performance oscillation, paving the way for fully stable deep
RL algorithms and providing a testbed for experimentation with Policy Mirror
Descent.
### 🌟 论文解读 | StaQ：为策略镜像下降打造可扩展神经网络，迈向稳定深度强化学习

### 📌 背景痛点/本文动机
深度强化学习（RL）在决策任务中取得卓越成果，但神经网络作为函数近似器加剧了探索难、对超参数敏感等问题，且实证表现常与理论理解脱节。正则化是解决这些问题的常用手段，如基于熵奖励的SAC、基于KL散度约束的TRPO等。策略镜像下降（PMD）是解决正则化策略优化的理论框架，但其闭式解需所有历史Q函数之和，在实践中因存储和计算不可行而难以落地——此前深度RL中PMD实现多采用近似方法，会引入额外误差。因此，本文旨在设计**有限记忆且理论收敛**的PMD类算法，平衡理论保证与实际可实现性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：有限记忆的PMD类算法理论分析  
提出仅存储最近M个Q函数的PMD类算法，证明当M足够大时，用“略作修改的策略$\tilde{\pi}_k$（删除最旧Q函数后的策略）”替代原PMD中需全部历史Q函数的策略$\pi_k$，算法仍能渐近收敛到最优策略$\pi^\star$。这为有限记忆下PMD的收敛性提供了理论支撑，填补了此前仅实验验证（如存储10个Q函数）但无理论证明的空白。  

💡 创新点2：高效并行的Q函数堆叠计算  
通过“批处理Q函数”，在GPU上高效并行计算Q函数的完整“堆叠”（stack）。这让有限记忆下存储M个Q函数并用于策略更新的过程更高效，降低了工程实现难度。  

💡 创新点3：StaQ算法设计与无误差策略更新  
基于上述有限记忆PMD框架，提出StaQ算法。其策略更新为**无优化步骤**的闭式更新（基于熵正则化的策略镜像下降），与此前TRPO、MDPO等依赖近似梯度更新（引入额外误差）不同，StaQ的策略更新在理论上“无误差”，仅策略评估和行为策略选择可能成为不稳定源，为深度RL的稳定性提供了新路径。  


### 📈 实验结果
在MuJoCo（离散动作场景）和MinAtar等多类环境中，StaQ与深度RL基线算法（如SAC、TRPO等）相比**竞争力相当**，且**学习过程震荡更小**。这验证了StaQ在实际任务中既能保持性能，又能提升训练稳定性，向“完全稳定的深度RL算法”更近一步。  


### 💬 可借鉴之处
1. **理论与工程的平衡**：通过有限记忆假设+理论收敛证明，将PMD从“理论框架”落地为“可实现算法”，为其他需历史信息的RL框架（如值迭代、策略迭代的正则化变体）提供“有限记忆+收敛性保障”的设计思路。  
2. **稳定训练的新思路**：StaQ的“无优化闭式策略更新”和“Q函数堆叠”机制，展示了“通过结构化存储历史信息+理论驱动的更新规则”来降低训练震荡的潜力，可启发后续稳定RL算法设计。  
3. **硬件友好的并行计算**：利用GPU并行批处理Q函数，为深度RL中多网络/多历史信息的高效利用提供了工程实践参考，尤其在需存储多个函数近似器（如多Q网络、多策略网络）的场景中，可借鉴此类并行化思路。  


StaQ不仅推进了PMD框架的实用化，更在“稳定深度RL算法”这一长期目标上迈出关键一步，为理论研究与工程实现架起桥梁，值得强化学习领域研究者与实践者关注。

## acereason-nemotron-1-1--advancing-math-and-code-reasoning-through-sft-and-rl-synergy
### Abstract
In this work, we investigate the synergy between supervised fine-tuning (SFT)
and reinforcement learning (RL) in developing strong reasoning models. We begin
by curating the SFT training data through two scaling strategies: increasing
the number of collected prompts and the number of generated responses per
prompt. Both approaches yield notable improvements in reasoning performance,
with scaling the number of prompts resulting in more substantial gains. We then
explore the following questions regarding the synergy between SFT and RL: (i)
Does a stronger SFT model consistently lead to better final performance after
large-scale RL training? (ii) How can we determine an appropriate sampling
temperature during RL training to effectively balance exploration and
exploitation for a given SFT initialization? Our findings suggest that (i)
holds true, provided effective RL training is conducted, particularly when the
sampling temperature is carefully chosen to maintain the temperature-adjusted
entropy around 0.3, a setting that strikes a good balance between exploration
and exploitation. Notably, the performance gap between initial SFT models
narrows significantly throughout the RL process. Leveraging a strong SFT
foundation and insights into the synergistic interplay between SFT and RL, our
AceReason-Nemotron-1.1 7B model significantly outperforms
AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among
Qwen2.5-7B-based reasoning models on challenging math and code benchmarks,
thereby demonstrating the effectiveness of our post-training recipe. We release
the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B
### 🌟 论文解读 | AceReason-Nemotron 1.1：SFT与RL协同推进数学和代码推理能力

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的数学与代码推理研究领域，尽管此前工作在短链式思维（CoT）推理上取得进展，但对监督微调（SFT）与强化学习（RL）协同作用的系统性研究仍较缺乏。同时，如何让中小规模模型（如7B量级）通过SFT与RL结合实现推理能力跃升，也是待深入探索的方向。此外，大模型长链式思维推理多依赖大规模RL，后续工作在中小模型上复现这类成功时，缺少对SFT与RL训练动态和协同性的全面分析，因此本文致力于填补这些研究空白，探索构建顶尖推理模型的后训练技术全景。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：SFT数据与训练的规模化策略  
通过两种维度规模化SFT训练数据：一是增加收集的prompt数量，二是增加每个prompt下生成的响应数量；同时增加训练轮次。发现两类数据规模化都能提升推理性能，且增加prompt数量带来的增益更显著；训练轮次上，从第1到第5轮性能持续提升，5 - 6轮后趋于平稳，一定程度“过拟合”反而因暴露偏差等因素提升长CoT生成的测试准确率。  

💡 创新点2：SFT与RL协同的关键探索  
从不同强度SFT模型启动RL训练，验证出更强SFT模型经大规模RL后仍能产出更优结果（虽训练中性能差距缩小）；针对给定SFT初始化，提出RL训练时采样温度的设置准则——让温度调整后的熵维持在0.3左右，以此平衡探索（exploration）与利用（exploitation），保障RL训练有效性。  

💡 创新点3：RL训练中超长响应的处理策略  
系统研究当响应长度未达预期（如24K tokens内没产出最终答案）时，是给负奖励还是屏蔽样本（超长过滤）。发现短token限制（如8K、16K）下超长过滤有明显收益；但24K时优势减弱，32K时甚至可能损害性能，为不同场景下RL训练的样本过滤提供决策依据。  

💡 创新点4：分阶段RL方法的普适性验证  
验证了此前针对“仅数学”“仅代码”prompt的分阶段RL方法，在远超DeepSeek - R1 - Distill - Qwen系列的更强SFT模型上依旧有效，证明该方法在不同强度SFT基础上的广泛适用性。  


### 📈 实验结果
在AIME 2024/2025、HMMT 2025、LiveCodeBench v5/v6等数学和代码基准测试中，AceReason - Nemotron - 1.1 7B模型显著超越前代AceReason - Nemotron - 1.0，且在基于Qwen2.5 - 7B的推理模型中达到全新SOTA。例如在AIME 2024（Pass@1）等多项任务里，准确率相较竞品有明显提升，有力证明SFT与RL协同后训练方案的有效性。  


### 💬 可借鉴之处
1. 数据规模化思路：在SFT阶段，从prompt数量、单prompt响应数等维度扩充数据，为提升推理性能提供了可操作的数据集构建方向；  
2. SFT与RL协同范式：明确更强SFT基础对RL后性能的正向影响，以及温度 - 熵调控在RL平衡探索利用中的关键作用，为后续模型训练流程设计提供指导；  
3. RL细节优化：超长响应处理策略随token预算变化的规律，能辅助从业者在不同任务约束下选择更优的训练样本过滤方式；  
4. 分阶段RL普适性：验证了特定分阶段RL方法在不同强度SFT模型上的有效性，为行业内复用类似训练范式提供信心与参考。

## research-on-optimal-control-problem-based-on-reinforcement-learning-under-knightian-uncertainty
### Abstract
Considering that the decision-making environment faced by reinforcement
learning (RL) agents is full of Knightian uncertainty, this paper describes the
exploratory state dynamics equation in Knightian uncertainty to study the
entropy-regularized relaxed stochastic control problem in a Knightian
uncertainty environment. By employing stochastic analysis theory and the
dynamic programming principle under nonlinear expectation, we derive the
Hamilton-Jacobi-Bellman (HJB) equation and solve for the optimal policy that
achieves a trade-off between exploration and exploitation. Subsequently, for
the linear-quadratic (LQ) case, we examine the agent's optimal randomized
feedback control under both state-dependent and state-independent reward
scenarios, proving that the optimal randomized feedback control follows a
Gaussian distribution in the LQ framework. Furthermore, we investigate how the
degree of Knightian uncertainty affects the variance of the optimal feedback
policy. Additionally, we establish the solvability equivalence between
non-exploratory and exploratory LQ problems under Knightian uncertainty and
analyze the associated exploration cost. Finally, we provide an LQ example and
validate the theoretical findings through numerical simulations.
### 🌟 论文解读 | 奈特不确定性下基于强化学习的最优控制问题研究

### 📌 背景痛点/本文动机
最优控制在工程、经济等多领域应用广泛，传统方法面对高维、非线性或环境不确定场景存在计算复杂、建模难等挑战。强化学习（RL）作为数据驱动方法为复杂最优控制提供新思路，但现有研究存在局限：一是多聚焦离散时间马尔可夫决策过程，对连续时间和空间问题关注少，而现实中如高频交易等场景需连续交互，离散方法对时间离散化敏感；二是连续设置下RL研究多限于确定性系统，对含环境噪声的随机系统及模型本身的奈特不确定性考虑不足。奈特不确定性指概率统计模型固有且不可约的不确定性，经典概率论难以应对未知概率模型场景，非线性期望可在模型不确定时进行风险分析，当前众多学者聚焦奈特不确定性下金融经济问题，而RL研究中对其关注较少，本文就此展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：针对奈特不确定性下强化学习智能体决策环境，刻画探索性状态动力学方程，研究熵正则化的松弛随机控制问题。运用随机分析理论与非线性期望下动态规划原理，推导哈密顿 - 雅可比 - 贝尔曼（HJB）方程，求解在探索与利用间权衡的最优策略。
💡 创新点2：在线性 - 二次（LQ）情形下，检验状态相关和状态无关奖励场景下智能体的最优随机反馈控制，证明LQ框架下最优随机反馈控制服从高斯分布；探究奈特不确定性程度对最优反馈策略方差的影响；建立奈特不确定性下非探索性与探索性LQ问题的可解性等价关系并分析探索成本。

### 📈 实验结果
提供LQ示例并通过数值模拟验证理论发现，验证了在LQ框架下关于最优随机反馈控制分布、奈特不确定性对策略方差影响等理论结论的正确性，也验证了非探索性与探索性LQ问题可解性等价等分析的合理性。

### 💬 可借鉴之处
本文为连续时间空间下、含奈特不确定性的强化学习最优控制问题提供了理论与方法参考。在处理复杂环境（如高频交易、自动驾驶等连续交互且模型存在不确定性的场景）的最优控制问题时，其刻画探索性状态动力学方程、利用非线性期望和动态规划推导HJB方程求解最优策略等思路，以及在LQ情形下的分析方法，可为后续相关领域（如工程控制、金融决策等）研究提供方法论借鉴，启发学者关注模型不确定性对强化学习最优控制的影响并探索应对方法。

## dr-sac--distributionally-robust-soft-actor-critic-for-reinforcement-learning-under-uncertainty
### Abstract
Deep reinforcement learning (RL) has achieved significant success, yet its
application in real-world scenarios is often hindered by a lack of robustness
to environmental uncertainties. To solve this challenge, some robust RL
algorithms have been proposed, but most are limited to tabular settings. In
this work, we propose Distributionally Robust Soft Actor-Critic (DR-SAC), a
novel algorithm designed to enhance the robustness of the state-of-the-art Soft
Actor-Critic (SAC) algorithm. DR-SAC aims to maximize the expected value with
entropy against the worst possible transition model lying in an uncertainty
set. A distributionally robust version of the soft policy iteration is derived
with a convergence guarantee. For settings where nominal distributions are
unknown, such as offline RL, a generative modeling approach is proposed to
estimate the required nominal distributions from data. Furthermore,
experimental results on a range of continuous control benchmark tasks
demonstrate our algorithm achieves up to $9.8$ times the average reward of the
SAC baseline under common perturbations. Additionally, compared with existing
robust reinforcement learning algorithms, DR-SAC significantly improves
computing efficiency and applicability to large-scale problems.
### 🌟 论文解读 | DR - SAC：不确定性下强化学习的分布式鲁棒软演员 - 评论家算法

### 📌 背景痛点/本文动机
深度强化学习（RL）虽取得巨大成功，但在现实场景中应用常受限于对环境不确定性缺乏鲁棒性。训练好的智能体在稍有不同的环境部署时性能易大幅下降，这源于训练与部署环境间的模型不匹配，如不确定的转移和奖励函数、观测与执行器误差等。已有鲁棒RL算法多限于表格型设置，难以应对连续控制等复杂场景。而SAC作为先进的深度RL算法，在连续控制任务表现出色但鲁棒性不足。因此，本文旨在增强SAC的鲁棒性，提出DR - SAC算法以应对环境不确定性挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：推导分布式鲁棒软策略迭代及收敛性
考虑策略熵的目标，推导了新颖的分布式鲁棒软策略迭代。假设环境真实转移分布处于以名义转移模型为中心的KL散度球构成的不确定性集合内，目标是学习在该集合最坏分布下最大化软值函数的策略。基于DR软Bellman方程，利用分布式鲁棒优化（DRO）理论的强对偶性，将原问题转化为易处理的对偶形式，解决了不确定性集合无限维带来的原问题难解性。同时证明了该策略对以名义转移分布为中心、KL散度约束的不确定性集合具有鲁棒性。
💡 创新点2：利用交换性质实现函数优化提升效率
为避免对每个状态 - 动作对求解优化问题，引入基于变分分析交换定理的函数优化技术。将KL约束不确定性集合内的优化问题重构为函数优化，实证表明该新形式能高效更新分布式鲁棒软策略迭代，与现有鲁棒强化学习算法相比，训练时间缩短至不足20%，大幅提升计算效率。
💡 创新点3：结合生成模型应对离线与连续空间任务
针对名义模型未知（如离线RL场景）和动作空间连续的情况，采用变分自编码器（VAEs）的生成建模方法。从离线数据集估计名义转移分布并生成样本形成经验测度，在增加少量计算和内存开销下，使DR - SAC能在具挑战性的离线学习和连续空间任务中实现分布式鲁棒软策略学习。

### 📈 实验结果
在一系列连续控制基准任务实验中，DR - SAC在常见扰动下，平均奖励可达SAC基线的9.8倍。与现有鲁棒强化学习算法相比，显著提升了计算效率，且在大规模问题上适用性更强。在五个带广泛扰动的离线RL环境验证中，DR - SAC展现出远超SAC的性能。

### 💬 可借鉴之处
1. 理论推导方面：将分布式鲁棒优化与强化学习结合，推导鲁棒版策略迭代并证明收敛性，为鲁棒RL算法理论奠基提供了思路，后续研究可借鉴这种跨领域理论融合方式处理不确定性问题。
2. 效率优化方面：引入函数优化技术替代逐状态 - 动作对优化，为提升算法计算效率提供了新视角，在处理大规模状态动作空间问题时可参考该优化思路。
3. 数据利用方面：在离线场景结合生成模型（如VAEs）估计未知分布，为离线RL中应对分布不确定性提供了可行方案，拓展了离线RL算法在鲁棒性方向的应用可能，其他离线鲁棒RL研究可借鉴生成模型与RL结合的模式。

## viability-of-future-actions--robust-safety-in-reinforcement-learning-via-entropy-regularization
### Abstract
Despite the many recent advances in reinforcement learning (RL), the question
of learning policies that robustly satisfy state constraints under unknown
disturbances remains open. In this paper, we offer a new perspective on
achieving robust safety by analyzing the interplay between two well-established
techniques in model-free RL: entropy regularization, and constraints
penalization. We reveal empirically that entropy regularization in constrained
RL inherently biases learning toward maximizing the number of future viable
actions, thereby promoting constraints satisfaction robust to action noise.
Furthermore, we show that by relaxing strict safety constraints through
penalties, the constrained RL problem can be approximated arbitrarily closely
by an unconstrained one and thus solved using standard model-free RL. This
reformulation preserves both safety and optimality while empirically improving
resilience to disturbances. Our results indicate that the connection between
entropy regularization and robustness is a promising avenue for further
empirical and theoretical investigation, as it enables robust safety in RL
through simple reward shaping.
### 🌟 论文解读 | 基于熵正则化实现强化学习鲁棒安全：未来动作可行性视角

### 📌 背景痛点/本文动机
尽管强化学习（RL）近年取得诸多进展，但在未知扰动下学习能鲁棒满足状态约束的策略仍是未解决的问题。现有工作中，鲁棒强化学习常被建模为约束优化问题，而利用无约束RL算法实现鲁棒安全策略颇具吸引力。本文旨在揭示鲁棒安全策略如何从RL中两种常见做法（最大熵RL与失败惩罚）自然涌现，探索熵正则化与鲁棒性的关联以实现更优安全保障。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：熵正则化诱导对动作噪声的鲁棒性  
通过实证表明，约束环境下的熵正则化会使智能体为避免约束边界而“牺牲”奖励，且这种回避程度由温度参数调节。熵正则化的策略倾向于最大化未来可行动作数量，将策略的累积折扣熵视为长期安全动作数量的代理，自然鼓励避开可行选项少的状态，进而转化为对动作噪声的鲁棒性（即保持长期可行动作数的策略更鲁棒）。  

💡 创新点2：失败惩罚近似约束问题  
证明了通过失败惩罚松弛严格安全约束后，约束RL问题能被无约束问题任意近似，从而可用标准无约束RL求解。当惩罚超过有限阈值时，所得策略的模式（mode）与约束问题的策略模式匹配，为学习鲁棒安全策略提供了实用的奖励塑造策略。  

💡 创新点3：从惩罚问题解中提取安全策略  
展示了能从惩罚问题的最优解中提取安全策略，且该策略具备鲁棒安全性，同时将温度系数解读为可调节的鲁棒性参数，为鲁棒性调控提供新视角。  

### 📈 实验结果
文中以“围栏悬崖（Fenced cliff）”等场景验证：熵正则化策略会避开可行动作少的状态，温度参数越高，策略模式离约束越远（更鲁棒但到达目标耗时增加）；同时实证表明这种约束回避能转化为对动作噪声的鲁棒性，即策略在训练后面对强于训练阶段的动作噪声时，仍能满足安全约束。  

### 💬 可借鉴之处
1. 启发对熵正则化与鲁棒性关联的深入研究：将熵正则化与鲁棒安全结合，为理论和实证探索开辟新方向，后续可围绕其数学原理与更复杂场景拓展分析。  
2. 奖励塑造实现鲁棒安全：借助失败惩罚与熵正则化的组合，无需复杂约束优化框架，用无约束RL技术就能逼近鲁棒安全策略，降低鲁棒安全RL的实现门槛。  
3. 超参数解读与调控：把温度参数视为鲁棒性可调参数，为工程实践中根据场景需求（如安全与效率权衡）调整策略提供直观指导。  

## exploration-by-random-reward-perturbation
### Abstract
We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.
### 🌟 论文解读 | 随机奖励扰动：轻量高效的强化学习探索新策略

### 📌 背景痛点/本文动机
在无模型强化学习（MFRL）中，高效探索多样场景、动作与结果是关键，尤其在稀疏奖励环境下。现有方法存在不足：基于动作扰动的探索策略（如$\epsilon$-greedy等）因缺乏方向指导，在长 horizon 或极稀疏奖励任务中表现挣扎；基于奖励塑造（RS）的方法需额外模型、计算开销，且平衡奖励依赖先验或手动调参，适应性受限。同时，通过奖励扰动探索的研究较少，理论支撑不足。基于此，论文提出随机奖励扰动（RRP）策略，填补奖励扰动探索的研究空白，简化探索逻辑以降低计算复杂度。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出随机奖励扰动（RRP）策略  
RRP 向智能体接收的环境奖励中注入零均值高斯噪声，即 $R_{\text{RRP}}(s) = R_{\text{env}}(s) + \varepsilon$（$\varepsilon \sim \mathcal{N}(0, \sigma^2)$）。通过扰动奖励，使策略在学习中向更随机方向优化，促进行为多样性，拓展探索范围。  

💡 创新点2：噪声退火机制实现探索到利用的平滑过渡  
为让智能体后期聚焦环境原始奖励目标，对噪声标准差 $\sigma$ 进行线性衰减：$\sigma(t) = \max\{0, \sigma_{\text{max}} - (\sigma_{\text{max}} - \sigma_{\text{min}})t/T\}$ 。其中 $\sigma_{\text{max}}$、$\sigma_{\text{min}}$ 是初始和最终标准差，$T$ 为噪声衰减的训练步数。当 $\sigma_{\text{min}} = 0$ 时，噪声完全消除，恢复原始奖励。  

💡 创新点3：兼容性与轻量性  
RRP 与基于动作扰动的探索策略（如 $\epsilon$-greedy、随机策略、熵正则化）完全兼容，可附加提升探索效果。只需对奖励函数简单修改，就能集成到现有 RL 算法，实现成本低、计算开销可忽略。同时，RRP 建立奖励塑造与噪声驱动探索的理论联系，凸显互补潜力。  


### 📈 实验结果
论文将 RRP 集成到 Proximal Policy Optimization（PPO）和 Soft Actor-Critic（SAC）算法，在 MuJoCo、Mobile Manipulator、Dexterous Hand 等多领域，稀疏和密集奖励场景下测试。结果表明：RRP-PPO 和 RRP-SAC 持续超越 vanilla 版本；在多任务中帮助智能体逃离局部最优，提升样本效率与收敛速度，在探索方法中表现具竞争力。  


### 💬 可借鉴之处
1. 探索策略创新：从奖励扰动角度切入强化学习探索，为解决稀疏奖励等场景下的探索难题提供新思路，启发后续对奖励层面探索机制的研究。  
2. 轻量集成思路：RRP 只需简单修改奖励函数就能嵌入现有算法，这种“低侵入性”的改进方式，为算法优化提供了高效且易实现的范式，便于工程实践中快速尝试。  
3. 噪声退火机制：通过动态调整噪声规模实现探索到利用的平滑过渡，这种动态调控的思路可迁移到其他需平衡探索与利用的强化学习或优化任务中。  

## state-entropy-regularization-for-robust-reinforcement-learning
### Abstract
State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.
### 🌟 论文解读 | 状态熵正则化：为鲁棒强化学习筑牢“防护盾”

### 📌 背景痛点/本文动机
强化学习（RL）在诸多合成领域取得亮眼成果，但走向真实世界仍需解决模型不完美、观测噪声、数据有限等难题。训练好的策略在部署时若遇环境动态或奖励结构偏离，表现可能骤降，这推动了鲁棒强化学习（Robust RL）发展——让策略在模型误设或不确定性下也可靠运行。  

此前，策略熵正则化已被研究与鲁棒性存在关联，而**状态熵正则化**虽在实验中展现出更好探索性与样本效率，其理论保障与鲁棒性联系却无人深究。比如策略熵鼓励动作选择随机性，但常沿单一路径扩散随机，遇路径级扰动（如大障碍物堵死最优路径）易失效；状态熵则激励覆盖更广泛状态空间，可能让智能体走多条高奖励路径。本文正是要探究：状态熵正则化是否能带来鲁棒性？带来何种鲁棒性？并对比它与策略熵的特性。

### 🚀 核心方法（介绍本文的几个创新点）
#### 💡 创新点1：状态熵正则化的鲁棒性理论分析（奖励与转移不确定性）
- **奖励鲁棒RL场景**：证明状态熵正则化能精准解决一类奖励鲁棒RL问题，刻画了其诱导的不确定性集合与对抗奖励。理论上，策略熵抵御“局部知情”的对抗扰动，状态熵则对“全局知情”扰动更鲁棒；且正则化强度能单调控制不确定性集合的“保守程度”——弱正则时趋近ℓ∞型不确定性，强正则时趋近ℓ1型。  
- **转移（核）不确定性场景**：证明状态熵正则化能在转移不确定性下给出非平凡的性能下界。对比“状态+策略熵”组合，发现加策略熵会削弱该下界，凸显单独用状态熵的结构优势。  

#### 💡 创新点2：熵正则化的局限性研究  
- 理论与实践层面限制：无论是策略熵、状态熵还是两者结合，**都无法解决所有转移核鲁棒RL问题**；且熵正则化可能在风险规避场景下大幅损害性能。  
- 对rollout数量的敏感性：状态熵正则化的鲁棒性优势，相比其他正则化，对“策略评估所用rollout数量”更敏感——样本少的场景下，其鲁棒性增益会缩水。  


### 📈 实验结果
在离散与连续控制任务中验证状态熵正则化的鲁棒性：  
- 面对**空间相关扰动**（如障碍物放置），状态熵正则化能提升性能；面对更细碎、均匀的扰动，也不会降低性能。  
- 鲁棒性收益依赖rollout预算（即评估策略时用多少条轨迹）：低样本场景下，状态熵带来的鲁棒性增益会减弱。  


### 💬 可借鉴之处
1. **理论视角**：首次为状态熵正则化的鲁棒性提供形式化保障，清晰对比它与策略熵在鲁棒性上的差异（如对抗扰动“知情范围”、转移不确定性下的下界表现），为后续鲁棒RL理论研究锚定新方向。  
2. **实践视角**：揭示状态熵对rollout数量的敏感性，提示工程落地时需关注样本规模；且实验验证其在“路径级扰动”（如障碍物）场景的鲁棒性优势，为真实世界（如机器人避障、复杂环境导航）中RL策略设计提供参考。  
3. **方法论视角**：对熵正则化“局限性”的剖析（如转移核鲁棒问题的不可解性、风险规避场景的副作用），帮助研究者更理性选择正则化手段，避免盲目堆叠方法。  


这篇论文从理论到实验，为“状态熵正则化如何赋能鲁棒RL”画出清晰图景，既补全了该方向的理论空白，又为工业界落地鲁棒策略提供了实操性洞察~

## when-maximum-entropy-misleads-policy-optimization
### Abstract
The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading
approach for achieving efficient learning and robust performance across many RL
tasks. However, MaxEnt methods have also been shown to struggle with
performance-critical control problems in practice, where non-MaxEnt algorithms
can successfully learn. In this work, we analyze how the trade-off between
robustness and optimality affects the performance of MaxEnt algorithms in
complex control tasks: while entropy maximization enhances exploration and
robustness, it can also mislead policy optimization, leading to failure in
tasks that require precise, low-entropy policies. Through experiments on a
variety of control problems, we concretely demonstrate this misleading effect.
Our analysis leads to better understanding of how to balance reward design and
entropy maximization in challenging control problems.
### 🌟 论文解读 | 当最大熵“误导”策略优化：复杂控制任务下MaxEnt RL的性能剖析

### 📌 背景痛点/本文动机
最大熵强化学习（MaxEnt RL）框架（如Soft - Actor Critic，SAC）在众多强化学习（RL）任务中展现出高效学习与鲁棒性能，其优势包括更好的探索性、优化 landscape 的平滑性以及对干扰的鲁棒性等。然而在实际关键性能控制问题中，MaxEnt 方法却表现挣扎，非 MaxEnt 算法（如PPO）反而能成功学习。现实中很多基于 RL 的机器人控制工作仍采用模仿学习结合非 MaxEnt 方法微调的方式，SAC 即便调参后性能也常逊于 PPO。例如四旋翼控制任务中，在简化动力学模型下 SAC 能快速学习稳定控制器，而在更真实动力学模型下却失败，PPO 却能成功。本文动机在于分析鲁棒性与最优性的权衡如何影响 MaxEnt 算法在复杂控制任务中的性能，揭示最大熵在何时会误导策略优化。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：揭示最大熵与回报最大化的冲突机制
分析在需要精确、低熵策略的关键性能控制问题中，最大熵与整体回报最大化的冲突会被放大并阻碍学习。复杂控制任务中达成期望性能常需在关键状态执行精确动作，这些状态下真实最优策略本就低熵；而偏离可行动作集的动作会导致不可恢复状态，MaxEnt 却可能因短期 “熵收益” 偏向这些次优行为，使智能体偏离解决硬控制问题的关键 —— 精确低熵最优策略。
💡 创新点2：形式化分析熵陷阱 —— 熵分叉扩展（Entropy Bifurcation Extension）
对任意马尔可夫决策过程（MDP），提出熵分叉扩展来显式构造熵陷阱：使得 MaxEnt 方法会被误导认为任意策略分布是 MaxEnt 最优的，而真实最优策略不受该扩展影响。这并非训练时的样本效率或探索偏差问题，而是 MaxEnt 算法收敛后的结果，说明熵的误导效应在标准策略优化方法不受影响的场景下也易出现。
💡 创新点3：多场景实验验证与分析
在多种真实控制环境（高速轮式车辆控制、四旋翼轨迹跟踪、对应硬件平台的四足机器人控制等）中分析 SAC 行为，展示 MaxEnt 和常规策略优化下价值 landscape 的差异如何解释 SAC 在这些环境中收敛到可行控制策略的困难；同时也说明该分析能帮助理解为何 MaxEnt 在受益于鲁棒探索的环境（如 OpenAI Gym 常见基准环境）中学习成功，为复杂控制问题中 MaxEnt 算法的奖励设计和超参数调优提供指导。

### 📈 实验结果
在四旋翼控制任务中，对比 SAC 和 PPO：简化动力学模型下 SAC 能快速学习稳定控制器，真实动力学模型下 SAC 失败而 PPO 成功；在高速轮式车辆控制、四足机器人控制等真实控制环境中，分析表明 MaxEnt 下价值 landscape 与常规策略优化的差异导致 SAC 难收敛到可行控制策略；通过这些实验具体展示了熵的误导效应在实践中如何影响学习，也验证了熵分叉扩展等分析的合理性。

### 💬 可借鉴之处
1. 对于强化学习研究者，本文让其更深入理解 MaxEnt RL 在复杂控制任务中的性能瓶颈，明白鲁棒性与最优性权衡在 MaxEnt 场景下的具体表现，为后续改进 MaxEnt 算法（如调整奖励设计、超参数 tuning 方向）提供理论与实验依据。
2. 针对机器人控制等实际应用领域从业者，解释了为何在真实复杂控制场景中 SAC 等 MaxEnt 方法有时表现不佳，帮助其在方法选择（如何时选 PPO 何时尝试改进 MaxEnt）和算法调优上做出更合理决策。
3. 从方法论角度，本文提出的熵分叉扩展等分析方式为研究强化学习中目标函数组件（如熵项）对策略优化的影响提供了新的思路和工具，后续研究可借鉴这种形式化分析与实验验证结合的方式来剖析其他 RL 算法组件的作用。

## causal-policy-learning-in-reinforcement-learning--backdoor-adjusted-soft-actor-critic
### Abstract
Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.
### 🌟 论文解读 | 因果强化学习新突破：DoSAC 解决隐藏混淆偏差

### 📌 背景痛点/本文动机
强化学习（RL）在机器人、连续控制等领域取得了显著进展，其中 Soft Actor - Critic（SAC）因高效探索和良好性能，适用于真实世界连续动作空间任务。但多数 RL 算法（包括 SAC）依赖观测数据，假设状态 - 动作转移是因果关系，当存在**隐藏混淆变量**（同时影响智能体状态观测和动作的未观测变量）时，这种假设不成立。隐藏混淆变量会引入偏差，损害价值函数和策略可靠性，限制泛化性与鲁棒性。而因果推断中的 do - calculus 和后门准则为解决混淆问题提供了思路，本文受此启发，提出 DoSAC 来修正隐藏混淆偏差。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 DoSAC 算法
DoSAC 是 SAC 的因果感知扩展，通过后门调整显式估计干预策略以修正隐藏混淆。不同于 SAC 基于可能有偏的观测分布 π(a|s) 优化策略，DoSAC 直接瞄准干预分布 π(a|do(s))，捕捉状态对动作的真实因果影响，在存在分布偏移场景下实现更鲁棒、泛化的决策。

💡 创新点2：设计 Backdoor Reconstructor 模块
引入可学习的 Backdoor Reconstructor，从当前状态推断**伪过去变量**（之前的状态和动作）。这些推断变量作为代理调整量满足后门准则，无需显式监督或访问潜在混淆变量，就能从观测数据估计干预效应，减少智能体训练时的混淆偏差，还能基于重放缓冲区观测数据对干预策略采样。

💡 创新点3：高效集成与泛化性
DoSAC 无缝集成到标准 SAC 训练流程，可端到端高效训练，无额外开销或数据要求；且是 SAC 的泛化，当无混淆变量影响动作执行时，自然退化为原始 SAC 形式。

### 📈 实验结果
在连续控制基准测试中，DoSAC 在混淆设置下超越基线方法（如标准 SAC 等），在鲁棒性、泛化性和策略可靠性方面均有提升，验证了其在受合成混淆影响的连续控制任务中改进策略鲁棒性与泛化性的能力。

### 💬 可借鉴之处
1. 因果与强化学习结合：为解决 RL 中隐藏混淆问题提供了因果视角的思路，将因果推断的后门调整等概念引入 RL 策略学习，启发后续在有偏观测场景下的 RL 算法设计。
2. 模块设计思路：Backdoor Reconstructor 这种从当前状态推断代理变量以满足因果准则的模块设计，为处理无显式混淆变量可用场景提供了可借鉴的“代理调整”范式。
3. 算法兼容性：DoSAC 对 SAC 的无缝集成与泛化特性，展示了在经典 RL 算法基础上进行因果增强时，如何兼顾原有优势与新功能拓展，为改进其他 RL 算法提供了集成思路。

## reinforcing-video-reasoning-with-focused-thinking
### Abstract
Recent advancements in reinforcement learning, particularly through Group
Relative Policy Optimization (GRPO), have significantly improved multimodal
large language models for complex reasoning tasks. However, two critical
limitations persist: 1) they often produce unfocused, verbose reasoning chains
that obscure salient spatiotemporal cues and 2) binary rewarding fails to
account for partially correct answers, resulting in high reward variance and
inefficient learning. In this paper, we propose TW-GRPO, a novel framework that
enhances visual reasoning with focused thinking and dense reward granularity.
Specifically, we employs a token weighting mechanism that prioritizes tokens
with high informational density (estimated by intra-group information entropy),
suppressing redundant tokens like generic reasoning prefixes. Furthermore, we
reformulate RL training by shifting from single-choice to multi-choice QA
tasks, where soft rewards enable finer-grained gradient estimation by
distinguishing partial correctness. Additionally, we propose question-answer
inversion, a data augmentation strategy to generate diverse multi-choice
samples from existing benchmarks. Experiments demonstrate state-of-the-art
performance on several video reasoning and general understanding benchmarks.
Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over
Video-R1) and 65.8\% on MMVU. Our codes are available at
\href{https://github.com/longmalongma/TW-GRPO}.
### 🌟 论文解读 | 聚焦思考强化视频推理：TW - GRPO 框架革新多模态大模型表现

### 📌 背景痛点/本文动机
近年来，强化学习（尤其是 Group Relative Policy Optimization，GRPO）在提升多模态大语言模型复杂推理任务表现上取得进展，但仍存在两大关键局限：一是生成的推理链常不聚焦、冗长，掩盖了关键时空线索；二是二元奖励机制无法考量部分正确答案，导致奖励方差大、学习低效。在视频推理等复杂多模态任务中，基于思维链（CoT）的推理易产生冗长无焦点的思考链，现有训练目标也难优先关注语义关键的时空线索；且现有方法依赖单选择问答任务的稀疏二元奖励，忽略部分正确性，而视频 QA 主流任务中此类单选择格式缺乏自然定义的多水平奖励信号，这些都限制了模型性能提升，因此本文提出 TW - GRPO 框架来解决这些问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：动态 token 加权机制
通过分析 token 位置的组内信息熵来估计 token 重要性，在损失计算时优先考虑信息密度高的 token，抑制通用推理前缀等冗余 token，让模型生成简洁且面向任务的推理链，避免冗余或无关细节，将模型注意力聚焦到对推理结果关键的内容上。

💡 创新点2：多粒度奖励建模与问答任务重构
将强化学习训练从单选择问答任务转为多选择问答任务，用多水平奖励替代稀疏二元奖励。区分部分正确和完全错误答案，实现更细粒度的梯度估计并稳定策略更新；同时为缓解多选择数据稀缺问题，提出 Question - Answer Inverse（QAI）数据增强策略，通过否定问题和反转答案将单选择任务转化为多选择格式。

### 📈 实验结果
在多个视频推理和通用理解基准测试中展现卓越性能，在 CLEVRER 基准上达到 50.4% 准确率（较 Video - R1 提升 18.8%），在 MMVU 上达到 65.8% 准确率，在 NExT - GQA 等基准也实现 state - of - the - art 表现，且聚焦思考让推理链更凝练，关注关键视觉或逻辑线索，多水平奖励降低了训练时的奖励方差。

### 💬 可借鉴之处
从技术创新角度，动态 token 加权为优化模型推理时的 token 关注度提供了新思路，可启发后续在处理序列生成任务时如何更精准聚焦关键信息；多粒度奖励建模将部分正确性纳入考量，为强化学习在多模态 QA 任务中更精细优化提供方向；问答反转的数据增强策略则为解决数据稀缺问题提供了巧妙的转换思路，这些创新点在多模态大模型优化、复杂任务推理等领域都有很好的借鉴价值，可指导研究者在模型训练机制、数据处理等方面进行改进探索。

## the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models
### Abstract
Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.
### 🌟 论文解读 | 大模型推理中幻觉困境的破局：面向事实性的强化学习优化

### 📌 背景痛点/本文动机
大语言模型（LLMs）借助强化学习（RL）优化在推理任务上取得显著进展，能在数学、多跳问答等复杂基准测试中展现强大能力。但研究发现，**面向推理的RL微调会大幅增加幻觉（hallucinations，即生成事实错误或编造内容）出现的概率**。这种幻觉常源于中间推理步骤的错误，即便最终答案偶尔正确，推理链里也可能存在无依据或错误的陈述，严重影响RL训练推理模型的可靠性与可解释性。为探究原因，作者从理论分析RL训练动态，发现高方差梯度、熵诱导的随机性、易陷入虚假局部最优这三个关键因素导致幻觉增多，进而提出解决方案。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：理论剖析RL训练致幻觉的成因  
深入分析面向推理任务的RL训练动态，明确三大致幻因素：其一，仅优化最终正确答案会因正确答案稀缺导致策略梯度方差极高，训练更新不稳定；其二，为探索“有奖励的输出”，策略预测需保持高熵，增加幻觉风险；其三，标准RL目标易陷入虚假局部最优，模型可能收敛到自信但错误、无奖励的答案。这三点共同解释了纯结果导向RL引发推理模型幻觉的根源。  

💡 创新点2：提出FSPO算法实现事实性感知的分步策略优化  
为缓解上述问题，提出**Factuality - aware Step - wise Policy Optimization（FSPO）**算法。核心是在每个推理步骤融入显式事实性验证，用分步事实性奖励信号调整单个token的优势值（advantage values），引导大模型生成更具事实性的内容。具体而言，用自动验证器检查每个生成的推理语句是否被给定证据蕴含，得到分步事实性分数；将这些分数整合到整体奖励信号中，优化时调整token的优势值——奖励事实正确token，惩罚错误token。这种事实感知的奖励塑造为策略提供更密集、信息更丰富的反馈，解决稀疏信号问题并降低训练不稳定性，让策略趋向于不仅答案正确、推理链也忠实可验证的解，直接缓解RL训练中的幻觉。

### 📈 实验结果
在数学推理和幻觉基准测试中，使用Qwen2.5（如7B - Base/Instruct等版本）和Llama模型（如Llama - 3.1 - 8B - Instruct）验证FSPO。结果显示：FSPO有效降低了幻觉，同时提升了推理准确率，在可靠性和性能上都有显著改进；且能在不损害生成内容质量的前提下，增强推理步骤的事实性。  

在初步实验中，对比有无大规模RL训练的多组模型（如DeepSeek系列、Qwen系列、Llama系列等）在TruthfulQA、HaluEval、HalluQA等幻觉基准的表现，发现经RL或长思维链（CoT）数据大规模训练后的模型，幻觉程度在三个基准上都显著更高；对HaluEval - QA中样本的分析也表明，多数错误源于中间推理步骤错误，佐证结果导向奖励建模易引发幻觉的猜想。

### 💬 可借鉴之处
1. 问题分析角度：从RL训练动态的理论层面剖析大模型推理中幻觉增多的原因，为理解RL与幻觉的关系提供了深入视角，后续研究可借鉴这种从训练机制本质找问题的思路。  
2. 方法创新方向：FSPO将“分步事实性验证”融入RL微调，通过奖励塑造优化token级优势值，为解决RL训练中稀疏奖励、幻觉等问题提供了新颖的“过程监督 + 事实性反馈”思路，可启发后续在强化学习与大模型对齐、可靠性提升方面的方法设计。  
3. 实验验证思路：在多模型、多基准（涵盖数学推理和幻觉评估）上开展实验，全面验证方法有效性，这种多维度实验验证的方式值得同类研究参考，以增强结论的说服力。

## on-policy-rl-with-optimal-reward-baseline
### Abstract
Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO integrates a practically feasible
formulation of the optimal reward baseline that minimizes gradient variance. We
evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its
superior performance and training stability without additional models or
regularization terms. Furthermore, OPO achieves lower policy shifts and higher
output entropy, encouraging more diverse and less repetitive responses. These
results highlight OPO as a promising direction for stable and effective
reinforcement learning in large language model alignment and reasoning tasks.
The implementation is merged into the verl library at
https://verl.readthedocs.io/en/latest/algo/opo.html.


## enhanced-dacer-algorithm-with-high-diffusion-efficiency
### Abstract
Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.
### 🌟 论文解读 | 高效扩散的增强型DACER算法：DACER2如何突破效率瓶颈？

### 📌 背景痛点/本文动机
扩散模型凭借强大的表达能力，在离线强化学习（RL）和模仿学习中展现潜力，而DACER算法将其拓展到在线RL领域，用反向扩散过程做策略近似器并结合策略梯度端到端训练，取得不错性能。但DACER存在明显缺陷：需要大量扩散步骤，严重影响训练效率；直接减少步骤又会让性能显著下降，推理效率不足成为实时在线RL场景应用扩散策略的瓶颈。所以，如何在保持甚至提升性能的同时，提升训练与推理效率，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Q - 梯度场目标作为辅助优化目标  
为了在每个扩散步骤中引导去噪过程、提升扩散策略训练效果，论文引入Q - 梯度场目标这一辅助优化目标，以此增强扩散策略在训练阶段的表现，让模型在学习过程中能更好地利用价值信息来优化策略。  

💡 创新点2：设计时间加权机制  
发现Q - 梯度场与扩散时间步独立会对扩散策略性能产生负面影响后，论文提出时间加权函数w(t)，将当前扩散时间步作为输入。该机制能让模型在扩散早期高效消除大规模噪声，在后期对动作进行精细优化，契合去噪过程不同阶段的需求（前期需要大振幅处理、后期需要小振幅精准控制）。  

### 📈 实验结果
在MuJoCo基准测试和多模态任务上开展实验：在MuJoCo控制任务中，DACER2仅用5个扩散步骤就在大多数任务上实现了最先进（SOTA）的性能；与DACER相比，DACER2展现出更强的多模态特性；在训练和推理效率方面，相同硬件配置PyTorch框架下，DACER2推理比DACER快60.6%、训练快41.7%，且推理效率在同类算法中最快。  

### 💬 可借鉴之处
1. 辅助优化目标设计思路：当主方法存在训练引导不足问题时，可考虑引入关联领域（如价值函数领域）的信息构建辅助目标，辅助主过程训练，提升效果。  
2. 时间维度动态机制：在涉及时间步迭代的过程（如扩散步骤）中，若发现过程与时间步独立性带来负面影响，可设计时间加权等动态机制，让不同阶段针对性处理，这在序列式、迭代式任务中具有参考价值。  
3. 多维度评估实践：论文不仅关注性能，还对训练和推理时间等效率维度在统一硬件配置下评估，这种多维度、公平对比的实验思路，能更全面展现算法优势，值得后续研究借鉴。

## bigger--regularized--categorical--high-capacity-value-functions-are-efficient-multi-task-learners
### Abstract
Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.


## maximizing-confidence-alone-improves-reasoning
### Abstract
Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen,
Mistral, and Llama families. The generality of our unsupervised learning method
lends itself to applicability in a wide range of domains where external
supervision is unavailable.


## the-entropy-mechanism-of-reinforcement-learning-for-reasoning-language-models
### Abstract
This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.


## skywork-open-reasoner-1-technical-report
### Abstract
The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.
### 🌟 论文解读 | Skywork-OR1：面向长思维链大模型的高效强化学习方案

### 📌 背景痛点/本文动机
近年来，基于强化学习（RL）的训练后技术在提升大语言模型（LLMs）推理能力上取得重大突破，像DeepSeek - R1等模型展现出RL在数学和编码任务上提升性能的潜力。不过，当前多数研究聚焦于将RL应用在基础模型，而非已完成有监督微调（SFT）的长思维链（CoT）模型，且对长CoT模型用RL高效且可扩展提升推理能力的方法尚不明确。同时，强化学习训练中探索与利用的平衡至关重要，过早的熵坍缩（过度利用）会影响模型性能，而如何应对这一问题也需深入研究。在此背景下，Skywork团队提出Skywork - OR1，旨在为长CoT模型提供高效可扩展的RL方案。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：针对长CoT模型的高效RL实现  
基于DeepSeek - R1 - Distill模型系列，提出Skywork - OR1这一适用于长思维链模型的强化学习方案。通过该方案在模型训练中实现性能提升，比如32B模型在AIME24、AIME25和LiveCodeBench基准测试的平均准确率从57.8%提升到72.8%，7B模型从43.6%提升到57.5%。  
💡 创新点2：全面的消融实验验证核心组件有效性  
对训练流程核心组件开展消融研究，涵盖数据混合、多阶段训练、截断响应的优势掩码、高温采样、自适应熵控制、无KL损失等方面，明确各组件对模型性能的影响，以验证方案有效性。例如数据混合方面，用严格过滤标准构建的数据混合比宽松质量阈值构建的基线混合表现更优；多阶段训练在初始阶段提升训练效率同时保留后续阶段可扩展性等。  
💡 创新点3：深入研究熵坍缩现象及应对  
对强化学习训练中过早熵坍缩现象进行全面研究，识别影响熵动态的关键因素，证明缓解过早熵坍缩对提升测试性能至关重要。通过大量消融实验探究如rollout多样性相关超参数、off - policy更新等对熵坍缩和性能的影响，找到预防过早熵坍缩的方法。  

### 📈 实验结果
在性能表现上，Skywork - OR1 - 32B在AIME24（82.2分）、AIME25（73.3分）基准测试中超过DeepSeek - R1和Qwen3 - 32B，在LiveCodeBench（63.0分）上表现相当；Skywork - OR1 - 7B在AIME24（70.2分）、AIME25（54.6分）和LiveCodeBench（47.6分）上在同尺寸模型中具竞争力；Skywork - OR1 - Math - 7B在同类尺寸模型中也表现强劲，AIME24得69.8分、AIME25得52.3分、LiveCodeBench得43.6分。同时，通过消融实验验证了各核心组件在提升性能、应对熵坍缩等方面的作用，如多阶段训练对训练效率的提升、自适应熵控制对模型探索能力和学习可塑性的维持等。  

### 💬 可借鉴之处
1. 数据层面：重视数据来源多样性与质量控制，通过严格过滤和质量评估构建训练数据，能加速模型学习，为后续模型训练数据准备提供思路，即在数据收集时要考虑多源且做好质量把控。  
2. 训练策略层面：多阶段训练、高温采样等策略在提升训练效率和最终性能上有积极作用，其他研究者可参考这些训练策略来优化自身模型训练流程，平衡训练效率与性能提升。  
3. 熵坍缩应对层面：深入研究熵坍缩现象及关键影响因素，为后续强化学习训练中平衡探索与利用提供了方向，在处理类似强化学习训练大模型任务时，可关注熵动态变化并采取相应措施缓解过早熵坍缩。  
4. 开源贡献层面：团队开源模型权重、训练代码和训练数据集，为社区研究提供充足资源，这种开放协作的方式值得学习，有助于推动整个大语言模型强化学习领域的发展。

## genpo--generative-diffusion-models-meet-on-policy-reinforcement-learning
### Abstract
Recent advances in reinforcement learning (RL) have demonstrated the powerful
exploration capabilities and multimodality of generative diffusion-based
policies. While substantial progress has been made in offline RL and off-policy
RL settings, integrating diffusion policies into on-policy frameworks like PPO
remains underexplored. This gap is particularly significant given the
widespread use of large-scale parallel GPU-accelerated simulators, such as
IsaacLab, which are optimized for on-policy RL algorithms and enable rapid
training of complex robotic tasks. A key challenge lies in computing
state-action log-likelihoods under diffusion policies, which is straightforward
for Gaussian policies but intractable for flow-based models due to irreversible
forward-reverse processes and discretization errors (e.g., Euler-Maruyama
approximations). To bridge this gap, we propose GenPO, a generative policy
optimization framework that leverages exact diffusion inversion to construct
invertible action mappings. GenPO introduces a novel doubled dummy action
mechanism that enables invertibility via alternating updates, resolving
log-likelihood computation barriers. Furthermore, we also use the action
log-likelihood for unbiased entropy and KL divergence estimation, enabling
KL-adaptive learning rates and entropy regularization in on-policy updates.
Extensive experiments on eight IsaacLab benchmarks, including legged locomotion
(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow
Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate
GenPO's superiority over existing RL baselines. Notably, GenPO is the first
method to successfully integrate diffusion policies into on-policy RL,
unlocking their potential for large-scale parallelized training and real-world
robotic deployment.
### 🌟 论文解读 | GenPO：让生成式扩散模型走进On-Policy强化学习

### 📌 背景痛点/本文动机
近年来，生成式扩散模型凭借强大的探索能力和多模态特性，在强化学习（RL）领域备受关注。不过现有工作大多聚焦于离线RL和离策略（Off-Policy）RL，将扩散策略整合到像PPO这样的**在线策略（On-Policy）** RL框架的研究仍很欠缺。  

而现实中，大规模并行GPU加速模拟器（如IsaacLab）对On-Policy算法做了优化，能快速训练复杂机器人任务，但扩散策略与On-Policy结合存在关键障碍：**计算状态 - 动作的对数似然**。高斯策略计算对数似然很直接，但扩散这类基于流的模型，因正逆过程不可逆、离散化误差（如Euler - Maruyama近似），难以处理对数似然计算，这阻碍了扩散策略在On-Policy场景下的应用与真实机器人部署。于是，GenPO应运而生，旨在填补这一空白。

### 🚀 核心方法
💡 创新点1：桥接生成式扩散模型与On-Policy RL  
GenPO是首个成功将扩散策略整合到On-Policy RL的方法。它借助**精确扩散逆过程**构造可逆的动作映射，突破了扩散策略正逆过程不匹配的问题，让扩散模型的探索能力和多模态特性，能在大规模GPU并行模拟器（如IsaacLab）中为On-Policy RL所用。  

💡 创新点2：解决扩散策略的对数似然计算难题  
GenPO提出**“双虚拟动作（doubled dummy action）”机制**。受精确扩散逆的限制，先构造一个动作空间是原空间两倍的新马尔可夫过程，通过交替更新虚拟动作的两部分实现可逆流过程。基于变量变换理论（类似归一化流），能精确计算新决策问题中给定动作的概率密度；推理时，将两部分虚拟动作平均后映射回原动作空间，既在重构的马尔可夫决策问题中更新生成式扩散策略，又能得到原任务的最优解。这一机制解决了扩散策略对数似然计算的壁垒，还能基于此实现：① 精确对数似然计算；② 无偏的熵估计；③ 解析性的KL散度计算，给表达能力强的扩散模型赋予类似高斯策略的优势。  

💡 创新点3：熵与KL散度的无偏估计及应用  
利用动作对数似然，GenPO实现了扩散策略熵和与行为扩散策略间KL散度的无偏估计。这让On-Policy更新中能融入**熵正则化**和**KL自适应学习率调整**，借鉴了PPO中这些技术的有效性，助力策略优化。  


### 📈 实验结果
论文在IsaacLab的8个基准任务上展开大量实验，涵盖legged locomotion（如Ant、Humanoid等机器人）、dexterous manipulation（Shadow Hand）、aerial control（Quadcopter）、robotic arm（Franka）等场景。结果显示：GenPO在累积回报上超越现有RL基线，同时样本效率相当且收敛更快。在大规模并行环境下，以往基于扩散的RL算法几乎无效，而GenPO在样本利用效率和 episodic rewards 方面表现最佳，充分验证了其优越性。  


### 💬 可借鉴之处
1. **领域融合思路**：为生成式模型与On-Policy RL的结合提供了成功范例，启发后续在不同模型与RL范式交叉领域的研究，尤其是当硬件（如并行模拟器）适配某类RL范式时，如何让强表达力的生成模型“上车”。  
2. **技术创新复用**：双虚拟动作机制解决对数似然难题的思路，以及基于对数似然做熵和KL估计来辅助策略更新的方式，可为其他面临“似然计算难、需利用概率密度辅助优化”的场景（如特定流模型、复杂生成模型结合RL）提供技术参考。  
3. **实验验证范式**：在多类机器人任务（运动、操作、飞行等）上的全面实验，展示了新方法在复杂真实机器人相关场景的泛化性验证思路，为后续算法在机器人领域的落地验证提供了参考模板。

## enhancing-efficiency-and-exploration-in-reinforcement-learning-for-llms
### Abstract
Reasoning large language models (LLMs) excel in complex tasks, which has
drawn significant attention to reinforcement learning (RL) for LLMs. However,
existing approaches allocate an equal number of rollouts to all questions
during the RL process, which is inefficient. This inefficiency stems from the
fact that training on simple questions yields limited gains, whereas more
rollouts are needed for challenging questions to sample correct answers.
Furthermore, while RL improves response precision, it limits the model's
exploration ability, potentially resulting in a performance cap below that of
the base model prior to RL. To address these issues, we propose a mechanism for
dynamically allocating rollout budgets based on the difficulty of the problems,
enabling more efficient RL training. Additionally, we introduce an adaptive
dynamic temperature adjustment strategy to maintain the entropy at a stable
level, thereby encouraging sufficient exploration. This enables LLMs to improve
response precision while preserving their exploratory ability to uncover
potential correct pathways. The code and data is available on:
https://github.com/LiaoMengqi/E3-RL4LLMs
### 🌟 论文解读 | 提升大语言模型强化学习效率与探索能力的新方法

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂任务推理上表现出色，强化学习（RL）也因此在LLMs领域受到广泛关注。然而现有RL方法存在两大问题：一是训练时对所有问题分配等量rollouts（采样次数），效率低下——简单问题训练收益有限，难题却需更多采样才能得到正确答案；二是RL虽提升了响应精度，却限制了模型探索能力，可能导致性能上限低于RL前的基础模型。此外，基于规则的奖励信号稀疏，易让策略更新梯度消失；结合熵正则化促进探索时，面对难题训练还可能性能下降甚至模型崩溃。为解决这些问题，论文提出针对性方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：动态rollout预算分配机制  
为更高效分配计算资源，论文依据问题难度动态分配rollout预算。先建模问题难度：在RL训练数据集里，记录每个问题的累计rollout次数与累计奖励，用平均奖励排序得到难度（排序越后难度越高）。之后将简单问题节省的rollout预算转移给难题——对模型已能熟练回答的简单问题减少rollout预算，把资源重新分配给更具挑战性的问题，提升采样到正确答案的概率，让RL训练更高效。  

💡 创新点2：自适应动态温度调整策略  
为在不引入有害梯度的前提下促进探索，论文设计温度调度器动态调整采样温度，维持策略熵稳定以鼓励充分探索，还结合退火机制平衡探索与利用。通过调整温度控制采样分布，让模型在训练中既能保持探索未知路径的能力，又能保证响应精度。  

### 📈 实验结果
在AIME 2024基准测试中，相比仅用GRPO训练，论文方法使7B规模模型的pass@1提升5.31%、pass@16提升3.33%；且在多个基准测试的pass@16指标上持续优于GRPO，验证了方法在提升模型性能与效率上的有效性。  

### 💬 可借鉴之处
1. 资源动态分配思路：在需资源分配的训练场景（不止LLMs RL），可参考依任务/样本难度动态分配资源，提升训练效率。  
2. 探索与利用平衡：通过温度调度等轻量手段调节策略熵来平衡探索和利用，为解决强化学习中常见的探索不足或过度探索问题提供了新思路，可迁移到其他RL任务中。  
3. 问题难度建模：基于累计交互数据（次数、奖励）来量化难度的方式，为后续任务优先级、资源倾斜等决策提供了可落地的参考范式。

## ppo-br--dual-signal-entropy-reward-adaptation-for-trust-region-policy-optimization
### Abstract
Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.
### 🌟 论文解读 | PPO - BR：为信任区域策略优化注入双信号熵 - 奖励自适应机制

### 📌 背景痛点/本文动机
在强化学习领域，近端策略优化（PPO）在从机器人控制到游戏AI等诸多场景中占据主导地位。然而，其静态信任区域存在固有缺陷：在学习的不同阶段，难以平衡探索与收敛。早期探索需要策略的随机性，而后期收敛需要稳定性，但PPO的静态信任区域会导致“探索匮乏”（高熵策略被过度裁剪，抑制状态覆盖）和“收敛不稳定”（固定的裁剪阈值在最优解附近允许噪声梯度更新）。此前的工作要么只改进探索（如基于熵的方法但忽略奖励动态），要么只增强稳定性（如奖励引导的方法但无视策略不确定性），还有的启发式调度缺乏理论保障，且没有方法能在信任区域机制内联合优化探索和收敛这两个信号，在稀疏奖励任务和安全关键领域这种缺陷尤为严重。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出PPO - BR框架
PPO - BR（Proximal Policy Optimization with Bidirectional Regularization）是一种双信号信任区域自适应框架，它基于策略熵和奖励进程动态调整裁剪阈值。在高熵阶段扩大信任区域以促进探索，在奖励提升进入平台期时收缩信任区域以确保稳定收敛。该机制有理论支撑，定理1保证了通过熵驱动扩展实现最小探索，引理2确保了收缩期间的单调改进，且无需辅助网络、元优化或架构改变，仅对PPO的裁剪逻辑进行轻量调整。

💡 创新点2：统一熵驱动探索与奖励引导收敛
PPO - BR是首个在单一理论有界的信任区域内统一熵驱动探索和奖励引导收敛的方法，填补了阶段感知强化学习中的关键空白。它能适配语言模型和一般强化学习环境，为不同场景下的强化学习提供了统一的熵 - 奖励自适应机制。

### 📈 实验结果
在MuJoCo、Atari和稀疏奖励等六个具有代表性的环境中进行实验验证。与标准PPO相比，PPO - BR收敛速度快29.1%（Wilcoxon检验p < 0.001）；在像Humanoid这样的高维任务中，奖励方差降低2.3倍；运行时开销小于1.8%，该开销比诸如Discriminator - Driven PPO（DD - PPO）等复杂基线引入的开销低17倍多。在模拟手术任务中，PPO - BR实现了98%的策略稳定性，而PPO仅为82%。

### 💬 可借鉴之处
1. 方法设计简洁且具理论保障：PPO - BR的即插即用特性和理论保证使其能在手术机器人、自主无人机等安全关键系统中部署，为安全关键领域的强化学习应用提供了可行方案。
2. 解决阶段感知学习难题：填补了强化学习中不同学习阶段平衡探索与收敛的关键空白，为后续强化学习算法在处理阶段感知学习问题上提供了新思路，展示了如何在信任区域机制内联合优化探索和收敛信号。
3. 低开销高效改进：仅需修改少量代码（约5行）就能实现性能提升，在实际工程应用中易于集成和推广，为算法优化提供了“轻量改进，高效收益”的范例。

## enter-the-void---planning-to-seek-entropy-when-reward-is-scarce
### Abstract
Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.
### 🌟 论文解读 | 奖励稀缺时主动探索高熵状态，提升MBRL效率

### 📌 背景痛点/本文动机
近年来强化学习（RL）在诸多领域取得成功，但在真实世界应用（如自主导航、科学探索等）中，奖励稀疏、环境部分可观测且随机，高效探索与样本效率仍是难题。 curiosity - 驱动的探索虽为替代方案，但传统 retrospective 方法（事后量化新奇性）实时适应性差，在非平稳环境易失效； anticipatory 方法虽有潜力但依赖基于集成的不确定性估计，在随机环境表现不佳。而基于模型的强化学习（MBRL）通过训练世界模型预测未来，为解决这些问题提供方向，但现有 MBRL 方法多优先优化 actor，忽视世界模型学习优化。提升世界模型保真度与收敛速度能带来下游收益，如提升 actor 性能。同时，现有基于模型的探索方法存在缺陷：如 Look before you leap 训练时回避不确定状态阻碍学习； MaxEnt - Dreamer 缺乏计划承诺，频繁重规划降低效率； RAIF 需事后评估收益，无法及时响应新状态。因此，本文旨在增强 Dreamer 以实现更高效探索。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：利用世界模型 transition 不确定性预测短视域状态熵并 densify 稀疏奖励
借助世界模型对环境动态的捕捉能力，利用其 transition 不确定性来预测短时间范围内状态的熵，将奖励不确定性补充到稀疏奖励中，使稀疏奖励信号更密集，加速 MBRL 训练过程，让智能体在奖励稀缺场景下也能更高效学习。

💡 创新点2：将 Dreamer KL 最小化重构为对抗 minmax 目标以最大化信息增益
把 Dreamer 中原本的 KL 最小化问题转化为对抗的 minmax 目标，以此来最大化信息增益，让由熵驱动的智能探索可以直接优化世界模型的学习，不再仅将世界模型作为 policy 学习工具，而是让 policy 助力世界模型训练，提升世界模型泛化能力以支持下游 actor 训练。

💡 创新点3：引入反应式分层规划器动态选择优化熵与奖励并灵活重规划
设计轻量级基于 PPO 的分层规划器，利用模型 rollouts 在高熵和高奖励轨迹间动态选择，还能动态决定何时重规划。该规划器能在优化熵（探索）和奖励（利用）之间做权衡，同时保留丢弃当前计划、重新规划新轨迹的灵活性，解决了许多基于模型预测控制（MPC）方法缺乏承诺、每一步都合成多步计划的问题。

### 📈 实验结果
将方法应用于 Dreamer 作为概念验证，在 Miniworld 程序生成的迷宫环境中，收敛时本文方法完成迷宫速度比基础 Dreamer 快 50%；且在想象中训练的策略收敛仅需基础 Dreamer 策略所需环境步数的 60%，证明了方法在提升效率与收敛速度上的有效性。

### 💬 可借鉴之处
1. 思路转换：将 policy 作为加速和优化世界模型训练的机制，不再仅把世界模型当 policy 学习工具，这种思路转换为 MBRL 中模型与策略的协同优化提供新视角，可启发后续研究重新审视二者关系以提升整体性能。
2. 不确定性利用：利用世界模型的 transition 不确定性来处理稀疏奖励和引导探索，为在奖励稀缺、环境复杂场景下设计探索机制提供了基于模型不确定性的有效范例，后续可借鉴此思路拓展到其他环境与任务。
3. 分层规划设计：反应式分层规划器动态决策的设计，为解决基于 MPC 方法的缺陷（如缺乏承诺）提供了可行方案，在需要平衡探索与利用、灵活规划的强化学习任务中，这种分层且动态决策的规划器设计思路值得参考与拓展。

## the-unreasonable-effectiveness-of-entropy-minimization-in-llm-reasoning
### Abstract
Entropy minimization (EM) trains the model to concentrate even more
probability mass on its most confident outputs. We show that this simple
objective alone, without any labeled data, can substantially improve large
language models' (LLMs) performance on challenging math, physics, and coding
tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy
similarly to instruction finetuning, but on unlabeled outputs drawn from the
model; (2) EM-RL: reinforcement learning with negative entropy as the only
reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce
entropy without any training data or parameter updates. On Qwen-7B, EM-RL,
without any labeled data, achieves comparable or better performance than strong
RL baselines such as GRPO and RLOO that are trained on 60K labeled examples.
Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of
proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the
challenging SciCode benchmark, while being 3x more efficient than
self-consistency and sequential refinement. Our findings reveal that many
pretrained LLMs possess previously underappreciated reasoning capabilities that
can be effectively elicited through entropy minimization alone, without any
labeled data or even any parameter updates.
### 🌟 论文解读 | 仅靠熵最小化，大模型推理能力竟能“无师自通”？

### 📌 背景痛点/本文动机
大语言模型（LLMs）在预训练阶段已吸收海量知识，但如何在**无标注数据**的情况下进一步激发其推理能力（如数学、物理、代码任务）仍是难题。传统的有监督微调（SFT）或强化学习（RL）依赖大量标注数据，成本高且适用场景受限。由此，论文聚焦“熵最小化（Entropy Minimization, EM）”这一思路：若模型足够“有能力”，其高置信度输出更可能正确，通过让模型把概率质量集中到高置信输出上，或许能在无标注数据时提升推理表现。

### 🚀 核心方法（介绍本文的几个创新点）
论文围绕“预训练后适配”与“推理时增强”两大阶段，提出**三种无标注数据的熵最小化方法**，覆盖微调、强化学习、推理三个维度：  

💡 创新点1：无监督微调（EM - FT）  
类似指令微调，但仅用模型自身生成的无标注输出来最小化“token级熵”。具体来说，给定输入prompt，让模型生成无标注输出，再在这些输出上优化token层面的熵损失，以此强化模型对高置信token的偏好。  

💡 创新点2：基于熵的强化学习（EM - RL）  
将“负熵”作为唯一奖励信号的强化学习。借鉴REINFORCE算法思路，但完全摒弃标注数据，仅通过“Rollout过程中token级熵的负数之和（结合基线调整）”作为奖励，让模型学会输出更“确定”的结果。  

💡 创新点3：推理时熵优化（EM - INF）  
推理阶段直接调整logits以降低模型输出分布的熵，**无需训练或参数更新**。在解码的每一步优化logits，让模型更聚焦高置信选项，适用于不确定性高的复杂任务（如高等数学、物理、科学代码）。  


### 📈 实验结果
论文在数学、物理、代码等挑战性任务上验证了方法的有效性：  
- **EM - RL（Qwen - 7B）**：在无标注数据时，性能比肩甚至超过基于6万标注数据训练的强RL基线（如GRPO、RLOO）；  
- **EM - INF（Qwen - 32B）**：在高难度SciCode基准测试中，性能追平/超越GPT - 4o、Claude 3 Opus、Gemini 1.5 Pro等闭源模型，且效率比“自一致性（Self - Consistency）”和“顺序精炼（Sequential Refinement）”高3倍；  
- 跨模型对比（如Llama - 3.1 - 8B vs Qwen - 2.5）显示：EM的效果依赖预训练模型本身的“能力底子”——若模型在某任务上先天能力弱，EM的增益也会受限。  


### 💬 可借鉴之处
1. **无标注数据的潜力释放**：证明仅通过“熵最小化”这一简洁目标，就能在无标注数据时大幅提升LLMs推理能力，为资源有限或标注成本高的场景提供新思路；  
2. **多阶段适配范式**：从微调（EM - FT）、强化学习（EM - RL）到推理增强（EM - INF），覆盖大模型生命周期多个阶段，展示了熵最小化的灵活应用空间；  
3. **基准与模型能力的反思**：强调预训练模型“固有能力”的重要性，呼吁未来算法评估中纳入EM作为基线，以更清晰区分“算法创新”和“模型本身能力”的贡献；  
4. **任务边界的启示**：EM在“置信度与正确性强相关”的推理任务（数学、代码）表现优异，但在“人类价值观对齐”等置信度≠质量的任务中效果有限，提示需根据任务特性选择方法。  


这篇论文撕开了“无标注数据下大模型能力挖掘”的新视角，让“熵最小化”从 regularization 配角变成了推理增强的主角——原来大模型的“潜力”，有时只需要一个简单的目标就能释放。

## aapo--enhance-the-reasoning-capabilities-of-llms-with-advantage-momentum
### Abstract
Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.
### 🌟 论文解读 | AAPO：用优势动量提升大模型推理能力

### 📌 背景痛点/本文动机
强化学习（RL）已成为增强大语言模型（LLMs）推理能力的有效途径，尤其在思维链（CoT）数据有限导致有监督微调（SFT）力不从心的场景下。基于RL的训练方法里，以Group Relative Policy Optimization（GRPO）为代表的**组相对优势估计**因摆脱对价值模型的依赖、简化训练流程而受关注。但现有组相对优势估计方法存在训练低效问题，当估计优势趋近于零时，模型参数更新易陷入停滞；而当组内奖励方差大时，又可能引发梯度不稳定。为解决这些缺陷，论文提出Advantage - Augmented Policy Optimization（AAPO）算法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：深入分析组相对优势估计类算法的优化行为  
论文聚焦RL在大模型后训练阶段采用组相对优势估计时的优化表现，重点剖析优势估计环节潜在的问题，比如优势趋近零导致无梯度更新、组内奖励方差大引发梯度不稳定等情况是如何偏离理想优化轨迹的。

💡 创新点2：提出AAPO算法引入优势动量改进优势估计  
AAPO是一种新颖的RL算法，通过基于动量的估计方案增强优势，进而优化交叉熵（CE）损失。其中，**优势动量**定义为策略模型响应奖励与参考模型响应奖励的差值，该方式将参考梯度融入原始梯度，即便优势趋近于零，也能提供反映整体改进方向的可靠优化信号，缓解组相对优势估计带来的训练低效问题。

### 📈 实验结果
论文在多个数学推理基准测试上开展实验，结果表明AAPO展现出更优的训练优化行为，在代表性数学推理任务中性能超越现有方法，有力证明了AAPO在提升大模型推理能力方面的有效性与鲁棒性。

### 💬 可借鉴之处
1. 问题分析角度：论文对现有组相对优势估计RL算法在大模型后训练场景下的优化行为分析，为后续研究同类方法的缺陷与改进方向提供了思路，启发研究者关注算法在实际训练时的细粒度表现。
2. 方法创新思路：AAPO引入优势动量改进优势估计的思路，为解决RL中优势估计相关的训练效率、稳定性问题提供了新范式，可借鉴这种通过引入额外信息（如参考模型对比产生的动量）来增强优化信号的方式，拓展到其他RL与大模型结合的任务场景。
3. 实验验证方向：在数学推理等专业领域基准测试上验证方法有效性的模式，为评估大模型推理能力提升类方法提供了参考，后续研究可参考这类有针对性的任务型基准测试验证思路。

## disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization
### Abstract
The recent success and openness of DeepSeek-R1 have brought widespread
attention to Group Relative Policy Optimization (GRPO) as a reinforcement
learning method for large reasoning models (LRMs). In this work, we analyze the
GRPO objective under a binary reward setting and reveal an inherent limitation
of question-level difficulty bias. We also identify a connection between GRPO
and traditional discriminative methods in supervised learning. Motivated by
these insights, we introduce a new Discriminative Constrained Optimization
(DisCO) framework for reinforcing LRMs, grounded in the principle of
discriminative learning. The main differences between DisCO and GRPO and its
recent variants are: (1) it replaces the group relative objective with a
discriminative objective defined by a scoring function; (2) it abandons
clipping-based surrogates in favor of non-clipping RL surrogate objectives used
as scoring functions; (3) it employs a simple yet effective constrained
optimization approach to enforce the KL divergence constraint, ensuring stable
training. As a result, DisCO offers notable advantages over GRPO and its
variants: (i) it completely eliminates difficulty bias by adopting
discriminative objectives; (ii) it addresses the entropy instability in GRPO
and its variants through the use of non-clipping scoring functions and a
constrained optimization approach; (iii) it allows the incorporation of
advanced discriminative learning techniques to address data imbalance, where a
significant number of questions have more negative than positive generated
answers during training. Our experiments on enhancing the mathematical
reasoning capabilities of SFT-finetuned models show that DisCO significantly
outperforms GRPO and its improved variants such as DAPO, achieving average
gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B
model.
### 🌟 论文解读 | DisCO：基于判别式约束优化增强大推理模型

### 📌 背景痛点/本文动机
随着DeepSeek - R1的成功与开源，其采用的Group Relative Policy Optimization（GRPO）作为大推理模型（LRMs）的强化学习方法受到广泛关注。然而，现有研究对GRPO固有局限性的分析与解决还存在不足。GRPO在二元奖励设置下存在问题级难度偏差，虽有一些改进变体（如DAPO、Dr. GRPO等），但这些方法多是启发式、临时性的，缺乏原则性基础，未能完全解决GRPO的固有局限，比如Dr. GRPO仍受难度偏差困扰，DAPO可能引发熵过度增长产生随机输出。因此，本文旨在以原则性方式设计更有效的大推理模型强化优化方法，避免GRPO的局限性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：揭示GRPO本质问题与关联
对GRPO及其变体在二元奖励设置下的目标函数分析，明确GRPO难度偏差的根源是其组相对优势函数，该函数对过易或过难问题赋予不成比例的小权重；同时发现GRPO与监督学习中传统判别式方法（AUC最大化）存在概念联系，AUC最大化旨在提高正输出分数同时降低负输出分数。
💡 创新点2：提出DisCO框架
基于判别式学习原理提出Discriminative Constrained Optimization（DisCO）框架强化大推理模型。与GRPO及其变体有三方面不同：用 scoring function 定义的判别式目标替代组相对目标；放弃基于裁剪的代理，采用非裁剪RL代理目标作为 scoring function；用简单有效的约束优化方法实施KL散度约束确保训练稳定。此框架带来优势：通过判别式目标消除难度偏差；利用非裁剪 scoring function 和约束优化解决GRPO及其变体的熵不稳定；结合先进判别式学习技术处理训练中数据不平衡（大量问题生成答案负样本多于正样本）。

### 📈 实验结果
在提升SFT微调模型数学推理能力的实验中，DisCO显著优于GRPO及其改进变体（如DAPO）。对于1.5B模型，在六个基准任务上，相较GRPO平均提升7%，相较DAPO平均提升6%；在对DeepSeek - R1 - Distill - Qwen - 1.5B/7B模型微调（训练和推理最大响应长度8k）的数学推理实验中，DisCO也显著优于所有基线，且比训练用24k长度、推理用32k长度的GRPO表现更好。

### 💬 可借鉴之处
从分析角度，本文对GRPO目标函数的深入剖析为理解现有大推理模型强化学习方法的局限性提供了思路，有助于后续针对性改进；从方法角度，DisCO框架基于判别式学习的设计思路为大模型强化学习提供了新范式，其对目标函数、代理目标、约束优化的处理方式，以及结合判别式技术处理数据不平衡等，为解决大模型强化学习中的偏差、不稳定、数据问题提供了可参考的技术路线；从实验角度，在数学推理任务上的实验设计与对比，为大模型特定能力（如数学推理）提升的研究提供了实验方法与效果验证的参考。

## preference-optimization-for-combinatorial-optimization-problems
### Abstract
Reinforcement Learning (RL) has emerged as a powerful tool for neural
combinatorial optimization, enabling models to learn heuristics that solve
complex problems without requiring expert knowledge. Despite significant
progress, existing RL approaches face challenges such as diminishing reward
signals and inefficient exploration in vast combinatorial action spaces,
leading to inefficiency. In this paper, we propose Preference Optimization, a
novel method that transforms quantitative reward signals into qualitative
preference signals via statistical comparison modeling, emphasizing the
superiority among sampled solutions. Methodologically, by reparameterizing the
reward function in terms of policy and utilizing preference models, we
formulate an entropy-regularized RL objective that aligns the policy directly
with preferences while avoiding intractable computations. Furthermore, we
integrate local search techniques into the fine-tuning rather than
post-processing to generate high-quality preference pairs, helping the policy
escape local optima. Empirical results on various benchmarks, such as the
Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem
(CVRP) and the Flexible Flow Shop Problem (FFSP), demonstrate that our method
significantly outperforms existing RL algorithms, achieving superior
convergence efficiency and solution quality.
### 🌟 论文解读 | 组合优化新突破：Preference Optimization 如何革新强化学习求解思路

### 📌 背景痛点/本文动机
组合优化问题（COPs）在路径规划、电路设计、调度等诸多实际场景中至关重要，需从指数级增长的候选解中找到最优解。由于其 NP - 难特性，精确求解受限，高效获取近似最优解成为关键。  

强化学习（RL）为求解组合优化问题提供了无需专家知识、自学习启发式策略的途径，但现有 RL 方法面临三大核心挑战：  
- 奖励信号衰减：策略提升过程中优势值幅度大幅降低，易引发梯度消失与收敛缓慢；  
- 动作空间探索低效：组合优化的庞大动作空间让传统探索技术（如轨迹熵正则化）计算不可行；  
- 推理耗时额外开销：局部搜索常作为后处理提升解质量，但会增加推理时间，在时间敏感场景受限。  

为解决这些问题，论文提出 **Preference Optimization（偏好优化，PO）** 方法，将定量奖励转化为定性偏好信号，重塑 RL 求解组合优化的范式。  


### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：基于偏好的 RL4CO 框架  
传统 RL 依赖定量奖励驱动学习，而组合优化中奖励差异衰减会干扰训练稳定性。PO 创新性地将**定量奖励信号转化为定性偏好信号**，聚焦采样解之间的“优劣关系”而非绝对奖励值。这种转换让学习过程对奖励尺度不敏感，既稳定训练，又能通过保留解的相对关系持续强调更优解，从根本上缓解奖励衰减难题。  

💡 创新点2：重参数化的熵正则化目标  
为避免传统熵正则化在组合动作空间的计算不可行性，PO 从策略角度**重参数化奖励函数**，结合统计偏好模型，构建熵正则化的 RL 目标函数。该目标让策略直接与偏好对齐，无需枚举整个动作空间，在保证计算可行性的同时，天然促进在庞大组合动作空间中的高效探索。  

💡 创新点3：局部搜索与微调的深度整合  
过往局部搜索多作为后处理步骤（会额外增加推理时间），PO 则**将局部搜索技术融入微调阶段**，而非仅用于后处理。这样策略能从经局部搜索优化后的高质量解中学习，助力摆脱局部最优，同时不产生额外推理耗时，在提升解质量的环节实现“一举两得”。  


### 📈 实验结果
论文在多个经典组合优化基准任务（旅行商问题 TSP、带容量约束的车辆路径问题 CVRP、柔性流水车间问题 FFSP）上验证 PO 性能：  
- 收敛效率：相比现有 RL 算法，PO 收敛速度显著提升，更快进入优质解区域；  
- 解质量：在各类问题实例中，生成解的质量（如更短路径、更优调度）远超对比方法；  
- 鲁棒性：偏好信号与局部搜索的整合让模型在不同规模、复杂度的问题上都展现出稳定优势。  


### 💬 可借鉴之处
1. **信号转换思路**：当任务中数值信号易衰减/不稳定时，可借鉴“定量→定性”的信号转换逻辑，用相对关系替代绝对数值驱动学习；  
2. **目标函数设计**：面对高维/离散大空间时，重参数化与熵正则化结合的思路为平衡探索 - 利用提供了新范式，避免暴力枚举；  
3. **后处理转内训**：将传统“后处理增强”改为“训练阶段整合”，让模型提前学习更优解的特征，是提升解质量与效率的巧妙思路，可推广到需后处理提优的其他任务（如图像生成、代码生成等）。  

PO 为强化学习求解组合优化难题开辟了更高效、更稳定的路径，其核心创新在信号利用、目标构建与流程整合层面的突破，也为其他依赖 RL 求解复杂离散问题的场景提供了宝贵借鉴。

## imagine--verify--execute--memory-guided-agentic-exploration-with-vision-language-models
### Abstract
Exploration is essential for general-purpose robotic learning, especially in
open-ended environments where dense rewards, explicit goals, or task-specific
supervision are scarce. Vision-language models (VLMs), with their semantic
reasoning over objects, spatial relations, and potential outcomes, present a
compelling foundation for generating high-level exploratory behaviors. However,
their outputs are often ungrounded, making it difficult to determine whether
imagined transitions are physically feasible or informative. To bridge the gap
between imagination and execution, we present IVE (Imagine, Verify, Execute),
an agentic exploration framework inspired by human curiosity. Human exploration
is often driven by the desire to discover novel scene configurations and to
deepen understanding of the environment. Similarly, IVE leverages VLMs to
abstract RGB-D observations into semantic scene graphs, imagine novel scenes,
predict their physical plausibility, and generate executable skill sequences
through action tools. We evaluate IVE in both simulated and real-world tabletop
environments. The results show that IVE enables more diverse and meaningful
exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the
entropy of visited states. Moreover, the collected experience supports
downstream learning, producing policies that closely match or exceed the
performance of those trained on human-collected demonstrations.
### 🌟 论文解读 | 从想象到执行：用视觉语言模型实现记忆引导的智能体探索

### 📌 背景痛点/本文动机
在通用机器人学习里，探索是核心能力，特别是在奖励稀疏、目标不明确或缺乏特定任务监督的开放式环境中。强化学习（RL）虽常用于自主探索，但在高维且语义丰富的真实机器人场景中表现不佳，存在行为无方向、效率低甚至有安全风险等问题。而视觉语言模型（VLMs）虽有语义推理优势，能为探索提供基础，但输出常脱离物理现实，且缺乏对过往交互的结构化记忆，易产生冗余、不可行的生成结果，阻碍有效探索与下游学习。因此，需要一种方法弥合想象与执行的差距，借鉴人类好奇心驱动探索的方式来设计机器人探索框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出IVE框架，模拟人类好奇心驱动探索  
IVE（Imagine, Verify, Execute）是受人类探索启发的智能体探索框架，能让智能体想象新颖未来场景配置、基于交互历史预测可行性并通过技能库执行行为。其包含多个模块：Scene Describer将原始观测抽象为语义表示；Explorer生成自主目标和技能计划；Verifier预测想象场景转换的物理合理性；基于检索的Memory Module存储过往经验辅助探索与验证；Action Tools把高层技能转为可执行机器人轨迹。这些模块紧密协作，让想象基于物理可行性和上下文相关性。

💡 创新点2：无奖励的语义交互数据收集  
构建全自动的、由视觉语言模型引导的智能体系统，无需外部奖励、示范或预定义目标，生成语义层面有意义的交互数据。借助VLMs的语义推理能力生成假设转换、量化新颖性等，同时结合各模块确保数据在物理上合理且语义丰富，实现场景多样性可达人类专家的82% - 122%。

💡 创新点3：结合记忆引导想象与物理合理性预测  
IVE把记忆引导的想象和物理合理性预测结合，让智能体像人类一样带着好奇心探索。Memory Module提供过往交互经验，辅助Explorer想象和Verifier验证，解决VLMs缺乏物理 grounding 和记忆的问题，使生成的探索行为更可行、多样。

### 📈 实验结果
在模拟和真实桌面环境评估IVE，结果显示：相比RL基线，IVE能实现更丰富且有意义的探索，访问状态的熵提升了4.1到7.8倍；收集的经验支持下游学习，训练出的策略性能接近甚至超过基于人类收集示范训练的策略；此外，IVE获取的数据也有助于更好地训练世界模型，体现其捕捉复杂环境潜在动态的有效性。

### 💬 可借鉴之处
从方法设计角度，模块化的框架设计值得借鉴，将复杂的探索任务拆解为场景描述、探索、验证、记忆、动作工具等模块，各模块各司其职又紧密配合，为解决复杂任务提供了分而治之的思路；在机器人学习与视觉语言模型结合方面，展示了如何利用VLMs的语义优势弥补传统RL在真实复杂场景的不足，为后续跨模态结合推动机器人自主探索提供了范例；从应用价值看，证明了无奖励自主收集数据用于下游任务（如行为克隆、世界模型学习）的可行性，为数据驱动的机器人学习在数据获取难题上提供了新途径，后续研究可参考这种自主探索 - 数据利用的闭环模式来提升机器人在开放环境的学习能力。 

