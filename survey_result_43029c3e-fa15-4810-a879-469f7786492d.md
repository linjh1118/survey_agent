# Paper List of Terms(RL+entropy)
- [25/08] **ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents**  
[[Paper](http://arxiv.org/pdf/2508.14040v1)] [[Code/Page]()] [[TLDR/Notes](#computerrl--scaling-end-to-end-online-reinforcement-learning-for-computer-use-agents)]

- [25/08] **Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation**  
[[Paper](http://arxiv.org/pdf/2508.14143v1)] [[Code/Page]()] [[TLDR/Notes](#beyond-turing--memory-amortized-inference-as-a-foundation-for-cognitive-computation)]

- [25/08] **From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR**  
[[Paper](http://arxiv.org/pdf/2508.07534v2)] [[Code/Page]()] [[TLDR/Notes](#from-trial-and-error-to-improvement--a-systematic-analysis-of-llm-exploration-mechanisms-in-rlvr)]

- [25/08] **AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance**  
[[Paper](http://arxiv.org/pdf/2508.06944v2)] [[Code/Page](https://github.com/hlxtsyj/AMFT.)] [[TLDR/Notes](#amft--aligning-llm-reasoners-by-meta-learning-the-optimal-imitation-exploration-balance)]

- [25/08] **GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy**  
[[Paper](http://arxiv.org/pdf/2508.04349v4)] [[Code/Page]()] [[TLDR/Notes](#gtpo-and-grpo-s--token-and-sequence-level-reward-shaping-with-policy-entropy)]

- [25/08] **Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following**  
[[Paper](http://arxiv.org/pdf/2508.03178v1)] [[Code/Page]()] [[TLDR/Notes](#light-if--endowing-llms-with-generalizable-reasoning-via-preview-and-self-checking-for-complex-instruction-following)]

- [25/08] **Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2508.02260v1)] [[Code/Page]()] [[TLDR/Notes](#decomposing-the-entropy-performance-exchange--the-missing-keys-to-unlocking-effective-reinforcement-learning)]

- [25/07] **MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning**  
[[Paper](http://arxiv.org/pdf/2507.20278v1)] [[Code/Page]()] [[TLDR/Notes](#mol-rl--distilling-multi-step-environmental-feedback-into-llms-for-feedback-independent-reasoning)]

- [25/07] **The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2507.20150v1)] [[Code/Page]()] [[TLDR/Notes](#the-policy-cliff--a-theoretical-analysis-of-reward-policy-maps-in-large-language-models)]

- [25/07] **Agentic Reinforced Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2507.19849v1)] [[Code/Page](https://github.com/dongguanting/ARPO)] [[TLDR/Notes](#agentic-reinforced-policy-optimization)]

- [25/07] **UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities**  
[[Paper](http://arxiv.org/pdf/2507.19766v1)] [[Code/Page]()] [[TLDR/Notes](#ulorl-an-ultra-long-output-reinforcement-learning-approach-for-advancing-large-language-models--reasoning-abilities)]

- [25/07] **Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2507.14748v1)] [[Code/Page]()] [[TLDR/Notes](#skill-learning-via-policy-diversity-yields-identifiable-representations-for-reinforcement-learning)]

- [25/07] **Perception-Aware Policy Optimization for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2507.06448v4)] [[Code/Page](https://mikewangwzhl.github.io/PAPO.)] [[TLDR/Notes](#perception-aware-policy-optimization-for-multimodal-reasoning)]

- [25/07] **Skywork-R1V3 Technical Report**  
[[Paper](http://arxiv.org/pdf/2507.06167v3)] [[Code/Page]()] [[TLDR/Notes](#skywork-r1v3-technical-report)]

- [25/07] **Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2507.01551v2)] [[Code/Page]()] [[TLDR/Notes](#self-guided-process-reward-optimization-with-redefined-step-wise-advantage-for-process-reinforcement-learning)]

- [25/07] **Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems**  
[[Paper](http://arxiv.org/pdf/2507.00358v2)] [[Code/Page]()] [[TLDR/Notes](#data-driven-exploration-for-a-class-of-continuous-time-indefinite-linear--quadratic-reinforcement-learning-problems)]

- [25/06] **EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework**  
[[Paper](http://arxiv.org/pdf/2506.22200v3)] [[Code/Page](https://github.com/597358816/EFRame.)] [[TLDR/Notes](#eframe--deeper-reasoning-via-exploration-filter-replay-reinforcement-learning-framework)]

- [25/06] **APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2506.21655v1)] [[Code/Page](https://github.com/Indolent-Kawhi/View-R1.)] [[TLDR/Notes](#apo--enhancing-reasoning-ability-of-mllms-via-asymmetric-policy-optimization)]

- [25/06] **SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.19767v1)] [[Code/Page]()] [[TLDR/Notes](#srft--a-single-stage-method-with-supervised-and-reinforcement-fine-tuning-for-reasoning)]

- [25/06] **Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning**  
[[Paper](http://arxiv.org/pdf/2506.18330v2)] [[Code/Page](https://github.com/netease-youdao/Confucius3-Math.)] [[TLDR/Notes](#confucius3-math--a-lightweight-high-performance-reasoning-llm-for-chinese-k-12-mathematics-learning)]

- [25/06] **AdapThink: Adaptive Thinking Preferences for Reasoning Language Model**  
[[Paper](http://arxiv.org/pdf/2506.18237v1)] [[Code/Page]()] [[TLDR/Notes](#adapthink--adaptive-thinking-preferences-for-reasoning-language-model)]

- [25/06] **Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs**  
[[Paper](http://arxiv.org/pdf/2506.14758v3)] [[Code/Page]()] [[TLDR/Notes](#reasoning-with-exploration--an-entropy-perspective-on-reinforcement-learning-for-llms)]

- [25/06] **Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs**  
[[Paper](http://arxiv.org/pdf/2506.14731v2)] [[Code/Page]()] [[TLDR/Notes](#ring-lite--scalable-reasoning-via-c3po-stabilized-reinforcement-learning-for-llms)]

- [25/06] **Unsupervised Skill Discovery through Skill Regions Differentiation**  
[[Paper](http://arxiv.org/pdf/2506.14420v1)] [[Code/Page]()] [[TLDR/Notes](#unsupervised-skill-discovery-through-skill-regions-differentiation)]

- [25/06] **StaQ it! Growing neural networks for Policy Mirror Descent**  
[[Paper](http://arxiv.org/pdf/2506.13862v1)] [[Code/Page]()] [[TLDR/Notes](#staq-it!-growing-neural-networks-for-policy-mirror-descent)]

- [25/06] **AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy**  
[[Paper](http://arxiv.org/pdf/2506.13284v1)] [[Code/Page](https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B)] [[TLDR/Notes](#acereason-nemotron-1-1--advancing-math-and-code-reasoning-through-sft-and-rl-synergy)]

- [25/06] **Research on Optimal Control Problem Based on Reinforcement Learning under Knightian Uncertainty**  
[[Paper](http://arxiv.org/pdf/2506.13207v1)] [[Code/Page]()] [[TLDR/Notes](#research-on-optimal-control-problem-based-on-reinforcement-learning-under-knightian-uncertainty)]

- [25/06] **DR-SAC: Distributionally Robust Soft Actor-Critic for Reinforcement Learning under Uncertainty**  
[[Paper](http://arxiv.org/pdf/2506.12622v1)] [[Code/Page]()] [[TLDR/Notes](#dr-sac--distributionally-robust-soft-actor-critic-for-reinforcement-learning-under-uncertainty)]

- [25/06] **Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization**  
[[Paper](http://arxiv.org/pdf/2506.10871v1)] [[Code/Page]()] [[TLDR/Notes](#viability-of-future-actions--robust-safety-in-reinforcement-learning-via-entropy-regularization)]

- [25/06] **Exploration by Random Reward Perturbation**  
[[Paper](http://arxiv.org/pdf/2506.08737v1)] [[Code/Page]()] [[TLDR/Notes](#exploration-by-random-reward-perturbation)]

- [25/06] **State Entropy Regularization for Robust Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.07085v2)] [[Code/Page]()] [[TLDR/Notes](#state-entropy-regularization-for-robust-reinforcement-learning)]

- [25/06] **When Maximum Entropy Misleads Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2506.05615v1)] [[Code/Page]()] [[TLDR/Notes](#when-maximum-entropy-misleads-policy-optimization)]

- [25/06] **Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic**  
[[Paper](http://arxiv.org/pdf/2506.05445v1)] [[Code/Page]()] [[TLDR/Notes](#causal-policy-learning-in-reinforcement-learning--backdoor-adjusted-soft-actor-critic)]

- [25/05] **Reinforcing Video Reasoning with Focused Thinking**  
[[Paper](http://arxiv.org/pdf/2505.24718v3)] [[Code/Page](https://github.com/longmalongma/TW-GRPO}.)] [[TLDR/Notes](#reinforcing-video-reasoning-with-focused-thinking)]

- [25/05] **The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models**  
[[Paper](http://arxiv.org/pdf/2505.24630v1)] [[Code/Page]()] [[TLDR/Notes](#the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models)]

- [25/05] **On-Policy RL with Optimal Reward Baseline**  
[[Paper](http://arxiv.org/pdf/2505.23585v2)] [[Code/Page](https://verl.readthedocs.io/en/latest/algo/opo.html.)] [[TLDR/Notes](#on-policy-rl-with-optimal-reward-baseline)]

- [25/05] **Enhanced DACER Algorithm with High Diffusion Efficiency**  
[[Paper](http://arxiv.org/pdf/2505.23426v1)] [[Code/Page]()] [[TLDR/Notes](#enhanced-dacer-algorithm-with-high-diffusion-efficiency)]

- [25/05] **Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners**  
[[Paper](http://arxiv.org/pdf/2505.23150v1)] [[Code/Page]()] [[TLDR/Notes](#bigger--regularized--categorical--high-capacity-value-functions-are-efficient-multi-task-learners)]

- [25/05] **Maximizing Confidence Alone Improves Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.22660v4)] [[Code/Page]()] [[TLDR/Notes](#maximizing-confidence-alone-improves-reasoning)]

- [25/05] **The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models**  
[[Paper](http://arxiv.org/pdf/2505.22617v1)] [[Code/Page]()] [[TLDR/Notes](#the-entropy-mechanism-of-reinforcement-learning-for-reasoning-language-models)]

- [25/05] **Skywork Open Reasoner 1 Technical Report**  
[[Paper](http://arxiv.org/pdf/2505.22312v2)] [[Code/Page]()] [[TLDR/Notes](#skywork-open-reasoner-1-technical-report)]

- [25/05] **GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.18763v2)] [[Code/Page]()] [[TLDR/Notes](#genpo--generative-diffusion-models-meet-on-policy-reinforcement-learning)]

- [25/05] **Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs**  
[[Paper](http://arxiv.org/pdf/2505.18573v1)] [[Code/Page](https://github.com/LiaoMengqi/E3-RL4LLMs)] [[TLDR/Notes](#enhancing-efficiency-and-exploration-in-reinforcement-learning-for-llms)]

- [25/05] **PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization**  
[[Paper](http://arxiv.org/pdf/2505.17714v1)] [[Code/Page]()] [[TLDR/Notes](#ppo-br--dual-signal-entropy-reward-adaptation-for-trust-region-policy-optimization)]

- [25/05] **Enter the Void - Planning to Seek Entropy When Reward is Scarce**  
[[Paper](http://arxiv.org/pdf/2505.16787v2)] [[Code/Page]()] [[TLDR/Notes](#enter-the-void---planning-to-seek-entropy-when-reward-is-scarce)]

- [25/05] **The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.15134v1)] [[Code/Page]()] [[TLDR/Notes](#the-unreasonable-effectiveness-of-entropy-minimization-in-llm-reasoning)]

- [25/05] **AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum**  
[[Paper](http://arxiv.org/pdf/2505.14264v1)] [[Code/Page]()] [[TLDR/Notes](#aapo--enhance-the-reasoning-capabilities-of-llms-with-advantage-momentum)]

- [25/05] **DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization**  
[[Paper](http://arxiv.org/pdf/2505.12366v2)] [[Code/Page]()] [[TLDR/Notes](#disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization)]

- [25/05] **Preference Optimization for Combinatorial Optimization Problems**  
[[Paper](http://arxiv.org/pdf/2505.08735v1)] [[Code/Page]()] [[TLDR/Notes](#preference-optimization-for-combinatorial-optimization-problems)]

- [25/05] **Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.07815v2)] [[Code/Page]()] [[TLDR/Notes](#imagine--verify--execute--memory-guided-agentic-exploration-with-vision-language-models)]



# TLDR/Notes
## computerrl--scaling-end-to-end-online-reinforcement-learning-for-computer-use-agents
### Abstract
We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)
### ğŸŒŸ è®ºæ–‡è§£è¯» | ComputerRLï¼šæ¨åŠ¨æ¡Œé¢æ™ºèƒ½ä»£ç†ç«¯åˆ°ç«¯åœ¨çº¿å¼ºåŒ–å­¦ä¹ è§„æ¨¡åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨åŠ¨äººå·¥æ™ºèƒ½èƒ½åŠ›æ‹“å±•ï¼ŒåŸºäºLLMçš„GUIä»£ç†åœ¨æ¡Œé¢è®¾å¤‡è‡ªä¸»æ‰§è¡Œå¤æ‚ä»»åŠ¡å—å…³æ³¨ï¼Œä½†è®©ä»£ç†åœ¨çœŸå®åœºæ™¯é•¿æœŸè‡ªä¸»è¿è¡Œä»å­˜æŒ‘æˆ˜ã€‚ä¸€æ˜¯GUIä¸ºäººç±»äº¤äº’è®¾è®¡ï¼Œä»£ç†æ¨¡æ‹Ÿäººç±»åŠ¨ä½œå›°éš¾ï¼›äºŒæ˜¯è¡Œä¸ºå…‹éš†ç­‰ä¸»æµæ–¹æ³•åœ¨å¯æ‰©å±•æ€§ä¸æœ‰æ•ˆæ€§ä¸Šå—é™ï¼Œæ‰‹åŠ¨æ ‡æ³¨ç¹çã€æ¨¡å‹è’¸é¦å—é™äºæ•™å¸ˆæ¨¡å‹ï¼›ä¸‰æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½æœ‰æ½œåŠ›ä½†å› è®¡ç®—å¤æ‚å’Œæ–¹æ³•æŒ‘æˆ˜ï¼Œåœ¨æ¡Œé¢è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­å¤§è§„æ¨¡åº”ç”¨å—é™ï¼Œå¦‚ç¯å¢ƒå¤æ‚ã€æ”¶æ•›æ…¢ç­‰ã€‚å› æ­¤ï¼Œéœ€æ–°æ¡†æ¶æ¨åŠ¨æ¡Œé¢çº§è§„åˆ’ã€æ¨ç†ä¸è®¾å¤‡æ“ä½œå‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAPI - GUIäº¤äº’èŒƒå¼
ç°æœ‰GUIä»£ç†ä¾èµ–ç±»äººGUIäº¤äº’æ“ä½œè®¾å¤‡å­˜åœ¨å›°éš¾ï¼Œçº¯APIæ“ä½œä¹Ÿæœ‰å®ç°å¤æ‚ã€é€‚åº”æ€§å—é™ç­‰é—®é¢˜ã€‚ä¸ºæ­¤æå‡ºAPI - GUIèŒƒå¼ï¼Œæ•´åˆå¤§è§„æ¨¡è‡ªåŠ¨æ„å»ºçš„APIç”Ÿæ€ç³»ç»Ÿä¸ä¼ ç»ŸGUIæ“ä½œï¼Œä»ä»¥äººä¸ºä¸­å¿ƒè½¬å‘ä»¥æœºå™¨ä¸ºå¯¼å‘çš„äº¤äº’æ¡†æ¶ï¼Œè§£å†³äººç±»è®¾è®¡ç•Œé¢ä¸æ™ºèƒ½ä»£ç†èƒ½åŠ›ä¸åŒ¹é…é—®é¢˜ï¼Œæå‡ä»»åŠ¡æ“ä½œæ•ˆç‡ä¸æ³›åŒ–æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤§è§„æ¨¡åˆ†å¸ƒå¼RLè®­ç»ƒåŸºç¡€è®¾æ–½
ç¯å¢ƒå¯æ‰©å±•æ€§é™åˆ¶å¤§è§„æ¨¡ä»£ç†è®­ç»ƒï¼Œé€šè¿‡é‡æ„åŸºäºDockerå’ŒgRPCåè®®çš„è™šæ‹Ÿæœºé›†ç¾¤ï¼Œæ­å»ºåˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€è®¾æ–½ï¼Œæ”¯æŒæ•°åƒå¹¶è¡Œç¯å¢ƒï¼Œå®ç°ç©ºå‰å¯æ‰©å±•æ€§ä¸”ä¸AgentBenchå…¼å®¹ï¼Œçªç ´RLåœ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†è®­ç»ƒä¸­è§„æ¨¡å®éªŒçš„ç“¶é¢ˆï¼Œç»“åˆAgentRLæ¡†æ¶å®ç°é«˜æ•ˆå¼‚æ­¥è®­ç»ƒï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEntropulseè®­ç»ƒç­–ç•¥
é•¿æ—¶é—´RLè®­ç»ƒå­˜åœ¨ç†µåç¼©å’ŒKLæ•£åº¦ç´¯ç§¯ç­‰åœæ»ä¸æ”¶æ•›é—®é¢˜ï¼Œæå‡ºEntropulseï¼Œé€šè¿‡åœ¨RLå’Œå‘¨æœŸæ€§ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µä¹‹é—´ç­–ç•¥æ€§äº¤æ›¿ï¼Œç³»ç»Ÿè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç»´æŒæ¢ç´¢èƒ½åŠ›å¹¶ç¡®ä¿æ€§èƒ½æŒç»­æå‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºComputerRLè®­ç»ƒçš„AutoGLM - OS - 9Bå–å¾—48.1%çš„æœ€æ–°å‡†ç¡®ç‡ï¼Œè¶…è¿‡OpenAI CUA o3ï¼ˆ42.9%ï¼‰ã€UI - TARS - 1.5ï¼ˆ42.5%ï¼‰ã€Anthropic Claude Sonnet 4ï¼ˆ30.7%ï¼‰ç­‰å…¶ä»–å…ˆè¿›æ¨¡å‹ï¼›ä¸”Entropulseæ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼Œè®­ç»ƒå¥–åŠ±æ›´é«˜ï¼Œæå‡äº†å­¦ä¹ æ•ˆç‡ä¸æœ€ç»ˆæ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨äº¤äº’èŒƒå¼ä¸Šï¼Œæ•´åˆä¸åŒæ“ä½œæ–¹å¼æ„å»ºæ›´é€‚é…æ™ºèƒ½ä»£ç†çš„äº¤äº’æ¨¡å¼ï¼Œä¸ºè·¨ç•Œé¢äº¤äº’ç±»ä»»åŠ¡æä¾›æ€è·¯ï¼›åœ¨è®­ç»ƒåŸºç¡€è®¾æ–½æ–¹é¢ï¼Œåˆ©ç”¨å®¹å™¨å’Œåˆ†å¸ƒå¼åè®®æ­å»ºå¤§è§„æ¨¡å¹¶è¡Œç¯å¢ƒï¼Œå¯¹éœ€å¤§è§„æ¨¡å®éªŒæ”¯æ’‘çš„ç ”ç©¶æœ‰å‚è€ƒä»·å€¼ï¼›åœ¨è®­ç»ƒç­–ç•¥ä¸Šï¼Œé’ˆå¯¹å¼ºåŒ–å­¦ä¹ é•¿æœŸè®­ç»ƒé—®é¢˜ï¼Œé€šè¿‡é˜¶æ®µäº¤æ›¿æ–¹å¼ç¼“è§£è®­ç»ƒå›°å¢ƒï¼Œä¸ºè§£å†³ç±»ä¼¼æ¨¡å‹è®­ç»ƒæ”¶æ•›ä¸æ€§èƒ½åœæ»é—®é¢˜æä¾›äº†åˆ›æ–°æ–¹æ³•å€Ÿé‰´ã€‚

## beyond-turing--memory-amortized-inference-as-a-foundation-for-cognitive-computation
### Abstract
Intelligence is fundamentally non-ergodic: it emerges not from uniform
sampling or optimization from scratch, but from the structured reuse of prior
inference trajectories. We introduce Memory-Amortized Inference (MAI) as a
formal framework in which cognition is modeled as inference over latent cycles
in memory, rather than recomputation through gradient descent. MAI systems
encode inductive biases via structural reuse, minimizing entropy and enabling
context-aware, structure-preserving inference. This approach reframes cognitive
systems not as ergodic samplers, but as navigators over constrained latent
manifolds, guided by persistent topological memory. Through the lens of
delta-homology, we show that MAI provides a principled foundation for
Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a
local inference operator over cycle-consistent memory states. Furthermore, we
establish a time-reversal duality between MAI and reinforcement learning:
whereas RL propagates value forward from reward, MAI reconstructs latent causes
backward from memory. This inversion paves a path toward energy-efficient
inference and addresses the computational bottlenecks facing modern AI. MAI
thus offers a unified, biologically grounded theory of intelligence based on
structure, reuse, and memory. We also briefly discuss the profound implications
of MAI for achieving artificial general intelligence (AGI).
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¶…è¶Šå›¾çµï¼šè®°å¿†æ‘Šé”€æ¨ç†ï¼ˆMAIï¼‰ä½œä¸ºè®¤çŸ¥è®¡ç®—çš„åŸºçŸ³

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è‡ªç„¶ä¸äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­ï¼Œæ™ºèƒ½æœ¬è´¨ä¸Šå…·æœ‰ééå†æ€§ï¼Œå®ƒå¹¶éæºäºå‡åŒ€é‡‡æ ·æˆ–ä»å¤´å¼€å§‹çš„ä¼˜åŒ–ï¼Œè€Œæ˜¯æ¥è‡ªå¯¹å…ˆå‰æ¨ç†è½¨è¿¹çš„ç»“æ„åŒ–å¤ç”¨ã€‚ä¼ ç»Ÿçš„è¯¸å¦‚å¼ºåŒ–å­¦ä¹ ã€è´å¶æ–¯å­¦ä¹ ç­‰éå†æ€§æ¨¡å‹ï¼Œå°†æ™ºèƒ½è§†ä¸ºéšæœºæ¸¸èµ°æˆ–å…¨å±€å‡½æ•°æ‹Ÿåˆï¼Œæœªå……åˆ†è€ƒè™‘è®°å¿†ã€ç›®æ ‡å’Œä¸Šä¸‹æ–‡å¡‘é€ çš„ç»“æ„åŒ–ã€è·¯å¾„ä¾èµ–è¡Œä¸ºï¼Œä¸”ç°ä»£AIé¢ä¸´è®¡ç®—ç“¶é¢ˆä¸èƒ½æ•ˆé—®é¢˜ã€‚åŒæ—¶ï¼Œæ™ºèƒ½å…·æœ‰æ—¶é—´ä¸å¯é€†æ€§ï¼ŒçŸ¥è¯†ç§¯ç´¯ç­‰å¼•å…¥æ—¶é—´ç®­å¤´ï¼Œè¿èƒŒéå†åŠ¨åŠ›å­¦å‡è®¾ï¼Œå› æ­¤éœ€è¦æ–°æ¡†æ¶æ¥æ•æ‰è¿™ä¸€æ ¸å¿ƒä¸å¯¹ç§°æ€§ï¼Œç†è§£è®°å¿†ã€æ¨ç†ä¸æ™ºèƒ½è¡Œä¸ºçš„å…³ç³»ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè®°å¿†æ‘Šé”€æ¨ç†ï¼ˆMAIï¼‰æ¡†æ¶  
å°†è®¤çŸ¥å»ºæ¨¡ä¸ºå¯¹è®°å¿†ä¸­æ½œåœ¨å¾ªç¯çš„æ¨ç†ï¼Œè€Œéé€šè¿‡æ¢¯åº¦ä¸‹é™é‡æ–°è®¡ç®—ã€‚MAIç³»ç»Ÿé€šè¿‡ç»“æ„åŒ–å¤ç”¨ç¼–ç å½’çº³åå·®ï¼Œæœ€å°åŒ–ç†µï¼Œå®ç°æƒ…å¢ƒæ„ŸçŸ¥ã€ç»“æ„ä¿æŒçš„æ¨ç†ã€‚æ™ºèƒ½ç³»ç»Ÿä¸å†æ˜¯éå†æ€§é‡‡æ ·å™¨ï¼Œè€Œæ˜¯åœ¨æŒä¹…æ‹“æ‰‘è®°å¿†å¼•å¯¼ä¸‹ï¼Œåœ¨å—é™æ½œåœ¨æµå½¢ä¸Šçš„å¯¼èˆªå™¨ï¼Œæ¨ç†åŸºäºè®°å¿†ä¸­ç»“æ„åŒ–å¾ªç¯çš„å¤ç”¨ï¼ŒæŠŠè®°å¿†ä»è¢«åŠ¨å­˜å‚¨è½¬ä¸ºä¸»åŠ¨æ‘Šé”€æ¨ç†çš„åŸºç¡€ï¼Œè®©é¢„æµ‹å’Œé‡å»ºä»æ½œåœ¨å¾ªç¯è€Œéå‚æ•°ä¼˜åŒ–ä¸­äº§ç”Ÿã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºdelta - åŒè°ƒä¸ºMountcastleé€šç”¨çš®å±‚ç®—æ³•æä¾›åŸç†æ€§åŸºç¡€  
å€ŸåŠ©delta - åŒè°ƒï¼Œå°†æ¯ä¸ªçš®å±‚æŸ±å»ºæ¨¡ä¸ºå¯¹å¾ªç¯ä¸€è‡´è®°å¿†çŠ¶æ€çš„å±€éƒ¨æ¨ç†ç®—å­ï¼Œçš®å±‚è¿æ¥æŠŠè¿™äº›ç®—å­ç²˜åˆä¸ºå…¨è„‘èŒƒå›´çš„è®°å¿† - æ¨ç†å±‚ã€‚delta - åŒè°ƒæ•æ‰äº†å±€éƒ¨å¾ªç¯åœ¨åˆ†å±‚å°ºåº¦ä¸Šçš„æŒç»­ã€äº¤äº’å’Œç¨³å®šï¼Œä¸ºè·¨æ¨¡æ€æ¨ç†ã€æ„Ÿè§‰è¿åŠ¨æ³›åŒ–å’Œè®¤çŸ¥è¡¨å¾çš„é²æ£’æ€§æä¾›åŸç†è§£é‡Šã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå»ºç«‹MAIä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ—¶é—´åè½¬å¯¹å¶æ€§  
RLä»å¥–åŠ±å‘å‰ä¼ æ’­ä»·å€¼ï¼Œè€ŒMAIä»è®°å¿†å‘åé‡å»ºæ½œåœ¨åŸå› ã€‚è¿™ç§åè½¬ä¸ºé«˜èƒ½æ•ˆæ¨ç†é“ºè·¯ï¼Œè§£å†³ç°ä»£AIçš„è®¡ç®—ç“¶é¢ˆã€‚MAIå°†å­¦ä¹ é‡æ„ä¸ºæ„å»ºå’Œç¨³å®šå¯å¤ç”¨æ‹“æ‰‘ç®—å­ç­‰çš„è¿‡ç¨‹ï¼Œè€Œéä»æ•°æ®æ‹Ÿåˆå…¨å±€å‡½æ•°ï¼Œè¿˜ä¸ºè®°å¿†ä¸å†³ç­–åˆ¶å®šæ­å»ºç†è®ºæ¡¥æ¢ï¼Œç”¨ç»“æ„æ„ŸçŸ¥å¤ç”¨æ›¿ä»£èƒ½æºå¯†é›†å‹è¿­ä»£ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡æœªæåŠä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®éªŒç»“æœå‘ˆç°ï¼ˆå¦‚å¯¹æ¯”å®éªŒæŒ‡æ ‡ç­‰ï¼‰ï¼Œä¸»è¦ä»ç†è®ºå±‚é¢è®ºè¯MAIçš„åˆç†æ€§ã€ä¸ç”Ÿç‰©ç†è®ºï¼ˆå¦‚Mountcastleé€šç”¨çš®å±‚ç®—æ³•ï¼‰çš„å¥‘åˆåº¦ã€ä¸RLçš„å¯¹å¶å…³ç³»ç­‰ï¼Œé€šè¿‡ç†è®ºæ¨å¯¼ã€æ¦‚å¿µç±»æ¯”ï¼ˆå¦‚éå†ä¸ééå†æ¨ç†è½¨è¿¹å¯¹æ¯”ã€delta - åŒè°ƒç±»æ¯”ç­‰ï¼‰æ¥æ”¯æ’‘MAIæ¡†æ¶çš„åˆ›æ–°æ€§ä¸ä»·å€¼ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è®¤çŸ¥å»ºæ¨¡è§†è§’ï¼šå¯å‘é‡æ–°å®¡è§†æ™ºèƒ½ç³»ç»Ÿï¼Œå°†å…¶è§†ä¸ºåœ¨å—é™æµå½¢ä¸ŠåŸºäºè®°å¿†å¾ªç¯æ¨ç†çš„å¯¼èˆªå™¨ï¼Œè€Œéç®€å•çš„éå†é‡‡æ ·å™¨ï¼Œä¸ºè®¤çŸ¥ç§‘å­¦é¢†åŸŸå»ºæ¨¡æ™ºèƒ½è¡Œä¸ºæä¾›æ–°èŒƒå¼ã€‚  
2. äººå·¥æ™ºèƒ½æ–¹å‘ï¼šé’ˆå¯¹ç°ä»£AIè®¡ç®—ä¸èƒ½æ•ˆç“¶é¢ˆï¼ŒMAIçš„ç»“æ„æ„ŸçŸ¥å¤ç”¨æ€è·¯ï¼Œä¸ºæ‰“é€ ç”Ÿç‰©åˆç†ã€é«˜èƒ½æ•ˆçš„äººå·¥æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰æä¾›äº†åŸºäºè®°å¿†è€Œéæš´åŠ›è®¡ç®—çš„è·¯å¾„å‚è€ƒã€‚  
3. è·¨å­¦ç§‘ç†è®ºèåˆï¼šå±•ç¤ºäº†å°†æ‹“æ‰‘å­¦ï¼ˆå¦‚delta - åŒè°ƒï¼‰ã€ç¥ç»ç§‘å­¦ï¼ˆMountcastleé€šç”¨çš®å±‚ç®—æ³•ï¼‰ä¸è®¡ç®—æœºç§‘å­¦ï¼ˆå¼ºåŒ–å­¦ä¹ ç­‰ï¼‰ç†è®ºèåˆçš„èŒƒä¾‹ï¼Œä¸ºè·¨å­¦ç§‘ç ”ç©¶æ™ºèƒ½æä¾›æ–¹æ³•è®ºå€Ÿé‰´ï¼Œæ¨åŠ¨ä¸åŒé¢†åŸŸç†è®ºåœ¨æ™ºèƒ½ç ”ç©¶ä¸­çš„äº¤å‰åº”ç”¨ã€‚

## from-trial-and-error-to-improvement--a-systematic-analysis-of-llm-exploration-mechanisms-in-rlvr
### Abstract
Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.


## amft--aligning-llm-reasoners-by-meta-learning-the-optimal-imitation-exploration-balance
### Abstract
Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.


## gtpo-and-grpo-s--token-and-sequence-level-reward-shaping-with-policy-entropy
### Abstract
Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨ç­–ç•¥ç†µé‡å¡‘å¥–åŠ±ï¼Œçªç ´é•¿é“¾æ¨ç†ç“¶é¢ˆï¼šGTPOä¸GRPO - S

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›è¿›æ­¥ï¼Œä½†åƒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰è¿™ç±»ç®—æ³•å­˜åœ¨ç²—ç²’åº¦ä¿¡ç”¨åˆ†é…é—®é¢˜â€”â€”å¯¹åºåˆ—ä¸­æ‰€æœ‰tokenæ–½åŠ ç»Ÿä¸€å¥–åŠ±ï¼Œåœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ï¼ˆå¦‚å¤šæ­¥éª¤æ•°å­¦è¯æ˜ï¼‰ä¸­å¼Šç«¯æ˜æ˜¾ã€‚æ¯”å¦‚50æ­¥æ•°å­¦è¯æ˜å‰49æ­¥æ­£ç¡®ã€æœ€åä¸€æ­¥é”™è¯¯å°±ç»™å…¨åºåˆ—é›¶å¥–åŠ±ï¼Œè®©æ­£ç¡®æ­¥éª¤å’Œé”™è¯¯æ­¥éª¤å—åŒç­‰æƒ©ç½šï¼Œç¨€ç–ç²—ç³™çš„åé¦ˆä¿¡å·æˆäº†LLMé•¿é“¾æ¨ç†èƒ½åŠ›æå‡çš„ç“¶é¢ˆã€‚åŒæ—¶ï¼Œç°æœ‰æ¨¡å‹å¯¹é½ç®—æ³•ä»å¤æ‚é«˜æˆæœ¬å‘ç®€å•é«˜æ•ˆæ¼”è¿›ï¼ŒGRPOè™½ç®€åŒ–æµç¨‹å´åœ¨ä¿¡ç”¨åˆ†é…ç²¾åº¦ä¸Šå¦¥åï¼Œå› æ­¤æœ¬æ–‡å¸Œæœ›é€šè¿‡åŠ¨æ€ç†µåŠ æƒè§£å†³è¯¥é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºGroup Token Policy Optimizationï¼ˆGTPOï¼‰
æ ¸å¿ƒæ˜¯ä¸ºæ¯ä¸ªtokenåˆ†é…ç†µåŠ æƒå¥–åŠ±å®ç°ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ã€‚ä¾æ®æ˜¯æ­£ç¡®æ¨ç†åºåˆ—ä¸­ï¼Œæ¨¡å‹ç­–ç•¥ç†µé«˜çš„tokenä½ç½®å¸¸å¯¹åº”æ¨ç†å…³é”®å†³ç­–ç‚¹æˆ–ä¸ç¡®å®šæ€§æ—¶åˆ»ï¼ˆå¦‚é€‰æ‹©åº”ç”¨å“ªä¸ªæ•°å­¦å®šç†ï¼‰ï¼Œç”¨tokençº§ç†µåŠ¨æ€åŠ æƒå¥–åŠ±ï¼Œèƒ½è®©ç­–ç•¥æ¢¯åº¦æ›´æ–°æ›´èšç„¦å¯¹æœ€ç»ˆç»“æœå…³é”®çš„æ—¶åˆ»ï¼Œåœ¨GRPOæ¡†æ¶å†…é¦–æ¬¡å®ç°çœŸæ­£çš„tokençº§ä¿¡ç”¨åˆ†é…ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSequence - Level Group Relative Policy Optimizationï¼ˆGRPO - Sï¼‰
è¿™æ˜¯è½»é‡çº§åºåˆ—çº§æ–¹æ¡ˆï¼ŒåŸºäºåºåˆ—çš„å¹³å‡tokenç†µä¸ºæ¯ä¸ªåºåˆ—åˆ†é…ç†µåŠ æƒå¥–åŠ±ï¼Œåœ¨æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚åŒæ—¶ï¼Œåœ¨å®šä¹‰GTPOå’ŒGRPO - Sç›®æ ‡å‡½æ•°æ—¶ï¼Œæä¾›å¤šç§é€‰é¡¹ä»¥å¹³è¡¡è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œè¿˜ä»æ–¹å·®å‡å°‘è§’åº¦å¯¹ç›®æ ‡å‡½æ•°è®¾è®¡è¿›è¡Œç†è®ºåˆ†æï¼Œè®ºè¯å…¶æ”¶æ•›æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTPOå’ŒGRPO - Sæ˜¾è‘—è¶…è¶Šå¼ºåŸºçº¿ï¼ˆåŒ…æ‹¬GRPOå’ŒDAPOç­‰ï¼‰ï¼ŒéªŒè¯äº†åŸºäºç†µçš„ä¿¡ç”¨åˆ†é…æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜ç†µåŠ æƒæœºåˆ¶æ˜¯æ€§èƒ½æå‡çš„å…³é”®é©±åŠ¨åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜æ´å¯Ÿè§’åº¦ï¼šå…³æ³¨åˆ°ç°æœ‰ç®—æ³•åœ¨é•¿é“¾æ¨ç†åœºæ™¯ä¸‹ä¿¡ç”¨åˆ†é…çš„ç¼ºé™·ï¼Œå°†å…¶å®šä½ä¸ºèƒ½åŠ›æå‡ç“¶é¢ˆï¼Œè¿™ç§ç²¾å‡†æ‰¾é—®é¢˜çš„æ€è·¯å€¼å¾—å€Ÿé‰´ï¼Œåœ¨ä¼˜åŒ–æ¨¡å‹æ—¶è¦å…ˆæ·±å…¥åˆ†æç°æœ‰æ–¹æ¡ˆçŸ­æ¿ã€‚
2. åˆ›æ–°æ€è·¯ï¼šåˆ©ç”¨æ¨ç†ä¸­tokençš„ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰ä½œä¸ºä¿¡ç”¨åˆ†é…å¯å‘å¼ä¿¡æ¯ï¼ŒæŠŠæ¨¡å‹å†…åœ¨ä¸ç¡®å®šæ€§è½¬åŒ–ä¸ºä¼˜åŒ–ä¿¡å·ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨LLMæ¨ç†ä¼˜åŒ–ä¸­çš„å¥–åŠ±è®¾è®¡æä¾›äº†æ–°è§†è§’â€”â€”å¯å…³æ³¨æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†…åœ¨ç‰¹å¾ï¼ˆå¦‚ç†µï¼‰æ¥ç»†åŒ–å¥–åŠ±æœºåˆ¶ã€‚
3. æ–¹æ³•æ‹“å±•æ€§ï¼šæå‡ºçš„GTPOå’ŒGRPO - Såœ¨GRPOåŸºç¡€ä¸Šåšä¸åŒç²’åº¦ï¼ˆtokenå’Œåºåˆ—çº§ï¼‰çš„æ”¹è¿›ï¼Œä¸”ç»™å‡ºç›®æ ‡å‡½æ•°å¤šé€‰é¡¹å¹³è¡¡æ•ˆç‡ä¸æ€§èƒ½ï¼Œè¿™ç§ä»ä¸åŒå±‚çº§ä¼˜åŒ–ä¸”é¢„ç•™çµæ´»æ€§çš„è®¾è®¡æ€è·¯ï¼Œå¯¹åç»­ç®—æ³•ä¼˜åŒ–æœ‰å‚è€ƒä»·å€¼ï¼Œæ¯”å¦‚åœ¨å…¶ä»–åŸºäºRLçš„æ¨¡å‹ä¼˜åŒ–ä»»åŠ¡ä¸­ï¼Œå¯æ€è€ƒå¦‚ä½•ä»ä¸åŒç²’åº¦ã€ä¸åŒå†…åœ¨ç‰¹å¾ç»´åº¦å»æ”¹è¿›å¥–åŠ±æˆ–æŸå¤±å‡½æ•°ã€‚

## light-if--endowing-llms-with-generalizable-reasoning-via-preview-and-self-checking-for-complex-instruction-following
### Abstract
While advancements in the reasoning abilities of LLMs have significantly
enhanced their performance in solving mathematical problems, coding tasks, and
general puzzles, their effectiveness in accurately adhering to instructions
remains inconsistent, particularly with more complex directives. Our
investigation identifies lazy reasoning during the thinking stage as the
primary factor contributing to poor instruction adherence. To mitigate this
issue, we propose a comprehensive framework designed to enable rigorous
reasoning processes involving preview and self-checking, essential for
satisfying strict instruction constraints. Specifically, we first generate
instructions with complex constraints and apply a filtering process to obtain
valid prompts, resulting in three distinct prompt datasets categorized as hard,
easy, and pass. Then, we employ rejection sampling on the pass prompts to
curate a small yet high-quality dataset, enabling a cold-start initialization
of the model and facilitating its adaptation to effective reasoning patterns.
Subsequently, we employ an entropy-preserving supervised fine-tuning
(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)
reinforcement learning guided by rule-based dense rewards. This approach
encourages the model to transform its reasoning mechanism, ultimately fostering
generalizable reasoning abilities that encompass preview and self-checking.
Extensive experiments conducted on instruction-following benchmarks demonstrate
remarkable performance improvements across various model scales. Notably, our
Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1
and closed-source models like Doubao-1.6.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Light-IFï¼šè®©å¤§æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤éµå¾ªä¸­å…·å¤‡å¯æ³›åŒ–æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦é—®é¢˜ã€ç¼–ç ä»»åŠ¡ç­‰æ–¹é¢æ¨ç†èƒ½åŠ›è¿›æ­¥æ˜¾è‘—ï¼Œä½†åœ¨å‡†ç¡®éµå¾ªå¤æ‚æŒ‡ä»¤æ—¶è¡¨ç°ä»ä¸ä¸€è‡´ã€‚ç ”ç©¶å‘ç°æ€è€ƒé˜¶æ®µçš„â€œæƒ°æ€§æ¨ç†â€æ˜¯æŒ‡ä»¤éµå¾ªæ•ˆæœå·®çš„ä¸»å› ï¼Œå³æ¨¡å‹é¢å¯¹å¤æ‚æŒ‡ä»¤æ—¶ä»…ç®€å•é‡å¤æŒ‡ä»¤ï¼ŒæœªçœŸæ­£æ£€æŸ¥æ˜¯å¦åˆè§„ã€‚åŒæ—¶ï¼Œç°æœ‰æå‡å¤æ‚æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„æ–¹æ³•ä¾èµ–å¤§é‡æœ‰ç›‘ç£æ•°æ®ï¼Œæ•°æ®æ”¶é›†éš¾åº¦å¤§ã€‚æ­¤å¤–ï¼ŒæŒ‡ä»¤éµå¾ªæ˜¯å¤§æ¨¡å‹ä»å•çº¯ä¸‹ä¸€ä¸ªtokené¢„æµ‹å™¨å‘å®ç”¨å¯é åŠ©æ‰‹è½¬å˜çš„å…³é”®èƒ½åŠ›ï¼Œè‹¥æ¨¡å‹æ— æ³•å‡†ç¡®éµå¾ªæŒ‡ä»¤ï¼Œåœ¨åŒ»ç–—ã€è‡ªåŠ¨é©¾é©¶ç­‰çœŸå®é¢†åŸŸåº”ç”¨ä¼šå—é™ï¼Œæ‰€ä»¥æå‡å¤æ‚æŒ‡ä»¤éµå¾ªèƒ½åŠ›è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºåˆ†å±‚æŒ‡ä»¤æ•°æ®é›†  
å…ˆç”Ÿæˆå¸¦å¤æ‚çº¦æŸçš„æŒ‡ä»¤ï¼Œç»ç­›é€‰å¾—åˆ°â€œéš¾ï¼ˆhardï¼‰â€â€œæ˜“ï¼ˆeasyï¼‰â€â€œé€šè¿‡ï¼ˆpassï¼‰â€ä¸‰ç±»ä¸åŒéš¾åº¦çš„æœ‰æ•ˆæç¤ºæ•°æ®é›†ï¼Œä¸ºåç»­è®­ç»ƒæä¾›åŸºç¡€æ•°æ®æ”¯æ’‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå†·å¯åŠ¨æ•°æ®é›†æ„å»º  
å¯¹â€œpassâ€ç±»æç¤ºé‡‡ç”¨æ‹’ç»é‡‡æ ·ï¼Œç²¾å¿ƒæ‰“é€ å°è€Œä¼˜è´¨çš„æ•°æ®é›†ï¼Œå®ç°æ¨¡å‹å†·å¯åŠ¨åˆå§‹åŒ–ï¼ŒåŠ©åŠ›æ¨¡å‹é€‚åº”æœ‰æ•ˆæ¨ç†æ¨¡å¼ï¼Œè§£å†³äº†ä¾èµ–å¤§è§„æ¨¡æ•°æ®çš„ç—›ç‚¹ï¼Œç”¨å°æ•°æ®å¼€å¯æœ‰æ•ˆè®­ç»ƒã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»“åˆEntropy - SFTä¸TEA - RLçš„è®­ç»ƒç­–ç•¥  
é‡‡ç”¨ä¿ç•™ç†µçš„ç›‘ç£å¾®è°ƒï¼ˆEntropy - preserving supervised fine - tuningï¼ŒEntropy - SFTï¼‰ç­–ç•¥ï¼Œæ­é…åŸºäºè§„åˆ™å¯†é›†å¥–åŠ±å¼•å¯¼çš„tokençº§ç†µè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ï¼ˆtoken - wise entropy - adaptive reinforcement learningï¼ŒTEA - RLï¼‰ã€‚è¿™ç§ç»„åˆä¿ƒä½¿æ¨¡å‹è½¬å˜æ¨ç†æœºåˆ¶ï¼ŒåŸ¹å…»å‡ºåŒ…å«â€œé¢„è§ˆï¼ˆpreviewï¼‰â€ä¸â€œè‡ªæ£€æŸ¥ï¼ˆself - checkingï¼‰â€çš„å¯æ³›åŒ–æ¨ç†èƒ½åŠ›ï¼Œä»è®­ç»ƒç­–ç•¥å±‚é¢æ¨åŠ¨æ¨¡å‹æ¨ç†èƒ½åŠ›å‡çº§ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸­å¼€å±•å¤§é‡å®éªŒï¼Œä¸åŒè§„æ¨¡æ¨¡å‹éƒ½æœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚å…¶ä¸­Light - IF - 32Bæ¨¡å‹è¡¨ç°çªå‡ºï¼Œè¶…è¿‡äº†åƒDeepSeek - R1è¿™æ ·æ›´å¤§çš„å¼€æºæ¨¡å‹ä»¥åŠDoubao - 1.6è¿™ç±»é—­æºæ¨¡å‹ï¼Œæœ‰åŠ›è¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®åˆ†å±‚ä¸ç­›é€‰æ€è·¯ï¼šåœ¨æ„å»ºè®­ç»ƒæ•°æ®æ—¶ï¼Œé€šè¿‡åˆç†åˆ†å±‚å’Œç­›é€‰å¾—åˆ°ä¸åŒéš¾åº¦æ•°æ®ï¼Œèƒ½é’ˆå¯¹æ€§åœ°ä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ¢¯åº¦å¼æ•°æ®èµ„æºï¼Œè¿™ç§æ•°æ®å¤„ç†æ–¹å¼å¯æ¨å¹¿åˆ°å…¶ä»–éœ€è¦åˆ†éš¾åº¦è®­ç»ƒçš„ä»»åŠ¡åœºæ™¯ã€‚  
2. å†·å¯åŠ¨å°æ•°æ®è®­ç»ƒï¼šåˆ©ç”¨æ‹’ç»é‡‡æ ·æ„å»ºå°è€Œä¼˜çš„å†·å¯åŠ¨æ•°æ®é›†ï¼Œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æ¨¡å‹åˆå§‹åŒ–æä¾›äº†æ–°æ€è·¯ï¼Œé¿å…è¿‡åº¦ä¾èµ–å¤§è§„æ¨¡æ•°æ®æ”¶é›†ã€‚  
3. è®­ç»ƒç­–ç•¥ç»„åˆåˆ›æ–°ï¼šå°†ä¿ç•™ç†µçš„ç›‘ç£å¾®è°ƒä¸tokençº§è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œä¸ºæ¨¡å‹æ¨ç†æœºåˆ¶è½¬å˜æä¾›äº†æœ‰æ•ˆæŠ€æœ¯è·¯å¾„ï¼Œåœ¨æå‡æ¨¡å‹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚æŒ‡ä»¤éµå¾ªï¼‰çš„è®­ç»ƒç­–ç•¥è®¾è®¡ä¸Šå…·æœ‰å‚è€ƒä»·å€¼ï¼Œå¯å¯å‘åç»­é’ˆå¯¹ä¸åŒèƒ½åŠ›æå‡çš„è®­ç»ƒæ–¹æ³•åˆ›æ–°ã€‚

## decomposing-the-entropy-performance-exchange--the-missing-keys-to-unlocking-effective-reinforcement-learning
### Abstract
Recently, reinforcement learning with verifiable rewards (RLVR) has been
widely used for enhancing the reasoning abilities of large language models
(LLMs). A core challenge in RLVR involves managing the exchange between entropy
and performance of policies. Despite the importance of this exchange, a
fine-grained understanding of when and how this exchange operates most
effectively remains limited. To bridge this gap, we conduct a systematic
empirical analysis of the entropy-performance exchange mechanism of RLVR across
different levels of granularity. Specifically, we first divide the training
process into two distinct stages based on entropy dynamics, i.e., rising stage
and plateau stage, and then systematically investigate how this mechanism
varies across stage-level, instance-level, and token-level granularitiess. Our
analysis reveals that, in the rising stage, entropy reduction in negative
samples facilitates the learning of effective reasoning patterns, which in turn
drives rapid performance gains. Moreover, in the plateau stage, learning
efficiency strongly correlates with high-entropy tokens present in
low-perplexity samples and those located at the end of sequences. Motivated by
these findings, we propose two methods that dynamically adjust the reward
signal using perplexity and positional information to focus RL updates on
tokens that exhibit high learning potential, achieving improvements compared to
the baseline methods on various LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ‹†è§£ç†µä¸æ€§èƒ½äº¤æ¢ï¼šè§£é”é«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„å…³é”®å¯†ç 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶ä¸­ç­–ç•¥çš„ç†µä¸æ€§èƒ½ä¹‹é—´çš„äº¤æ¢æœºåˆ¶ç¼ºä¹ç»†ç²’åº¦ç†è§£ã€‚ç°æœ‰ç ”ç©¶å¯¹ç†µ - æ€§èƒ½æƒè¡¡çš„åˆ†æç²’åº¦è¾ƒç²—ï¼Œå°†RLVRè®­ç»ƒè§†ä¸ºå•ä¸€è¿‡ç¨‹ï¼Œæœªæ·±å…¥æ¢ç©¶è®­ç»ƒè¿‡ç¨‹ä¸­ç†µåŠ¨æ€ä¸æ¨¡å‹æ€§èƒ½çš„äº¤äº’ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè®ºæ–‡å¯¹RLVRä¸­ç†µ - æ€§èƒ½äº¤æ¢æœºåˆ¶åœ¨ä¸åŒç²’åº¦ä¸‹å±•å¼€ç³»ç»Ÿå®è¯åˆ†æã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ†é˜¶æ®µåˆ†æè®­ç»ƒè¿‡ç¨‹
å°†RLVRè®­ç»ƒè¿‡ç¨‹ä¾æ®ç†µåŠ¨æ€åˆ†ä¸ºä¸Šå‡é˜¶æ®µå’Œå¹³å°é˜¶æ®µã€‚åœ¨ä¸Šå‡é˜¶æ®µï¼Œè´Ÿæ ·æœ¬çš„ç†µå‡æœ‰åŠ©äºæ¨¡å‹å»ºç«‹æœ‰æ•ˆæ¨ç†æ¨¡å¼ï¼›å¹³å°é˜¶æ®µåˆ™èšç„¦é«˜ç†µ tokens å®ç°ç¼“æ…¢ä½†ç¨³å®šçš„æ€§èƒ½æå‡ï¼Œæ˜ç¡®ä¸åŒé˜¶æ®µæ€§èƒ½æå‡æœºåˆ¶å·®å¼‚ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç²’åº¦æœºåˆ¶æ¢ç©¶
ä»é˜¶æ®µã€å®ä¾‹ã€token ä¸‰ä¸ªç²’åº¦ç³»ç»Ÿç ”ç©¶ç†µ - æ€§èƒ½äº¤æ¢æœºåˆ¶ã€‚å®ä¾‹ç²’åº¦ä¸‹å‘ç°ç†µæ˜¾è‘—å˜åŒ–çš„ tokens å¤šæ¥è‡ªä½å›°æƒ‘åº¦å“åº”ï¼›token ç²’åº¦ä¸‹æ­ç¤ºå“åº”å¼€å¤´é«˜ç†µ tokens åŠ©åŠ›æ¢ç´¢ã€ç»“å°¾ tokens æ‰¿è½½ä»»åŠ¡ç‰¹å®šä¿¡æ¯è¾…åŠ©æœ€ç»ˆå†³ç­–ï¼Œæ˜ç¡®ä¸åŒ tokens å­¦ä¹ æ½œåŠ›å·®å¼‚ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºå‘ç°çš„å¥–åŠ±å¡‘é€ æ–¹æ³•
ä¾æ®ä¸Šè¿°å®è¯å‘ç°ï¼Œæå‡ºä¸¤ç§å¥–åŠ±å¡‘é€ æ–¹æ³•ã€‚åˆ©ç”¨å›°æƒ‘åº¦å’Œä½ç½®ä¿¡æ¯åŠ¨æ€è°ƒæ•´ token ä¼˜åŠ¿ï¼Œå¼•å¯¼æ¨¡å‹æ›´æ–°èšç„¦é«˜å­¦ä¹ æ½œåŠ› tokensï¼Œé‡Šæ”¾RLVRæ½œåŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡é€šè¿‡å¯¹ä¸åŒç²’åº¦ä¸‹ç†µ - æ€§èƒ½äº¤æ¢æœºåˆ¶çš„åˆ†æï¼ŒéªŒè¯äº†å„é˜¶æ®µã€å®ä¾‹ã€token å±‚é¢çš„ç°è±¡ã€‚åŸºäºæå‡ºçš„å¥–åŠ±å¡‘é€ æ–¹æ³•ï¼Œåœ¨å¤šç§LLMså’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•å®ç°äº†æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡é‡‡ç”¨å¤šç²’åº¦åˆ†ææ€è·¯ï¼Œä¸ºå¤æ‚è®­ç»ƒè¿‡ç¨‹æœºåˆ¶ç ”ç©¶æä¾›äº†èŒƒå¼ï¼Œå¯å€Ÿé‰´äºå…¶ä»–æ¶‰åŠåŠ¨æ€è¿‡ç¨‹ä¸æ€§èƒ½å…³è”çš„ç ”ç©¶ï¼›æå‡ºçš„åŸºäºå›°æƒ‘åº¦å’Œä½ç½®ä¿¡æ¯çš„å¥–åŠ±å¡‘é€ æ–¹æ³•ï¼Œä¸ºä¼˜åŒ–å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è°ƒèŠ‚æ€è·¯ï¼Œå¯å‘åç»­é’ˆå¯¹æ¨¡å‹ä¸åŒéƒ¨åˆ†ã€ä¸åŒé˜¶æ®µè®¾è®¡æ›´ç²¾ç»†çš„è®­ç»ƒç­–ç•¥ã€‚

## mol-rl--distilling-multi-step-environmental-feedback-into-llms-for-feedback-independent-reasoning
### Abstract
Large language models (LLMs) face significant challenges in effectively
leveraging sequential environmental feedback (EF) signals, such as natural
language evaluations, for feedback-independent chain-of-thought (CoT)
reasoning. Existing approaches either convert EF into scalar rewards, losing
rich contextual information, or employ refinement datasets, failing to exploit
the multi-step and discrete nature of EF interactions. To address these
limitations, we propose MoL-RL, a novel training paradigm that integrates
multi-step EF signals into LLMs through a dual-objective optimization
framework. Our method combines MoL (Mixture-of-Losses) continual training,
which decouples domain-specific EF signals (optimized via cross-entropy loss)
and general language capabilities (preserved via Kullback-Leibler divergence),
with GRPO-based post-training to distill sequential EF interactions into
single-step inferences. This synergy enables robust feedback-independent
reasoning without relying on external feedback loops. Experimental results on
mathematical reasoning (MATH-500, AIME24/AIME25) and code generation
(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art
performance with the Qwen3-8B model, while maintaining strong generalization
across model scales (Qwen3-4B). This work provides a promising approach for
leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities
in diverse domains.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MoL - RLï¼šå°†å¤šæ­¥ç¯å¢ƒåé¦ˆæ³¨å…¥å¤§æ¨¡å‹ï¼Œå®ç°æ— åé¦ˆç‹¬ç«‹æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰æ•ˆåˆ©ç”¨é¡ºåºç¯å¢ƒåé¦ˆï¼ˆEFï¼‰ä¿¡å·è¿›è¡Œæ— åé¦ˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå°†EFè½¬åŒ–ä¸ºæ ‡é‡å¥–åŠ±ï¼Œä¸¢å¤±ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›è¦ä¹ˆé‡‡ç”¨ç²¾ç‚¼æ•°æ®é›†ï¼Œæœªèƒ½åˆ©ç”¨EFäº¤äº’çš„å¤šæ­¥å’Œç¦»æ•£ç‰¹æ€§ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæœ¬æ–‡æå‡ºMoL - RLè®­ç»ƒèŒƒå¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMoL - RLè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åŒç›®æ ‡ä¼˜åŒ–æ¡†æ¶æ•´åˆå¤šæ­¥EFä¿¡å·åˆ°LLMsä¸­ã€‚é‡‡ç”¨MoLï¼ˆMixture - of - Lossesï¼‰æŒç»­è®­ç»ƒï¼Œè§£è€¦ç‰¹å®šé¢†åŸŸEFä¿¡å·ï¼ˆé€šè¿‡äº¤å‰ç†µæŸå¤±ä¼˜åŒ–ï¼‰å’Œé€šç”¨è¯­è¨€èƒ½åŠ›ï¼ˆé€šè¿‡Kullback - Leibleræ•£åº¦ä¿ç•™ï¼‰ï¼Œé˜²æ­¢é€šç”¨èƒ½åŠ›ç¾éš¾æ€§é—å¿˜åŒæ—¶å¸æ”¶ç‰¹å®šé¢†åŸŸä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆåŸºäºGRPOçš„åè®­ç»ƒï¼Œå°†é¡ºåºEFäº¤äº’æç‚¼ä¸ºå•æ­¥æ¨ç†ã€‚åœ¨æŒç»­é¢„è®­ç»ƒåï¼Œå®æ–½Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒååŒæ¿€æ´»æ¨¡å‹å›ºæœ‰é€šç”¨èƒ½åŠ›å’Œå‹ç¼©åçš„EFçŸ¥è¯†ï¼Œè¯±å¯¼å‡ºæ— åé¦ˆçš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦æ¨ç†ï¼ˆMATH - 500ã€AIME24/AIME25ï¼‰å’Œä»£ç ç”Ÿæˆï¼ˆCodeAgent - Testï¼‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoL - RLåœ¨Qwen3 - 8Bæ¨¡å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨¡å‹è§„æ¨¡ï¼ˆQwen3 - 4Bï¼‰ä¸Šä¿æŒå¼ºæ³›åŒ–æ€§ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡MoLæŒç»­è®­ç»ƒå¯¹EFçš„é¡ºåºæ•´åˆæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹RLçš„å“åº”æ€§ï¼ŒMoLè®­ç»ƒæ­¥éª¤ä¸RLé˜¶æ®µå¥–åŠ±æ”¹è¿›æ½œåŠ›å‘ˆæ­£ç›¸å…³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„MoL - RLä¸ºæ™ºèƒ½ä½“ - ç¯å¢ƒç³»ç»Ÿä¸­çš„æŒç»­å­¦ä¹ å»ºç«‹äº†å¯æ¨å¹¿çš„èŒƒå¼ï¼Œä¸ºåˆ©ç”¨å¤šæ­¥æ–‡æœ¬åé¦ˆå¢å¼ºLLMsåœ¨ä¸åŒé¢†åŸŸçš„æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†å¤šæ­¥äº¤äº’åé¦ˆã€é˜²æ­¢æ¨¡å‹èƒ½åŠ›é—å¿˜ä»¥åŠå¼ºåŒ–å­¦ä¹ ä¸å¤§æ¨¡å‹ç»“åˆæå‡æ¨ç†èƒ½åŠ›ç­‰æ–¹é¢éƒ½æä¾›äº†æ–°çš„æ€è·¯å’Œå®è·µæ–¹å‘ï¼Œå¯ç”¨äºæŒ‡å¯¼åç»­å¤§æ¨¡å‹åœ¨å¤æ‚åé¦ˆåœºæ™¯ä¸‹çš„è®­ç»ƒä¼˜åŒ–å·¥ä½œã€‚

## the-policy-cliff--a-theoretical-analysis-of-reward-policy-maps-in-large-language-models
### Abstract
Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§è¯­è¨€æ¨¡å‹ä¸­å¥–åŠ±-ç­–ç•¥æ˜ å°„çš„ç†è®ºåˆ†æï¼šæ­ç§˜â€œç­–ç•¥æ‚¬å´–â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¡‘é€ å¤§è¯­è¨€ä¸æ¨ç†æ¨¡å‹ï¼ˆLLMs/LRMsï¼‰è¡Œä¸ºæ–¹é¢è‡³å…³é‡è¦ï¼Œä½†å¸¸äº§ç”Ÿè„†å¼±ä¸”ä¸ç¨³å®šçš„ç­–ç•¥ï¼Œå¼•å‘å¦‚è™šå‡æ¨ç†ã€æ¬ºéª—æ€§å¯¹é½ã€æŒ‡ä»¤ä¸æœä»ç­‰é—®é¢˜ï¼ŒæŸå®³æ¨¡å‹å¯ä¿¡æ€§ä¸å®‰å…¨æ€§ã€‚ç›®å‰è¿™äº›é—®é¢˜ç¼ºä¹ç»Ÿä¸€ç†è®ºè§£é‡Šï¼Œå¤šé ä¸´æ—¶å¯å‘å¼æ–¹æ³•åº”å¯¹ã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨æ„å»ºä¸¥è°¨æ•°å­¦æ¡†æ¶åˆ†æä»å¥–åŠ±å‡½æ•°åˆ°æœ€ä¼˜ç­–ç•¥æ˜ å°„çš„ç¨³å®šæ€§ï¼Œæ¨è¿›ç­–ç•¥ç¨³å®šæ€§åˆ†æä»ç»éªŒå¯å‘å¼å‘åŸç†æ€§ç†è®ºå‘å±•ï¼Œä¸ºè®¾è®¡æ›´å®‰å…¨å¯ä¿¡AIç³»ç»Ÿæä¾›æ´è§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¥–åŠ± - ç­–ç•¥æ˜ å°„ç¨³å®šæ€§åˆ†ææ¡†æ¶  
æå‡ºä¸¥è°¨æ•°å­¦æ¡†æ¶åˆ†æå¥–åŠ±å‡½æ•°åˆ°æœ€ä¼˜ç­–ç•¥æ˜ å°„çš„ç¨³å®šæ€§ï¼ŒæŒ‡å‡ºç­–ç•¥è„†å¼±æ€§å¸¸æºäºæœ€ä¼˜åŠ¨ä½œä¸å”¯ä¸€ï¼ˆæ¨ç†ä»»åŠ¡ä¸­å¤šæœ‰æ•ˆè½¨è¿¹æ—¶å¸¸è§æƒ…å†µï¼‰ï¼Œä»¥æ­¤ç»Ÿä¸€è§£é‡Šè¯¸å¤šçœ‹ä¼¼ä¸åŒçš„å¤±è´¥æ¡ˆä¾‹ï¼Œå°†å…¶é‡æ„ä¸ºå¥–åŠ±ä¼˜åŒ–ï¼ˆå¥–åŠ±å¯èƒ½ä¸å®Œæ•´æˆ–å«å™ªå£°ï¼Œå°¤å…¶åŠ¨ä½œé€€åŒ–æ—¶ï¼‰çš„åˆç†ç»“æœã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä»å•å¥–åŠ±åˆ°å¤šå¥–åŠ±RLçš„æ‹“å±•åˆ†æ  
å°†åˆ†æä»åŸºç¡€å•å¥–åŠ±åœºæ™¯æ‹“å±•åˆ°æ›´ç°å®çš„å¤šé¢†åŸŸå¤šå¥–åŠ±RLåœºæ™¯ï¼Œæ­ç¤ºç¨³å®šæ€§ç”±â€œæœ‰æ•ˆå¥–åŠ±â€èšåˆæœºåˆ¶ä¸»å¯¼ï¼Œé˜é‡Šä¸åŒé¢†åŸŸä¸‹å¤šå¥–åŠ±å¦‚ä½•å½±å“ç­–ç•¥ç¨³å®šæ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†µæ­£åˆ™åŒ–å¯¹ç­–ç•¥ç¨³å®šæ€§çš„ä½œç”¨è¯æ˜  
è¯æ˜ç†µæ­£åˆ™åŒ–èƒ½ä»¥å¢åŠ éšæœºæ€§ä¸ºä»£ä»·æ¢å¤ç­–ç•¥ç¨³å®šæ€§ï¼Œä¸ºç¼“è§£ç­–ç•¥ä¸ç¨³å®šæä¾›ç†è®ºä¾æ®ä¸æ–¹æ³•æ–¹å‘ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¤šå¥–åŠ±RLä¸­çš„æ‰°åŠ¨å®éªŒéªŒè¯æ¡†æ¶æœ‰æ•ˆæ€§ï¼Œä¸”è¯¥æ¡†æ¶èƒ½ç»Ÿä¸€è§£é‡Šå¦‚æ¬ºéª—æ€§æ¨ç†ã€æŒ‡ä»¤éµå¾ªæƒè¡¡ã€RLHFè¯±å¯¼è¯¡è¾©ç­‰è¿‘æœŸå®è¯å‘ç°ï¼Œåœ¨ç†è®ºä¸å®è·µç»“åˆå±‚é¢å±•ç°è¯´æœåŠ›ï¼Œæ”¯æ’‘äº†ä»å•å¥–åŠ±åˆ°å¤šå¥–åŠ±åœºæ™¯ä¸‹å¯¹ç­–ç•¥ç¨³å®šæ€§ç›¸å…³ç°è±¡çš„åˆ†æä¸è§£é‡Šã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è®ºæ–‡ä¸ºå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ç­–ç•¥ç¨³å®šæ€§é—®é¢˜æä¾›äº†åŸç†æ€§ç†è®ºåŸºç¡€ï¼Œè®©å¼€å‘è€…ç†è§£ç­–ç•¥è„†å¼±ã€ä¸ç¨³å®šæ ¹æºï¼ˆå¦‚åŠ¨ä½œé€€åŒ–ä¸‹çš„å¥–åŠ±ä¼˜åŒ–ï¼‰ï¼Œå¯æŒ‡å¯¼è®¾è®¡æ›´åˆç†å¥–åŠ±å‡½æ•°ï¼ˆå¦‚åˆ©ç”¨â€œæœ‰æ•ˆå¥–åŠ±â€èšåˆã€æ·»åŠ ç†µæ­£åˆ™åŒ–ç­‰ï¼‰æ¥ç¼“è§£è™šå‡æ¨ç†ã€æŒ‡ä»¤ä¸æœä»ç­‰é—®é¢˜ï¼›åŒæ—¶å¯å‘åç»­ç ”ç©¶ä»ç†è®ºå±‚é¢æ·±å…¥æ¢ç´¢å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ å¯¹é½ä¸æ¨ç†èƒ½åŠ›è®­ç»ƒçš„ç¨³å®šæ€§æœºåˆ¶ï¼Œæ¨åŠ¨AIç³»ç»Ÿå‘æ›´å®‰å…¨å¯ä¿¡æ–¹å‘å‘å±•ï¼Œå‡å°‘ä¾èµ–ä¸´æ—¶å¯å‘å¼è°ƒå‚çš„ç°çŠ¶ï¼Œèµ°å‘æ›´å…·åŸåˆ™æ€§çš„è®¾è®¡è·¯å¾„ã€‚ 

## agentic-reinforced-policy-optimization
### Abstract
Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO
### ğŸŒŸ è®ºæ–‡è§£è¯» | é¢å‘å¤šè½®å·¥å…·äº¤äº’çš„æ™ºèƒ½ä½“å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼šARPO å¦‚ä½•çªç ´ LLM è®­ç»ƒç“¶é¢ˆï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨å•è½®æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºé‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ½œåŠ›çš„èƒ½åŠ›ã€‚ä½†åœ¨çœŸå®å¼€æ”¾çš„æ¨ç†åœºæ™¯ä¸­ï¼ŒLLM éœ€å…¼å…·é•¿ç¨‹æ¨ç†èƒ½åŠ›ä¸å¤šè½®å·¥å…·äº¤äº’èƒ½åŠ›ï¼Œè€Œç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•éš¾ä»¥å¹³è¡¡è¿™ä¸¤è€…ã€‚å½“å‰æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å¤šé‡‡ç”¨è½¨è¿¹çº§ç®—æ³•ï¼ˆå¦‚ GRPOã€DAPOï¼‰ï¼Œè¿™ç±»æ–¹æ³•ä¾§é‡å®Œæ•´å·¥å…·ä½¿ç”¨è½¨è¿¹é‡‡æ ·å¹¶åŸºäºæœ€ç»ˆè¾“å‡ºç»™å¥–åŠ±ï¼Œå´å¿½è§†äº† LLM ä¸å·¥å…·ç¯å¢ƒå¤šè½®äº¤äº’ä¸­çš„ç»†ç²’åº¦è¡Œä¸ºæ¢ç´¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç° LLM åœ¨æ¯æ¬¡å·¥å…·è°ƒç”¨åé¦ˆåç”Ÿæˆçš„åˆå§‹ token ç†µå€¼ä¼šæ˜¾è‘—å‡é«˜ï¼ˆå³ä¸ç¡®å®šæ€§å¤§å¢ï¼‰ï¼Œä½†ç°æœ‰è½¨è¿¹çº§æ–¹æ³•æœªå……åˆ†åˆ©ç”¨è¿™ä¸€ç‰¹æ€§å»æ¢ç´¢å·¥å…·ä½¿ç”¨åçš„é«˜ä¸ç¡®å®šæ€§æ­¥éª¤è¡Œä¸ºï¼Œé™åˆ¶äº† LLM æ™ºèƒ½ä½“æ½œåŠ›çš„å‘æŒ¥ã€‚å› æ­¤ï¼ŒäºŸéœ€è®¾è®¡é€‚é…æ™ºèƒ½ä½“ - ç¯å¢ƒäº¤äº’ç‰¹æ€§çš„ RL ç®—æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹ 1ï¼šåŸºäºç†µçš„è‡ªé€‚åº” rollout æœºåˆ¶  
ARPO æå‡ºä¸€ç§èåˆå…¨å±€ä¸å±€éƒ¨é‡‡æ ·è§†è§’çš„ç†µåŸºè‡ªé€‚åº” rollout æœºåˆ¶ã€‚åœ¨ rollout é˜¶æ®µï¼ŒLLM å…ˆè¿›è¡Œå¤šæ¬¡å…¨å±€é‡‡æ ·å¹¶è®°å½•å„æ ·æœ¬åˆå§‹ç†µåˆ†å¸ƒï¼›æ¯æ¬¡å·¥å…·è°ƒç”¨åï¼Œç›‘æµ‹å®æ—¶ token ç†µå˜å¹¶å°†å…¶ä½œä¸ºåˆ†æ”¯ä¾æ®ï¼Œè‹¥ç†µå˜è¶…é¢„è®¾é˜ˆå€¼ï¼Œæ¨¡å‹æ‰§è¡Œé¢å¤–å±€éƒ¨é‡‡æ ·ä»¥æ¢ç´¢æ›´å¤šæ ·åŒ–çš„å·¥å…·é›†æˆæ¨ç†è¡Œä¸ºã€‚è¯¥æœºåˆ¶è®© ARPO åœ¨å¹³è¡¡å…¨å±€ä¸æ­¥éª¤çº§å·¥å…·ä½¿ç”¨è¡Œä¸ºå­¦ä¹ çš„åŒæ—¶ï¼Œæœ‰æ•ˆæ‰©å±•åŸå§‹é‡‡æ ·ç©ºé—´ï¼Œé’ˆå¯¹æ€§æ¢ç´¢å·¥å…·ä½¿ç”¨åé«˜ä¸ç¡®å®šæ€§æ­¥éª¤çš„è¡Œä¸ºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹ 2ï¼šä¼˜åŠ¿å½’å› ä¼°è®¡ï¼ˆAdvantage Attribution Estimationï¼‰  
ä¸ºå……åˆ†åˆ©ç”¨è‡ªé€‚åº”é‡‡æ ·ä¼˜åŠ¿ï¼ŒARPO å¼•å…¥ä¼˜åŠ¿å½’å› ä¼°è®¡ã€‚å…·ä½“æ¢ç´¢äº†ç¡¬ã€è½¯ä¼˜åŠ¿è®¾ç½®ï¼Œç»™åŒä¸€æºæ¨ç†è·¯å¾„ä¸Šçš„ token åˆ†é…å…±äº«ä¼˜åŠ¿å€¼ï¼Œåˆ†æ”¯è·¯å¾„ä¸Šçš„ token åˆ†é…ä¸åŒä¼˜åŠ¿å€¼ï¼ŒåŠ©åŠ› LLM åœ¨å†…åŒ–é€æ­¥å·¥å…·ä½¿ç”¨äº¤äº’ä¸­çš„ä¼˜åŠ¿å·®å¼‚ï¼Œè¿›è€Œä¼˜åŒ–ç­–ç•¥å­¦ä¹ ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹ 3ï¼šç†è®ºä¸å®è·µç»“åˆçš„ç®—æ³•éªŒè¯  
é™¤äº†å¯å‘å¼åŠ¨æœºï¼Œè®ºæ–‡è¿˜ä»ç†è®ºå±‚é¢è®ºè¯äº† ARPO åœ¨ LLM æ™ºèƒ½ä½“è®­ç»ƒä¸­åº”ç”¨çš„åˆç†æ€§ï¼Œä¸ºç®—æ³•æœ‰æ•ˆæ€§æä¾›æ›´åšå®æ”¯æ’‘ï¼ŒåŒºåˆ«äºä»…é å®éªŒéªŒè¯çš„å¤šæ•°å·¥ä½œã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨è®¡ç®—æ¨ç†ã€çŸ¥è¯†æ¨ç†ã€æ·±åº¦æœç´¢ä¸‰å¤§é¢†åŸŸçš„ 13 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•é›†ä¸Šå¼€å±•å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼š  
- ARPO åœ¨æ™ºèƒ½ä½“è®­ç»ƒä¸­æŒç»­è¶…è¶Šä¼ ç»Ÿè½¨è¿¹çº§ RL ç®—æ³•ï¼›  
- ä»…ç”¨ç°æœ‰è½¨è¿¹çº§ RL æ–¹æ³•ä¸€åŠçš„å·¥å…·è°ƒç”¨é¢„ç®—ï¼Œå°±èƒ½å®ç°æ€§èƒ½æå‡ï¼Œåœ¨ç²¾åº¦ä¸æ•ˆç‡é—´è¾¾æˆæ›´ä¼˜å¹³è¡¡ï¼›  
- è¿›ä¸€æ­¥çš„æ‰©å±•æ€§åˆ†æéªŒè¯äº† ARPO ä»¥å¯æ‰©å±•æ–¹å¼å¢å¼º LLM æ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¡Œä¸ºé‡åŒ–è§†è§’ï¼šé€šè¿‡é‡åŒ– LLM æ™ºèƒ½ä½“æ¨ç†æ—¶çš„ token ç†µå˜ï¼Œæ­ç¤ºè½¨è¿¹çº§ RL ç®—æ³•åœ¨å¯¹é½ LLM æ™ºèƒ½ä½“ä¸Šçš„å›ºæœ‰å±€é™ï¼Œä¸ºåç»­ç®—æ³•æ”¹è¿›æä¾›äº†åŸºäºâ€œç†µ - ä¸ç¡®å®šæ€§â€å…³è”çš„åˆ†ææ€è·¯ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨ç»†ç²’åº¦è¡Œä¸ºç‰¹å¾å¯¹è®­ç»ƒçš„å½±å“ã€‚  
2. è‡ªé€‚åº”é‡‡æ ·èŒƒå¼ï¼šARPO çš„ç†µåŸºè‡ªé€‚åº” rollout æœºåˆ¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨æ¨¡å‹è¡Œä¸ºä¸­çš„åŠ¨æ€ç‰¹å¾ï¼ˆå¦‚ç†µå˜ï¼‰æ¥è®¾è®¡è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œä¸ºå¤„ç†å…·æœ‰ç¯å¢ƒåé¦ˆä¸ç¡®å®šæ€§çš„å¤šè½®äº¤äº’ä»»åŠ¡æä¾›äº†å¯å‚è€ƒçš„é‡‡æ ·ä¼˜åŒ–èŒƒå¼ã€‚  
3. ä¼˜åŠ¿å†…åŒ–æ€è·¯ï¼šä¼˜åŠ¿å½’å› ä¼°è®¡æœºåˆ¶ä¸ºå¤šæ­¥éª¤ã€å¤šåˆ†æ”¯çš„äº¤äº’åœºæ™¯ä¸‹ï¼Œå¦‚ä½•è®©æ¨¡å‹å­¦ä¹ æ­¥éª¤é—´ä¼˜åŠ¿å·®å¼‚æä¾›äº†æ–¹æ³•å‚è€ƒï¼Œå¯è¿ç§»åˆ°éœ€é€æ­¥å†³ç­–ã€å¤šè½®äº¤äº’çš„æ™ºèƒ½ä½“è®­ç»ƒä»»åŠ¡ä¸­ã€‚  
4. æ•ˆç‡ - æ€§èƒ½å¹³è¡¡ï¼šåœ¨é™ä½å·¥å…·ä½¿ç”¨é¢„ç®—åŒæ—¶æå‡æ€§èƒ½ï¼Œè¯æ˜äº†ç®—æ³•åœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„å®ç”¨æ€§ï¼Œä¸ºå·¥ä¸šç•Œè½åœ° LLM æ™ºèƒ½ä½“è®­ç»ƒæä¾›äº†é«˜æ•ˆæ–¹æ¡ˆå€Ÿé‰´æ–¹å‘ã€‚  

## ulorl-an-ultra-long-output-reinforcement-learning-approach-for-advancing-large-language-models--reasoning-abilities
### Abstract
Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.
### ğŸŒŸ è®ºæ–‡è§£è¯» | UloRLï¼šçªç ´è¶…é•¿è¾“å‡ºç“¶é¢ˆï¼Œå¼ºåŒ–å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æå‡å¸¸ä¾èµ–å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ï¼Œä½†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶åœ¨å¤„ç†è¶…é•¿è¾“å‡ºæ—¶å­˜åœ¨ä¸¤å¤§ç—›ç‚¹ï¼šä¸€æ˜¯é•¿åºåˆ—é•¿åº¦çš„é•¿å°¾åˆ†å¸ƒï¼Œå¯¼è‡´è®­ç»ƒæ—¶éœ€ç­‰æ‰¹æ¬¡æ‰€æœ‰æ ·æœ¬è§£ç å®Œæˆï¼Œé•¿å°¾æ ·æœ¬æ˜“æˆä¸ºè®­ç»ƒç“¶é¢ˆï¼Œæ•ˆç‡ä½ä¸‹ï¼›äºŒæ˜¯è®­ç»ƒä¸­å‡ºç°çš„ç†µåç¼©é—®é¢˜ï¼Œæ¨¡å‹å¤šæ ·æ€§è¿‡æ—©æµå¤±ï¼Œæ€§èƒ½å—é™ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè…¾è®¯æ··å…ƒå›¢é˜Ÿæå‡ºé¢å‘è¶…é•¿è¾“å‡ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•UloRLï¼ŒåŠ©åŠ›å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›è¿›é˜¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ†æ®µæ»šåŠ¨è¾“å‡ºï¼ˆSegment Rolloutï¼‰  
å°†è¶…é•¿è¾“å‡ºçš„è§£ç è¿‡ç¨‹æ‹†åˆ†ä¸ºå¤šä¸ªçŸ­ç‰‡æ®µï¼Œæ¯ä¸€æ­¥ä»…è§£ç æ›´çŸ­çš„ç‰‡æ®µã€‚å·²å®Œæˆè§£ç çš„æ ·æœ¬å¯ç«‹å³è¿›å…¥ç»éªŒæ± ç”¨äºè®­ç»ƒï¼Œæœªå®Œæˆçš„åç»­è¿­ä»£ç»§ç»­è§£ç ã€‚æ­¤æ–¹å¼è§„é¿äº†é•¿å°¾æ ·æœ¬å¸¦æ¥çš„ä¸å¿…è¦å»¶è¿Ÿï¼ŒåŠ é€Ÿè®­ç»ƒåŒæ—¶é«˜æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºã€‚è¿˜é…å¥—æå‡ºSegment - Aware Importance Samplingï¼ˆSAISï¼‰å’ŒPesudo On - Policy Importance Samplingï¼ˆPOISï¼‰ï¼Œé€‚é…åˆ†æ®µæ»šåŠ¨åœºæ™¯ä¸‹çš„é‡è¦æ€§é‡‡æ ·æœºåˆ¶ï¼Œä¿éšœè®­ç»ƒåŠ¨æ€ç¨³å®šã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå·²æŒæ¡æ­£ tokens åŠ¨æ€æ©ç ï¼ˆDMMPTsï¼‰  
ç†µåç¼©æºäºæ¨¡å‹å¯¹å·²æŒæ¡æ­£ tokensï¼ˆMPTsï¼Œå³æ¨¡å‹èƒ½é«˜ç½®ä¿¡åº¦é¢„æµ‹çš„ tokensï¼‰è¿‡æ‹Ÿåˆã€‚DMMPTs ä¾æ®æ¨¡å‹å½“å‰ç†µå€¼è‡ªé€‚åº”æ§åˆ¶ MPTs è®­ç»ƒï¼šå½“ç†µä½äºé˜ˆå€¼ï¼Œæ©ç  MPTs ä½¿å…¶ä¸å‚ä¸è®­ç»ƒï¼›å¦åˆ™å…¨é‡ token å‚ä¸ã€‚è¯¥ç­–ç•¥æ—¢ä¸å¼•å…¥é¢å¤–ä¼˜åŒ–ç›®æ ‡ï¼Œä¹Ÿä¸ä¾èµ–é‡è¦æ€§é‡‡æ ·ï¼Œè§„é¿äº†ç°æœ‰æ–¹æ³•å±€é™ï¼Œå®éªŒè¡¨æ˜èƒ½è®©æ¨¡å‹è®­ç»ƒä¸­ç†µç¨³å®šã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç”Ÿæˆå¼éªŒè¯å™¨ä¸æ•°æ®å¤„ç†  
å¼•å…¥ç”Ÿæˆå¼éªŒè¯å™¨æ¨¡å‹æå‡ RL è®­ç»ƒå¥–åŠ±è®¡ç®—å‡†ç¡®æ€§ï¼Œç›¸è¾ƒä¼ ç»ŸåŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œå®ƒå€Ÿç”Ÿæˆèƒ½åŠ›åˆ¤æ–­é¢„æµ‹ä¸å‚è€ƒç­”æ¡ˆç­‰ä»·æ€§ã€‚åŒæ—¶é‡è§†æ•°æ®æ¸…æ´—ä¸è½¬æ¢ï¼Œå¦‚è¿‡æ»¤å™ªå£°æ•°æ®ã€å¤„ç†å¤šå­é—®é¢˜é¢˜ç›®ã€è§„èŒƒæ ¼å¼ä¸ç®€åŒ–å‚è€ƒç­”ç­‰ï¼Œä¿éšœå¥–åŠ±ä¿¡å·è´¨é‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ Qwen3 - 30B - A3B æ¨¡å‹ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼š  
- è®­ç»ƒæ•ˆç‡ï¼šé‡‡ç”¨åˆ†æ®µæ»šåŠ¨çš„ RL è®­ç»ƒé€Ÿåº¦æå‡ 2.06 å€ï¼›  
- æ¨ç†æ€§èƒ½ï¼š128k token è¾“å‡ºçš„ RL è®­ç»ƒåï¼ŒAIME2025 ä»»åŠ¡ä¸Šæ€§èƒ½ä» 70.9% æå‡è‡³ 85.1%ï¼ŒBeyondAIME ä» 50.7% æå‡è‡³ 61.9%ï¼Œç”šè‡³è¶…è¿‡ Qwen3 - 235B - A22B æ¨¡å‹ï¼ˆAIME2025 ä¸º 81.5%ã€BeyondAIME ä¸º 59.0%ï¼‰ï¼Œæ¶¨å¹…æ˜¾è‘—ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¶…é•¿åºåˆ—è®­ç»ƒä¼˜åŒ–ï¼šåˆ†æ®µå¤„ç†æ€è·¯ä¸ºé•¿æ–‡æœ¬ç”Ÿæˆç±»ä»»åŠ¡çš„ RL è®­ç»ƒæä¾›äº†é«˜æ•ˆæ‰§è¡ŒèŒƒå¼ï¼Œå¯è¿ç§»åˆ°éœ€é•¿è¾“å‡ºçš„ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ç­‰åœºæ™¯ï¼›  
2. ç†µåç¼©è§£å†³ï¼šDMMPTs ä» token é€‰æ‹©è§’åº¦åŠ¨æ€è°ƒæ§è®­ç»ƒï¼Œä¸é¢å¤–å¢åŠ ä¼˜åŒ–ç›®æ ‡çš„æ€è·¯ï¼Œä¸ºå¹³è¡¡æ¨¡å‹æ¢ç´¢ä¸åˆ©ç”¨ã€é¿å…è¿‡æ‹Ÿåˆæä¾›æ–°è§†è§’ï¼›  
3. å¥–åŠ±æœºåˆ¶å®Œå–„ï¼šç”Ÿæˆå¼éªŒè¯å™¨ç»“åˆæ•°æ®æ²»ç†çš„æ–¹å¼ï¼Œå¯¹ä¾èµ–å¥–åŠ±ä¿¡å·çš„ RL è®­ç»ƒï¼ˆå¦‚ RLHF ç­‰ï¼‰ä¸­æå‡å¥–åŠ±å‡†ç¡®æ€§æœ‰å‚è€ƒä»·å€¼ï¼›  
4. å¼€æºç”Ÿæ€ï¼šå›¢é˜Ÿè®¡åˆ’å¼€æºä»£ç ä¸æ¨¡å‹ï¼Œåˆ©äºç¤¾åŒºåŸºäºæ­¤è¿›ä¸€æ­¥æ¢ç´¢è¶…é•¿è¾“å‡ºä¸‹å¤§æ¨¡å‹èƒ½åŠ›æå‡ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## skill-learning-via-policy-diversity-yields-identifiable-representations-for-reinforcement-learning
### Abstract
Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ ä¸­åŸºäºç­–ç•¥å¤šæ ·æ€§çš„æŠ€èƒ½å­¦ä¹ å¦‚ä½•å®ç°å¯è¯†åˆ«è¡¨å¾

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢†åŸŸé¢ä¸´ç¨€ç–å¥–åŠ±ä¸‹å­¦ä¹ ã€ç¯å¢ƒæ¢ç´¢åŠå¥–åŠ±å‡½æ•°è®¾è®¡ç­‰æŒ‘æˆ˜ï¼Œè‡ªç›‘ç£ç‰¹å¾å­¦ä¹ ä¸é¢„è®­ç»ƒæ–¹æ³•å¸¸ä¾èµ–äº’ä¿¡æ¯æŠ€èƒ½å­¦ä¹ ï¼ˆMISLï¼‰è¿™ç±»ä¿¡æ¯è®ºåŸç†ï¼Œä½†MISLä¸­è¡¨å¾å’Œäº’ä¿¡æ¯å‚æ•°åŒ–çš„ç†è®ºä½œç”¨å°šæœªæ˜æ™°ã€‚åŒæ—¶ï¼Œè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸MISLçš„å…³è”ä¸ºæ¢ç©¶è¯¥é—®é¢˜æä¾›äº†è·¯å¾„ï¼Œè€Œå¯è¯†åˆ«æ€§ç»“æœå¯¹éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ä¸­ä»è§‚æµ‹æ¨æ–­çœŸå®çŠ¶æ€è‡³å…³é‡è¦ï¼Œå› æ­¤æœ¬æ–‡ä»å¯è¯†åˆ«è¡¨å¾å­¦ä¹ è§†è§’æ¢ç©¶MISLï¼Œèšç„¦å¯¹æ¯”åç»§ç‰¹å¾ï¼ˆCSFï¼‰æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºMISLæˆåŠŸçš„åŸç†  
é€šè¿‡åˆ†æå¯¹æ¯”åç»§ç‰¹å¾ï¼ˆCSFï¼‰æ–¹æ³•ï¼Œé˜é‡ŠMISLçš„æˆåŠŸæºäº**å¤šæ ·ç­–ç•¥ä¸‹çš„äº’ä¿¡æ¯ä¼°è®¡ä¸å†…ç§¯æ¨¡å‹å‚æ•°åŒ–çš„ç›¸äº’ä½œç”¨**ã€‚è¯æ˜äº†CSFèƒ½åœ¨ç‰¹å¾å†…ç§¯å‚æ•°åŒ–å’Œåˆ¤åˆ«æ€§æŠ€èƒ½å¤šæ ·æ€§çš„æ¡ä»¶ä¸‹ï¼Œä»¥çº¿æ€§å˜æ¢çš„ç²¾åº¦æ¢å¤ç¯å¢ƒçš„çœŸå®ç‰¹å¾ï¼Œç»™å‡ºäº†å¼ºåŒ–å­¦ä¹ è¡¨å¾å­¦ä¹ é¢†åŸŸé¦–ä¸ªå¯è¯†åˆ«æ€§ä¿è¯ï¼Œè§£é‡Šäº†ä¸ºä½•ä¹‹å‰ä¸€äº›æ–¹æ³•çš„ç›®æ ‡å’Œå‚æ•°åŒ–è¡¨ç°æ›´ä¼˜ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡åŒ–å¤šæ ·ç­–ç•¥å¹¶æŒ‡æ˜æ–¹æ³•å±€é™ä¸å®è·µå»ºè®®  
å€Ÿé‰´ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰å’Œå› æœè¡¨å¾å­¦ä¹ ï¼ˆCRLï¼‰ä¸­çš„å¤šæ ·æ€§ä¸å˜å¼‚æ€§å‡è®¾ï¼Œ**å½¢å¼åŒ–å®šä¹‰äº†å¤šæ ·ç­–ç•¥**ï¼Œåˆ†æç‰¹å¾ç»´åº¦å’ŒæŠ€èƒ½ç©ºé—´è¦†ç›–çš„ä½œç”¨ï¼ŒæŒ‡å‡ºå¦‚æœ€å¤§ç†µç­–ç•¥åœ¨æŠ€èƒ½å­¦ä¹ ä¸­å¹¶éæœ€ä¼˜ç­‰å…ˆå‰æ–¹æ³•çš„å±€é™ï¼Œè¿›è€Œç»™å‡ºå®è·µå±‚é¢çš„å»ºè®®ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MuJoCoå’ŒDeepMind Controlç¯å¢ƒä¸­ï¼Œé’ˆå¯¹åŸºäºçŠ¶æ€å’Œåƒç´ çš„åœºæ™¯éªŒè¯ç†è®ºä¸»å¼ ï¼Œå±•ç¤ºäº†CSFèƒ½å¤Ÿåœ¨ç†è®ºä¸Šä»çŠ¶æ€å’Œåƒç´ ä¸­æ¢å¤çœŸå®ç‰¹å¾ï¼Œå®è¯äº†æ–¹æ³•åœ¨ç‰¹å¾å¯è¯†åˆ«æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»ç†è®ºå±‚é¢ï¼Œä¸ºç†è§£MISLæ–¹æ³•ä¸­äº’ä¿¡æ¯ç›®æ ‡ã€ç†µæ­£åˆ™åŒ–çš„å½±å“ç­‰æä¾›äº†æ–°è§†è§’ï¼Œå¸®åŠ©è§£é‡Šä¸åŒè®¾è®¡é€‰æ‹©çš„ä¼˜åŠ£ï¼›ä»å®è·µå±‚é¢ï¼Œæ˜ç¡®äº†å¤šæ ·ç­–ç•¥çš„å®šä¹‰åŠç‰¹å¾ã€æŠ€èƒ½ç©ºé—´ç­‰è¦ç´ çš„ä½œç”¨ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ ä¸­è‡ªç›‘ç£è¡¨å¾å­¦ä¹ æ–¹æ³•è®¾è®¡æä¾›äº†æŒ‡å¯¼ï¼Œæ¯”å¦‚åœ¨æŠ€èƒ½å‘ç°å’Œæ¢ç´¢ç±»ä»»åŠ¡ä¸­å¦‚ä½•è®¾è®¡ç­–ç•¥ä¸æ¨¡å‹å‚æ•°åŒ–æ–¹å¼ç­‰ã€‚ 

## perception-aware-policy-optimization-for-multimodal-reasoning
### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose PAPO, a novel policy
gradient algorithm that encourages the model to learn to perceive while
learning to reason. Specifically, we introduce the Implicit Perception Loss in
the form of a KL divergence term, which can be seamlessly plugged into
mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely
on additional data curation, reward models, or stronger teacher models. To
further enhance the training stability of PAPO, we introduce the Double Entropy
Loss, which effectively regularizes the new KL objective without compromising
performance. Despite its simplicity, PAPO yields significant overall
improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements
are more pronounced, approaching 8.0%-19.1%, on tasks with high vision
dependency. We also observe a substantial reduction of 30.5% in perception
errors, indicating improved perceptual capabilities with PAPO. Overall, our
work introduces a deeper integration of perception-aware supervision into core
learning objectives and lays the groundwork for a new RL framework that
encourages visually grounded reasoning. Code and data will be made publicly
available for research purposes. Project page:
https://mikewangwzhl.github.io/PAPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ„ŸçŸ¥æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–åŠ©åŠ›å¤šæ¨¡æ€æ¨ç†æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸä¸­ï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨èµ‹äºˆæ¨¡å‹å¼ºå¤§å¤šæ­¥æ¨ç†èƒ½åŠ›ä¸Šæˆæ•ˆæ˜¾è‘—ï¼Œä½†ç°æœ‰è®¾è®¡å’Œä¼˜åŒ–å¤šé’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œåº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ã€‚å½“å‰å¤šæ¨¡æ€æ¨ç†ä¸­ï¼Œè§†è§‰è¾“å…¥æ„ŸçŸ¥æ˜¯ä¸»è¦é”™è¯¯æ¥æºä¹‹ä¸€ã€‚æ­¤å‰å¤šæ•°é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†çš„RLVRç›¸å…³å·¥ä½œï¼Œæˆ–èšç„¦æ•°æ®ä¸rolloutè´¨é‡ã€å¥–åŠ±è®¾è®¡ï¼Œæˆ–ç®—æ³•ä¿®æ”¹ç…§æ¬çº¯æ–‡æœ¬æ¨¡å‹ç»éªŒï¼Œæœªä»å¤šæ¨¡æ€è§†è§’é‡æ–°æ€è€ƒã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºPAPOç®—æ³•æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºPAPOç®—æ³•
PAPOæ˜¯GRPOçš„åˆ›æ–°æ€§æ‰©å±•ï¼Œæ—¨åœ¨è®©æ¨¡å‹åœ¨å­¦ä¹ æ¨ç†åŒæ—¶å­¦ä¹ æ„ŸçŸ¥ï¼Œä¸”å®Œå…¨ä¾èµ–å†…éƒ¨ç›‘ç£ä¿¡å·ï¼Œæ— éœ€é¢å¤–æ•°æ®æ•´ç†ã€å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹ã€‚å¼•å…¥ä»¥KLæ•£åº¦å½¢å¼å‘ˆç°çš„éšå¼æ„ŸçŸ¥æŸå¤±ï¼ˆImplicit Perception Lossï¼‰åˆ°GRPOç›®æ ‡ä¸­ï¼Œé€šè¿‡è®¡ç®—æ¨¡å‹åœ¨åŸå§‹å›¾åƒå’Œè¢«ç ´åå›¾åƒï¼ˆå¦‚éšæœºé®ç›–éƒ¨åˆ†å›¾åƒå—å¾—åˆ°çš„å›¾åƒï¼‰æ¡ä»¶ä¸‹è¾“å‡ºçš„KLæ•£åº¦æ¥å®ç°ï¼Œä»¥æ­¤æ¿€åŠ±æ¨¡å‹åˆ©ç”¨æœ‰ä¿¡æ¯çš„è§†è§‰å†…å®¹ç”Ÿæˆå“åº”ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥åŒç†µæŸå¤±ï¼ˆDouble Entropy Lossï¼‰
ç”±äºKLç›®æ ‡çš„æ— ç•Œæ€§ï¼Œè‹¥éšå¼æ„ŸçŸ¥æŸå¤±ç³»æ•°è®¾ç½®è¿‡é«˜ï¼ŒPAPOå¯èƒ½è¿‡åº¦ä¼˜åŒ–KLprcpé¡¹å¼•å‘â€œç›®æ ‡é»‘å®¢æ”»å‡»â€é—®é¢˜ï¼Œå¯¼è‡´å¥–åŠ±éª¤é™ã€‚ä¸ºæ­¤å¼•å…¥åŒç†µæŸå¤±ï¼Œåœ¨ä¸å½±å“æ€§èƒ½å‰æä¸‹æœ‰æ•ˆæ­£åˆ™åŒ–æ–°çš„KLç›®æ ‡ï¼Œç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å…«ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPAPOç›¸æ¯”GRPOå®ç°æŒç»­æ”¹è¿›ï¼Œå¹³å‡æå‡4.4%ï¼›åœ¨é«˜åº¦ä¾èµ–è§†è§‰åŸºç¡€ä¸”è¾“å…¥é—®é¢˜è§†è§‰çº¿ç´¢å°‘çš„ä»»åŠ¡ä¸­ï¼Œæå‡æ•ˆæœæ›´æ˜¾è‘—ï¼Œè¾¾8.0%ï¼›ç»“åˆç§»é™¤å‚è€ƒKLæ—¶ï¼Œåœ¨3Bè§„æ¨¡æ¨¡å‹ä¸Šæ”¹è¿›å¯è¾¾11.2%ï¼›æ„ŸçŸ¥ç›¸å…³é”™è¯¯å‡å°‘30.5%ï¼›ä¸”PAPOæ”¶æ•›æ›´å¿«ï¼Œæ—©æœŸçº¦25æ­¥å·¦å³å°±å¼€å§‹æœ‰æ€§èƒ½å¢ç›Šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»å¤šæ¨¡æ€è§†è§’é‡æ–°æ€è€ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡ï¼Œä¸ºè§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­æ„ŸçŸ¥ç“¶é¢ˆæä¾›æ–°æ€è·¯ï¼›æå‡ºçš„éšå¼æ„ŸçŸ¥æŸå¤±æ— éœ€é¢å¤–å¤æ‚ç»„ä»¶ï¼Œç®€å•æœ‰æ•ˆæ˜“æ•´åˆåˆ°ä¸»æµRLVRç®—æ³•ï¼ˆå¦‚GRPOã€DAPOï¼‰ä¸­ï¼›é’ˆå¯¹KLç›®æ ‡æ½œåœ¨é—®é¢˜æå‡ºçš„åŒç†µæŸå¤±ï¼Œä¸ºå¤„ç†ç±»ä¼¼æ— ç•Œç›®æ ‡ä¼˜åŒ–æ—¶çš„è®­ç»ƒç¨³å®šæ€§é—®é¢˜æä¾›äº†å‚è€ƒæ–¹æ³•ï¼›æ•´ä½“å·¥ä½œå°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥åœ°èå…¥RLVRå­¦ä¹ ç›®æ ‡ï¼Œä¸ºé¼“åŠ±è§†è§‰åŸºç¡€æ¨ç†çš„æ–°å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¥ å®šåŸºç¡€ï¼Œåç»­åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¼˜åŒ–ç­‰æ–¹å‘å¯å€Ÿé‰´å…¶æ€è·¯æ¥æå‡æ¨¡å‹å¯¹è§†è§‰è¾“å…¥çš„æœ‰æ•ˆåˆ©ç”¨ä¸æ¨ç†èƒ½åŠ›ã€‚ 

## skywork-r1v3-technical-report
### Abstract
We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-R1V3ï¼šå¼€æºå¤šæ¨¡æ€æ¨ç†çš„çªç ´æ€§è¿›å±•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºé€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®èŒƒå¼ï¼Œåœ¨å¤šé¢†åŸŸå–å¾—è¿›å±•ï¼Œä½†ä¸é—­æºæ¨¡å‹ç›¸æ¯”ï¼Œå¼€æºVLMsåœ¨å¤æ‚è§†è§‰æ¨ç†ç­‰ä»»åŠ¡ä¸Šå­˜åœ¨å·®è·ï¼Œä¸”å¤šæ¨¡æ€æ•°æ®å¼‚è´¨æ€§å¸¦æ¥è®­ç»ƒæ¨ç†ç“¶é¢ˆï¼Œè§†è§‰-è¯­è¨€æ ‡æ³¨ç¨€ç¼ºä¸å‡ä¹ŸåŠ å‰§äº†å¼€æºä¸é—­æºVLMsçš„èƒ½åŠ›é¸¿æ²Ÿã€‚åŒæ—¶ï¼Œç°æœ‰å¼€æºVLMsæ¨ç†èƒ½åŠ›å’Œç»“æ„åŒ–æ€ç»´ä¸è¶³ï¼Œå› æ­¤æ¨è¿›å¼€æºVLMsåœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡çš„å‘å±•è‡³å…³é‡è¦ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ›æ–°æ–¹æ³•æå‡å¼€æºVLMsæ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç²¾å·§çš„RLåè®­ç»ƒæ¡†æ¶  
æå‡ºç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œæ— éœ€é¢å¤–æŒç»­é¢„è®­ç»ƒï¼Œå°±èƒ½æœ‰æ•ˆæ¿€æ´»å’Œå¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå®ç°ä»çº¯æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ°è§†è§‰ä»»åŠ¡çš„æ¨ç†æŠ€èƒ½è¿ç§»ï¼Œä¸ºæ¨¡å‹æ¨ç†èƒ½åŠ›æå‡æä¾›æœ‰åŠ›æ”¯æ’‘ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…³é”®æ¨ç†tokenç†µçš„èƒ½åŠ›æŒ‡æ ‡  
å¼•å…¥ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†tokençš„ç†µï¼Œåœ¨RLè®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡ç›‘æµ‹å…³é”®æ¨ç†èµ·å§‹ç‚¹çš„ç†µå€¼ï¼ŒåŒºåˆ†çœŸæ­£æœ‰æ¨ç†èƒ½åŠ›å’Œä»…æ¨¡ä»¿æ¨ç†æ¨¡å¼çš„æ¨¡å‹ï¼Œè¯¥æŒ‡æ ‡ä¸éªŒè¯é›†å®é™…æ¨ç†æ€§èƒ½å¼ºç›¸å…³ï¼Œä¸ºRLè®­ç»ƒä¸­é«˜è´¨é‡æ£€æŸ¥ç‚¹é€‰æ‹©æä¾›é«˜æ•ˆæ–¹æ³•ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ­ç¤ºconnectoræ¨¡å—æ ¸å¿ƒä½œç”¨  
åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥æ˜ç¡®VLMsä¸­connectoræ¨¡å—å¯¹ç»´æŒè·¨æ¨¡æ€å¯¹é½çš„å…³é”®ä½œç”¨ï¼Œè¿˜å‘ç°RLåä»…å¯¹connectorè°ƒä¼˜æ˜¯åœ¨ä¸æŸå®³æ¨ç†èƒ½åŠ›å‰æä¸‹é‡æ–°å¹³è¡¡æ¨¡å‹çŸ¥è¯†åˆ†å¸ƒçš„æœ‰æ•ˆç­–ç•¥ï¼Œæ·±å…¥æ¢ç©¶äº†å¤šæ¨¡æ€æ¨ç†ä¸­è·¨æ¨¡æ€å¯¹é½çš„å®ç°æœºåˆ¶ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šæ–¹é¢è®­ç»ƒç­–ç•¥æ¢ç´¢  
æ¶µç›–å†·å¯åŠ¨å¾®è°ƒï¼ˆåˆ©ç”¨æ—©æœŸSkywork R1V2æ„å»ºå†·å¯åŠ¨æ•°æ®é›†ï¼Œå°†è¯­è¨€æ¨ç†æ¨¡å‹æ¨ç†æ¨¡å¼è¿ç§»åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰ã€è¯¾ç¨‹å­¦ä¹ åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ã€å¼ºåŒ–å­¦ä¹ æŠ€å·§ï¼ˆå¦‚Clip - Higherå’ŒDynamic Samplingï¼‰å¤ç°ç­‰å¤šæ–¹é¢è®­ç»ƒç­–ç•¥æ¢ç´¢ï¼Œå®Œå–„äº†VLMsåè®­ç»ƒæ–¹æ³•ä½“ç³»ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
Skywork - R1V3åœ¨MMMUåŸºå‡†æµ‹è¯•ä¸­å–å¾—76.0%çš„å½“å‰æœ€ä¼˜ç»“æœï¼Œæ€§èƒ½åŒ¹é…å…¥é—¨çº§äººç±»ä¸“å®¶èƒ½åŠ›ï¼›å…¶38Bå‚æ•°æ¨¡å‹å€ŸåŠ©RLé©±åŠ¨çš„åè®­ç»ƒæ–¹æ³•ï¼Œèƒ½ä¸é¡¶çº§é—­æºVLMsåª²ç¾ï¼›ä¸”æˆåŠŸå°†æ•°å­¦æ¨ç†è¿ç§»åˆ°å…¶ä»–å­¦ç§‘ç›¸å…³æ¨ç†ä»»åŠ¡ï¼Œå……åˆ†éªŒè¯äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„å¼ºå¤§æ€§èƒ½ä¸è¿ç§»èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. åè®­ç»ƒæ¡†æ¶è®¾è®¡ï¼šå…¶RLåè®­ç»ƒæ¡†æ¶æ— éœ€é¢å¤–é¢„è®­ç»ƒå°±èƒ½æ¿€æ´»æ¨ç†èƒ½åŠ›çš„æ€è·¯ï¼Œä¸ºæå‡æ¨¡å‹èƒ½åŠ›æä¾›äº†è½»é‡é«˜æ•ˆçš„è·¯å¾„å‚è€ƒï¼Œå¯ç”¨äºå…¶ä»–æ¨¡å‹æ¨ç†èƒ½åŠ›å¼ºåŒ–åœºæ™¯ã€‚  
2. èƒ½åŠ›è¯„ä¼°æŒ‡æ ‡ï¼šå…³é”®æ¨ç†tokenç†µè¿™ä¸€åˆ›æ–°æŒ‡æ ‡ï¼Œä¸ºæ¨¡å‹è®­ç»ƒä¸­è¯„ä¼°æ¨ç†èƒ½åŠ›ã€é€‰æ‹©ä¼˜è´¨æ£€æŸ¥ç‚¹æä¾›äº†æ–°çš„æœ‰æ•ˆç»´åº¦ï¼Œåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ç›‘æ§ä¸ä¼˜åŒ–æ–¹é¢å…·æœ‰å€Ÿé‰´ä»·å€¼ã€‚  
3. æ¨¡å—ä½œç”¨æ¢ç©¶ï¼šå¯¹connectoræ¨¡å—åœ¨è·¨æ¨¡æ€å¯¹é½ä¸­ä½œç”¨çš„æ·±å…¥ç ”ç©¶ï¼Œä»¥åŠåç»­è°ƒä¼˜ç­–ç•¥ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹ä¸­æ¨¡å—åŠŸèƒ½æŒ–æ˜ä¸ä¼˜åŒ–æä¾›äº†å®è·µæ¡ˆä¾‹ï¼ŒåŠ©åŠ›å¤šæ¨¡æ€æ¨¡å‹æ¶æ„è®¾è®¡ä¸ä¼˜åŒ–ã€‚  
4. å¤šç­–ç•¥èåˆè®­ç»ƒï¼šå†·å¯åŠ¨å¾®è°ƒã€è¯¾ç¨‹å­¦ä¹ ç»“åˆå¼ºåŒ–å­¦ä¹ ç­‰å¤šç­–ç•¥åœ¨VLMsåè®­ç»ƒä¸­çš„åº”ç”¨å®è·µï¼Œä¸ºæ¨¡å‹è®­ç»ƒæµç¨‹è®¾è®¡æä¾›äº†ä¸°å¯Œçš„ç­–ç•¥ç»„åˆå‚è€ƒï¼Œæœ‰åŠ©äºæå‡æ¨¡å‹è®­ç»ƒæ•ˆæœä¸æ³›åŒ–èƒ½åŠ›ã€‚  
5. æ¨ç†è¡Œä¸ºåˆ†æï¼šå¯¹è§†è§‰æ¨ç†æ¨¡å‹ä¸­è®°å¿†ä¸æ³›åŒ–å¹³è¡¡ã€æ¨ç†æ—¶å¿«æ€è€ƒæ…¢æ€è€ƒã€æ€è€ƒtokené¢„ç®—æœ‰æ•ˆæ€§ã€æ¨ç†è¿ç§» pipeline ä¸­å¹»è§‰ç­‰æ–¹é¢çš„åˆ†æï¼Œä¸ºæ·±å…¥ç†è§£å¤šæ¨¡æ€æ¨¡å‹æ¨ç†æœºåˆ¶ã€ä¼˜åŒ–æ¨ç†è¿‡ç¨‹æä¾›äº†ç ”ç©¶æ–¹å‘ä¸æ€è·¯å€Ÿé‰´ã€‚

## self-guided-process-reward-optimization-with-redefined-step-wise-advantage-for-process-reinforcement-learning
### Abstract
Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— é¢å¤–å¼€é”€ï¼SPROé©æ–°è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¸ä¼˜åŠ¿ä¼°è®¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ï¼ˆPRLï¼‰åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›ä¸Šæ½œåŠ›å·¨å¤§ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤å¤§å…³é”®é—®é¢˜ï¼šä¸€æ˜¯å¼•å…¥é¢å¤–è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä¼šå¸¦æ¥é«˜æ˜‚è®¡ç®—å¼€é”€ï¼›äºŒæ˜¯ç¼ºä¹ç»Ÿä¸€çš„è¿‡ç¨‹çº§ä¼˜åŠ¿ä¼°è®¡ç†è®ºæ¡†æ¶ã€‚å½“å‰åŸºäºè¾…åŠ©è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰çš„æ–¹æ³•ï¼Œè¿˜å­˜åœ¨è®­ç»ƒéš¾ï¼ˆäººå·¥æ ‡æ³¨æˆ–è‡ªåŠ¨æ ‡æ³¨éƒ½éš¾æ»¡è¶³éœ€æ±‚ï¼‰ã€è®¡ç®—æˆæœ¬é«˜ï¼ˆé¢å¤–æ¨¡å‹å æ˜¾å­˜å½±å“è®­ç»ƒæ•ˆç‡ï¼‰ã€åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­éš¾æ‰©å±•ç­‰ç¼ºé™·ã€‚åŒæ—¶ï¼ŒåƒPRIMEè¿™ç±»ä¾èµ–è¾…åŠ©å¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼Œä¸ä»…æœ‰è®¡ç®—å¼€é”€ï¼Œä¼˜åŠ¿ä¼°è®¡è¿˜æ˜“æœ‰åå·®ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§æ—¢é«˜æ•ˆåˆèƒ½å‡†ç¡®ä¼°è®¡è¿‡ç¨‹ä¼˜åŠ¿çš„æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºSPROæ¥è§£å†³è¿™äº›ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¿‡ç¨‹å¥–åŠ±è‡ªå¼•å¯¼ï¼Œæ‘†è„±é¢å¤–æ¨¡å‹ä¾èµ–  
ç†è®ºä¸Šè¯æ˜è¿‡ç¨‹å¥–åŠ±å¯ä»ç­–ç•¥æ¨¡å‹æœ¬èº«å†…åœ¨æ¨å¯¼å¾—å‡ºï¼Œæ— éœ€é¢å¤–è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚è¿™æ—¢å…å»äº†PRMsè®­ç»ƒéš¾ã€è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œåˆä¿ç•™äº†ç»“æœç›‘ç£RLç®—æ³•ï¼ˆå¦‚GRPOï¼‰çš„ç®€æ´æ€§ä¸å¯æ‰©å±•æ€§ï¼Œå¯¹å·¥ä¸šè½åœ°å‹å¥½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡å®šä¹‰é€æ­¥ä¼˜åŠ¿ï¼Œç²¾å‡†ä¼°è®¡è¿‡ç¨‹ä¼˜åŠ¿  
å¼•å…¥**ç´¯ç§¯è¿‡ç¨‹å¥–åŠ±ï¼ˆCPRï¼‰**å’Œ**æ©ç é€æ­¥ä¼˜åŠ¿ï¼ˆMSAï¼‰**æ¥é‡æ–°å®šä¹‰é€æ­¥ä¼˜åŠ¿ã€‚CPRéšå¼èšåˆå‰ç¼€åºåˆ—ä¸­æ‰€æœ‰å…ˆå‰æ­¥éª¤çš„è¿‡ç¨‹å¥–åŠ±ï¼Œä½œä¸ºè¿‡ç¨‹å¥–åŠ±çš„æ›¿ä»£ï¼Œèƒ½åœ¨æ¯ä¸ªæ—¶é—´æ­¥æ›´å‡†ç¡®ä¼°è®¡æœŸæœ›å›æŠ¥ï¼›MSAåˆ™åœ¨å…±äº«æç¤ºé‡‡æ ·ç»„å†…ä¸¥æ ¼æ‰§è¡Œæ¯ä¸€æ­¥çš„æ¯”è¾ƒï¼Œå®ç°åˆç†çš„é€æ­¥åŠ¨ä½œä¼˜åŠ¿ä¼°è®¡ï¼Œè®©ä¼˜åŠ¿ä¼°è®¡æ›´è´´åˆåŸºäºä¼˜åŠ¿çš„ç­–ç•¥æ¢¯åº¦æ¡†æ¶ï¼Œå‡å°‘åå·®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜SPROç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼ˆå¦‚ vanilla GRPOï¼‰ä¼˜åŠ¿æ˜¾è‘—ï¼šè®­ç»ƒæ•ˆç‡æå‡3.4å€ï¼Œæµ‹è¯•å‡†ç¡®ç‡æå‡17.5%ï¼›è®­ç»ƒå…¨ç¨‹èƒ½ç»´æŒç¨³å®šä¸”è¾ƒé«˜çš„ç­–ç•¥ç†µï¼Œä¿è¯å……åˆ†æ¢ç´¢ï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘çº¦1/3ï¼Œé¿å…å¥–åŠ±é»‘å®¢é—®é¢˜ï¼›ä¸”å’ŒGRPOè¿™ç±»ç»“æœç›‘ç£RLæ–¹æ³•ç›¸æ¯”ï¼Œæ— é¢å¤–è®¡ç®—å¼€é”€ï¼Œåˆ©äºå·¥ä¸šåœºæ™¯å®æ–½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯åˆ›æ–°ï¼šæ‰“ç ´â€œè¿‡ç¨‹å¥–åŠ±å¿…é¡»ä¾èµ–é¢å¤–æ¨¡å‹â€çš„æ€ç»´å®šå¼ï¼Œè¯æ˜ä»ç­–ç•¥æ¨¡å‹è‡ªèº«æ¨å¯¼è¿‡ç¨‹å¥–åŠ±çš„å¯è¡Œæ€§ï¼Œä¸ºåç»­è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡å¼€è¾Ÿæ–°æ–¹å‘ã€‚  
2. ä¼˜åŠ¿ä¼°è®¡èŒƒå¼ï¼šé€šè¿‡CPRå’ŒMSAé‡æ–°å®šä¹‰é€æ­¥ä¼˜åŠ¿ï¼Œæä¾›äº†æ›´ä¸¥è°¨çš„è¿‡ç¨‹çº§ä¼˜åŠ¿ä¼°è®¡ç†è®ºæ¡†æ¶ï¼Œåç»­ç ”ç©¶åœ¨å¤„ç†åºåˆ—é€æ­¥å¥–åŠ±ä¸ä¼˜åŠ¿æ—¶å¯å‚è€ƒè¯¥èŒƒå¼ã€‚  
3. å·¥ä¸šè½åœ°å‹å¥½ï¼šåœ¨ä¿è¯æ€§èƒ½æå‡çš„åŒæ—¶æ§åˆ¶è®¡ç®—å¼€é”€ï¼Œä¸ºå·¥ä¸šç•Œå°†è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ç”¨äºæå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„æ–¹æ¡ˆå‚è€ƒã€‚

## data-driven-exploration-for-a-class-of-continuous-time-indefinite-linear--quadratic-reinforcement-learning-problems
### Abstract
We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¿ç»­æ—¶é—´ä¸å®šçº¿æ€§äºŒæ¬¡å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æ•°æ®é©±åŠ¨æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
çº¿æ€§ - äºŒæ¬¡ï¼ˆLQï¼‰æ§åˆ¶æ˜¯æœ€ä¼˜æ§åˆ¶ç†è®ºçš„åŸºçŸ³ï¼Œä½†ä¼ ç»Ÿç ”ç©¶å¤šåŸºäºæ¨¡å‹å·²çŸ¥çš„èŒƒå¼ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ¨¡å‹å‚æ•°å¾€å¾€éš¾ä»¥å®Œå…¨çŸ¥æ™“ï¼ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•å­˜åœ¨å‚æ•°ä¼°è®¡éš¾ã€å¯¹å‚æ•°æ•æ„Ÿç­‰é—®é¢˜ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºæœªçŸ¥æ¨¡å‹å‚æ•°çš„éšæœºæ§åˆ¶æä¾›äº†è§£å†³æ€è·¯ï¼Œç„¶è€Œç°æœ‰é’ˆå¯¹è¿ç»­æ—¶é—´RLçš„æ¢ç´¢ç­–ç•¥å¤šé‡‡ç”¨å›ºå®šæˆ–ç¡®å®šæ€§æ¢ç´¢è°ƒåº¦ï¼Œå­˜åœ¨éœ€å¤§é‡æ‰‹åŠ¨è°ƒå‚ã€ç¼ºä¹é€‚åº”æ€§ã€æ”¶æ•›æ…¢ç­‰ç¼ºé™·ã€‚åŒæ—¶ï¼Œä¸å®šLQæ§åˆ¶å› ç›®æ ‡å‡½æ•°ä¸­æ§åˆ¶é¡¹ä¸å®šï¼Œéœ€ç‰¹æ®Šå¤„ç†ï¼Œä¼ ç»ŸåŸºäºæ¨¡å‹æ–¹æ³•ä¾èµ–å¤æ‚çš„å¹¿ä¹‰Riccatiæ–¹ç¨‹ï¼Œè€ŒRLæœ‰æœ›ç»•å¼€æ­¤éš¾é¢˜ã€‚æœ¬æ–‡é’ˆå¯¹è¿ç»­æ—¶é—´éšæœºä¸å®šLQæ§åˆ¶é—®é¢˜ï¼Œæ—¨åœ¨æå‡ºæ›´é«˜æ•ˆçš„è‡ªé€‚åº”æ¢ç´¢æœºåˆ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ— æ¨¡å‹ã€æ•°æ®é©±åŠ¨çš„æ¢ç´¢æœºåˆ¶  
é’ˆå¯¹è¿ç»­æ—¶é—´éšæœºä¸å®šLQæ§åˆ¶é—®é¢˜ï¼Œè®¾è®¡äº†èƒ½è‡ªé€‚åº”è°ƒæ•´è¯„è®ºå®¶ï¼ˆcriticï¼‰çš„ç†µæ­£åˆ™åŒ–å’Œè¡ŒåŠ¨è€…ï¼ˆactorï¼‰çš„ç­–ç•¥æ–¹å·®çš„æ¢ç´¢æœºåˆ¶ï¼Œä¸åŒäºä»¥å¾€å›ºå®šæˆ–ç¡®å®šæ€§æ¢ç´¢è°ƒåº¦ï¼Œè¯¥æœºåˆ¶ä¾æ®æ™ºèƒ½ä½“å­¦ä¹ è¿›ç¨‹åŠ¨æ€è°ƒæ•´ï¼Œå‡å°‘è°ƒå‚éœ€æ±‚ä¸”æå‡å­¦ä¹ æ•ˆç‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†è®ºä¿è¯ä¸ regret ç•Œåˆ†æ  
åœ¨çµæ´»çš„è‡ªé€‚åº”æ¢ç´¢ä¸‹ï¼Œä»å®ç°äº†ä¸è¯¥ç±»LQé—®é¢˜å·²çŸ¥æœ€ä½³æ— æ¨¡å‹ç»“æœåŒ¹é…çš„æ¬¡çº¿æ€§ regret ç•Œï¼ˆ$O_p(N^{3/4})$ï¼‰ï¼ŒåŒæ—¶è¯æ˜äº†è‡ªé€‚åº”æ¢ç´¢å’Œç­–ç•¥å‚æ•°çš„å‡ ä¹å¿…ç„¶æ”¶æ•›æ€§ï¼Œä¸”æ— éœ€éé›¶åˆå§‹çŠ¶æ€å‡è®¾ï¼Œå®Œå…¨ä»æ•°æ®ä¸­å­¦ä¹ ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡æ•°å€¼å®éªŒéªŒè¯ç†è®ºç»“æœï¼Œä¸éè‡ªé€‚åº”æ— æ¨¡å‹åŠåŸºäºæ¨¡å‹çš„æ–¹æ³•å¯¹æ¯”ï¼Œè‡ªé€‚åº”æ¢ç´¢åŠ é€Ÿäº†æ”¶æ•›è¿‡ç¨‹ï¼Œåœ¨ regret æ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜ã€‚å³ä¾¿ç†è®ºä¸Šä¸å…ˆå‰å›ºå®šæ¢ç´¢è°ƒåº¦æ–¹æ³•çš„ regret ç•ŒåŒé˜¶ï¼Œä½†æ•°å€¼å®éªŒæ˜¾ç¤ºæœ¬æ–‡è‡ªé€‚åº”æ¢ç´¢ç­–ç•¥å§‹ç»ˆæ›´å‡ºè‰²ï¼Œèƒ½å®ç°æ›´ä½çš„ regretã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºè¿ç»­æ—¶é—´RLå°¤å…¶æ˜¯LQæ§åˆ¶åœºæ™¯ï¼Œæä¾›äº†è‡ªé€‚åº”æ¢ç´¢çš„æ–°æ€è·¯ï¼Œå±•ç¤ºäº†æ•°æ®é©±åŠ¨è°ƒæ•´æ¢ç´¢å‚æ•°ä»¥æå‡æ•ˆç‡çš„å¯è¡Œæ€§ï¼›åœ¨ç†è®ºåˆ†æä¸Šï¼Œä¸ºè‡ªé€‚åº”æ¢ç´¢æœºåˆ¶ä¸‹çš„æ”¶æ•›æ€§å’Œ regret ç•Œåˆ†ææä¾›äº†èŒƒä¾‹ï¼Œå¯æŒ‡å¯¼åç»­ç›¸å…³è¿ç»­æ—¶é—´RLé—®é¢˜çš„ç†è®ºç ”ç©¶ï¼›ç»•å¼€å¤æ‚å¹¿ä¹‰Riccatiæ–¹ç¨‹çš„RLè§£å†³ä¸å®šLQæ§åˆ¶é—®é¢˜çš„æ€è·¯ï¼Œä¸ºæ¨¡å‹æœªçŸ¥æ—¶çš„ä¸å®šæ§åˆ¶é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œåœ¨å·¥ç¨‹ã€ç»æµç­‰ä¾èµ–LQæ§åˆ¶çš„é¢†åŸŸæœ‰æ½œåœ¨åº”ç”¨å‚è€ƒä»·å€¼ã€‚

## eframe--deeper-reasoning-via-exploration-filter-replay-reinforcement-learning-framework
### Abstract
Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filter-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame not only enables
fine-grained categorization of training samples for deeper insight into their
contributions, but also introduces an efficient and precise mechanism for
entropy control, which is critical for balancing exploration and convergence in
RL training. Our code is available at https://github.com/597358816/EFRame.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EFRameï¼šæ¢ç´¢-è¿‡æ»¤-å›æ”¾æ¡†æ¶åŠ©åŠ›å¤§æ¨¡å‹å®ç°æ›´æ·±åº¦æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒä¸­ä½œç”¨å…³é”®ï¼Œèƒ½å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä½œä¸ºPPOé«˜æ•ˆå˜ä½“ï¼Œè™½é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨æ¢ç´¢èƒ½åŠ›æœ‰é™ã€æ ·æœ¬æ•ˆç‡ä½å’Œè®­ç»ƒä¸ç¨³å®šç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚æ¯”å¦‚RLVRä¾èµ–æ¨¡å‹è‡ªèº«pass@kè¾“å‡ºæå–å¥–åŠ±ä¿¡å·ï¼Œå­˜åœ¨å¥–åŠ±ä¿¡å·æ¶ˆå¤±å¯¼è‡´æ¨¡å‹éš¾ä»æ•°æ®ä¸­æå–æœ‰ç”¨çŸ¥è¯†ã€æ¢ç´¢å—é™ç­‰æƒ…å†µï¼›GRPOä¹Ÿå› è¿™äº›é—®é¢˜åœ¨åæœŸè®­ç»ƒæ˜“ä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæœ¬æ–‡æå‡ºEFRameæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢ - è¿‡æ»¤ - å›æ”¾ä¸€ä½“åŒ–æ¡†æ¶è®¾è®¡
EFRameä»ä¸‰ä¸ªå…³é”®ç»´åº¦ç³»ç»Ÿå¢å¼ºGRPOã€‚ä¸€æ˜¯é¢å¤–rolloutsæ¢ç´¢ï¼Œé’ˆå¯¹å¸¸è§„rolloutséš¾å¤„ç†çš„æœ‰æŒ‘æˆ˜promptï¼Œæ–°å¢rollouté˜¶æ®µè¿›è¡Œé’ˆå¯¹æ€§æ¢ç´¢ï¼Œç”Ÿæˆå°‘é‡é«˜è´¨é‡æ­£æ ·æœ¬å’Œå¤§é‡ä½è´¨é‡è´Ÿæ ·æœ¬ï¼›äºŒæ˜¯åœ¨çº¿è¿‡æ»¤æœºåˆ¶ï¼Œä¸¢å¼ƒä½è´¨é‡æ ·æœ¬ä»¥é™ä½è®­ç»ƒæ–¹å·®ã€æå‡æ•ˆç‡ï¼›ä¸‰æ˜¯ç»éªŒå›æ”¾ï¼Œå°†é«˜ä¼˜åŠ¿æ ·æœ¬å­˜å…¥å›æ”¾ç¼“å†²åŒºå¹¶åœ¨è®­ç»ƒä¸­å¤ç”¨ï¼Œæ”¾å¤§å…¶å½±å“å¹¶å¢å¼ºæ¨¡å‹ç¨³å®šæ€§ï¼Œæ„å»ºä»æ¢ç´¢åˆ°æ”¶æ•›çš„å®Œæ•´ç¨³å®šå­¦ä¹ å‘¨æœŸã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ ·æœ¬ç±»å‹æ„ŸçŸ¥è®­ç»ƒ
å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œæœ‰åŸåˆ™çš„åˆ†ç±»ï¼Œæ·±å…¥ç†è§£ä¸åŒæ ·æœ¬åœ¨å­¦ä¹ ä¸­çš„ç‹¬ç‰¹ä½œç”¨ã€‚æ˜ç¡®é«˜ä¼˜åŠ¿ã€ä½æ¦‚ç‡æ­£æ ·æœ¬å¯¹è§¦å‘æ¢ç´¢å’Œè§£é”æ·±å±‚çŸ¥è¯†è‡³å…³é‡è¦ï¼Œè€Œä½ä¼˜åŠ¿è´Ÿæ ·æœ¬æ˜“å¯¼è‡´è¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜è§£éœ€æŠ‘åˆ¶ï¼Œå®ç°å¯¹æ ·æœ¬è´¡çŒ®çš„ç»†ç²’åº¦è®¤çŸ¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç²¾å‡†é«˜æ•ˆçš„ç†µæ§åˆ¶
é€šè¿‡è”åˆè°ƒæ•´é¢å¤–rolloutæ¸©åº¦å’Œå›æ”¾æ ·æœ¬æ•°é‡å®ç°ç†µæ§åˆ¶ï¼Œç›¸æ¯”ä¼ ç»ŸåŸºäºå¥–åŠ±çš„ç†µæ­£åˆ™åŒ–ï¼Œæä¾›æ›´çµæ´»ç¨³å®šçš„ç†µè°ƒèŠ‚ï¼Œæ”¯æ’‘é«˜æ•ˆæ¢ç´¢ä¸ç¨³å®šæ”¶æ•›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†EFRameçš„æœ‰æ•ˆæ€§ï¼Œå¦‚åœ¨å…·æŒ‘æˆ˜æ€§çš„Geometry3KåŸºå‡†ä¸Šï¼Œæ¯”GRPOæ€§èƒ½æå‡36.5%ï¼›åœ¨å¤šæ¨¡æ€å’Œæ•°å­¦æ¨ç†ç­‰å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸­ä¹ŸæŒç»­å–å¾—å¢ç›Šï¼Œå±•ç°å‡ºå¼ºæ³›åŒ–æ€§ä¸é²æ£’æ€§ï¼Œè¯æ˜å…¶ä¸ä»…æå‡è®­ç»ƒé²æ£’æ€§å’Œæ•ˆç‡ï¼Œè¿˜èƒ½è®©æ¨¡å‹è·å¾—åœ¨åŸå§‹GRPOä¸‹éš¾ä»¥å®ç°çš„æ›´æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡æ€è·¯ï¼šé’ˆå¯¹å·²æœ‰æ–¹æ³•å±€é™ï¼Œä»æ¢ç´¢ã€è¿‡æ»¤ã€å›æ”¾å¤šç»´åº¦ç³»ç»Ÿæ„å»ºæ¡†æ¶è§£å†³é—®é¢˜ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„æ¡†æ¶è®¾è®¡æä¾›äº†å¤šç»´åº¦ååŒä¼˜åŒ–çš„æ€è·¯ï¼Œå¯å€Ÿé‰´è¿™ç§ç³»ç»Ÿæ€§å¢å¼ºå·²æœ‰ç®—æ³•çš„æ–¹å¼åº”å¯¹ç®—æ³•ç“¶é¢ˆã€‚
2. æ ·æœ¬ç®¡ç†ä¸åˆ†æï¼šå¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œç»†ç²’åº¦åˆ†ç±»å¹¶åˆ†æä¸åŒæ ·æœ¬ä½œç”¨ï¼Œè¿™ç§å…³æ³¨æ ·æœ¬åœ¨å­¦ä¹ ä¸­è§’è‰²çš„æ€è·¯ï¼Œæœ‰åŠ©äºåœ¨å…¶ä»–æ¨¡å‹è®­ç»ƒä»»åŠ¡ä¸­æ›´ç²¾å‡†åˆ©ç”¨æ•°æ®ï¼Œæå‡è®­ç»ƒæ•ˆæœã€‚
3. ç†µæ§åˆ¶æ–¹å¼ï¼šåˆ›æ–°çš„ç†µæ§åˆ¶æœºåˆ¶ï¼Œè„±ç¦»ä¼ ç»Ÿä»…åŸºäºå¥–åŠ±çš„ç†µæ­£åˆ™åŒ–æ€è·¯ï¼Œé€šè¿‡è”åˆè°ƒæ•´å‚æ•°å®ç°æ›´çµæ´»ç¨³å®šè°ƒèŠ‚ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­å¹³è¡¡æ¢ç´¢ä¸æ”¶æ•›çš„ç†µæ§åˆ¶æä¾›äº†æ–°çš„å®ç”¨æ–¹æ³•å‚è€ƒã€‚ 
4. ä»£ç å¼€æºï¼šæä¾›äº†ä»£ç ä»“åº“ï¼ˆhttps://github.com/597358816/EFRameï¼‰ï¼Œæ–¹ä¾¿ç ”ç©¶è€…å¤ç°å®éªŒã€å­¦ä¹ æ–¹æ³•ç»†èŠ‚å¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›æ‹“å±•ï¼Œåˆ©äºç›¸å…³é¢†åŸŸç ”ç©¶çš„æ¨è¿›ä¸äº¤æµã€‚

## apo--enhancing-reasoning-ability-of-mllms-via-asymmetric-policy-optimization
### Abstract
Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | APOï¼šç”¨éå¯¹ç§°ç­–ç•¥ä¼˜åŒ–æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è™½èƒ½æ•´åˆå¤šç§æ¨¡æ€æ•°æ®ï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ¬ ä½³ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½èƒ½å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›ï¼Œå¯åº”ç”¨åˆ°MLLMsæ—¶å´é¢ä¸´éš¾é¢˜ï¼šä¸€æ˜¯åœ¨é€šç”¨ä»»åŠ¡ä¸Šæ€§èƒ½ä¸‹é™ï¼ŒäºŒæ˜¯æ˜“äº§ç”Ÿâ€œè¿‡åº¦æ€è€ƒâ€å¼çš„å†—é•¿æ¨ç†ã€‚åŒæ—¶ï¼ŒKLæƒ©ç½šä¸â€œè¿‡åº¦æ€è€ƒâ€å¯¹MLLMsçš„RLè®­ç»ƒå½±å“æœºåˆ¶å°šä¸æ¸…æ™°ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æ¢ç´¢å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ä»¥æå‡MLLMsæ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºéå¯¹ç§°ç­–ç•¥ä¼˜åŒ–ï¼ˆAPOï¼‰æ¡†æ¶  
å°†é‡‡æ ·å¾—åˆ°çš„å“åº”åˆ†ä¸ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ä¸¤ç»„ï¼Œåˆ†åˆ«é’ˆå¯¹æ€§ä¼˜åŒ–ã€‚å¯¹æ­£æ ·æœ¬ï¼Œè®¾è®¡éš¾åº¦è‡ªé€‚åº”æ•£åº¦å¡‘é€ ï¼ˆDADSï¼‰ï¼›å¯¹è´Ÿæ ·æœ¬ï¼Œæå‡ºæ¬¡ä¼˜è½¨è¿¹å¤æ‚åº¦æ­£åˆ™åŒ–ï¼ˆSTCRï¼‰ï¼ŒåŒç®¡é½ä¸‹æå‡MLLMsæ¨ç†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDifficulty - Adaptive Divergence Shapingï¼ˆDADSï¼‰  
é’ˆå¯¹æ­£æ ·æœ¬ï¼Œä¾æ®æ ·æœ¬éš¾åº¦åŠ¨æ€è°ƒæ•´KLæ•£åº¦æƒé‡ã€‚æ­¤æ–¹æ³•é¿å…ç­–ç•¥ç†µéª¤é™ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ï¼Œæ›´é«˜æ•ˆåˆ©ç”¨æ ·æœ¬ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹å·²æœ‰çŸ¥è¯†ã€‚è§£å†³äº†â€œåŠ KLæƒ©ç½šä¿é€šç”¨èƒ½åŠ›ä½†ç‰ºç‰²æ¨ç†ï¼Œå»KLæƒ©ç½šææ¨ç†ä½†ä¸¢é€šç”¨â€çš„ä¸¤éš¾é—®é¢˜ï¼Œå®ç°æ¨ç†ä¸é€šç”¨èƒ½åŠ›å¹³è¡¡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šSuboptimal Trajectory Complexity Regularizationï¼ˆSTCRï¼‰  
é’ˆå¯¹è´Ÿæ ·æœ¬ï¼Œæƒ©ç½šè¿‡é•¿å“åº”ä»¥ç¼“è§£â€œè¿‡åº¦æ€è€ƒâ€ã€‚è®©æ¨¡å‹ç”Ÿæˆæ›´ç®€æ´æ¨ç†çš„åŒæ—¶ï¼Œä¿ç•™æ¢ç´¢èƒ½åŠ›ï¼Œå‡å°‘æ¨ç†ä¸­å†—ä½™é‡å¤ä¿¡æ¯å¹²æ‰°ï¼Œé¿å…é”™è¯¯æ¨å¯¼ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
ä»¥Qwen2.5 - VL - 3Bä¸ºåŸºç¡€æ¨¡å‹ï¼Œè®­ç»ƒå¾—åˆ°View - R1 - 3Bã€‚åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¯”åŸºç¡€æ¨¡å‹å¹³å‡æå‡7%ï¼Œè¿˜èƒ½è¶…è¶Š7 - 11Bè§„æ¨¡çš„æ›´å¤§MLLMsã€‚æ›´å…³é”®çš„æ˜¯ï¼Œå…¶ä»–æ¨ç†è°ƒä¼˜çš„MLLMsåœ¨é€šç”¨ä»»åŠ¡ä¸Šå¸¸æ€§èƒ½ä¸‹é™ï¼Œè€ŒView - R1 - 3Båœ¨é€šç”¨ä»»åŠ¡ä¸Šä¹Ÿä¿æŒæ€§èƒ½æå‡ï¼Œæ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼ŒéªŒè¯äº†DADSå’ŒSTCRæŠ€æœ¯åœ¨æ¨è¿›MLLMså¤æ‚å¤šæ¨¡æ€æ¨ç†ä¸Šçš„æœ‰æ•ˆæ€§ä¸å¹¿æ³›é€‚ç”¨æ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜åˆ†æè§’åº¦ï¼šæ·±å…¥åˆ†æKLæƒ©ç½šå’Œâ€œè¿‡åº¦æ€è€ƒâ€å¯¹MLLMs RLè®­ç»ƒçš„å½±å“ï¼Œä¸ºåç»­ä¼˜åŒ–æŒ‡æ˜æ–¹å‘ï¼Œè¿™ç§å¯¹è®­ç»ƒä¸­å…³é”®å› ç´ çš„å‰–ææ€è·¯å€¼å¾—å€Ÿé‰´ã€‚  
2. æ ·æœ¬åˆ†ç»„ä¼˜åŒ–ï¼šå°†æ ·æœ¬åˆ†æ­£è´Ÿç»„åˆ†åˆ«è®¾è®¡ä¼˜åŒ–ç­–ç•¥ï¼Œé’ˆå¯¹æ€§è§£å†³ä¸åŒé—®é¢˜ï¼Œä¸ºå¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„æ¨¡å‹è®­ç»ƒæä¾›â€œåˆ†è€Œæ²»ä¹‹â€çš„æ€è·¯ã€‚  
3. åŠ¨æ€ä¸æ­£åˆ™åŒ–æŠ€å·§ï¼šDADSçš„åŠ¨æ€æƒé‡è°ƒæ•´ã€STCRçš„å¤æ‚åº¦æ­£åˆ™åŒ–ï¼Œåœ¨å¹³è¡¡æ¨¡å‹èƒ½åŠ›ï¼ˆå¦‚æ¨ç†ä¸é€šç”¨ã€æ¢ç´¢ä¸ç®€æ´ï¼‰æ–¹é¢æä¾›äº†å…·ä½“æŠ€æœ¯æ‰‹æ®µï¼Œå¯è¿ç§»åˆ°å…¶ä»–æ¨¡å‹è®­ç»ƒåœºæ™¯è§£å†³ç±»ä¼¼æƒè¡¡é—®é¢˜ã€‚
```

## srft--a-single-stage-method-with-supervised-and-reinforcement-fine-tuning-for-reasoning
### Abstract
Large language models (LLMs) have achieved remarkable progress in reasoning
tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) remains a fundamental challenge. Through
comprehensive analysis of token distributions, learning dynamics, and
integration mechanisms from entropy-based perspectives, we reveal key
differences between these paradigms: SFT induces coarse-grained global changes
to LLM policy distributions, while RL performs fine-grained selective
optimizations, with entropy serving as a critical indicator of training
effectiveness. Building on these observations, we propose Supervised
Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both
fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach
simultaneously applies SFT and RL to directly optimize the LLM using
demonstrations and self-exploration rollouts rather than through two-stage
sequential methods. Extensive experiments show that SRFT achieves 59.1% average
accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning
benchmarks and 10.9% on three out-of-distribution benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SRFTï¼šå•é˜¶æ®µèåˆç›‘ç£ä¸å¼ºåŒ–å¾®è°ƒï¼Œçªç ´å¤§æ¨¡å‹æ¨ç†ç“¶é¢ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¦‚ä½•**æœ€ä¼˜æ•´åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ä»æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¼ ç»Ÿåšæ³•å°†SFTå’ŒRLä½œä¸ºç‹¬ç«‹ã€é¡ºåºçš„é˜¶æ®µï¼šSFTè™½èƒ½è®©æ¨¡å‹å­¦ä¹ æŒ‡ä»¤éµå¾ªï¼Œä½†æ˜“å¯¼è‡´æ¨¡å¼è®°å¿†ç”šè‡³è¿‡æ‹Ÿåˆï¼Œç¼ºä¹çœŸæ­£æ¨ç†èƒ½åŠ›ï¼›RLè™½æ“…é•¿æ¢ç´¢ä¸å¥–åŠ±ä¼˜åŒ–ï¼Œå´å­˜åœ¨æ ·æœ¬ä½æ•ˆã€å¤§ç©ºé—´æ¢ç´¢å›°éš¾æˆ–æ¨¡å¼åå¡Œç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œç®€å•çš„â€œå…ˆSFTå†RLâ€ä¸¤é˜¶æ®µæ¨¡å¼ï¼Œéš¾ä»¥å¹³è¡¡çŸ¥è¯†è’¸é¦ä¸ç­–ç•¥ä¼˜åŒ–ï¼Œè¦ä¹ˆæ•´åˆä¸è¶³å¼•å‘è¯¯å·®ä¼ æ’­ï¼Œè¦ä¹ˆè¿‡åº¦ä¾èµ–æ¼”ç¤ºå¯¼è‡´æ¢ç´¢å—é™ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å•é˜¶æ®µç»Ÿä¸€SFTä¸RLã€å…¼é¡¾æ¼”ç¤ºå­¦ä¹ ä¸ç­–ç•¥æ¢ç´¢çš„æ–°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»ç†µè§†è§’å‰–æSFTä¸RLçš„æœ¬è´¨å·®å¼‚  
é€šè¿‡å¯¹tokenåˆ†å¸ƒã€å­¦ä¹ åŠ¨æ€å’Œæ•´åˆæœºåˆ¶çš„ç†µåŸºåˆ†æï¼Œæ­ç¤ºä¸¤å¤§èŒƒå¼æ ¸å¿ƒåŒºåˆ«ï¼š**SFTå¯¹LLMç­–ç•¥åˆ†å¸ƒäº§ç”Ÿâ€œç²—ç²’åº¦å…¨å±€å˜åŒ–â€**ï¼Œä¾§é‡æ‹Ÿåˆæ¼”ç¤ºæ•°æ®ï¼›**RLåˆ™å®ç°â€œç»†ç²’åº¦é€‰æ‹©æ€§ä¼˜åŒ–â€**ï¼Œèšç„¦ç­–ç•¥é‚»åŸŸçš„æ¢ç´¢ã€‚è€Œâ€œç†µâ€å¯ä½œä¸ºè®­ç»ƒæœ‰æ•ˆæ€§çš„å…³é”®æŒ‡æ ‡ï¼Œåæ˜ è®­ç»ƒè¿‡ç¨‹ä¸­åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ä¸æ¢ç´¢ç¨‹åº¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºå•é˜¶æ®µæ–¹æ³•SRFTï¼ˆSupervised Reinforcement Fine-Tuningï¼‰  
SRFTé€šè¿‡**ç†µæ„ŸçŸ¥åŠ æƒæœºåˆ¶**ç»Ÿä¸€SFTä¸RLï¼šä¸å†åˆ†é˜¶æ®µï¼Œè€Œæ˜¯åŒæ—¶åº”ç”¨SFTä¸RLï¼Œåˆ©ç”¨æ¼”ç¤ºæ•°æ®å’Œè‡ªæ¢ç´¢rolloutsç›´æ¥ä¼˜åŒ–LLMã€‚å¯¹LLMç­–ç•¥ç”Ÿæˆçš„æ ·æœ¬ï¼Œä¾å¥–åŠ±æ­£è´Ÿé‡‡ç”¨ä¸åŒRLæŸå¤±ï¼›å¯¹æ¼”ç¤ºæ•°æ®é›†æ ·æœ¬ï¼ŒåŒæ—¶æ–½åŠ SFTä¸RLç›®æ ‡ã€‚è¿™ç§è®¾è®¡è®©æ¨¡å‹åœ¨å¤šç²’åº¦ä¸‹ç¨³å®šå­¦ä¹ æ¼”ç¤ºï¼Œåˆèƒ½æ¡¥æ¥SFTï¼ˆçŸ¥è¯†ä¼ é€’ï¼‰ä¸RLï¼ˆç­–ç•¥æ¢ç´¢ï¼‰çš„äº’è¡¥ä¼˜åŠ¿ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨5ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å’Œ3ä¸ªåˆ†å¸ƒå¤–ï¼ˆOODï¼‰åŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŸºäºQwen - 2.5 - Math - 7Bçš„SRFTå¹³å‡å‡†ç¡®ç‡è¾¾59.1%ï¼š  
- æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¯”æ— RLçš„æ–¹æ³•é«˜å‡º9.0%ï¼›  
- OODä»»åŠ¡ä¸­ï¼Œç›¸å¯¹é›¶RLåŸºçº¿æå‡10.9%ï¼›  
- ä¸å…¶ä»–ç”¨æ¼”ç¤ºæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ³›åŒ–èƒ½åŠ›å¹³å‡æå‡è¶…4.7%ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **åˆ†æè§†è§’åˆ›æ–°**ï¼šä»â€œç†µâ€åˆ‡å…¥åˆ†æSFTä¸RLå¯¹ç­–ç•¥åˆ†å¸ƒã€å­¦ä¹ åŠ¨æ€çš„å½±å“ï¼Œä¸ºç†è§£ä¸¤å¤§å¾®è°ƒèŒƒå¼æä¾›äº†æ–°çš„ç†è®ºé€é•œï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç±»â€œé‡åŒ–+æœºåˆ¶è§£æâ€çš„åˆ†ææ€è·¯ã€‚  
2. **å•é˜¶æ®µèŒƒå¼çªç ´**ï¼šæ‰“ç ´â€œå…ˆSFTåRLâ€çš„å›ºæœ‰æ€ç»´ï¼Œè¯æ˜å•é˜¶æ®µèåˆèƒ½æ›´é«˜æ•ˆä¼˜åŒ–æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹å¾®è°ƒæµç¨‹è®¾è®¡æä¾›äº†èŒƒå¼çº§å¯å‘ã€‚  
3. **å·¥ç¨‹è½åœ°ä»·å€¼**ï¼šSRFTåœ¨æ•°å­¦æ¨ç†ä¸OODä»»åŠ¡çš„æ˜¾è‘—å¢ç›Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚æ¨ç†ã€æ³›åŒ–åœºæ™¯çš„å®ç”¨æ€§ï¼Œå¯¹éœ€å¼ºæ¨ç†èƒ½åŠ›çš„LLMä¸‹æ¸¸åº”ç”¨ï¼ˆå¦‚æ•°å­¦è§£é¢˜ã€é€»è¾‘æ¨ç†ç±»AIäº§å“ï¼‰æœ‰ç›´æ¥å‚è€ƒä»·å€¼ã€‚

## confucius3-math--a-lightweight-high-performance-reasoning-llm-for-chinese-k-12-mathematics-learning
### Abstract
We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç½‘æ˜“æœ‰é“å¼€æºConfucius3-Mathï¼šä¸ºä¸­å›½K-12æ•°å­¦æ•™è‚²æ‰“é€ è½»é‡é«˜æ€§èƒ½æ¨ç†å¤§æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æˆä¸ºç ”ç©¶çƒ­ç‚¹çš„å½“ä¸‹ï¼ŒOpenAIçš„o1ç­‰æ¨¡å‹è™½å±•ç°å¼ºå¤§æ¨ç†æ€§èƒ½ä½†æœªå¼€æºæŠ€æœ¯ç»†èŠ‚ï¼Œä¸”æ¨ç†æ—¶è®¡ç®—ï¼ˆTTCï¼‰åœ¨LLMåœºæ™¯è½åœ°æœ‰æŒ‘æˆ˜ï¼›åŒæ—¶ï¼Œæ•™è‚²é¢†åŸŸå¯¹æ¨¡å‹å‡†ç¡®æ€§è¦æ±‚é«˜ï¼Œç°æœ‰å¼ºæ¨ç†LLMåœ¨K - 12ä»»åŠ¡è¡¨ç°ä¸ä½³ï¼Œä¸”é«˜æ€§èƒ½LLMå¼€å‘éƒ¨ç½²æˆæœ¬é«˜åŠ å‰§æ•™è‚²èµ„æºæ•°å­—é¸¿æ²Ÿã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œç½‘æ˜“æœ‰é“å›¢é˜Ÿèšç„¦ä¸­å›½K - 12æ•°å­¦æ•™è‚²é¢†åŸŸï¼Œæ—¨åœ¨æ‰“é€ ä½æˆæœ¬ã€é«˜æ€§èƒ½ã€å¼€æºçš„æ¨ç†å¤§æ¨¡å‹ï¼ŒåŠ©åŠ›æ•™è‚²æ™®æƒ ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šTargeted Entropy Regularizationï¼ˆé¶å‘ç†µæ­£åˆ™åŒ–ï¼‰
æå‡ºäº†ä¸€ç§å…¨æ–°çš„ç†µæ­£åˆ™åŒ–æ–¹å¼ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è¯¥æ­£åˆ™åŒ–æ‰‹æ®µèƒ½æ˜¾è‘—ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œè®©æ¨¡å‹è®­ç»ƒæ›´å¹³ç¨³ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æå‡æä¾›æ”¯æ’‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šRecent Sample Recoveryï¼ˆè¿‘æœŸæ ·æœ¬æ¢å¤ï¼‰
è®¾è®¡äº†æ–°é¢–çš„æ•°æ®è°ƒåº¦ç­–ç•¥ï¼Œå€ŸåŠ©è¯¥ç­–ç•¥å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨è®­ç»ƒæ•°æ®ï¼Œæå‡æ•°æ®ä½¿ç”¨æ•ˆç‡ï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒæ—¶èƒ½æ›´å……åˆ†åœ°ä»æ•°æ®ä¸­å­¦ä¹ æœ‰æ•ˆä¿¡æ¯ï¼Œè¿›è€ŒåŠ©åŠ›æ¨¡å‹æ€§èƒ½æå‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šPolicy - Specific Hardness Weightingï¼ˆç‰¹å®šç­–ç•¥éš¾åº¦åŠ æƒï¼‰
æ”¹è¿›äº†ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨ï¼Œé€šè¿‡è€ƒè™‘ç­–ç•¥ç›¸å…³çš„éš¾åº¦åŠ æƒï¼Œåœ¨RLè®­ç»ƒä¸­è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒæ•ˆæœï¼Œå¯¹æå‡æ¨¡å‹æ€§èƒ½èµ·åˆ°ç§¯æä½œç”¨ã€‚

æ­¤å¤–ï¼Œæ•´ä½“é‡‡ç”¨åŸºäºå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹å¼ï¼Œè®©æ¨¡å‹èƒ½è´´åˆå›½å®¶è¯¾ç¨‹æ ‡å‡†ï¼Œä½æˆæœ¬åœ°è§£å†³ä¸­å›½K - 12ä¸»æµæ•°å­¦é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Confucius3 - Mathåœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œè¾¾æˆSOTAæ€§èƒ½ã€‚åœ¨å¦‚CK12 - MATHã€GAOKAO - Bench (Math)ã€MathBench (K12)ç­‰ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•é‡Œï¼Œå¯¹æ¯”DeepSeek - R1ã€Qwen3 - 14Bç­‰æ¨¡å‹ï¼Œåœ¨å¾ˆå¤šä»»åŠ¡ä¸Šå‡†ç¡®ç‡é¢†å…ˆï¼Œä¾‹å¦‚åœ¨éƒ¨åˆ†æµ‹è¯•ä¸­å‡†ç¡®ç‡è¶…è¿‡90%ç”šè‡³æ›´é«˜ï¼Œä¸”åœ¨ç›®æ ‡é¢†åŸŸï¼ˆä¸­å›½K - 12æ•°å­¦å­¦ä¹ ï¼‰è¡¨ç°çªå‡ºï¼Œè¿˜èƒ½åœ¨å•æ¶ˆè´¹çº§GPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œæ¨ç†æ€§èƒ½çº¦ä¸ºDeepSeek - R1çš„15å€ï¼Œè®­ç»ƒæˆæœ¬ä»…26000ç¾å…ƒï¼Œæœ‰åŠ›è¯æ˜äº†åœ¨ç‰¹å®šé¢†åŸŸä½æˆæœ¬æ„å»ºå¼ºæ¨ç†æ¨¡å‹çš„å¯è¡Œæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é¢†åŸŸèšç„¦ä¸æ™®æƒ æ€è·¯ï¼šèšç„¦K - 12æ•™è‚²è¿™ç±»ç‰¹å®šé¢†åŸŸæ‰“é€ æ¨¡å‹ï¼Œå…³æ³¨æ•™è‚²æ™®æƒ ï¼Œä¸ºåˆ©ç”¨AIç¼©å°æ•™è‚²èµ„æºå·®è·æä¾›äº†å®è·µèŒƒä¾‹ï¼Œå¯ç¤ºåç»­ç ”ç©¶å¯ç„å‡†å‚ç›´é¢†åŸŸåšæ·±åº¦ä¼˜åŒ–ã€‚
2. æŠ€æœ¯åˆ›æ–°å¤ç”¨ï¼šæ–‡ä¸­æå‡ºçš„Targeted Entropy Regularizationã€Recent Sample Recoveryã€Policy - Specific Hardness Weightingè¿™ä¸‰é¡¹æŠ€æœ¯åˆ›æ–°ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§æ¨¡å‹å°¤å…¶æ˜¯é¢†åŸŸæ¨ç†æ¨¡å‹æ—¶ï¼Œä¸ºç¨³å®šè®­ç»ƒã€æå‡æ•°æ®æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½æä¾›äº†æ–°çš„æŠ€æœ¯æ€è·¯å’Œæ–¹æ³•å‚è€ƒï¼Œåç»­ç›¸å…³æ–¹å‘ç ”ç©¶å¯å€Ÿé‰´è¿™äº›åˆ›æ–°ç‚¹æ¥ä¼˜åŒ–è®­ç»ƒæµç¨‹ã€‚
3. ä½æˆæœ¬å®è·µè·¯å¾„ï¼šå±•ç¤ºäº†åŸºäºåˆé€‚åŸºç¡€æ¨¡å‹ã€é€‚é‡é«˜è´¨é‡æ•°æ®å’Œè‰¯å¥½è®­ç»ƒæ–¹æ¡ˆï¼Œé€šè¿‡RLåè®­ç»ƒåœ¨è½»é‡æ¨¡å‹ä¸­æ¿€å‘å¼ºæ¨ç†èƒ½åŠ›çš„è·¯å¾„ï¼Œè¯æ˜äº†ä½æˆæœ¬æ„å»ºç‰¹å®šé¢†åŸŸå¼ºæ¨ç†æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºèµ„æºæœ‰é™ä½†æƒ³åšé¢†åŸŸæ¨¡å‹çš„å›¢é˜Ÿæä¾›äº†å®è·µå‚è€ƒã€‚
4. å¼€æºç”Ÿæ€è´¡çŒ®ï¼šå¼€æºæ¨¡å‹ä¸ä»£ç ï¼Œé‚€è¯·ç¤¾åŒºå‚ä¸ï¼Œè¿™ç§å¼€æ”¾åä½œçš„æ¨¡å¼æœ‰åŠ©äºæ¨åŠ¨æ•´ä¸ªLLMæ¨ç†æ¨¡å‹åœ¨æ•™è‚²ç­‰é¢†åŸŸçš„åº”ç”¨å‘å±•ï¼Œä¸ºè¡Œä¸šå¼€æºç”Ÿæ€å»ºè®¾æä¾›äº†ç§¯æèŒƒä¾‹ã€‚

## adapthink--adaptive-thinking-preferences-for-reasoning-language-model
### Abstract
Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdapThinkï¼šè®©æ¨ç†è¯­è¨€æ¨¡å‹æ‹¥æœ‰è‡ªé€‚åº”â€œæ€è€ƒåå¥½â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæå¤§æå‡äº†è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå‚¬ç”Ÿäº†ç²¾ç»†çš„è‡ªæˆ‘åæ€è¿‡ç¨‹ï¼ˆå³â€œæ…¢æ€è€ƒâ€èŒƒå¼ï¼‰ã€‚ä½†è¯¥èŒƒå¼å­˜åœ¨æ¨ç†æ•ˆç‡éš¾é¢˜ï¼šæ¨¡å‹é¢å¯¹ç®€å•é—®é¢˜å¯èƒ½è¿‡åº¦è®¡ç®—ï¼Œé¢å¯¹å¤æ‚é—®é¢˜å´å¯èƒ½è¿‡æ—©åˆ‡æ¢æ¨ç†æ€è·¯ã€‚ä»¥å¾€æœºåˆ¶ä¾èµ–é™æ€é•¿åº¦é¢„ç®—æˆ–é¢„å®šä¹‰è§„åˆ™ï¼Œç¼ºä¹å¯¹é—®é¢˜å¤æ‚åº¦å˜åŒ–å’Œæ¨¡å‹èƒ½åŠ›æ¼”è¿›çš„é€‚åº”æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§è‡ªé€‚åº”åè®­ç»ƒæ¡†æ¶ï¼Œåœ¨ç»´æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶æå‡æ•ˆç‡ï¼ŒAdapThink åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°  
è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°æ¥è°ƒæ•´æ¨¡å‹å½“å‰æ¨ç†åå¥½ã€‚è¯¥å‡½æ•°å€ŸåŠ©ç”Ÿæˆå“åº”çš„ç»„å†…å‡†ç¡®ç‡ï¼Œè®©æ¨¡å‹å­¦ä¹ ç¡®å®šåˆé€‚çš„åæ€åå¥½ï¼›åŒæ—¶é€šè¿‡ç»Ÿè®¡è®­ç»ƒæ ·æœ¬ç»„ä¸­å…³é”®è¿‡æ¸¡è¯æ•°é‡ï¼Œå®šé‡è¡¡é‡æ¨ç†æ•ˆç‡ï¼Œæ— éœ€ä¾èµ–å›ºå®šé•¿åº¦åå¥½ï¼Œè€Œæ˜¯åŸºäºæ¨¡å‹ç½®ä¿¡åº¦å’Œå“åº”ç‰¹å¾åŠ¨æ€è°ƒæ•´ä¸åæ€ç›¸å…³çš„è¿‡æ¸¡è¯åå¥½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶  
æå‡ºå¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶ä»¥å¹³è¡¡è®­ç»ƒæ ·æœ¬ç»„çš„è§£é¢˜å‡†ç¡®ç‡ä¸æ¨ç†å¤šæ ·æ€§ã€‚å…ˆå¯¹æ¨ç†å®ä¾‹è¿‡é‡‡æ ·ï¼Œå†ç”¨ç²¾å¿ƒå®šä¹‰çš„å¤šæ ·æ€§æŒ‡æ ‡è¯„ä¼°å®ä¾‹çš„æœ€ç»ˆç­”æ¡ˆå’Œä¸­é—´æ­¥éª¤ï¼Œæœ€åé€šè¿‡å¤šæ ·æ€§æ„ŸçŸ¥ä¸‹é‡‡æ ·æ¥ç­›é€‰å’Œæå‡ç”¨äº RL åè®­ç»ƒçš„å®ä¾‹æ•´ä½“è´¨é‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ DeepSeek è’¸é¦æ¨¡å‹è¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒAdapThink åœ¨ä½¿æ¨¡å‹å…·å¤‡è‡ªé€‚åº”æ¨ç†æ¨¡å¼ã€ç¼“è§£æ¨ç†ä½æ•ˆé—®é¢˜ä¸Šå±•ç°å‡ºä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œå¯¹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ä»… 2K token çš„ DeepSeek è’¸é¦ Qwen æ¨¡å‹è¿›è¡Œåè®­ç»ƒåï¼Œåœ¨ 8K token é™åˆ¶ä¸‹æµ‹è¯•æ—¶ï¼Œç›¸æ¯”å¤šä¸ªé•¿åº¦æ§åˆ¶åŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œè¯æ˜äº†å…¶åœ¨æ‰“é€ å¼ºå¤§ä¸”é«˜æ•ˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è‡ªé€‚åº”æ€è·¯ï¼šæ‰“ç ´é™æ€è§„åˆ™æŸç¼šï¼Œä»æ¨¡å‹ç½®ä¿¡åº¦ã€å“åº”ç‰¹å¾ç­‰ç»´åº¦åŠ¨æ€è°ƒæ•´æ¨ç†åå¥½ï¼Œä¸ºå¤„ç†â€œæ¨¡å‹èƒ½åŠ›æ¼”è¿›ä¸‹çš„æ¨ç†æ•ˆç‡â€é—®é¢˜æä¾›äº†è‡ªé€‚åº”æ€è·¯å‚è€ƒã€‚  
2. ç»„ç›¸å¯¹ä¸å¤šæ ·æ€§æœºåˆ¶ï¼šç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°ä»ç¾¤ä½“æ ·æœ¬è§’åº¦ä¼˜åŒ–æ¨ç†åå¥½ï¼Œå¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·å…¼é¡¾å‡†ç¡®ç‡ä¸è¿‡ç¨‹å¤šæ ·æ€§ï¼Œè¿™ç±»ä»â€œç¾¤ä½“å±‚é¢ + å¤šç»´åº¦å¹³è¡¡â€çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»åˆ°éœ€è€ƒè™‘æ ·æœ¬å¤šæ ·æ€§ã€åŠ¨æ€è°ƒæ•´ç­–ç•¥çš„å…¶ä»– RL åè®­ç»ƒæˆ–ç”Ÿæˆä»»åŠ¡åœºæ™¯ä¸­ã€‚  
3. å°æ¨¡å‹é«˜æ•ˆæ¨ç†ï¼šåœ¨å°å‚æ•°è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚å®éªŒä¸­ DeepSeek è’¸é¦æ¨¡å‹ï¼‰ä¸ŠéªŒè¯äº†æå‡æ¨ç†æ•ˆç‡ä¸æ€§èƒ½çš„å¯è¡Œæ€§ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹æ‰“é€ é«˜æ•ˆæ¨ç†æ¨¡å‹æä¾›äº†å®è·µèŒƒä¾‹ã€‚

## reasoning-with-exploration--an-entropy-perspective-on-reinforcement-learning-for-llms
### Abstract
Balancing exploration and exploitation is a central goal in reinforcement
learning (RL). Despite recent advances in enhancing large language model (LLM)
reasoning, most methods lean toward exploitation, and increasingly encounter
performance plateaus. In this work, we revisit entropy -- a signal of
exploration in RL -- and examine its relationship to exploratory reasoning in
LLMs. Through empirical analysis, we uncover positive correlations between
high-entropy regions and three types of exploratory reasoning actions: (1)
pivotal tokens that determine or connect logical steps, (2) reflective actions
such as self-verification and correction, and (3) rare behaviors under-explored
by the base LLMs. Motivated by this, we introduce a minimal modification to
standard RL with only one line of code: augmenting the advantage function with
an entropy-based term. Unlike traditional maximum-entropy methods which
encourage exploration by promoting uncertainty, we encourage exploration by
promoting longer and deeper reasoning chains. Notably, our method achieves
significant gains on the Pass@K metric -- an upper-bound estimator of LLM
reasoning capabilities -- even when evaluated with extremely large K values,
pushing the boundaries of LLM reasoning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»ç†µè§†è§’æ¢ç´¢å¤§æ¨¡å‹æ¨ç†ï¼šå¼ºåŒ–å­¦ä¹ ä¸­å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„æ–°è·¯å¾„

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é‡Œï¼Œå¹³è¡¡æ¢ç´¢ï¼ˆexplorationï¼‰ä¸åˆ©ç”¨ï¼ˆexploitationï¼‰æ˜¯æ ¸å¿ƒç›®æ ‡ã€‚å½“ä¸‹æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å¤§å¤šåå‘â€œåˆ©ç”¨â€ï¼Œå®¹æ˜“é™·å…¥æ€§èƒ½ç“¶é¢ˆâ€”â€”æ¨¡å‹ä¼šæ”¶æ•›åˆ°ç‹­çª„ä¸”è¿‡åº¦ä¼˜åŒ–çš„è¡Œä¸ºï¼Œå¤±å»æ¢ç´¢å…¶ä»–ç­–ç•¥çš„åŠ¨åŠ›ï¼Œè¿›è€Œå‰Šå¼±å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ä¼ ç»ŸRLä¸­ï¼Œâ€œç†µâ€æ˜¯è¡¡é‡æ¢ç´¢çš„å…³é”®æŒ‡æ ‡ï¼ˆåæ˜ ç­–ç•¥åŠ¨ä½œåˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ï¼‰ï¼Œå—æ­¤å¯å‘ï¼Œè®ºæ–‡æ¢ç©¶äº†ç†µä¸LLMæ¢ç´¢æ€§æ¨ç†çš„å…³è”ï¼Œè¯•å›¾ç”¨ç†µæ¥å¼•å¯¼LLMæ›´é«˜æ•ˆåœ°æ¢ç´¢æ¨ç†è·¯å¾„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºç†µä¸æ¢ç´¢æ€§æ¨ç†çš„å¼ºå…³è”  
é€šè¿‡å®è¯åˆ†æå‘ç°ï¼Œé«˜ç†µåŒºåŸŸå’Œä¸‰ç±»æ¢ç´¢æ€§æ¨ç†è¡Œä¸ºé«˜åº¦ç›¸å…³ï¼š  
- å†³å®šæˆ–è¿æ¥é€»è¾‘æ­¥éª¤çš„**å…³é”®token**ï¼ˆå¦‚firstã€becauseã€howeverç­‰é€»è¾‘è¿æ¥è¯ï¼‰ï¼›  
- è‡ªæˆ‘éªŒè¯ã€çº é”™ç­‰**åæ€æ€§åŠ¨ä½œ**ï¼›  
- åŸºåº§LLMä¸­æœªå……åˆ†æ¢ç´¢çš„**ç¨€æœ‰è¡Œä¸º**ã€‚  
è¿™äº›å‘ç°è¯æ˜â€œç†µâ€å¯ä½œä¸ºè¯†åˆ«LLMæ¢ç´¢æ€§æ¨ç†è¡Œä¸ºçš„æœ‰æ•ˆä¿¡å·ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæç®€RLæ”¹è¿›ï¼šç†µè¾…åŠ©ä¼˜åŠ¿å‡½æ•°  
åŸºäºä¸Šè¿°å‘ç°ï¼Œè®ºæ–‡å¯¹æ ‡å‡†RLä»…åšâ€œä¸€è¡Œä»£ç çº§â€çš„æç®€ä¿®æ”¹â€”â€”**åœ¨ä¼˜åŠ¿å‡½æ•°ä¸­åŠ å…¥åŸºäºç†µçš„é¡¹**ã€‚å’Œä¼ ç»Ÿâ€œæœ€å¤§ç†µæ–¹æ³•é æå‡ä¸ç¡®å®šæ€§æ¥é¼“åŠ±æ¢ç´¢â€ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¿ƒè¿›â€œæ›´é•¿ã€æ›´æ·±çš„æ¨ç†é“¾â€æ¥é¼“åŠ±æ¢ç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œç»™PPO/GRPOç­‰ç®—æ³•çš„ä¼˜åŠ¿å‡½æ•°å¼•å…¥â€œæˆªæ–­ä¸”æ¢¯åº¦åˆ†ç¦»çš„ç†µé¡¹â€ï¼šæˆªæ–­ä¿è¯ç†µé¡¹ä¸ä¼šä¸»å¯¼æˆ–åè½¬åŸå§‹ä¼˜åŠ¿çš„ç¬¦å·ï¼Œæ¢¯åº¦åˆ†ç¦»åˆ™ä¿ç•™åŸå§‹ä¼˜åŒ–æ–¹å‘ã€‚è¿™æ ·æ—¢æ”¾å¤§äº†ä¸ç¡®å®šæ€§ä¸‹çš„æ¢ç´¢æ€§æ¨ç†è¡Œä¸ºï¼Œåˆç»´æŒäº†åŸç­–ç•¥æ¢¯åº¦æµï¼›ä¸”ç†µé¡¹ä¼šéšç½®ä¿¡åº¦è‡ªç„¶è¡°å‡ï¼Œè®­ç»ƒå‰æœŸé¼“åŠ±æ¢ç´¢ã€åæœŸé¿å…è¿‡åº¦æ¢ç´¢ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸»æµRLVRç®—æ³•ï¼ˆGRPOã€PPOï¼‰ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼š  
- å¢å¼ºæ¢ç´¢æ€§æ¨ç†è¡Œä¸ºï¼šå…³é”®tokenä½¿ç”¨ã€åæ€åŠ¨ä½œç­‰æ˜¾è‘—å¢åŠ ï¼›  
- ç”Ÿæˆæ›´é•¿æ›´å…·æ¢ç´¢æ€§çš„å›å¤ï¼Œä¸”ä¸æå‡é‡å¤ç‡ï¼Œæ”¯æŒè¿è´¯å¤šæ­¥æ¨ç†ï¼›  
- Pass@1ç²¾åº¦åœ¨ä¸åŒåŸºå‡†ä¸ŠæŒç»­æå‡ï¼›  
- è¯„ä¼°â€œPass@Kï¼ˆæ¨ç†èƒ½åŠ›çš„ä¸Šç•Œä¼°è®¡æŒ‡æ ‡ï¼‰â€æ—¶ï¼Œå³ä¾¿Kæå¤§ï¼Œæ–¹æ³•ä»å®ç°æ˜¾è‘—å¢ç›Šï¼Œçªç ´äº†LLMæ¨ç†çš„è¾¹ç•Œï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼Œç†µè¾…åŠ©çš„PPO/GRPOåœ¨å¤§Kå€¼ä¸‹Pass@Kè¡¨ç°è¿œè¶…åŸºçº¿ï¼‰ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **è§†è§’åˆ›æ–°**ï¼šæŠŠä¼ ç»ŸRLä¸­â€œç†µè¡¡é‡æ¢ç´¢â€çš„æ€è·¯è¿ç§»åˆ°LLMæ¨ç†åœºæ™¯ï¼Œå®è¯åˆ†æç†µä¸æ¨ç†è¡Œä¸ºçš„å…³è”ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†â€œç”¨RLä¸­æ¢ç´¢æŒ‡æ ‡èµ‹èƒ½LLMæ¨ç†â€çš„æ–°è§†è§’ï¼›  
2. **æç®€æœ‰æ•ˆ**ï¼šä»…éœ€ä¿®æ”¹ä¼˜åŠ¿å‡½æ•°è¿™ä¸€â€œä¸€è¡Œä»£ç çº§â€æ”¹åŠ¨ï¼Œå°±èƒ½æ— ç¼èå…¥ç°æœ‰RLVRè®­ç»ƒ pipelineï¼Œå·¥ç¨‹è½åœ°æˆæœ¬ä½ã€æ˜“å¤ç°ï¼›  
3. **æŒ‡æ ‡æ‹“å±•**ï¼šç”¨Pass@Kè¯„ä¼°æ¨ç†èƒ½åŠ›ä¸Šç•Œï¼Œä¸ºè¡¡é‡LLMå¤æ‚æ¨ç†æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„æµ‹è¯•ç»´åº¦ï¼Œä¹ŸéªŒè¯äº†æ–¹æ³•åœ¨â€œæé™æ¢ç´¢æ¨ç†â€åœºæ™¯çš„æ½œåŠ›ã€‚  


è¿™ç¯‡è®ºæ–‡ä»â€œç†µâ€è¿™ä¸€ç»å…¸RLæ¦‚å¿µåˆ‡å…¥ï¼Œä¸ºLLMæ¨ç†çš„â€œæ¢ç´¢-åˆ©ç”¨â€å¹³è¡¡éš¾é¢˜æä¾›äº†ç®€æ´ä¸”æœ‰æ•ˆçš„è§£æ³•ï¼Œæ— è®ºæ˜¯ç†è®ºå…³è”çš„æŒ–æ˜è¿˜æ˜¯å·¥ç¨‹å®ç°çš„è½»é‡åŒ–ï¼Œéƒ½ä¸ºå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡å¼€è¾Ÿäº†æ–°æ–¹å‘~

## ring-lite--scalable-reasoning-via-c3po-stabilized-reinforcement-learning-for-llms
### Abstract
We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model
optimized via reinforcement learning (RL) to achieve efficient and robust
reasoning capabilities. Built upon the publicly available Ling-lite model, a
16.8 billion parameter model with 2.75 billion activated parameters, our
approach matches the performance of state-of-the-art (SOTA) small-scale
reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,
GPQA-Diamond) while activating only one-third of the parameters required by
comparable models. To accomplish this, we introduce a joint training pipeline
integrating distillation with RL, revealing undocumented challenges in MoE RL
training. First, we identify optimization instability during RL training, and
we propose Constrained Contextual Computation Policy Optimization(C3PO), a
novel approach that enhances training stability and improves computational
throughput via algorithm-system co-design methodology. Second, we empirically
demonstrate that selecting distillation checkpoints based on entropy loss for
RL training, rather than validation metrics, yields superior
performance-efficiency trade-offs in subsequent RL training. Finally, we
develop a two-stage training paradigm to harmonize multi-domain data
integration, addressing domain conflicts that arise in training with mixed
dataset. We will release the model, dataset, and code.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Ring - liteï¼šåŸºäºC3POå¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå¯æ‰©å±•æ¨ç†MoEå¤§æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œä½†åƒOpenAI - O1ç³»åˆ—ç­‰æ¨¡å‹è®­ç»ƒç»†èŠ‚æœªå…¬å¼€ï¼Œå½¢æˆçŸ¥è¯†é¸¿æ²Ÿã€‚ç°æœ‰ç ”ç©¶å¤šèšç„¦ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•°å­¦æˆ–ä»£ç ç”Ÿæˆï¼‰ä¸”åœ¨å¯†é›†æ¨¡å‹æ¶æ„ï¼Œå¯¹æ··åˆä¸“å®¶ï¼ˆMoEï¼‰èŒƒå¼å…³æ³¨å°‘ï¼ŒMoEæ¶æ„ä¸‹RLè®­ç»ƒå­˜åœ¨ç¨³å®šæ€§éš¾é¢˜ï¼ˆå¦‚æ¢¯åº¦åŒæ­¥ã€å‚æ•°æ›´æ–°å†²çªè‡´è®­ç»ƒä¸ç¨³å®šï¼‰ï¼Œé™åˆ¶MoEä¼˜åŠ¿å‘æŒ¥ã€‚åŒæ—¶ï¼Œå¤šé¢†åŸŸæ•°æ®è®­ç»ƒæ˜“ç°å†²çªï¼Œç¼ºä¹æœ‰æ•ˆåº”å¯¹æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œå›¢é˜Ÿæ¨å‡ºRing - liteæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼€æºå¤šé¢†åŸŸMoEæ¨ç†æ¨¡å‹åŠå…¨æµç¨‹  
é¦–æ¬¡å¼€æºæ¶µç›–åŸºç¡€è®¾æ–½ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®é›†çš„å¤šé¢†åŸŸMoEæ¨ç†æ¨¡å‹ï¼Œè¯¦ç»†å…¬å¼€åŒ…æ‹¬Long - CoTç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ¨ç†ç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„é€æ˜è®­ç»ƒ pipelineï¼Œä¸ºç ”ç©¶ç¤¾åŒºæä¾›å…¨é¢èµ„æºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºC3POè§£å†³è®­ç»ƒä¸ç¨³å®š  
è¯†åˆ«æ¨ç†æ¨¡å‹è®­ç»ƒä¸ç¨³å®šé—®é¢˜åï¼Œæå‡ºConstrained Contextual Computation Policy Optimizationï¼ˆC3POï¼‰æ¡†æ¶ã€‚é€šè¿‡å›ºå®šè®­ç»ƒtokenå¤§å°ï¼ˆé¢„ç®—ï¼‰æ¶ˆé™¤å“åº”é•¿åº¦å·®å¼‚ï¼Œé€‰æ‹©é«˜ç†µåŸºæ¨¡å‹ç¨³å®šå­¦ä¹ åŠ¨æ€ï¼›è¯¥æ¡†æ¶åŸºäºç®—æ³• - å·¥ç¨‹ååŒè®¾è®¡èŒƒå¼æ•´åˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¿éšœé•¿æœŸè®­ç»ƒç¨³å®šåŒæ—¶æå‡è®¡ç®—æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸¤é˜¶æ®µè®­ç»ƒåº”å¯¹å¤šé¢†åŸŸæ•°æ®å†²çª  
åœ¨æ•°å­¦ã€ä»£ç å’Œç§‘å­¦ç­‰å¤šé¢†åŸŸå‘ç°åŸŸé—´æ•°æ®å†²çªï¼Œå¼•å…¥èƒ½åŠ›æ•´åˆæ–¹æ³•ï¼ˆé˜¶æ®µå¼è®­ç»ƒå’Œå¹³è¡¡æ•°æ®æ··åˆï¼‰æ¥è§£å†³å¤šé¢†åŸŸæ•°æ®è®­ç»ƒæ—¶çš„å†²çªé—®é¢˜ï¼Œå®ç°è·¨åŸŸèƒ½åŠ›èåˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåŸºäºç†µæŸå¤±é€‰è’¸é¦ checkpoint  
å®è¯è¡¨æ˜ï¼ŒRLè®­ç»ƒæ—¶ä¾æ®ç†µæŸå¤±è€ŒééªŒè¯æŒ‡æ ‡é€‰æ‹©è’¸é¦æ£€æŸ¥ç‚¹ï¼Œåœ¨åç»­RLè®­ç»ƒä¸­èƒ½äº§ç”Ÿæ›´ä¼˜çš„æ€§èƒ½ - æ•ˆç‡æƒè¡¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Ring - liteï¼ˆæ€»å‚æ•°168äº¿ï¼Œæ¿€æ´»å‚æ•°27.5äº¿ï¼‰åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼šæ•°å­¦ç«èµ›ç±»åŸºå‡†AIME2024å’ŒAIME2025åˆ†åˆ«è·76.61%å’Œ69.11%ï¼›ä»£ç ç«èµ›åŸºå‡†LiveCodeBenchå’ŒCodeforcesåˆ†åˆ«è·60.66%å’Œ86.45%ï¼›ç ”ç©¶ç”Ÿçº§ç§‘å­¦QAåŸºå‡†GPQA - diamondè·61.05%ã€‚åœ¨æ€§èƒ½ä¸ŠåŒ¹é…æˆ–è¶…è¶Š10Bå‚æ•°å†…çš„å¯†é›†æ¨¡å‹ï¼Œå¹³å‡è¡¨ç°è¶…è¿‡Qwen3 - 8Bï¼Œé€¼è¿‘å…¶ä»–é¡¶çº§æ¨ç†æ¨¡å‹ï¼Œä¸”ä»…æ¿€æ´»å¯æ¯”æ¨¡å‹ä¸‰åˆ†ä¹‹ä¸€å‚æ•°å°±èƒ½è¾¾åˆ°SOTAå°å°ºåº¦æ¨ç†æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¼€æºç”Ÿæ€æ„å»ºï¼šä¸ºAIç ”ç©¶ç¤¾åŒºæä¾›ä»æ¨¡å‹åˆ°æ•°æ®ã€ä»£ç çš„å…¨æ ˆå¼€æºèµ„æºï¼Œé™ä½å¤šé¢†åŸŸæ¨ç†ç ”ç©¶é—¨æ§›ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚
2. è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–ï¼šC3POæ¡†æ¶ä¸ºMoEæ¶æ„ä¸‹RLè®­ç»ƒç¨³å®šæ€§æä¾›æ–°æ€è·¯ï¼Œç®—æ³• - ç³»ç»ŸååŒè®¾è®¡æ€è·¯å¯å€Ÿé‰´åˆ°å…¶ä»–å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–åœºæ™¯ã€‚
3. å¤šåŸŸæ•°æ®å¤„ç†ï¼šä¸¤é˜¶æ®µè®­ç»ƒå’ŒåŸºäºç†µæŸå¤±é€‰ checkpoint ç­‰æ–¹æ³•ï¼Œä¸ºå¤šé¢†åŸŸæ•°æ®èåˆè®­ç»ƒã€è’¸é¦ä¸RLç»“åˆç­‰æä¾›å®è·µå‚è€ƒï¼ŒåŠ©åŠ›è§£å†³å¤æ‚åœºæ™¯ä¸‹æ¨¡å‹è®­ç»ƒéš¾é¢˜ã€‚

## unsupervised-skill-discovery-through-skill-regions-differentiation
### Abstract
Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors
that can accelerate the learning of downstream tasks. Previous methods
typically focus on entropy-based exploration or empowerment-driven skill
learning. However, entropy-based exploration struggles in large-scale state
spaces (e.g., images), and empowerment-based methods with Mutual Information
(MI) estimations have limitations in state exploration. To address these
challenges, we propose a novel skill discovery objective that maximizes the
deviation of the state density of one skill from the explored regions of other
skills, encouraging inter-skill state diversity similar to the initial MI
objective. For state-density estimation, we construct a novel conditional
autoencoder with soft modularization for different skill policies in
high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we
formulate an intrinsic reward based on the learned autoencoder that resembles
count-based exploration in a compact latent space. Through extensive
experiments in challenging state and image-based tasks, we find our method
learns meaningful skills and achieves superior performance in various
downstream tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— ç›‘ç£æŠ€èƒ½å‘ç°æ–°çªç ´ï¼šåŸºäºæŠ€èƒ½åŒºåŸŸåˆ†åŒ–çš„æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¯¸å¤šé¢†åŸŸå–å¾—æˆåŠŸï¼Œä½†ä¼ ç»ŸRLä¾èµ–ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œè®¾è®¡æˆæœ¬é«˜ä¸”æ³›åŒ–æ€§æœ‰é™ã€‚æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆUnsupervised RLï¼‰æ—¨åœ¨æ— å¤–éƒ¨å¥–åŠ±æ—¶å­¦ä¹ æœ‰ç”¨è¡Œä¸ºï¼Œä»¥å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•åˆ†åŸºäºèµ‹èƒ½ï¼ˆempowermentï¼‰çš„æŠ€èƒ½å‘ç°å’Œçº¯æ¢ç´¢æ–¹æ³•ï¼šåŸºäºèµ‹èƒ½æ–¹æ³•åœ¨å¤§è§„æ¨¡çŠ¶æ€ç©ºé—´ï¼ˆå¦‚å›¾åƒï¼‰ä¸­çŠ¶æ€è¦†ç›–æœ‰é™ï¼›çº¯æ¢ç´¢æ˜“äº§ç”Ÿæ— æ„ä¹‰è¡Œä¸ºï¼Œä¸”äº’ä¿¡æ¯ï¼ˆMIï¼‰å’Œç†µä¼°è®¡åœ¨å¤§è§„æ¨¡ç©ºé—´éš¾æ‰©å±•ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºæ–°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºæŠ€èƒ½çŠ¶æ€å¯†åº¦åå·®çš„æŠ€èƒ½å‘ç°ç›®æ ‡  
æå‡ºæœ€å¤§åŒ–å•ä¸ªæŠ€èƒ½çŠ¶æ€å¯†åº¦ä¸å…¶ä»–æŠ€èƒ½æ¢ç´¢åŒºåŸŸåå·®çš„ç›®æ ‡ï¼ˆISD3ï¼‰ï¼Œé¼“åŠ±æŠ€èƒ½é—´çŠ¶æ€å¤šæ ·æ€§ï¼Œç±»ä¼¼åˆå§‹MIç›®æ ‡ï¼Œè®©å„æŠ€èƒ½æ¢ç´¢åŒºåŸŸå°½å¯èƒ½ä¸åŒï¼Œå­¦ä¹ åˆ°æœ‰åŒºåˆ†åº¦çš„æŠ€èƒ½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¸¦è½¯æ¨¡å—åŒ–çš„æ¡ä»¶è‡ªåŠ¨ç¼–ç å™¨ç”¨äºçŠ¶æ€å¯†åº¦ä¼°è®¡  
åœ¨é«˜ç»´ç©ºé—´ä¸ºä¸åŒæŠ€èƒ½ç­–ç•¥æ„å»ºæ¡ä»¶è‡ªåŠ¨ç¼–ç å™¨ï¼Œé‡‡ç”¨è½¯æ¨¡å—åŒ–ï¼Œä½¿æŠ€èƒ½æ¡ä»¶ç½‘ç»œæŒ‰æŠ€èƒ½å†³å®šçš„è·¯ç”±ç½‘ç»œæˆä¸ºå…±äº«æ¨¡å—çš„åŠ æƒç»„åˆï¼Œç¨³å®šä¼°è®¡å·®å¼‚æ˜æ˜¾æŠ€èƒ½çš„çŠ¶æ€å¯†åº¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æŠ€èƒ½å†…æ¢ç´¢å†…åœ¨å¥–åŠ±  
åˆ©ç”¨å­¦ä¹ åˆ°çš„è‡ªåŠ¨ç¼–ç å™¨æ½œåœ¨ç©ºé—´ï¼Œæ„é€ ç±»ä¼¼è¡¨æ ¼å‹è®¡æ•°æ¢ç´¢çš„å†…åœ¨å¥–åŠ±ï¼Œæ¿€åŠ±å•ä¸ªæŠ€èƒ½å†…çš„æ¢ç´¢ï¼Œä¸”å¯æ‰©å±•åˆ°å¤§è§„æ¨¡é—®é¢˜ï¼Œç†è®ºä¸Šä¸è¡¨æ ¼å‹é«˜æ•ˆè®¡æ•°æ¢ç´¢ç›¸å…³ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Mazeã€åŸºäºçŠ¶æ€çš„æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ åŸºå‡†ï¼ˆURLBï¼‰å’Œå…·æŒ‘æˆ˜æ€§çš„åŸºäºå›¾åƒçš„URLBç¯å¢ƒä¸­å¼€å±•å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•ï¼ˆSD3ï¼‰èƒ½å­¦ä¹ åˆ°å…·æ¢ç´¢æ€§å’Œå¤šæ ·æ€§çš„æŠ€èƒ½ï¼Œåœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œä¸”åœ¨åŸºäºå›¾åƒçš„URLBä»»åŠ¡ä¸­å±•ç°å‡ºå¯æ‰©å±•æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»çŠ¶æ€å¯†åº¦åå·®è§’åº¦è®¾è®¡æŠ€èƒ½å‘ç°ç›®æ ‡ï¼Œä¸ºå­¦ä¹ ä¸åŒçŠ¶æ€å ç”¨çš„å¤šæ ·æŠ€èƒ½æä¾›ç›´æ¥æ€è·¯ï¼Œå¯å‘åç»­æ— ç›‘ç£RLä¸­æŠ€èƒ½åˆ†åŒ–æ–¹å‘çš„ç ”ç©¶ã€‚  
2. å¸¦è½¯æ¨¡å—åŒ–çš„æ¡ä»¶è‡ªåŠ¨ç¼–ç å™¨è®¾è®¡ï¼Œä¸ºé«˜ç»´ç©ºé—´å¤šæŠ€èƒ½çŠ¶æ€å¯†åº¦ç¨³å®šä¼°è®¡æä¾›æ–°èŒƒå¼ï¼Œå¯å€Ÿé‰´åˆ°éœ€å¯¹å¤šç±»åˆ«/å¤šç­–ç•¥è¿›è¡Œå¯†åº¦ä¼°è®¡çš„åœºæ™¯ã€‚  
3. åŸºäºè‡ªåŠ¨ç¼–ç å™¨æ½œåœ¨ç©ºé—´æ„é€ å†…åœ¨å¥–åŠ±ï¼Œå°†è¡¨æ ¼å‹è®¡æ•°æ¢ç´¢æ€è·¯æ‹“å±•åˆ°é«˜ç»´è¿ç»­ç©ºé—´ï¼Œä¸ºè§£å†³å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„æ¢ç´¢é—®é¢˜æä¾›å‚è€ƒã€‚  

## staq-it!-growing-neural-networks-for-policy-mirror-descent
### Abstract
In Reinforcement Learning (RL), regularization has emerged as a popular tool
both in theory and practice, typically based either on an entropy bonus or a
Kullback-Leibler divergence that constrains successive policies. In practice,
these approaches have been shown to improve exploration, robustness and
stability, giving rise to popular Deep RL algorithms such as SAC and TRPO.
Policy Mirror Descent (PMD) is a theoretical framework that solves this general
regularized policy optimization problem, however the closed-form solution
involves the sum of all past Q-functions, which is intractable in practice. We
propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions
in memory, and show that for finite and large enough $M$, a convergent
algorithm can be derived, introducing no error in the policy update, unlike
prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong
theoretical guarantees and is competitive with deep RL baselines, while
exhibiting less performance oscillation, paving the way for fully stable deep
RL algorithms and providing a testbed for experimentation with Policy Mirror
Descent.
### ğŸŒŸ è®ºæ–‡è§£è¯» | StaQï¼šä¸ºç­–ç•¥é•œåƒä¸‹é™æ‰“é€ å¯æ‰©å±•ç¥ç»ç½‘ç»œï¼Œè¿ˆå‘ç¨³å®šæ·±åº¦å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å†³ç­–ä»»åŠ¡ä¸­å–å¾—å“è¶Šæˆæœï¼Œä½†ç¥ç»ç½‘ç»œä½œä¸ºå‡½æ•°è¿‘ä¼¼å™¨åŠ å‰§äº†æ¢ç´¢éš¾ã€å¯¹è¶…å‚æ•°æ•æ„Ÿç­‰é—®é¢˜ï¼Œä¸”å®è¯è¡¨ç°å¸¸ä¸ç†è®ºç†è§£è„±èŠ‚ã€‚æ­£åˆ™åŒ–æ˜¯è§£å†³è¿™äº›é—®é¢˜çš„å¸¸ç”¨æ‰‹æ®µï¼Œå¦‚åŸºäºç†µå¥–åŠ±çš„SACã€åŸºäºKLæ•£åº¦çº¦æŸçš„TRPOç­‰ã€‚ç­–ç•¥é•œåƒä¸‹é™ï¼ˆPMDï¼‰æ˜¯è§£å†³æ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–çš„ç†è®ºæ¡†æ¶ï¼Œä½†å…¶é—­å¼è§£éœ€æ‰€æœ‰å†å²Qå‡½æ•°ä¹‹å’Œï¼Œåœ¨å®è·µä¸­å› å­˜å‚¨å’Œè®¡ç®—ä¸å¯è¡Œè€Œéš¾ä»¥è½åœ°â€”â€”æ­¤å‰æ·±åº¦RLä¸­PMDå®ç°å¤šé‡‡ç”¨è¿‘ä¼¼æ–¹æ³•ï¼Œä¼šå¼•å…¥é¢å¤–è¯¯å·®ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è®¾è®¡**æœ‰é™è®°å¿†ä¸”ç†è®ºæ”¶æ•›**çš„PMDç±»ç®—æ³•ï¼Œå¹³è¡¡ç†è®ºä¿è¯ä¸å®é™…å¯å®ç°æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ‰é™è®°å¿†çš„PMDç±»ç®—æ³•ç†è®ºåˆ†æ  
æå‡ºä»…å­˜å‚¨æœ€è¿‘Mä¸ªQå‡½æ•°çš„PMDç±»ç®—æ³•ï¼Œè¯æ˜å½“Mè¶³å¤Ÿå¤§æ—¶ï¼Œç”¨â€œç•¥ä½œä¿®æ”¹çš„ç­–ç•¥$\tilde{\pi}_k$ï¼ˆåˆ é™¤æœ€æ—§Qå‡½æ•°åçš„ç­–ç•¥ï¼‰â€æ›¿ä»£åŸPMDä¸­éœ€å…¨éƒ¨å†å²Qå‡½æ•°çš„ç­–ç•¥$\pi_k$ï¼Œç®—æ³•ä»èƒ½æ¸è¿‘æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥$\pi^\star$ã€‚è¿™ä¸ºæœ‰é™è®°å¿†ä¸‹PMDçš„æ”¶æ•›æ€§æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œå¡«è¡¥äº†æ­¤å‰ä»…å®éªŒéªŒè¯ï¼ˆå¦‚å­˜å‚¨10ä¸ªQå‡½æ•°ï¼‰ä½†æ— ç†è®ºè¯æ˜çš„ç©ºç™½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆå¹¶è¡Œçš„Qå‡½æ•°å †å è®¡ç®—  
é€šè¿‡â€œæ‰¹å¤„ç†Qå‡½æ•°â€ï¼Œåœ¨GPUä¸Šé«˜æ•ˆå¹¶è¡Œè®¡ç®—Qå‡½æ•°çš„å®Œæ•´â€œå †å â€ï¼ˆstackï¼‰ã€‚è¿™è®©æœ‰é™è®°å¿†ä¸‹å­˜å‚¨Mä¸ªQå‡½æ•°å¹¶ç”¨äºç­–ç•¥æ›´æ–°çš„è¿‡ç¨‹æ›´é«˜æ•ˆï¼Œé™ä½äº†å·¥ç¨‹å®ç°éš¾åº¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šStaQç®—æ³•è®¾è®¡ä¸æ— è¯¯å·®ç­–ç•¥æ›´æ–°  
åŸºäºä¸Šè¿°æœ‰é™è®°å¿†PMDæ¡†æ¶ï¼Œæå‡ºStaQç®—æ³•ã€‚å…¶ç­–ç•¥æ›´æ–°ä¸º**æ— ä¼˜åŒ–æ­¥éª¤**çš„é—­å¼æ›´æ–°ï¼ˆåŸºäºç†µæ­£åˆ™åŒ–çš„ç­–ç•¥é•œåƒä¸‹é™ï¼‰ï¼Œä¸æ­¤å‰TRPOã€MDPOç­‰ä¾èµ–è¿‘ä¼¼æ¢¯åº¦æ›´æ–°ï¼ˆå¼•å…¥é¢å¤–è¯¯å·®ï¼‰ä¸åŒï¼ŒStaQçš„ç­–ç•¥æ›´æ–°åœ¨ç†è®ºä¸Šâ€œæ— è¯¯å·®â€ï¼Œä»…ç­–ç•¥è¯„ä¼°å’Œè¡Œä¸ºç­–ç•¥é€‰æ‹©å¯èƒ½æˆä¸ºä¸ç¨³å®šæºï¼Œä¸ºæ·±åº¦RLçš„ç¨³å®šæ€§æä¾›äº†æ–°è·¯å¾„ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MuJoCoï¼ˆç¦»æ•£åŠ¨ä½œåœºæ™¯ï¼‰å’ŒMinAtarç­‰å¤šç±»ç¯å¢ƒä¸­ï¼ŒStaQä¸æ·±åº¦RLåŸºçº¿ç®—æ³•ï¼ˆå¦‚SACã€TRPOç­‰ï¼‰ç›¸æ¯”**ç«äº‰åŠ›ç›¸å½“**ï¼Œä¸”**å­¦ä¹ è¿‡ç¨‹éœ‡è¡æ›´å°**ã€‚è¿™éªŒè¯äº†StaQåœ¨å®é™…ä»»åŠ¡ä¸­æ—¢èƒ½ä¿æŒæ€§èƒ½ï¼Œåˆèƒ½æå‡è®­ç»ƒç¨³å®šæ€§ï¼Œå‘â€œå®Œå…¨ç¨³å®šçš„æ·±åº¦RLç®—æ³•â€æ›´è¿‘ä¸€æ­¥ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºä¸å·¥ç¨‹çš„å¹³è¡¡**ï¼šé€šè¿‡æœ‰é™è®°å¿†å‡è®¾+ç†è®ºæ”¶æ•›è¯æ˜ï¼Œå°†PMDä»â€œç†è®ºæ¡†æ¶â€è½åœ°ä¸ºâ€œå¯å®ç°ç®—æ³•â€ï¼Œä¸ºå…¶ä»–éœ€å†å²ä¿¡æ¯çš„RLæ¡†æ¶ï¼ˆå¦‚å€¼è¿­ä»£ã€ç­–ç•¥è¿­ä»£çš„æ­£åˆ™åŒ–å˜ä½“ï¼‰æä¾›â€œæœ‰é™è®°å¿†+æ”¶æ•›æ€§ä¿éšœâ€çš„è®¾è®¡æ€è·¯ã€‚  
2. **ç¨³å®šè®­ç»ƒçš„æ–°æ€è·¯**ï¼šStaQçš„â€œæ— ä¼˜åŒ–é—­å¼ç­–ç•¥æ›´æ–°â€å’Œâ€œQå‡½æ•°å †å â€æœºåˆ¶ï¼Œå±•ç¤ºäº†â€œé€šè¿‡ç»“æ„åŒ–å­˜å‚¨å†å²ä¿¡æ¯+ç†è®ºé©±åŠ¨çš„æ›´æ–°è§„åˆ™â€æ¥é™ä½è®­ç»ƒéœ‡è¡çš„æ½œåŠ›ï¼Œå¯å¯å‘åç»­ç¨³å®šRLç®—æ³•è®¾è®¡ã€‚  
3. **ç¡¬ä»¶å‹å¥½çš„å¹¶è¡Œè®¡ç®—**ï¼šåˆ©ç”¨GPUå¹¶è¡Œæ‰¹å¤„ç†Qå‡½æ•°ï¼Œä¸ºæ·±åº¦RLä¸­å¤šç½‘ç»œ/å¤šå†å²ä¿¡æ¯çš„é«˜æ•ˆåˆ©ç”¨æä¾›äº†å·¥ç¨‹å®è·µå‚è€ƒï¼Œå°¤å…¶åœ¨éœ€å­˜å‚¨å¤šä¸ªå‡½æ•°è¿‘ä¼¼å™¨ï¼ˆå¦‚å¤šQç½‘ç»œã€å¤šç­–ç•¥ç½‘ç»œï¼‰çš„åœºæ™¯ä¸­ï¼Œå¯å€Ÿé‰´æ­¤ç±»å¹¶è¡ŒåŒ–æ€è·¯ã€‚  


StaQä¸ä»…æ¨è¿›äº†PMDæ¡†æ¶çš„å®ç”¨åŒ–ï¼Œæ›´åœ¨â€œç¨³å®šæ·±åº¦RLç®—æ³•â€è¿™ä¸€é•¿æœŸç›®æ ‡ä¸Šè¿ˆå‡ºå…³é”®ä¸€æ­¥ï¼Œä¸ºç†è®ºç ”ç©¶ä¸å·¥ç¨‹å®ç°æ¶èµ·æ¡¥æ¢ï¼Œå€¼å¾—å¼ºåŒ–å­¦ä¹ é¢†åŸŸç ”ç©¶è€…ä¸å®è·µè€…å…³æ³¨ã€‚

## acereason-nemotron-1-1--advancing-math-and-code-reasoning-through-sft-and-rl-synergy
### Abstract
In this work, we investigate the synergy between supervised fine-tuning (SFT)
and reinforcement learning (RL) in developing strong reasoning models. We begin
by curating the SFT training data through two scaling strategies: increasing
the number of collected prompts and the number of generated responses per
prompt. Both approaches yield notable improvements in reasoning performance,
with scaling the number of prompts resulting in more substantial gains. We then
explore the following questions regarding the synergy between SFT and RL: (i)
Does a stronger SFT model consistently lead to better final performance after
large-scale RL training? (ii) How can we determine an appropriate sampling
temperature during RL training to effectively balance exploration and
exploitation for a given SFT initialization? Our findings suggest that (i)
holds true, provided effective RL training is conducted, particularly when the
sampling temperature is carefully chosen to maintain the temperature-adjusted
entropy around 0.3, a setting that strikes a good balance between exploration
and exploitation. Notably, the performance gap between initial SFT models
narrows significantly throughout the RL process. Leveraging a strong SFT
foundation and insights into the synergistic interplay between SFT and RL, our
AceReason-Nemotron-1.1 7B model significantly outperforms
AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among
Qwen2.5-7B-based reasoning models on challenging math and code benchmarks,
thereby demonstrating the effectiveness of our post-training recipe. We release
the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B
### ğŸŒŸ è®ºæ–‡è§£è¯» | AceReason-Nemotron 1.1ï¼šSFTä¸RLååŒæ¨è¿›æ•°å­¦å’Œä»£ç æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦ä¸ä»£ç æ¨ç†ç ”ç©¶é¢†åŸŸï¼Œå°½ç®¡æ­¤å‰å·¥ä½œåœ¨çŸ­é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ä¸Šå–å¾—è¿›å±•ï¼Œä½†å¯¹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ååŒä½œç”¨çš„ç³»ç»Ÿæ€§ç ”ç©¶ä»è¾ƒç¼ºä¹ã€‚åŒæ—¶ï¼Œå¦‚ä½•è®©ä¸­å°è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚7Bé‡çº§ï¼‰é€šè¿‡SFTä¸RLç»“åˆå®ç°æ¨ç†èƒ½åŠ›è·ƒå‡ï¼Œä¹Ÿæ˜¯å¾…æ·±å…¥æ¢ç´¢çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œå¤§æ¨¡å‹é•¿é“¾å¼æ€ç»´æ¨ç†å¤šä¾èµ–å¤§è§„æ¨¡RLï¼Œåç»­å·¥ä½œåœ¨ä¸­å°æ¨¡å‹ä¸Šå¤ç°è¿™ç±»æˆåŠŸæ—¶ï¼Œç¼ºå°‘å¯¹SFTä¸RLè®­ç»ƒåŠ¨æ€å’ŒååŒæ€§çš„å…¨é¢åˆ†æï¼Œå› æ­¤æœ¬æ–‡è‡´åŠ›äºå¡«è¡¥è¿™äº›ç ”ç©¶ç©ºç™½ï¼Œæ¢ç´¢æ„å»ºé¡¶å°–æ¨ç†æ¨¡å‹çš„åè®­ç»ƒæŠ€æœ¯å…¨æ™¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSFTæ•°æ®ä¸è®­ç»ƒçš„è§„æ¨¡åŒ–ç­–ç•¥  
é€šè¿‡ä¸¤ç§ç»´åº¦è§„æ¨¡åŒ–SFTè®­ç»ƒæ•°æ®ï¼šä¸€æ˜¯å¢åŠ æ”¶é›†çš„promptæ•°é‡ï¼ŒäºŒæ˜¯å¢åŠ æ¯ä¸ªpromptä¸‹ç”Ÿæˆçš„å“åº”æ•°é‡ï¼›åŒæ—¶å¢åŠ è®­ç»ƒè½®æ¬¡ã€‚å‘ç°ä¸¤ç±»æ•°æ®è§„æ¨¡åŒ–éƒ½èƒ½æå‡æ¨ç†æ€§èƒ½ï¼Œä¸”å¢åŠ promptæ•°é‡å¸¦æ¥çš„å¢ç›Šæ›´æ˜¾è‘—ï¼›è®­ç»ƒè½®æ¬¡ä¸Šï¼Œä»ç¬¬1åˆ°ç¬¬5è½®æ€§èƒ½æŒç»­æå‡ï¼Œ5 - 6è½®åè¶‹äºå¹³ç¨³ï¼Œä¸€å®šç¨‹åº¦â€œè¿‡æ‹Ÿåˆâ€åè€Œå› æš´éœ²åå·®ç­‰å› ç´ æå‡é•¿CoTç”Ÿæˆçš„æµ‹è¯•å‡†ç¡®ç‡ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSFTä¸RLååŒçš„å…³é”®æ¢ç´¢  
ä»ä¸åŒå¼ºåº¦SFTæ¨¡å‹å¯åŠ¨RLè®­ç»ƒï¼ŒéªŒè¯å‡ºæ›´å¼ºSFTæ¨¡å‹ç»å¤§è§„æ¨¡RLåä»èƒ½äº§å‡ºæ›´ä¼˜ç»“æœï¼ˆè™½è®­ç»ƒä¸­æ€§èƒ½å·®è·ç¼©å°ï¼‰ï¼›é’ˆå¯¹ç»™å®šSFTåˆå§‹åŒ–ï¼Œæå‡ºRLè®­ç»ƒæ—¶é‡‡æ ·æ¸©åº¦çš„è®¾ç½®å‡†åˆ™â€”â€”è®©æ¸©åº¦è°ƒæ•´åçš„ç†µç»´æŒåœ¨0.3å·¦å³ï¼Œä»¥æ­¤å¹³è¡¡æ¢ç´¢ï¼ˆexplorationï¼‰ä¸åˆ©ç”¨ï¼ˆexploitationï¼‰ï¼Œä¿éšœRLè®­ç»ƒæœ‰æ•ˆæ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRLè®­ç»ƒä¸­è¶…é•¿å“åº”çš„å¤„ç†ç­–ç•¥  
ç³»ç»Ÿç ”ç©¶å½“å“åº”é•¿åº¦æœªè¾¾é¢„æœŸï¼ˆå¦‚24K tokenså†…æ²¡äº§å‡ºæœ€ç»ˆç­”æ¡ˆï¼‰æ—¶ï¼Œæ˜¯ç»™è´Ÿå¥–åŠ±è¿˜æ˜¯å±è”½æ ·æœ¬ï¼ˆè¶…é•¿è¿‡æ»¤ï¼‰ã€‚å‘ç°çŸ­tokené™åˆ¶ï¼ˆå¦‚8Kã€16Kï¼‰ä¸‹è¶…é•¿è¿‡æ»¤æœ‰æ˜æ˜¾æ”¶ç›Šï¼›ä½†24Kæ—¶ä¼˜åŠ¿å‡å¼±ï¼Œ32Kæ—¶ç”šè‡³å¯èƒ½æŸå®³æ€§èƒ½ï¼Œä¸ºä¸åŒåœºæ™¯ä¸‹RLè®­ç»ƒçš„æ ·æœ¬è¿‡æ»¤æä¾›å†³ç­–ä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ†é˜¶æ®µRLæ–¹æ³•çš„æ™®é€‚æ€§éªŒè¯  
éªŒè¯äº†æ­¤å‰é’ˆå¯¹â€œä»…æ•°å­¦â€â€œä»…ä»£ç â€promptçš„åˆ†é˜¶æ®µRLæ–¹æ³•ï¼Œåœ¨è¿œè¶…DeepSeek - R1 - Distill - Qwenç³»åˆ—çš„æ›´å¼ºSFTæ¨¡å‹ä¸Šä¾æ—§æœ‰æ•ˆï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒå¼ºåº¦SFTåŸºç¡€ä¸Šçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AIME 2024/2025ã€HMMT 2025ã€LiveCodeBench v5/v6ç­‰æ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAceReason - Nemotron - 1.1 7Bæ¨¡å‹æ˜¾è‘—è¶…è¶Šå‰ä»£AceReason - Nemotron - 1.0ï¼Œä¸”åœ¨åŸºäºQwen2.5 - 7Bçš„æ¨ç†æ¨¡å‹ä¸­è¾¾åˆ°å…¨æ–°SOTAã€‚ä¾‹å¦‚åœ¨AIME 2024ï¼ˆPass@1ï¼‰ç­‰å¤šé¡¹ä»»åŠ¡é‡Œï¼Œå‡†ç¡®ç‡ç›¸è¾ƒç«å“æœ‰æ˜æ˜¾æå‡ï¼Œæœ‰åŠ›è¯æ˜SFTä¸RLååŒåè®­ç»ƒæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®è§„æ¨¡åŒ–æ€è·¯ï¼šåœ¨SFTé˜¶æ®µï¼Œä»promptæ•°é‡ã€å•promptå“åº”æ•°ç­‰ç»´åº¦æ‰©å……æ•°æ®ï¼Œä¸ºæå‡æ¨ç†æ€§èƒ½æä¾›äº†å¯æ“ä½œçš„æ•°æ®é›†æ„å»ºæ–¹å‘ï¼›  
2. SFTä¸RLååŒèŒƒå¼ï¼šæ˜ç¡®æ›´å¼ºSFTåŸºç¡€å¯¹RLåæ€§èƒ½çš„æ­£å‘å½±å“ï¼Œä»¥åŠæ¸©åº¦ - ç†µè°ƒæ§åœ¨RLå¹³è¡¡æ¢ç´¢åˆ©ç”¨ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºåç»­æ¨¡å‹è®­ç»ƒæµç¨‹è®¾è®¡æä¾›æŒ‡å¯¼ï¼›  
3. RLç»†èŠ‚ä¼˜åŒ–ï¼šè¶…é•¿å“åº”å¤„ç†ç­–ç•¥éštokené¢„ç®—å˜åŒ–çš„è§„å¾‹ï¼Œèƒ½è¾…åŠ©ä»ä¸šè€…åœ¨ä¸åŒä»»åŠ¡çº¦æŸä¸‹é€‰æ‹©æ›´ä¼˜çš„è®­ç»ƒæ ·æœ¬è¿‡æ»¤æ–¹å¼ï¼›  
4. åˆ†é˜¶æ®µRLæ™®é€‚æ€§ï¼šéªŒè¯äº†ç‰¹å®šåˆ†é˜¶æ®µRLæ–¹æ³•åœ¨ä¸åŒå¼ºåº¦SFTæ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºè¡Œä¸šå†…å¤ç”¨ç±»ä¼¼è®­ç»ƒèŒƒå¼æä¾›ä¿¡å¿ƒä¸å‚è€ƒã€‚

## research-on-optimal-control-problem-based-on-reinforcement-learning-under-knightian-uncertainty
### Abstract
Considering that the decision-making environment faced by reinforcement
learning (RL) agents is full of Knightian uncertainty, this paper describes the
exploratory state dynamics equation in Knightian uncertainty to study the
entropy-regularized relaxed stochastic control problem in a Knightian
uncertainty environment. By employing stochastic analysis theory and the
dynamic programming principle under nonlinear expectation, we derive the
Hamilton-Jacobi-Bellman (HJB) equation and solve for the optimal policy that
achieves a trade-off between exploration and exploitation. Subsequently, for
the linear-quadratic (LQ) case, we examine the agent's optimal randomized
feedback control under both state-dependent and state-independent reward
scenarios, proving that the optimal randomized feedback control follows a
Gaussian distribution in the LQ framework. Furthermore, we investigate how the
degree of Knightian uncertainty affects the variance of the optimal feedback
policy. Additionally, we establish the solvability equivalence between
non-exploratory and exploratory LQ problems under Knightian uncertainty and
analyze the associated exploration cost. Finally, we provide an LQ example and
validate the theoretical findings through numerical simulations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¥ˆç‰¹ä¸ç¡®å®šæ€§ä¸‹åŸºäºå¼ºåŒ–å­¦ä¹ çš„æœ€ä¼˜æ§åˆ¶é—®é¢˜ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æœ€ä¼˜æ§åˆ¶åœ¨å·¥ç¨‹ã€ç»æµç­‰å¤šé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä¼ ç»Ÿæ–¹æ³•é¢å¯¹é«˜ç»´ã€éçº¿æ€§æˆ–ç¯å¢ƒä¸ç¡®å®šåœºæ™¯å­˜åœ¨è®¡ç®—å¤æ‚ã€å»ºæ¨¡éš¾ç­‰æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæ•°æ®é©±åŠ¨æ–¹æ³•ä¸ºå¤æ‚æœ€ä¼˜æ§åˆ¶æä¾›æ–°æ€è·¯ï¼Œä½†ç°æœ‰ç ”ç©¶å­˜åœ¨å±€é™ï¼šä¸€æ˜¯å¤šèšç„¦ç¦»æ•£æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¯¹è¿ç»­æ—¶é—´å’Œç©ºé—´é—®é¢˜å…³æ³¨å°‘ï¼Œè€Œç°å®ä¸­å¦‚é«˜é¢‘äº¤æ˜“ç­‰åœºæ™¯éœ€è¿ç»­äº¤äº’ï¼Œç¦»æ•£æ–¹æ³•å¯¹æ—¶é—´ç¦»æ•£åŒ–æ•æ„Ÿï¼›äºŒæ˜¯è¿ç»­è®¾ç½®ä¸‹RLç ”ç©¶å¤šé™äºç¡®å®šæ€§ç³»ç»Ÿï¼Œå¯¹å«ç¯å¢ƒå™ªå£°çš„éšæœºç³»ç»ŸåŠæ¨¡å‹æœ¬èº«çš„å¥ˆç‰¹ä¸ç¡®å®šæ€§è€ƒè™‘ä¸è¶³ã€‚å¥ˆç‰¹ä¸ç¡®å®šæ€§æŒ‡æ¦‚ç‡ç»Ÿè®¡æ¨¡å‹å›ºæœ‰ä¸”ä¸å¯çº¦çš„ä¸ç¡®å®šæ€§ï¼Œç»å…¸æ¦‚ç‡è®ºéš¾ä»¥åº”å¯¹æœªçŸ¥æ¦‚ç‡æ¨¡å‹åœºæ™¯ï¼Œéçº¿æ€§æœŸæœ›å¯åœ¨æ¨¡å‹ä¸ç¡®å®šæ—¶è¿›è¡Œé£é™©åˆ†æï¼Œå½“å‰ä¼—å¤šå­¦è€…èšç„¦å¥ˆç‰¹ä¸ç¡®å®šæ€§ä¸‹é‡‘èç»æµé—®é¢˜ï¼Œè€ŒRLç ”ç©¶ä¸­å¯¹å…¶å…³æ³¨è¾ƒå°‘ï¼Œæœ¬æ–‡å°±æ­¤å±•å¼€ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé’ˆå¯¹å¥ˆç‰¹ä¸ç¡®å®šæ€§ä¸‹å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å†³ç­–ç¯å¢ƒï¼Œåˆ»ç”»æ¢ç´¢æ€§çŠ¶æ€åŠ¨åŠ›å­¦æ–¹ç¨‹ï¼Œç ”ç©¶ç†µæ­£åˆ™åŒ–çš„æ¾å¼›éšæœºæ§åˆ¶é—®é¢˜ã€‚è¿ç”¨éšæœºåˆ†æç†è®ºä¸éçº¿æ€§æœŸæœ›ä¸‹åŠ¨æ€è§„åˆ’åŸç†ï¼Œæ¨å¯¼å“ˆå¯†é¡¿ - é›…å¯æ¯” - è´å°”æ›¼ï¼ˆHJBï¼‰æ–¹ç¨‹ï¼Œæ±‚è§£åœ¨æ¢ç´¢ä¸åˆ©ç”¨é—´æƒè¡¡çš„æœ€ä¼˜ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨çº¿æ€§ - äºŒæ¬¡ï¼ˆLQï¼‰æƒ…å½¢ä¸‹ï¼Œæ£€éªŒçŠ¶æ€ç›¸å…³å’ŒçŠ¶æ€æ— å…³å¥–åŠ±åœºæ™¯ä¸‹æ™ºèƒ½ä½“çš„æœ€ä¼˜éšæœºåé¦ˆæ§åˆ¶ï¼Œè¯æ˜LQæ¡†æ¶ä¸‹æœ€ä¼˜éšæœºåé¦ˆæ§åˆ¶æœä»é«˜æ–¯åˆ†å¸ƒï¼›æ¢ç©¶å¥ˆç‰¹ä¸ç¡®å®šæ€§ç¨‹åº¦å¯¹æœ€ä¼˜åé¦ˆç­–ç•¥æ–¹å·®çš„å½±å“ï¼›å»ºç«‹å¥ˆç‰¹ä¸ç¡®å®šæ€§ä¸‹éæ¢ç´¢æ€§ä¸æ¢ç´¢æ€§LQé—®é¢˜çš„å¯è§£æ€§ç­‰ä»·å…³ç³»å¹¶åˆ†ææ¢ç´¢æˆæœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æä¾›LQç¤ºä¾‹å¹¶é€šè¿‡æ•°å€¼æ¨¡æ‹ŸéªŒè¯ç†è®ºå‘ç°ï¼ŒéªŒè¯äº†åœ¨LQæ¡†æ¶ä¸‹å…³äºæœ€ä¼˜éšæœºåé¦ˆæ§åˆ¶åˆ†å¸ƒã€å¥ˆç‰¹ä¸ç¡®å®šæ€§å¯¹ç­–ç•¥æ–¹å·®å½±å“ç­‰ç†è®ºç»“è®ºçš„æ­£ç¡®æ€§ï¼Œä¹ŸéªŒè¯äº†éæ¢ç´¢æ€§ä¸æ¢ç´¢æ€§LQé—®é¢˜å¯è§£æ€§ç­‰ä»·ç­‰åˆ†æçš„åˆç†æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºè¿ç»­æ—¶é—´ç©ºé—´ä¸‹ã€å«å¥ˆç‰¹ä¸ç¡®å®šæ€§çš„å¼ºåŒ–å­¦ä¹ æœ€ä¼˜æ§åˆ¶é—®é¢˜æä¾›äº†ç†è®ºä¸æ–¹æ³•å‚è€ƒã€‚åœ¨å¤„ç†å¤æ‚ç¯å¢ƒï¼ˆå¦‚é«˜é¢‘äº¤æ˜“ã€è‡ªåŠ¨é©¾é©¶ç­‰è¿ç»­äº¤äº’ä¸”æ¨¡å‹å­˜åœ¨ä¸ç¡®å®šæ€§çš„åœºæ™¯ï¼‰çš„æœ€ä¼˜æ§åˆ¶é—®é¢˜æ—¶ï¼Œå…¶åˆ»ç”»æ¢ç´¢æ€§çŠ¶æ€åŠ¨åŠ›å­¦æ–¹ç¨‹ã€åˆ©ç”¨éçº¿æ€§æœŸæœ›å’ŒåŠ¨æ€è§„åˆ’æ¨å¯¼HJBæ–¹ç¨‹æ±‚è§£æœ€ä¼˜ç­–ç•¥ç­‰æ€è·¯ï¼Œä»¥åŠåœ¨LQæƒ…å½¢ä¸‹çš„åˆ†ææ–¹æ³•ï¼Œå¯ä¸ºåç»­ç›¸å…³é¢†åŸŸï¼ˆå¦‚å·¥ç¨‹æ§åˆ¶ã€é‡‘èå†³ç­–ç­‰ï¼‰ç ”ç©¶æä¾›æ–¹æ³•è®ºå€Ÿé‰´ï¼Œå¯å‘å­¦è€…å…³æ³¨æ¨¡å‹ä¸ç¡®å®šæ€§å¯¹å¼ºåŒ–å­¦ä¹ æœ€ä¼˜æ§åˆ¶çš„å½±å“å¹¶æ¢ç´¢åº”å¯¹æ–¹æ³•ã€‚

## dr-sac--distributionally-robust-soft-actor-critic-for-reinforcement-learning-under-uncertainty
### Abstract
Deep reinforcement learning (RL) has achieved significant success, yet its
application in real-world scenarios is often hindered by a lack of robustness
to environmental uncertainties. To solve this challenge, some robust RL
algorithms have been proposed, but most are limited to tabular settings. In
this work, we propose Distributionally Robust Soft Actor-Critic (DR-SAC), a
novel algorithm designed to enhance the robustness of the state-of-the-art Soft
Actor-Critic (SAC) algorithm. DR-SAC aims to maximize the expected value with
entropy against the worst possible transition model lying in an uncertainty
set. A distributionally robust version of the soft policy iteration is derived
with a convergence guarantee. For settings where nominal distributions are
unknown, such as offline RL, a generative modeling approach is proposed to
estimate the required nominal distributions from data. Furthermore,
experimental results on a range of continuous control benchmark tasks
demonstrate our algorithm achieves up to $9.8$ times the average reward of the
SAC baseline under common perturbations. Additionally, compared with existing
robust reinforcement learning algorithms, DR-SAC significantly improves
computing efficiency and applicability to large-scale problems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DR - SACï¼šä¸ç¡®å®šæ€§ä¸‹å¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼é²æ£’è½¯æ¼”å‘˜ - è¯„è®ºå®¶ç®—æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½å–å¾—å·¨å¤§æˆåŠŸï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­åº”ç”¨å¸¸å—é™äºå¯¹ç¯å¢ƒä¸ç¡®å®šæ€§ç¼ºä¹é²æ£’æ€§ã€‚è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“åœ¨ç¨æœ‰ä¸åŒçš„ç¯å¢ƒéƒ¨ç½²æ—¶æ€§èƒ½æ˜“å¤§å¹…ä¸‹é™ï¼Œè¿™æºäºè®­ç»ƒä¸éƒ¨ç½²ç¯å¢ƒé—´çš„æ¨¡å‹ä¸åŒ¹é…ï¼Œå¦‚ä¸ç¡®å®šçš„è½¬ç§»å’Œå¥–åŠ±å‡½æ•°ã€è§‚æµ‹ä¸æ‰§è¡Œå™¨è¯¯å·®ç­‰ã€‚å·²æœ‰é²æ£’RLç®—æ³•å¤šé™äºè¡¨æ ¼å‹è®¾ç½®ï¼Œéš¾ä»¥åº”å¯¹è¿ç»­æ§åˆ¶ç­‰å¤æ‚åœºæ™¯ã€‚è€ŒSACä½œä¸ºå…ˆè¿›çš„æ·±åº¦RLç®—æ³•ï¼Œåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡è¡¨ç°å‡ºè‰²ä½†é²æ£’æ€§ä¸è¶³ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨å¢å¼ºSACçš„é²æ£’æ€§ï¼Œæå‡ºDR - SACç®—æ³•ä»¥åº”å¯¹ç¯å¢ƒä¸ç¡®å®šæ€§æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¨å¯¼åˆ†å¸ƒå¼é²æ£’è½¯ç­–ç•¥è¿­ä»£åŠæ”¶æ•›æ€§
è€ƒè™‘ç­–ç•¥ç†µçš„ç›®æ ‡ï¼Œæ¨å¯¼äº†æ–°é¢–çš„åˆ†å¸ƒå¼é²æ£’è½¯ç­–ç•¥è¿­ä»£ã€‚å‡è®¾ç¯å¢ƒçœŸå®è½¬ç§»åˆ†å¸ƒå¤„äºä»¥åä¹‰è½¬ç§»æ¨¡å‹ä¸ºä¸­å¿ƒçš„KLæ•£åº¦çƒæ„æˆçš„ä¸ç¡®å®šæ€§é›†åˆå†…ï¼Œç›®æ ‡æ˜¯å­¦ä¹ åœ¨è¯¥é›†åˆæœ€ååˆ†å¸ƒä¸‹æœ€å¤§åŒ–è½¯å€¼å‡½æ•°çš„ç­–ç•¥ã€‚åŸºäºDRè½¯Bellmanæ–¹ç¨‹ï¼Œåˆ©ç”¨åˆ†å¸ƒå¼é²æ£’ä¼˜åŒ–ï¼ˆDROï¼‰ç†è®ºçš„å¼ºå¯¹å¶æ€§ï¼Œå°†åŸé—®é¢˜è½¬åŒ–ä¸ºæ˜“å¤„ç†çš„å¯¹å¶å½¢å¼ï¼Œè§£å†³äº†ä¸ç¡®å®šæ€§é›†åˆæ— é™ç»´å¸¦æ¥çš„åŸé—®é¢˜éš¾è§£æ€§ã€‚åŒæ—¶è¯æ˜äº†è¯¥ç­–ç•¥å¯¹ä»¥åä¹‰è½¬ç§»åˆ†å¸ƒä¸ºä¸­å¿ƒã€KLæ•£åº¦çº¦æŸçš„ä¸ç¡®å®šæ€§é›†åˆå…·æœ‰é²æ£’æ€§ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨äº¤æ¢æ€§è´¨å®ç°å‡½æ•°ä¼˜åŒ–æå‡æ•ˆç‡
ä¸ºé¿å…å¯¹æ¯ä¸ªçŠ¶æ€ - åŠ¨ä½œå¯¹æ±‚è§£ä¼˜åŒ–é—®é¢˜ï¼Œå¼•å…¥åŸºäºå˜åˆ†åˆ†æäº¤æ¢å®šç†çš„å‡½æ•°ä¼˜åŒ–æŠ€æœ¯ã€‚å°†KLçº¦æŸä¸ç¡®å®šæ€§é›†åˆå†…çš„ä¼˜åŒ–é—®é¢˜é‡æ„ä¸ºå‡½æ•°ä¼˜åŒ–ï¼Œå®è¯è¡¨æ˜è¯¥æ–°å½¢å¼èƒ½é«˜æ•ˆæ›´æ–°åˆ†å¸ƒå¼é²æ£’è½¯ç­–ç•¥è¿­ä»£ï¼Œä¸ç°æœ‰é²æ£’å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³ä¸è¶³20%ï¼Œå¤§å¹…æå‡è®¡ç®—æ•ˆç‡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»“åˆç”Ÿæˆæ¨¡å‹åº”å¯¹ç¦»çº¿ä¸è¿ç»­ç©ºé—´ä»»åŠ¡
é’ˆå¯¹åä¹‰æ¨¡å‹æœªçŸ¥ï¼ˆå¦‚ç¦»çº¿RLåœºæ™¯ï¼‰å’ŒåŠ¨ä½œç©ºé—´è¿ç»­çš„æƒ…å†µï¼Œé‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ã€‚ä»ç¦»çº¿æ•°æ®é›†ä¼°è®¡åä¹‰è½¬ç§»åˆ†å¸ƒå¹¶ç”Ÿæˆæ ·æœ¬å½¢æˆç»éªŒæµ‹åº¦ï¼Œåœ¨å¢åŠ å°‘é‡è®¡ç®—å’Œå†…å­˜å¼€é”€ä¸‹ï¼Œä½¿DR - SACèƒ½åœ¨å…·æŒ‘æˆ˜æ€§çš„ç¦»çº¿å­¦ä¹ å’Œè¿ç»­ç©ºé—´ä»»åŠ¡ä¸­å®ç°åˆ†å¸ƒå¼é²æ£’è½¯ç­–ç•¥å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸€ç³»åˆ—è¿ç»­æ§åˆ¶åŸºå‡†ä»»åŠ¡å®éªŒä¸­ï¼ŒDR - SACåœ¨å¸¸è§æ‰°åŠ¨ä¸‹ï¼Œå¹³å‡å¥–åŠ±å¯è¾¾SACåŸºçº¿çš„9.8å€ã€‚ä¸ç°æœ‰é²æ£’å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œä¸”åœ¨å¤§è§„æ¨¡é—®é¢˜ä¸Šé€‚ç”¨æ€§æ›´å¼ºã€‚åœ¨äº”ä¸ªå¸¦å¹¿æ³›æ‰°åŠ¨çš„ç¦»çº¿RLç¯å¢ƒéªŒè¯ä¸­ï¼ŒDR - SACå±•ç°å‡ºè¿œè¶…SACçš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ç†è®ºæ¨å¯¼æ–¹é¢ï¼šå°†åˆ†å¸ƒå¼é²æ£’ä¼˜åŒ–ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œæ¨å¯¼é²æ£’ç‰ˆç­–ç•¥è¿­ä»£å¹¶è¯æ˜æ”¶æ•›æ€§ï¼Œä¸ºé²æ£’RLç®—æ³•ç†è®ºå¥ åŸºæä¾›äº†æ€è·¯ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§è·¨é¢†åŸŸç†è®ºèåˆæ–¹å¼å¤„ç†ä¸ç¡®å®šæ€§é—®é¢˜ã€‚
2. æ•ˆç‡ä¼˜åŒ–æ–¹é¢ï¼šå¼•å…¥å‡½æ•°ä¼˜åŒ–æŠ€æœ¯æ›¿ä»£é€çŠ¶æ€ - åŠ¨ä½œå¯¹ä¼˜åŒ–ï¼Œä¸ºæå‡ç®—æ³•è®¡ç®—æ•ˆç‡æä¾›äº†æ–°è§†è§’ï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡çŠ¶æ€åŠ¨ä½œç©ºé—´é—®é¢˜æ—¶å¯å‚è€ƒè¯¥ä¼˜åŒ–æ€è·¯ã€‚
3. æ•°æ®åˆ©ç”¨æ–¹é¢ï¼šåœ¨ç¦»çº¿åœºæ™¯ç»“åˆç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚VAEsï¼‰ä¼°è®¡æœªçŸ¥åˆ†å¸ƒï¼Œä¸ºç¦»çº¿RLä¸­åº”å¯¹åˆ†å¸ƒä¸ç¡®å®šæ€§æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œæ‹“å±•äº†ç¦»çº¿RLç®—æ³•åœ¨é²æ£’æ€§æ–¹å‘çš„åº”ç”¨å¯èƒ½ï¼Œå…¶ä»–ç¦»çº¿é²æ£’RLç ”ç©¶å¯å€Ÿé‰´ç”Ÿæˆæ¨¡å‹ä¸RLç»“åˆçš„æ¨¡å¼ã€‚

## viability-of-future-actions--robust-safety-in-reinforcement-learning-via-entropy-regularization
### Abstract
Despite the many recent advances in reinforcement learning (RL), the question
of learning policies that robustly satisfy state constraints under unknown
disturbances remains open. In this paper, we offer a new perspective on
achieving robust safety by analyzing the interplay between two well-established
techniques in model-free RL: entropy regularization, and constraints
penalization. We reveal empirically that entropy regularization in constrained
RL inherently biases learning toward maximizing the number of future viable
actions, thereby promoting constraints satisfaction robust to action noise.
Furthermore, we show that by relaxing strict safety constraints through
penalties, the constrained RL problem can be approximated arbitrarily closely
by an unconstrained one and thus solved using standard model-free RL. This
reformulation preserves both safety and optimality while empirically improving
resilience to disturbances. Our results indicate that the connection between
entropy regularization and robustness is a promising avenue for further
empirical and theoretical investigation, as it enables robust safety in RL
through simple reward shaping.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºç†µæ­£åˆ™åŒ–å®ç°å¼ºåŒ–å­¦ä¹ é²æ£’å®‰å…¨ï¼šæœªæ¥åŠ¨ä½œå¯è¡Œæ€§è§†è§’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‘å¹´å–å¾—è¯¸å¤šè¿›å±•ï¼Œä½†åœ¨æœªçŸ¥æ‰°åŠ¨ä¸‹å­¦ä¹ èƒ½é²æ£’æ»¡è¶³çŠ¶æ€çº¦æŸçš„ç­–ç•¥ä»æ˜¯æœªè§£å†³çš„é—®é¢˜ã€‚ç°æœ‰å·¥ä½œä¸­ï¼Œé²æ£’å¼ºåŒ–å­¦ä¹ å¸¸è¢«å»ºæ¨¡ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œè€Œåˆ©ç”¨æ— çº¦æŸRLç®—æ³•å®ç°é²æ£’å®‰å…¨ç­–ç•¥é¢‡å…·å¸å¼•åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨æ­ç¤ºé²æ£’å®‰å…¨ç­–ç•¥å¦‚ä½•ä»RLä¸­ä¸¤ç§å¸¸è§åšæ³•ï¼ˆæœ€å¤§ç†µRLä¸å¤±è´¥æƒ©ç½šï¼‰è‡ªç„¶æ¶Œç°ï¼Œæ¢ç´¢ç†µæ­£åˆ™åŒ–ä¸é²æ£’æ€§çš„å…³è”ä»¥å®ç°æ›´ä¼˜å®‰å…¨ä¿éšœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç†µæ­£åˆ™åŒ–è¯±å¯¼å¯¹åŠ¨ä½œå™ªå£°çš„é²æ£’æ€§  
é€šè¿‡å®è¯è¡¨æ˜ï¼Œçº¦æŸç¯å¢ƒä¸‹çš„ç†µæ­£åˆ™åŒ–ä¼šä½¿æ™ºèƒ½ä½“ä¸ºé¿å…çº¦æŸè¾¹ç•Œè€Œâ€œç‰ºç‰²â€å¥–åŠ±ï¼Œä¸”è¿™ç§å›é¿ç¨‹åº¦ç”±æ¸©åº¦å‚æ•°è°ƒèŠ‚ã€‚ç†µæ­£åˆ™åŒ–çš„ç­–ç•¥å€¾å‘äºæœ€å¤§åŒ–æœªæ¥å¯è¡ŒåŠ¨ä½œæ•°é‡ï¼Œå°†ç­–ç•¥çš„ç´¯ç§¯æŠ˜æ‰£ç†µè§†ä¸ºé•¿æœŸå®‰å…¨åŠ¨ä½œæ•°é‡çš„ä»£ç†ï¼Œè‡ªç„¶é¼“åŠ±é¿å¼€å¯è¡Œé€‰é¡¹å°‘çš„çŠ¶æ€ï¼Œè¿›è€Œè½¬åŒ–ä¸ºå¯¹åŠ¨ä½œå™ªå£°çš„é²æ£’æ€§ï¼ˆå³ä¿æŒé•¿æœŸå¯è¡ŒåŠ¨ä½œæ•°çš„ç­–ç•¥æ›´é²æ£’ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤±è´¥æƒ©ç½šè¿‘ä¼¼çº¦æŸé—®é¢˜  
è¯æ˜äº†é€šè¿‡å¤±è´¥æƒ©ç½šæ¾å¼›ä¸¥æ ¼å®‰å…¨çº¦æŸåï¼Œçº¦æŸRLé—®é¢˜èƒ½è¢«æ— çº¦æŸé—®é¢˜ä»»æ„è¿‘ä¼¼ï¼Œä»è€Œå¯ç”¨æ ‡å‡†æ— çº¦æŸRLæ±‚è§£ã€‚å½“æƒ©ç½šè¶…è¿‡æœ‰é™é˜ˆå€¼æ—¶ï¼Œæ‰€å¾—ç­–ç•¥çš„æ¨¡å¼ï¼ˆmodeï¼‰ä¸çº¦æŸé—®é¢˜çš„ç­–ç•¥æ¨¡å¼åŒ¹é…ï¼Œä¸ºå­¦ä¹ é²æ£’å®‰å…¨ç­–ç•¥æä¾›äº†å®ç”¨çš„å¥–åŠ±å¡‘é€ ç­–ç•¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»æƒ©ç½šé—®é¢˜è§£ä¸­æå–å®‰å…¨ç­–ç•¥  
å±•ç¤ºäº†èƒ½ä»æƒ©ç½šé—®é¢˜çš„æœ€ä¼˜è§£ä¸­æå–å®‰å…¨ç­–ç•¥ï¼Œä¸”è¯¥ç­–ç•¥å…·å¤‡é²æ£’å®‰å…¨æ€§ï¼ŒåŒæ—¶å°†æ¸©åº¦ç³»æ•°è§£è¯»ä¸ºå¯è°ƒèŠ‚çš„é²æ£’æ€§å‚æ•°ï¼Œä¸ºé²æ£’æ€§è°ƒæ§æä¾›æ–°è§†è§’ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
æ–‡ä¸­ä»¥â€œå›´æ æ‚¬å´–ï¼ˆFenced cliffï¼‰â€ç­‰åœºæ™¯éªŒè¯ï¼šç†µæ­£åˆ™åŒ–ç­–ç•¥ä¼šé¿å¼€å¯è¡ŒåŠ¨ä½œå°‘çš„çŠ¶æ€ï¼Œæ¸©åº¦å‚æ•°è¶Šé«˜ï¼Œç­–ç•¥æ¨¡å¼ç¦»çº¦æŸè¶Šè¿œï¼ˆæ›´é²æ£’ä½†åˆ°è¾¾ç›®æ ‡è€—æ—¶å¢åŠ ï¼‰ï¼›åŒæ—¶å®è¯è¡¨æ˜è¿™ç§çº¦æŸå›é¿èƒ½è½¬åŒ–ä¸ºå¯¹åŠ¨ä½œå™ªå£°çš„é²æ£’æ€§ï¼Œå³ç­–ç•¥åœ¨è®­ç»ƒåé¢å¯¹å¼ºäºè®­ç»ƒé˜¶æ®µçš„åŠ¨ä½œå™ªå£°æ—¶ï¼Œä»èƒ½æ»¡è¶³å®‰å…¨çº¦æŸã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯å‘å¯¹ç†µæ­£åˆ™åŒ–ä¸é²æ£’æ€§å…³è”çš„æ·±å…¥ç ”ç©¶ï¼šå°†ç†µæ­£åˆ™åŒ–ä¸é²æ£’å®‰å…¨ç»“åˆï¼Œä¸ºç†è®ºå’Œå®è¯æ¢ç´¢å¼€è¾Ÿæ–°æ–¹å‘ï¼Œåç»­å¯å›´ç»•å…¶æ•°å­¦åŸç†ä¸æ›´å¤æ‚åœºæ™¯æ‹“å±•åˆ†æã€‚  
2. å¥–åŠ±å¡‘é€ å®ç°é²æ£’å®‰å…¨ï¼šå€ŸåŠ©å¤±è´¥æƒ©ç½šä¸ç†µæ­£åˆ™åŒ–çš„ç»„åˆï¼Œæ— éœ€å¤æ‚çº¦æŸä¼˜åŒ–æ¡†æ¶ï¼Œç”¨æ— çº¦æŸRLæŠ€æœ¯å°±èƒ½é€¼è¿‘é²æ£’å®‰å…¨ç­–ç•¥ï¼Œé™ä½é²æ£’å®‰å…¨RLçš„å®ç°é—¨æ§›ã€‚  
3. è¶…å‚æ•°è§£è¯»ä¸è°ƒæ§ï¼šæŠŠæ¸©åº¦å‚æ•°è§†ä¸ºé²æ£’æ€§å¯è°ƒå‚æ•°ï¼Œä¸ºå·¥ç¨‹å®è·µä¸­æ ¹æ®åœºæ™¯éœ€æ±‚ï¼ˆå¦‚å®‰å…¨ä¸æ•ˆç‡æƒè¡¡ï¼‰è°ƒæ•´ç­–ç•¥æä¾›ç›´è§‚æŒ‡å¯¼ã€‚  

## exploration-by-random-reward-perturbation
### Abstract
We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | éšæœºå¥–åŠ±æ‰°åŠ¨ï¼šè½»é‡é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ¢ç´¢æ–°ç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆMFRLï¼‰ä¸­ï¼Œé«˜æ•ˆæ¢ç´¢å¤šæ ·åœºæ™¯ã€åŠ¨ä½œä¸ç»“æœæ˜¯å…³é”®ï¼Œå°¤å…¶åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼šåŸºäºåŠ¨ä½œæ‰°åŠ¨çš„æ¢ç´¢ç­–ç•¥ï¼ˆå¦‚$\epsilon$-greedyç­‰ï¼‰å› ç¼ºä¹æ–¹å‘æŒ‡å¯¼ï¼Œåœ¨é•¿ horizon æˆ–æç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸­è¡¨ç°æŒ£æ‰ï¼›åŸºäºå¥–åŠ±å¡‘é€ ï¼ˆRSï¼‰çš„æ–¹æ³•éœ€é¢å¤–æ¨¡å‹ã€è®¡ç®—å¼€é”€ï¼Œä¸”å¹³è¡¡å¥–åŠ±ä¾èµ–å…ˆéªŒæˆ–æ‰‹åŠ¨è°ƒå‚ï¼Œé€‚åº”æ€§å—é™ã€‚åŒæ—¶ï¼Œé€šè¿‡å¥–åŠ±æ‰°åŠ¨æ¢ç´¢çš„ç ”ç©¶è¾ƒå°‘ï¼Œç†è®ºæ”¯æ’‘ä¸è¶³ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºéšæœºå¥–åŠ±æ‰°åŠ¨ï¼ˆRRPï¼‰ç­–ç•¥ï¼Œå¡«è¡¥å¥–åŠ±æ‰°åŠ¨æ¢ç´¢çš„ç ”ç©¶ç©ºç™½ï¼Œç®€åŒ–æ¢ç´¢é€»è¾‘ä»¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºéšæœºå¥–åŠ±æ‰°åŠ¨ï¼ˆRRPï¼‰ç­–ç•¥  
RRP å‘æ™ºèƒ½ä½“æ¥æ”¶çš„ç¯å¢ƒå¥–åŠ±ä¸­æ³¨å…¥é›¶å‡å€¼é«˜æ–¯å™ªå£°ï¼Œå³ $R_{\text{RRP}}(s) = R_{\text{env}}(s) + \varepsilon$ï¼ˆ$\varepsilon \sim \mathcal{N}(0, \sigma^2)$ï¼‰ã€‚é€šè¿‡æ‰°åŠ¨å¥–åŠ±ï¼Œä½¿ç­–ç•¥åœ¨å­¦ä¹ ä¸­å‘æ›´éšæœºæ–¹å‘ä¼˜åŒ–ï¼Œä¿ƒè¿›è¡Œä¸ºå¤šæ ·æ€§ï¼Œæ‹“å±•æ¢ç´¢èŒƒå›´ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå™ªå£°é€€ç«æœºåˆ¶å®ç°æ¢ç´¢åˆ°åˆ©ç”¨çš„å¹³æ»‘è¿‡æ¸¡  
ä¸ºè®©æ™ºèƒ½ä½“åæœŸèšç„¦ç¯å¢ƒåŸå§‹å¥–åŠ±ç›®æ ‡ï¼Œå¯¹å™ªå£°æ ‡å‡†å·® $\sigma$ è¿›è¡Œçº¿æ€§è¡°å‡ï¼š$\sigma(t) = \max\{0, \sigma_{\text{max}} - (\sigma_{\text{max}} - \sigma_{\text{min}})t/T\}$ ã€‚å…¶ä¸­ $\sigma_{\text{max}}$ã€$\sigma_{\text{min}}$ æ˜¯åˆå§‹å’Œæœ€ç»ˆæ ‡å‡†å·®ï¼Œ$T$ ä¸ºå™ªå£°è¡°å‡çš„è®­ç»ƒæ­¥æ•°ã€‚å½“ $\sigma_{\text{min}} = 0$ æ—¶ï¼Œå™ªå£°å®Œå…¨æ¶ˆé™¤ï¼Œæ¢å¤åŸå§‹å¥–åŠ±ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå…¼å®¹æ€§ä¸è½»é‡æ€§  
RRP ä¸åŸºäºåŠ¨ä½œæ‰°åŠ¨çš„æ¢ç´¢ç­–ç•¥ï¼ˆå¦‚ $\epsilon$-greedyã€éšæœºç­–ç•¥ã€ç†µæ­£åˆ™åŒ–ï¼‰å®Œå…¨å…¼å®¹ï¼Œå¯é™„åŠ æå‡æ¢ç´¢æ•ˆæœã€‚åªéœ€å¯¹å¥–åŠ±å‡½æ•°ç®€å•ä¿®æ”¹ï¼Œå°±èƒ½é›†æˆåˆ°ç°æœ‰ RL ç®—æ³•ï¼Œå®ç°æˆæœ¬ä½ã€è®¡ç®—å¼€é”€å¯å¿½ç•¥ã€‚åŒæ—¶ï¼ŒRRP å»ºç«‹å¥–åŠ±å¡‘é€ ä¸å™ªå£°é©±åŠ¨æ¢ç´¢çš„ç†è®ºè”ç³»ï¼Œå‡¸æ˜¾äº’è¡¥æ½œåŠ›ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡å°† RRP é›†æˆåˆ° Proximal Policy Optimizationï¼ˆPPOï¼‰å’Œ Soft Actor-Criticï¼ˆSACï¼‰ç®—æ³•ï¼Œåœ¨ MuJoCoã€Mobile Manipulatorã€Dexterous Hand ç­‰å¤šé¢†åŸŸï¼Œç¨€ç–å’Œå¯†é›†å¥–åŠ±åœºæ™¯ä¸‹æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼šRRP-PPO å’Œ RRP-SAC æŒç»­è¶…è¶Š vanilla ç‰ˆæœ¬ï¼›åœ¨å¤šä»»åŠ¡ä¸­å¸®åŠ©æ™ºèƒ½ä½“é€ƒç¦»å±€éƒ¨æœ€ä¼˜ï¼Œæå‡æ ·æœ¬æ•ˆç‡ä¸æ”¶æ•›é€Ÿåº¦ï¼Œåœ¨æ¢ç´¢æ–¹æ³•ä¸­è¡¨ç°å…·ç«äº‰åŠ›ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¢ç´¢ç­–ç•¥åˆ›æ–°ï¼šä»å¥–åŠ±æ‰°åŠ¨è§’åº¦åˆ‡å…¥å¼ºåŒ–å­¦ä¹ æ¢ç´¢ï¼Œä¸ºè§£å†³ç¨€ç–å¥–åŠ±ç­‰åœºæ™¯ä¸‹çš„æ¢ç´¢éš¾é¢˜æä¾›æ–°æ€è·¯ï¼Œå¯å‘åç»­å¯¹å¥–åŠ±å±‚é¢æ¢ç´¢æœºåˆ¶çš„ç ”ç©¶ã€‚  
2. è½»é‡é›†æˆæ€è·¯ï¼šRRP åªéœ€ç®€å•ä¿®æ”¹å¥–åŠ±å‡½æ•°å°±èƒ½åµŒå…¥ç°æœ‰ç®—æ³•ï¼Œè¿™ç§â€œä½ä¾µå…¥æ€§â€çš„æ”¹è¿›æ–¹å¼ï¼Œä¸ºç®—æ³•ä¼˜åŒ–æä¾›äº†é«˜æ•ˆä¸”æ˜“å®ç°çš„èŒƒå¼ï¼Œä¾¿äºå·¥ç¨‹å®è·µä¸­å¿«é€Ÿå°è¯•ã€‚  
3. å™ªå£°é€€ç«æœºåˆ¶ï¼šé€šè¿‡åŠ¨æ€è°ƒæ•´å™ªå£°è§„æ¨¡å®ç°æ¢ç´¢åˆ°åˆ©ç”¨çš„å¹³æ»‘è¿‡æ¸¡ï¼Œè¿™ç§åŠ¨æ€è°ƒæ§çš„æ€è·¯å¯è¿ç§»åˆ°å…¶ä»–éœ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„å¼ºåŒ–å­¦ä¹ æˆ–ä¼˜åŒ–ä»»åŠ¡ä¸­ã€‚  

## state-entropy-regularization-for-robust-reinforcement-learning
### Abstract
State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | çŠ¶æ€ç†µæ­£åˆ™åŒ–ï¼šä¸ºé²æ£’å¼ºåŒ–å­¦ä¹ ç­‘ç‰¢â€œé˜²æŠ¤ç›¾â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¯¸å¤šåˆæˆé¢†åŸŸå–å¾—äº®çœ¼æˆæœï¼Œä½†èµ°å‘çœŸå®ä¸–ç•Œä»éœ€è§£å†³æ¨¡å‹ä¸å®Œç¾ã€è§‚æµ‹å™ªå£°ã€æ•°æ®æœ‰é™ç­‰éš¾é¢˜ã€‚è®­ç»ƒå¥½çš„ç­–ç•¥åœ¨éƒ¨ç½²æ—¶è‹¥é‡ç¯å¢ƒåŠ¨æ€æˆ–å¥–åŠ±ç»“æ„åç¦»ï¼Œè¡¨ç°å¯èƒ½éª¤é™ï¼Œè¿™æ¨åŠ¨äº†é²æ£’å¼ºåŒ–å­¦ä¹ ï¼ˆRobust RLï¼‰å‘å±•â€”â€”è®©ç­–ç•¥åœ¨æ¨¡å‹è¯¯è®¾æˆ–ä¸ç¡®å®šæ€§ä¸‹ä¹Ÿå¯é è¿è¡Œã€‚  

æ­¤å‰ï¼Œç­–ç•¥ç†µæ­£åˆ™åŒ–å·²è¢«ç ”ç©¶ä¸é²æ£’æ€§å­˜åœ¨å…³è”ï¼Œè€Œ**çŠ¶æ€ç†µæ­£åˆ™åŒ–**è™½åœ¨å®éªŒä¸­å±•ç°å‡ºæ›´å¥½æ¢ç´¢æ€§ä¸æ ·æœ¬æ•ˆç‡ï¼Œå…¶ç†è®ºä¿éšœä¸é²æ£’æ€§è”ç³»å´æ— äººæ·±ç©¶ã€‚æ¯”å¦‚ç­–ç•¥ç†µé¼“åŠ±åŠ¨ä½œé€‰æ‹©éšæœºæ€§ï¼Œä½†å¸¸æ²¿å•ä¸€è·¯å¾„æ‰©æ•£éšæœºï¼Œé‡è·¯å¾„çº§æ‰°åŠ¨ï¼ˆå¦‚å¤§éšœç¢ç‰©å µæ­»æœ€ä¼˜è·¯å¾„ï¼‰æ˜“å¤±æ•ˆï¼›çŠ¶æ€ç†µåˆ™æ¿€åŠ±è¦†ç›–æ›´å¹¿æ³›çŠ¶æ€ç©ºé—´ï¼Œå¯èƒ½è®©æ™ºèƒ½ä½“èµ°å¤šæ¡é«˜å¥–åŠ±è·¯å¾„ã€‚æœ¬æ–‡æ­£æ˜¯è¦æ¢ç©¶ï¼šçŠ¶æ€ç†µæ­£åˆ™åŒ–æ˜¯å¦èƒ½å¸¦æ¥é²æ£’æ€§ï¼Ÿå¸¦æ¥ä½•ç§é²æ£’æ€§ï¼Ÿå¹¶å¯¹æ¯”å®ƒä¸ç­–ç•¥ç†µçš„ç‰¹æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçŠ¶æ€ç†µæ­£åˆ™åŒ–çš„é²æ£’æ€§ç†è®ºåˆ†æï¼ˆå¥–åŠ±ä¸è½¬ç§»ä¸ç¡®å®šæ€§ï¼‰
- **å¥–åŠ±é²æ£’RLåœºæ™¯**ï¼šè¯æ˜çŠ¶æ€ç†µæ­£åˆ™åŒ–èƒ½ç²¾å‡†è§£å†³ä¸€ç±»å¥–åŠ±é²æ£’RLé—®é¢˜ï¼Œåˆ»ç”»äº†å…¶è¯±å¯¼çš„ä¸ç¡®å®šæ€§é›†åˆä¸å¯¹æŠ—å¥–åŠ±ã€‚ç†è®ºä¸Šï¼Œç­–ç•¥ç†µæŠµå¾¡â€œå±€éƒ¨çŸ¥æƒ…â€çš„å¯¹æŠ—æ‰°åŠ¨ï¼ŒçŠ¶æ€ç†µåˆ™å¯¹â€œå…¨å±€çŸ¥æƒ…â€æ‰°åŠ¨æ›´é²æ£’ï¼›ä¸”æ­£åˆ™åŒ–å¼ºåº¦èƒ½å•è°ƒæ§åˆ¶ä¸ç¡®å®šæ€§é›†åˆçš„â€œä¿å®ˆç¨‹åº¦â€â€”â€”å¼±æ­£åˆ™æ—¶è¶‹è¿‘â„“âˆå‹ä¸ç¡®å®šæ€§ï¼Œå¼ºæ­£åˆ™æ—¶è¶‹è¿‘â„“1å‹ã€‚  
- **è½¬ç§»ï¼ˆæ ¸ï¼‰ä¸ç¡®å®šæ€§åœºæ™¯**ï¼šè¯æ˜çŠ¶æ€ç†µæ­£åˆ™åŒ–èƒ½åœ¨è½¬ç§»ä¸ç¡®å®šæ€§ä¸‹ç»™å‡ºéå¹³å‡¡çš„æ€§èƒ½ä¸‹ç•Œã€‚å¯¹æ¯”â€œçŠ¶æ€+ç­–ç•¥ç†µâ€ç»„åˆï¼Œå‘ç°åŠ ç­–ç•¥ç†µä¼šå‰Šå¼±è¯¥ä¸‹ç•Œï¼Œå‡¸æ˜¾å•ç‹¬ç”¨çŠ¶æ€ç†µçš„ç»“æ„ä¼˜åŠ¿ã€‚  

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç†µæ­£åˆ™åŒ–çš„å±€é™æ€§ç ”ç©¶  
- ç†è®ºä¸å®è·µå±‚é¢é™åˆ¶ï¼šæ— è®ºæ˜¯ç­–ç•¥ç†µã€çŠ¶æ€ç†µè¿˜æ˜¯ä¸¤è€…ç»“åˆï¼Œ**éƒ½æ— æ³•è§£å†³æ‰€æœ‰è½¬ç§»æ ¸é²æ£’RLé—®é¢˜**ï¼›ä¸”ç†µæ­£åˆ™åŒ–å¯èƒ½åœ¨é£é™©è§„é¿åœºæ™¯ä¸‹å¤§å¹…æŸå®³æ€§èƒ½ã€‚  
- å¯¹rolloutæ•°é‡çš„æ•æ„Ÿæ€§ï¼šçŠ¶æ€ç†µæ­£åˆ™åŒ–çš„é²æ£’æ€§ä¼˜åŠ¿ï¼Œç›¸æ¯”å…¶ä»–æ­£åˆ™åŒ–ï¼Œå¯¹â€œç­–ç•¥è¯„ä¼°æ‰€ç”¨rolloutæ•°é‡â€æ›´æ•æ„Ÿâ€”â€”æ ·æœ¬å°‘çš„åœºæ™¯ä¸‹ï¼Œå…¶é²æ£’æ€§å¢ç›Šä¼šç¼©æ°´ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç¦»æ•£ä¸è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­éªŒè¯çŠ¶æ€ç†µæ­£åˆ™åŒ–çš„é²æ£’æ€§ï¼š  
- é¢å¯¹**ç©ºé—´ç›¸å…³æ‰°åŠ¨**ï¼ˆå¦‚éšœç¢ç‰©æ”¾ç½®ï¼‰ï¼ŒçŠ¶æ€ç†µæ­£åˆ™åŒ–èƒ½æå‡æ€§èƒ½ï¼›é¢å¯¹æ›´ç»†ç¢ã€å‡åŒ€çš„æ‰°åŠ¨ï¼Œä¹Ÿä¸ä¼šé™ä½æ€§èƒ½ã€‚  
- é²æ£’æ€§æ”¶ç›Šä¾èµ–rollouté¢„ç®—ï¼ˆå³è¯„ä¼°ç­–ç•¥æ—¶ç”¨å¤šå°‘æ¡è½¨è¿¹ï¼‰ï¼šä½æ ·æœ¬åœºæ™¯ä¸‹ï¼ŒçŠ¶æ€ç†µå¸¦æ¥çš„é²æ£’æ€§å¢ç›Šä¼šå‡å¼±ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç†è®ºè§†è§’**ï¼šé¦–æ¬¡ä¸ºçŠ¶æ€ç†µæ­£åˆ™åŒ–çš„é²æ£’æ€§æä¾›å½¢å¼åŒ–ä¿éšœï¼Œæ¸…æ™°å¯¹æ¯”å®ƒä¸ç­–ç•¥ç†µåœ¨é²æ£’æ€§ä¸Šçš„å·®å¼‚ï¼ˆå¦‚å¯¹æŠ—æ‰°åŠ¨â€œçŸ¥æƒ…èŒƒå›´â€ã€è½¬ç§»ä¸ç¡®å®šæ€§ä¸‹çš„ä¸‹ç•Œè¡¨ç°ï¼‰ï¼Œä¸ºåç»­é²æ£’RLç†è®ºç ”ç©¶é”šå®šæ–°æ–¹å‘ã€‚  
2. **å®è·µè§†è§’**ï¼šæ­ç¤ºçŠ¶æ€ç†µå¯¹rolloutæ•°é‡çš„æ•æ„Ÿæ€§ï¼Œæç¤ºå·¥ç¨‹è½åœ°æ—¶éœ€å…³æ³¨æ ·æœ¬è§„æ¨¡ï¼›ä¸”å®éªŒéªŒè¯å…¶åœ¨â€œè·¯å¾„çº§æ‰°åŠ¨â€ï¼ˆå¦‚éšœç¢ç‰©ï¼‰åœºæ™¯çš„é²æ£’æ€§ä¼˜åŠ¿ï¼Œä¸ºçœŸå®ä¸–ç•Œï¼ˆå¦‚æœºå™¨äººé¿éšœã€å¤æ‚ç¯å¢ƒå¯¼èˆªï¼‰ä¸­RLç­–ç•¥è®¾è®¡æä¾›å‚è€ƒã€‚  
3. **æ–¹æ³•è®ºè§†è§’**ï¼šå¯¹ç†µæ­£åˆ™åŒ–â€œå±€é™æ€§â€çš„å‰–æï¼ˆå¦‚è½¬ç§»æ ¸é²æ£’é—®é¢˜çš„ä¸å¯è§£æ€§ã€é£é™©è§„é¿åœºæ™¯çš„å‰¯ä½œç”¨ï¼‰ï¼Œå¸®åŠ©ç ”ç©¶è€…æ›´ç†æ€§é€‰æ‹©æ­£åˆ™åŒ–æ‰‹æ®µï¼Œé¿å…ç›²ç›®å †å æ–¹æ³•ã€‚  


è¿™ç¯‡è®ºæ–‡ä»ç†è®ºåˆ°å®éªŒï¼Œä¸ºâ€œçŠ¶æ€ç†µæ­£åˆ™åŒ–å¦‚ä½•èµ‹èƒ½é²æ£’RLâ€ç”»å‡ºæ¸…æ™°å›¾æ™¯ï¼Œæ—¢è¡¥å…¨äº†è¯¥æ–¹å‘çš„ç†è®ºç©ºç™½ï¼Œåˆä¸ºå·¥ä¸šç•Œè½åœ°é²æ£’ç­–ç•¥æä¾›äº†å®æ“æ€§æ´å¯Ÿ~

## when-maximum-entropy-misleads-policy-optimization
### Abstract
The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading
approach for achieving efficient learning and robust performance across many RL
tasks. However, MaxEnt methods have also been shown to struggle with
performance-critical control problems in practice, where non-MaxEnt algorithms
can successfully learn. In this work, we analyze how the trade-off between
robustness and optimality affects the performance of MaxEnt algorithms in
complex control tasks: while entropy maximization enhances exploration and
robustness, it can also mislead policy optimization, leading to failure in
tasks that require precise, low-entropy policies. Through experiments on a
variety of control problems, we concretely demonstrate this misleading effect.
Our analysis leads to better understanding of how to balance reward design and
entropy maximization in challenging control problems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å½“æœ€å¤§ç†µâ€œè¯¯å¯¼â€ç­–ç•¥ä¼˜åŒ–ï¼šå¤æ‚æ§åˆ¶ä»»åŠ¡ä¸‹MaxEnt RLçš„æ€§èƒ½å‰–æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ï¼ˆMaxEnt RLï¼‰æ¡†æ¶ï¼ˆå¦‚Soft - Actor Criticï¼ŒSACï¼‰åœ¨ä¼—å¤šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºé«˜æ•ˆå­¦ä¹ ä¸é²æ£’æ€§èƒ½ï¼Œå…¶ä¼˜åŠ¿åŒ…æ‹¬æ›´å¥½çš„æ¢ç´¢æ€§ã€ä¼˜åŒ– landscape çš„å¹³æ»‘æ€§ä»¥åŠå¯¹å¹²æ‰°çš„é²æ£’æ€§ç­‰ã€‚ç„¶è€Œåœ¨å®é™…å…³é”®æ€§èƒ½æ§åˆ¶é—®é¢˜ä¸­ï¼ŒMaxEnt æ–¹æ³•å´è¡¨ç°æŒ£æ‰ï¼Œé MaxEnt ç®—æ³•ï¼ˆå¦‚PPOï¼‰åè€Œèƒ½æˆåŠŸå­¦ä¹ ã€‚ç°å®ä¸­å¾ˆå¤šåŸºäº RL çš„æœºå™¨äººæ§åˆ¶å·¥ä½œä»é‡‡ç”¨æ¨¡ä»¿å­¦ä¹ ç»“åˆé MaxEnt æ–¹æ³•å¾®è°ƒçš„æ–¹å¼ï¼ŒSAC å³ä¾¿è°ƒå‚åæ€§èƒ½ä¹Ÿå¸¸é€Šäº PPOã€‚ä¾‹å¦‚å››æ—‹ç¿¼æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œåœ¨ç®€åŒ–åŠ¨åŠ›å­¦æ¨¡å‹ä¸‹ SAC èƒ½å¿«é€Ÿå­¦ä¹ ç¨³å®šæ§åˆ¶å™¨ï¼Œè€Œåœ¨æ›´çœŸå®åŠ¨åŠ›å­¦æ¨¡å‹ä¸‹å´å¤±è´¥ï¼ŒPPO å´èƒ½æˆåŠŸã€‚æœ¬æ–‡åŠ¨æœºåœ¨äºåˆ†æé²æ£’æ€§ä¸æœ€ä¼˜æ€§çš„æƒè¡¡å¦‚ä½•å½±å“ MaxEnt ç®—æ³•åœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæ­ç¤ºæœ€å¤§ç†µåœ¨ä½•æ—¶ä¼šè¯¯å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºæœ€å¤§ç†µä¸å›æŠ¥æœ€å¤§åŒ–çš„å†²çªæœºåˆ¶
åˆ†æåœ¨éœ€è¦ç²¾ç¡®ã€ä½ç†µç­–ç•¥çš„å…³é”®æ€§èƒ½æ§åˆ¶é—®é¢˜ä¸­ï¼Œæœ€å¤§ç†µä¸æ•´ä½“å›æŠ¥æœ€å¤§åŒ–çš„å†²çªä¼šè¢«æ”¾å¤§å¹¶é˜»ç¢å­¦ä¹ ã€‚å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­è¾¾æˆæœŸæœ›æ€§èƒ½å¸¸éœ€åœ¨å…³é”®çŠ¶æ€æ‰§è¡Œç²¾ç¡®åŠ¨ä½œï¼Œè¿™äº›çŠ¶æ€ä¸‹çœŸå®æœ€ä¼˜ç­–ç•¥æœ¬å°±ä½ç†µï¼›è€Œåç¦»å¯è¡ŒåŠ¨ä½œé›†çš„åŠ¨ä½œä¼šå¯¼è‡´ä¸å¯æ¢å¤çŠ¶æ€ï¼ŒMaxEnt å´å¯èƒ½å› çŸ­æœŸ â€œç†µæ”¶ç›Šâ€ åå‘è¿™äº›æ¬¡ä¼˜è¡Œä¸ºï¼Œä½¿æ™ºèƒ½ä½“åç¦»è§£å†³ç¡¬æ§åˆ¶é—®é¢˜çš„å…³é”® â€”â€” ç²¾ç¡®ä½ç†µæœ€ä¼˜ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå½¢å¼åŒ–åˆ†æç†µé™·é˜± â€”â€” ç†µåˆ†å‰æ‰©å±•ï¼ˆEntropy Bifurcation Extensionï¼‰
å¯¹ä»»æ„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œæå‡ºç†µåˆ†å‰æ‰©å±•æ¥æ˜¾å¼æ„é€ ç†µé™·é˜±ï¼šä½¿å¾— MaxEnt æ–¹æ³•ä¼šè¢«è¯¯å¯¼è®¤ä¸ºä»»æ„ç­–ç•¥åˆ†å¸ƒæ˜¯ MaxEnt æœ€ä¼˜çš„ï¼Œè€ŒçœŸå®æœ€ä¼˜ç­–ç•¥ä¸å—è¯¥æ‰©å±•å½±å“ã€‚è¿™å¹¶éè®­ç»ƒæ—¶çš„æ ·æœ¬æ•ˆç‡æˆ–æ¢ç´¢åå·®é—®é¢˜ï¼Œè€Œæ˜¯ MaxEnt ç®—æ³•æ”¶æ•›åçš„ç»“æœï¼Œè¯´æ˜ç†µçš„è¯¯å¯¼æ•ˆåº”åœ¨æ ‡å‡†ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ä¸å—å½±å“çš„åœºæ™¯ä¸‹ä¹Ÿæ˜“å‡ºç°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šåœºæ™¯å®éªŒéªŒè¯ä¸åˆ†æ
åœ¨å¤šç§çœŸå®æ§åˆ¶ç¯å¢ƒï¼ˆé«˜é€Ÿè½®å¼è½¦è¾†æ§åˆ¶ã€å››æ—‹ç¿¼è½¨è¿¹è·Ÿè¸ªã€å¯¹åº”ç¡¬ä»¶å¹³å°çš„å››è¶³æœºå™¨äººæ§åˆ¶ç­‰ï¼‰ä¸­åˆ†æ SAC è¡Œä¸ºï¼Œå±•ç¤º MaxEnt å’Œå¸¸è§„ç­–ç•¥ä¼˜åŒ–ä¸‹ä»·å€¼ landscape çš„å·®å¼‚å¦‚ä½•è§£é‡Š SAC åœ¨è¿™äº›ç¯å¢ƒä¸­æ”¶æ•›åˆ°å¯è¡Œæ§åˆ¶ç­–ç•¥çš„å›°éš¾ï¼›åŒæ—¶ä¹Ÿè¯´æ˜è¯¥åˆ†æèƒ½å¸®åŠ©ç†è§£ä¸ºä½• MaxEnt åœ¨å—ç›Šäºé²æ£’æ¢ç´¢çš„ç¯å¢ƒï¼ˆå¦‚ OpenAI Gym å¸¸è§åŸºå‡†ç¯å¢ƒï¼‰ä¸­å­¦ä¹ æˆåŠŸï¼Œä¸ºå¤æ‚æ§åˆ¶é—®é¢˜ä¸­ MaxEnt ç®—æ³•çš„å¥–åŠ±è®¾è®¡å’Œè¶…å‚æ•°è°ƒä¼˜æä¾›æŒ‡å¯¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å››æ—‹ç¿¼æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œå¯¹æ¯” SAC å’Œ PPOï¼šç®€åŒ–åŠ¨åŠ›å­¦æ¨¡å‹ä¸‹ SAC èƒ½å¿«é€Ÿå­¦ä¹ ç¨³å®šæ§åˆ¶å™¨ï¼ŒçœŸå®åŠ¨åŠ›å­¦æ¨¡å‹ä¸‹ SAC å¤±è´¥è€Œ PPO æˆåŠŸï¼›åœ¨é«˜é€Ÿè½®å¼è½¦è¾†æ§åˆ¶ã€å››è¶³æœºå™¨äººæ§åˆ¶ç­‰çœŸå®æ§åˆ¶ç¯å¢ƒä¸­ï¼Œåˆ†æè¡¨æ˜ MaxEnt ä¸‹ä»·å€¼ landscape ä¸å¸¸è§„ç­–ç•¥ä¼˜åŒ–çš„å·®å¼‚å¯¼è‡´ SAC éš¾æ”¶æ•›åˆ°å¯è¡Œæ§åˆ¶ç­–ç•¥ï¼›é€šè¿‡è¿™äº›å®éªŒå…·ä½“å±•ç¤ºäº†ç†µçš„è¯¯å¯¼æ•ˆåº”åœ¨å®è·µä¸­å¦‚ä½•å½±å“å­¦ä¹ ï¼Œä¹ŸéªŒè¯äº†ç†µåˆ†å‰æ‰©å±•ç­‰åˆ†æçš„åˆç†æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹äºå¼ºåŒ–å­¦ä¹ ç ”ç©¶è€…ï¼Œæœ¬æ–‡è®©å…¶æ›´æ·±å…¥ç†è§£ MaxEnt RL åœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œæ˜ç™½é²æ£’æ€§ä¸æœ€ä¼˜æ€§æƒè¡¡åœ¨ MaxEnt åœºæ™¯ä¸‹çš„å…·ä½“è¡¨ç°ï¼Œä¸ºåç»­æ”¹è¿› MaxEnt ç®—æ³•ï¼ˆå¦‚è°ƒæ•´å¥–åŠ±è®¾è®¡ã€è¶…å‚æ•° tuning æ–¹å‘ï¼‰æä¾›ç†è®ºä¸å®éªŒä¾æ®ã€‚
2. é’ˆå¯¹æœºå™¨äººæ§åˆ¶ç­‰å®é™…åº”ç”¨é¢†åŸŸä»ä¸šè€…ï¼Œè§£é‡Šäº†ä¸ºä½•åœ¨çœŸå®å¤æ‚æ§åˆ¶åœºæ™¯ä¸­ SAC ç­‰ MaxEnt æ–¹æ³•æœ‰æ—¶è¡¨ç°ä¸ä½³ï¼Œå¸®åŠ©å…¶åœ¨æ–¹æ³•é€‰æ‹©ï¼ˆå¦‚ä½•æ—¶é€‰ PPO ä½•æ—¶å°è¯•æ”¹è¿› MaxEntï¼‰å’Œç®—æ³•è°ƒä¼˜ä¸Šåšå‡ºæ›´åˆç†å†³ç­–ã€‚
3. ä»æ–¹æ³•è®ºè§’åº¦ï¼Œæœ¬æ–‡æå‡ºçš„ç†µåˆ†å‰æ‰©å±•ç­‰åˆ†ææ–¹å¼ä¸ºç ”ç©¶å¼ºåŒ–å­¦ä¹ ä¸­ç›®æ ‡å‡½æ•°ç»„ä»¶ï¼ˆå¦‚ç†µé¡¹ï¼‰å¯¹ç­–ç•¥ä¼˜åŒ–çš„å½±å“æä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§å½¢å¼åŒ–åˆ†æä¸å®éªŒéªŒè¯ç»“åˆçš„æ–¹å¼æ¥å‰–æå…¶ä»– RL ç®—æ³•ç»„ä»¶çš„ä½œç”¨ã€‚

## causal-policy-learning-in-reinforcement-learning--backdoor-adjusted-soft-actor-critic
### Abstract
Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å› æœå¼ºåŒ–å­¦ä¹ æ–°çªç ´ï¼šDoSAC è§£å†³éšè—æ··æ·†åå·®

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æœºå™¨äººã€è¿ç»­æ§åˆ¶ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…¶ä¸­ Soft Actor - Criticï¼ˆSACï¼‰å› é«˜æ•ˆæ¢ç´¢å’Œè‰¯å¥½æ€§èƒ½ï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œè¿ç»­åŠ¨ä½œç©ºé—´ä»»åŠ¡ã€‚ä½†å¤šæ•° RL ç®—æ³•ï¼ˆåŒ…æ‹¬ SACï¼‰ä¾èµ–è§‚æµ‹æ•°æ®ï¼Œå‡è®¾çŠ¶æ€ - åŠ¨ä½œè½¬ç§»æ˜¯å› æœå…³ç³»ï¼Œå½“å­˜åœ¨**éšè—æ··æ·†å˜é‡**ï¼ˆåŒæ—¶å½±å“æ™ºèƒ½ä½“çŠ¶æ€è§‚æµ‹å’ŒåŠ¨ä½œçš„æœªè§‚æµ‹å˜é‡ï¼‰æ—¶ï¼Œè¿™ç§å‡è®¾ä¸æˆç«‹ã€‚éšè—æ··æ·†å˜é‡ä¼šå¼•å…¥åå·®ï¼ŒæŸå®³ä»·å€¼å‡½æ•°å’Œç­–ç•¥å¯é æ€§ï¼Œé™åˆ¶æ³›åŒ–æ€§ä¸é²æ£’æ€§ã€‚è€Œå› æœæ¨æ–­ä¸­çš„ do - calculus å’Œåé—¨å‡†åˆ™ä¸ºè§£å†³æ··æ·†é—®é¢˜æä¾›äº†æ€è·¯ï¼Œæœ¬æ–‡å—æ­¤å¯å‘ï¼Œæå‡º DoSAC æ¥ä¿®æ­£éšè—æ··æ·†åå·®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º DoSAC ç®—æ³•
DoSAC æ˜¯ SAC çš„å› æœæ„ŸçŸ¥æ‰©å±•ï¼Œé€šè¿‡åé—¨è°ƒæ•´æ˜¾å¼ä¼°è®¡å¹²é¢„ç­–ç•¥ä»¥ä¿®æ­£éšè—æ··æ·†ã€‚ä¸åŒäº SAC åŸºäºå¯èƒ½æœ‰åçš„è§‚æµ‹åˆ†å¸ƒ Ï€(a|s) ä¼˜åŒ–ç­–ç•¥ï¼ŒDoSAC ç›´æ¥ç„å‡†å¹²é¢„åˆ†å¸ƒ Ï€(a|do(s))ï¼Œæ•æ‰çŠ¶æ€å¯¹åŠ¨ä½œçš„çœŸå®å› æœå½±å“ï¼Œåœ¨å­˜åœ¨åˆ†å¸ƒåç§»åœºæ™¯ä¸‹å®ç°æ›´é²æ£’ã€æ³›åŒ–çš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡ Backdoor Reconstructor æ¨¡å—
å¼•å…¥å¯å­¦ä¹ çš„ Backdoor Reconstructorï¼Œä»å½“å‰çŠ¶æ€æ¨æ–­**ä¼ªè¿‡å»å˜é‡**ï¼ˆä¹‹å‰çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼‰ã€‚è¿™äº›æ¨æ–­å˜é‡ä½œä¸ºä»£ç†è°ƒæ•´é‡æ»¡è¶³åé—¨å‡†åˆ™ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£æˆ–è®¿é—®æ½œåœ¨æ··æ·†å˜é‡ï¼Œå°±èƒ½ä»è§‚æµ‹æ•°æ®ä¼°è®¡å¹²é¢„æ•ˆåº”ï¼Œå‡å°‘æ™ºèƒ½ä½“è®­ç»ƒæ—¶çš„æ··æ·†åå·®ï¼Œè¿˜èƒ½åŸºäºé‡æ”¾ç¼“å†²åŒºè§‚æµ‹æ•°æ®å¯¹å¹²é¢„ç­–ç•¥é‡‡æ ·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé«˜æ•ˆé›†æˆä¸æ³›åŒ–æ€§
DoSAC æ— ç¼é›†æˆåˆ°æ ‡å‡† SAC è®­ç»ƒæµç¨‹ï¼Œå¯ç«¯åˆ°ç«¯é«˜æ•ˆè®­ç»ƒï¼Œæ— é¢å¤–å¼€é”€æˆ–æ•°æ®è¦æ±‚ï¼›ä¸”æ˜¯ SAC çš„æ³›åŒ–ï¼Œå½“æ— æ··æ·†å˜é‡å½±å“åŠ¨ä½œæ‰§è¡Œæ—¶ï¼Œè‡ªç„¶é€€åŒ–ä¸ºåŸå§‹ SAC å½¢å¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDoSAC åœ¨æ··æ·†è®¾ç½®ä¸‹è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼ˆå¦‚æ ‡å‡† SAC ç­‰ï¼‰ï¼Œåœ¨é²æ£’æ€§ã€æ³›åŒ–æ€§å’Œç­–ç•¥å¯é æ€§æ–¹é¢å‡æœ‰æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨å—åˆæˆæ··æ·†å½±å“çš„è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æ”¹è¿›ç­–ç•¥é²æ£’æ€§ä¸æ³›åŒ–æ€§çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å› æœä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼šä¸ºè§£å†³ RL ä¸­éšè—æ··æ·†é—®é¢˜æä¾›äº†å› æœè§†è§’çš„æ€è·¯ï¼Œå°†å› æœæ¨æ–­çš„åé—¨è°ƒæ•´ç­‰æ¦‚å¿µå¼•å…¥ RL ç­–ç•¥å­¦ä¹ ï¼Œå¯å‘åç»­åœ¨æœ‰åè§‚æµ‹åœºæ™¯ä¸‹çš„ RL ç®—æ³•è®¾è®¡ã€‚
2. æ¨¡å—è®¾è®¡æ€è·¯ï¼šBackdoor Reconstructor è¿™ç§ä»å½“å‰çŠ¶æ€æ¨æ–­ä»£ç†å˜é‡ä»¥æ»¡è¶³å› æœå‡†åˆ™çš„æ¨¡å—è®¾è®¡ï¼Œä¸ºå¤„ç†æ— æ˜¾å¼æ··æ·†å˜é‡å¯ç”¨åœºæ™¯æä¾›äº†å¯å€Ÿé‰´çš„â€œä»£ç†è°ƒæ•´â€èŒƒå¼ã€‚
3. ç®—æ³•å…¼å®¹æ€§ï¼šDoSAC å¯¹ SAC çš„æ— ç¼é›†æˆä¸æ³›åŒ–ç‰¹æ€§ï¼Œå±•ç¤ºäº†åœ¨ç»å…¸ RL ç®—æ³•åŸºç¡€ä¸Šè¿›è¡Œå› æœå¢å¼ºæ—¶ï¼Œå¦‚ä½•å…¼é¡¾åŸæœ‰ä¼˜åŠ¿ä¸æ–°åŠŸèƒ½æ‹“å±•ï¼Œä¸ºæ”¹è¿›å…¶ä»– RL ç®—æ³•æä¾›äº†é›†æˆæ€è·¯ã€‚

## reinforcing-video-reasoning-with-focused-thinking
### Abstract
Recent advancements in reinforcement learning, particularly through Group
Relative Policy Optimization (GRPO), have significantly improved multimodal
large language models for complex reasoning tasks. However, two critical
limitations persist: 1) they often produce unfocused, verbose reasoning chains
that obscure salient spatiotemporal cues and 2) binary rewarding fails to
account for partially correct answers, resulting in high reward variance and
inefficient learning. In this paper, we propose TW-GRPO, a novel framework that
enhances visual reasoning with focused thinking and dense reward granularity.
Specifically, we employs a token weighting mechanism that prioritizes tokens
with high informational density (estimated by intra-group information entropy),
suppressing redundant tokens like generic reasoning prefixes. Furthermore, we
reformulate RL training by shifting from single-choice to multi-choice QA
tasks, where soft rewards enable finer-grained gradient estimation by
distinguishing partial correctness. Additionally, we propose question-answer
inversion, a data augmentation strategy to generate diverse multi-choice
samples from existing benchmarks. Experiments demonstrate state-of-the-art
performance on several video reasoning and general understanding benchmarks.
Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over
Video-R1) and 65.8\% on MMVU. Our codes are available at
\href{https://github.com/longmalongma/TW-GRPO}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | èšç„¦æ€è€ƒå¼ºåŒ–è§†é¢‘æ¨ç†ï¼šTW - GRPO æ¡†æ¶é©æ–°å¤šæ¨¡æ€å¤§æ¨¡å‹è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯ Group Relative Policy Optimizationï¼ŒGRPOï¼‰åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†ä»»åŠ¡è¡¨ç°ä¸Šå–å¾—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸¤å¤§å…³é”®å±€é™ï¼šä¸€æ˜¯ç”Ÿæˆçš„æ¨ç†é“¾å¸¸ä¸èšç„¦ã€å†—é•¿ï¼Œæ©ç›–äº†å…³é”®æ—¶ç©ºçº¿ç´¢ï¼›äºŒæ˜¯äºŒå…ƒå¥–åŠ±æœºåˆ¶æ— æ³•è€ƒé‡éƒ¨åˆ†æ­£ç¡®ç­”æ¡ˆï¼Œå¯¼è‡´å¥–åŠ±æ–¹å·®å¤§ã€å­¦ä¹ ä½æ•ˆã€‚åœ¨è§†é¢‘æ¨ç†ç­‰å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼ŒåŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ¨ç†æ˜“äº§ç”Ÿå†—é•¿æ— ç„¦ç‚¹çš„æ€è€ƒé“¾ï¼Œç°æœ‰è®­ç»ƒç›®æ ‡ä¹Ÿéš¾ä¼˜å…ˆå…³æ³¨è¯­ä¹‰å…³é”®çš„æ—¶ç©ºçº¿ç´¢ï¼›ä¸”ç°æœ‰æ–¹æ³•ä¾èµ–å•é€‰æ‹©é—®ç­”ä»»åŠ¡çš„ç¨€ç–äºŒå…ƒå¥–åŠ±ï¼Œå¿½ç•¥éƒ¨åˆ†æ­£ç¡®æ€§ï¼Œè€Œè§†é¢‘ QA ä¸»æµä»»åŠ¡ä¸­æ­¤ç±»å•é€‰æ‹©æ ¼å¼ç¼ºä¹è‡ªç„¶å®šä¹‰çš„å¤šæ°´å¹³å¥–åŠ±ä¿¡å·ï¼Œè¿™äº›éƒ½é™åˆ¶äº†æ¨¡å‹æ€§èƒ½æå‡ï¼Œå› æ­¤æœ¬æ–‡æå‡º TW - GRPO æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€ token åŠ æƒæœºåˆ¶
é€šè¿‡åˆ†æ token ä½ç½®çš„ç»„å†…ä¿¡æ¯ç†µæ¥ä¼°è®¡ token é‡è¦æ€§ï¼Œåœ¨æŸå¤±è®¡ç®—æ—¶ä¼˜å…ˆè€ƒè™‘ä¿¡æ¯å¯†åº¦é«˜çš„ tokenï¼ŒæŠ‘åˆ¶é€šç”¨æ¨ç†å‰ç¼€ç­‰å†—ä½™ tokenï¼Œè®©æ¨¡å‹ç”Ÿæˆç®€æ´ä¸”é¢å‘ä»»åŠ¡çš„æ¨ç†é“¾ï¼Œé¿å…å†—ä½™æˆ–æ— å…³ç»†èŠ‚ï¼Œå°†æ¨¡å‹æ³¨æ„åŠ›èšç„¦åˆ°å¯¹æ¨ç†ç»“æœå…³é”®çš„å†…å®¹ä¸Šã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç²’åº¦å¥–åŠ±å»ºæ¨¡ä¸é—®ç­”ä»»åŠ¡é‡æ„
å°†å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»å•é€‰æ‹©é—®ç­”ä»»åŠ¡è½¬ä¸ºå¤šé€‰æ‹©é—®ç­”ä»»åŠ¡ï¼Œç”¨å¤šæ°´å¹³å¥–åŠ±æ›¿ä»£ç¨€ç–äºŒå…ƒå¥–åŠ±ã€‚åŒºåˆ†éƒ¨åˆ†æ­£ç¡®å’Œå®Œå…¨é”™è¯¯ç­”æ¡ˆï¼Œå®ç°æ›´ç»†ç²’åº¦çš„æ¢¯åº¦ä¼°è®¡å¹¶ç¨³å®šç­–ç•¥æ›´æ–°ï¼›åŒæ—¶ä¸ºç¼“è§£å¤šé€‰æ‹©æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡º Question - Answer Inverseï¼ˆQAIï¼‰æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡å¦å®šé—®é¢˜å’Œåè½¬ç­”æ¡ˆå°†å•é€‰æ‹©ä»»åŠ¡è½¬åŒ–ä¸ºå¤šé€‰æ‹©æ ¼å¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªè§†é¢‘æ¨ç†å’Œé€šç”¨ç†è§£åŸºå‡†æµ‹è¯•ä¸­å±•ç°å“è¶Šæ€§èƒ½ï¼Œåœ¨ CLEVRER åŸºå‡†ä¸Šè¾¾åˆ° 50.4% å‡†ç¡®ç‡ï¼ˆè¾ƒ Video - R1 æå‡ 18.8%ï¼‰ï¼Œåœ¨ MMVU ä¸Šè¾¾åˆ° 65.8% å‡†ç¡®ç‡ï¼Œåœ¨ NExT - GQA ç­‰åŸºå‡†ä¹Ÿå®ç° state - of - the - art è¡¨ç°ï¼Œä¸”èšç„¦æ€è€ƒè®©æ¨ç†é“¾æ›´å‡ç»ƒï¼Œå…³æ³¨å…³é”®è§†è§‰æˆ–é€»è¾‘çº¿ç´¢ï¼Œå¤šæ°´å¹³å¥–åŠ±é™ä½äº†è®­ç»ƒæ—¶çš„å¥–åŠ±æ–¹å·®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æŠ€æœ¯åˆ›æ–°è§’åº¦ï¼ŒåŠ¨æ€ token åŠ æƒä¸ºä¼˜åŒ–æ¨¡å‹æ¨ç†æ—¶çš„ token å…³æ³¨åº¦æä¾›äº†æ–°æ€è·¯ï¼Œå¯å¯å‘åç»­åœ¨å¤„ç†åºåˆ—ç”Ÿæˆä»»åŠ¡æ—¶å¦‚ä½•æ›´ç²¾å‡†èšç„¦å…³é”®ä¿¡æ¯ï¼›å¤šç²’åº¦å¥–åŠ±å»ºæ¨¡å°†éƒ¨åˆ†æ­£ç¡®æ€§çº³å…¥è€ƒé‡ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€ QA ä»»åŠ¡ä¸­æ›´ç²¾ç»†ä¼˜åŒ–æä¾›æ–¹å‘ï¼›é—®ç­”åè½¬çš„æ•°æ®å¢å¼ºç­–ç•¥åˆ™ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†å·§å¦™çš„è½¬æ¢æ€è·¯ï¼Œè¿™äº›åˆ›æ–°ç‚¹åœ¨å¤šæ¨¡æ€å¤§æ¨¡å‹ä¼˜åŒ–ã€å¤æ‚ä»»åŠ¡æ¨ç†ç­‰é¢†åŸŸéƒ½æœ‰å¾ˆå¥½çš„å€Ÿé‰´ä»·å€¼ï¼Œå¯æŒ‡å¯¼ç ”ç©¶è€…åœ¨æ¨¡å‹è®­ç»ƒæœºåˆ¶ã€æ•°æ®å¤„ç†ç­‰æ–¹é¢è¿›è¡Œæ”¹è¿›æ¢ç´¢ã€‚

## the-hallucination-dilemma--factuality-aware-reinforcement-learning-for-large-reasoning-models
### Abstract
Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§æ¨¡å‹æ¨ç†ä¸­å¹»è§‰å›°å¢ƒçš„ç ´å±€ï¼šé¢å‘äº‹å®æ€§çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–åœ¨æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œèƒ½åœ¨æ•°å­¦ã€å¤šè·³é—®ç­”ç­‰å¤æ‚åŸºå‡†æµ‹è¯•ä¸­å±•ç°å¼ºå¤§èƒ½åŠ›ã€‚ä½†ç ”ç©¶å‘ç°ï¼Œ**é¢å‘æ¨ç†çš„RLå¾®è°ƒä¼šå¤§å¹…å¢åŠ å¹»è§‰ï¼ˆhallucinationsï¼Œå³ç”Ÿæˆäº‹å®é”™è¯¯æˆ–ç¼–é€ å†…å®¹ï¼‰å‡ºç°çš„æ¦‚ç‡**ã€‚è¿™ç§å¹»è§‰å¸¸æºäºä¸­é—´æ¨ç†æ­¥éª¤çš„é”™è¯¯ï¼Œå³ä¾¿æœ€ç»ˆç­”æ¡ˆå¶å°”æ­£ç¡®ï¼Œæ¨ç†é“¾é‡Œä¹Ÿå¯èƒ½å­˜åœ¨æ— ä¾æ®æˆ–é”™è¯¯çš„é™ˆè¿°ï¼Œä¸¥é‡å½±å“RLè®­ç»ƒæ¨ç†æ¨¡å‹çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§ã€‚ä¸ºæ¢ç©¶åŸå› ï¼Œä½œè€…ä»ç†è®ºåˆ†æRLè®­ç»ƒåŠ¨æ€ï¼Œå‘ç°é«˜æ–¹å·®æ¢¯åº¦ã€ç†µè¯±å¯¼çš„éšæœºæ€§ã€æ˜“é™·å…¥è™šå‡å±€éƒ¨æœ€ä¼˜è¿™ä¸‰ä¸ªå…³é”®å› ç´ å¯¼è‡´å¹»è§‰å¢å¤šï¼Œè¿›è€Œæå‡ºè§£å†³æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç†è®ºå‰–æRLè®­ç»ƒè‡´å¹»è§‰çš„æˆå›   
æ·±å…¥åˆ†æé¢å‘æ¨ç†ä»»åŠ¡çš„RLè®­ç»ƒåŠ¨æ€ï¼Œæ˜ç¡®ä¸‰å¤§è‡´å¹»å› ç´ ï¼šå…¶ä¸€ï¼Œä»…ä¼˜åŒ–æœ€ç»ˆæ­£ç¡®ç­”æ¡ˆä¼šå› æ­£ç¡®ç­”æ¡ˆç¨€ç¼ºå¯¼è‡´ç­–ç•¥æ¢¯åº¦æ–¹å·®æé«˜ï¼Œè®­ç»ƒæ›´æ–°ä¸ç¨³å®šï¼›å…¶äºŒï¼Œä¸ºæ¢ç´¢â€œæœ‰å¥–åŠ±çš„è¾“å‡ºâ€ï¼Œç­–ç•¥é¢„æµ‹éœ€ä¿æŒé«˜ç†µï¼Œå¢åŠ å¹»è§‰é£é™©ï¼›å…¶ä¸‰ï¼Œæ ‡å‡†RLç›®æ ‡æ˜“é™·å…¥è™šå‡å±€éƒ¨æœ€ä¼˜ï¼Œæ¨¡å‹å¯èƒ½æ”¶æ•›åˆ°è‡ªä¿¡ä½†é”™è¯¯ã€æ— å¥–åŠ±çš„ç­”æ¡ˆã€‚è¿™ä¸‰ç‚¹å…±åŒè§£é‡Šäº†çº¯ç»“æœå¯¼å‘RLå¼•å‘æ¨ç†æ¨¡å‹å¹»è§‰çš„æ ¹æºã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºFSPOç®—æ³•å®ç°äº‹å®æ€§æ„ŸçŸ¥çš„åˆ†æ­¥ç­–ç•¥ä¼˜åŒ–  
ä¸ºç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œæå‡º**Factuality - aware Step - wise Policy Optimizationï¼ˆFSPOï¼‰**ç®—æ³•ã€‚æ ¸å¿ƒæ˜¯åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤èå…¥æ˜¾å¼äº‹å®æ€§éªŒè¯ï¼Œç”¨åˆ†æ­¥äº‹å®æ€§å¥–åŠ±ä¿¡å·è°ƒæ•´å•ä¸ªtokençš„ä¼˜åŠ¿å€¼ï¼ˆadvantage valuesï¼‰ï¼Œå¼•å¯¼å¤§æ¨¡å‹ç”Ÿæˆæ›´å…·äº‹å®æ€§çš„å†…å®¹ã€‚å…·ä½“è€Œè¨€ï¼Œç”¨è‡ªåŠ¨éªŒè¯å™¨æ£€æŸ¥æ¯ä¸ªç”Ÿæˆçš„æ¨ç†è¯­å¥æ˜¯å¦è¢«ç»™å®šè¯æ®è•´å«ï¼Œå¾—åˆ°åˆ†æ­¥äº‹å®æ€§åˆ†æ•°ï¼›å°†è¿™äº›åˆ†æ•°æ•´åˆåˆ°æ•´ä½“å¥–åŠ±ä¿¡å·ä¸­ï¼Œä¼˜åŒ–æ—¶è°ƒæ•´tokençš„ä¼˜åŠ¿å€¼â€”â€”å¥–åŠ±äº‹å®æ­£ç¡®tokenï¼Œæƒ©ç½šé”™è¯¯tokenã€‚è¿™ç§äº‹å®æ„ŸçŸ¥çš„å¥–åŠ±å¡‘é€ ä¸ºç­–ç•¥æä¾›æ›´å¯†é›†ã€ä¿¡æ¯æ›´ä¸°å¯Œçš„åé¦ˆï¼Œè§£å†³ç¨€ç–ä¿¡å·é—®é¢˜å¹¶é™ä½è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œè®©ç­–ç•¥è¶‹å‘äºä¸ä»…ç­”æ¡ˆæ­£ç¡®ã€æ¨ç†é“¾ä¹Ÿå¿ å®å¯éªŒè¯çš„è§£ï¼Œç›´æ¥ç¼“è§£RLè®­ç»ƒä¸­çš„å¹»è§‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å­¦æ¨ç†å’Œå¹»è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨Qwen2.5ï¼ˆå¦‚7B - Base/Instructç­‰ç‰ˆæœ¬ï¼‰å’ŒLlamaæ¨¡å‹ï¼ˆå¦‚Llama - 3.1 - 8B - Instructï¼‰éªŒè¯FSPOã€‚ç»“æœæ˜¾ç¤ºï¼šFSPOæœ‰æ•ˆé™ä½äº†å¹»è§‰ï¼ŒåŒæ—¶æå‡äº†æ¨ç†å‡†ç¡®ç‡ï¼Œåœ¨å¯é æ€§å’Œæ€§èƒ½ä¸Šéƒ½æœ‰æ˜¾è‘—æ”¹è¿›ï¼›ä¸”èƒ½åœ¨ä¸æŸå®³ç”Ÿæˆå†…å®¹è´¨é‡çš„å‰æä¸‹ï¼Œå¢å¼ºæ¨ç†æ­¥éª¤çš„äº‹å®æ€§ã€‚  

åœ¨åˆæ­¥å®éªŒä¸­ï¼Œå¯¹æ¯”æœ‰æ— å¤§è§„æ¨¡RLè®­ç»ƒçš„å¤šç»„æ¨¡å‹ï¼ˆå¦‚DeepSeekç³»åˆ—ã€Qwenç³»åˆ—ã€Llamaç³»åˆ—ç­‰ï¼‰åœ¨TruthfulQAã€HaluEvalã€HalluQAç­‰å¹»è§‰åŸºå‡†çš„è¡¨ç°ï¼Œå‘ç°ç»RLæˆ–é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®å¤§è§„æ¨¡è®­ç»ƒåçš„æ¨¡å‹ï¼Œå¹»è§‰ç¨‹åº¦åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šéƒ½æ˜¾è‘—æ›´é«˜ï¼›å¯¹HaluEval - QAä¸­æ ·æœ¬çš„åˆ†æä¹Ÿè¡¨æ˜ï¼Œå¤šæ•°é”™è¯¯æºäºä¸­é—´æ¨ç†æ­¥éª¤é”™è¯¯ï¼Œä½è¯ç»“æœå¯¼å‘å¥–åŠ±å»ºæ¨¡æ˜“å¼•å‘å¹»è§‰çš„çŒœæƒ³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜åˆ†æè§’åº¦ï¼šä»RLè®­ç»ƒåŠ¨æ€çš„ç†è®ºå±‚é¢å‰–æå¤§æ¨¡å‹æ¨ç†ä¸­å¹»è§‰å¢å¤šçš„åŸå› ï¼Œä¸ºç†è§£RLä¸å¹»è§‰çš„å…³ç³»æä¾›äº†æ·±å…¥è§†è§’ï¼Œåç»­ç ”ç©¶å¯å€Ÿé‰´è¿™ç§ä»è®­ç»ƒæœºåˆ¶æœ¬è´¨æ‰¾é—®é¢˜çš„æ€è·¯ã€‚  
2. æ–¹æ³•åˆ›æ–°æ–¹å‘ï¼šFSPOå°†â€œåˆ†æ­¥äº‹å®æ€§éªŒè¯â€èå…¥RLå¾®è°ƒï¼Œé€šè¿‡å¥–åŠ±å¡‘é€ ä¼˜åŒ–tokençº§ä¼˜åŠ¿å€¼ï¼Œä¸ºè§£å†³RLè®­ç»ƒä¸­ç¨€ç–å¥–åŠ±ã€å¹»è§‰ç­‰é—®é¢˜æä¾›äº†æ–°é¢–çš„â€œè¿‡ç¨‹ç›‘ç£ + äº‹å®æ€§åé¦ˆâ€æ€è·¯ï¼Œå¯å¯å‘åç»­åœ¨å¼ºåŒ–å­¦ä¹ ä¸å¤§æ¨¡å‹å¯¹é½ã€å¯é æ€§æå‡æ–¹é¢çš„æ–¹æ³•è®¾è®¡ã€‚  
3. å®éªŒéªŒè¯æ€è·¯ï¼šåœ¨å¤šæ¨¡å‹ã€å¤šåŸºå‡†ï¼ˆæ¶µç›–æ•°å­¦æ¨ç†å’Œå¹»è§‰è¯„ä¼°ï¼‰ä¸Šå¼€å±•å®éªŒï¼Œå…¨é¢éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œè¿™ç§å¤šç»´åº¦å®éªŒéªŒè¯çš„æ–¹å¼å€¼å¾—åŒç±»ç ”ç©¶å‚è€ƒï¼Œä»¥å¢å¼ºç»“è®ºçš„è¯´æœåŠ›ã€‚

## on-policy-rl-with-optimal-reward-baseline
### Abstract
Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO integrates a practically feasible
formulation of the optimal reward baseline that minimizes gradient variance. We
evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its
superior performance and training stability without additional models or
regularization terms. Furthermore, OPO achieves lower policy shifts and higher
output entropy, encouraging more diverse and less repetitive responses. These
results highlight OPO as a promising direction for stable and effective
reinforcement learning in large language model alignment and reasoning tasks.
The implementation is merged into the verl library at
https://verl.readthedocs.io/en/latest/algo/opo.html.


## enhanced-dacer-algorithm-with-high-diffusion-efficiency
### Abstract
Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜æ•ˆæ‰©æ•£çš„å¢å¼ºå‹DACERç®—æ³•ï¼šDACER2å¦‚ä½•çªç ´æ•ˆç‡ç“¶é¢ˆï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ‰©æ•£æ¨¡å‹å‡­å€Ÿå¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ï¼Œåœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡ä»¿å­¦ä¹ ä¸­å±•ç°æ½œåŠ›ï¼Œè€ŒDACERç®—æ³•å°†å…¶æ‹“å±•åˆ°åœ¨çº¿RLé¢†åŸŸï¼Œç”¨åå‘æ‰©æ•£è¿‡ç¨‹åšç­–ç•¥è¿‘ä¼¼å™¨å¹¶ç»“åˆç­–ç•¥æ¢¯åº¦ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå–å¾—ä¸é”™æ€§èƒ½ã€‚ä½†DACERå­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼šéœ€è¦å¤§é‡æ‰©æ•£æ­¥éª¤ï¼Œä¸¥é‡å½±å“è®­ç»ƒæ•ˆç‡ï¼›ç›´æ¥å‡å°‘æ­¥éª¤åˆä¼šè®©æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ¨ç†æ•ˆç‡ä¸è¶³æˆä¸ºå®æ—¶åœ¨çº¿RLåœºæ™¯åº”ç”¨æ‰©æ•£ç­–ç•¥çš„ç“¶é¢ˆã€‚æ‰€ä»¥ï¼Œå¦‚ä½•åœ¨ä¿æŒç”šè‡³æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œæå‡è®­ç»ƒä¸æ¨ç†æ•ˆç‡ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºQ - æ¢¯åº¦åœºç›®æ ‡ä½œä¸ºè¾…åŠ©ä¼˜åŒ–ç›®æ ‡  
ä¸ºäº†åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­å¼•å¯¼å»å™ªè¿‡ç¨‹ã€æå‡æ‰©æ•£ç­–ç•¥è®­ç»ƒæ•ˆæœï¼Œè®ºæ–‡å¼•å…¥Q - æ¢¯åº¦åœºç›®æ ‡è¿™ä¸€è¾…åŠ©ä¼˜åŒ–ç›®æ ‡ï¼Œä»¥æ­¤å¢å¼ºæ‰©æ•£ç­–ç•¥åœ¨è®­ç»ƒé˜¶æ®µçš„è¡¨ç°ï¼Œè®©æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½æ›´å¥½åœ°åˆ©ç”¨ä»·å€¼ä¿¡æ¯æ¥ä¼˜åŒ–ç­–ç•¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡æ—¶é—´åŠ æƒæœºåˆ¶  
å‘ç°Q - æ¢¯åº¦åœºä¸æ‰©æ•£æ—¶é—´æ­¥ç‹¬ç«‹ä¼šå¯¹æ‰©æ•£ç­–ç•¥æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“åï¼Œè®ºæ–‡æå‡ºæ—¶é—´åŠ æƒå‡½æ•°w(t)ï¼Œå°†å½“å‰æ‰©æ•£æ—¶é—´æ­¥ä½œä¸ºè¾“å…¥ã€‚è¯¥æœºåˆ¶èƒ½è®©æ¨¡å‹åœ¨æ‰©æ•£æ—©æœŸé«˜æ•ˆæ¶ˆé™¤å¤§è§„æ¨¡å™ªå£°ï¼Œåœ¨åæœŸå¯¹åŠ¨ä½œè¿›è¡Œç²¾ç»†ä¼˜åŒ–ï¼Œå¥‘åˆå»å™ªè¿‡ç¨‹ä¸åŒé˜¶æ®µçš„éœ€æ±‚ï¼ˆå‰æœŸéœ€è¦å¤§æŒ¯å¹…å¤„ç†ã€åæœŸéœ€è¦å°æŒ¯å¹…ç²¾å‡†æ§åˆ¶ï¼‰ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MuJoCoåŸºå‡†æµ‹è¯•å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå¼€å±•å®éªŒï¼šåœ¨MuJoCoæ§åˆ¶ä»»åŠ¡ä¸­ï¼ŒDACER2ä»…ç”¨5ä¸ªæ‰©æ•£æ­¥éª¤å°±åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼›ä¸DACERç›¸æ¯”ï¼ŒDACER2å±•ç°å‡ºæ›´å¼ºçš„å¤šæ¨¡æ€ç‰¹æ€§ï¼›åœ¨è®­ç»ƒå’Œæ¨ç†æ•ˆç‡æ–¹é¢ï¼Œç›¸åŒç¡¬ä»¶é…ç½®PyTorchæ¡†æ¶ä¸‹ï¼ŒDACER2æ¨ç†æ¯”DACERå¿«60.6%ã€è®­ç»ƒå¿«41.7%ï¼Œä¸”æ¨ç†æ•ˆç‡åœ¨åŒç±»ç®—æ³•ä¸­æœ€å¿«ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¾…åŠ©ä¼˜åŒ–ç›®æ ‡è®¾è®¡æ€è·¯ï¼šå½“ä¸»æ–¹æ³•å­˜åœ¨è®­ç»ƒå¼•å¯¼ä¸è¶³é—®é¢˜æ—¶ï¼Œå¯è€ƒè™‘å¼•å…¥å…³è”é¢†åŸŸï¼ˆå¦‚ä»·å€¼å‡½æ•°é¢†åŸŸï¼‰çš„ä¿¡æ¯æ„å»ºè¾…åŠ©ç›®æ ‡ï¼Œè¾…åŠ©ä¸»è¿‡ç¨‹è®­ç»ƒï¼Œæå‡æ•ˆæœã€‚  
2. æ—¶é—´ç»´åº¦åŠ¨æ€æœºåˆ¶ï¼šåœ¨æ¶‰åŠæ—¶é—´æ­¥è¿­ä»£çš„è¿‡ç¨‹ï¼ˆå¦‚æ‰©æ•£æ­¥éª¤ï¼‰ä¸­ï¼Œè‹¥å‘ç°è¿‡ç¨‹ä¸æ—¶é—´æ­¥ç‹¬ç«‹æ€§å¸¦æ¥è´Ÿé¢å½±å“ï¼Œå¯è®¾è®¡æ—¶é—´åŠ æƒç­‰åŠ¨æ€æœºåˆ¶ï¼Œè®©ä¸åŒé˜¶æ®µé’ˆå¯¹æ€§å¤„ç†ï¼Œè¿™åœ¨åºåˆ—å¼ã€è¿­ä»£å¼ä»»åŠ¡ä¸­å…·æœ‰å‚è€ƒä»·å€¼ã€‚  
3. å¤šç»´åº¦è¯„ä¼°å®è·µï¼šè®ºæ–‡ä¸ä»…å…³æ³¨æ€§èƒ½ï¼Œè¿˜å¯¹è®­ç»ƒå’Œæ¨ç†æ—¶é—´ç­‰æ•ˆç‡ç»´åº¦åœ¨ç»Ÿä¸€ç¡¬ä»¶é…ç½®ä¸‹è¯„ä¼°ï¼Œè¿™ç§å¤šç»´åº¦ã€å…¬å¹³å¯¹æ¯”çš„å®éªŒæ€è·¯ï¼Œèƒ½æ›´å…¨é¢å±•ç°ç®—æ³•ä¼˜åŠ¿ï¼Œå€¼å¾—åç»­ç ”ç©¶å€Ÿé‰´ã€‚

## bigger--regularized--categorical--high-capacity-value-functions-are-efficient-multi-task-learners
### Abstract
Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.


## maximizing-confidence-alone-improves-reasoning
### Abstract
Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen,
Mistral, and Llama families. The generality of our unsupervised learning method
lends itself to applicability in a wide range of domains where external
supervision is unavailable.


## the-entropy-mechanism-of-reinforcement-learning-for-reasoning-language-models
### Abstract
This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.


## skywork-open-reasoner-1-technical-report
### Abstract
The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Skywork-OR1ï¼šé¢å‘é•¿æ€ç»´é“¾å¤§æ¨¡å‹çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒåæŠ€æœ¯åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›ä¸Šå–å¾—é‡å¤§çªç ´ï¼ŒåƒDeepSeek - R1ç­‰æ¨¡å‹å±•ç°å‡ºRLåœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸Šæå‡æ€§èƒ½çš„æ½œåŠ›ã€‚ä¸è¿‡ï¼Œå½“å‰å¤šæ•°ç ”ç©¶èšç„¦äºå°†RLåº”ç”¨åœ¨åŸºç¡€æ¨¡å‹ï¼Œè€Œéå·²å®Œæˆæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨¡å‹ï¼Œä¸”å¯¹é•¿CoTæ¨¡å‹ç”¨RLé«˜æ•ˆä¸”å¯æ‰©å±•æå‡æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å°šä¸æ˜ç¡®ã€‚åŒæ—¶ï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡è‡³å…³é‡è¦ï¼Œè¿‡æ—©çš„ç†µåç¼©ï¼ˆè¿‡åº¦åˆ©ç”¨ï¼‰ä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼Œè€Œå¦‚ä½•åº”å¯¹è¿™ä¸€é—®é¢˜ä¹Ÿéœ€æ·±å…¥ç ”ç©¶ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼ŒSkyworkå›¢é˜Ÿæå‡ºSkywork - OR1ï¼Œæ—¨åœ¨ä¸ºé•¿CoTæ¨¡å‹æä¾›é«˜æ•ˆå¯æ‰©å±•çš„RLæ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé’ˆå¯¹é•¿CoTæ¨¡å‹çš„é«˜æ•ˆRLå®ç°  
åŸºäºDeepSeek - R1 - Distillæ¨¡å‹ç³»åˆ—ï¼Œæå‡ºSkywork - OR1è¿™ä¸€é€‚ç”¨äºé•¿æ€ç»´é“¾æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆã€‚é€šè¿‡è¯¥æ–¹æ¡ˆåœ¨æ¨¡å‹è®­ç»ƒä¸­å®ç°æ€§èƒ½æå‡ï¼Œæ¯”å¦‚32Bæ¨¡å‹åœ¨AIME24ã€AIME25å’ŒLiveCodeBenchåŸºå‡†æµ‹è¯•çš„å¹³å‡å‡†ç¡®ç‡ä»57.8%æå‡åˆ°72.8%ï¼Œ7Bæ¨¡å‹ä»43.6%æå‡åˆ°57.5%ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¨é¢çš„æ¶ˆèå®éªŒéªŒè¯æ ¸å¿ƒç»„ä»¶æœ‰æ•ˆæ€§  
å¯¹è®­ç»ƒæµç¨‹æ ¸å¿ƒç»„ä»¶å¼€å±•æ¶ˆèç ”ç©¶ï¼Œæ¶µç›–æ•°æ®æ··åˆã€å¤šé˜¶æ®µè®­ç»ƒã€æˆªæ–­å“åº”çš„ä¼˜åŠ¿æ©ç ã€é«˜æ¸©é‡‡æ ·ã€è‡ªé€‚åº”ç†µæ§åˆ¶ã€æ— KLæŸå¤±ç­‰æ–¹é¢ï¼Œæ˜ç¡®å„ç»„ä»¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä»¥éªŒè¯æ–¹æ¡ˆæœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚æ•°æ®æ··åˆæ–¹é¢ï¼Œç”¨ä¸¥æ ¼è¿‡æ»¤æ ‡å‡†æ„å»ºçš„æ•°æ®æ··åˆæ¯”å®½æ¾è´¨é‡é˜ˆå€¼æ„å»ºçš„åŸºçº¿æ··åˆè¡¨ç°æ›´ä¼˜ï¼›å¤šé˜¶æ®µè®­ç»ƒåœ¨åˆå§‹é˜¶æ®µæå‡è®­ç»ƒæ•ˆç‡åŒæ—¶ä¿ç•™åç»­é˜¶æ®µå¯æ‰©å±•æ€§ç­‰ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ·±å…¥ç ”ç©¶ç†µåç¼©ç°è±¡åŠåº”å¯¹  
å¯¹å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­è¿‡æ—©ç†µåç¼©ç°è±¡è¿›è¡Œå…¨é¢ç ”ç©¶ï¼Œè¯†åˆ«å½±å“ç†µåŠ¨æ€çš„å…³é”®å› ç´ ï¼Œè¯æ˜ç¼“è§£è¿‡æ—©ç†µåç¼©å¯¹æå‡æµ‹è¯•æ€§èƒ½è‡³å…³é‡è¦ã€‚é€šè¿‡å¤§é‡æ¶ˆèå®éªŒæ¢ç©¶å¦‚rolloutå¤šæ ·æ€§ç›¸å…³è¶…å‚æ•°ã€off - policyæ›´æ–°ç­‰å¯¹ç†µåç¼©å’Œæ€§èƒ½çš„å½±å“ï¼Œæ‰¾åˆ°é¢„é˜²è¿‡æ—©ç†µåç¼©çš„æ–¹æ³•ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ€§èƒ½è¡¨ç°ä¸Šï¼ŒSkywork - OR1 - 32Båœ¨AIME24ï¼ˆ82.2åˆ†ï¼‰ã€AIME25ï¼ˆ73.3åˆ†ï¼‰åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡DeepSeek - R1å’ŒQwen3 - 32Bï¼Œåœ¨LiveCodeBenchï¼ˆ63.0åˆ†ï¼‰ä¸Šè¡¨ç°ç›¸å½“ï¼›Skywork - OR1 - 7Båœ¨AIME24ï¼ˆ70.2åˆ†ï¼‰ã€AIME25ï¼ˆ54.6åˆ†ï¼‰å’ŒLiveCodeBenchï¼ˆ47.6åˆ†ï¼‰ä¸Šåœ¨åŒå°ºå¯¸æ¨¡å‹ä¸­å…·ç«äº‰åŠ›ï¼›Skywork - OR1 - Math - 7Båœ¨åŒç±»å°ºå¯¸æ¨¡å‹ä¸­ä¹Ÿè¡¨ç°å¼ºåŠ²ï¼ŒAIME24å¾—69.8åˆ†ã€AIME25å¾—52.3åˆ†ã€LiveCodeBenchå¾—43.6åˆ†ã€‚åŒæ—¶ï¼Œé€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å„æ ¸å¿ƒç»„ä»¶åœ¨æå‡æ€§èƒ½ã€åº”å¯¹ç†µåç¼©ç­‰æ–¹é¢çš„ä½œç”¨ï¼Œå¦‚å¤šé˜¶æ®µè®­ç»ƒå¯¹è®­ç»ƒæ•ˆç‡çš„æå‡ã€è‡ªé€‚åº”ç†µæ§åˆ¶å¯¹æ¨¡å‹æ¢ç´¢èƒ½åŠ›å’Œå­¦ä¹ å¯å¡‘æ€§çš„ç»´æŒç­‰ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®å±‚é¢ï¼šé‡è§†æ•°æ®æ¥æºå¤šæ ·æ€§ä¸è´¨é‡æ§åˆ¶ï¼Œé€šè¿‡ä¸¥æ ¼è¿‡æ»¤å’Œè´¨é‡è¯„ä¼°æ„å»ºè®­ç»ƒæ•°æ®ï¼Œèƒ½åŠ é€Ÿæ¨¡å‹å­¦ä¹ ï¼Œä¸ºåç»­æ¨¡å‹è®­ç»ƒæ•°æ®å‡†å¤‡æä¾›æ€è·¯ï¼Œå³åœ¨æ•°æ®æ”¶é›†æ—¶è¦è€ƒè™‘å¤šæºä¸”åšå¥½è´¨é‡æŠŠæ§ã€‚  
2. è®­ç»ƒç­–ç•¥å±‚é¢ï¼šå¤šé˜¶æ®µè®­ç»ƒã€é«˜æ¸©é‡‡æ ·ç­‰ç­–ç•¥åœ¨æå‡è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ä¸Šæœ‰ç§¯æä½œç”¨ï¼Œå…¶ä»–ç ”ç©¶è€…å¯å‚è€ƒè¿™äº›è®­ç»ƒç­–ç•¥æ¥ä¼˜åŒ–è‡ªèº«æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œå¹³è¡¡è®­ç»ƒæ•ˆç‡ä¸æ€§èƒ½æå‡ã€‚  
3. ç†µåç¼©åº”å¯¹å±‚é¢ï¼šæ·±å…¥ç ”ç©¶ç†µåç¼©ç°è±¡åŠå…³é”®å½±å“å› ç´ ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æä¾›äº†æ–¹å‘ï¼Œåœ¨å¤„ç†ç±»ä¼¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§æ¨¡å‹ä»»åŠ¡æ—¶ï¼Œå¯å…³æ³¨ç†µåŠ¨æ€å˜åŒ–å¹¶é‡‡å–ç›¸åº”æªæ–½ç¼“è§£è¿‡æ—©ç†µåç¼©ã€‚  
4. å¼€æºè´¡çŒ®å±‚é¢ï¼šå›¢é˜Ÿå¼€æºæ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œè®­ç»ƒæ•°æ®é›†ï¼Œä¸ºç¤¾åŒºç ”ç©¶æä¾›å……è¶³èµ„æºï¼Œè¿™ç§å¼€æ”¾åä½œçš„æ–¹å¼å€¼å¾—å­¦ä¹ ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ•´ä¸ªå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„å‘å±•ã€‚

## genpo--generative-diffusion-models-meet-on-policy-reinforcement-learning
### Abstract
Recent advances in reinforcement learning (RL) have demonstrated the powerful
exploration capabilities and multimodality of generative diffusion-based
policies. While substantial progress has been made in offline RL and off-policy
RL settings, integrating diffusion policies into on-policy frameworks like PPO
remains underexplored. This gap is particularly significant given the
widespread use of large-scale parallel GPU-accelerated simulators, such as
IsaacLab, which are optimized for on-policy RL algorithms and enable rapid
training of complex robotic tasks. A key challenge lies in computing
state-action log-likelihoods under diffusion policies, which is straightforward
for Gaussian policies but intractable for flow-based models due to irreversible
forward-reverse processes and discretization errors (e.g., Euler-Maruyama
approximations). To bridge this gap, we propose GenPO, a generative policy
optimization framework that leverages exact diffusion inversion to construct
invertible action mappings. GenPO introduces a novel doubled dummy action
mechanism that enables invertibility via alternating updates, resolving
log-likelihood computation barriers. Furthermore, we also use the action
log-likelihood for unbiased entropy and KL divergence estimation, enabling
KL-adaptive learning rates and entropy regularization in on-policy updates.
Extensive experiments on eight IsaacLab benchmarks, including legged locomotion
(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow
Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate
GenPO's superiority over existing RL baselines. Notably, GenPO is the first
method to successfully integrate diffusion policies into on-policy RL,
unlocking their potential for large-scale parallelized training and real-world
robotic deployment.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GenPOï¼šè®©ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹èµ°è¿›On-Policyå¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹å‡­å€Ÿå¼ºå¤§çš„æ¢ç´¢èƒ½åŠ›å’Œå¤šæ¨¡æ€ç‰¹æ€§ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢†åŸŸå¤‡å—å…³æ³¨ã€‚ä¸è¿‡ç°æœ‰å·¥ä½œå¤§å¤šèšç„¦äºç¦»çº¿RLå’Œç¦»ç­–ç•¥ï¼ˆOff-Policyï¼‰RLï¼Œå°†æ‰©æ•£ç­–ç•¥æ•´åˆåˆ°åƒPPOè¿™æ ·çš„**åœ¨çº¿ç­–ç•¥ï¼ˆOn-Policyï¼‰** RLæ¡†æ¶çš„ç ”ç©¶ä»å¾ˆæ¬ ç¼ºã€‚  

è€Œç°å®ä¸­ï¼Œå¤§è§„æ¨¡å¹¶è¡ŒGPUåŠ é€Ÿæ¨¡æ‹Ÿå™¨ï¼ˆå¦‚IsaacLabï¼‰å¯¹On-Policyç®—æ³•åšäº†ä¼˜åŒ–ï¼Œèƒ½å¿«é€Ÿè®­ç»ƒå¤æ‚æœºå™¨äººä»»åŠ¡ï¼Œä½†æ‰©æ•£ç­–ç•¥ä¸On-Policyç»“åˆå­˜åœ¨å…³é”®éšœç¢ï¼š**è®¡ç®—çŠ¶æ€ - åŠ¨ä½œçš„å¯¹æ•°ä¼¼ç„¶**ã€‚é«˜æ–¯ç­–ç•¥è®¡ç®—å¯¹æ•°ä¼¼ç„¶å¾ˆç›´æ¥ï¼Œä½†æ‰©æ•£è¿™ç±»åŸºäºæµçš„æ¨¡å‹ï¼Œå› æ­£é€†è¿‡ç¨‹ä¸å¯é€†ã€ç¦»æ•£åŒ–è¯¯å·®ï¼ˆå¦‚Euler - Maruyamaè¿‘ä¼¼ï¼‰ï¼Œéš¾ä»¥å¤„ç†å¯¹æ•°ä¼¼ç„¶è®¡ç®—ï¼Œè¿™é˜»ç¢äº†æ‰©æ•£ç­–ç•¥åœ¨On-Policyåœºæ™¯ä¸‹çš„åº”ç”¨ä¸çœŸå®æœºå™¨äººéƒ¨ç½²ã€‚äºæ˜¯ï¼ŒGenPOåº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¡¥æ¥ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ä¸On-Policy RL  
GenPOæ˜¯é¦–ä¸ªæˆåŠŸå°†æ‰©æ•£ç­–ç•¥æ•´åˆåˆ°On-Policy RLçš„æ–¹æ³•ã€‚å®ƒå€ŸåŠ©**ç²¾ç¡®æ‰©æ•£é€†è¿‡ç¨‹**æ„é€ å¯é€†çš„åŠ¨ä½œæ˜ å°„ï¼Œçªç ´äº†æ‰©æ•£ç­–ç•¥æ­£é€†è¿‡ç¨‹ä¸åŒ¹é…çš„é—®é¢˜ï¼Œè®©æ‰©æ•£æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œå¤šæ¨¡æ€ç‰¹æ€§ï¼Œèƒ½åœ¨å¤§è§„æ¨¡GPUå¹¶è¡Œæ¨¡æ‹Ÿå™¨ï¼ˆå¦‚IsaacLabï¼‰ä¸­ä¸ºOn-Policy RLæ‰€ç”¨ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£å†³æ‰©æ•£ç­–ç•¥çš„å¯¹æ•°ä¼¼ç„¶è®¡ç®—éš¾é¢˜  
GenPOæå‡º**â€œåŒè™šæ‹ŸåŠ¨ä½œï¼ˆdoubled dummy actionï¼‰â€æœºåˆ¶**ã€‚å—ç²¾ç¡®æ‰©æ•£é€†çš„é™åˆ¶ï¼Œå…ˆæ„é€ ä¸€ä¸ªåŠ¨ä½œç©ºé—´æ˜¯åŸç©ºé—´ä¸¤å€çš„æ–°é©¬å°”å¯å¤«è¿‡ç¨‹ï¼Œé€šè¿‡äº¤æ›¿æ›´æ–°è™šæ‹ŸåŠ¨ä½œçš„ä¸¤éƒ¨åˆ†å®ç°å¯é€†æµè¿‡ç¨‹ã€‚åŸºäºå˜é‡å˜æ¢ç†è®ºï¼ˆç±»ä¼¼å½’ä¸€åŒ–æµï¼‰ï¼Œèƒ½ç²¾ç¡®è®¡ç®—æ–°å†³ç­–é—®é¢˜ä¸­ç»™å®šåŠ¨ä½œçš„æ¦‚ç‡å¯†åº¦ï¼›æ¨ç†æ—¶ï¼Œå°†ä¸¤éƒ¨åˆ†è™šæ‹ŸåŠ¨ä½œå¹³å‡åæ˜ å°„å›åŸåŠ¨ä½œç©ºé—´ï¼Œæ—¢åœ¨é‡æ„çš„é©¬å°”å¯å¤«å†³ç­–é—®é¢˜ä¸­æ›´æ–°ç”Ÿæˆå¼æ‰©æ•£ç­–ç•¥ï¼Œåˆèƒ½å¾—åˆ°åŸä»»åŠ¡çš„æœ€ä¼˜è§£ã€‚è¿™ä¸€æœºåˆ¶è§£å†³äº†æ‰©æ•£ç­–ç•¥å¯¹æ•°ä¼¼ç„¶è®¡ç®—çš„å£å’ï¼Œè¿˜èƒ½åŸºäºæ­¤å®ç°ï¼šâ‘  ç²¾ç¡®å¯¹æ•°ä¼¼ç„¶è®¡ç®—ï¼›â‘¡ æ— åçš„ç†µä¼°è®¡ï¼›â‘¢ è§£ææ€§çš„KLæ•£åº¦è®¡ç®—ï¼Œç»™è¡¨è¾¾èƒ½åŠ›å¼ºçš„æ‰©æ•£æ¨¡å‹èµ‹äºˆç±»ä¼¼é«˜æ–¯ç­–ç•¥çš„ä¼˜åŠ¿ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç†µä¸KLæ•£åº¦çš„æ— åä¼°è®¡åŠåº”ç”¨  
åˆ©ç”¨åŠ¨ä½œå¯¹æ•°ä¼¼ç„¶ï¼ŒGenPOå®ç°äº†æ‰©æ•£ç­–ç•¥ç†µå’Œä¸è¡Œä¸ºæ‰©æ•£ç­–ç•¥é—´KLæ•£åº¦çš„æ— åä¼°è®¡ã€‚è¿™è®©On-Policyæ›´æ–°ä¸­èƒ½èå…¥**ç†µæ­£åˆ™åŒ–**å’Œ**KLè‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´**ï¼Œå€Ÿé‰´äº†PPOä¸­è¿™äº›æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼ŒåŠ©åŠ›ç­–ç•¥ä¼˜åŒ–ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨IsaacLabçš„8ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå±•å¼€å¤§é‡å®éªŒï¼Œæ¶µç›–legged locomotionï¼ˆå¦‚Antã€Humanoidç­‰æœºå™¨äººï¼‰ã€dexterous manipulationï¼ˆShadow Handï¼‰ã€aerial controlï¼ˆQuadcopterï¼‰ã€robotic armï¼ˆFrankaï¼‰ç­‰åœºæ™¯ã€‚ç»“æœæ˜¾ç¤ºï¼šGenPOåœ¨ç´¯ç§¯å›æŠ¥ä¸Šè¶…è¶Šç°æœ‰RLåŸºçº¿ï¼ŒåŒæ—¶æ ·æœ¬æ•ˆç‡ç›¸å½“ä¸”æ”¶æ•›æ›´å¿«ã€‚åœ¨å¤§è§„æ¨¡å¹¶è¡Œç¯å¢ƒä¸‹ï¼Œä»¥å¾€åŸºäºæ‰©æ•£çš„RLç®—æ³•å‡ ä¹æ— æ•ˆï¼Œè€ŒGenPOåœ¨æ ·æœ¬åˆ©ç”¨æ•ˆç‡å’Œ episodic rewards æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå……åˆ†éªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **é¢†åŸŸèåˆæ€è·¯**ï¼šä¸ºç”Ÿæˆå¼æ¨¡å‹ä¸On-Policy RLçš„ç»“åˆæä¾›äº†æˆåŠŸèŒƒä¾‹ï¼Œå¯å‘åç»­åœ¨ä¸åŒæ¨¡å‹ä¸RLèŒƒå¼äº¤å‰é¢†åŸŸçš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯å½“ç¡¬ä»¶ï¼ˆå¦‚å¹¶è¡Œæ¨¡æ‹Ÿå™¨ï¼‰é€‚é…æŸç±»RLèŒƒå¼æ—¶ï¼Œå¦‚ä½•è®©å¼ºè¡¨è¾¾åŠ›çš„ç”Ÿæˆæ¨¡å‹â€œä¸Šè½¦â€ã€‚  
2. **æŠ€æœ¯åˆ›æ–°å¤ç”¨**ï¼šåŒè™šæ‹ŸåŠ¨ä½œæœºåˆ¶è§£å†³å¯¹æ•°ä¼¼ç„¶éš¾é¢˜çš„æ€è·¯ï¼Œä»¥åŠåŸºäºå¯¹æ•°ä¼¼ç„¶åšç†µå’ŒKLä¼°è®¡æ¥è¾…åŠ©ç­–ç•¥æ›´æ–°çš„æ–¹å¼ï¼Œå¯ä¸ºå…¶ä»–é¢ä¸´â€œä¼¼ç„¶è®¡ç®—éš¾ã€éœ€åˆ©ç”¨æ¦‚ç‡å¯†åº¦è¾…åŠ©ä¼˜åŒ–â€çš„åœºæ™¯ï¼ˆå¦‚ç‰¹å®šæµæ¨¡å‹ã€å¤æ‚ç”Ÿæˆæ¨¡å‹ç»“åˆRLï¼‰æä¾›æŠ€æœ¯å‚è€ƒã€‚  
3. **å®éªŒéªŒè¯èŒƒå¼**ï¼šåœ¨å¤šç±»æœºå™¨äººä»»åŠ¡ï¼ˆè¿åŠ¨ã€æ“ä½œã€é£è¡Œç­‰ï¼‰ä¸Šçš„å…¨é¢å®éªŒï¼Œå±•ç¤ºäº†æ–°æ–¹æ³•åœ¨å¤æ‚çœŸå®æœºå™¨äººç›¸å…³åœºæ™¯çš„æ³›åŒ–æ€§éªŒè¯æ€è·¯ï¼Œä¸ºåç»­ç®—æ³•åœ¨æœºå™¨äººé¢†åŸŸçš„è½åœ°éªŒè¯æä¾›äº†å‚è€ƒæ¨¡æ¿ã€‚

## enhancing-efficiency-and-exploration-in-reinforcement-learning-for-llms
### Abstract
Reasoning large language models (LLMs) excel in complex tasks, which has
drawn significant attention to reinforcement learning (RL) for LLMs. However,
existing approaches allocate an equal number of rollouts to all questions
during the RL process, which is inefficient. This inefficiency stems from the
fact that training on simple questions yields limited gains, whereas more
rollouts are needed for challenging questions to sample correct answers.
Furthermore, while RL improves response precision, it limits the model's
exploration ability, potentially resulting in a performance cap below that of
the base model prior to RL. To address these issues, we propose a mechanism for
dynamically allocating rollout budgets based on the difficulty of the problems,
enabling more efficient RL training. Additionally, we introduce an adaptive
dynamic temperature adjustment strategy to maintain the entropy at a stable
level, thereby encouraging sufficient exploration. This enables LLMs to improve
response precision while preserving their exploratory ability to uncover
potential correct pathways. The code and data is available on:
https://github.com/LiaoMengqi/E3-RL4LLMs
### ğŸŒŸ è®ºæ–‡è§£è¯» | æå‡å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ•ˆç‡ä¸æ¢ç´¢èƒ½åŠ›çš„æ–°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡æ¨ç†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¹Ÿå› æ­¤åœ¨LLMsé¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œç°æœ‰RLæ–¹æ³•å­˜åœ¨ä¸¤å¤§é—®é¢˜ï¼šä¸€æ˜¯è®­ç»ƒæ—¶å¯¹æ‰€æœ‰é—®é¢˜åˆ†é…ç­‰é‡rolloutsï¼ˆé‡‡æ ·æ¬¡æ•°ï¼‰ï¼Œæ•ˆç‡ä½ä¸‹â€”â€”ç®€å•é—®é¢˜è®­ç»ƒæ”¶ç›Šæœ‰é™ï¼Œéš¾é¢˜å´éœ€æ›´å¤šé‡‡æ ·æ‰èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆï¼›äºŒæ˜¯RLè™½æå‡äº†å“åº”ç²¾åº¦ï¼Œå´é™åˆ¶äº†æ¨¡å‹æ¢ç´¢èƒ½åŠ›ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸Šé™ä½äºRLå‰çš„åŸºç¡€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒåŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·ç¨€ç–ï¼Œæ˜“è®©ç­–ç•¥æ›´æ–°æ¢¯åº¦æ¶ˆå¤±ï¼›ç»“åˆç†µæ­£åˆ™åŒ–ä¿ƒè¿›æ¢ç´¢æ—¶ï¼Œé¢å¯¹éš¾é¢˜è®­ç»ƒè¿˜å¯èƒ½æ€§èƒ½ä¸‹é™ç”šè‡³æ¨¡å‹å´©æºƒã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºé’ˆå¯¹æ€§æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€rollouté¢„ç®—åˆ†é…æœºåˆ¶  
ä¸ºæ›´é«˜æ•ˆåˆ†é…è®¡ç®—èµ„æºï¼Œè®ºæ–‡ä¾æ®é—®é¢˜éš¾åº¦åŠ¨æ€åˆ†é…rollouté¢„ç®—ã€‚å…ˆå»ºæ¨¡é—®é¢˜éš¾åº¦ï¼šåœ¨RLè®­ç»ƒæ•°æ®é›†é‡Œï¼Œè®°å½•æ¯ä¸ªé—®é¢˜çš„ç´¯è®¡rolloutæ¬¡æ•°ä¸ç´¯è®¡å¥–åŠ±ï¼Œç”¨å¹³å‡å¥–åŠ±æ’åºå¾—åˆ°éš¾åº¦ï¼ˆæ’åºè¶Šåéš¾åº¦è¶Šé«˜ï¼‰ã€‚ä¹‹åå°†ç®€å•é—®é¢˜èŠ‚çœçš„rollouté¢„ç®—è½¬ç§»ç»™éš¾é¢˜â€”â€”å¯¹æ¨¡å‹å·²èƒ½ç†Ÿç»ƒå›ç­”çš„ç®€å•é—®é¢˜å‡å°‘rollouté¢„ç®—ï¼ŒæŠŠèµ„æºé‡æ–°åˆ†é…ç»™æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæå‡é‡‡æ ·åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œè®©RLè®­ç»ƒæ›´é«˜æ•ˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªé€‚åº”åŠ¨æ€æ¸©åº¦è°ƒæ•´ç­–ç•¥  
ä¸ºåœ¨ä¸å¼•å…¥æœ‰å®³æ¢¯åº¦çš„å‰æä¸‹ä¿ƒè¿›æ¢ç´¢ï¼Œè®ºæ–‡è®¾è®¡æ¸©åº¦è°ƒåº¦å™¨åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¸©åº¦ï¼Œç»´æŒç­–ç•¥ç†µç¨³å®šä»¥é¼“åŠ±å……åˆ†æ¢ç´¢ï¼Œè¿˜ç»“åˆé€€ç«æœºåˆ¶å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚é€šè¿‡è°ƒæ•´æ¸©åº¦æ§åˆ¶é‡‡æ ·åˆ†å¸ƒï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒä¸­æ—¢èƒ½ä¿æŒæ¢ç´¢æœªçŸ¥è·¯å¾„çš„èƒ½åŠ›ï¼Œåˆèƒ½ä¿è¯å“åº”ç²¾åº¦ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AIME 2024åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”ä»…ç”¨GRPOè®­ç»ƒï¼Œè®ºæ–‡æ–¹æ³•ä½¿7Bè§„æ¨¡æ¨¡å‹çš„pass@1æå‡5.31%ã€pass@16æå‡3.33%ï¼›ä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•çš„pass@16æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºGRPOï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨æå‡æ¨¡å‹æ€§èƒ½ä¸æ•ˆç‡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èµ„æºåŠ¨æ€åˆ†é…æ€è·¯ï¼šåœ¨éœ€èµ„æºåˆ†é…çš„è®­ç»ƒåœºæ™¯ï¼ˆä¸æ­¢LLMs RLï¼‰ï¼Œå¯å‚è€ƒä¾ä»»åŠ¡/æ ·æœ¬éš¾åº¦åŠ¨æ€åˆ†é…èµ„æºï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚  
2. æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ï¼šé€šè¿‡æ¸©åº¦è°ƒåº¦ç­‰è½»é‡æ‰‹æ®µè°ƒèŠ‚ç­–ç•¥ç†µæ¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä¸ºè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¸¸è§çš„æ¢ç´¢ä¸è¶³æˆ–è¿‡åº¦æ¢ç´¢é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–RLä»»åŠ¡ä¸­ã€‚  
3. é—®é¢˜éš¾åº¦å»ºæ¨¡ï¼šåŸºäºç´¯è®¡äº¤äº’æ•°æ®ï¼ˆæ¬¡æ•°ã€å¥–åŠ±ï¼‰æ¥é‡åŒ–éš¾åº¦çš„æ–¹å¼ï¼Œä¸ºåç»­ä»»åŠ¡ä¼˜å…ˆçº§ã€èµ„æºå€¾æ–œç­‰å†³ç­–æä¾›äº†å¯è½åœ°çš„å‚è€ƒèŒƒå¼ã€‚

## ppo-br--dual-signal-entropy-reward-adaptation-for-trust-region-policy-optimization
### Abstract
Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PPO - BRï¼šä¸ºä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–æ³¨å…¥åŒä¿¡å·ç†µ - å¥–åŠ±è‡ªé€‚åº”æœºåˆ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åœ¨ä»æœºå™¨äººæ§åˆ¶åˆ°æ¸¸æˆAIç­‰è¯¸å¤šåœºæ™¯ä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚ç„¶è€Œï¼Œå…¶é™æ€ä¿¡ä»»åŒºåŸŸå­˜åœ¨å›ºæœ‰ç¼ºé™·ï¼šåœ¨å­¦ä¹ çš„ä¸åŒé˜¶æ®µï¼Œéš¾ä»¥å¹³è¡¡æ¢ç´¢ä¸æ”¶æ•›ã€‚æ—©æœŸæ¢ç´¢éœ€è¦ç­–ç•¥çš„éšæœºæ€§ï¼Œè€ŒåæœŸæ”¶æ•›éœ€è¦ç¨³å®šæ€§ï¼Œä½†PPOçš„é™æ€ä¿¡ä»»åŒºåŸŸä¼šå¯¼è‡´â€œæ¢ç´¢åŒ®ä¹â€ï¼ˆé«˜ç†µç­–ç•¥è¢«è¿‡åº¦è£å‰ªï¼ŒæŠ‘åˆ¶çŠ¶æ€è¦†ç›–ï¼‰å’Œâ€œæ”¶æ•›ä¸ç¨³å®šâ€ï¼ˆå›ºå®šçš„è£å‰ªé˜ˆå€¼åœ¨æœ€ä¼˜è§£é™„è¿‘å…è®¸å™ªå£°æ¢¯åº¦æ›´æ–°ï¼‰ã€‚æ­¤å‰çš„å·¥ä½œè¦ä¹ˆåªæ”¹è¿›æ¢ç´¢ï¼ˆå¦‚åŸºäºç†µçš„æ–¹æ³•ä½†å¿½ç•¥å¥–åŠ±åŠ¨æ€ï¼‰ï¼Œè¦ä¹ˆåªå¢å¼ºç¨³å®šæ€§ï¼ˆå¦‚å¥–åŠ±å¼•å¯¼çš„æ–¹æ³•ä½†æ— è§†ç­–ç•¥ä¸ç¡®å®šæ€§ï¼‰ï¼Œè¿˜æœ‰çš„å¯å‘å¼è°ƒåº¦ç¼ºä¹ç†è®ºä¿éšœï¼Œä¸”æ²¡æœ‰æ–¹æ³•èƒ½åœ¨ä¿¡ä»»åŒºåŸŸæœºåˆ¶å†…è”åˆä¼˜åŒ–æ¢ç´¢å’Œæ”¶æ•›è¿™ä¸¤ä¸ªä¿¡å·ï¼Œåœ¨ç¨€ç–å¥–åŠ±ä»»åŠ¡å’Œå®‰å…¨å…³é”®é¢†åŸŸè¿™ç§ç¼ºé™·å°¤ä¸ºä¸¥é‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºPPO - BRæ¡†æ¶
PPO - BRï¼ˆProximal Policy Optimization with Bidirectional Regularizationï¼‰æ˜¯ä¸€ç§åŒä¿¡å·ä¿¡ä»»åŒºåŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œå®ƒåŸºäºç­–ç•¥ç†µå’Œå¥–åŠ±è¿›ç¨‹åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼ã€‚åœ¨é«˜ç†µé˜¶æ®µæ‰©å¤§ä¿¡ä»»åŒºåŸŸä»¥ä¿ƒè¿›æ¢ç´¢ï¼Œåœ¨å¥–åŠ±æå‡è¿›å…¥å¹³å°æœŸæ—¶æ”¶ç¼©ä¿¡ä»»åŒºåŸŸä»¥ç¡®ä¿ç¨³å®šæ”¶æ•›ã€‚è¯¥æœºåˆ¶æœ‰ç†è®ºæ”¯æ’‘ï¼Œå®šç†1ä¿è¯äº†é€šè¿‡ç†µé©±åŠ¨æ‰©å±•å®ç°æœ€å°æ¢ç´¢ï¼Œå¼•ç†2ç¡®ä¿äº†æ”¶ç¼©æœŸé—´çš„å•è°ƒæ”¹è¿›ï¼Œä¸”æ— éœ€è¾…åŠ©ç½‘ç»œã€å…ƒä¼˜åŒ–æˆ–æ¶æ„æ”¹å˜ï¼Œä»…å¯¹PPOçš„è£å‰ªé€»è¾‘è¿›è¡Œè½»é‡è°ƒæ•´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»Ÿä¸€ç†µé©±åŠ¨æ¢ç´¢ä¸å¥–åŠ±å¼•å¯¼æ”¶æ•›
PPO - BRæ˜¯é¦–ä¸ªåœ¨å•ä¸€ç†è®ºæœ‰ç•Œçš„ä¿¡ä»»åŒºåŸŸå†…ç»Ÿä¸€ç†µé©±åŠ¨æ¢ç´¢å’Œå¥–åŠ±å¼•å¯¼æ”¶æ•›çš„æ–¹æ³•ï¼Œå¡«è¡¥äº†é˜¶æ®µæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ä¸­çš„å…³é”®ç©ºç™½ã€‚å®ƒèƒ½é€‚é…è¯­è¨€æ¨¡å‹å’Œä¸€èˆ¬å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œä¸ºä¸åŒåœºæ™¯ä¸‹çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†ç»Ÿä¸€çš„ç†µ - å¥–åŠ±è‡ªé€‚åº”æœºåˆ¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MuJoCoã€Atariå’Œç¨€ç–å¥–åŠ±ç­‰å…­ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„ç¯å¢ƒä¸­è¿›è¡Œå®éªŒéªŒè¯ã€‚ä¸æ ‡å‡†PPOç›¸æ¯”ï¼ŒPPO - BRæ”¶æ•›é€Ÿåº¦å¿«29.1%ï¼ˆWilcoxonæ£€éªŒp < 0.001ï¼‰ï¼›åœ¨åƒHumanoidè¿™æ ·çš„é«˜ç»´ä»»åŠ¡ä¸­ï¼Œå¥–åŠ±æ–¹å·®é™ä½2.3å€ï¼›è¿è¡Œæ—¶å¼€é”€å°äº1.8%ï¼Œè¯¥å¼€é”€æ¯”è¯¸å¦‚Discriminator - Driven PPOï¼ˆDD - PPOï¼‰ç­‰å¤æ‚åŸºçº¿å¼•å…¥çš„å¼€é”€ä½17å€å¤šã€‚åœ¨æ¨¡æ‹Ÿæ‰‹æœ¯ä»»åŠ¡ä¸­ï¼ŒPPO - BRå®ç°äº†98%çš„ç­–ç•¥ç¨³å®šæ€§ï¼Œè€ŒPPOä»…ä¸º82%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ–¹æ³•è®¾è®¡ç®€æ´ä¸”å…·ç†è®ºä¿éšœï¼šPPO - BRçš„å³æ’å³ç”¨ç‰¹æ€§å’Œç†è®ºä¿è¯ä½¿å…¶èƒ½åœ¨æ‰‹æœ¯æœºå™¨äººã€è‡ªä¸»æ— äººæœºç­‰å®‰å…¨å…³é”®ç³»ç»Ÿä¸­éƒ¨ç½²ï¼Œä¸ºå®‰å…¨å…³é”®é¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ åº”ç”¨æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚
2. è§£å†³é˜¶æ®µæ„ŸçŸ¥å­¦ä¹ éš¾é¢˜ï¼šå¡«è¡¥äº†å¼ºåŒ–å­¦ä¹ ä¸­ä¸åŒå­¦ä¹ é˜¶æ®µå¹³è¡¡æ¢ç´¢ä¸æ”¶æ•›çš„å…³é”®ç©ºç™½ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤„ç†é˜¶æ®µæ„ŸçŸ¥å­¦ä¹ é—®é¢˜ä¸Šæä¾›äº†æ–°æ€è·¯ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨ä¿¡ä»»åŒºåŸŸæœºåˆ¶å†…è”åˆä¼˜åŒ–æ¢ç´¢å’Œæ”¶æ•›ä¿¡å·ã€‚
3. ä½å¼€é”€é«˜æ•ˆæ”¹è¿›ï¼šä»…éœ€ä¿®æ”¹å°‘é‡ä»£ç ï¼ˆçº¦5è¡Œï¼‰å°±èƒ½å®ç°æ€§èƒ½æå‡ï¼Œåœ¨å®é™…å·¥ç¨‹åº”ç”¨ä¸­æ˜“äºé›†æˆå’Œæ¨å¹¿ï¼Œä¸ºç®—æ³•ä¼˜åŒ–æä¾›äº†â€œè½»é‡æ”¹è¿›ï¼Œé«˜æ•ˆæ”¶ç›Šâ€çš„èŒƒä¾‹ã€‚

## enter-the-void---planning-to-seek-entropy-when-reward-is-scarce
### Abstract
Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¥–åŠ±ç¨€ç¼ºæ—¶ä¸»åŠ¨æ¢ç´¢é«˜ç†µçŠ¶æ€ï¼Œæå‡MBRLæ•ˆç‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¯¸å¤šé¢†åŸŸå–å¾—æˆåŠŸï¼Œä½†åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚è‡ªä¸»å¯¼èˆªã€ç§‘å­¦æ¢ç´¢ç­‰ï¼‰ä¸­ï¼Œå¥–åŠ±ç¨€ç–ã€ç¯å¢ƒéƒ¨åˆ†å¯è§‚æµ‹ä¸”éšæœºï¼Œé«˜æ•ˆæ¢ç´¢ä¸æ ·æœ¬æ•ˆç‡ä»æ˜¯éš¾é¢˜ã€‚ curiosity - é©±åŠ¨çš„æ¢ç´¢è™½ä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œä½†ä¼ ç»Ÿ retrospective æ–¹æ³•ï¼ˆäº‹åé‡åŒ–æ–°å¥‡æ€§ï¼‰å®æ—¶é€‚åº”æ€§å·®ï¼Œåœ¨éå¹³ç¨³ç¯å¢ƒæ˜“å¤±æ•ˆï¼› anticipatory æ–¹æ³•è™½æœ‰æ½œåŠ›ä½†ä¾èµ–åŸºäºé›†æˆçš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œåœ¨éšæœºç¯å¢ƒè¡¨ç°ä¸ä½³ã€‚è€ŒåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰é€šè¿‡è®­ç»ƒä¸–ç•Œæ¨¡å‹é¢„æµ‹æœªæ¥ï¼Œä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›æ–¹å‘ï¼Œä½†ç°æœ‰ MBRL æ–¹æ³•å¤šä¼˜å…ˆä¼˜åŒ– actorï¼Œå¿½è§†ä¸–ç•Œæ¨¡å‹å­¦ä¹ ä¼˜åŒ–ã€‚æå‡ä¸–ç•Œæ¨¡å‹ä¿çœŸåº¦ä¸æ”¶æ•›é€Ÿåº¦èƒ½å¸¦æ¥ä¸‹æ¸¸æ”¶ç›Šï¼Œå¦‚æå‡ actor æ€§èƒ½ã€‚åŒæ—¶ï¼Œç°æœ‰åŸºäºæ¨¡å‹çš„æ¢ç´¢æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šå¦‚ Look before you leap è®­ç»ƒæ—¶å›é¿ä¸ç¡®å®šçŠ¶æ€é˜»ç¢å­¦ä¹ ï¼› MaxEnt - Dreamer ç¼ºä¹è®¡åˆ’æ‰¿è¯ºï¼Œé¢‘ç¹é‡è§„åˆ’é™ä½æ•ˆç‡ï¼› RAIF éœ€äº‹åè¯„ä¼°æ”¶ç›Šï¼Œæ— æ³•åŠæ—¶å“åº”æ–°çŠ¶æ€ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨å¢å¼º Dreamer ä»¥å®ç°æ›´é«˜æ•ˆæ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ä¸–ç•Œæ¨¡å‹ transition ä¸ç¡®å®šæ€§é¢„æµ‹çŸ­è§†åŸŸçŠ¶æ€ç†µå¹¶ densify ç¨€ç–å¥–åŠ±
å€ŸåŠ©ä¸–ç•Œæ¨¡å‹å¯¹ç¯å¢ƒåŠ¨æ€çš„æ•æ‰èƒ½åŠ›ï¼Œåˆ©ç”¨å…¶ transition ä¸ç¡®å®šæ€§æ¥é¢„æµ‹çŸ­æ—¶é—´èŒƒå›´å†…çŠ¶æ€çš„ç†µï¼Œå°†å¥–åŠ±ä¸ç¡®å®šæ€§è¡¥å……åˆ°ç¨€ç–å¥–åŠ±ä¸­ï¼Œä½¿ç¨€ç–å¥–åŠ±ä¿¡å·æ›´å¯†é›†ï¼ŒåŠ é€Ÿ MBRL è®­ç»ƒè¿‡ç¨‹ï¼Œè®©æ™ºèƒ½ä½“åœ¨å¥–åŠ±ç¨€ç¼ºåœºæ™¯ä¸‹ä¹Ÿèƒ½æ›´é«˜æ•ˆå­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°† Dreamer KL æœ€å°åŒ–é‡æ„ä¸ºå¯¹æŠ— minmax ç›®æ ‡ä»¥æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Š
æŠŠ Dreamer ä¸­åŸæœ¬çš„ KL æœ€å°åŒ–é—®é¢˜è½¬åŒ–ä¸ºå¯¹æŠ—çš„ minmax ç›®æ ‡ï¼Œä»¥æ­¤æ¥æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šï¼Œè®©ç”±ç†µé©±åŠ¨çš„æ™ºèƒ½æ¢ç´¢å¯ä»¥ç›´æ¥ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹çš„å­¦ä¹ ï¼Œä¸å†ä»…å°†ä¸–ç•Œæ¨¡å‹ä½œä¸º policy å­¦ä¹ å·¥å…·ï¼Œè€Œæ˜¯è®© policy åŠ©åŠ›ä¸–ç•Œæ¨¡å‹è®­ç»ƒï¼Œæå‡ä¸–ç•Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ä»¥æ”¯æŒä¸‹æ¸¸ actor è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥ååº”å¼åˆ†å±‚è§„åˆ’å™¨åŠ¨æ€é€‰æ‹©ä¼˜åŒ–ç†µä¸å¥–åŠ±å¹¶çµæ´»é‡è§„åˆ’
è®¾è®¡è½»é‡çº§åŸºäº PPO çš„åˆ†å±‚è§„åˆ’å™¨ï¼Œåˆ©ç”¨æ¨¡å‹ rollouts åœ¨é«˜ç†µå’Œé«˜å¥–åŠ±è½¨è¿¹é—´åŠ¨æ€é€‰æ‹©ï¼Œè¿˜èƒ½åŠ¨æ€å†³å®šä½•æ—¶é‡è§„åˆ’ã€‚è¯¥è§„åˆ’å™¨èƒ½åœ¨ä¼˜åŒ–ç†µï¼ˆæ¢ç´¢ï¼‰å’Œå¥–åŠ±ï¼ˆåˆ©ç”¨ï¼‰ä¹‹é—´åšæƒè¡¡ï¼ŒåŒæ—¶ä¿ç•™ä¸¢å¼ƒå½“å‰è®¡åˆ’ã€é‡æ–°è§„åˆ’æ–°è½¨è¿¹çš„çµæ´»æ€§ï¼Œè§£å†³äº†è®¸å¤šåŸºäºæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ–¹æ³•ç¼ºä¹æ‰¿è¯ºã€æ¯ä¸€æ­¥éƒ½åˆæˆå¤šæ­¥è®¡åˆ’çš„é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å°†æ–¹æ³•åº”ç”¨äº Dreamer ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œåœ¨ Miniworld ç¨‹åºç”Ÿæˆçš„è¿·å®«ç¯å¢ƒä¸­ï¼Œæ”¶æ•›æ—¶æœ¬æ–‡æ–¹æ³•å®Œæˆè¿·å®«é€Ÿåº¦æ¯”åŸºç¡€ Dreamer å¿« 50%ï¼›ä¸”åœ¨æƒ³è±¡ä¸­è®­ç»ƒçš„ç­–ç•¥æ”¶æ•›ä»…éœ€åŸºç¡€ Dreamer ç­–ç•¥æ‰€éœ€ç¯å¢ƒæ­¥æ•°çš„ 60%ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨æå‡æ•ˆç‡ä¸æ”¶æ•›é€Ÿåº¦ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ€è·¯è½¬æ¢ï¼šå°† policy ä½œä¸ºåŠ é€Ÿå’Œä¼˜åŒ–ä¸–ç•Œæ¨¡å‹è®­ç»ƒçš„æœºåˆ¶ï¼Œä¸å†ä»…æŠŠä¸–ç•Œæ¨¡å‹å½“ policy å­¦ä¹ å·¥å…·ï¼Œè¿™ç§æ€è·¯è½¬æ¢ä¸º MBRL ä¸­æ¨¡å‹ä¸ç­–ç•¥çš„ååŒä¼˜åŒ–æä¾›æ–°è§†è§’ï¼Œå¯å¯å‘åç»­ç ”ç©¶é‡æ–°å®¡è§†äºŒè€…å…³ç³»ä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚
2. ä¸ç¡®å®šæ€§åˆ©ç”¨ï¼šåˆ©ç”¨ä¸–ç•Œæ¨¡å‹çš„ transition ä¸ç¡®å®šæ€§æ¥å¤„ç†ç¨€ç–å¥–åŠ±å’Œå¼•å¯¼æ¢ç´¢ï¼Œä¸ºåœ¨å¥–åŠ±ç¨€ç¼ºã€ç¯å¢ƒå¤æ‚åœºæ™¯ä¸‹è®¾è®¡æ¢ç´¢æœºåˆ¶æä¾›äº†åŸºäºæ¨¡å‹ä¸ç¡®å®šæ€§çš„æœ‰æ•ˆèŒƒä¾‹ï¼Œåç»­å¯å€Ÿé‰´æ­¤æ€è·¯æ‹“å±•åˆ°å…¶ä»–ç¯å¢ƒä¸ä»»åŠ¡ã€‚
3. åˆ†å±‚è§„åˆ’è®¾è®¡ï¼šååº”å¼åˆ†å±‚è§„åˆ’å™¨åŠ¨æ€å†³ç­–çš„è®¾è®¡ï¼Œä¸ºè§£å†³åŸºäº MPC æ–¹æ³•çš„ç¼ºé™·ï¼ˆå¦‚ç¼ºä¹æ‰¿è¯ºï¼‰æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œåœ¨éœ€è¦å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€çµæ´»è§„åˆ’çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œè¿™ç§åˆ†å±‚ä¸”åŠ¨æ€å†³ç­–çš„è§„åˆ’å™¨è®¾è®¡æ€è·¯å€¼å¾—å‚è€ƒä¸æ‹“å±•ã€‚

## the-unreasonable-effectiveness-of-entropy-minimization-in-llm-reasoning
### Abstract
Entropy minimization (EM) trains the model to concentrate even more
probability mass on its most confident outputs. We show that this simple
objective alone, without any labeled data, can substantially improve large
language models' (LLMs) performance on challenging math, physics, and coding
tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy
similarly to instruction finetuning, but on unlabeled outputs drawn from the
model; (2) EM-RL: reinforcement learning with negative entropy as the only
reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce
entropy without any training data or parameter updates. On Qwen-7B, EM-RL,
without any labeled data, achieves comparable or better performance than strong
RL baselines such as GRPO and RLOO that are trained on 60K labeled examples.
Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of
proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the
challenging SciCode benchmark, while being 3x more efficient than
self-consistency and sequential refinement. Our findings reveal that many
pretrained LLMs possess previously underappreciated reasoning capabilities that
can be effectively elicited through entropy minimization alone, without any
labeled data or even any parameter updates.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»…é ç†µæœ€å°åŒ–ï¼Œå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ç«Ÿèƒ½â€œæ— å¸ˆè‡ªé€šâ€ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„è®­ç»ƒé˜¶æ®µå·²å¸æ”¶æµ·é‡çŸ¥è¯†ï¼Œä½†å¦‚ä½•åœ¨**æ— æ ‡æ³¨æ•°æ®**çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æ¿€å‘å…¶æ¨ç†èƒ½åŠ›ï¼ˆå¦‚æ•°å­¦ã€ç‰©ç†ã€ä»£ç ä»»åŠ¡ï¼‰ä»æ˜¯éš¾é¢˜ã€‚ä¼ ç»Ÿçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜ä¸”é€‚ç”¨åœºæ™¯å—é™ã€‚ç”±æ­¤ï¼Œè®ºæ–‡èšç„¦â€œç†µæœ€å°åŒ–ï¼ˆEntropy Minimization, EMï¼‰â€è¿™ä¸€æ€è·¯ï¼šè‹¥æ¨¡å‹è¶³å¤Ÿâ€œæœ‰èƒ½åŠ›â€ï¼Œå…¶é«˜ç½®ä¿¡åº¦è¾“å‡ºæ›´å¯èƒ½æ­£ç¡®ï¼Œé€šè¿‡è®©æ¨¡å‹æŠŠæ¦‚ç‡è´¨é‡é›†ä¸­åˆ°é«˜ç½®ä¿¡è¾“å‡ºä¸Šï¼Œæˆ–è®¸èƒ½åœ¨æ— æ ‡æ³¨æ•°æ®æ—¶æå‡æ¨ç†è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
è®ºæ–‡å›´ç»•â€œé¢„è®­ç»ƒåé€‚é…â€ä¸â€œæ¨ç†æ—¶å¢å¼ºâ€ä¸¤å¤§é˜¶æ®µï¼Œæå‡º**ä¸‰ç§æ— æ ‡æ³¨æ•°æ®çš„ç†µæœ€å°åŒ–æ–¹æ³•**ï¼Œè¦†ç›–å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ã€æ¨ç†ä¸‰ä¸ªç»´åº¦ï¼š  

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ— ç›‘ç£å¾®è°ƒï¼ˆEM - FTï¼‰  
ç±»ä¼¼æŒ‡ä»¤å¾®è°ƒï¼Œä½†ä»…ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ— æ ‡æ³¨è¾“å‡ºæ¥æœ€å°åŒ–â€œtokençº§ç†µâ€ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šè¾“å…¥promptï¼Œè®©æ¨¡å‹ç”Ÿæˆæ— æ ‡æ³¨è¾“å‡ºï¼Œå†åœ¨è¿™äº›è¾“å‡ºä¸Šä¼˜åŒ–tokenå±‚é¢çš„ç†µæŸå¤±ï¼Œä»¥æ­¤å¼ºåŒ–æ¨¡å‹å¯¹é«˜ç½®ä¿¡tokençš„åå¥½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºç†µçš„å¼ºåŒ–å­¦ä¹ ï¼ˆEM - RLï¼‰  
å°†â€œè´Ÿç†µâ€ä½œä¸ºå”¯ä¸€å¥–åŠ±ä¿¡å·çš„å¼ºåŒ–å­¦ä¹ ã€‚å€Ÿé‰´REINFORCEç®—æ³•æ€è·¯ï¼Œä½†å®Œå…¨æ‘’å¼ƒæ ‡æ³¨æ•°æ®ï¼Œä»…é€šè¿‡â€œRolloutè¿‡ç¨‹ä¸­tokençº§ç†µçš„è´Ÿæ•°ä¹‹å’Œï¼ˆç»“åˆåŸºçº¿è°ƒæ•´ï¼‰â€ä½œä¸ºå¥–åŠ±ï¼Œè®©æ¨¡å‹å­¦ä¼šè¾“å‡ºæ›´â€œç¡®å®šâ€çš„ç»“æœã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨ç†æ—¶ç†µä¼˜åŒ–ï¼ˆEM - INFï¼‰  
æ¨ç†é˜¶æ®µç›´æ¥è°ƒæ•´logitsä»¥é™ä½æ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„ç†µï¼Œ**æ— éœ€è®­ç»ƒæˆ–å‚æ•°æ›´æ–°**ã€‚åœ¨è§£ç çš„æ¯ä¸€æ­¥ä¼˜åŒ–logitsï¼Œè®©æ¨¡å‹æ›´èšç„¦é«˜ç½®ä¿¡é€‰é¡¹ï¼Œé€‚ç”¨äºä¸ç¡®å®šæ€§é«˜çš„å¤æ‚ä»»åŠ¡ï¼ˆå¦‚é«˜ç­‰æ•°å­¦ã€ç‰©ç†ã€ç§‘å­¦ä»£ç ï¼‰ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨æ•°å­¦ã€ç‰©ç†ã€ä»£ç ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼š  
- **EM - RLï¼ˆQwen - 7Bï¼‰**ï¼šåœ¨æ— æ ‡æ³¨æ•°æ®æ—¶ï¼Œæ€§èƒ½æ¯”è‚©ç”šè‡³è¶…è¿‡åŸºäº6ä¸‡æ ‡æ³¨æ•°æ®è®­ç»ƒçš„å¼ºRLåŸºçº¿ï¼ˆå¦‚GRPOã€RLOOï¼‰ï¼›  
- **EM - INFï¼ˆQwen - 32Bï¼‰**ï¼šåœ¨é«˜éš¾åº¦SciCodeåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½è¿½å¹³/è¶…è¶ŠGPT - 4oã€Claude 3 Opusã€Gemini 1.5 Proç­‰é—­æºæ¨¡å‹ï¼Œä¸”æ•ˆç‡æ¯”â€œè‡ªä¸€è‡´æ€§ï¼ˆSelf - Consistencyï¼‰â€å’Œâ€œé¡ºåºç²¾ç‚¼ï¼ˆSequential Refinementï¼‰â€é«˜3å€ï¼›  
- è·¨æ¨¡å‹å¯¹æ¯”ï¼ˆå¦‚Llama - 3.1 - 8B vs Qwen - 2.5ï¼‰æ˜¾ç¤ºï¼šEMçš„æ•ˆæœä¾èµ–é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«çš„â€œèƒ½åŠ›åº•å­â€â€”â€”è‹¥æ¨¡å‹åœ¨æŸä»»åŠ¡ä¸Šå…ˆå¤©èƒ½åŠ›å¼±ï¼ŒEMçš„å¢ç›Šä¹Ÿä¼šå—é™ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ— æ ‡æ³¨æ•°æ®çš„æ½œåŠ›é‡Šæ”¾**ï¼šè¯æ˜ä»…é€šè¿‡â€œç†µæœ€å°åŒ–â€è¿™ä¸€ç®€æ´ç›®æ ‡ï¼Œå°±èƒ½åœ¨æ— æ ‡æ³¨æ•°æ®æ—¶å¤§å¹…æå‡LLMsæ¨ç†èƒ½åŠ›ï¼Œä¸ºèµ„æºæœ‰é™æˆ–æ ‡æ³¨æˆæœ¬é«˜çš„åœºæ™¯æä¾›æ–°æ€è·¯ï¼›  
2. **å¤šé˜¶æ®µé€‚é…èŒƒå¼**ï¼šä»å¾®è°ƒï¼ˆEM - FTï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆEM - RLï¼‰åˆ°æ¨ç†å¢å¼ºï¼ˆEM - INFï¼‰ï¼Œè¦†ç›–å¤§æ¨¡å‹ç”Ÿå‘½å‘¨æœŸå¤šä¸ªé˜¶æ®µï¼Œå±•ç¤ºäº†ç†µæœ€å°åŒ–çš„çµæ´»åº”ç”¨ç©ºé—´ï¼›  
3. **åŸºå‡†ä¸æ¨¡å‹èƒ½åŠ›çš„åæ€**ï¼šå¼ºè°ƒé¢„è®­ç»ƒæ¨¡å‹â€œå›ºæœ‰èƒ½åŠ›â€çš„é‡è¦æ€§ï¼Œå‘¼åæœªæ¥ç®—æ³•è¯„ä¼°ä¸­çº³å…¥EMä½œä¸ºåŸºçº¿ï¼Œä»¥æ›´æ¸…æ™°åŒºåˆ†â€œç®—æ³•åˆ›æ–°â€å’Œâ€œæ¨¡å‹æœ¬èº«èƒ½åŠ›â€çš„è´¡çŒ®ï¼›  
4. **ä»»åŠ¡è¾¹ç•Œçš„å¯ç¤º**ï¼šEMåœ¨â€œç½®ä¿¡åº¦ä¸æ­£ç¡®æ€§å¼ºç›¸å…³â€çš„æ¨ç†ä»»åŠ¡ï¼ˆæ•°å­¦ã€ä»£ç ï¼‰è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨â€œäººç±»ä»·å€¼è§‚å¯¹é½â€ç­‰ç½®ä¿¡åº¦â‰ è´¨é‡çš„ä»»åŠ¡ä¸­æ•ˆæœæœ‰é™ï¼Œæç¤ºéœ€æ ¹æ®ä»»åŠ¡ç‰¹æ€§é€‰æ‹©æ–¹æ³•ã€‚  


è¿™ç¯‡è®ºæ–‡æ’•å¼€äº†â€œæ— æ ‡æ³¨æ•°æ®ä¸‹å¤§æ¨¡å‹èƒ½åŠ›æŒ–æ˜â€çš„æ–°è§†è§’ï¼Œè®©â€œç†µæœ€å°åŒ–â€ä» regularization é…è§’å˜æˆäº†æ¨ç†å¢å¼ºçš„ä¸»è§’â€”â€”åŸæ¥å¤§æ¨¡å‹çš„â€œæ½œåŠ›â€ï¼Œæœ‰æ—¶åªéœ€è¦ä¸€ä¸ªç®€å•çš„ç›®æ ‡å°±èƒ½é‡Šæ”¾ã€‚

## aapo--enhance-the-reasoning-capabilities-of-llms-with-advantage-momentum
### Abstract
Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AAPOï¼šç”¨ä¼˜åŠ¿åŠ¨é‡æå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆé€”å¾„ï¼Œå°¤å…¶åœ¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®æœ‰é™å¯¼è‡´æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŠ›ä¸ä»å¿ƒçš„åœºæ™¯ä¸‹ã€‚åŸºäºRLçš„è®­ç»ƒæ–¹æ³•é‡Œï¼Œä»¥Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä¸ºä»£è¡¨çš„**ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡**å› æ‘†è„±å¯¹ä»·å€¼æ¨¡å‹çš„ä¾èµ–ã€ç®€åŒ–è®­ç»ƒæµç¨‹è€Œå—å…³æ³¨ã€‚ä½†ç°æœ‰ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•å­˜åœ¨è®­ç»ƒä½æ•ˆé—®é¢˜ï¼Œå½“ä¼°è®¡ä¼˜åŠ¿è¶‹è¿‘äºé›¶æ—¶ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°æ˜“é™·å…¥åœæ»ï¼›è€Œå½“ç»„å†…å¥–åŠ±æ–¹å·®å¤§æ—¶ï¼Œåˆå¯èƒ½å¼•å‘æ¢¯åº¦ä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™äº›ç¼ºé™·ï¼Œè®ºæ–‡æå‡ºAdvantage - Augmented Policy Optimizationï¼ˆAAPOï¼‰ç®—æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ·±å…¥åˆ†æç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ç±»ç®—æ³•çš„ä¼˜åŒ–è¡Œä¸º  
è®ºæ–‡èšç„¦RLåœ¨å¤§æ¨¡å‹åè®­ç»ƒé˜¶æ®µé‡‡ç”¨ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ—¶çš„ä¼˜åŒ–è¡¨ç°ï¼Œé‡ç‚¹å‰–æä¼˜åŠ¿ä¼°è®¡ç¯èŠ‚æ½œåœ¨çš„é—®é¢˜ï¼Œæ¯”å¦‚ä¼˜åŠ¿è¶‹è¿‘é›¶å¯¼è‡´æ— æ¢¯åº¦æ›´æ–°ã€ç»„å†…å¥–åŠ±æ–¹å·®å¤§å¼•å‘æ¢¯åº¦ä¸ç¨³å®šç­‰æƒ…å†µæ˜¯å¦‚ä½•åç¦»ç†æƒ³ä¼˜åŒ–è½¨è¿¹çš„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºAAPOç®—æ³•å¼•å…¥ä¼˜åŠ¿åŠ¨é‡æ”¹è¿›ä¼˜åŠ¿ä¼°è®¡  
AAPOæ˜¯ä¸€ç§æ–°é¢–çš„RLç®—æ³•ï¼Œé€šè¿‡åŸºäºåŠ¨é‡çš„ä¼°è®¡æ–¹æ¡ˆå¢å¼ºä¼˜åŠ¿ï¼Œè¿›è€Œä¼˜åŒ–äº¤å‰ç†µï¼ˆCEï¼‰æŸå¤±ã€‚å…¶ä¸­ï¼Œ**ä¼˜åŠ¿åŠ¨é‡**å®šä¹‰ä¸ºç­–ç•¥æ¨¡å‹å“åº”å¥–åŠ±ä¸å‚è€ƒæ¨¡å‹å“åº”å¥–åŠ±çš„å·®å€¼ï¼Œè¯¥æ–¹å¼å°†å‚è€ƒæ¢¯åº¦èå…¥åŸå§‹æ¢¯åº¦ï¼Œå³ä¾¿ä¼˜åŠ¿è¶‹è¿‘äºé›¶ï¼Œä¹Ÿèƒ½æä¾›åæ˜ æ•´ä½“æ”¹è¿›æ–¹å‘çš„å¯é ä¼˜åŒ–ä¿¡å·ï¼Œç¼“è§£ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å¸¦æ¥çš„è®­ç»ƒä½æ•ˆé—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¼€å±•å®éªŒï¼Œç»“æœè¡¨æ˜AAPOå±•ç°å‡ºæ›´ä¼˜çš„è®­ç»ƒä¼˜åŒ–è¡Œä¸ºï¼Œåœ¨ä»£è¡¨æ€§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæœ‰åŠ›è¯æ˜äº†AAPOåœ¨æå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. é—®é¢˜åˆ†æè§’åº¦ï¼šè®ºæ–‡å¯¹ç°æœ‰ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡RLç®—æ³•åœ¨å¤§æ¨¡å‹åè®­ç»ƒåœºæ™¯ä¸‹çš„ä¼˜åŒ–è¡Œä¸ºåˆ†æï¼Œä¸ºåç»­ç ”ç©¶åŒç±»æ–¹æ³•çš„ç¼ºé™·ä¸æ”¹è¿›æ–¹å‘æä¾›äº†æ€è·¯ï¼Œå¯å‘ç ”ç©¶è€…å…³æ³¨ç®—æ³•åœ¨å®é™…è®­ç»ƒæ—¶çš„ç»†ç²’åº¦è¡¨ç°ã€‚
2. æ–¹æ³•åˆ›æ–°æ€è·¯ï¼šAAPOå¼•å…¥ä¼˜åŠ¿åŠ¨é‡æ”¹è¿›ä¼˜åŠ¿ä¼°è®¡çš„æ€è·¯ï¼Œä¸ºè§£å†³RLä¸­ä¼˜åŠ¿ä¼°è®¡ç›¸å…³çš„è®­ç»ƒæ•ˆç‡ã€ç¨³å®šæ€§é—®é¢˜æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å€Ÿé‰´è¿™ç§é€šè¿‡å¼•å…¥é¢å¤–ä¿¡æ¯ï¼ˆå¦‚å‚è€ƒæ¨¡å‹å¯¹æ¯”äº§ç”Ÿçš„åŠ¨é‡ï¼‰æ¥å¢å¼ºä¼˜åŒ–ä¿¡å·çš„æ–¹å¼ï¼Œæ‹“å±•åˆ°å…¶ä»–RLä¸å¤§æ¨¡å‹ç»“åˆçš„ä»»åŠ¡åœºæ™¯ã€‚
3. å®éªŒéªŒè¯æ–¹å‘ï¼šåœ¨æ•°å­¦æ¨ç†ç­‰ä¸“ä¸šé¢†åŸŸåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§çš„æ¨¡å¼ï¼Œä¸ºè¯„ä¼°å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ç±»æ–¹æ³•æä¾›äº†å‚è€ƒï¼Œåç»­ç ”ç©¶å¯å‚è€ƒè¿™ç±»æœ‰é’ˆå¯¹æ€§çš„ä»»åŠ¡å‹åŸºå‡†æµ‹è¯•éªŒè¯æ€è·¯ã€‚

## disco--reinforcing-large-reasoning-models-with-discriminative-constrained-optimization
### Abstract
The recent success and openness of DeepSeek-R1 have brought widespread
attention to Group Relative Policy Optimization (GRPO) as a reinforcement
learning method for large reasoning models (LRMs). In this work, we analyze the
GRPO objective under a binary reward setting and reveal an inherent limitation
of question-level difficulty bias. We also identify a connection between GRPO
and traditional discriminative methods in supervised learning. Motivated by
these insights, we introduce a new Discriminative Constrained Optimization
(DisCO) framework for reinforcing LRMs, grounded in the principle of
discriminative learning. The main differences between DisCO and GRPO and its
recent variants are: (1) it replaces the group relative objective with a
discriminative objective defined by a scoring function; (2) it abandons
clipping-based surrogates in favor of non-clipping RL surrogate objectives used
as scoring functions; (3) it employs a simple yet effective constrained
optimization approach to enforce the KL divergence constraint, ensuring stable
training. As a result, DisCO offers notable advantages over GRPO and its
variants: (i) it completely eliminates difficulty bias by adopting
discriminative objectives; (ii) it addresses the entropy instability in GRPO
and its variants through the use of non-clipping scoring functions and a
constrained optimization approach; (iii) it allows the incorporation of
advanced discriminative learning techniques to address data imbalance, where a
significant number of questions have more negative than positive generated
answers during training. Our experiments on enhancing the mathematical
reasoning capabilities of SFT-finetuned models show that DisCO significantly
outperforms GRPO and its improved variants such as DAPO, achieving average
gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B
model.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DisCOï¼šåŸºäºåˆ¤åˆ«å¼çº¦æŸä¼˜åŒ–å¢å¼ºå¤§æ¨ç†æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€DeepSeek - R1çš„æˆåŠŸä¸å¼€æºï¼Œå…¶é‡‡ç”¨çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä½œä¸ºå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¯¹GRPOå›ºæœ‰å±€é™æ€§çš„åˆ†æä¸è§£å†³è¿˜å­˜åœ¨ä¸è¶³ã€‚GRPOåœ¨äºŒå…ƒå¥–åŠ±è®¾ç½®ä¸‹å­˜åœ¨é—®é¢˜çº§éš¾åº¦åå·®ï¼Œè™½æœ‰ä¸€äº›æ”¹è¿›å˜ä½“ï¼ˆå¦‚DAPOã€Dr. GRPOç­‰ï¼‰ï¼Œä½†è¿™äº›æ–¹æ³•å¤šæ˜¯å¯å‘å¼ã€ä¸´æ—¶æ€§çš„ï¼Œç¼ºä¹åŸåˆ™æ€§åŸºç¡€ï¼Œæœªèƒ½å®Œå…¨è§£å†³GRPOçš„å›ºæœ‰å±€é™ï¼Œæ¯”å¦‚Dr. GRPOä»å—éš¾åº¦åå·®å›°æ‰°ï¼ŒDAPOå¯èƒ½å¼•å‘ç†µè¿‡åº¦å¢é•¿äº§ç”Ÿéšæœºè¾“å‡ºã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨ä»¥åŸåˆ™æ€§æ–¹å¼è®¾è®¡æ›´æœ‰æ•ˆçš„å¤§æ¨ç†æ¨¡å‹å¼ºåŒ–ä¼˜åŒ–æ–¹æ³•ï¼Œé¿å…GRPOçš„å±€é™æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ­ç¤ºGRPOæœ¬è´¨é—®é¢˜ä¸å…³è”
å¯¹GRPOåŠå…¶å˜ä½“åœ¨äºŒå…ƒå¥–åŠ±è®¾ç½®ä¸‹çš„ç›®æ ‡å‡½æ•°åˆ†æï¼Œæ˜ç¡®GRPOéš¾åº¦åå·®çš„æ ¹æºæ˜¯å…¶ç»„ç›¸å¯¹ä¼˜åŠ¿å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯¹è¿‡æ˜“æˆ–è¿‡éš¾é—®é¢˜èµ‹äºˆä¸æˆæ¯”ä¾‹çš„å°æƒé‡ï¼›åŒæ—¶å‘ç°GRPOä¸ç›‘ç£å­¦ä¹ ä¸­ä¼ ç»Ÿåˆ¤åˆ«å¼æ–¹æ³•ï¼ˆAUCæœ€å¤§åŒ–ï¼‰å­˜åœ¨æ¦‚å¿µè”ç³»ï¼ŒAUCæœ€å¤§åŒ–æ—¨åœ¨æé«˜æ­£è¾“å‡ºåˆ†æ•°åŒæ—¶é™ä½è´Ÿè¾“å‡ºåˆ†æ•°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºDisCOæ¡†æ¶
åŸºäºåˆ¤åˆ«å¼å­¦ä¹ åŸç†æå‡ºDiscriminative Constrained Optimizationï¼ˆDisCOï¼‰æ¡†æ¶å¼ºåŒ–å¤§æ¨ç†æ¨¡å‹ã€‚ä¸GRPOåŠå…¶å˜ä½“æœ‰ä¸‰æ–¹é¢ä¸åŒï¼šç”¨ scoring function å®šä¹‰çš„åˆ¤åˆ«å¼ç›®æ ‡æ›¿ä»£ç»„ç›¸å¯¹ç›®æ ‡ï¼›æ”¾å¼ƒåŸºäºè£å‰ªçš„ä»£ç†ï¼Œé‡‡ç”¨éè£å‰ªRLä»£ç†ç›®æ ‡ä½œä¸º scoring functionï¼›ç”¨ç®€å•æœ‰æ•ˆçš„çº¦æŸä¼˜åŒ–æ–¹æ³•å®æ–½KLæ•£åº¦çº¦æŸç¡®ä¿è®­ç»ƒç¨³å®šã€‚æ­¤æ¡†æ¶å¸¦æ¥ä¼˜åŠ¿ï¼šé€šè¿‡åˆ¤åˆ«å¼ç›®æ ‡æ¶ˆé™¤éš¾åº¦åå·®ï¼›åˆ©ç”¨éè£å‰ª scoring function å’Œçº¦æŸä¼˜åŒ–è§£å†³GRPOåŠå…¶å˜ä½“çš„ç†µä¸ç¨³å®šï¼›ç»“åˆå…ˆè¿›åˆ¤åˆ«å¼å­¦ä¹ æŠ€æœ¯å¤„ç†è®­ç»ƒä¸­æ•°æ®ä¸å¹³è¡¡ï¼ˆå¤§é‡é—®é¢˜ç”Ÿæˆç­”æ¡ˆè´Ÿæ ·æœ¬å¤šäºæ­£æ ·æœ¬ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æå‡SFTå¾®è°ƒæ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›çš„å®éªŒä¸­ï¼ŒDisCOæ˜¾è‘—ä¼˜äºGRPOåŠå…¶æ”¹è¿›å˜ä½“ï¼ˆå¦‚DAPOï¼‰ã€‚å¯¹äº1.5Bæ¨¡å‹ï¼Œåœ¨å…­ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒGRPOå¹³å‡æå‡7%ï¼Œç›¸è¾ƒDAPOå¹³å‡æå‡6%ï¼›åœ¨å¯¹DeepSeek - R1 - Distill - Qwen - 1.5B/7Bæ¨¡å‹å¾®è°ƒï¼ˆè®­ç»ƒå’Œæ¨ç†æœ€å¤§å“åº”é•¿åº¦8kï¼‰çš„æ•°å­¦æ¨ç†å®éªŒä¸­ï¼ŒDisCOä¹Ÿæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œä¸”æ¯”è®­ç»ƒç”¨24ké•¿åº¦ã€æ¨ç†ç”¨32ké•¿åº¦çš„GRPOè¡¨ç°æ›´å¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»åˆ†æè§’åº¦ï¼Œæœ¬æ–‡å¯¹GRPOç›®æ ‡å‡½æ•°çš„æ·±å…¥å‰–æä¸ºç†è§£ç°æœ‰å¤§æ¨ç†æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§æä¾›äº†æ€è·¯ï¼Œæœ‰åŠ©äºåç»­é’ˆå¯¹æ€§æ”¹è¿›ï¼›ä»æ–¹æ³•è§’åº¦ï¼ŒDisCOæ¡†æ¶åŸºäºåˆ¤åˆ«å¼å­¦ä¹ çš„è®¾è®¡æ€è·¯ä¸ºå¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ æä¾›äº†æ–°èŒƒå¼ï¼Œå…¶å¯¹ç›®æ ‡å‡½æ•°ã€ä»£ç†ç›®æ ‡ã€çº¦æŸä¼˜åŒ–çš„å¤„ç†æ–¹å¼ï¼Œä»¥åŠç»“åˆåˆ¤åˆ«å¼æŠ€æœ¯å¤„ç†æ•°æ®ä¸å¹³è¡¡ç­‰ï¼Œä¸ºè§£å†³å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„åå·®ã€ä¸ç¨³å®šã€æ•°æ®é—®é¢˜æä¾›äº†å¯å‚è€ƒçš„æŠ€æœ¯è·¯çº¿ï¼›ä»å®éªŒè§’åº¦ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè®¾è®¡ä¸å¯¹æ¯”ï¼Œä¸ºå¤§æ¨¡å‹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰æå‡çš„ç ”ç©¶æä¾›äº†å®éªŒæ–¹æ³•ä¸æ•ˆæœéªŒè¯çš„å‚è€ƒã€‚

## preference-optimization-for-combinatorial-optimization-problems
### Abstract
Reinforcement Learning (RL) has emerged as a powerful tool for neural
combinatorial optimization, enabling models to learn heuristics that solve
complex problems without requiring expert knowledge. Despite significant
progress, existing RL approaches face challenges such as diminishing reward
signals and inefficient exploration in vast combinatorial action spaces,
leading to inefficiency. In this paper, we propose Preference Optimization, a
novel method that transforms quantitative reward signals into qualitative
preference signals via statistical comparison modeling, emphasizing the
superiority among sampled solutions. Methodologically, by reparameterizing the
reward function in terms of policy and utilizing preference models, we
formulate an entropy-regularized RL objective that aligns the policy directly
with preferences while avoiding intractable computations. Furthermore, we
integrate local search techniques into the fine-tuning rather than
post-processing to generate high-quality preference pairs, helping the policy
escape local optima. Empirical results on various benchmarks, such as the
Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem
(CVRP) and the Flexible Flow Shop Problem (FFSP), demonstrate that our method
significantly outperforms existing RL algorithms, achieving superior
convergence efficiency and solution quality.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç»„åˆä¼˜åŒ–æ–°çªç ´ï¼šPreference Optimization å¦‚ä½•é©æ–°å¼ºåŒ–å­¦ä¹ æ±‚è§£æ€è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰åœ¨è·¯å¾„è§„åˆ’ã€ç”µè·¯è®¾è®¡ã€è°ƒåº¦ç­‰è¯¸å¤šå®é™…åœºæ™¯ä¸­è‡³å…³é‡è¦ï¼Œéœ€ä»æŒ‡æ•°çº§å¢é•¿çš„å€™é€‰è§£ä¸­æ‰¾åˆ°æœ€ä¼˜è§£ã€‚ç”±äºå…¶ NP - éš¾ç‰¹æ€§ï¼Œç²¾ç¡®æ±‚è§£å—é™ï¼Œé«˜æ•ˆè·å–è¿‘ä¼¼æœ€ä¼˜è§£æˆä¸ºå…³é”®ã€‚  

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºæ±‚è§£ç»„åˆä¼˜åŒ–é—®é¢˜æä¾›äº†æ— éœ€ä¸“å®¶çŸ¥è¯†ã€è‡ªå­¦ä¹ å¯å‘å¼ç­–ç•¥çš„é€”å¾„ï¼Œä½†ç°æœ‰ RL æ–¹æ³•é¢ä¸´ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼š  
- å¥–åŠ±ä¿¡å·è¡°å‡ï¼šç­–ç•¥æå‡è¿‡ç¨‹ä¸­ä¼˜åŠ¿å€¼å¹…åº¦å¤§å¹…é™ä½ï¼Œæ˜“å¼•å‘æ¢¯åº¦æ¶ˆå¤±ä¸æ”¶æ•›ç¼“æ…¢ï¼›  
- åŠ¨ä½œç©ºé—´æ¢ç´¢ä½æ•ˆï¼šç»„åˆä¼˜åŒ–çš„åºå¤§åŠ¨ä½œç©ºé—´è®©ä¼ ç»Ÿæ¢ç´¢æŠ€æœ¯ï¼ˆå¦‚è½¨è¿¹ç†µæ­£åˆ™åŒ–ï¼‰è®¡ç®—ä¸å¯è¡Œï¼›  
- æ¨ç†è€—æ—¶é¢å¤–å¼€é”€ï¼šå±€éƒ¨æœç´¢å¸¸ä½œä¸ºåå¤„ç†æå‡è§£è´¨é‡ï¼Œä½†ä¼šå¢åŠ æ¨ç†æ—¶é—´ï¼Œåœ¨æ—¶é—´æ•æ„Ÿåœºæ™¯å—é™ã€‚  

ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡º **Preference Optimizationï¼ˆåå¥½ä¼˜åŒ–ï¼ŒPOï¼‰** æ–¹æ³•ï¼Œå°†å®šé‡å¥–åŠ±è½¬åŒ–ä¸ºå®šæ€§åå¥½ä¿¡å·ï¼Œé‡å¡‘ RL æ±‚è§£ç»„åˆä¼˜åŒ–çš„èŒƒå¼ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºåå¥½çš„ RL4CO æ¡†æ¶  
ä¼ ç»Ÿ RL ä¾èµ–å®šé‡å¥–åŠ±é©±åŠ¨å­¦ä¹ ï¼Œè€Œç»„åˆä¼˜åŒ–ä¸­å¥–åŠ±å·®å¼‚è¡°å‡ä¼šå¹²æ‰°è®­ç»ƒç¨³å®šæ€§ã€‚PO åˆ›æ–°æ€§åœ°å°†**å®šé‡å¥–åŠ±ä¿¡å·è½¬åŒ–ä¸ºå®šæ€§åå¥½ä¿¡å·**ï¼Œèšç„¦é‡‡æ ·è§£ä¹‹é—´çš„â€œä¼˜åŠ£å…³ç³»â€è€Œéç»å¯¹å¥–åŠ±å€¼ã€‚è¿™ç§è½¬æ¢è®©å­¦ä¹ è¿‡ç¨‹å¯¹å¥–åŠ±å°ºåº¦ä¸æ•æ„Ÿï¼Œæ—¢ç¨³å®šè®­ç»ƒï¼Œåˆèƒ½é€šè¿‡ä¿ç•™è§£çš„ç›¸å¯¹å…³ç³»æŒç»­å¼ºè°ƒæ›´ä¼˜è§£ï¼Œä»æ ¹æœ¬ä¸Šç¼“è§£å¥–åŠ±è¡°å‡éš¾é¢˜ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡å‚æ•°åŒ–çš„ç†µæ­£åˆ™åŒ–ç›®æ ‡  
ä¸ºé¿å…ä¼ ç»Ÿç†µæ­£åˆ™åŒ–åœ¨ç»„åˆåŠ¨ä½œç©ºé—´çš„è®¡ç®—ä¸å¯è¡Œæ€§ï¼ŒPO ä»ç­–ç•¥è§’åº¦**é‡å‚æ•°åŒ–å¥–åŠ±å‡½æ•°**ï¼Œç»“åˆç»Ÿè®¡åå¥½æ¨¡å‹ï¼Œæ„å»ºç†µæ­£åˆ™åŒ–çš„ RL ç›®æ ‡å‡½æ•°ã€‚è¯¥ç›®æ ‡è®©ç­–ç•¥ç›´æ¥ä¸åå¥½å¯¹é½ï¼Œæ— éœ€æšä¸¾æ•´ä¸ªåŠ¨ä½œç©ºé—´ï¼Œåœ¨ä¿è¯è®¡ç®—å¯è¡Œæ€§çš„åŒæ—¶ï¼Œå¤©ç„¶ä¿ƒè¿›åœ¨åºå¤§ç»„åˆåŠ¨ä½œç©ºé—´ä¸­çš„é«˜æ•ˆæ¢ç´¢ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå±€éƒ¨æœç´¢ä¸å¾®è°ƒçš„æ·±åº¦æ•´åˆ  
è¿‡å¾€å±€éƒ¨æœç´¢å¤šä½œä¸ºåå¤„ç†æ­¥éª¤ï¼ˆä¼šé¢å¤–å¢åŠ æ¨ç†æ—¶é—´ï¼‰ï¼ŒPO åˆ™**å°†å±€éƒ¨æœç´¢æŠ€æœ¯èå…¥å¾®è°ƒé˜¶æ®µ**ï¼Œè€Œéä»…ç”¨äºåå¤„ç†ã€‚è¿™æ ·ç­–ç•¥èƒ½ä»ç»å±€éƒ¨æœç´¢ä¼˜åŒ–åçš„é«˜è´¨é‡è§£ä¸­å­¦ä¹ ï¼ŒåŠ©åŠ›æ‘†è„±å±€éƒ¨æœ€ä¼˜ï¼ŒåŒæ—¶ä¸äº§ç”Ÿé¢å¤–æ¨ç†è€—æ—¶ï¼Œåœ¨æå‡è§£è´¨é‡çš„ç¯èŠ‚å®ç°â€œä¸€ä¸¾ä¸¤å¾—â€ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨å¤šä¸ªç»å…¸ç»„åˆä¼˜åŒ–åŸºå‡†ä»»åŠ¡ï¼ˆæ—…è¡Œå•†é—®é¢˜ TSPã€å¸¦å®¹é‡çº¦æŸçš„è½¦è¾†è·¯å¾„é—®é¢˜ CVRPã€æŸ”æ€§æµæ°´è½¦é—´é—®é¢˜ FFSPï¼‰ä¸ŠéªŒè¯ PO æ€§èƒ½ï¼š  
- æ”¶æ•›æ•ˆç‡ï¼šç›¸æ¯”ç°æœ‰ RL ç®—æ³•ï¼ŒPO æ”¶æ•›é€Ÿåº¦æ˜¾è‘—æå‡ï¼Œæ›´å¿«è¿›å…¥ä¼˜è´¨è§£åŒºåŸŸï¼›  
- è§£è´¨é‡ï¼šåœ¨å„ç±»é—®é¢˜å®ä¾‹ä¸­ï¼Œç”Ÿæˆè§£çš„è´¨é‡ï¼ˆå¦‚æ›´çŸ­è·¯å¾„ã€æ›´ä¼˜è°ƒåº¦ï¼‰è¿œè¶…å¯¹æ¯”æ–¹æ³•ï¼›  
- é²æ£’æ€§ï¼šåå¥½ä¿¡å·ä¸å±€éƒ¨æœç´¢çš„æ•´åˆè®©æ¨¡å‹åœ¨ä¸åŒè§„æ¨¡ã€å¤æ‚åº¦çš„é—®é¢˜ä¸Šéƒ½å±•ç°å‡ºç¨³å®šä¼˜åŠ¿ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ä¿¡å·è½¬æ¢æ€è·¯**ï¼šå½“ä»»åŠ¡ä¸­æ•°å€¼ä¿¡å·æ˜“è¡°å‡/ä¸ç¨³å®šæ—¶ï¼Œå¯å€Ÿé‰´â€œå®šé‡â†’å®šæ€§â€çš„ä¿¡å·è½¬æ¢é€»è¾‘ï¼Œç”¨ç›¸å¯¹å…³ç³»æ›¿ä»£ç»å¯¹æ•°å€¼é©±åŠ¨å­¦ä¹ ï¼›  
2. **ç›®æ ‡å‡½æ•°è®¾è®¡**ï¼šé¢å¯¹é«˜ç»´/ç¦»æ•£å¤§ç©ºé—´æ—¶ï¼Œé‡å‚æ•°åŒ–ä¸ç†µæ­£åˆ™åŒ–ç»“åˆçš„æ€è·¯ä¸ºå¹³è¡¡æ¢ç´¢ - åˆ©ç”¨æä¾›äº†æ–°èŒƒå¼ï¼Œé¿å…æš´åŠ›æšä¸¾ï¼›  
3. **åå¤„ç†è½¬å†…è®­**ï¼šå°†ä¼ ç»Ÿâ€œåå¤„ç†å¢å¼ºâ€æ”¹ä¸ºâ€œè®­ç»ƒé˜¶æ®µæ•´åˆâ€ï¼Œè®©æ¨¡å‹æå‰å­¦ä¹ æ›´ä¼˜è§£çš„ç‰¹å¾ï¼Œæ˜¯æå‡è§£è´¨é‡ä¸æ•ˆç‡çš„å·§å¦™æ€è·¯ï¼Œå¯æ¨å¹¿åˆ°éœ€åå¤„ç†æä¼˜çš„å…¶ä»–ä»»åŠ¡ï¼ˆå¦‚å›¾åƒç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ï¼‰ã€‚  

PO ä¸ºå¼ºåŒ–å­¦ä¹ æ±‚è§£ç»„åˆä¼˜åŒ–éš¾é¢˜å¼€è¾Ÿäº†æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„è·¯å¾„ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨ä¿¡å·åˆ©ç”¨ã€ç›®æ ‡æ„å»ºä¸æµç¨‹æ•´åˆå±‚é¢çš„çªç ´ï¼Œä¹Ÿä¸ºå…¶ä»–ä¾èµ– RL æ±‚è§£å¤æ‚ç¦»æ•£é—®é¢˜çš„åœºæ™¯æä¾›äº†å®è´µå€Ÿé‰´ã€‚

## imagine--verify--execute--memory-guided-agentic-exploration-with-vision-language-models
### Abstract
Exploration is essential for general-purpose robotic learning, especially in
open-ended environments where dense rewards, explicit goals, or task-specific
supervision are scarce. Vision-language models (VLMs), with their semantic
reasoning over objects, spatial relations, and potential outcomes, present a
compelling foundation for generating high-level exploratory behaviors. However,
their outputs are often ungrounded, making it difficult to determine whether
imagined transitions are physically feasible or informative. To bridge the gap
between imagination and execution, we present IVE (Imagine, Verify, Execute),
an agentic exploration framework inspired by human curiosity. Human exploration
is often driven by the desire to discover novel scene configurations and to
deepen understanding of the environment. Similarly, IVE leverages VLMs to
abstract RGB-D observations into semantic scene graphs, imagine novel scenes,
predict their physical plausibility, and generate executable skill sequences
through action tools. We evaluate IVE in both simulated and real-world tabletop
environments. The results show that IVE enables more diverse and meaningful
exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the
entropy of visited states. Moreover, the collected experience supports
downstream learning, producing policies that closely match or exceed the
performance of those trained on human-collected demonstrations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»æƒ³è±¡åˆ°æ‰§è¡Œï¼šç”¨è§†è§‰è¯­è¨€æ¨¡å‹å®ç°è®°å¿†å¼•å¯¼çš„æ™ºèƒ½ä½“æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨é€šç”¨æœºå™¨äººå­¦ä¹ é‡Œï¼Œæ¢ç´¢æ˜¯æ ¸å¿ƒèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±ç¨€ç–ã€ç›®æ ‡ä¸æ˜ç¡®æˆ–ç¼ºä¹ç‰¹å®šä»»åŠ¡ç›‘ç£çš„å¼€æ”¾å¼ç¯å¢ƒä¸­ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½å¸¸ç”¨äºè‡ªä¸»æ¢ç´¢ï¼Œä½†åœ¨é«˜ç»´ä¸”è¯­ä¹‰ä¸°å¯Œçš„çœŸå®æœºå™¨äººåœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨è¡Œä¸ºæ— æ–¹å‘ã€æ•ˆç‡ä½ç”šè‡³æœ‰å®‰å…¨é£é™©ç­‰é—®é¢˜ã€‚è€Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½æœ‰è¯­ä¹‰æ¨ç†ä¼˜åŠ¿ï¼Œèƒ½ä¸ºæ¢ç´¢æä¾›åŸºç¡€ï¼Œä½†è¾“å‡ºå¸¸è„±ç¦»ç‰©ç†ç°å®ï¼Œä¸”ç¼ºä¹å¯¹è¿‡å¾€äº¤äº’çš„ç»“æ„åŒ–è®°å¿†ï¼Œæ˜“äº§ç”Ÿå†—ä½™ã€ä¸å¯è¡Œçš„ç”Ÿæˆç»“æœï¼Œé˜»ç¢æœ‰æ•ˆæ¢ç´¢ä¸ä¸‹æ¸¸å­¦ä¹ ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•å¼¥åˆæƒ³è±¡ä¸æ‰§è¡Œçš„å·®è·ï¼Œå€Ÿé‰´äººç±»å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢çš„æ–¹å¼æ¥è®¾è®¡æœºå™¨äººæ¢ç´¢æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºIVEæ¡†æ¶ï¼Œæ¨¡æ‹Ÿäººç±»å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢  
IVEï¼ˆImagine, Verify, Executeï¼‰æ˜¯å—äººç±»æ¢ç´¢å¯å‘çš„æ™ºèƒ½ä½“æ¢ç´¢æ¡†æ¶ï¼Œèƒ½è®©æ™ºèƒ½ä½“æƒ³è±¡æ–°é¢–æœªæ¥åœºæ™¯é…ç½®ã€åŸºäºäº¤äº’å†å²é¢„æµ‹å¯è¡Œæ€§å¹¶é€šè¿‡æŠ€èƒ½åº“æ‰§è¡Œè¡Œä¸ºã€‚å…¶åŒ…å«å¤šä¸ªæ¨¡å—ï¼šScene Describerå°†åŸå§‹è§‚æµ‹æŠ½è±¡ä¸ºè¯­ä¹‰è¡¨ç¤ºï¼›Explorerç”Ÿæˆè‡ªä¸»ç›®æ ‡å’ŒæŠ€èƒ½è®¡åˆ’ï¼›Verifieré¢„æµ‹æƒ³è±¡åœºæ™¯è½¬æ¢çš„ç‰©ç†åˆç†æ€§ï¼›åŸºäºæ£€ç´¢çš„Memory Moduleå­˜å‚¨è¿‡å¾€ç»éªŒè¾…åŠ©æ¢ç´¢ä¸éªŒè¯ï¼›Action ToolsæŠŠé«˜å±‚æŠ€èƒ½è½¬ä¸ºå¯æ‰§è¡Œæœºå™¨äººè½¨è¿¹ã€‚è¿™äº›æ¨¡å—ç´§å¯†åä½œï¼Œè®©æƒ³è±¡åŸºäºç‰©ç†å¯è¡Œæ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— å¥–åŠ±çš„è¯­ä¹‰äº¤äº’æ•°æ®æ”¶é›†  
æ„å»ºå…¨è‡ªåŠ¨çš„ã€ç”±è§†è§‰è¯­è¨€æ¨¡å‹å¼•å¯¼çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ— éœ€å¤–éƒ¨å¥–åŠ±ã€ç¤ºèŒƒæˆ–é¢„å®šä¹‰ç›®æ ‡ï¼Œç”Ÿæˆè¯­ä¹‰å±‚é¢æœ‰æ„ä¹‰çš„äº¤äº’æ•°æ®ã€‚å€ŸåŠ©VLMsçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ç”Ÿæˆå‡è®¾è½¬æ¢ã€é‡åŒ–æ–°é¢–æ€§ç­‰ï¼ŒåŒæ—¶ç»“åˆå„æ¨¡å—ç¡®ä¿æ•°æ®åœ¨ç‰©ç†ä¸Šåˆç†ä¸”è¯­ä¹‰ä¸°å¯Œï¼Œå®ç°åœºæ™¯å¤šæ ·æ€§å¯è¾¾äººç±»ä¸“å®¶çš„82% - 122%ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»“åˆè®°å¿†å¼•å¯¼æƒ³è±¡ä¸ç‰©ç†åˆç†æ€§é¢„æµ‹  
IVEæŠŠè®°å¿†å¼•å¯¼çš„æƒ³è±¡å’Œç‰©ç†åˆç†æ€§é¢„æµ‹ç»“åˆï¼Œè®©æ™ºèƒ½ä½“åƒäººç±»ä¸€æ ·å¸¦ç€å¥½å¥‡å¿ƒæ¢ç´¢ã€‚Memory Moduleæä¾›è¿‡å¾€äº¤äº’ç»éªŒï¼Œè¾…åŠ©Exploreræƒ³è±¡å’ŒVerifieréªŒè¯ï¼Œè§£å†³VLMsç¼ºä¹ç‰©ç† grounding å’Œè®°å¿†çš„é—®é¢˜ï¼Œä½¿ç”Ÿæˆçš„æ¢ç´¢è¡Œä¸ºæ›´å¯è¡Œã€å¤šæ ·ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ¡Œé¢ç¯å¢ƒè¯„ä¼°IVEï¼Œç»“æœæ˜¾ç¤ºï¼šç›¸æ¯”RLåŸºçº¿ï¼ŒIVEèƒ½å®ç°æ›´ä¸°å¯Œä¸”æœ‰æ„ä¹‰çš„æ¢ç´¢ï¼Œè®¿é—®çŠ¶æ€çš„ç†µæå‡äº†4.1åˆ°7.8å€ï¼›æ”¶é›†çš„ç»éªŒæ”¯æŒä¸‹æ¸¸å­¦ä¹ ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥æ€§èƒ½æ¥è¿‘ç”šè‡³è¶…è¿‡åŸºäºäººç±»æ”¶é›†ç¤ºèŒƒè®­ç»ƒçš„ç­–ç•¥ï¼›æ­¤å¤–ï¼ŒIVEè·å–çš„æ•°æ®ä¹Ÿæœ‰åŠ©äºæ›´å¥½åœ°è®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼Œä½“ç°å…¶æ•æ‰å¤æ‚ç¯å¢ƒæ½œåœ¨åŠ¨æ€çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œæ¨¡å—åŒ–çš„æ¡†æ¶è®¾è®¡å€¼å¾—å€Ÿé‰´ï¼Œå°†å¤æ‚çš„æ¢ç´¢ä»»åŠ¡æ‹†è§£ä¸ºåœºæ™¯æè¿°ã€æ¢ç´¢ã€éªŒè¯ã€è®°å¿†ã€åŠ¨ä½œå·¥å…·ç­‰æ¨¡å—ï¼Œå„æ¨¡å—å„å¸å…¶èŒåˆç´§å¯†é…åˆï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡æä¾›äº†åˆ†è€Œæ²»ä¹‹çš„æ€è·¯ï¼›åœ¨æœºå™¨äººå­¦ä¹ ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆæ–¹é¢ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨VLMsçš„è¯­ä¹‰ä¼˜åŠ¿å¼¥è¡¥ä¼ ç»ŸRLåœ¨çœŸå®å¤æ‚åœºæ™¯çš„ä¸è¶³ï¼Œä¸ºåç»­è·¨æ¨¡æ€ç»“åˆæ¨åŠ¨æœºå™¨äººè‡ªä¸»æ¢ç´¢æä¾›äº†èŒƒä¾‹ï¼›ä»åº”ç”¨ä»·å€¼çœ‹ï¼Œè¯æ˜äº†æ— å¥–åŠ±è‡ªä¸»æ”¶é›†æ•°æ®ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è¡Œä¸ºå…‹éš†ã€ä¸–ç•Œæ¨¡å‹å­¦ä¹ ï¼‰çš„å¯è¡Œæ€§ï¼Œä¸ºæ•°æ®é©±åŠ¨çš„æœºå™¨äººå­¦ä¹ åœ¨æ•°æ®è·å–éš¾é¢˜ä¸Šæä¾›äº†æ–°é€”å¾„ï¼Œåç»­ç ”ç©¶å¯å‚è€ƒè¿™ç§è‡ªä¸»æ¢ç´¢ - æ•°æ®åˆ©ç”¨çš„é—­ç¯æ¨¡å¼æ¥æå‡æœºå™¨äººåœ¨å¼€æ”¾ç¯å¢ƒçš„å­¦ä¹ èƒ½åŠ›ã€‚ 

