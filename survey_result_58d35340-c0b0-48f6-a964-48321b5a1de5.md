# Paper List of Terms(Reward+RL+Medical)
- [25/06] **CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making**  
[[Paper](http://arxiv.org/pdf/2506.12849v1)] [[Code/Page]()] [[TLDR/Notes](#capo--reinforcing-consistent-reasoning-in-medical-decision-making)]

- [25/06] **Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback**  
[[Paper](http://arxiv.org/pdf/2506.12323v1)] [[Code/Page]()] [[TLDR/Notes](#doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback)]

- [25/06] **RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints**  
[[Paper](http://arxiv.org/pdf/2506.06600v2)] [[Code/Page]()] [[TLDR/Notes](#rarl--improving-medical-vlm-reasoning-and-generalization-with-reinforcement-learning-and-lora-under-data-and-hardware-constraints)]

- [25/05] **DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue**  
[[Paper](http://arxiv.org/pdf/2505.19630v1)] [[Code/Page](https://github.com/JarvisUSTC/DoctorAgent-RL)] [[TLDR/Notes](#doctoragent-rl--a-multi-agent-collaborative-reinforcement-learning-system-for-multi-turn-clinical-dialogue)]

- [25/05] **Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2505.19213v1)] [[Code/Page]()] [[TLDR/Notes](#improving-medical-reasoning-with-curriculum-aware-reinforcement-learning)]

- [25/05] **Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL**  
[[Paper](http://arxiv.org/pdf/2505.17952v1)] [[Code/Page]()] [[TLDR/Notes](#beyond-distillation--pushing-the-limits-of-medical-llm-reasoning-with-minimalist-rule-based-rl)]

- [25/05] **WiNGPT-3.0 Technical Report**  
[[Paper](http://arxiv.org/pdf/2505.17387v2)] [[Code/Page]()] [[TLDR/Notes](#wingpt-3-0-technical-report)]

- [25/05] **Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies**  
[[Paper](http://arxiv.org/pdf/2505.16242v1)] [[Code/Page]()] [[TLDR/Notes](#offline-guarded-safe-reinforcement-learning-for-medical-treatment-optimization-strategies)]

- [25/05] **s3: You Don't Need That Much Data to Train a Search Agent via RL**  
[[Paper](http://arxiv.org/pdf/2505.14146v1)] [[Code/Page]()] [[TLDR/Notes](#s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl)]

- [25/05] **Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2505.13973v1)] [[Code/Page]()] [[TLDR/Notes](#toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models)]



# TLDR/Notes
## capo--reinforcing-consistent-reasoning-in-medical-decision-making
### Abstract
In medical visual question answering (Med-VQA), achieving accurate responses
relies on three critical steps: precise perception of medical imaging data,
logical reasoning grounded in visual input and textual questions, and coherent
answer derivation from the reasoning process. Recent advances in general
vision-language models (VLMs) show that large-scale reinforcement learning (RL)
could significantly enhance both reasoning capabilities and overall model
performance. However, their application in medical domains is hindered by two
fundamental challenges: 1) misalignment between perceptual understanding and
reasoning stages, and 2) inconsistency between reasoning pathways and answer
generation, both compounded by the scarcity of high-quality medical datasets
for effective large-scale RL. In this paper, we first introduce Med-Zero-17K, a
curated dataset for pure RL-based training, encompassing over 30 medical image
modalities and 24 clinical tasks. Moreover, we propose a novel large-scale RL
framework for Med-VLMs, Consistency-Aware Preference Optimization (CAPO), which
integrates rewards to ensure fidelity between perception and reasoning,
consistency in reasoning-to-answer derivation, and rule-based accuracy for
final responses. Extensive experiments on both in-domain and out-of-domain
scenarios demonstrate the superiority of our method over strong VLM baselines,
showcasing strong generalization capability to 3D Med-VQA benchmarks and
R1-like training paradigms.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CAPOï¼šå¼ºåŒ–åŒ»ç–—å†³ç­–ä¸­çš„ä¸€è‡´æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed - VQAï¼‰é¢†åŸŸï¼Œå‡†ç¡®å“åº”ä¾èµ–ç²¾å‡†æ„ŸçŸ¥åŒ»å­¦å½±åƒæ•°æ®ã€åŸºäºè§†è§‰å’Œæ–‡æœ¬é—®é¢˜çš„é€»è¾‘æ¨ç†ä»¥åŠä»æ¨ç†è¿‡ç¨‹å¾—åˆ°è¿è´¯ç­”æ¡ˆè¿™ä¸‰ä¸ªå…³é”®æ­¥éª¤ã€‚é€šç”¨è§†è§‰ - è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¢å¼ºæ¨ç†èƒ½åŠ›å’Œæ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨å—ä¸¤å¤§æŒ‘æˆ˜é˜»ç¢ï¼šä¸€æ˜¯æ„ŸçŸ¥ç†è§£ä¸æ¨ç†é˜¶æ®µçš„é”™ä½ï¼Œé€šç”¨VLMsé¢„è®­ç»ƒæ—¶æ¥è§¦åŒ»ç–—å›¾åƒæœ‰é™ï¼Œæ˜“è¿‡åº¦ä¾èµ–LLM backboneçš„å‚æ•°çŸ¥è¯†ï¼Œäº§ç”Ÿè„±ç¦»åŒ»å­¦å›¾åƒçš„è™šå‡æ¨ç†ï¼›äºŒæ˜¯æ¨ç†è·¯å¾„ä¸ç­”æ¡ˆç”Ÿæˆçš„ä¸ä¸€è‡´ï¼Œä¸”é«˜è´¨é‡åŒ»ç–—æ•°æ®é›†ç¨€ç¼ºåŠ å‰§äº†è¿™äº›é—®é¢˜ã€‚åŒæ—¶ï¼Œå°†é€šç”¨é¢†åŸŸçš„RL - Zeroé£æ ¼è®­ç»ƒèŒƒå¼ç›´æ¥é€‚é…åˆ°åŒ»ç–—é¢†åŸŸæ”¹è¿›æœ‰é™ï¼Œå› æ­¤éœ€è¦æ–°æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºMed - Zero - 17Kæ•°æ®é›†
ä¸ºè§£å†³åŒ»ç–—é¢†åŸŸé«˜è´¨é‡RLæ•°æ®é›†ç¨€ç¼ºé—®é¢˜ï¼Œæ„å»ºäº†Med - Zero - 17Kæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ•´åˆå¤šä¸ªå…¬å¼€åŒ»ç–—æ•°æ®æºï¼Œåˆ©ç”¨Qwen - VL - 72Bæ¨¡å‹ä»Pubmedvisionçš„å›¾åƒ - æ ‡é¢˜æ•°æ®ç”ŸæˆVQAå¯¹ï¼Œå¢å¼ºäº†æ¨¡æ€å¤šæ ·æ€§å’Œä»»åŠ¡è¦†ç›–åº¦ã€‚é‡‡ç”¨æ··åˆéš¾åº¦æ•°æ®è¿‡æ»¤ç­–ç•¥ï¼Œä»åŸºç¡€æ¨¡å‹é‡‡æ ·10ä¸ªå“åº”ï¼ˆæ¸©åº¦è®¾ä¸º0.9ï¼‰ï¼Œæ’é™¤å…¨å¯¹æˆ–å…¨é”™çš„QAå¯¹ï¼Œä¿ç•™è‡³å°‘æœ‰ä¸€ä¸ªæ­£ç¡®å“åº”çš„å¯¹ï¼Œä¿è¯éš¾åº¦å¹³è¡¡ã€‚è¿˜å°†ä¸´åºŠé—®é¢˜åˆ†ä¸ºä¸‰ä¸ªå¤æ‚åº¦çº§åˆ«ï¼Œæ¶µç›–ä»åŸºç¡€è§†è§‰åˆ†æåˆ°å¤æ‚è¯Šæ–­æ¨ç†çš„ä»»åŠ¡ï¼ŒåŒ…å«30å¤šç§åŒ»å­¦å½±åƒæ¨¡æ€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºCAPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶
æå‡ºConsistency - Aware Preference Optimizationï¼ˆCAPOï¼‰æ¡†æ¶ç”¨äºåŒ»ç–—VLMsã€‚é™¤åŸºäºå‡†ç¡®æ€§çš„å¥–åŠ±å¤–ï¼Œæ•´åˆä¸¤ç§æ–°çš„ä¸€è‡´æ€§å¥–åŠ±ã€‚æ„ŸçŸ¥ä¸€è‡´æ€§å¥–åŠ±é€šè¿‡æ‰°åŠ¨è¾“å…¥åŒ»å­¦å›¾åƒï¼Œæš´éœ²æ–‡æœ¬åå‘çš„æ¨ç†è¿‡ç¨‹ï¼Œå¥–åŠ±æ¨¡å‹åœ¨åŸå§‹å›¾åƒä¸‹æ›´åå¥½æ­£ç¡®æ¨ç†ï¼Œé¼“åŠ±ä¸å›¾åƒä¸€è‡´çš„å¯ä¿¡æ¨ç†ï¼›å†³ç­–ä¸€è‡´æ€§å¥–åŠ±åˆ©ç”¨å¤–éƒ¨LLMä½œä¸ºè¯„åˆ¤ï¼Œè¯„ä¼°æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆä¹‹é—´çš„ä¸€è‡´æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCAPOæ¡†æ¶å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¶…è¶Šäº†ç‰¹å®šé¢†åŸŸå’Œé€šç”¨ç›®çš„çš„VLMsã€‚ä¸ä¼ ç»ŸGRPOç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½è·å¾—æ›´ç¨³å®šå’Œæ›´é«˜çš„å¥–åŠ±ï¼Œä¸”CAPOå¼•å…¥çš„ä¸€è‡´æ€§ç›¸å…³å¥–åŠ±é¼“åŠ±äº†æ›´é•¿æ›´æ·±çš„ä¸´åºŠæ¨ç†æ¨¡å¼ã€‚ä¸åŒäºä¼ ç»ŸSFTåœ¨å¦‚PMC - VQAæ•°æ®é›†ä¸Šçš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼ŒCAPOåœ¨é›¶æ ·æœ¬æ¡ä»¶ä¸‹æŒç»­å¸¦æ¥ç¨³å®šä¸”é²æ£’çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒCAPOåœ¨æœªè§è¿‡çš„æ¨¡æ€å’Œä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨OmnimedVQAå’ŒMMMU Health & MedicineåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ— éœ€è®¿é—®é‡å è®­ç»ƒæ•°æ®å°±è¶…è¶Šäº†åŸºçº¿æ¨¡å‹å’ŒSFTæ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ•°æ®é›†æ„å»ºæ–¹é¢ï¼šé¢å¯¹ç‰¹å®šé¢†åŸŸé«˜è´¨é‡æ•°æ®ç¨€ç¼ºé—®é¢˜æ—¶ï¼Œå¯å€Ÿé‰´Med - Zero - 17Kçš„æ„å»ºæ€è·¯ï¼Œæ•´åˆå¤šæºæ•°æ®ã€åˆ©ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ•°æ®å¢å¼ºå¤šæ ·æ€§ã€è®¾è®¡ç­–ç•¥ä¿è¯æ•°æ®éš¾åº¦å¹³è¡¡ç­‰ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›ä¼˜è´¨æ•°æ®æ”¯æ’‘ã€‚
2. å¼ºåŒ–å­¦ä¹ æ¡†æ¶è®¾è®¡æ–¹é¢ï¼šåœ¨è§£å†³é¢†åŸŸå†…æ¨¡å‹æ¨ç†ä¸æ„ŸçŸ¥ã€æ¨ç†ä¸ç­”æ¡ˆç”Ÿæˆä¸€è‡´æ€§é—®é¢˜æ—¶ï¼ŒCAPOçš„æ€è·¯å€¼å¾—å‚è€ƒï¼Œé€šè¿‡è®¾è®¡é’ˆå¯¹æ€§çš„ä¸€è‡´æ€§å¥–åŠ±æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œå¯åº”ç”¨åˆ°å…¶ä»–éœ€è¦ä¿è¯æ¨ç†ä¸€è‡´æ€§çš„é¢†åŸŸä»»åŠ¡ä¸­ã€‚
3. å®éªŒéªŒè¯æ–¹é¢ï¼šå…¶åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ã€é›¶æ ·æœ¬ç­‰å¤šç§åœºæ™¯ä¸‹çš„å®éªŒéªŒè¯æ–¹å¼ï¼Œä¸ºè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›ç­‰æä¾›äº†å…¨é¢çš„å‚è€ƒèŒƒå¼ï¼Œåœ¨åç»­ç ”ç©¶ä¸­å¯å€Ÿé‰´è¿™ç§å¤šåœºæ™¯éªŒè¯çš„æ€è·¯æ¥æ›´å…¨é¢è¯„ä¼°æ–¹æ³•æœ‰æ•ˆæ€§ã€‚

## doctor-approved--generating-medically-accurate-skin-disease-images-through-ai-expert-feedback
### Abstract
Paucity of medical data severely limits the generalizability of diagnostic ML
models, as the full spectrum of disease variability can not be represented by a
small clinical dataset. To address this, diffusion models (DMs) have been
considered as a promising avenue for synthetic image generation and
augmentation. However, they frequently produce medically inaccurate images,
deteriorating the model performance. Expert domain knowledge is critical for
synthesizing images that correctly encode clinical information, especially when
data is scarce and quality outweighs quantity. Existing approaches for
incorporating human feedback, such as reinforcement learning (RL) and Direct
Preference Optimization (DPO), rely on robust reward functions or demand
labor-intensive expert evaluations. Recent progress in Multimodal Large
Language Models (MLLMs) reveals their strong visual reasoning capabilities,
making them adept candidates as evaluators. In this work, we propose a novel
framework, coined MAGIC (Medically Accurate Generation of Images through
AI-Expert Collaboration), that synthesizes clinically accurate skin disease
images for data augmentation. Our method creatively translates expert-defined
criteria into actionable feedback for image synthesis of DMs, significantly
improving clinical accuracy while reducing the direct human workload.
Experiments demonstrate that our method greatly improves the clinical quality
of synthesized skin disease images, with outputs aligning with dermatologist
assessments. Additionally, augmenting training data with these synthesized
images improves diagnostic accuracy by +9.02% on a challenging 20-condition
skin disease classification task, and by +13.89% in the few-shot setting.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒ»ç”Ÿè®¤å¯ï¼šå€ŸAIä¸“å®¶åé¦ˆç”ŸæˆåŒ»å­¦å‡†ç¡®çš®è‚¤ç—…å›¾åƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨çš®è‚¤ç—…è¯Šæ–­çš„æœºå™¨å­¦ä¹ æ¨¡å‹é¢†åŸŸï¼ŒåŒ»ç–—æ•°æ®åŒ®ä¹æ˜¯ä¸€å¤§éš¾é¢˜ï¼Œå°ä¸´åºŠæ•°æ®é›†éš¾ä»¥æ¶µç›–ç–¾ç—…å˜å¼‚å…¨è²Œï¼Œé™åˆ¶äº†è¯Šæ–­æ¨¡å‹æ³›åŒ–æ€§ã€‚æ‰©æ•£æ¨¡å‹è™½ä¸ºåˆæˆå›¾åƒç”Ÿæˆä¸æ•°æ®å¢å¼ºæä¾›å¯èƒ½ï¼Œä½†ç”Ÿæˆå›¾åƒå¸¸å­˜åœ¨åŒ»å­¦ä¸å‡†ç¡®é—®é¢˜ï¼Œå½±å“æ¨¡å‹è¡¨ç°ã€‚ç°æœ‰èå…¥äººç±»åé¦ˆçš„æ–¹æ³•ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼‰ä¾èµ–å¯é å¥–åŠ±å‡½æ•°æˆ–éœ€å¤§é‡ä¸“å®¶è¯„ä¼°å·¥ä½œã€‚è€Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è§†è§‰æ¨ç†èƒ½åŠ›å¼ºï¼Œé€‚åˆä½œä¸ºè¯„ä¼°è€…ï¼ŒåŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºMAGICæ¡†æ¶è§£å†³åŒ»å­¦å›¾åƒåˆæˆå‡†ç¡®æ€§ä¸å‡å°‘ä¸“å®¶å·¥ä½œé‡é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMAGICæ¡†æ¶  
MAGICï¼ˆMedically Accurate Generation of Images through AI - Expert Collaborationï¼‰æ¡†æ¶æ—¨åœ¨åˆæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç—…å›¾åƒç”¨äºæ•°æ®å¢å¼ºã€‚è¯¥æ¡†æ¶åˆ›é€ æ€§åœ°å°†ä¸“å®¶å®šä¹‰æ ‡å‡†è½¬åŒ–ä¸ºæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å›¾åƒåˆæˆçš„å¯æ“ä½œåé¦ˆï¼Œåœ¨æå‡ä¸´åºŠå‡†ç¡®æ€§åŒæ—¶å‡å°‘äººç±»ç›´æ¥å·¥ä½œé‡ã€‚æ¡†æ¶è¿˜çº³å…¥Image - to - Imageï¼ˆI2Iï¼‰æ¨¡å—ï¼Œä»ä¸­é—´æ—¶é—´æ­¥è€Œéçº¯é«˜æ–¯å™ªå£°å¼€å§‹å»å™ªï¼ŒåŠ é€Ÿé‡‡æ ·é˜¶æ®µä¸”ä¿è¯ç—…å˜å˜æ¢è´´åˆçœŸå®æ•°æ®åˆ†å¸ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAI - ä¸“å®¶åä½œèŒƒå¼  
äººç±»ä¸“å®¶ä¸»è¦å®Œæˆä¸¤é¡¹å·¥ä½œï¼šä¸€æ˜¯ä»å¯é æ¥æºåˆ¶å®šæ˜“è¢«MLLMéªŒè¯çš„æ£€æŸ¥æ¸…å•ï¼›äºŒæ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹è®­ç»ƒæœŸé—´ç›‘ç£MLLMå¯¹åˆæˆå›¾åƒçš„åé¦ˆã€‚å€ŸåŠ©MLLMå¼ºå¤§è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå°†è§†è§‰è¯„ä¼°å·¥ä½œåœ¨æœ€å°‘ä¸“å®¶ç›‘ç£ä¸‹äº¤ç”±MLLMå®Œæˆï¼Œæœ‰æ•ˆåˆ©ç”¨é¢†åŸŸçŸ¥è¯†ä¸”æ— éœ€å¤§é‡æ ‡æ³¨å·¥ä½œï¼Œè®©æ‰©æ•£æ¨¡å‹åœ¨è¿­ä»£å­¦ä¹ ä¸­ç”Ÿæˆæ›´è´´åˆåŒ»å­¦ä¸€è‡´æ€§çš„å›¾åƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»“åˆä¸åŒå¾®è°ƒæ–¹å¼å±•ç°ä¼˜åŠ¿  
MAGICæ¡†æ¶èƒ½ä¸åŸºäºå¥–åŠ±çš„å¾®è°ƒï¼ˆRFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç»“åˆä½¿ç”¨ï¼Œä¸”åœ¨DPOä¸‹è¡¨ç°çªå‡ºã€‚MAGIC - DPO pipelineä¼˜åŒ–æ‰©æ•£æ¨¡å‹ï¼Œéšç€è®­ç»ƒæ¨è¿›å’Œæ›´å¤šå›¾åƒ - åé¦ˆå¯¹ä½¿ç”¨ï¼Œç”Ÿæˆå‡†ç¡®ä»£è¡¨æ¯ç§ç—…ç—‡ç‹¬ç‰¹è§†è§‰ç‰¹å¾çš„åˆæˆæ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜MAGICå¤§å¹…æå‡åˆæˆçš®è‚¤ç—…å›¾åƒä¸´åºŠè´¨é‡ï¼Œè¾“å‡ºä¸çš®è‚¤ç§‘åŒ»ç”Ÿè¯„ä¼°ä¸€è‡´ã€‚åœ¨å…·æŒ‘æˆ˜æ€§çš„20ç§çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç”¨åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®åè¯Šæ–­å‡†ç¡®ç‡æå‡9.02%ï¼›åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå‡†ç¡®ç‡æå‡13.89%ã€‚åŒæ—¶ï¼Œçš®è‚¤ç§‘åŒ»ç”Ÿè¯„ä¼°åˆ†æ•°æå‡ã€FrÃ©chet Inception Distanceï¼ˆFIDï¼‰åˆ†æ•°é™ä½ï¼Œè¡¨æ˜ä¸´åºŠå‡†ç¡®æ€§å’Œä¿çœŸåº¦æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¡†æ¶è®¾è®¡å±‚é¢ï¼šMAGICæ•´åˆä¸“å®¶çŸ¥è¯†åˆ°æ‰©æ•£æ¨¡å‹çš„æ€è·¯ï¼Œä¸ºé¢†åŸŸçŸ¥è¯†èå…¥ç”Ÿæˆæ¨¡å‹æä¾›èŒƒä¾‹ï¼Œä¸”I2Iæ¨¡å—åŠ é€Ÿé‡‡æ ·ä¸ä¿è¯æ•°æ®åˆ†å¸ƒè´´åˆçš„è®¾è®¡ï¼Œåœ¨å…¶ä»–éœ€æ•°æ®å¢å¼ºä¸”å…³æ³¨ç”Ÿæˆæ•ˆç‡ä¸çœŸå®æ€§åœºæ™¯å¯å€Ÿé‰´ã€‚
2. åä½œèŒƒå¼å±‚é¢ï¼šAI - ä¸“å®¶åä½œï¼Œåˆ©ç”¨MLLMæ‰¿æ‹…è§†è§‰è¯„ä¼°å‡è½»ä¸“å®¶è´Ÿæ‹…çš„æ¨¡å¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–åŒ»ç–—å›¾åƒç”šè‡³è·¨é¢†åŸŸï¼ˆå¦‚æ”¾å°„ç§‘å›¾åƒç­‰ï¼‰çš„ç”Ÿæˆä¸è¯„ä¼°ä»»åŠ¡ï¼Œä¸ºè§£å†³é¢†åŸŸä¸“å®¶èµ„æºæœ‰é™é—®é¢˜æä¾›æ€è·¯ã€‚
3. åº”ç”¨æ•ˆæœå±‚é¢ï¼šåœ¨æ•°æ®å¢å¼ºåæå‡æ¨¡å‹è¯Šæ–­æ€§èƒ½ï¼Œè¯æ˜åˆæˆé«˜è´¨é‡åŒ»å­¦å›¾åƒå¯¹ä¸‹æ¸¸ä»»åŠ¡ä»·å€¼ï¼Œä¸ºåŒ»ç–—AIé¢†åŸŸæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹æ¨¡å‹æ€§èƒ½æå‡æä¾›æœ‰æ•ˆé€”å¾„ã€‚

## rarl--improving-medical-vlm-reasoning-and-generalization-with-reinforcement-learning-and-lora-under-data-and-hardware-constraints
### Abstract
The growing integration of vision-language models (VLMs) in medical
applications offers promising support for diagnostic reasoning. However,
current medical VLMs often face limitations in generalization, transparency,
and computational efficiency-barriers that hinder deployment in real-world,
resource-constrained settings. To address these challenges, we propose a
Reasoning-Aware Reinforcement Learning framework, \textbf{RARL}, that enhances
the reasoning capabilities of medical VLMs while remaining efficient and
adaptable to low-resource environments. Our approach fine-tunes a lightweight
base model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward
functions that jointly consider diagnostic accuracy and reasoning quality.
Training is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the
feasibility of deploying such models in constrained environments. We evaluate
the model using an LLM-as-judge framework that scores both correctness and
explanation quality. Experimental results show that RARL significantly improves
VLM performance in medical image analysis and clinical reasoning, outperforming
supervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while
requiring fewer computational resources. Additionally, we demonstrate the
generalization capabilities of our approach on unseen datasets, achieving
around 27% improved performance compared to supervised fine-tuning and about 4%
over traditional RL fine-tuning. Our experiments also illustrate that diversity
prompting during training and reasoning prompting during inference are crucial
for enhancing VLM performance. Our findings highlight the potential of
reasoning-guided learning and reasoning prompting to steer medical VLMs toward
more transparent, accurate, and resource-efficient clinical decision-making.
Code and data are publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RARLï¼šèµ„æºå—é™ä¸‹ç”¨å¼ºåŒ–å­¦ä¹ ä¸LoRAæå‡åŒ»ç–—VLMæ¨ç†ä¸æ³›åŒ–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŒ»ç–—åº”ç”¨ä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½ä¸ºè¯Šæ–­æ¨ç†æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œä½†å½“å‰åŒ»ç–—VLMsåœ¨æ³›åŒ–æ€§ã€é€æ˜åº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨å±€é™ï¼Œé˜»ç¢äº†å…¶åœ¨èµ„æºå—é™çš„çœŸå®åœºæ™¯ä¸­éƒ¨ç½²ã€‚æ¯”å¦‚é«˜æ€§èƒ½åŒ»ç–—VLMä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†ä¸å¤§é‡è®¡ç®—èµ„æºï¼Œå°åŒ»ç–—æœºæ„éš¾ä»¥æ‰¿å—ï¼›ä¸”å¾ˆå¤šæ¨¡å‹é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–æ•°æ®é›†ä¼˜åŒ–ï¼Œå¯¹æœªè§è¿‡çš„ä¸´åºŠåœºæ™¯æ³›åŒ–å·®ï¼ŒåŒæ—¶ç¼ºä¹ä¸´åºŠä¿¡ä»»æ‰€éœ€çš„é€æ˜æ¨ç†èƒ½åŠ›ï¼Œè¯„ä¼°ä¹Ÿå¸¸å¿½ç•¥æ¨ç†è¿‡ç¨‹åªå…³æ³¨æœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºRARLæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºRARLæ¡†æ¶  
ç»“åˆReasoning - Aware Reinforcement Learningï¼ˆæ¨ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼‰ä¸Low - Rank Adaptationï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œå¢å¼ºåŒ»ç–—VLMsæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿è¯æ•ˆç‡å¹¶é€‚åº”ä½èµ„æºç¯å¢ƒã€‚ä»¥è½»é‡çº§æ¨¡å‹Qwen2 - VL - 2B - Instructä¸ºåŸºç¡€ï¼Œç”¨LoRAå’Œè€ƒè™‘è¯Šæ–­å‡†ç¡®æ€§ä¸æ¨ç†è´¨é‡çš„è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°è¿›è¡Œå¾®è°ƒã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå•GPUè®­ç»ƒéªŒè¯å¯è¡Œæ€§  
åœ¨å•ä¸ªNVIDIA A100 - PCIE - 40GB GPUä¸Šå®Œæˆè®­ç»ƒï¼Œè¯æ˜äº†åœ¨å—é™ç¯å¢ƒéƒ¨ç½²æ­¤ç±»æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºèµ„æºæœ‰é™çš„åŒ»ç–—åœºæ™¯åº”ç”¨æä¾›å¯èƒ½ã€‚  
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šLLM - as - judgeè¯„ä¼°æ¡†æ¶  
é‡‡ç”¨è¯¥æ¡†æ¶è¯„ä¼°æ¨¡å‹ï¼Œå…¨é¢è€ƒé‡ç­”æ¡ˆæ­£ç¡®æ€§ä¸æ¨ç†è´¨é‡ï¼Œå¼¥è¡¥äº†ä»¥å¾€è¯„ä¼°åªå…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„ä¸è¶³ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åŒ»ç–—å›¾åƒåˆ†æå’Œä¸´åºŠæ¨ç†ä»»åŠ¡ä¸­ï¼ŒRARLæ˜¾è‘—æå‡VLMæ€§èƒ½ï¼šåœ¨æ¨ç†èšç„¦ä»»åŠ¡ä¸Šï¼Œæ¯”æœ‰ç›‘ç£å¾®è°ƒæ€§èƒ½æå‡çº¦7.78%ä¸”è®¡ç®—èµ„æºéœ€æ±‚æ›´å°‘ï¼›åœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šæ³›åŒ–èƒ½åŠ›å¼ºï¼Œæ¯”æœ‰ç›‘ç£å¾®è°ƒæ€§èƒ½æå‡çº¦27%ï¼Œæ¯”ä¼ ç»ŸRLå¾®è°ƒæå‡çº¦4%ï¼›è¿˜è¡¨æ˜è®­ç»ƒæ—¶çš„å¤šæ ·æ€§æç¤ºå’Œæ¨ç†æ—¶çš„æ¨ç†æç¤ºå¯¹æå‡VLMæ€§èƒ½è‡³å…³é‡è¦ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èµ„æºé«˜æ•ˆåˆ©ç”¨æ€è·¯ï¼šRARLç»“åˆRLä¸LoRAï¼Œåœ¨å•GPUä¸Šå®ç°æœ‰æ•ˆè®­ç»ƒï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹æ¨¡å‹ä¼˜åŒ–æä¾›äº†èŒƒä¾‹ï¼Œå°æœºæ„æˆ–ä½èµ„æºåŒ»ç–—åœºæ™¯å¯å‚è€ƒè¿™ç§èµ„æºé«˜æ•ˆåˆ©ç”¨çš„æ¨¡å‹è®­ç»ƒæ–¹å¼ã€‚  
2. æ¨ç†ä¸æ³›åŒ–èƒ½åŠ›æå‡ï¼šé€šè¿‡æ¨ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ å…³æ³¨æ¨ç†è´¨é‡ä¸è¯Šæ–­å‡†ç¡®æ€§ï¼Œä»¥åŠå¯¹æ³›åŒ–èƒ½åŠ›çš„æå‡ç­–ç•¥ï¼Œä¸ºåŒ»ç–—é¢†åŸŸåŠå…¶ä»–é¢†åŸŸVLMsæå‡æ¨ç†å’Œæ³›åŒ–æ€§èƒ½æä¾›äº†æ–¹æ³•å‚è€ƒã€‚  
3. è¯„ä¼°æ–¹å¼åˆ›æ–°ï¼šLLM - as - judgeæ¡†æ¶å…¼é¡¾ç­”æ¡ˆæ­£ç¡®æ€§ä¸æ¨ç†è´¨é‡ï¼Œåœ¨éœ€è¦å…³æ³¨è¿‡ç¨‹çš„AIä»»åŠ¡è¯„ä¼°ä¸­ï¼Œè¿™ç§å…¨é¢è¯„ä¼°çš„æ€è·¯å€¼å¾—å€Ÿé‰´ã€‚  
4. æç¤ºç­–ç•¥é‡è¦æ€§ï¼šè®­ç»ƒæ—¶å¤šæ ·æ€§æç¤ºå’Œæ¨ç†æ—¶æ¨ç†æç¤ºå¯¹æ€§èƒ½æå‡çš„å‘ç°ï¼Œä¸ºåç»­æ¨¡å‹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„ä¼˜åŒ–æä¾›äº†æ–¹å‘ï¼Œå¯åœ¨å…¶ä»–VLMsä»»åŠ¡ä¸­å°è¯•æ­¤ç±»æç¤ºç­–ç•¥æ¥æå‡æ€§èƒ½ã€‚

## doctoragent-rl--a-multi-agent-collaborative-reinforcement-learning-system-for-multi-turn-clinical-dialogue
### Abstract
Large language models (LLMs) have demonstrated excellent capabilities in the
field of biomedical question answering, but their application in real-world
clinical consultations still faces core challenges. Existing systems rely on a
one-way information transmission mode where patients must fully describe their
symptoms in a single round, leading to nonspecific diagnostic recommendations
when complaints are vague. Traditional multi-turn dialogue methods based on
supervised learning are constrained by static data-driven paradigms, lacking
generalizability and struggling to intelligently extract key clinical
information. To address these limitations, we propose DoctorAgent-RL, a
reinforcement learning (RL)-based multi-agent collaborative framework that
models medical consultations as a dynamic decision-making process under
uncertainty. The doctor agent continuously optimizes its questioning strategy
within the RL framework through multi-turn interactions with the patient agent,
dynamically adjusting its information-gathering path based on comprehensive
rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables
LLMs to autonomously develop interaction strategies aligned with clinical
reasoning logic, rather than superficially imitating patterns in existing
dialogue data. Notably, we constructed MTMedDialog, the first English
multi-turn medical consultation dataset capable of simulating patient
interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing
models in both multi-turn reasoning capability and final diagnostic
performance, demonstrating practical value in assisting clinical consultations.
https://github.com/JarvisUSTC/DoctorAgent-RL
### ğŸŒŸ è®ºæ–‡è§£è¯» | DoctorAgent-RLï¼šç”¨å¼ºåŒ–å­¦ä¹ é‡å¡‘å¤šè½®ä¸´åºŠå¯¹è¯çš„æ™ºèƒ½è¯Šç–—ç³»ç»Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦é—®ç­”é¢†åŸŸå±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†åœ¨çœŸå®ä¸´åºŠé—®è¯Šåœºæ™¯ä¸­ä»é¢ä¸´æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç°æœ‰ç³»ç»Ÿå¤šé‡‡ç”¨å•å‘ä¿¡æ¯ä¼ é€’æ¨¡å¼ï¼Œè¦æ±‚æ‚£è€…å•è½®å®Œæ•´æè¿°ç—‡çŠ¶ï¼Œæ¨¡ç³Šä¸»è¯‰ä¸‹æ˜“äº§ç”Ÿéç‰¹å¼‚æ€§è¯Šæ–­å»ºè®®ï¼›ä¼ ç»ŸåŸºäºç›‘ç£å­¦ä¹ çš„å¤šè½®å¯¹è¯æ–¹æ³•å—é™äºé™æ€æ•°æ®é©±åŠ¨èŒƒå¼ï¼Œç¼ºä¹æ³›åŒ–æ€§ä¸”éš¾æ™ºèƒ½æå–å…³é”®ä¸´åºŠä¿¡æ¯ã€‚ä¸´åºŠè¯Šæ–­éœ€ä¸»åŠ¨ã€å¤šè½®åœ°é’ˆå¯¹æ€§é—®è¯Šæ¥æ˜ç¡®ç—…æƒ…ï¼Œè€Œç°æœ‰æ¨¡å¼ä¸è¿™ä¸€éœ€æ±‚å­˜åœ¨æœ¬è´¨çŸ›ç›¾ï¼Œå‚¬ç”Ÿäº†å¯¹åŠ¨æ€å†³ç­–å¼ä¸´åºŠå¯¹è¯æ¡†æ¶çš„è¿«åˆ‡éœ€æ±‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºDoctorAgent - RLå¤šæ™ºèƒ½ä½“åä½œå¼ºåŒ–å­¦ä¹ æ¡†æ¶  
å°†ä¸´åºŠé—®è¯Šå»ºæ¨¡ä¸ºä¸ç¡®å®šæ€§ä¸‹çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œæ„å»ºä¸‰ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“ï¼šåŸºäºLLMçš„é«˜ä¿çœŸæ‚£è€…æ™ºèƒ½ä½“ï¼Œèƒ½æ¨¡æ‹ŸçœŸå®æ²Ÿé€šå¤šæ ·æ€§å¹¶ç”Ÿæˆç—…ç†ä¸€è‡´çš„å›åº”ï¼›ç”±çœŸå®é—®è¯Šè®°å½•å…‹éš†åŒ»ç–—è¡Œä¸ºåˆå§‹åŒ–ã€ç»å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–çš„åŒ»ç”Ÿæ™ºèƒ½ä½“ï¼Œä»¥æ­¤æŒæ¡æœ‰æ•ˆé—®è¯Šç­–ç•¥ï¼›ä¾æ®è¯Šæ–­å‡†ç¡®æ€§ã€æ‚£è€…ä¿¡æ¯å“åº”åº¦å’Œé—®é¢˜è§„èŒƒæ€§æä¾›å¤šç»´åº¦å¥–åŠ±çš„é—®è¯Šè¯„ä¼°å™¨ã€‚é€šè¿‡RLæ¡†æ¶ï¼ŒåŒ»ç”Ÿæ™ºèƒ½ä½“åœ¨ä¸æ‚£è€…æ™ºèƒ½ä½“å¤šè½®äº¤äº’ä¸­æŒç»­ä¼˜åŒ–é—®è¯Šç­–ç•¥ï¼ŒåŸºäºè¯„ä¼°å™¨ç»¼åˆå¥–åŠ±åŠ¨æ€è°ƒæ•´ä¿¡æ¯æ”¶é›†è·¯å¾„ï¼Œè®©LLMè‡ªä¸»å‘å±•å¥‘åˆä¸´åºŠæ¨ç†é€»è¾‘çš„äº¤äº’ç­–ç•¥ï¼Œè€Œéè¡¨é¢æ¨¡ä»¿ç°æœ‰å¯¹è¯æ•°æ®æ¨¡å¼ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºMTMedDialogå¤šè½®åŒ»ç–—å¯¹è¯æ•°æ®é›†  
æ‰“é€ é¦–ä¸ªå¯æ¨¡æ‹ŸçœŸå®æ‚£è€…äº¤äº’çš„è‹±æ–‡å¤šè½®åŒ»ç–—é—®è¯Šæ•°æ®é›†MTMedDialogï¼Œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨åŠ¨æ€ä¸´åºŠæ¨ç†åœºæ™¯ä¸‹çš„è®­ç»ƒä¸è¯„ä¼°æä¾›æ•°æ®æ”¯æ’‘ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸé«˜è´¨é‡å¤šè½®äº¤äº’æ•°æ®é›†çš„ç©ºç™½ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒè¡¨æ˜ï¼ŒDoctorAgent - RLåœ¨å¤šè½®æ¨ç†èƒ½åŠ›å’Œæœ€ç»ˆè¯Šæ–­æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰æ¨¡å‹ï¼ˆå¦‚åœ¨è¯Šæ–­ä¸å»ºè®®çš„å¹³å‡å‡†ç¡®ç‡åˆ†æ•°ã€å¹³å‡äº¤äº’è½®æ¬¡ç­‰ç»´åº¦è¡¨ç°æ›´ä¼˜ï¼Œè§å›¾1å¯¹æ¯”ï¼‰ï¼Œåœ¨è¾…åŠ©ä¸´åºŠé—®è¯Šä¸­å±•ç°å‡ºå®ç”¨ä»·å€¼ï¼Œèƒ½æ›´é«˜æ•ˆåœ°é€šè¿‡å¤šè½®äº¤äº’å®ç°ç²¾å‡†è¯Šæ–­ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•å±‚é¢ï¼Œåˆ›æ–°æ€§åœ°å°†å¼ºåŒ–å­¦ä¹ ä¸å¤šæ™ºèƒ½ä½“åä½œç»“åˆï¼Œä¸ºè§£å†³é™æ€æ•°æ®é©±åŠ¨èŒƒå¼å±€é™ã€å®ç°åŠ¨æ€å†³ç­–å¼çš„é¢†åŸŸå¯¹è¯ç³»ç»Ÿæä¾›äº†èŒƒæœ¬ï¼Œå¯å¯å‘å…¶ä»–å‚ç›´é¢†åŸŸï¼ˆå¦‚é‡‘èå’¨è¯¢ã€æ•™è‚²è¾…å¯¼ç­‰ï¼‰æ„å»ºæ™ºèƒ½äº¤äº’æ¡†æ¶ï¼›ä»æ•°æ®å±‚é¢ï¼Œæ„å»ºä¸“å±å¤šè½®å¯¹è¯æ•°æ®é›†çš„æ€è·¯ï¼Œä¸ºé¢†åŸŸå†…æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°æ•°æ®èµ„æºå»ºè®¾æä¾›äº†å€Ÿé‰´ï¼ŒåŠ©åŠ›åç»­ç ”ç©¶çªç ´æ•°æ®ç“¶é¢ˆï¼›ä»åº”ç”¨å±‚é¢ï¼ŒéªŒè¯äº†å¼ºåŒ–å­¦ä¹ é©±åŠ¨ä¸‹æ™ºèƒ½ä½“è‡ªä¸»ä¼˜åŒ–äº¤äº’ç­–ç•¥åœ¨å¤æ‚ä¸“ä¸šåœºæ™¯çš„å¯è¡Œæ€§ï¼Œä¸ºAIè¾…åŠ©ä¸“ä¸šæœåŠ¡ï¼ˆå¦‚ä¸´åºŠé—®è¯Šï¼‰çš„è½åœ°æä¾›äº†æ–°è·¯å¾„ã€‚

## improving-medical-reasoning-with-curriculum-aware-reinforcement-learning
### Abstract
Recent advances in reinforcement learning with verifiable, rule-based rewards
have greatly enhanced the reasoning capabilities and out-of-distribution
generalization of VLMs/LLMs, obviating the need for manually crafted reasoning
chains. Despite these promising developments in the general domain, their
translation to medical imaging remains limited. Current medical reinforcement
fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby
restricting the model's ability to engage in world knowledge retrieval and
flexible task adaptation. More critically, these methods fall short of
addressing the critical clinical demand for open-ended, reasoning-intensive
decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first
multimodal reinforcement learning framework tailored for medical VQA that
unifies close-ended and open-ended data within a curriculum-driven RFT
paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of
close-ended medical VQA tasks to establish domain-grounded reasoning
capabilities, and is then progressively adapted to open-ended tasks to foster
deeper knowledge enhancement and clinical interpretability. We validate MedCCO
across eight challenging medical VQA benchmarks, spanning both close-ended and
open-ended settings. Experimental results show that MedCCO consistently
enhances performance and generalization, achieving a 11.4\% accuracy gain
across three in-domain tasks, and a 5.7\% improvement on five out-of-domain
benchmarks. These findings highlight the promise of curriculum-guided RL in
advancing robust, clinically-relevant reasoning in medical multimodal language
models.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨è¯¾ç¨‹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æå‡åŒ»ç–—æ¨ç†ï¼šMedCCOæ¡†æ¶æ¥è¢­

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨é€šç”¨é¢†åŸŸï¼ŒåŸºäºå¯éªŒè¯ã€è§„åˆ™å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ å·²å¤§å¹…æå‡å¤§æ¨¡å‹æ¨ç†ä¸æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨åŒ»ç–—å½±åƒé¢†åŸŸçš„åº”ç”¨æœ‰é™ã€‚å½“å‰åŒ»ç–—å¼ºåŒ–å¾®è°ƒæ–¹æ³•å¤šèšç„¦å°é—­åŸŸVQAï¼ˆè§†è§‰é—®ç­”ï¼‰ï¼Œé™åˆ¶äº†æ¨¡å‹çŸ¥è¯†æ£€ç´¢ä¸ä»»åŠ¡é€‚é…èƒ½åŠ›ï¼Œä¹Ÿéš¾ä»¥æ»¡è¶³ä¸´åºŠå¼€æ”¾å¼ã€é«˜æ¨ç†å†³ç­–éœ€æ±‚ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºé¢å‘åŒ»ç–—VQAçš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¡†æ¶MedCCOã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–ä¸ªç»Ÿä¸€å¤„ç†å°é—­ä¸å¼€æ”¾å¼åŒ»ç–—VQAçš„å¤šæ¨¡æ€æ¡†æ¶  
MedCCOæ˜¯é¦–ä¸ªä¸ºåŒ»ç–—VQAé‡èº«æ‰“é€ çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåœ¨è¯¾ç¨‹é©±åŠ¨çš„å¼ºåŒ–å¾®è°ƒèŒƒå¼ä¸‹æ•´åˆå°é—­åŸŸä¸å¼€æ”¾åŸŸæ•°æ®ã€‚å…ˆåŸºäºå¤šæ ·å°é—­åŸŸåŒ»ç–—VQAä»»åŠ¡å¾®è°ƒï¼Œå»ºç«‹é¢†åŸŸåŸºç¡€æ¨ç†èƒ½åŠ›ï¼›å†é€æ­¥é€‚é…å¼€æ”¾åŸŸä»»åŠ¡ï¼Œå¼ºåŒ–çŸ¥è¯†å¢å¼ºä¸ä¸´åºŠå¯è§£é‡Šæ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯¾ç¨‹å¼å¼ºåŒ–å¾®è°ƒç­–ç•¥  
é‡‡ç”¨ä¸¤é˜¶æ®µè¯¾ç¨‹é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼šç¬¬ä¸€é˜¶æ®µå€Ÿå°é—­åŸŸåŒ»ç–—VQAçš„å¼ºåŒ–å­¦ä¹ è·å–é¢†åŸŸç‰¹å®šæ¨ç†èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µä»¥æ¸è¿›æ–¹å¼é€‚é…æ›´å…·æŒ‘æˆ˜çš„å¼€æ”¾åŸŸä»»åŠ¡ã€‚è¿™ç§è®­ç»ƒ scheme å¹³è¡¡äº†åˆ¤åˆ«å‡†ç¡®æ€§ä¸ç”Ÿæˆçµæ´»æ€§ï¼Œè®©æ¨¡å‹æ— éœ€äººå·¥æ ‡æ³¨å°±èƒ½æ£€ç´¢çŸ¥è¯†ã€æ„å»ºæ¨ç†é“¾ã€‚åŒæ—¶ï¼Œæ¶ˆèå®éªŒè¡¨æ˜â€œå…ˆå°é—­åå¼€æ”¾â€çš„è¯¾ç¨‹è®­ç»ƒï¼Œæ¯”åŒæ—¶è®­ç»ƒä¸¤ç±»ä»»åŠ¡æ•ˆæœæ›´ä¼˜ï¼Œé¿å…äº†ç¦»æ•£ä¸è¿ç»­å¥–åŠ±å†²çªåŠä»»åŠ¡éš¾åº¦å·®å¼‚å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨8ä¸ªåŒ»ç–—VQAåŸºå‡†ï¼ˆæ¶µç›–å°é—­ä¸å¼€æ”¾åœºæ™¯ï¼‰éªŒè¯MedCCOï¼šåŸŸå†…3é¡¹ä»»åŠ¡ç²¾åº¦æå‡11.4%ï¼›5é¡¹åŸŸå¤–åŸºå‡†æ€§èƒ½æå‡5.7%ï¼›SLAKEåŸºå‡†è¯„ä¼°ä¹Ÿå±•ç°è·¨æ¨¡æ€é²æ£’æ€§ï¼ŒæŒç»­è¶…è¶Šç«å“åŸºçº¿ï¼Œè¯æ˜è¯¾ç¨‹å¼•å¯¼å¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—å¤šæ¨¡æ€æ¨¡å‹ä¸­æå‡æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»»åŠ¡ç»Ÿä¸€è§†è§’ï¼šå°†å°é—­ä¸å¼€æ”¾å¼ä»»åŠ¡çº³å…¥åŒä¸€æ¡†æ¶ï¼Œä¸ºå¤šç±»å‹åŒ»ç–—é—®ç­”ä»»åŠ¡èåˆæä¾›æ€è·¯ï¼Œå¯å‘è·¨ä»»åŠ¡åœºæ™¯çš„æ¨¡å‹è®¾è®¡ã€‚  
2. è¯¾ç¨‹å­¦ä¹ èŒƒå¼ï¼šâ€œä»ç®€åˆ°éš¾â€çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥ï¼Œåœ¨ä¿ç•™å·²æœ‰çŸ¥è¯†åŒæ—¶æ³¨å…¥æ–°èƒ½åŠ›ï¼Œä¸ºéœ€åˆ†é˜¶æ®µæå‡çš„å¤æ‚ä»»åŠ¡ï¼ˆå¦‚åŒ»ç–—ã€æ•™è‚²ç­‰é¢†åŸŸï¼‰æä¾›è®­ç»ƒèŒƒå¼å‚è€ƒã€‚  
3. åŒ»ç–—é¢†åŸŸé€‚é…ï¼šé’ˆå¯¹åŒ»ç–—VQAè®¾è®¡å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨å‚ç›´é¢†åŸŸï¼ˆåŒ»ç–—å½±åƒ+è¯­è¨€ï¼‰çš„è½åœ°ï¼Œä¸ºè¡Œä¸šåº”ç”¨ä¸­æ¨¡å‹æ¨ç†èƒ½åŠ›ä¼˜åŒ–æä¾›å®è·µèŒƒä¾‹ã€‚  
```

## beyond-distillation--pushing-the-limits-of-medical-llm-reasoning-with-minimalist-rule-based-rl
### Abstract
Improving performance on complex tasks and enabling interpretable decision
making in large language models (LLMs), especially for clinical applications,
requires effective reasoning. Yet this remains challenging without supervised
fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from
closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the
first medical LLM to show that reasoning capability can emerge purely through
reinforcement learning (RL), using minimalist rule-based rewards on public
multiple-choice QA datasets, without relying on SFT or distilled CoT data.
AlphaMed achieves state-of-the-art results on six medical QA benchmarks,
outperforming models trained with conventional SFT+RL pipelines. On challenging
benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source
models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the
factors behind this success, we conduct a comprehensive data-centric analysis
guided by three questions: (i) Can minimalist rule-based RL incentivize
reasoning without distilled CoT supervision? (ii) How do dataset quantity and
diversity impact reasoning? (iii) How does question difficulty shape the
emergence and generalization of reasoning? Our findings show that dataset
informativeness is a key driver of reasoning performance, and that minimalist
RL on informative, multiple-choice QA data is effective at inducing reasoning
without CoT supervision. We also observe divergent trends across benchmarks,
underscoring limitations in current evaluation and the need for more
challenging, reasoning-oriented medical QA benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— éœ€è’¸é¦ï¼ç”¨æç®€è§„åˆ™å¼ºåŒ–å­¦ä¹ çªç ´åŒ»ç–—å¤§æ¨¡å‹æ¨ç†æé™

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¦å®Œæˆå¤æ‚ä»»åŠ¡å¹¶å®ç°å¯è§£é‡Šçš„å†³ç­–ï¼Œæœ‰æ•ˆæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä½†å½“å‰å¤šæ•°åŒ»ç–—LLMè·å–æ¨ç†èƒ½åŠ›ä¾èµ–äºåœ¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè€Œè¿™ç±»CoTæ•°æ®è¦ä¹ˆäººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆä»é—­æºæ¨¡å‹ï¼ˆå¦‚GPT - 4oï¼‰è’¸é¦è€Œæ¥ï¼Œå­˜åœ¨å¯æ‰©å±•æ€§ä¸å¯åŠæ€§éš¾é¢˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡è¯•å›¾æ¢ç´¢ï¼šèƒ½å¦åœ¨ä¸ä¾èµ–è’¸é¦CoTæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æç®€è§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼Ÿ

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAlphaMedæ¨¡å‹ï¼Œé¦–æ¬¡ä»…é€šè¿‡æç®€è§„åˆ™å¼ºåŒ–å­¦ä¹ æ¿€å‘æ¨ç†èƒ½åŠ›  
AlphaMedæ‘†è„±äº†å¯¹SFTå’Œè’¸é¦CoTæ•°æ®çš„ä¾èµ–ï¼Œç›´æ¥åŸºäºå…¬å¼€å¤šé€‰é—®ç­”ï¼ˆQAï¼‰æ•°æ®é›†è®¾è®¡ç®€å•çš„è§„åˆ™å¥–åŠ±ï¼Œå€ŸåŠ©å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¯æ˜äº†åŒ»ç–—LLMçš„æ¨ç†èƒ½åŠ›å¯ä»¥åœ¨æ— è’¸é¦CoTæ•°æ®ç›‘ç£ä¸‹ï¼Œé€šè¿‡æç®€è§„åˆ™RLåŸ¹å…»å‡ºæ¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå›´ç»•æ•°æ®å¼€å±•å…¨é¢åˆ†æï¼Œæ­ç¤ºæ¨ç†èƒ½åŠ›æå‡å…³é”®å› ç´   
ä»ä¸‰ä¸ªé—®é¢˜å‡ºå‘åˆ†æï¼šè§„åˆ™RLèƒ½å¦æ— CoTç›‘ç£æ¿€åŠ±æ¨ç†ã€æ•°æ®é›†æ•°é‡å’Œå¤šæ ·æ€§å¦‚ä½•å½±å“æ¨ç†ã€é—®é¢˜éš¾åº¦æ€æ ·å¡‘é€ æ¨ç†æ³›åŒ–ã€‚å‘ç°æ•°æ®é›†ä¿¡æ¯æ€§æ˜¯æ¨ç†æ€§èƒ½å…³é”®é©±åŠ¨å› ç´ ï¼Œä¿¡æ¯ä¸°å¯Œçš„å¤šé€‰QAæ•°æ®ä¸Šçš„æç®€RLèƒ½æœ‰æ•ˆè¯±å¯¼æ¨ç†ï¼›åŒæ—¶æ¨ç†å¯åœ¨ä½éš¾åº¦æ•°æ®ä¸‹è¢«æ¿€åŠ±ï¼Œé«˜éš¾åº¦æ•°æ®èƒ½å¢å¼ºæ¨ç†ï¼Œä¸”ä¸åŒéš¾åº¦æ··åˆå¯¹æ³›åŒ–å¾ˆå…³é”®ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
AlphaMedåœ¨å…­ä¸ªåŒ»ç–—QAåŸºå‡†æµ‹è¯•ä¸­å–å¾—state - of - the - artç»“æœï¼Œè¶…è¶Šäº†ç”¨ä¼ ç»ŸSFT + RL pipelineè®­ç»ƒçš„æ¨¡å‹ï¼›åœ¨å¦‚MedXpertç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ä¸Šï¼Œç”šè‡³è¶…è¿‡äº†æ›´å¤§è§„æ¨¡æˆ–é—­æºçš„æ¨¡å‹ï¼ˆå¦‚DeepSeek - V3 - 671Bã€Claude - 3.5 - Sonnetï¼‰ã€‚æ­¤å¤–ï¼Œä¸åŒåŸºå‡†ä¸Šçš„å®éªŒè¿˜å‘ç°å½“å‰è¯„ä¼°å­˜åœ¨å±€é™æ€§ï¼Œå‡¸æ˜¾äº†éœ€è¦æ›´å…·æŒ‘æˆ˜æ€§ã€é¢å‘æ¨ç†çš„åŒ»ç–—QAåŸºå‡†ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å‹è®­ç»ƒæ€è·¯åˆ›æ–°ï¼šå±•ç¤ºäº†ä¸ä¾èµ–æ˜‚è´µCoTæ•°æ®å’ŒSFTï¼Œä»…ç”¨è§„åˆ™RLè®­ç»ƒå‡ºå¼ºæ¨ç†èƒ½åŠ›æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºåŒ»ç–—LLMè®­ç»ƒå¼€è¾Ÿæ–°è·¯å¾„ã€‚  
2. æ•°æ®é©±åŠ¨åˆ†æï¼šä»æ•°æ®æ•°é‡ã€å¤šæ ·æ€§ã€ä¿¡æ¯æ€§ã€é—®é¢˜éš¾åº¦ç­‰ç»´åº¦åˆ†æå¯¹æ¨ç†çš„å½±å“ï¼Œä¸ºåç»­åŒ»ç–—é¢†åŸŸæ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒä¸­æ•°æ®é€‰æ‹©æä¾›äº†åˆ†ææ¡†æ¶ä¸å‚è€ƒä¾æ®ã€‚  
3. è¯„ä¼°æ–¹å‘å¯ç¤ºï¼šå®éªŒä¸­å‘ç°ç°æœ‰åŸºå‡†å±€é™æ€§ï¼Œæç¤ºåç»­éœ€æ‰“é€ æ›´å…·æŒ‘æˆ˜æ€§ã€èšç„¦æ¨ç†çš„åŒ»ç–—QAè¯„ä¼°åŸºå‡†ï¼Œè¿™å¯¹æ•´ä¸ªåŒ»ç–—NLPè¯„ä¼°ç”Ÿæ€å‘å±•æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

## wingpt-3-0-technical-report
### Abstract
Current Large Language Models (LLMs) exhibit significant limitations, notably
in structured, interpretable, and verifiable medical reasoning, alongside
practical deployment challenges related to computational resources and data
privacy. This report focused on the development of WiNGPT-3.0, the 32-billion
parameter LLMs, engineered with the objective of enhancing its capacity for
medical reasoning and exploring its potential for effective integration within
healthcare IT infrastructures. The broader aim is to advance towards clinically
applicable models. The approach involved a multi-stage training pipeline
tailored for general, medical, and clinical reasoning. This pipeline
incorporated supervised fine-tuning (SFT) and reinforcement learning (RL),
leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward
models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0
demonstrated strong performance: specific model variants achieved scores of
66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training
improved performance on a clinical reasoning task from a baseline score of 58.1
to 62.5. These findings suggest that reinforcement learning, even when applied
with a limited dataset of only a few thousand examples, can enhance medical
reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited
data and computation paves the way for more trustworthy and practically
deployable LLMs within clinical workflows and health information
infrastructures.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WiNGPT - 3.0ï¼šåŒ»ç–—åœºæ™¯ä¸‹å¤§æ¨¡å‹çš„çªç ´ä¸å®è·µ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨è¯¸å¤šå±€é™ï¼Œä¸€æ–¹é¢ï¼Œåœ¨ç»“æ„åŒ–ã€å¯è§£é‡Šä¸”å¯éªŒè¯çš„åŒ»ç–—æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¤§æ¨¡å‹æ¨ç†å¤šä¾èµ–æ¨¡å¼è¯†åˆ«è€ŒéåŠ¨æ€å¤šå› ç´ ä¸´åºŠè¯„ä¼°ï¼Œéš¾ä»¥è¿½æº¯å’ŒéªŒè¯ï¼›å¦ä¸€æ–¹é¢ï¼Œåœ¨åŒ»ç–—åœºæ™¯éƒ¨ç½²æ—¶é¢ä¸´è®¡ç®—èµ„æºéœ€æ±‚é«˜ã€æ•°æ®éšç§ä¸¥æ ¼ç­‰æŒ‘æˆ˜ï¼Œä¸”å¤§æ¨¡å‹æ˜“äº§ç”Ÿâ€œå¹»è§‰â€ã€çŸ¥è¯†æ›´æ–°éš¾ã€é€‚é…ä¸åŒåŒ»ç–—æ ‡å‡†å¤æ‚ç­‰é—®é¢˜ã€‚åŒæ—¶ï¼ŒåŒ»ç–—ä¸“ä¸šäººå‘˜ä¹Ÿåé¦ˆäº†è¿™äº›ç—›ç‚¹ã€‚ä¸ºå…‹æœè¿™äº›éšœç¢ï¼Œå›¢é˜Ÿå¼€å‘äº†åŒ»ç–—é¢†åŸŸé’ˆå¯¹æ€§çš„å¤§æ¨¡å‹WiNGPT - 3.0ï¼Œè‡´åŠ›äºæå‡åŒ»ç–—æ¨ç†èƒ½åŠ›å¹¶æ¢ç´¢å…¶åœ¨åŒ»ç–—ITåŸºç¡€è®¾æ–½é›†æˆçš„æ½œåŠ›ï¼Œå‘ä¸´åºŠé€‚ç”¨æ¨¡å‹è¿ˆè¿›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šé˜¶æ®µè®­ç»ƒ pipeline é€‚é…åŒ»ç–—æ¨ç†
è®¾è®¡äº†é’ˆå¯¹é€šç”¨ã€åŒ»ç–—å’Œä¸´åºŠæ¨ç†çš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œæ•´åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚åˆ©ç”¨ç²¾å¿ƒç­–åˆ’çš„é•¿æ€ç»´é“¾ï¼ˆLong Chain - of - Thoughtï¼ŒCoTï¼‰æ•°æ®é›†ã€è¾…åŠ©å¥–åŠ±æ¨¡å‹ä»¥åŠåŸºäºè¯æ®çš„è¯Šæ–­é“¾æ¨¡æ‹Ÿæ¥ä¼˜åŒ–è®­ç»ƒï¼Œè®©æ¨¡å‹èƒ½æ›´å¥½åœ°è¿›è¡ŒåŒ»ç–—æ¨ç†ç›¸å…³å­¦ä¹ ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ•°æ®å¤„ç†ä¸è¾…åŠ©æ¨¡å‹ç»“åˆ
æ„å»ºè¿‘200ä¸‡é—®é¢˜çš„å¤§è§„æ¨¡å¤šç±»åˆ«ï¼ˆæ•°å­¦ã€ç¼–ç¨‹ã€å¸¸è¯†ã€åŒ»ç–—ç­‰ï¼‰ä¸”å¤šè¯­è¨€ï¼ˆä¸­è‹±ï¼‰æ•°æ®é›†ï¼Œé€šè¿‡å¤šé˜¶æ®µæ•°æ®å¤„ç† pipelineï¼ˆé•¿æ€ç»´é“¾ç­”æ¡ˆç”Ÿæˆã€æ•°æ®è¿‡æ»¤åˆ†ç±»ã€æ•°æ®é‡‡æ ·ç­‰ï¼‰ç”Ÿæˆé€‚åˆSFTå’ŒRLè®­ç»ƒçš„æ•°æ®ã€‚åŒæ—¶å¼€å‘è®­ç»ƒäº†ä¸‰ä¸ªä¸“ä¸šè¾…åŠ©æ¨¡å‹ï¼šåŸºäºåå¥½çš„å¥–åŠ±æ¨¡å‹ã€åŸºäºéªŒè¯å™¨çš„å¥–åŠ±æ¨¡å‹å’Œæ€ç»´è¿½è¸ªæ¨¡å‹æ¥æ”¯æ’‘æ•°æ®å¤„ç†ä¸è®­ç»ƒã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šèšç„¦åŒ»ç–—åœºæ™¯æ·±åº¦é›†æˆä¸å®ç”¨åŒ–
WiNGPT - 3.0åŸºäºQwen - 2.5çš„320äº¿å‚æ•°æ¶æ„ï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œç°åœºéƒ¨ç½²å¯è¡Œæ€§é—´å–å¾—å¹³è¡¡ã€‚å®ƒç´§å¯†é›†æˆåˆ°WiNEXåŒ»ç–—ä¿¡æ¯ç³»ç»Ÿï¼Œé€šè¿‡åŒ»ç”Ÿå®¡æ ¸ã€ç»“æ„åŒ–çŸ¥è¯†åº“ã€å¯å®šåˆ¶è§„åˆ™æ¨¡æ¿ä¸‰é‡ä¿éšœè¾…åŠ©ä¸´åºŠåŒ»ç”Ÿï¼›è¿˜ä¼˜å…ˆè€ƒè™‘ä¸åŒ»é™¢å·¥ä½œæµç¨‹æ·±åº¦é›†æˆï¼Œå…·å¤‡é¢„é…ç½®ç”µå­ç—…å†æ¨¡æ¿ã€æ—¢å®šè§„åˆ™å’Œä»ç”¨æˆ·äº¤äº’ä¸­æŒç»­å­¦ä¹ ç­‰ç‰¹æ€§ï¼Œé€‚é…ç‰¹å®šæœºæ„æ ‡å‡†å’Œæ“ä½œæµç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
WiNGPT - 3.0å±•ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼šç‰¹å®šæ¨¡å‹å˜ä½“åœ¨MedCalcä¸Šå¾—åˆ†66.6ï¼Œåœ¨MedQA - USMLEä¸Šå¾—åˆ†87.1ï¼›é’ˆå¯¹ä¸´åºŠæ¨ç†ä»»åŠ¡çš„å®šå‘è®­ç»ƒï¼Œå°†åŸºçº¿åˆ†æ•°ä»58.1æå‡åˆ°62.5ã€‚è¿™äº›ç»“æœè¡¨æ˜å³ä½¿åœ¨ä»…å‡ åƒä¸ªæ ·æœ¬çš„æœ‰é™æ•°æ®é›†ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œä¹Ÿèƒ½æå‡åŒ»ç–—æ¨ç†å‡†ç¡®æ€§ï¼Œä¸ºä¸´åºŠå·¥ä½œæµå’ŒåŒ»ç–—ä¿¡æ¯åŸºç¡€è®¾æ–½ä¸­æ›´å¯ä¿¡ã€æ›´æ˜“éƒ¨ç½²çš„å¤§æ¨¡å‹é“ºè·¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨æ•°æ®åˆ©ç”¨ä¸Šï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ•°æ®è’¸é¦ã€æ€ç»´é“¾ç”Ÿæˆç­‰æŠ€æœ¯åœ¨èµ„æºçº¦æŸä¸‹é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡åŒ»ç–—æ•°æ®é›†ï¼Œä¸ºé¢†åŸŸç‰¹å®šæ•°æ®æ„å»ºæä¾›æ€è·¯ï¼›è®­ç»ƒç­–ç•¥æ–¹é¢ï¼ŒéªŒè¯äº†å¤šé˜¶æ®µè®­ç»ƒç»“åˆå¼ºåŒ–å­¦ä¹ åœ¨æå‡ç‰¹å®šé¢†åŸŸï¼ˆåŒ»ç–—ï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ï¼Œå°æ•°æ®é‡ä¸‹RLä¹Ÿèƒ½å‘æŒ¥ä½œç”¨ï¼Œä¸ºèµ„æºæœ‰é™åœºæ™¯ä¸‹æ¨¡å‹ä¼˜åŒ–æä¾›å‚è€ƒï¼›åœºæ™¯é›†æˆè§’åº¦ï¼Œå¼ºè°ƒæ¨¡å‹ä¸åŒ»ç–—ä¿¡æ¯ç³»ç»Ÿã€åŒ»é™¢å·¥ä½œæµç¨‹æ·±åº¦é›†æˆï¼Œé€šè¿‡å¤šç§æœºåˆ¶ä¿éšœä¸´åºŠé€‚ç”¨æ€§ã€å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ï¼Œä¸ºå¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸè½åœ°æä¾›äº†ä»æŠ€æœ¯åˆ°æµç¨‹çš„å®Œæ•´å€Ÿé‰´èŒƒä¾‹ï¼Œå°¤å…¶æ˜¯åœ¨åˆè§„æ€§ã€å®ç”¨æ€§å¹³è¡¡ä¸Šç»™å‡ºå®è·µæ–¹å‘ã€‚

## offline-guarded-safe-reinforcement-learning-for-medical-treatment-optimization-strategies
### Abstract
When applying offline reinforcement learning (RL) in healthcare scenarios,
the out-of-distribution (OOD) issues pose significant risks, as inappropriate
generalization beyond clinical expertise can result in potentially harmful
recommendations. While existing methods like conservative Q-learning (CQL)
attempt to address the OOD issue, their effectiveness is limited by only
constraining action selection by suppressing uncertain actions. This
action-only regularization imitates clinician actions that prioritize
short-term rewards, but it fails to regulate downstream state trajectories,
thereby limiting the discovery of improved long-term treatment strategies. To
safely improve policy beyond clinician recommendations while ensuring that
state-action trajectories remain in-distribution, we propose \textit{Offline
Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically
grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel
dual constraint mechanism for improving policy with reliability and safety.
First, the OOD guardian is established to specify clinically validated regions
for safe policy exploration. By constraining optimization within these regions,
it enables the reliable exploration of treatment strategies that outperform
clinician behavior by leveraging the full patient state history, without
drifting into unsupported state-action trajectories. Second, we introduce a
safety cost constraint that encodes medical knowledge about physiological
safety boundaries, providing domain-specific safeguards even in areas where
training data might contain potentially unsafe interventions. Notably, we
provide theoretical guarantees on safety and near-optimality: policies that
satisfy these constraints remain in safe and reliable regions and achieve
performance close to the best possible policy supported by the data.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒ»ç–—ä¼˜åŒ–æ–°çªç ´ï¼šOffline Guarded Safe RLä¿éšœå®‰å…¨ä¸æ•ˆæœ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŒ»ç–—ç­‰å®‰å…¨å…³é”®é¢†åŸŸåº”ç”¨æ—¶ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰é¢ä¸´åˆ†å¸ƒå¤–ï¼ˆOODï¼‰é—®é¢˜çš„é‡å¤§é£é™©ã€‚åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œè¶…å‡ºä¸´åºŠä¸“ä¸šçŸ¥è¯†çš„ä¸æ°å½“æ³›åŒ–å¯èƒ½ç»™å‡ºæœ‰å®³å»ºè®®ã€‚ç°æœ‰æ–¹æ³•å¦‚ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰ä»…é€šè¿‡æŠ‘åˆ¶ä¸ç¡®å®šåŠ¨ä½œçº¦æŸåŠ¨ä½œé€‰æ‹©ï¼Œåªèƒ½æ¨¡ä»¿ä¸´åºŠåŒ»ç”Ÿä¼˜å…ˆçŸ­æœŸå¥–åŠ±çš„è¡Œä¸ºï¼Œå´æ— æ³•è°ƒæ§ä¸‹æ¸¸çŠ¶æ€è½¨è¿¹ï¼Œé™åˆ¶äº†æ›´ä¼˜é•¿æœŸæ²»ç–—ç­–ç•¥çš„å‘ç°ã€‚åŒæ—¶ï¼ŒåŒ»ç–—ä¼˜åŒ–è¿˜å­˜åœ¨æ— æ³•ä¸»åŠ¨äº¤äº’æ¢ç´¢ã€éœ€æ•´åˆå®‰å…¨çº¦æŸä¸å¥–åŠ±å‡½æ•°ç­‰æŒ‘æˆ˜ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½åœ¨ä¿éšœçŠ¶æ€ - åŠ¨ä½œè½¨è¿¹åˆ†å¸ƒå†…çš„å‰æä¸‹ï¼Œå®‰å…¨æ”¹è¿›ç­–ç•¥çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºOOD guardianæœºåˆ¶
å»ºç«‹OOD guardianæ¥æ˜ç¡®ä¸´åºŠéªŒè¯çš„å®‰å…¨ç­–ç•¥æ¢ç´¢åŒºåŸŸã€‚ä¸ä»…æŠ‘åˆ¶OODåŠ¨ä½œçš„CQLç­‰æ–¹æ³•ä¸åŒï¼ŒOGSRLæ˜ç¡®çº¦æŸçŠ¶æ€å’ŒåŠ¨ä½œï¼Œå……åˆ†åˆ©ç”¨æ•°æ®é›†ä¸­åµŒå…¥çš„ä¸´åºŠåŒ»ç”ŸçŸ¥è¯†ã€‚é€šè¿‡å°†ä¼˜åŒ–çº¦æŸåœ¨è¿™äº›åŒºåŸŸå†…ï¼Œèƒ½å¤Ÿå€ŸåŠ©å®Œæ•´æ‚£è€…çŠ¶æ€å†å²ï¼Œå¯é åœ°æ¢ç´¢ä¼˜äºä¸´åºŠåŒ»ç”Ÿè¡Œä¸ºçš„æ²»ç–—ç­–ç•¥ï¼Œä¸”ä¸ä¼šé™·å…¥æ— æ”¯æŒçš„çŠ¶æ€ - åŠ¨ä½œè½¨è¿¹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥å®‰å…¨æˆæœ¬çº¦æŸä¸æä¾›ç†è®ºä¿éšœ
å¼•å…¥ç¼–ç ç”Ÿç†å®‰å…¨è¾¹ç•ŒåŒ»ç–—çŸ¥è¯†çš„å®‰å…¨æˆæœ¬çº¦æŸï¼Œå³ä¾¿åœ¨è®­ç»ƒæ•°æ®å¯èƒ½åŒ…å«æ½œåœ¨ä¸å®‰å…¨å¹²é¢„çš„åŒºåŸŸï¼Œä¹Ÿèƒ½æä¾›ç‰¹å®šé¢†åŸŸçš„ä¿æŠ¤ã€‚åŒæ—¶ä»ç†è®ºä¸Šä¿è¯äº†å®‰å…¨æ€§å’Œè¿‘ä¼¼æœ€ä¼˜æ€§ï¼šæ»¡è¶³çº¦æŸçš„ç­–ç•¥èƒ½ç•™åœ¨å®‰å…¨å¯é åŒºåŸŸï¼Œä¸”æ€§èƒ½æ¥è¿‘æ•°æ®æ”¯æŒçš„æœ€ä¼˜ç­–ç•¥ã€‚è¿˜ç»“åˆåŸºäºæ¨¡å‹çš„RLï¼Œæä¾›äº†å®‰å…¨æ€§å’Œè¿‘ä¼¼æœ€ä¼˜æ€§çš„æ¦‚ç‡ä¿è¯ï¼Œå¹¶é‡åŒ–äº†æ•°æ®é›†å¤§å°å¯¹ç­–ç•¥å¯é æ€§çš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MIMIC - IIIè„“æ¯’ç—‡æ²»ç–—æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒOGSRLåœ¨åˆ†å¸ƒå¤–å¤„ç†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ä¸ä¸´åºŠåŒ»ç”Ÿå†³ç­–ç›¸æ¯”ï¼ŒOGSRLä½¿æ­»äº¡ç‡ä¼°è®¡é™ä½äº†78%ï¼Œå¥–åŠ±å¢åŠ äº†51%ï¼Œåœ¨ç´¯ç§¯å¥–åŠ±ã€å®‰å…¨çº¦æŸæ»¡è¶³åº¦å’Œä¸ä¸´åºŠè¡Œä¸ºçš„ä¸€è‡´æ€§æ–¹é¢æŒç»­è¶…è¶Šå¼ºå¤§çš„ç¦»çº¿RLåŸºçº¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. å¯¹äºå®‰å…¨å…³é”®é¢†åŸŸçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ åº”ç”¨ï¼Œæä¾›äº†ä¸€ç§ç»“åˆé¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚åŒ»ç–—é¢†åŸŸçš„ä¸´åºŠçŸ¥è¯†ï¼‰æ¥çº¦æŸç­–ç•¥æ¢ç´¢å’Œä¿éšœå®‰å…¨çš„æ€è·¯ï¼Œå¯æ¨å¹¿åˆ°æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰å…¶ä»–å®‰å…¨å…³é”®é¢†åŸŸã€‚
2. ç†è®ºä¿éšœæ–¹é¢çš„å·¥ä½œä¸ºåç»­ç›¸å…³ç ”ç©¶æä¾›äº†å¦‚ä½•ä»ç†è®ºå±‚é¢è¯æ˜æ–¹æ³•å®‰å…¨æ€§ä¸æœ€ä¼˜æ€§çš„èŒƒä¾‹ï¼Œæœ‰åŠ©äºæ¨åŠ¨é¢†åŸŸå†…æ–¹æ³•çš„ç†è®ºä¸¥è°¨æ€§å‘å±•ã€‚
3. æ¨¡å‹ä¸­å¯¹æ•°æ®é›†ä¸­ä¸“å®¶çŸ¥è¯†çš„å……åˆ†åˆ©ç”¨æ–¹å¼ï¼Œä¸ºå…¶ä»–éœ€è¦ç»“åˆä¸“ä¸šé¢†åŸŸå…ˆéªŒçŸ¥è¯†çš„ç¦»çº¿RLä»»åŠ¡æä¾›äº†å‚è€ƒï¼ŒæŒ‡å¯¼å¦‚ä½•ä»æ•°æ®ä¸­æŒ–æ˜å’Œåº”ç”¨è¿™ç±»çŸ¥è¯†æ¥æ”¹è¿›ç­–ç•¥ã€‚

## s3--you-don-t-need-that-much-data-to-train-a-search-agent-via-rl
### Abstract
Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | s3ï¼šå°æ•°æ®é‡ä¸‹ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœç´¢æ™ºèƒ½ä½“çš„æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å€ŸåŠ©å¤–éƒ¨çŸ¥è¯†æå‡æ¨ç†èƒ½åŠ›ï¼Œè€Œè¿‘æœŸç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®©LLMæ‰®æ¼”æœç´¢æ™ºèƒ½ä½“çš„è¿›å±•ï¼Œå¯é€šè¿‡å¤šè½®äº¤äº’ä¼˜åŒ–ä¿¡æ¯è·å–ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šè¦ä¹ˆç”¨ä»…å…³æ³¨æœç´¢çš„æŒ‡æ ‡ï¼ˆå¦‚NDCGï¼‰ä¼˜åŒ–æ£€ç´¢ï¼Œå¿½ç•¥ä¸‹æ¸¸æ•ˆç”¨ï¼›è¦ä¹ˆå¾®è°ƒæ•´ä¸ªLLMæ¥è”åˆæ¨ç†ä¸æ£€ç´¢ï¼ŒæŠŠæ£€ç´¢å’Œç”Ÿæˆè€¦åˆï¼Œé™åˆ¶äº†å®é™…æœç´¢æ•ˆç”¨ä¸”éš¾é€‚é…å†»ç»“æˆ–é—­æºæ¨¡å‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§è§£è€¦æœç´¢ä¸ç”Ÿæˆã€è½»é‡ä¸”æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œåœ¨å°æ•°æ®é‡ä¸‹æå‡ä¸‹æ¸¸æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºs3æ¡†æ¶ï¼Œè§£è€¦æœç´¢å™¨ä¸ç”Ÿæˆå™¨  
s3æ˜¯è½»é‡ã€æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œå°†æœç´¢å™¨ï¼ˆSearcherï¼‰ä»ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ä¸­è§£è€¦ã€‚è®­ç»ƒæ—¶å›ºå®šç”Ÿæˆå™¨ï¼ˆå¯å¤ç”¨ä»»æ„å†»ç»“çš„LLMï¼‰ï¼Œä»…ä¸“æ³¨äºè®­ç»ƒæœç´¢å™¨ï¼Œè®©æœç´¢ä¼˜åŒ–èšç„¦åœ¨å¯¹ä¸‹æ¸¸ç”Ÿæˆè´¨é‡çš„æå‡ä¸Šï¼Œé¿å…äº†ç”Ÿæˆä¸æ£€ç´¢çš„è€¦åˆé—®é¢˜ï¼Œä¹Ÿèƒ½é€‚é…é—­æºæˆ–å†»ç»“çš„LLMã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®šä¹‰â€œGain Beyond RAGï¼ˆGBRï¼‰â€å¥–åŠ±ä¿¡å·  
GBRä½œä¸ºå…¨æ–°çš„å¥–åŠ±æŒ‡æ ‡ï¼Œè¡¡é‡äº†s3æ£€ç´¢åˆ°çš„æ–‡æ¡£è®©ç”Ÿæˆå™¨è¡¨ç°ï¼Œç›¸æ¯”â€œæœ´ç´ RAGï¼ˆnaÃ¯ve RAGï¼‰æ£€ç´¢â€åœ¨ç”Ÿæˆå‡†ç¡®ç‡ä¸Šçš„æå‡å¹…åº¦ã€‚é€šè¿‡è¯¥å¥–åŠ±è®­ç»ƒæœç´¢å™¨ï¼Œèƒ½ç›´æ¥é’ˆå¯¹ä¸‹æ¸¸ç”Ÿæˆæ•ˆç”¨ä¼˜åŒ–æ£€ç´¢ç»„ä»¶ï¼Œæ‘†è„±ä»…å…³æ³¨æœç´¢æŒ‡æ ‡æˆ–æ˜“è¿‡æ‹Ÿåˆçš„ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ç±»æŒ‡æ ‡çš„å±€é™ï¼Œè®©æ£€ç´¢ä¼˜åŒ–æ›´è´´åˆå®é™…ç”Ÿæˆéœ€æ±‚ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨6ä¸ªé€šç”¨QAåŸºå‡†å’Œ5ä¸ªåŒ»ç–—QAåŸºå‡†æµ‹è¯•ä¸­ï¼Œs3å±•ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼šä»…ç”¨2.4kè®­ç»ƒæ ·æœ¬ï¼Œå°±è¶…è¶Šäº†ä½¿ç”¨è¶…70å€æ•°æ®ï¼ˆå¦‚70kç”šè‡³æ›´å¤šï¼‰è®­ç»ƒçš„åŸºçº¿æ–¹æ³•ï¼ˆåƒDeepRetrievalã€Search - R1ç­‰ï¼‰ã€‚åœ¨é€šç”¨å’ŒåŒ»ç–—é¢†åŸŸçš„å¹³å‡å¾—åˆ†ä¸Šï¼Œs3å¯¹æ¯”å…¶ä»–ç»å…¸RAGã€Active RAGã€RL - Zeroé˜¶æ®µçš„æ–¹æ³•ï¼Œéƒ½å®ç°äº†æ›´ä¼˜çš„ä¸‹æ¸¸ç”Ÿæˆè¡¨ç°ï¼ŒéªŒè¯äº†å°æ•°æ®é‡ä¸‹é«˜æ•ˆè®­ç»ƒæœç´¢æ™ºèƒ½ä½“çš„èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ¨¡å—åŒ–è®¾è®¡æ€è·¯ï¼šå°†å¤æ‚çš„â€œæ£€ç´¢ + ç”Ÿæˆâ€ä»»åŠ¡è§£è€¦ä¸ºæœç´¢å™¨å’Œç”Ÿæˆå™¨ç‹¬ç«‹ä¼˜åŒ–ï¼Œä¸ºæ„å»ºæ›´çµæ´»çš„RAGç³»ç»Ÿæä¾›äº†æ¨¡å—åŒ–æ€è·¯ï¼Œåç»­å¯é’ˆå¯¹ä¸åŒæ¨¡å—åˆ†åˆ«è¿­ä»£å‡çº§ã€‚  
2. å¥–åŠ±ä¿¡å·è®¾è®¡ï¼šGBRå¥–åŠ±å°†ä¸‹æ¸¸ç”Ÿæˆæ•ˆç”¨ä½œä¸ºæ£€ç´¢ä¼˜åŒ–çš„æ ¸å¿ƒä¾æ®ï¼Œè·³å‡ºäº†ä¼ ç»Ÿä»…çœ‹æœç´¢ç¯èŠ‚æŒ‡æ ‡çš„æ€ç»´å®šå¼ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨æ£€ç´¢å¢å¼ºåœºæ™¯ä¸‹çš„å¥–åŠ±å‡½æ•°è®¾è®¡æä¾›äº†æ–°èŒƒå¼ï¼Œå¯å¯å‘å…¶ä»–éœ€è¦è·¨ç¯èŠ‚ä¼˜åŒ–ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡ã€‚  
3. å°æ•°æ®é«˜æ•ˆè®­ç»ƒï¼šè¯æ˜äº†åœ¨å°‘é‡ä¼˜è´¨æ ·æœ¬ä¸‹ä¹Ÿèƒ½è®­ç»ƒå‡ºé«˜æ€§èƒ½æœç´¢æ™ºèƒ½ä½“ï¼Œé™ä½äº†å¤§è§„æ¨¡æ•°æ®æ ‡æ³¨ä¸è®­ç»ƒæˆæœ¬ï¼Œå¯¹èµ„æºæœ‰é™ä½†éœ€æ„å»ºRAGç³»ç»Ÿçš„åœºæ™¯ï¼ˆå¦‚å‚ç›´é¢†åŸŸå°å›¢é˜Ÿï¼‰æœ‰å¾ˆå¼ºçš„å€Ÿé‰´æ„ä¹‰ã€‚

## toward-effective-reinforcement-learning-fine-tuning-for-medical-vqa-in-vision-language-models
### Abstract
Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.
```
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒ»ç–—VQAåœºæ™¯ä¸‹åŸºäºå¼ºåŒ–å­¦ä¹ å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è°ƒä¼˜æ¨åŠ¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼Œå°¤å…¶æ˜¯Group Relative Policy Optimizationï¼ˆGRPOï¼‰æ–¹æ³•å‡ºç°åã€‚ä½†å°†å…¶ç›´æ¥åº”ç”¨äºåŒ»ç–—ä»»åŠ¡ä»¥å®ç°ç¬¦åˆä¸´åºŠè¦æ±‚çš„æ¨¡å‹è¡Œä¸ºä»å…·æŒ‘æˆ˜ã€‚ä¸ºä½¿æ¨¡å‹å“åº”ä¸ä¸´åºŠé¢„æœŸå¯¹é½ï¼Œæœ¬æ–‡é’ˆå¯¹åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­å½±å“RLè°ƒä¼˜æœ‰æ•ˆæ€§çš„å››ä¸ªå…³é”®ç»´åº¦å±•å¼€ç ”ç©¶ï¼šåŸºç¡€æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥ã€åŒ»ç–—è¯­ä¹‰å¯¹é½çš„ä½œç”¨ã€åŸºäºé•¿åº¦çš„å¥–åŠ±å¯¹é•¿é“¾æ¨ç†çš„å½±å“ä»¥åŠåå·®çš„ä½œç”¨ï¼ŒåŒæ—¶éªŒè¯GRPO - åŸºäºRLçš„è°ƒä¼˜ç›¸è¾ƒæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ä¼˜åŠ¿ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç»´åº¦åˆ†æGRPO - åŸºäºRLåœ¨åŒ»ç–—MLLMsçš„è¡¨ç° 
ä»äº”ä¸ªå…³é”®æ–¹é¢åˆ†æï¼šå¯¹æ¯”ä»å¤´è®­ç»ƒä¸å¾®è°ƒç­–ç•¥ï¼Œæ¢ç©¶è®­ç»ƒåˆå§‹æ–¹å¼å¯¹æ¨¡å‹åœ¨åŒ»ç–—çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸Šçš„æƒè¡¡ï¼›å¼•å…¥åŒ»ç–—è¯­ä¹‰å¥–åŠ±ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è¯„ä¼°ç»“åˆæç¤ºå·¥ç¨‹ï¼Œè§£å†³é€šç”¨å¥–åŠ±å¯¹ä¸´åºŠä»»åŠ¡ä¸è¶³çš„é—®é¢˜ï¼›ç ”ç©¶ä»…ä¾èµ–åŸºäºé•¿åº¦çš„å¥–åŠ±å¯¹åŒ»ç–—VQAé•¿é“¾æ¨ç†çš„å½±å“ï¼Œå‘ç°å…¶æ˜“å¯¼è‡´å›ç­”å†—é•¿ä¸”ä¸å‡†ç¡®ï¼›è¯„ä¼°é—®é¢˜çº§å½’ä¸€åŒ–åœ¨åŒ»ç–—VQAä¸­å¯¹æ¨¡å‹åå·®çš„å½±å“ï¼Œé€šè¿‡å®ç°Dr.GRPOéªŒè¯å…¶åœ¨æå‡å›ç­”å‡†ç¡®æ€§å’Œtokenæ•ˆç‡ä¸Šçš„ä½œç”¨ï¼›å¯¹æ¯”SFTä¸GRPO - åŸºäºRLè°ƒä¼˜ï¼Œå±•ç°GRPO - RLçš„ä¼˜åŠ¿ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç³»ç»Ÿåˆ†æä¸å®éªŒéªŒè¯ç»“åˆ 
å¯¹GRPO - åŸºäºRLåœ¨åŒ»ç–—MLLMsä¸­å›´ç»•åˆå§‹åŒ–ç­–ç•¥ã€åŒ»ç–—è¯­ä¹‰å¯¹é½ã€åŸºäºé•¿åº¦å¥–åŠ±å½±å“å’Œåå·®ç›¸å…³è¡Œä¸ºå±•å¼€ç³»ç»Ÿåˆ†æï¼›åœ¨åŒ»ç–—VQAåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œå¤§è§„æ¨¡å®éªŒéªŒè¯å‘ç°ï¼Œä¸ºRLä¸ä¸´åºŠæœ‰æ„ä¹‰è¡Œä¸ºå¯¹é½æä¾›å®ç”¨è§è§£ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€‰æ‹©Qwen2 - VL - 2Bä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨PMC - VQAå­é›†ï¼ˆ10Kè®­ç»ƒæ ·æœ¬ã€7Kæµ‹è¯•æ ·æœ¬ï¼‰ä¸Šå®éªŒã€‚ä»å¤´è®­ç»ƒä¸å¾®è°ƒå¯¹æ¯”ä¸­ï¼Œä»å¤´è®­ç»ƒæ¨¡å‹æ¨ç†æ›´å¯¹é½æœ‰ç”¨ä½†å›ç­”æ­£ç¡®æ€§å’Œæµç•…æ€§æ¬ ä½³ï¼Œå¾®è°ƒæ¨¡å‹å›ç­”æ›´å‡†ç¡®æµç•…ï¼›å¼•å…¥åŒ»ç–—è¯­ä¹‰å¯¹é½å¥–åŠ±åï¼Œæ¨¡å‹æ€§èƒ½å’Œæ¨ç†è´¨é‡æå‡ï¼Œå‡†ç¡®ç‡æé«˜1.82%ï¼›ä»…åŸºäºé•¿åº¦çš„å¥–åŠ±æ˜“äº§ç”Ÿå†—é•¿ä¸å‡†ç¡®å›ç­”ï¼›Dr.GRPOèƒ½æå‡å›ç­”å‡†ç¡®æ€§å’Œtokenæ•ˆç‡ï¼›GRPO - åŸºäºRLè°ƒä¼˜åœ¨å‡†ç¡®ç‡å’Œæ¨ç†è´¨é‡ä¸ŠæŒç»­ä¼˜äºSFTæ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
åœ¨é¢†åŸŸç‰¹å®šæ¨¡å‹è°ƒä¼˜æ–¹é¢ï¼Œæœ¬æ–‡å¯¹åŸºç¡€æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥çš„ç ”ç©¶ä¸ºåŒ»ç–—ç­‰å‚ç›´é¢†åŸŸæ¨¡å‹åˆå§‹åŒ–æä¾›å‚è€ƒï¼Œå¸®åŠ©å¹³è¡¡å›ç­”è´¨é‡ä¸æ¨ç†èƒ½åŠ›ï¼›å¥–åŠ±è®¾è®¡ä¸Šï¼ŒåŒ»ç–—è¯­ä¹‰å¥–åŠ±çš„å¼•å…¥æ€è·¯å¯æ¨å¹¿åˆ°å…¶ä»–å‚ç›´é¢†åŸŸï¼Œåˆ©ç”¨é¢†åŸŸå†…å¤§æ¨¡å‹è¯„ä¼°æ¥è®¾è®¡é’ˆå¯¹æ€§å¥–åŠ±ï¼›æ–¹æ³•å¯¹æ¯”ä¸Šï¼ŒéªŒè¯äº†GRPO - åŸºäºRLè°ƒä¼˜åœ¨åŒ»ç–—VQAä»»åŠ¡ä¸­ç›¸è¾ƒSFTçš„ä¼˜åŠ¿ï¼Œä¸ºåç»­åŒ»ç–—MLLMsè°ƒä¼˜æ–¹æ³•é€‰æ‹©æä¾›ä¾æ®ï¼›åå·®åˆ†ææ–¹é¢ï¼Œå¯¹é—®é¢˜çº§å½’ä¸€åŒ–åå·®çš„ç ”ç©¶æé†’åœ¨æ¨¡å‹è®­ç»ƒä¸­å…³æ³¨æ­¤ç±»æ½œåœ¨å½±å“å› ç´ ï¼ŒDr.GRPOçš„å®è·µä¹Ÿæä¾›äº†åº”å¯¹æ€è·¯ã€‚
```

