import streamlit as st
import os
import time
import tempfile
import uuid
import arxiv
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from survey_agent.arxiv_tools.download import process_paper
from survey_agent.survey.generator import generate_survey_from_bib
from survey_agent.utils.bib_parser import BibParser
from survey_agent.llm.summarize import get_summarizer
from survey_agent.survey.generator import generate_markdown

# è®¾ç½®ç¯å¢ƒå˜é‡
from survey_agent.env import *



st.set_page_config(page_title="AI è®ºæ–‡ç»¼è¿°ç”Ÿæˆå™¨ (BIBæ–‡ä»¶ç‰ˆ)", layout="wide")
st.title("ğŸ“š AI è®ºæ–‡ç»¼è¿°ç”Ÿæˆå™¨ (BIBæ–‡ä»¶ç‰ˆ) ğŸ”¬")

st.markdown("""
<style>
.big-font { font-size:22px !important; }
.prompt-preview { 
    background-color: #f0f2f6; 
    padding: 10px; 
    border-radius: 5px; 
    border-left: 5px solid #4CAF50; 
    margin: 10px 0;
}
</style>
""", unsafe_allow_html=True)

def fetch_arxiv_paper_parallel(arxiv_ids, progress_placeholder, paper_list_placeholder, max_workers=4):
    """å¹¶è¡Œä»arXivè·å–è®ºæ–‡å¯¹è±¡"""
    papers = [None] * len(arxiv_ids)  # Pre-allocate list to maintain order
    
    def fetch_single_paper(idx_id):
        idx, arxiv_id = idx_id
        try:
            client = arxiv.Client()
            search = arxiv.Search(id_list=[arxiv_id])
            results = list(client.results(search))
            if results:
                return idx, results[0]
            else:
                return idx, None
        except Exception as e:
            print(f"Error fetching paper {arxiv_id}: {e}")
            return idx, None
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Create list of (idx, arxiv_id) tuples for processing
        futures = [executor.submit(fetch_single_paper, (i, arxiv_id)) 
                  for i, arxiv_id in enumerate(arxiv_ids)]
        
        for future in as_completed(futures):
            idx, result = future.result()
            papers[idx] = result
            
            # Update progress display
            completed_count = sum(1 for p in papers if p is not None)
            progress_placeholder.info(f"ğŸ” æ­£åœ¨ä»arXivè·å–è®ºæ–‡ä¿¡æ¯ {completed_count}/{len(arxiv_ids)}")
            
            # Update paper list display
            paper_list_placeholder.markdown(
                "\n".join([
                    f"- {'ğŸ“„ ' if papers[i] is not None else 'ğŸ” '}{arxiv_ids[i]}" 
                    for i in range(len(arxiv_ids))
                ])
            )
    
    # Filter out None results
    return [p for p in papers if p is not None]

def search_papers_by_title_parallel(entries_without_arxiv, progress_placeholder, paper_list_placeholder, max_workers=4):
    """å¹¶è¡Œé€šè¿‡æ ‡é¢˜æœç´¢arXivè®ºæ–‡"""
    found_papers = [None] * len(entries_without_arxiv)  # Pre-allocate list to maintain order
    
    def search_single_paper(idx_entry):
        idx, entry = idx_entry
        try:
            parser = BibParser()
            # å¯ç”¨è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼Œåœ¨ç»ˆç«¯æ˜¾ç¤ºæœç´¢è¿‡ç¨‹
            print(f"\nğŸ“‹ [{idx+1}/{len(entries_without_arxiv)}] å¼€å§‹æœç´¢è®ºæ–‡...")
            result = parser.search_paper_by_title(entry['cleaned_title'], verbose=True)
            return idx, result, entry
        except Exception as e:
            print(f"âŒ Error searching paper with title '{entry['cleaned_title']}': {e}\n")
            return idx, None, entry
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Create list of (idx, entry) tuples for processing
        futures = [executor.submit(search_single_paper, (i, entry)) 
                  for i, entry in enumerate(entries_without_arxiv)]
        
        for future in as_completed(futures):
            idx, result, entry = future.result()
            found_papers[idx] = result
            
            # Update progress display
            completed_count = sum(1 for p in found_papers if p is not None)
            progress_placeholder.info(f"ğŸ” æ­£åœ¨é€šè¿‡æ ‡é¢˜æœç´¢è®ºæ–‡ {completed_count}/{len(entries_without_arxiv)}")
            
            # Update paper list display
            paper_list_placeholder.markdown(
                "\n".join([
                    f"- {'ğŸ“„ ' if found_papers[i] is not None else 'âŒ '}{entries_without_arxiv[i]['cleaned_title'][:60]}..." 
                    for i in range(len(entries_without_arxiv))
                ])
            )
    
    # Filter out None results
    return [p for p in found_papers if p is not None]

def process_papers_parallel(papers, progress_placeholder, paper_list_placeholder, pdf_dir=None, max_workers=4):
    """å¹¶è¡Œå¤„ç†è®ºæ–‡ï¼šä¸‹è½½PDFå¹¶æå–æ–‡æœ¬"""
    processed_papers = [None] * len(papers)  # Pre-allocate list to maintain order
    
    def process_with_progress(idx_paper):
        idx, paper = idx_paper
        result = process_paper(paper, pdf_dir)
        return idx, result
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Create list of (idx, paper) tuples for processing
        futures = [executor.submit(process_with_progress, (i, paper)) 
                  for i, paper in enumerate(papers)]
        
        for future in as_completed(futures):
            idx, result = future.result()
            processed_papers[idx] = result
            
            # Update progress display
            progress_placeholder.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ä¸å¤„ç†è®ºæ–‡ {sum(1 for p in processed_papers if p is not None)}/{len(papers)}")
            paper_list_placeholder.markdown(
                "\n".join([
                    f"- {'ğŸ“¥ ' if processed_papers[i] is not None else 'ğŸ” '}{papers[i].title}" 
                    for i in range(len(papers))
                ])
            )
    
    return [p for p in processed_papers if p is not None]

def summarize_papers_parallel(processed_papers, summarizer, progress_placeholder, paper_list_placeholder, max_workers=4):
    """å¹¶è¡Œç”Ÿæˆè®ºæ–‡æ€»ç»“"""
    summarized_papers = [None] * len(processed_papers)  # Pre-allocate list to maintain order
    
    def summarize_with_progress(idx_paper):
        idx, paper = idx_paper
        paper['llm_summary_res'] = summarizer.summarize(paper)
        return idx, paper
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(summarize_with_progress, (i, paper)) 
                  for i, paper in enumerate(processed_papers)]
        
        for future in as_completed(futures):
            idx, result = future.result()
            summarized_papers[idx] = result
            
            # Update progress display
            progress_placeholder.info(f"âœ¨ LLM æ€»ç»“ä¸­ {sum(1 for p in summarized_papers if p is not None)}/{len(processed_papers)}")
            paper_list_placeholder.markdown(
                "\n".join([
                    f"- {'âœ¨ ' if summarized_papers[i] is not None else 'ğŸ“¥ '}{processed_papers[i]['title']}" 
                    for i in range(len(processed_papers))
                ])
            )
    
    return [p for p in summarized_papers if p is not None]

def generate_survey_from_bib_parallel(bib_file: str,
                                    output_file: str = "survey_from_bib.md",
                                    llm_provider: str = "openai",
                                    model_name: str = None,
                                    custom_prompt: str = None,
                                    pdf_dir: str = None,
                                    progress_placeholder=None,
                                    paper_list_placeholder=None,
                                    max_workers: int = 4) -> str:
    """
    å¹¶è¡Œç‰ˆæœ¬çš„ä»BIBæ–‡ä»¶ç”Ÿæˆç»¼è¿°å‡½æ•°ï¼Œæ”¯æŒé€šè¿‡æ ‡é¢˜æœç´¢æ²¡æœ‰arXiv IDçš„æ¡ç›®
    """
    # Step 1: Parse BIB file and extract arXiv IDs and entries without arXiv IDs
    if progress_placeholder:
        progress_placeholder.info(f"ğŸ“š æ­£åœ¨è§£æ BIB æ–‡ä»¶...")
    
    parser = BibParser()
    content = Path(bib_file).read_text(encoding='utf-8')
    parse_result = parser.parse_string(content)
    
    arxiv_ids = parse_result['arxiv_ids']
    entries_without_arxiv = parse_result['entries_without_arxiv']
    
    if not arxiv_ids and not entries_without_arxiv:
        if progress_placeholder:
            progress_placeholder.error("âŒ åœ¨BIBæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„è®ºæ–‡")
        return None
    
    if progress_placeholder:
        progress_placeholder.info(f"âœ… æ‰¾åˆ° {len(arxiv_ids)} ä¸ªarXiv IDï¼Œ{len(entries_without_arxiv)} ä¸ªéœ€è¦é€šè¿‡æ ‡é¢˜æœç´¢çš„æ¡ç›®")
    
    all_papers = []
    
    # Step 2: å¹¶è¡Œä»arXivè·å–æœ‰IDçš„è®ºæ–‡å¯¹è±¡
    if arxiv_ids:
        if progress_placeholder:
            progress_placeholder.info("ğŸ” æ­£åœ¨ä»arXivè·å–æœ‰IDçš„è®ºæ–‡è¯¦ç»†ä¿¡æ¯...")
        
        papers_from_ids = fetch_arxiv_paper_parallel(
            arxiv_ids, progress_placeholder, paper_list_placeholder, max_workers
        )
        all_papers.extend(papers_from_ids)
        
        if progress_placeholder:
            progress_placeholder.info(f"âœ… æˆåŠŸè·å– {len(papers_from_ids)} ç¯‡æœ‰IDçš„è®ºæ–‡ä¿¡æ¯")
    
    # Step 3: å¹¶è¡Œé€šè¿‡æ ‡é¢˜æœç´¢æ²¡æœ‰arXiv IDçš„è®ºæ–‡
    if entries_without_arxiv:
        if progress_placeholder:
            progress_placeholder.info("ğŸ” æ­£åœ¨é€šè¿‡æ ‡é¢˜æœç´¢è®ºæ–‡...")
        
        papers_from_titles = search_papers_by_title_parallel(
            entries_without_arxiv, progress_placeholder, paper_list_placeholder, max_workers
        )
        all_papers.extend(papers_from_titles)
        
        if progress_placeholder:
            progress_placeholder.info(f"âœ… é€šè¿‡æ ‡é¢˜æœç´¢æˆåŠŸæ‰¾åˆ° {len(papers_from_titles)} ç¯‡è®ºæ–‡")
    
    if not all_papers:
        if progress_placeholder:
            progress_placeholder.error("âŒ æ²¡æœ‰æˆåŠŸè·å–åˆ°ä»»ä½•è®ºæ–‡ä¿¡æ¯")
        return None
    
    if progress_placeholder:
        progress_placeholder.info(f"âœ… æ€»å…±è·å– {len(all_papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
    
    # Step 4: å¹¶è¡Œå¤„ç†è®ºæ–‡ (ä¸‹è½½PDFså’Œæå–æ–‡æœ¬)
    if progress_placeholder:
        progress_placeholder.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½PDFså’Œæå–æ–‡æœ¬...")
    
    processed_papers = process_papers_parallel(
        all_papers, progress_placeholder, paper_list_placeholder, pdf_dir, max_workers
    )
    
    if not processed_papers:
        if progress_placeholder:
            progress_placeholder.error("âŒ æ²¡æœ‰è®ºæ–‡è¢«æˆåŠŸå¤„ç†")
        return None
    
    # Step 5: å¹¶è¡Œç”Ÿæˆæ€»ç»“
    if progress_placeholder:
        progress_placeholder.info("ğŸ¤– æ­£åœ¨ä½¿ç”¨LLMç”Ÿæˆæ€»ç»“...")
    
    summarizer = get_summarizer(llm_provider, model_name, custom_prompt)
    summarized_papers = summarize_papers_parallel(
        processed_papers, summarizer, progress_placeholder, paper_list_placeholder, max_workers
    )
    
    
    markdown_content = generate_markdown(summarized_papers, output_file, bib_file=bib_file)
    
    if progress_placeholder:
        progress_placeholder.success("ğŸ‰ ç»¼è¿°ç”Ÿæˆå®Œæˆï¼")
    
    return output_file

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'bib_content' not in st.session_state:
    st.session_state.bib_content = None
if 'arxiv_papers' not in st.session_state:
    st.session_state.arxiv_papers = []
if 'title_papers' not in st.session_state:
    st.session_state.title_papers = []
if 'all_papers' not in st.session_state:
    st.session_state.all_papers = []

# ä¾§è¾¹æ ï¼šé…ç½®é€‰é¡¹
st.sidebar.header("âš™ï¸ é…ç½®é€‰é¡¹")

# LLMè®¾ç½®
llm_provider = "openai"
model_name = st.sidebar.selectbox("ğŸ¤– LLM æ¨¡å‹", ["ep-20250529110941-khvtx", "doubao"])

# å¹¶è¡Œå¤„ç†è®¾ç½®
max_workers = st.sidebar.slider("ğŸ”§ å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°", min_value=1, max_value=8, value=4, 
                               help="å¢åŠ çº¿ç¨‹æ•°å¯ä»¥åŠ å¿«å¤„ç†é€Ÿåº¦ï¼Œä½†ä¼šæ¶ˆè€—æ›´å¤šç³»ç»Ÿèµ„æº")

# ä¸»ç•Œé¢
col1, col2 = st.columns([1, 1])

with col1:
    st.header("ğŸ“ BIB æ–‡ä»¶ä¸Šä¼ ")
    uploaded_bib = st.file_uploader(
        "ä¸Šä¼ æ‚¨çš„ BibTeX æ–‡ä»¶",
        type=['bib', 'txt'],
        help="æ”¯æŒ .bib å’Œ .txt æ ¼å¼çš„ BibTeX æ–‡ä»¶"
    )
    
    if uploaded_bib is not None:
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹å¹¶å¤„ç†ç¼–ç 
            file_content = uploaded_bib.read()
            if isinstance(file_content, bytes):
                # å°è¯•ä¸åŒçš„ç¼–ç 
                encodings = ['utf-8', 'gbk', 'latin-1', 'cp1252']
                bib_content = None
                for encoding in encodings:
                    try:
                        bib_content = file_content.decode(encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                
                if bib_content is None:
                    st.error("âŒ æ— æ³•è§£ç æ–‡ä»¶å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç ")
                    st.stop()
            else:
                bib_content = file_content
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.bib_content = bib_content
            
            # é¢„è§ˆBIBæ–‡ä»¶å†…å®¹
            st.subheader("ğŸ“‹ BIB æ–‡ä»¶é¢„è§ˆ")
            preview_content = bib_content[:1000] + "..." if len(bib_content) > 1000 else bib_content
            st.text_area("æ–‡ä»¶å†…å®¹", preview_content, height=200, disabled=True)
            
            # è§£æBIBæ–‡ä»¶ï¼Œæå–arxiv IDså’Œéœ€è¦æ ‡é¢˜æœç´¢çš„æ¡ç›®
            try:
                parser = BibParser()
                parse_result = parser.parse_string(bib_content)
                
                arxiv_ids = parse_result['arxiv_ids']
                entries_without_arxiv = parse_result['entries_without_arxiv']
                
                # æ„é€ æœ‰arXiv IDçš„è®ºæ–‡å­—å…¸åˆ—è¡¨
                arxiv_papers = [
                    {
                        'arxiv_id': arxiv_id,
                        'url': f"https://arxiv.org/abs/{arxiv_id}",
                        'pdf_url': f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    }
                    for arxiv_id in arxiv_ids
                ]
                
                # æ„é€ éœ€è¦æ ‡é¢˜æœç´¢çš„æ¡ç›®åˆ—è¡¨  
                title_papers = [
                    {
                        'title': entry['cleaned_title'],
                        'original_title': entry['original_title'],
                        'authors': entry['authors'],
                        'year': entry['year'],
                        'key': entry['key']
                    }
                    for entry in entries_without_arxiv
                ]
                
                # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.arxiv_papers = arxiv_papers
                st.session_state.title_papers = title_papers
                st.session_state.all_papers = arxiv_papers + title_papers
                
                if arxiv_ids or entries_without_arxiv:
                    st.success(f"âœ… æˆåŠŸè§£æBIBæ–‡ä»¶ï¼")
                    st.info(f"ğŸ“Š æ‰¾åˆ° {len(arxiv_ids)} ä¸ªarXiv IDï¼Œ{len(entries_without_arxiv)} ä¸ªéœ€è¦é€šè¿‡æ ‡é¢˜æœç´¢çš„æ¡ç›®")
                    
                    # æ˜¾ç¤ºæœ‰arXiv IDçš„è®ºæ–‡åˆ—è¡¨
                    if arxiv_ids:
                        with st.expander("ğŸ“„ æŸ¥çœ‹æœ‰arXiv IDçš„è®ºæ–‡"):
                            for i, arxiv_id in enumerate(arxiv_ids):
                                st.write(f"{i+1}. **{arxiv_id}**")
                                st.write(f"   - URL: https://arxiv.org/abs/{arxiv_id}")
                    
                    # æ˜¾ç¤ºéœ€è¦æ ‡é¢˜æœç´¢çš„æ¡ç›®åˆ—è¡¨
                    if entries_without_arxiv:
                        with st.expander("ğŸ” æŸ¥çœ‹éœ€è¦é€šè¿‡æ ‡é¢˜æœç´¢çš„æ¡ç›®"):
                            for i, entry in enumerate(entries_without_arxiv):
                                st.write(f"{i+1}. **{entry['cleaned_title']}**")
                                if entry['authors']:
                                    st.write(f"   - ä½œè€…: {entry['authors']}")
                                if entry['year']:
                                    st.write(f"   - å¹´ä»½: {entry['year']}")
                else:
                    st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„è®ºæ–‡")
                    st.session_state.arxiv_papers = []
                    st.session_state.title_papers = []
                    st.session_state.all_papers = []
            
            except Exception as e:
                st.error(f"âŒ BIBæ–‡ä»¶è§£æå¤±è´¥: {e}")
                st.session_state.arxiv_papers = []
        
        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    elif st.session_state.bib_content:
        # å¦‚æœæœ‰ä¹‹å‰ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæ˜¾ç¤ºä¿¡æ¯
        st.info("ğŸ“„ å·²ä¸Šä¼  BIB æ–‡ä»¶")
        st.info(f"ğŸ“Š æ€»å…± {len(st.session_state.all_papers)} ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ {len(st.session_state.arxiv_papers)} ç¯‡æœ‰ arXiv IDï¼Œ{len(st.session_state.title_papers)} ç¯‡éœ€è¦æ ‡é¢˜æœç´¢")

with col2:
    st.header("ğŸ¯ è‡ªå®šä¹‰ Prompt")
    
    # Prompt æ¨¡æ¿é€‰æ‹©
    prompt_type = st.selectbox(
        "é€‰æ‹© Prompt æ¨¡æ¿",
        ["é»˜è®¤æ¨¡æ¿", "å­¦æœ¯åˆ†ææ¨¡æ¿", "è‹±æ–‡ç®€æ´æ¨¡æ¿", "è‡ªå®šä¹‰æ¨¡æ¿"]
    )
    
    # é¢„å®šä¹‰çš„promptæ¨¡æ¿
    prompt_templates = {
        "é»˜è®¤æ¨¡æ¿": """### ä»»åŠ¡
ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸“å®¶ï¼Œä½ èƒ½å¤Ÿå¿«é€Ÿé˜…è¯»arxivä¸Šçš„å„ç§AIå‰æ²¿è®ºæ–‡ï¼Œå¹¶ç»™å‡ºéå¸¸å¥½çš„è®ºæ–‡æ€»ç»“ã€‚
ç°åœ¨è¯·ä½ é˜…è¯»ä»¥ä¸‹è®ºæ–‡ï¼Œå¹¶ç»™å‡ºä½ å¯¹è¿™ç¯‡è®ºæ–‡çš„è¯¦ç»†ä»‹ç»ï¼Œä»è€Œæ”¾åˆ°ä½ çš„åšå®¢ä¸Šï¼Œè®©æ›´å¤šäººäº†è§£è¿™ç¯‡è®ºæ–‡ã€‚

### è®ºæ–‡ä¿¡æ¯
#### 1. è®ºæ–‡æ ‡é¢˜: {title}
#### 2. è®ºæ–‡æ€»ç»“: {abstract}
#### 3. è®ºæ–‡å…¨æ–‡: {pdf_text}

### è¾“å‡ºæ ¼å¼ï¼ˆè¾“å‡ºè¯­è¨€ç”¨ä¸­æ–‡ï¼‰
```
## ğŸŒŸ è®ºæ–‡è§£è¯» | <æƒ³ä¸€ä¸ªï¼Œå®£ä¼ è¯¥è®ºæ–‡çš„æ–‡æ¡ˆæ ‡é¢˜>

## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
...(ä»‹ç»è®ºæ–‡çš„èƒŒæ™¯æˆ–ç—›ç‚¹ï¼Œæˆ–è€…æœ¬æ–‡çš„åŠ¨æœº)

## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
...

ğŸ’¡ åˆ›æ–°ç‚¹2
...

## ğŸ“ˆ å®éªŒç»“æœ
...

## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
...(ä»‹ç»è®ºæ–‡çš„å¯å€Ÿé‰´ä¹‹å¤„)
```""",
        
        "å­¦æœ¯åˆ†ææ¨¡æ¿": """### ä»»åŠ¡
ä½ æ˜¯ä¸€ä¸ªAIç ”ç©¶ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹è®ºæ–‡è¿›è¡Œä¸“ä¸šçš„å­¦æœ¯åˆ†æã€‚

### è®ºæ–‡ä¿¡æ¯
#### æ ‡é¢˜: {title}
#### ä½œè€…: {authors}
#### å¹´ä»½: {year}
#### æ€»ç»“: {abstract}
#### è®ºæ–‡å…¨æ–‡: {pdf_text}

### è¾“å‡ºæ ¼å¼
```
## ğŸ“Š è®ºæ–‡æ·±åº¦åˆ†æ | {title}

## ğŸ¯ ç ”ç©¶é—®é¢˜ä¸åŠ¨æœº
(ç®€è¦æè¿°è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜å’Œç ”ç©¶åŠ¨æœº)

## ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•
(è¯¦ç»†æè¿°è®ºæ–‡æå‡ºçš„ä¸»è¦æŠ€æœ¯æ–¹æ³•å’Œåˆ›æ–°ç‚¹)

## ğŸ“ˆ å®éªŒè®¾è®¡ä¸ç»“æœ
(åˆ†æå®éªŒè®¾ç½®ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’Œä¸»è¦ç»“æœ)

## ğŸ’¡ ä¼˜åŠ¿ä¸å±€é™æ€§
### ä¼˜åŠ¿:
- ...
### å±€é™æ€§:
- ...

## ğŸ”® æœªæ¥ç ”ç©¶æ–¹å‘
(åŸºäºè®ºæ–‡å†…å®¹ï¼Œæå‡ºå¯èƒ½çš„åç»­ç ”ç©¶æ–¹å‘)

## ğŸ“ å…³é”®è¦ç‚¹æ€»ç»“
(ç”¨3-5ä¸ªè¦ç‚¹æ€»ç»“è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®)
```""",
        
        "è‹±æ–‡ç®€æ´æ¨¡æ¿": """### Task
You are an AI research expert. Please analyze the following paper and provide a concise summary.

### Paper Information
Title: {title}
Authors: {authors}
Abstract: {abstract}
Full Text: {pdf_text}

### Output Format
```
## ğŸ“„ Paper Summary | {title}

## ğŸ¯ Problem & Motivation
...

## ğŸ› ï¸ Method
...

## ğŸ“Š Experiments
...

## ğŸ’­ Takeaways
...
```""",
        
        "è‡ªå®šä¹‰æ¨¡æ¿": ""
    }
    
    if prompt_type == "è‡ªå®šä¹‰æ¨¡æ¿":
        custom_prompt = st.text_area(
            "è‡ªå®šä¹‰ Prompt",
            value="",
            height=300,
            help="å¯ç”¨å ä½ç¬¦: {title}, {abstract}, {authors}, {year}, {url}, {pdf_text}, {full_pdf_text}, {doi}, {arxiv_id}"
        )
    else:
        custom_prompt = prompt_templates[prompt_type]
        st.markdown('<div class="prompt-preview">', unsafe_allow_html=True)
        st.markdown("**å½“å‰ Prompt é¢„è§ˆ:**")
        st.text_area("Prompté¢„è§ˆ", custom_prompt, height=300, disabled=True, label_visibility="collapsed")
        st.markdown('</div>', unsafe_allow_html=True)
    
    # æ˜¾ç¤ºå¯ç”¨å ä½ç¬¦
    with st.expander("ğŸ“ å¯ç”¨å ä½ç¬¦è¯´æ˜"):
        placeholders = [
            "{title} - è®ºæ–‡æ ‡é¢˜",
            "{abstract} - è®ºæ–‡æ€»ç»“", 
            "{authors} - ä½œè€…ä¿¡æ¯",
            "{year} - å‘è¡¨å¹´ä»½",
            "{url} - è®ºæ–‡é“¾æ¥",
            "{pdf_text} - è®ºæ–‡å…¨æ–‡(æˆªæ–­ç‰ˆ)",
            "{full_pdf_text} - è®ºæ–‡å…¨æ–‡(å®Œæ•´ç‰ˆ)",
            "{doi} - DOI ä¿¡æ¯",
            "{arxiv_id} - arXiv ID"
        ]
        for placeholder in placeholders:
            st.write(f"â€¢ {placeholder}")

# ç”Ÿæˆç»¼è¿°æŒ‰é’®
st.header("ğŸš€ ç”Ÿæˆç»¼è¿°")

with st.form("bib_survey_form"):
    output_filename = st.text_input("è¾“å‡ºæ–‡ä»¶å", value="survey_from_bib_result.md")
    submitted = st.form_submit_button("ğŸ¯ å¼€å§‹ç”Ÿæˆç»¼è¿°", use_container_width=True)

# è¿›åº¦æ˜¾ç¤ºåŒºåŸŸ
progress_placeholder = st.empty()
paper_list_placeholder = st.empty()
summary_placeholder = st.empty()
download_placeholder = st.empty()

if submitted:
    if st.session_state.bib_content is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼  BIB æ–‡ä»¶")
    elif len(st.session_state.all_papers) == 0:
        st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„è®ºæ–‡")
    else:
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜BIBå†…å®¹
            with tempfile.NamedTemporaryFile(mode='w', suffix='.bib', delete=False, encoding='utf-8') as tmp_file:
                tmp_file.write(st.session_state.bib_content)
                temp_bib_path = tmp_file.name
            
            progress_placeholder.info("ğŸ“š å¼€å§‹å¹¶è¡Œå¤„ç† BIB æ–‡ä»¶...")
            
            # ç¡®å®šä½¿ç”¨çš„prompt
            final_prompt = custom_prompt if custom_prompt.strip() else None
            
            # å¹¶è¡Œç”Ÿæˆç»¼è¿°
            output_path = generate_survey_from_bib_parallel(
                bib_file=temp_bib_path,
                output_file=f"output/{output_filename}",
                llm_provider=llm_provider,
                model_name=model_name,
                custom_prompt=final_prompt,
                pdf_dir="pdfs/",
                progress_placeholder=progress_placeholder,
                paper_list_placeholder=paper_list_placeholder,
                max_workers=max_workers
            )
            
            if output_path and Path(output_path).exists():
                # è¯»å–ç”Ÿæˆçš„markdownå†…å®¹
                markdown_content = Path(output_path).read_text(encoding='utf-8')
                
                # æ˜¾ç¤ºé¢„è§ˆ
                summary_placeholder.markdown("### ğŸ“‹ ç»¼è¿°é¢„è§ˆ")
                with summary_placeholder.expander("æŸ¥çœ‹ç”Ÿæˆçš„ç»¼è¿°", expanded=True):
                    # é¢„å¤„ç†markdownå†…å®¹ï¼Œç§»é™¤ä»£ç å—æ ‡è®°è®©å†…å®¹æ­£å¸¸æ˜¾ç¤º
                    def replace_code_blocks(match):
                        return match.group(1)
                    
                    display_markdown = re.sub(r'```[a-zA-Z]*\n(.*?)\n```', replace_code_blocks, markdown_content, flags=re.DOTALL)
                    st.markdown(display_markdown)
                
                # ç”ŸæˆHTMLç‰ˆæœ¬
                import markdown
                # é¢„å¤„ç†markdownå†…å®¹ï¼Œç§»é™¤ä»£ç å—æ ‡è®°è®©å†…å®¹æ­£å¸¸æ¸²æŸ“
                processed_markdown = markdown_content
                # æ›¿æ¢markdownä»£ç å—ä¸ºæ™®é€šæ®µè½ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
                def replace_code_blocks(match):
                    return match.group(1)
                
                processed_markdown = re.sub(r'```[a-zA-Z]*\n(.*?)\n```', replace_code_blocks, processed_markdown, flags=re.DOTALL)
                # ç”ŸæˆHTML
                html_content = markdown.markdown(processed_markdown, extensions=['tables', 'fenced_code', 'nl2br'])
                
                # ä¸‹è½½æŒ‰é’®
                download_placeholder.markdown("### ğŸ“¥ ä¸‹è½½ç»“æœ")
                col_download1, col_download2 = download_placeholder.columns(2)
                
                with col_download1:
                    st.download_button(
                        "ğŸ“„ ä¸‹è½½ Markdown æ–‡ä»¶",
                        markdown_content,
                        file_name=output_filename,
                        mime="text/markdown"
                    )
                
                with col_download2:
                    st.download_button(
                        "ğŸŒ ä¸‹è½½ HTML æ–‡ä»¶", 
                        html_content,
                        file_name=output_filename.replace('.md', '.html'),
                        mime="text/html"
                    )
            else:
                progress_placeholder.error("âŒ ç»¼è¿°ç”Ÿæˆå¤±è´¥")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_bib_path)
            except:
                pass
                
        except Exception as e:
            progress_placeholder.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            st.exception(e)

# ä½¿ç”¨è¯´æ˜
with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### ğŸ¯ åŠŸèƒ½ç‰¹ç‚¹
    
    1. **BIB æ–‡ä»¶æ”¯æŒ**: ä¸Šä¼ æ‚¨çš„ BibTeX æ–‡ä»¶ï¼Œè‡ªåŠ¨è§£æè®ºæ–‡ä¿¡æ¯
    2. **æ™ºèƒ½è®ºæ–‡è¯†åˆ«**: 
       - ğŸ†” **arXiv ID è¯†åˆ«**: è‡ªåŠ¨æå–åŒ…å« arXiv ID çš„è®ºæ–‡
       - ğŸ” **å¤šå±‚æ ‡é¢˜æœç´¢**: å¯¹æ²¡æœ‰ arXiv ID çš„è®ºæ–‡ï¼Œé‡‡ç”¨ä¸‰å±‚æœç´¢ç­–ç•¥ï¼š
         * ç²¾ç¡®æ ‡é¢˜åŒ¹é…
         * çµæ´»æ ‡é¢˜æœç´¢
         * å…³é”®è¯ç»„åˆæœç´¢
       - ğŸ§¹ **æ ‡é¢˜æ¸…ç†**: è‡ªåŠ¨æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ {}ã€\\ ç­‰ï¼‰
       - ğŸ¯ **æ™ºèƒ½åŒ¹é…**: ç»¼åˆè¯æ±‡é‡å åº¦ã€å­—ç¬¦ç›¸ä¼¼åº¦å’Œé•¿åº¦ç›¸ä¼¼åº¦è¿›è¡ŒåŒ¹é…
    3. **å…¨æµç¨‹å¹¶è¡Œå¤„ç†**: 
       - ğŸ” **å¹¶è¡ŒarXivæœç´¢**: åŒæ—¶è·å–å¤šç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
       - ğŸ“¥ **å¹¶è¡ŒPDFå¤„ç†**: åŒæ—¶ä¸‹è½½å’Œå¤„ç†å¤šä¸ªPDFæ–‡ä»¶
       - âœ¨ **å¹¶è¡ŒLLMæ€»ç»“**: å¹¶è¡Œç”Ÿæˆè®ºæ–‡æ€»ç»“ï¼Œå¤§å¹…æå‡æ•ˆç‡
    4. **å®æ—¶è¿›åº¦æ˜¾ç¤º**: è¯¦ç»†çš„è¿›åº¦æ¡å’ŒçŠ¶æ€æ›´æ–°
    5. **è‡ªå®šä¹‰ Prompt**: æ”¯æŒå¤šç§é¢„è®¾æ¨¡æ¿å’Œå®Œå…¨è‡ªå®šä¹‰çš„ Prompt
    6. **å¤šæ ¼å¼è¾“å‡º**: åŒæ—¶ç”Ÿæˆ Markdown å’Œ HTML æ ¼å¼çš„ç»¼è¿°
    
    ### ğŸ“ ä½¿ç”¨æ­¥éª¤
    
    1. **ä¸Šä¼ æ–‡ä»¶**: é€‰æ‹©æ‚¨çš„ BibTeX æ–‡ä»¶ä¸Šä¼ 
    2. **é…ç½®å‚æ•°**: é€‰æ‹© LLM æ¨¡å‹å’Œå¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°
    3. **é…ç½® Prompt**: é€‰æ‹©é¢„è®¾æ¨¡æ¿æˆ–è‡ªå®šä¹‰ Prompt
    4. **ç”Ÿæˆç»¼è¿°**: ç‚¹å‡»ç”ŸæˆæŒ‰é’®ï¼Œäº«å—å…¨æµç¨‹å¹¶è¡Œå¤„ç†çš„é«˜æ•ˆä½“éªŒ
    5. **ä¸‹è½½ç»“æœ**: è·å–ç”Ÿæˆçš„ç»¼è¿°æ–‡ä»¶
    
    ### âš¡ å¹¶è¡Œå¤„ç†ä¼˜åŠ¿
    
    - **arXivæœç´¢å¹¶è¡Œ**: åŒæ—¶æŸ¥è¯¢å¤šä¸ªè®ºæ–‡IDï¼Œå¤§å¹…å‡å°‘ç½‘ç»œç­‰å¾…æ—¶é—´
    - **è®ºæ–‡ä¸‹è½½å¹¶è¡Œ**: åŒæ—¶ä¸‹è½½å¤šä¸ªè®ºæ–‡PDFï¼Œæ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦
    - **LLMæ€»ç»“å¹¶è¡Œ**: å¹¶è¡Œè°ƒç”¨LLMç”Ÿæˆæ€»ç»“ï¼Œå……åˆ†åˆ©ç”¨è®¡ç®—èµ„æº
    - **å¯è°ƒèŠ‚çº¿ç¨‹æ•°**: æ ¹æ®ç³»ç»Ÿæ€§èƒ½å’Œç½‘ç»œçŠ¶å†µè°ƒæ•´å¹¶è¡Œåº¦
    
    ### ğŸ”§ æ€§èƒ½è°ƒä¼˜å»ºè®®
    
    - **ç½‘ç»œè‰¯å¥½**: å¯å°†çº¿ç¨‹æ•°è®¾ç½®ä¸º6-8ï¼ŒåŠ å¿«arXivæœç´¢å’Œä¸‹è½½
    - **ç½‘ç»œä¸€èˆ¬**: å»ºè®®ä½¿ç”¨3-4ä¸ªçº¿ç¨‹ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§
    - **ç³»ç»Ÿèµ„æºæœ‰é™**: ä½¿ç”¨1-2ä¸ªçº¿ç¨‹ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ
    
    ### ğŸ’¡ Prompt å ä½ç¬¦
    
    åœ¨è‡ªå®šä¹‰ Prompt ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å ä½ç¬¦æ¥æ’å…¥è®ºæ–‡ä¿¡æ¯ï¼š
    - `{title}` - è®ºæ–‡æ ‡é¢˜
    - `{abstract}` - è®ºæ–‡æ€»ç»“
    - `{authors}` - ä½œè€…ä¿¡æ¯
    - `{year}` - å‘è¡¨å¹´ä»½
    - `{pdf_text}` - è®ºæ–‡å…¨æ–‡(æˆªæ–­ç‰ˆï¼Œçº¦10000å­—ç¬¦)
    - `{full_pdf_text}` - è®ºæ–‡å…¨æ–‡(å®Œæ•´ç‰ˆ)
    
    ### âš ï¸ æ³¨æ„äº‹é¡¹
    
    - æ”¯æŒåŒ…å« arXiv ID çš„è®ºæ–‡å’Œé€šè¿‡æ ‡é¢˜æœç´¢çš„è®ºæ–‡
    - **æ ‡é¢˜æœç´¢ä¼˜åŒ–**: é‡‡ç”¨å¤šå±‚æœç´¢ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜åŒ¹é…æˆåŠŸç‡
    - **HTML è¾“å‡ºä¼˜åŒ–**: è‡ªåŠ¨å¤„ç†ä»£ç å—æ ‡è®°ï¼Œç¡®ä¿è®ºæ–‡å†…å®¹æ­£ç¡®æ¸²æŸ“
    - å¹¶è¡Œå¤„ç†ä¼šæå‡é€Ÿåº¦ï¼Œä½†ä¹Ÿä¼šå¢åŠ ç½‘ç»œå’Œç³»ç»Ÿèµ„æºæ¶ˆè€—
    - arXiv APIæœ‰é€Ÿç‡é™åˆ¶ï¼Œè¿‡é«˜çš„å¹¶è¡Œåº¦å¯èƒ½å¯¼è‡´è¯·æ±‚å¤±è´¥
    - å»ºè®®åœ¨ç½‘ç»œç¨³å®šçš„ç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œä»¥è·å¾—æœ€ä½³ä½“éªŒ
    """)

# é¡µè„š
st.markdown("---")
st.markdown("ğŸ’¡ **æç¤º**: ç°åœ¨æ”¯æŒæ™ºèƒ½è®ºæ–‡è¯†åˆ«å’Œå…¨æµç¨‹å¹¶è¡Œå¤„ç†ï¼é‡‡ç”¨å¤šå±‚æœç´¢ç­–ç•¥æé«˜è®ºæ–‡åŒ¹é…æˆåŠŸç‡ï¼Œå¹¶ä¼˜åŒ–HTMLè¾“å‡ºæ ¼å¼ï¼Œè®©ç»¼è¿°å†…å®¹æ›´å®Œæ•´ã€æ›´ç¾è§‚ã€‚")
