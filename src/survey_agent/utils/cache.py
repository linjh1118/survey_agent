import json
import os
import hashlib
from typing import Dict, Any, Optional
from datetime import datetime, timedelta


class PaperCache:
    """è®ºæ–‡æ‘˜è¦ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "cache", cache_expire_days: int = 30):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
            cache_expire_days: ç¼“å­˜è¿‡æœŸå¤©æ•°ï¼Œé»˜è®¤30å¤©
        """
        self.cache_dir = cache_dir
        self.cache_expire_days = cache_expire_days
        self.cache_file = os.path.join(cache_dir, "paper_summaries.json")
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½ç°æœ‰ç¼“å­˜
        self._cache = self._load_cache()
    
    def _load_cache(self) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                current_time = datetime.now()
                cleaned_cache = {}
                
                for arxiv_id, entry in cache_data.items():
                    cached_time = datetime.fromisoformat(entry.get('cached_at', '1970-01-01'))
                    if current_time - cached_time < timedelta(days=self.cache_expire_days):
                        cleaned_cache[arxiv_id] = entry
                
                # å¦‚æœæ¸…ç†äº†ç¼“å­˜ï¼Œä¿å­˜æ›´æ–°åçš„ç¼“å­˜
                if len(cleaned_cache) != len(cache_data):
                    self._save_cache(cleaned_cache)
                
                return cleaned_cache
                
            except (json.JSONDecodeError, KeyError, ValueError) as e:
                print(f"ç¼“å­˜æ–‡ä»¶æŸåï¼Œé‡æ–°åˆ›å»º: {e}")
                return {}
        
        return {}
    
    def _save_cache(self, cache_data: Dict[str, Any] = None):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        data_to_save = cache_data if cache_data is not None else self._cache
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _extract_arxiv_id(self, paper: Dict[str, Any]) -> Optional[str]:
        """ä»è®ºæ–‡ä¿¡æ¯ä¸­æå–arxiv.id"""
        # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„å­—æ®µè·å–arxiv.id
        arxiv_id = paper.get('arxiv_id')
        if arxiv_id:
            return arxiv_id
        
        # ä»URLä¸­æå–
        url = paper.get('url', '')
        if 'arxiv.org' in url:
            # åŒ¹é…ç±»ä¼¼ "http://arxiv.org/pdf/2406.09172v2" çš„URL
            import re
            match = re.search(r'arxiv\.org/(?:pdf|abs)/(\d+\.\d+)', url)
            if match:
                return match.group(1)
        
        # ä»DOIä¸­æå–
        doi = paper.get('doi', '')
        if 'arxiv' in doi.lower():
            match = re.search(r'(\d+\.\d+)', doi)
            if match:
                return match.group(1)
        
        return None
    
    def _generate_content_hash(self, paper: Dict[str, Any]) -> str:
        """ç”Ÿæˆè®ºæ–‡å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹å†…å®¹å˜åŒ–"""
        # ä½¿ç”¨æ ‡é¢˜ã€æ‘˜è¦å’ŒPDFæ–‡æœ¬çš„å‰1000å­—ç¬¦ç”Ÿæˆå“ˆå¸Œ
        content_parts = [
            paper.get('title', ''),
            paper.get('summary', ''),
            paper.get('pdf_text', '')[:1000]
        ]
        content = ''.join(content_parts)
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get_cached_summary(self, paper: Dict[str, Any]) -> Optional[str]:
        """
        è·å–ç¼“å­˜çš„è®ºæ–‡æ‘˜è¦
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            ç¼“å­˜çš„æ‘˜è¦å­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰ç¼“å­˜åˆ™è¿”å›None
        """
        arxiv_id = self._extract_arxiv_id(paper)
        if not arxiv_id:
            return None
        
        if arxiv_id in self._cache:
            cached_entry = self._cache[arxiv_id]
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰å˜åŒ–ï¼ˆå¯é€‰ï¼‰
            current_hash = self._generate_content_hash(paper)
            cached_hash = cached_entry.get('content_hash', '')
            
            if current_hash == cached_hash:
                print(f"ğŸ“ ä½¿ç”¨ç¼“å­˜æ‘˜è¦: {paper.get('title', 'Unknown')[:50]}...")
                return cached_entry.get('summary', '')
            else:
                print(f"ğŸ”„ è®ºæ–‡å†…å®¹å·²æ›´æ–°ï¼Œé‡æ–°ç”Ÿæˆæ‘˜è¦: {arxiv_id}")
                # åˆ é™¤è¿‡æœŸç¼“å­˜
                del self._cache[arxiv_id]
        
        return None
    
    def cache_summary(self, paper: Dict[str, Any], summary: str):
        """
        ç¼“å­˜è®ºæ–‡æ‘˜è¦
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            summary: ç”Ÿæˆçš„æ‘˜è¦
        """
        arxiv_id = self._extract_arxiv_id(paper)
        if not arxiv_id or not summary:
            return
        
        cache_entry = {
            'arxiv_id': arxiv_id,
            'title': paper.get('title', ''),
            'summary': summary,
            'content_hash': self._generate_content_hash(paper),
            'cached_at': datetime.now().isoformat(),
            'model_info': {
                'provider': getattr(self, '_last_provider', 'unknown'),
                'model_name': getattr(self, '_last_model', 'unknown')
            }
        }
        
        self._cache[arxiv_id] = cache_entry
        self._save_cache()
        print(f"ğŸ’¾ å·²ç¼“å­˜æ‘˜è¦: {paper.get('title', 'Unknown')[:50]}... (arxiv:{arxiv_id})")
    
    def set_model_info(self, provider: str, model_name: str):
        """è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºç¼“å­˜è®°å½•"""
        self._last_provider = provider
        self._last_model = model_name
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_entries': len(self._cache),
            'cache_file': self.cache_file,
            'cache_size_mb': round(os.path.getsize(self.cache_file) / 1024 / 1024, 2) if os.path.exists(self.cache_file) else 0,
            'oldest_entry': min([entry.get('cached_at', '') for entry in self._cache.values()]) if self._cache else None,
            'newest_entry': max([entry.get('cached_at', '') for entry in self._cache.values()]) if self._cache else None
        }
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self._cache = {}
        self._save_cache()
        print("ğŸ—‘ï¸  å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜")


# å…¨å±€ç¼“å­˜å®ä¾‹
_global_cache = None


def get_paper_cache() -> PaperCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹"""
    global _global_cache
    if _global_cache is None:
        _global_cache = PaperCache()
    return _global_cache 