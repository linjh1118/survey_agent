import streamlit as st
import os
import time
import re
from survey_agent.survey.generator import generate_survey, generate_markdown
from survey_agent.arxiv_tools.search import search_papers
from survey_agent.arxiv_tools.download import process_paper
from survey_agent.llm.summarize import get_summarizer
from survey_agent.arxiv_tools.caption_crawler import crawl_arxiv_html
from survey_agent.arxiv_tools.html_generator import (
    parse_markdown_summaries,
    find_all_papers,
    generate_html_with_summary
)

from survey_agent.env import *
import uuid

st.set_page_config(page_title="AI è®ºæ–‡ç»¼è¿°ç”Ÿæˆå™¨", layout="wide")
st.title("ğŸŒŸ AI è®ºæ–‡ç»¼è¿°ç”Ÿæˆå™¨ ğŸ’¡")
st.markdown("""
<style>
.big-font { font-size:22px !important; }
</style>
""", unsafe_allow_html=True)

from concurrent.futures import ThreadPoolExecutor, as_completed

def process_papers_parallel(papers, process_paper, progress_placeholder, paper_list_placeholder, max_workers=4):
    processed_papers = [None] * len(papers)  # Pre-allocate list to maintain order
    
    def process_with_progress(idx_paper):
        idx, paper = idx_paper
        result = process_paper(paper)
        return idx, result
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Create list of (idx, paper) tuples for processing
        futures = [executor.submit(process_with_progress, (i, paper)) 
                  for i, paper in enumerate(papers)]
        
        for future in as_completed(futures):
            idx, result = future.result()
            processed_papers[idx] = result
            
            # Update progress display
            progress_placeholder.info(f"æ­£åœ¨ä¸‹è½½ä¸å¤„ç†è®ºæ–‡ {sum(1 for p in processed_papers if p is not None)}/{len(papers)}")
            paper_list_placeholder.markdown(
                "\n".join([
                    f"- {'ğŸ“¥ ' if processed_papers[i] is not None else 'ğŸ” '}{papers[i].title}" 
                    for i in range(len(papers))
                ])
            )
    
    return [p for p in processed_papers if p is not None]

def summarize_papers_parallel(processed_papers, summarizer, progress_placeholder, paper_list_placeholder, max_workers=4):
    summarized_papers = [None] * len(processed_papers)  # Pre-allocate list to maintain order

    def summarize_with_progress(idx_paper):
        idx, paper = idx_paper
        paper['llm_summary_res'] = summarizer.summarize(paper)
        return idx, paper

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(summarize_with_progress, (i, paper))
                  for i, paper in enumerate(processed_papers)]

        for future in as_completed(futures):
            idx, result = future.result()
            summarized_papers[idx] = result

            # Update progress display
            progress_placeholder.info(f"LLM æ€»ç»“ä¸­ {sum(1 for p in summarized_papers if p is not None)}/{len(processed_papers)}")
            paper_list_placeholder.markdown(
                "\n".join([
                    f"- {'âœ¨ ' if summarized_papers[i] is not None else 'ğŸ“¥ '}{processed_papers[i]['title']}"
                    for i in range(len(processed_papers))
                ])
            )

    return [p for p in summarized_papers if p is not None]


def crawl_paper_images_parallel(processed_papers, output_dir, progress_placeholder, paper_list_placeholder, max_workers=4):
    """å¹¶è¡Œçˆ¬å–è®ºæ–‡å›¾ç‰‡å’ŒCaption"""
    crawl_results = [None] * len(processed_papers)

    def extract_arxiv_id(paper):
        """ä»è®ºæ–‡æ•°æ®ä¸­æå– arXiv ID"""
        # å°è¯•ä» entry_id æå–
        if 'entry_id' in paper:
            match = re.search(r'(\d{4}\.\d{5})', paper['entry_id'])
            if match:
                return match.group(1)

        # å°è¯•ä» pdf_url æå–
        if 'pdf_url' in paper:
            match = re.search(r'(\d{4}\.\d{5})', paper['pdf_url'])
            if match:
                return match.group(1)

        return None

    def crawl_with_progress(idx_paper):
        idx, paper = idx_paper
        arxiv_id = extract_arxiv_id(paper)

        if not arxiv_id:
            return idx, None

        # çˆ¬å–è¯¥è®ºæ–‡çš„ HTML é¡µé¢
        url = f"https://arxiv.org/html/{arxiv_id}"
        try:
            result = crawl_arxiv_html(url, output_dir)
            return idx, result
        except Exception as e:
            print(f"çˆ¬å– {arxiv_id} å¤±è´¥: {e}")
            return idx, None

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(crawl_with_progress, (i, paper))
                  for i, paper in enumerate(processed_papers)]

        for future in as_completed(futures):
            idx, result = future.result()
            crawl_results[idx] = result

            # Update progress display
            completed = sum(1 for r in crawl_results if r is not None)
            progress_placeholder.info(f"æ­£åœ¨çˆ¬å–è®ºæ–‡å›¾ç‰‡ {sum(1 for r in crawl_results if r is not False)}/{len(processed_papers)} (æˆåŠŸ: {completed})")
            paper_list_placeholder.markdown(
                "\n".join([
                    f"- {'ğŸ–¼ï¸ ' if crawl_results[i] is not None else 'âœ¨ '}{processed_papers[i]['title']}"
                    for i in range(len(processed_papers))
                ])
            )

    return crawl_results




with st.form("query_form"):
    query = st.text_input("è¯·è¾“å…¥ä½ çš„è®ºæ–‡æŸ¥è¯¢å…³é”®è¯ï¼ˆæ”¯æŒå¤šä¸ªå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼‰", "Vision Language Model, Game")
    max_results = st.slider("æœ€å¤šæ£€ç´¢è®ºæ–‡æ•°", 5, 50, 10)
    # llm_provider = st.selectbox("LLM æä¾›å•†", ["openai"])
    llm_provider = "openai"
    # model_name = st.text_input("LLM æ¨¡å‹å", "glm-4-air")
    model_name = st.selectbox("LLM æ¨¡å‹å", ['None', "ep-20250529110941-khvtx"])

    # æ–°å¢ï¼šæ˜¯å¦çˆ¬å–å›¾ç‰‡é€‰é¡¹
    crawl_images = st.checkbox("çˆ¬å–è®ºæ–‡å›¾ç‰‡å’Œ Captionï¼ˆç”Ÿæˆå¯è§†åŒ– HTMLï¼‰", value=True)

    submitted = st.form_submit_button("å¼€å§‹ç”Ÿæˆç»¼è¿°")

progress_placeholder = st.empty()
paper_list_placeholder = st.empty()
summary_placeholder = st.empty()
download_placeholder = st.empty()

if submitted:
    terms = [t.strip() for t in query.split(",") if t.strip()]
    progress_placeholder.info("æ­£åœ¨æ£€ç´¢è®ºæ–‡...")
    papers = search_papers(terms=terms, max_results=max_results)
    titles = [p.title for p in papers]
    paper_list_placeholder.markdown(
        "\n".join([f"- ğŸ” {t}" for t in titles]) or "æœªæ£€ç´¢åˆ°è®ºæ–‡ã€‚"
    )
    
    # ä¿®æ”¹åŸå§‹ä»£ç éƒ¨åˆ†
    processed_papers = process_papers_parallel(papers, process_paper, progress_placeholder, paper_list_placeholder)
    progress_placeholder.info("æ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆæ€»ç»“...")
    # summarizer = get_summarizer(llm_provider, model_name)
    if model_name != "None":
        summarizer = get_summarizer(llm_provider, model_name)
    else:
        summarizer = get_summarizer(llm_provider)
    processed_papers = summarize_papers_parallel(processed_papers, summarizer, progress_placeholder, paper_list_placeholder)   
        
    progress_placeholder.success("å…¨éƒ¨è®ºæ–‡å¤„ç†å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆ markdown...")
    unique_id = str(uuid.uuid4())
    output_md = f"survey_result_{unique_id}.md"
    markdown_content = generate_markdown(processed_papers, output_md, terms)

    # åˆå§‹åŒ– HTML ç›¸å…³å˜é‡
    html_content_with_images = None
    html_output_file = None
    successful_crawls = 0

    # æ­¥éª¤ 4: çˆ¬å–è®ºæ–‡å›¾ç‰‡å’Œ Captionï¼ˆå¯é€‰ï¼‰
    if crawl_images:
        progress_placeholder.info("æ­£åœ¨çˆ¬å–è®ºæ–‡å›¾ç‰‡å’Œ Caption...")
        output_dir = "paper_captions"
        os.makedirs(output_dir, exist_ok=True)

        crawl_results = crawl_paper_images_parallel(
            processed_papers,
            output_dir,
            progress_placeholder,
            paper_list_placeholder,
            max_workers=4
        )

        # ç»Ÿè®¡çˆ¬å–ç»“æœ
        successful_crawls = sum(1 for r in crawl_results if r is not None)
        progress_placeholder.success(f"å›¾ç‰‡çˆ¬å–å®Œæˆï¼æˆåŠŸ: {successful_crawls}/{len(processed_papers)}")

        # æ­¥éª¤ 5: ç”Ÿæˆå¸¦å›¾ç‰‡çš„ HTML
        progress_placeholder.info("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ– HTML...")

        # è§£æ Markdown æ€»ç»“
        summaries = parse_markdown_summaries(output_md)

        # æŸ¥æ‰¾çˆ¬å–çš„è®ºæ–‡æ•°æ®
        papers_data = find_all_papers(output_dir)

        # ç”Ÿæˆ HTML
        html_output_file = f"paper_captions_{unique_id}.html"

        if papers_data:
            generate_html_with_summary(papers_data, summaries, html_output_file)
            progress_placeholder.success(f"å¯è§†åŒ– HTML å·²ç”Ÿæˆ: {html_output_file}")

            # è¯»å–ç”Ÿæˆçš„ HTML å†…å®¹ç”¨äºä¸‹è½½
            with open(html_output_file, 'r', encoding='utf-8') as f:
                html_content_with_images = f.read()
        else:
            progress_placeholder.warning("æœªæ‰¾åˆ°å›¾ç‰‡æ•°æ®ï¼Œç”Ÿæˆæ ‡å‡† HTML")
            import markdown
            html_content_with_images = markdown.markdown(markdown_content, extensions=['tables', 'fenced_code'])
    else:
        # ä¸çˆ¬å–å›¾ç‰‡ï¼Œç”Ÿæˆæ ‡å‡† HTML
        import markdown
        html_content_with_images = markdown.markdown(markdown_content, extensions=['tables', 'fenced_code'])
        html_output_file = f"survey_result_{unique_id}.html"

    # æ˜¾ç¤ºé¢„è§ˆ
    summary_placeholder.markdown("### ç»¼è¿° Markdown é¢„è§ˆ")
    summary_placeholder.markdown(markdown_content)

    # æä¾›ä¸‹è½½
    download_placeholder.markdown("### ä¸‹è½½ç»“æœ")

    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            "ğŸ“„ ä¸‹è½½ Markdown æ–‡ä»¶",
            markdown_content,
            file_name=output_md,
            mime="text/markdown"
        )

    with col2:
        if html_content_with_images:
            button_label = "ğŸŒ ä¸‹è½½ HTML æ–‡ä»¶ï¼ˆå«å›¾ç‰‡ï¼‰" if crawl_images and successful_crawls > 0 else "ğŸŒ ä¸‹è½½ HTML æ–‡ä»¶"
            st.download_button(
                button_label,
                html_content_with_images,
                file_name=html_output_file,
                mime="text/html"
            )

    # æœ€ç»ˆçŠ¶æ€
    if crawl_images:
        progress_placeholder.success(f"ğŸ‰ ç»¼è¿°å·²ç”Ÿæˆï¼å…±å¤„ç† {len(processed_papers)} ç¯‡è®ºæ–‡ï¼Œçˆ¬å– {successful_crawls} ç¯‡å›¾ç‰‡")
    else:
        progress_placeholder.success(f"ğŸ‰ ç»¼è¿°å·²ç”Ÿæˆï¼å…±å¤„ç† {len(processed_papers)} ç¯‡è®ºæ–‡")

    # æ˜¾ç¤ºæ–‡ä»¶è·¯å¾„ä¿¡æ¯
    st.markdown("---")
    st.markdown("### ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶")

    # Markdown æ–‡ä»¶
    md_abs_path = os.path.abspath(output_md)
    st.markdown(f"**ğŸ“„ Markdown æ–‡ä»¶ï¼š**")
    st.code(md_abs_path, language="")

    # HTML æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    if html_output_file and os.path.exists(html_output_file):
        html_abs_path = os.path.abspath(html_output_file)
        st.markdown(f"**ğŸŒ HTML æ–‡ä»¶ï¼š**")
        st.code(html_abs_path, language="")

        # æä¾›æ‰“å¼€é“¾æ¥
        file_url = f"file://{html_abs_path}"
        st.markdown(f"ğŸ‘‰ [ç‚¹å‡»åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ HTML]({file_url})")

        # è‡ªåŠ¨æ‰“å¼€é€‰é¡¹
        col_auto1, col_auto2 = st.columns([1, 3])
        with col_auto1:
            if st.button("ğŸš€ è‡ªåŠ¨æ‰“å¼€ HTML"):
                import webbrowser
                webbrowser.open(file_url)
                st.success("âœ“ å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼")
        with col_auto2:
            st.caption("ç‚¹å‡»æŒ‰é’®åœ¨é»˜è®¤æµè§ˆå™¨ä¸­è‡ªåŠ¨æ‰“å¼€ HTML æ–‡ä»¶") 