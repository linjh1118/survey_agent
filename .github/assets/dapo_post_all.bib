
@misc{team_mimo_2025,
	title = {{MiMo}: {Unlocking} the {Reasoning} {Potential} of {Language} {Model} -- {From} {Pretraining} to {Posttraining}},
	shorttitle = {{MiMo}},
	url = {http://arxiv.org/abs/2505.07608},
	doi = {10.48550/arXiv.2505.07608},
	abstract = {We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Team, Xiaomi LLM-Core and Xia, Bingquan and Shen, Bowen and Cici and Zhu, Dawei and Zhang, Di and Wang, Gang and Zhang, Hailin and Liu, Huaqiu and Xiao, Jiebao and Dong, Jinhao and Zhao, Liang and Li, Peidian and Wang, Peng and Yu, Shihua and Chen, Shimao and Wang, Weikun and Ma, Wenhan and Deng, Xiangwei and Huang, Yi and Song, Yifan and Jiang, Zihan and Ye, Bowen and Cai, Can and He, Chenhong and Zhang, Dong and Zhang, Duo and Wang, Guoan and Tian, Hao and Zhao, Haochen and Qu, Heng and Xu, Hongshen and Shi, Jun and Bao, Kainan and Fang, QingKai and Zhou, Kang and Zhou, Kangyang and Li, Lei and Zhu, Menghang and Chen, Nuo and Wang, Qiantong and Liu, Shaohui and Li, Shicheng and Gu, Shuhao and Ren, Shuhuai and Liu, Shuo and Deng, Sirui and Zhuang, Weiji and Lv, Weiwei and Yang, Wenyu and Zhang, Xin and Yong, Xing and Zhang, Xing and Song, Xingchen and Xu, Xinzhe and Wang, Xu and Yan, Yihan and Tu, Yu and Tian, Yuanyuan and Wang, Yudong and Yu, Yue and Lin, Zhenru and Song, Zhichao and Yue, Zihao},
	month = may,
	year = {2025},
	note = {arXiv:2505.07608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1857/Team 等 - 2025 - MiMo Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining.pdf:application/pdf;Snapshot:files/1847/2505.html:text/html},
}

@misc{su_crossing_2025,
	title = {Crossing the {Reward} {Bridge}: {Expanding} {RL} with {Verifiable} {Rewards} {Across} {Diverse} {Domains}},
	shorttitle = {Crossing the {Reward} {Bridge}},
	url = {http://arxiv.org/abs/2503.23829},
	doi = {10.48550/arXiv.2503.23829},
	abstract = {Reinforcement learning with verifiable rewards (RLVR) has demonstrated significant success in enhancing mathematical reasoning and coding performance of large language models (LLMs), especially when structured reference answers are accessible for verification. However, its extension to broader, less structured domains remains unexplored. In this work, we investigate the effectiveness and scalability of RLVR across diverse real-world domains including medicine, chemistry, psychology, economics, and education, where structured reference answers are typically unavailable. We reveal that binary verification judgments on broad-domain tasks exhibit high consistency across various LLMs provided expert-written reference answers exist. Motivated by this finding, we utilize a generative scoring technique that yields soft, model-based reward signals to overcome limitations posed by binary verifications, especially in free-form, unstructured answer scenarios. We further demonstrate the feasibility of training cross-domain generative reward models using relatively small (7B) LLMs without the need for extensive domain-specific annotation. Through comprehensive experiments, our RLVR framework establishes clear performance gains, significantly outperforming state-of-the-art open-source aligned models such as Qwen2.5-72B and DeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach notably enhances the robustness, flexibility, and scalability of RLVR, representing a substantial step towards practical reinforcement learning applications in complex, noisy-label scenarios.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Su, Yi and Yu, Dian and Song, Linfeng and Li, Juntao and Mi, Haitao and Tu, Zhaopeng and Zhang, Min and Yu, Dong},
	month = apr,
	year = {2025},
	note = {arXiv:2503.23829 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1858/Su 等 - 2025 - Crossing the Reward Bridge Expanding RL with Verifiable Rewards Across Diverse Domains.pdf:application/pdf;Snapshot:files/1848/2503.html:text/html},
}

@misc{qu_survey_2025,
	title = {A {Survey} of {Efficient} {Reasoning} for {Large} {Reasoning} {Models}: {Language}, {Multimodality}, and {Beyond}},
	shorttitle = {A {Survey} of {Efficient} {Reasoning} for {Large} {Reasoning} {Models}},
	url = {http://arxiv.org/abs/2503.21614},
	doi = {10.48550/arXiv.2503.21614},
	abstract = {Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Qu, Xiaoye and Li, Yafu and Su, Zhaochen and Sun, Weigao and Yan, Jianhao and Liu, Dongrui and Cui, Ganqu and Liu, Daizong and Liang, Shuxian and He, Junxian and Li, Peng and Wei, Wei and Shao, Jing and Lu, Chaochao and Zhang, Yue and Hua, Xian-Sheng and Zhou, Bowen and Cheng, Yu},
	month = mar,
	year = {2025},
	note = {arXiv:2503.21614 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1859/Qu 等 - 2025 - A Survey of Efficient Reasoning for Large Reasoning Models Language, Multimodality, and Beyond.pdf:application/pdf;Snapshot:files/1849/2503.html:text/html},
}

@misc{zhang_tinyllava-video-r1_2025,
	title = {{TinyLLaVA}-{Video}-{R1}: {Towards} {Smaller} {LMMs} for {Video} {Reasoning}},
	shorttitle = {{TinyLLaVA}-{Video}-{R1}},
	url = {http://arxiv.org/abs/2504.09641},
	doi = {10.48550/arXiv.2504.09641},
	abstract = {Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of "aha moments". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhang, Xingjian and Wen, Siwei and Wu, Wenjun and Huang, Lei},
	month = apr,
	year = {2025},
	note = {arXiv:2504.09641 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/1860/Zhang 等 - 2025 - TinyLLaVA-Video-R1 Towards Smaller LMMs for Video Reasoning.pdf:application/pdf;Snapshot:files/1850/2504.html:text/html},
}

@misc{zhang_gvpo_2025,
	title = {{GVPO}: {Group} {Variance} {Policy} {Optimization} for {Large} {Language} {Model} {Post}-{Training}},
	shorttitle = {{GVPO}},
	url = {http://arxiv.org/abs/2504.19599},
	doi = {10.48550/arXiv.2504.19599},
	abstract = {Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. To address this challenge, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhang, Kaichen and Hong, Yuzhong and Bao, Junwei and Jiang, Hongfei and Song, Yang and Hong, Dingqian and Xiong, Hui},
	month = may,
	year = {2025},
	note = {arXiv:2504.19599 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1861/Zhang 等 - 2025 - GVPO Group Variance Policy Optimization for Large Language Model Post-Training.pdf:application/pdf;Snapshot:files/1851/2504.html:text/html},
}

@misc{wang_vl-rethinker_2025,
	title = {{VL}-{Rethinker}: {Incentivizing} {Self}-{Reflection} of {Vision}-{Language} {Models} with {Reinforcement} {Learning}},
	shorttitle = {{VL}-{Rethinker}},
	url = {http://arxiv.org/abs/2504.08837},
	doi = {10.48550/arXiv.2504.08837},
	abstract = {Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4\%, 63.5\% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. Our empirical results show the effectiveness of our approaches.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wang, Haozhe and Qu, Chao and Huang, Zuming and Chu, Wei and Lin, Fangzhen and Chen, Wenhu},
	month = may,
	year = {2025},
	note = {arXiv:2504.08837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1862/Wang 等 - 2025 - VL-Rethinker Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning.pdf:application/pdf;Snapshot:files/1852/2504.html:text/html},
}

@misc{wu_sailing_2025,
	title = {Sailing {AI} by the {Stars}: {A} {Survey} of {Learning} from {Rewards} in {Post}-{Training} and {Test}-{Time} {Scaling} of {Large} {Language} {Models}},
	shorttitle = {Sailing {AI} by the {Stars}},
	url = {http://arxiv.org/abs/2505.02686},
	doi = {10.48550/arXiv.2505.02686},
	abstract = {Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wu, Xiaobao},
	month = may,
	year = {2025},
	note = {arXiv:2505.02686 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1863/Wu - 2025 - Sailing AI by the Stars A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of.pdf:application/pdf;Snapshot:files/1853/2505.html:text/html},
}

@misc{wu_rlvr-world_2025,
	title = {{RLVR}-{World}: {Training} {World} {Models} with {Reinforcement} {Learning}},
	shorttitle = {{RLVR}-{World}},
	url = {http://arxiv.org/abs/2505.13934},
	doi = {10.48550/arXiv.2505.13934},
	abstract = {World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wu, Jialong and Yin, Shaofeng and Feng, Ningya and Long, Mingsheng},
	month = may,
	year = {2025},
	note = {arXiv:2505.13934 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1864/Wu 等 - 2025 - RLVR-World Training World Models with Reinforcement Learning.pdf:application/pdf;Snapshot:files/1854/2505.html:text/html},
}

@misc{shi_heimdall_2025,
	title = {Heimdall: test-time scaling on the generative verification},
	shorttitle = {Heimdall},
	url = {http://arxiv.org/abs/2504.10337},
	doi = {10.48550/arXiv.2504.10337},
	abstract = {An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5\% to 94.5\% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5\%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2\% to 70.0\% with 16x compute budget and to 83.3\% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0\%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Shi, Wenlei and Jin, Xing},
	month = apr,
	year = {2025},
	note = {arXiv:2504.10337 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1865/Shi和Jin - 2025 - Heimdall test-time scaling on the generative verification.pdf:application/pdf;Snapshot:files/1855/2504.html:text/html},
}

@misc{zha_rl_2025,
	title = {{RL} {Tango}: {Reinforcing} {Generator} and {Verifier} {Together} for {Language} {Reasoning}},
	shorttitle = {{RL} {Tango}},
	url = {http://arxiv.org/abs/2505.15034},
	doi = {10.48550/arXiv.2505.15034},
	abstract = {Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zha, Kaiwen and Gao, Zhengqi and Shen, Maohao and Hong, Zhang-Wei and Boning, Duane S. and Katabi, Dina},
	month = may,
	year = {2025},
	note = {arXiv:2505.15034 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1866/Zha 等 - 2025 - RL Tango Reinforcing Generator and Verifier Together for Language Reasoning.pdf:application/pdf;Snapshot:files/1856/2505.html:text/html},
}

@misc{chen_towards_2025,
	title = {Towards {Reasoning} {Era}: {A} {Survey} of {Long} {Chain}-of-{Thought} for {Reasoning} {Large} {Language} {Models}},
	shorttitle = {Towards {Reasoning} {Era}},
	url = {http://arxiv.org/abs/2503.09567},
	doi = {10.48550/arXiv.2503.09567},
	abstract = {Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like "overthinking" and "test-time scaling." This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Chen, Qiguang and Qin, Libo and Liu, Jinhao and Peng, Dengyun and Guan, Jiannan and Wang, Peng and Hu, Mengkang and Zhou, Yuhang and Gao, Te and Che, Wanxiang},
	month = apr,
	year = {2025},
	note = {arXiv:2503.09567 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1893/Chen 等 - 2025 - Towards Reasoning Era A Survey of Long Chain-of-Thought for Reasoning Large Language Models.pdf:application/pdf;Snapshot:files/1883/2503.html:text/html},
}

@misc{yue_does_2025,
	title = {Does {Reinforcement} {Learning} {Really} {Incentivize} {Reasoning} {Capacity} in {LLMs} {Beyond} the {Base} {Model}?},
	url = {http://arxiv.org/abs/2504.13837},
	doi = {10.48550/arXiv.2504.13837},
	abstract = {Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Yue, Yang and Chen, Zhiqi and Lu, Rui and Zhao, Andrew and Wang, Zhaokai and Yue, Yang and Song, Shiji and Huang, Gao},
	month = may,
	year = {2025},
	note = {arXiv:2504.13837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/1894/Yue 等 - 2025 - Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model.pdf:application/pdf;Snapshot:files/1884/2504.html:text/html},
}

@misc{feng_efficient_2025,
	title = {Efficient {Reasoning} {Models}: {A} {Survey}},
	shorttitle = {Efficient {Reasoning} {Models}},
	url = {http://arxiv.org/abs/2504.10903},
	doi = {10.48550/arXiv.2504.10903},
	abstract = {Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this "slow-thinking" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Feng, Sicheng and Fang, Gongfan and Ma, Xinyin and Wang, Xinchao},
	month = apr,
	year = {2025},
	note = {arXiv:2504.10903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1895/Feng 等 - 2025 - Efficient Reasoning Models A Survey.pdf:application/pdf;Snapshot:files/1885/2504.html:text/html},
}

@misc{teng_atom_2025,
	title = {Atom of {Thoughts} for {Markov} {LLM} {Test}-{Time} {Scaling}},
	url = {http://arxiv.org/abs/2502.12018},
	doi = {10.48550/arXiv.2502.12018},
	abstract = {Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning can be achieved by solving a series of independent and self-contained subquestions. These subquestions are essentially {\textbackslash}textit\{atomic questions\}, exhibiting the memoryless property similar to Markov processes. Based on this observation, we propose Atom of Thoughts ({\textbackslash}our), where each state transition consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a simplified question that maintains answer equivalence with the original problem. This answer preservation enables the iterative {\textbackslash}textit\{decomposition-contraction\} process to naturally form a meaningful Markov reasoning process. Furthermore, these atomic states can be seamlessly integrated into existing test-time scaling methods, enabling {\textbackslash}our to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of {\textbackslash}our both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, {\textbackslash}our achieves an {\textbackslash}textbf\{80.6{\textbackslash}\%\} F1 score, surpassing o3-mini by {\textbackslash}textbf\{3.4{\textbackslash}\%\} and DeepSeek-R1 by {\textbackslash}textbf\{10.6{\textbackslash}\%\}. The code is available at {\textbackslash}href\{https://github.com/qixucen/atom\}\{https://github.com/qixucen/atom\}.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Teng, Fengwei and Yu, Zhaoyang and Shi, Quan and Zhang, Jiayi and Wu, Chenglin and Luo, Yuyu},
	month = mar,
	year = {2025},
	note = {arXiv:2502.12018 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1896/Teng 等 - 2025 - Atom of Thoughts for Markov LLM Test-Time Scaling.pdf:application/pdf;Snapshot:files/1886/2502.html:text/html},
}

@misc{qian_toolrl_2025,
	title = {{ToolRL}: {Reward} is {All} {Tool} {Learning} {Needs}},
	shorttitle = {{ToolRL}},
	url = {http://arxiv.org/abs/2504.13958},
	doi = {10.48550/arXiv.2504.13958},
	abstract = {Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17\% improvement over base models and a 15\% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Qian, Cheng and Acikgoz, Emre Can and He, Qi and Wang, Hongru and Chen, Xiusi and Hakkani-Tür, Dilek and Tur, Gokhan and Ji, Heng},
	month = apr,
	year = {2025},
	note = {arXiv:2504.13958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1897/Qian 等 - 2025 - ToolRL Reward is All Tool Learning Needs.pdf:application/pdf;Snapshot:files/1887/2504.html:text/html},
}

@misc{zuo_ttrl_2025,
	title = {{TTRL}: {Test}-{Time} {Reinforcement} {Learning}},
	shorttitle = {{TTRL}},
	url = {http://arxiv.org/abs/2504.16084},
	doi = {10.48550/arXiv.2504.16084},
	abstract = {This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 211\% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the maj@n metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model maj@n, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zuo, Yuxin and Zhang, Kaiyan and Sheng, Li and Qu, Shang and Cui, Ganqu and Zhu, Xuekai and Li, Haozhan and Zhang, Yuchen and Long, Xinwei and Hua, Ermo and Qi, Biqing and Sun, Youbang and Ma, Zhiyuan and Yuan, Lifan and Ding, Ning and Zhou, Bowen},
	month = may,
	year = {2025},
	note = {arXiv:2504.16084 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1898/Zuo 等 - 2025 - TTRL Test-Time Reinforcement Learning.pdf:application/pdf;Snapshot:files/1888/2504.html:text/html},
}

@misc{jiang_t2i-r1_2025,
	title = {{T2I}-{R1}: {Reinforcing} {Image} {Generation} with {Collaborative} {Semantic}-level and {Token}-level {CoT}},
	shorttitle = {{T2I}-{R1}},
	url = {http://arxiv.org/abs/2505.00703},
	doi = {10.48550/arXiv.2505.00703},
	abstract = {Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13\% improvement on T2I-CompBench and 19\% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Jiang, Dongzhi and Guo, Ziyu and Zhang, Renrui and Zong, Zhuofan and Li, Hao and Zhuo, Le and Yan, Shilin and Heng, Pheng-Ann and Li, Hongsheng},
	month = may,
	year = {2025},
	note = {arXiv:2505.00703 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1899/Jiang 等 - 2025 - T2I-R1 Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT.pdf:application/pdf;Snapshot:files/1889/2505.html:text/html},
}

@misc{hochlehnert_sober_2025,
	title = {A {Sober} {Look} at {Progress} in {Language} {Model} {Reasoning}: {Pitfalls} and {Paths} to {Reproducibility}},
	shorttitle = {A {Sober} {Look} at {Progress} in {Language} {Model} {Reasoning}},
	url = {http://arxiv.org/abs/2504.07086},
	doi = {10.48550/arXiv.2504.07086},
	abstract = {Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Hochlehnert, Andreas and Bhatnagar, Hardik and Udandarao, Vishaal and Albanie, Samuel and Prabhu, Ameya and Bethge, Matthias},
	month = apr,
	year = {2025},
	note = {arXiv:2504.07086 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1900/Hochlehnert 等 - 2025 - A Sober Look at Progress in Language Model Reasoning Pitfalls and Paths to Reproducibility.pdf:application/pdf;Snapshot:files/1890/2504.html:text/html},
}

@misc{zhao_echo_2025,
	title = {Echo {Chamber}: {RL} {Post}-training {Amplifies} {Behaviors} {Learned} in {Pretraining}},
	shorttitle = {Echo {Chamber}},
	url = {http://arxiv.org/abs/2504.07912},
	doi = {10.48550/arXiv.2504.07912},
	abstract = {Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhao, Rosie and Meterez, Alexandru and Kakade, Sham and Pehlevan, Cengiz and Jelassi, Samy and Malach, Eran},
	month = apr,
	year = {2025},
	note = {arXiv:2504.07912 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:files/1901/Zhao 等 - 2025 - Echo Chamber RL Post-training Amplifies Behaviors Learned in Pretraining.pdf:application/pdf;Snapshot:files/1891/2504.html:text/html},
}

@misc{lambert_reinforcement_2025,
	title = {Reinforcement {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2504.12501},
	doi = {10.48550/arXiv.2504.12501},
	abstract = {Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some level of quantitative background. The book starts with the origins of RLHF -- both in recent literature and in a convergence of disparate fields of science in economics, philosophy, and optimal control. We then set the stage with definitions, problem formulation, data collection, and other common math used in the literature. The core of the book details every optimization stage in using RLHF, from starting with instruction tuning to training a reward model and finally all of rejection sampling, reinforcement learning, and direct alignment algorithms. The book concludes with advanced topics -- understudied research questions in synthetic data and evaluation -- and open questions for the field.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Lambert, Nathan},
	month = apr,
	year = {2025},
	note = {arXiv:2504.12501 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:files/1902/Lambert - 2025 - Reinforcement Learning from Human Feedback.pdf:application/pdf;Snapshot:files/1892/2504.html:text/html},
}

@misc{xu_not_2025,
	title = {Not {All} {Rollouts} are {Useful}: {Down}-{Sampling} {Rollouts} in {LLM} {Reinforcement} {Learning}},
	shorttitle = {Not {All} {Rollouts} are {Useful}},
	url = {http://arxiv.org/abs/2504.13818},
	doi = {10.48550/arXiv.2504.13818},
	abstract = {Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing reasoning capabilities in large language models, but faces a fundamental asymmetry in computation and memory requirements: inference is embarrassingly parallel with a minimal memory footprint, while policy updates require extensive synchronization and are memory-intensive. To address this asymmetry, we introduce PODS (Policy Optimization with Down-Sampling), a framework that strategically decouples these phases by generating numerous rollouts in parallel but updating only on an informative subset. Within this framework, we develop max-variance down-sampling, a theoretically motivated method that selects rollouts with maximally diverse reward signals. We prove that this approach has an efficient algorithmic solution, and empirically demonstrate that GRPO with PODS using max-variance down-sampling achieves superior performance over standard GRPO on the GSM8K benchmark.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Xu, Yixuan Even and Savani, Yash and Fang, Fei and Kolter, Zico},
	month = apr,
	year = {2025},
	note = {arXiv:2504.13818 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1918/Xu 等 - 2025 - Not All Rollouts are Useful Down-Sampling Rollouts in LLM Reinforcement Learning.pdf:application/pdf;Snapshot:files/1919/2504.html:text/html},
}

@misc{zhang_survey_2025,
	title = {A {Survey} on {Test}-{Time} {Scaling} in {Large} {Language} {Models}: {What}, {How}, {Where}, and {How} {Well}?},
	shorttitle = {A {Survey} on {Test}-{Time} {Scaling} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2503.24235},
	doi = {10.48550/arXiv.2503.24235},
	abstract = {As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q\&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhang, Qiyuan and Lyu, Fuyuan and Sun, Zexu and Wang, Lei and Zhang, Weixu and Hua, Wenyue and Wu, Haolun and Guo, Zhihan and Wang, Yufei and Muennighoff, Niklas and King, Irwin and Liu, Xue and Ma, Chen},
	month = may,
	year = {2025},
	note = {arXiv:2503.24235 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1929/Zhang 等 - 2025 - A Survey on Test-Time Scaling in Large Language Models What, How, Where, and How Well.pdf:application/pdf;Snapshot:files/1920/2503.html:text/html},
}

@misc{ferrag_reasoning_2025,
	title = {Reasoning {Beyond} {Limits}: {Advances} and {Open} {Problems} for {LLMs}},
	shorttitle = {Reasoning {Beyond} {Limits}},
	url = {http://arxiv.org/abs/2503.22732},
	doi = {10.48550/arXiv.2503.22732},
	abstract = {Recent generative reasoning breakthroughs have transformed how large language models (LLMs) tackle complex problems by dynamically retrieving and refining information while generating coherent, multi-step thought processes. Techniques such as inference-time scaling, reinforcement learning, supervised fine-tuning, and distillation have been successfully applied to models like DeepSeek-R1, OpenAI's o1 \& o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in enhanced reasoning capabilities. In this paper, we provide a comprehensive analysis of the top 27 LLM models released between 2023 and 2025 (including models such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and phi-4). Then, we present an extensive overview of training methodologies that spans general training approaches, mixture-of-experts (MoE) and architectural innovations, retrieval-augmented generation (RAG), chain-of-thought and self-improvement techniques, as well as test-time compute scaling, distillation, and reinforcement learning (RL) methods. Finally, we discuss the key challenges in advancing LLM capabilities, including improving multi-step reasoning without human supervision, overcoming limitations in chained tasks, balancing structured prompts with flexibility, and enhancing long-context retrieval and external tool integration.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Ferrag, Mohamed Amine and Tihanyi, Norbert and Debbah, Merouane},
	month = mar,
	year = {2025},
	note = {arXiv:2503.22732 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1930/Ferrag 等 - 2025 - Reasoning Beyond Limits Advances and Open Problems for LLMs.pdf:application/pdf;Snapshot:files/1921/2503.html:text/html},
}

@misc{zhang_right_2025,
	title = {Right {Question} is {Already} {Half} the {Answer}: {Fully} {Unsupervised} {LLM} {Reasoning} {Incentivization}},
	shorttitle = {Right {Question} is {Already} {Half} the {Answer}},
	url = {http://arxiv.org/abs/2504.05812},
	doi = {10.48550/arXiv.2504.05812},
	abstract = {Existing methods to enhance the reasoning capability of large language models predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data. These approaches critically depend on external supervisions--such as labeled reasoning traces, verified golden answers, or pre-trained reward models. In this work, we propose Entropy Minimized Policy Optimization ({\textbackslash}ours), which makes an early attempt at fully unsupervised LLM reasoning incentivization. By continuously minimizing the predictive entropy of LLMs on unlabeled questions in a latent semantic space, {\textbackslash}ours achieves competitive performance compared to supervised counterparts on both mathematical and free-form natural reasoning tasks. Specifically, without any supervised signals, {\textbackslash}ours boosts the accuracy of Qwen2.5-Math-7B Base from 30.7{\textbackslash}\% to 48.1{\textbackslash}\% on mathematical benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1{\textbackslash}\% to 50.1{\textbackslash}\% on MMLU-Pro. Primary experiments and analysis are also provided to interpret the effectiveness of {\textbackslash}ours. Code is available at https://github.com/QingyangZhang/EMPO.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhang, Qingyang and Wu, Haitao and Zhang, Changqing and Zhao, Peilin and Bian, Yatao},
	month = may,
	year = {2025},
	note = {arXiv:2504.05812 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:files/1931/Zhang 等 - 2025 - Right Question is Already Half the Answer Fully Unsupervised LLM Reasoning Incentivization.pdf:application/pdf;Snapshot:files/1922/2504.html:text/html},
}

@misc{liu_noisyrollout_2025,
	title = {{NoisyRollout}: {Reinforcing} {Visual} {Reasoning} with {Data} {Augmentation}},
	shorttitle = {{NoisyRollout}},
	url = {http://arxiv.org/abs/2504.13055},
	doi = {10.48550/arXiv.2504.13055},
	abstract = {Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to better scale test-time compute remains largely underexplored. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective data augmentation method that mixes trajectories from both clean and moderately distorted images during RL training. By injecting targeted diversity in visual perception and the resulting reasoning patterns, NoisyRollout promotes better policy exploration through vision-oriented inductive biases, ultimately leading to more robust reasoning behaviors. We further adopt a noise annealing schedule that gradually reduces distortion strength over training, leveraging noisy signals early on while ensuring training stability in later stages. Crucially, our method is easy-to-adopt--requiring no additional training cost and no modifications to the RL objective. Extensive experiments on \$2\$ distinct training datasets demonstrate that NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models across \$5\$ out-of-domain reasoning and perception benchmarks. Furthermore, we validate the effectiveness of NoisyRollout across model sizes (\$7\$B and \$32\$B) and data scales (from \$1\$K to \$6\$K), highlighting its generalizability and scalability.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Liu, Xiangyan and Ni, Jinjie and Wu, Zijian and Du, Chao and Dou, Longxu and Wang, Haonan and Pang, Tianyu and Shieh, Michael Qizhe},
	month = may,
	year = {2025},
	note = {arXiv:2504.13055 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/1932/Liu 等 - 2025 - NoisyRollout Reinforcing Visual Reasoning with Data Augmentation.pdf:application/pdf;Snapshot:files/1923/2504.html:text/html},
}

@misc{yang_speculative_2025,
	title = {Speculative {Thinking}: {Enhancing} {Small}-{Model} {Reasoning} with {Large} {Model} {Guidance} at {Inference} {Time}},
	shorttitle = {Speculative {Thinking}},
	url = {http://arxiv.org/abs/2504.12329},
	doi = {10.48550/arXiv.2504.12329},
	abstract = {Recent advances leverage post-training to enhance model reasoning performance, which typically requires costly training pipelines and still suffers from inefficient, overly lengthy outputs. We introduce Speculative Thinking, a training-free framework that enables large reasoning models to guide smaller ones during inference at the reasoning level, distinct from speculative decoding, which operates at the token level. Our approach is based on two observations: (1) reasoning-supportive tokens such as "wait" frequently appear after structural delimiters like "{\textbackslash}n{\textbackslash}n", serving as signals for reflection or continuation; and (2) larger models exhibit stronger control over reflective behavior, reducing unnecessary backtracking while improving reasoning quality. By strategically delegating reflective steps to a more capable model, our method significantly boosts the reasoning accuracy of reasoning models while shortening their output. With the assistance of the 32B reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2\% to 89.4\%, marking a substantial improvement of 6.2\%. Simultaneously, the average output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7\% decrease. Moreover, when applied to a non-reasoning model (Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0\% to 81.8\% on the same benchmark, achieving a relative improvement of 7.8\%.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Yang, Wang and Yue, Xiang and Chaudhary, Vipin and Han, Xiaotian},
	month = apr,
	year = {2025},
	note = {arXiv:2504.12329 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1933/Yang 等 - 2025 - Speculative Thinking Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time.pdf:application/pdf;Snapshot:files/1924/2504.html:text/html},
}

@misc{wang_deep_2025,
	title = {Deep {Reasoning} {Translation} via {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2504.10187},
	doi = {10.48550/arXiv.2504.10187},
	abstract = {Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown promising performance in various complex tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation and taking cultural differences into account. This task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning. Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought process. Given the source sentences, the reward model teaches the deep translation model how to think and free-translate them during reinforcement learning. In this way, training DeepTrans does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3\% in literature translation, and outperforms strong deep reasoning baselines as well as baselines that are fine-tuned with synthesized data. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wang, Jiaan and Meng, Fandong and Zhou, Jie},
	month = apr,
	year = {2025},
	note = {arXiv:2504.10187 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1934/Wang 等 - 2025 - Deep Reasoning Translation via Reinforcement Learning.pdf:application/pdf;Snapshot:files/1925/2504.html:text/html},
}

@misc{liao_improved_2025,
	title = {Improved {Visual}-{Spatial} {Reasoning} via {R1}-{Zero}-{Like} {Training}},
	url = {http://arxiv.org/abs/2504.00883},
	doi = {10.48550/arXiv.2504.00883},
	abstract = {Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1\% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Liao, Zhenyi and Xie, Qingsong and Zhang, Yanhao and Kong, Zijian and Lu, Haonan and Yang, Zhenyu and Deng, Zhijie},
	month = apr,
	year = {2025},
	note = {arXiv:2504.00883 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/1935/Liao 等 - 2025 - Improved Visual-Spatial Reasoning via R1-Zero-Like Training.pdf:application/pdf;Snapshot:files/1926/2504.html:text/html},
}

@misc{zhong_streamrl_2025,
	title = {{StreamRL}: {Scalable}, {Heterogeneous}, and {Elastic} {RL} for {LLMs} with {Disaggregated} {Stream} {Generation}},
	shorttitle = {{StreamRL}},
	url = {http://arxiv.org/abs/2504.15930},
	doi = {10.48550/arXiv.2504.15930},
	abstract = {Reinforcement learning (RL) has become the core post-training technique for large language models (LLMs). RL for LLMs involves two stages: generation and training. The LLM first generates samples online, which are then used to derive rewards for training. The conventional view holds that the colocated architecture, where the two stages share resources via temporal multiplexing, outperforms the disaggregated architecture, in which dedicated resources are assigned to each stage. However, in real-world deployments, we observe that the colocated architecture suffers from resource coupling, where the two stages are constrained to use the same resources. This coupling compromises the scalability and cost-efficiency of colocated RL in large-scale training. In contrast, the disaggregated architecture allows for flexible resource allocation, supports heterogeneous training setups, and facilitates cross-datacenter deployment. StreamRL is designed with disaggregation from first principles and fully unlocks its potential by addressing two types of performance bottlenecks in existing disaggregated RL frameworks: pipeline bubbles, caused by stage dependencies, and skewness bubbles, resulting from long-tail output length distributions. To address pipeline bubbles, StreamRL breaks the traditional stage boundary in synchronous RL algorithms through stream generation and achieves full overlapping in asynchronous RL. To address skewness bubbles, StreamRL employs an output-length ranker model to identify long-tail samples and reduces generation time via skewness-aware dispatching and scheduling. Experiments show that StreamRL improves throughput by up to 2.66x compared to existing state-of-the-art systems, and improves cost-effectiveness by up to 1.33x in a heterogeneous, cross-datacenter setting.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhong, Yinmin and Zhang, Zili and Song, Xiaoniu and Hu, Hanpeng and Jin, Chao and Wu, Bingyang and Chen, Nuo and Chen, Yukun and Zhou, Yu and Wan, Changyi and Zhou, Hongyu and Jiang, Yimin and Zhu, Yibo and Jiang, Daxin},
	month = apr,
	year = {2025},
	note = {arXiv:2504.15930 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1936/Zhong 等 - 2025 - StreamRL Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation.pdf:application/pdf;Snapshot:files/1927/2504.html:text/html},
}

@misc{zhang_srpo_2025,
	title = {{SRPO}: {A} {Cross}-{Domain} {Implementation} of {Large}-{Scale} {Reinforcement} {Learning} on {LLM}},
	shorttitle = {{SRPO}},
	url = {http://arxiv.org/abs/2504.14286},
	doi = {10.48550/arXiv.2504.14286},
	abstract = {Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, offering valuable insights into scaling LLM reasoning capabilities across diverse tasks.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhang, Xiaojiang and Wang, Jinghui and Cheng, Zifei and Zhuang, Wenhao and Lin, Zheng and Zhang, Minglei and Wang, Shaojie and Cui, Yinghan and Wang, Chao and Peng, Junyi and Jiang, Shimiao and Kuang, Shiqi and Yin, Shouyu and Wen, Chaohang and Zhang, Haotian and Chen, Bin and Yu, Bing},
	month = apr,
	year = {2025},
	note = {arXiv:2504.14286 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:files/1937/Zhang 等 - 2025 - SRPO A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM.pdf:application/pdf;Snapshot:files/1928/2504.html:text/html},
}

@misc{xing_echoink-r1_2025,
	title = {{EchoInk}-{R1}: {Exploring} {Audio}-{Visual} {Reasoning} in {Multimodal} {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{EchoInk}-{R1}},
	url = {http://arxiv.org/abs/2505.04623},
	doi = {10.48550/arXiv.2505.04623},
	abstract = {Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77\% accuracy on the validation set, outperforming the base model, which scores 80.53\%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Xing, Zhenghao and Hu, Xiaowei and Fu, Chi-Wing and Wang, Wenhai and Dai, Jifeng and Heng, Pheng-Ann},
	month = may,
	year = {2025},
	note = {arXiv:2505.04623 [eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:files/1964/Xing 等 - 2025 - EchoInk-R1 Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning.pdf:application/pdf;Snapshot:files/1954/2505.html:text/html},
}

@misc{yao_r1-sharevl_2025,
	title = {R1-{ShareVL}: {Incentivizing} {Reasoning} {Capability} of {Multimodal} {Large} {Language} {Models} via {Share}-{GRPO}},
	shorttitle = {R1-{ShareVL}},
	url = {http://arxiv.org/abs/2505.16673},
	doi = {10.48550/arXiv.2505.16673},
	abstract = {In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Yao, Huanjin and Yin, Qixiang and Zhang, Jingyi and Yang, Min and Wang, Yibo and Wu, Wenhao and Su, Fei and Shen, Li and Qiu, Minghui and Tao, Dacheng and Huang, Jiaxing},
	month = may,
	year = {2025},
	note = {arXiv:2505.16673 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/1965/Yao 等 - 2025 - R1-ShareVL Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO.pdf:application/pdf;Snapshot:files/1955/2505.html:text/html},
}

@misc{jin_empirical_2025,
	title = {An {Empirical} {Study} on {Reinforcement} {Learning} for {Reasoning}-{Search} {Interleaved} {LLM} {Agents}},
	url = {http://arxiv.org/abs/2505.15117},
	doi = {10.48550/arXiv.2505.15117},
	abstract = {Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Jin, Bowen and Yoon, Jinsung and Kargupta, Priyanka and Arik, Sercan O. and Han, Jiawei},
	month = may,
	year = {2025},
	note = {arXiv:2505.15117 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:files/1966/Jin 等 - 2025 - An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents.pdf:application/pdf;Snapshot:files/1956/2505.html:text/html},
}

@misc{fan_sophiavl-r1_2025,
	title = {{SophiaVL}-{R1}: {Reinforcing} {MLLMs} {Reasoning} with {Thinking} {Reward}},
	shorttitle = {{SophiaVL}-{R1}},
	url = {http://arxiv.org/abs/2505.17018},
	doi = {10.48550/arXiv.2505.17018},
	abstract = {Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at https://github.com/kxfan2002/SophiaVL-R1.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Fan, Kaixuan and Feng, Kaituo and Lyu, Haoming and Zhou, Dongzhan and Yue, Xiangyu},
	month = may,
	year = {2025},
	note = {arXiv:2505.17018 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/1967/Fan 等 - 2025 - SophiaVL-R1 Reinforcing MLLMs Reasoning with Thinking Reward.pdf:application/pdf;Snapshot:files/1957/2505.html:text/html},
}

@misc{xu_j4r_2025,
	title = {{J4R}: {Learning} to {Judge} with {Equivalent} {Initial} {State} {Group} {Relative} {Policy} {Optimization}},
	shorttitle = {{J4R}},
	url = {http://arxiv.org/abs/2505.13346},
	doi = {10.48550/arXiv.2505.13346},
	abstract = {To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7\% and 9\%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Xu, Austin and Zhou, Yilun and Nguyen, Xuan-Phi and Xiong, Caiming and Joty, Shafiq},
	month = may,
	year = {2025},
	note = {arXiv:2505.13346 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1968/Xu 等 - 2025 - J4R Learning to Judge with Equivalent Initial State Group Relative Policy Optimization.pdf:application/pdf;Snapshot:files/1958/2505.html:text/html},
}

@misc{zhou_reinforced_2025,
	title = {Reinforced {MLLM}: {A} {Survey} on {RL}-{Based} {Reasoning} in {Multimodal} {Large} {Language} {Models}},
	shorttitle = {Reinforced {MLLM}},
	url = {http://arxiv.org/abs/2504.21277},
	doi = {10.48550/arXiv.2504.21277},
	abstract = {The application of reinforcement learning (RL) to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) constitutes a rapidly advancing research area. While MLLMs extend Large Language Models (LLMs) to handle diverse modalities such as vision, audio, and video, enabling robust reasoning across multimodal inputs remains challenging. This paper provides a systematic review of recent advances in RL-based reasoning for MLLMs, covering key algorithmic designs, reward mechanism innovations, and practical applications. We highlight two main RL paradigms, value-model-free and value-model-based methods, and analyze how RL enhances reasoning abilities by optimizing reasoning trajectories and aligning multimodal information. Additionally, we provide an extensive overview of benchmark datasets, evaluation protocols, and current limitations, and propose future research directions to address challenges such as sparse rewards, inefficient cross-modal reasoning, and real-world deployment constraints. Our goal is to provide a comprehensive and structured guide to RL-based multimodal reasoning.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhou, Guanghao and Qiu, Panjia and Chen, Cen and Wang, Jie and Yang, Zheming and Xu, Jian and Qiu, Minghui},
	month = may,
	year = {2025},
	note = {arXiv:2504.21277 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1969/Zhou 等 - 2025 - Reinforced MLLM A Survey on RL-Based Reasoning in Multimodal Large Language Models.pdf:application/pdf;Snapshot:files/1959/2504.html:text/html},
}

@misc{xu_mind_2025,
	title = {Mind the {Gap}: {Bridging} {Thought} {Leap} for {Improved} {Chain}-of-{Thought} {Tuning}},
	shorttitle = {Mind the {Gap}},
	url = {http://arxiv.org/abs/2505.14684},
	doi = {10.48550/arXiv.2505.14684},
	abstract = {Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87\% on NuminaMath. Our approach effectively enhances distilled data (+3.02\%) and provides better starting points for reinforcement learning (+3.1\%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Xu, Haolei and Yan, Yuchen and Shen, Yongliang and Zhang, Wenqi and Hou, Guiyang and Jiang, Shengpei and Song, Kaitao and Lu, Weiming and Xiao, Jun and Zhuang, Yueting},
	month = may,
	year = {2025},
	note = {arXiv:2505.14684 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1970/Xu 等 - 2025 - Mind the Gap Bridging Thought Leap for Improved Chain-of-Thought Tuning.pdf:application/pdf;Snapshot:files/1960/2505.html:text/html},
}

@misc{lu_arpoend--end_2025,
	title = {{ARPO}:{End}-to-{End} {Policy} {Optimization} for {GUI} {Agents} with {Experience} {Replay}},
	shorttitle = {{ARPO}},
	url = {http://arxiv.org/abs/2505.16282},
	doi = {10.48550/arXiv.2505.16282},
	abstract = {Training large language models (LLMs) as interactive agents for controlling graphical user interfaces (GUIs) presents a unique challenge to optimize long-horizon action sequences with multimodal feedback from complex environments. While recent works have advanced multi-turn reinforcement learning (RL) for reasoning and tool-using capabilities in LLMs, their application to GUI-based agents remains relatively underexplored due to the difficulty of sparse rewards, delayed feedback, and high rollout costs. In this paper, we investigate end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an end-to-end RL approach that augments Group Relative Policy Optimization (GRPO) with a replay buffer to reuse the successful experience across training iterations. To further stabilize the training process, we propose a task selection strategy that filters tasks based on baseline agent performance, allowing the agent to focus on learning from informative interactions. Additionally, we compare ARPO with offline preference optimization approaches, highlighting the advantages of policy-based methods in GUI environments. Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive results, establishing a new performance baseline for LLM-based GUI agents trained via reinforcement learning. Our findings underscore the effectiveness of reinforcement learning for training multi-turn, vision-language GUI agents capable of managing complex real-world UI interactions. Codes and models:https://github.com/dvlab-research/ARPO.git.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Lu, Fanbin and Zhong, Zhisheng and Liu, Shu and Fu, Chi-Wing and Jia, Jiaya},
	month = may,
	year = {2025},
	note = {arXiv:2505.16282 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/1971/Lu 等 - 2025 - ARPOEnd-to-End Policy Optimization for GUI Agents with Experience Replay.pdf:application/pdf;Snapshot:files/1961/2505.html:text/html},
}

@misc{tong_delving_2025,
	title = {Delving into {RL} for {Image} {Generation} with {CoT}: {A} {Study} on {DPO} vs. {GRPO}},
	shorttitle = {Delving into {RL} for {Image} {Generation} with {CoT}},
	url = {http://arxiv.org/abs/2505.17017},
	doi = {10.48550/arXiv.2505.17017},
	abstract = {Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Tong, Chengzhuo and Guo, Ziyu and Zhang, Renrui and Shan, Wenyu and Wei, Xinyu and Xing, Zhenghao and Li, Hongsheng and Heng, Pheng-Ann},
	month = may,
	year = {2025},
	note = {arXiv:2505.17017 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1972/Tong 等 - 2025 - Delving into RL for Image Generation with CoT A Study on DPO vs. GRPO.pdf:application/pdf;Snapshot:files/1962/2505.html:text/html},
}

@misc{fu_scaling_2025,
	title = {Scaling {Reasoning}, {Losing} {Control}: {Evaluating} {Instruction} {Following} in {Large} {Reasoning} {Models}},
	shorttitle = {Scaling {Reasoning}, {Losing} {Control}},
	url = {http://arxiv.org/abs/2505.14810},
	doi = {10.48550/arXiv.2505.14810},
	abstract = {Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Fu, Tingchen and Gu, Jiawei and Li, Yafu and Qu, Xiaoye and Cheng, Yu},
	month = may,
	year = {2025},
	note = {arXiv:2505.14810 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/1973/Fu 等 - 2025 - Scaling Reasoning, Losing Control Evaluating Instruction Following in Large Reasoning Models.pdf:application/pdf;Snapshot:files/1963/2505.html:text/html},
}

@misc{hu_lmgame-bench_2025,
	title = {lmgame-{Bench}: {How} {Good} are {LLMs} at {Playing} {Games}?},
	shorttitle = {lmgame-{Bench}},
	url = {http://arxiv.org/abs/2505.15146},
	doi = {10.48550/arXiv.2505.15146},
	abstract = {Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Hu, Lanxiang and Huo, Mingjia and Zhang, Yuxuan and Yu, Haoyang and Xing, Eric P. and Stoica, Ion and Rosing, Tajana and Jin, Haojian and Zhang, Hao},
	month = may,
	year = {2025},
	note = {arXiv:2505.15146 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1998/Hu 等 - 2025 - lmgame-Bench How Good are LLMs at Playing Games.pdf:application/pdf;Snapshot:files/1988/2505.html:text/html},
}

@misc{li_disco_2025,
	title = {{DisCO}: {Reinforcing} {Large} {Reasoning} {Models} with {Discriminative} {Constrained} {Optimization}},
	shorttitle = {{DisCO}},
	url = {http://arxiv.org/abs/2505.12366},
	doi = {10.48550/arXiv.2505.12366},
	abstract = {The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint, ensuring stable training. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7{\textbackslash}\% over GRPO and 6{\textbackslash}\% over DAPO across six benchmark tasks for an 1.5B model.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Li, Gang and Lin, Ming and Galanti, Tomer and Tu, Zhengzhong and Yang, Tianbao},
	month = may,
	year = {2025},
	note = {arXiv:2505.12366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1999/Li 等 - 2025 - DisCO Reinforcing Large Reasoning Models with Discriminative Constrained Optimization.pdf:application/pdf;Snapshot:files/1989/2505.html:text/html},
}

@misc{liu_uft_2025,
	title = {{UFT}: {Unifying} {Supervised} and {Reinforcement} {Fine}-{Tuning}},
	shorttitle = {{UFT}},
	url = {http://arxiv.org/abs/2505.16984},
	doi = {10.48550/arXiv.2505.16984},
	abstract = {Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Liu, Mingyang and Farina, Gabriele and Ozdaglar, Asuman},
	month = may,
	year = {2025},
	note = {arXiv:2505.16984 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2000/Liu 等 - 2025 - UFT Unifying Supervised and Reinforcement Fine-Tuning.pdf:application/pdf;Snapshot:files/1990/2505.html:text/html},
}

@misc{sun_ktae_2025,
	title = {{KTAE}: {A} {Model}-{Free} {Algorithm} to {Key}-{Tokens} {Advantage} {Estimation} in {Mathematical} {Reasoning}},
	shorttitle = {{KTAE}},
	url = {http://arxiv.org/abs/2505.16826},
	doi = {10.48550/arXiv.2505.16826},
	abstract = {Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models, even without supervised fine-tuning. However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions and hindering effective learning. To address this limitation, we propose Key-token Advantage Estimation (KTAE) - a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Sun, Wei and Yang, Wen and Jian, Pu and Du, Qianlong and Cui, Fuwei and Ren, Shuo and Zhang, Jiajun},
	month = may,
	year = {2025},
	note = {arXiv:2505.16826 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/2001/Sun 等 - 2025 - KTAE A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning.pdf:application/pdf;Snapshot:files/1991/2505.html:text/html},
}

@misc{mei_o2-searcher_2025,
	title = {O\${\textasciicircum}2\$-{Searcher}: {A} {Searching}-based {Agent} {Model} for {Open}-{Domain} {Open}-{Ended} {Question} {Answering}},
	shorttitle = {O\${\textasciicircum}2\$-{Searcher}},
	url = {http://arxiv.org/abs/2505.16582},
	doi = {10.48550/arXiv.2505.16582},
	abstract = {Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O\${\textasciicircum}2\$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O\${\textasciicircum}2\$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from model's sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O\${\textasciicircum}2\$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O\${\textasciicircum}2\$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O\${\textasciicircum}2\$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Mei, Jianbiao and Hu, Tao and Fu, Daocheng and Wen, Licheng and Yang, Xuemeng and Wu, Rong and Cai, Pinlong and Cai, Xinyu and Gao, Xing and Yang, Yu and Xie, Chengjun and Shi, Botian and Liu, Yong and Qiao, Yu},
	month = may,
	year = {2025},
	note = {arXiv:2505.16582 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/2002/Mei 等 - 2025 - O\$^2\$-Searcher A Searching-based Agent Model for Open-Domain Open-Ended Question Answering.pdf:application/pdf;Snapshot:files/1992/2505.html:text/html},
}

@misc{wang_think_2025,
	title = {Think {Deep}, {Think} {Fast}: {Investigating} {Efficiency} of {Verifier}-free {Inference}-time-scaling {Methods}},
	shorttitle = {Think {Deep}, {Think} {Fast}},
	url = {http://arxiv.org/abs/2504.14047},
	doi = {10.48550/arXiv.2504.14047},
	abstract = {There is intense interest in investigating how inference time compute (ITC) (e.g. repeated sampling, refinements, etc) can improve large language model (LLM) capabilities. At the same time, recent breakthroughs in reasoning models, such as Deepseek-R1, unlock the opportunity for reinforcement learning to improve LLM reasoning skills. An in-depth understanding of how ITC interacts with reasoning across different models could provide important guidance on how to further advance the LLM frontier. This work conducts a comprehensive analysis of inference-time scaling methods for both reasoning and non-reasoning models on challenging reasoning tasks. Specifically, we focus our research on verifier-free inference time-scaling methods due to its generalizability without needing a reward model. We construct the Pareto frontier of quality and efficiency. We find that non-reasoning models, even with an extremely high inference budget, still fall substantially behind reasoning models. For reasoning models, majority voting proves to be a robust inference strategy, generally competitive or outperforming other more sophisticated ITC methods like best-of-N and sequential revisions, while the additional inference compute offers minimal improvements. We further perform in-depth analyses of the association of key response features (length and linguistic markers) with response quality, with which we can improve the existing ITC methods. We find that correct responses from reasoning models are typically shorter and have fewer hedging and thinking markers (but more discourse markers) than the incorrect responses.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wang, Junlin and Zhu, Shang and Saad-Falcon, Jon and Athiwaratkun, Ben and Wu, Qingyang and Wang, Jue and Song, Shuaiwen Leon and Zhang, Ce and Dhingra, Bhuwan and Zou, James},
	month = apr,
	year = {2025},
	note = {arXiv:2504.14047 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/2003/Wang 等 - 2025 - Think Deep, Think Fast Investigating Efficiency of Verifier-free Inference-time-scaling Methods.pdf:application/pdf;Snapshot:files/1993/2504.html:text/html},
}

@misc{ning_not_2025,
	title = {Not {All} {Thoughts} are {Generated} {Equal}: {Efficient} {LLM} {Reasoning} via {Multi}-{Turn} {Reinforcement} {Learning}},
	shorttitle = {Not {All} {Thoughts} are {Generated} {Equal}},
	url = {http://arxiv.org/abs/2505.11827},
	doi = {10.48550/arXiv.2505.11827},
	abstract = {Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long\${\textbackslash}otimes\$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80\% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks. Our data and code are available at https://github.com/usail-hkust/LongShort.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Ning, Yansong and Li, Wei and Fang, Jun and Tan, Naiqiang and Liu, Hao},
	month = may,
	year = {2025},
	note = {arXiv:2505.11827 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/2004/Ning 等 - 2025 - Not All Thoughts are Generated Equal Efficient LLM Reasoning via Multi-Turn Reinforcement Learning.pdf:application/pdf;Snapshot:files/1994/2505.html:text/html},
}

@misc{feng_group--group_2025,
	title = {Group-in-{Group} {Policy} {Optimization} for {LLM} {Agent} {Training}},
	url = {http://arxiv.org/abs/2505.10978},
	doi = {10.48550/arXiv.2505.10978},
	abstract = {Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to long-horizon LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals and achieves performance gains of {\textgreater} 12{\textbackslash}\% on ALFWorld and {\textgreater} 9{\textbackslash}\% on WebShop over the GRPO baseline: all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Feng, Lang and Xue, Zhenghai and Liu, Tingcong and An, Bo},
	month = may,
	year = {2025},
	note = {arXiv:2505.10978 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2005/Feng 等 - 2025 - Group-in-Group Policy Optimization for LLM Agent Training.pdf:application/pdf;Snapshot:files/1995/2505.html:text/html},
}

@misc{zhu_using_2025,
	title = {Using {Reinforcement} {Learning} to {Train} {Large} {Language} {Models} to {Explain} {Human} {Decisions}},
	url = {http://arxiv.org/abs/2505.11614},
	doi = {10.48550/arXiv.2505.11614},
	abstract = {A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhu, Jian-Qiao and Xie, Hanbo and Arumugam, Dilip and Wilson, Robert C. and Griffiths, Thomas L.},
	month = may,
	year = {2025},
	note = {arXiv:2505.11614 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/2006/Zhu 等 - 2025 - Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions.pdf:application/pdf;Snapshot:files/1996/2505.html:text/html},
}

@misc{chen_solver-informed_2025,
	title = {Solver-{Informed} {RL}: {Grounding} {Large} {Language} {Models} for {Authentic} {Optimization} {Modeling}},
	shorttitle = {Solver-{Informed} {RL}},
	url = {http://arxiv.org/abs/2505.11792},
	doi = {10.48550/arXiv.2505.11792},
	abstract = {Optimization modeling is fundamental to decision-making across diverse domains.Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models due to hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL).This novel framework leverages external optimization solvers as verifiable reward mechanisms to significantly improve the authenticity of LLMs for optimization modeling.Acting as precise verifiers, these solvers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality that directly inform the RL process. This automated verification process, powered by classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Chen, Yitian and Xia, Jingfan and Shao, Siyu and Ge, Dongdong and Ye, Yinyu},
	month = may,
	year = {2025},
	note = {arXiv:2505.11792 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/2007/Chen 等 - 2025 - Solver-Informed RL Grounding Large Language Models for Authentic Optimization Modeling.pdf:application/pdf;Snapshot:files/1997/2505.html:text/html},
}

@misc{luo_pharmolixfm_2025,
	title = {{PharMolixFM}: {All}-{Atom} {Foundation} {Models} for {Molecular} {Modeling} and {Generation}},
	shorttitle = {{PharMolixFM}},
	url = {http://arxiv.org/abs/2503.21788},
	doi = {10.48550/arXiv.2503.21788},
	abstract = {Structural biology relies on accurate three-dimensional biomolecular structures to advance our understanding of biological functions, disease mechanisms, and therapeutics. While recent advances in deep learning have enabled the development of all-atom foundation models for molecular modeling and generation, existing approaches face challenges in generalization due to the multi-modal nature of atomic data and the lack of comprehensive analysis of training and sampling strategies. To address these limitations, we propose PharMolixFM, a unified framework for constructing all-atom foundation models based on multi-modal generative techniques. Our framework includes three variants using state-of-the-art multi-modal generative models. By formulating molecular tasks as a generalized denoising process with task-specific priors, PharMolixFM achieves robust performance across various structural biology applications. Experimental results demonstrate that PharMolixFM-Diff achieves competitive prediction accuracy in protein-small-molecule docking (83.9\% vs. 90.2\% RMSD {\textless} 2\{{\textbackslash}AA\}, given pocket) with significantly improved inference speed. Moreover, we explore the empirical inference scaling law by introducing more sampling repeats or steps. Our code and model are available at https://github.com/PharMolix/OpenBioMed.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Luo, Yizhen and Wang, Jiashuo and Fan, Siqi and Nie, Zaiqing},
	month = apr,
	year = {2025},
	note = {arXiv:2503.21788 [q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {Preprint PDF:files/2022/Luo 等 - 2025 - PharMolixFM All-Atom Foundation Models for Molecular Modeling and Generation.pdf:application/pdf;Snapshot:files/2027/2503.html:text/html},
}

@misc{stechly_beyond_2025,
	title = {Beyond {Semantics}: {The} {Unreasonable} {Effectiveness} of {Reasonless} {Intermediate} {Tokens}},
	shorttitle = {Beyond {Semantics}},
	url = {http://arxiv.org/abs/2505.13775},
	doi = {10.48550/arXiv.2505.13775},
	abstract = {Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), and especially of the process of training on CoTs sampled from base LLMs in order to help find new reasoning patterns. In this paper, we critically examine that interpretation by investigating how the semantics of intermediate tokens-often anthropomorphized as "thoughts" or reasoning traces and which are claimed to display behaviors like backtracking, self-verification etc.-actually influence model performance. We train transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver (in our case, A* search). By constructing a formal interpreter of the semantics of our problems and intended algorithm, we systematically evaluate not only solution accuracy but also the correctness of intermediate traces, thus allowing us to evaluate whether the latter causally influences the former. We notice that, despite significant improvements on the solution-only baseline, models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. To further show that trace accuracy is only loosely connected to solution accuracy, we then train models on noisy, corrupted traces which have no relation to the specific problem each is paired with, and find that not only does performance remain largely consistent with models trained on correct data, but in some cases can improve upon it and generalize more robustly on out-of-distribution tasks. These results challenge the assumption that intermediate tokens or "Chains of Thought" induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly correct forms) as evidence of human-like or algorithmic behaviors in language models.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Stechly, Kaya and Valmeekam, Karthik and Gundawar, Atharva and Palod, Vardhan and Kambhampati, Subbarao},
	month = may,
	year = {2025},
	note = {arXiv:2505.13775 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2023/Stechly 等 - 2025 - Beyond Semantics The Unreasonable Effectiveness of Reasonless Intermediate Tokens.pdf:application/pdf;Snapshot:files/2028/2505.html:text/html},
}

@misc{yao_optimizing_2025,
	title = {Optimizing {Chain}-of-{Thought} {Reasoners} via {Gradient} {Variance} {Minimization} in {Rejection} {Sampling} and {RL}},
	url = {http://arxiv.org/abs/2505.02391},
	doi = {10.48550/arXiv.2505.02391},
	abstract = {Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Yao, Jiarui and Hao, Yifan and Zhang, Hanning and Dong, Hanze and Xiong, Wei and Jiang, Nan and Zhang, Tong},
	month = may,
	year = {2025},
	note = {arXiv:2505.02391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2024/Yao 等 - 2025 - Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and R.pdf:application/pdf;Snapshot:files/2029/2505.html:text/html},
}

@misc{guo_observe-r1_2025,
	title = {Observe-{R1}: {Unlocking} {Reasoning} {Abilities} of {MLLMs} with {Dynamic} {Progressive} {Reinforcement} {Learning}},
	shorttitle = {Observe-{R1}},
	url = {http://arxiv.org/abs/2505.12432},
	doi = {10.48550/arXiv.2505.12432},
	abstract = {Reinforcement Learning (RL) has shown promise in improving the reasoning abilities of Large Language Models (LLMs). However, the specific challenges of adapting RL to multimodal data and formats remain relatively unexplored. In this work, we present Observe-R1, a novel framework aimed at enhancing the reasoning capabilities of multimodal large language models (MLLMs). We draw inspirations from human learning progression--from simple to complex and easy to difficult, and propose a gradual learning paradigm for MLLMs. To this end, we construct the NeuraLadder dataset, which is organized and sampled according to the difficulty and complexity of data samples for RL training. To tackle multimodal tasks, we introduce a multimodal format constraint that encourages careful observation of images, resulting in enhanced visual abilities and clearer and more structured responses. Additionally, we implement a bonus reward system that favors concise, correct answers within a length constraint, alongside a dynamic weighting mechanism that prioritizes uncertain and medium-difficulty problems, ensuring that more informative samples have a greater impact on training. Our experiments with the Qwen2.5-VL-3B and Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that Observe-R1 outperforms a series of larger reasoning models on both reasoning and general benchmarks, achieving superior clarity and conciseness in reasoning chains. Ablation studies validate the effectiveness of our strategies, highlighting the robustness and generalization of our approach. The dataset and code will be released at https://github.com/zrguo/Observe-R1.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Guo, Zirun and Hong, Minjie and Jin, Tao},
	month = may,
	year = {2025},
	note = {arXiv:2505.12432 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2025/Guo 等 - 2025 - Observe-R1 Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning.pdf:application/pdf;Snapshot:files/2030/2505.html:text/html},
}

@misc{chen_acereason-nemotron_2025,
	title = {{AceReason}-{Nemotron}: {Advancing} {Math} and {Code} {Reasoning} through {Reinforcement} {Learning}},
	shorttitle = {{AceReason}-{Nemotron}},
	url = {http://arxiv.org/abs/2505.16400},
	doi = {10.48550/arXiv.2505.16400},
	abstract = {Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6\% / +17.2\% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8\% / +5.8\% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Chen, Yang and Yang, Zhuolin and Liu, Zihan and Lee, Chankyu and Xu, Peng and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
	month = may,
	year = {2025},
	note = {arXiv:2505.16400 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2026/Chen 等 - 2025 - AceReason-Nemotron Advancing Math and Code Reasoning through Reinforcement Learning.pdf:application/pdf;Snapshot:files/2031/2505.html:text/html},
}

@misc{wang_think_2025-1,
	title = {Think or {Not}? {Selective} {Reasoning} via {Reinforcement} {Learning} for {Vision}-{Language} {Models}},
	shorttitle = {Think or {Not}?},
	url = {http://arxiv.org/abs/2505.16854},
	doi = {10.48550/arXiv.2505.16854},
	abstract = {Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90\% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wang, Jiaqi and Lin, Kevin Qinghong and Cheng, James and Shou, Mike Zheng},
	month = may,
	year = {2025},
	note = {arXiv:2505.16854 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/2037/Wang 等 - 2025 - Think or Not Selective Reasoning via Reinforcement Learning for Vision-Language Models.pdf:application/pdf;Snapshot:files/2032/2505.html:text/html},
}

@misc{chen_spectral_2025,
	title = {Spectral {Policy} {Optimization}: {Coloring} your {Incorrect} {Reasoning} in {GRPO}},
	shorttitle = {Spectral {Policy} {Optimization}},
	url = {http://arxiv.org/abs/2505.11595},
	doi = {10.48550/arXiv.2505.11595},
	abstract = {Reinforcement learning (RL) has demonstrated significant success in enhancing reasoning capabilities in large language models (LLMs). One of the most widely used RL methods is Group Relative Policy Optimization (GRPO){\textasciitilde}{\textbackslash}cite\{Shao-2024-Deepseekmath\}, known for its memory efficiency and success in training DeepSeek-R1{\textasciitilde}{\textbackslash}cite\{Guo-2025-Deepseek\}. However, GRPO stalls when all sampled responses in a group are incorrect -- referred to as an {\textbackslash}emph\{all-negative-sample\} group -- as it fails to update the policy, hindering learning progress. The contributions of this paper are two-fold. First, we propose a simple yet effective framework that introduces response diversity within all-negative-sample groups in GRPO using AI feedback. We also provide a theoretical analysis, via a stylized model, showing how this diversification improves learning dynamics. Second, we empirically validate our approach, showing the improved performance across various model sizes (7B, 14B, 32B) in both offline and online learning settings with 10 benchmarks, including base and distilled variants. Our findings highlight that learning from all-negative-sample groups is not only feasible but beneficial, advancing recent insights from {\textbackslash}citet\{Xiong-2025-Minimalist\}.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Chen, Peter and Li, Xiaopeng and Li, Ziniu and Chen, Xi and Lin, Tianyi},
	month = may,
	year = {2025},
	note = {arXiv:2505.11595 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2038/Chen 等 - 2025 - Spectral Policy Optimization Coloring your Incorrect Reasoning in GRPO.pdf:application/pdf;Snapshot:files/2033/2505.html:text/html},
}

@misc{wu_thought-augmented_2025,
	title = {Thought-{Augmented} {Policy} {Optimization}: {Bridging} {External} {Guidance} and {Internal} {Capabilities}},
	shorttitle = {Thought-{Augmented} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/2505.15692},
	doi = {10.48550/arXiv.2505.15692},
	abstract = {Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance ("thought patterns"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99\% on AIME, 41\% on AMC, and 17\% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wu, Jinyang and Liao, Chonghua and Feng, Mingkuan and Zhang, Shuai and Wen, Zhengqi and Shao, Pengpeng and Xu, Huazhe and Tao, Jianhua},
	month = may,
	year = {2025},
	note = {arXiv:2505.15692 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2039/Wu 等 - 2025 - Thought-Augmented Policy Optimization Bridging External Guidance and Internal Capabilities.pdf:application/pdf;Snapshot:files/2034/2505.html:text/html},
}

@misc{chen_unlocking_2025,
	title = {Unlocking the {Potential} of {Difficulty} {Prior} in {RL}-based {Multimodal} {Reasoning}},
	url = {http://arxiv.org/abs/2505.13261},
	doi = {10.48550/arXiv.2505.13261},
	abstract = {In this work, we investigate how explicitly modeling problem's difficulty prior information shapes the effectiveness of reinforcement learning based fine-tuning for multimodal reasoning. Our exploration mainly comprises of following three perspective: First, through offline data curation, we analyze the U-shaped difficulty distribution of two given datasets using the base model by multi-round sampling, and then filter out prompts that are either too simple or extremely difficult to provide meaningful gradients and perform subsequent two-stage training. Second, we implement an online advantage differentiation, computing group-wise empirical accuracy as a difficulty proxy to adaptively reweight advantages estimation, providing stronger learning signals for more challenging problems. Finally, we introduce difficulty hints as explicit prompts for more complex samples in the second training stage, encouraging the model to calibrate its reasoning depth and perform reflective validation checks. Our comprehensive approach demonstrates significant performances across various multi-modal mathematical reasoning benchmarks with only 2K+0.6K two-stage training data.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Chen, Mingrui and Liu, Haogeng and Liang, Hao and Huang, Huaibo and Zhang, Wentao and He, Ran},
	month = may,
	year = {2025},
	note = {arXiv:2505.13261 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/2040/Chen 等 - 2025 - Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning.pdf:application/pdf;Snapshot:files/2035/2505.html:text/html},
}

@misc{tu_learning_2025,
	title = {Learning {When} to {Think}: {Shaping} {Adaptive} {Reasoning} in {R1}-{Style} {Models} via {Multi}-{Stage} {RL}},
	shorttitle = {Learning {When} to {Think}},
	url = {http://arxiv.org/abs/2505.10832},
	doi = {10.48550/arXiv.2505.10832},
	abstract = {Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis ("...") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Tu, Songjun and Lin, Jiahao and Zhang, Qichao and Tian, Xiangyu and Li, Linjing and Lan, Xiangyuan and Zhao, Dongbin},
	month = may,
	year = {2025},
	note = {arXiv:2505.10832 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/2041/Tu 等 - 2025 - Learning When to Think Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL.pdf:application/pdf;Snapshot:files/2036/2505.html:text/html},
}

@misc{koh_adastar_2025,
	title = {{AdaSTaR}: {Adaptive} {Data} {Sampling} for {Training} {Self}-{Taught} {Reasoners}},
	shorttitle = {{AdaSTaR}},
	url = {http://arxiv.org/abs/2505.16322},
	doi = {10.48550/arXiv.2505.16322},
	abstract = {Self-Taught Reasoners (STaR), synonymously known as Rejection sampling Fine-Tuning (RFT), is an integral part of the training pipeline of self-improving reasoning Language Models (LMs). The self-improving mechanism often employs random observation (data) sampling. However, this results in trained observation imbalance; inefficiently over-training on solved examples while under-training on challenging ones. In response, we introduce Adaptive STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting balanced training across observations, and (2) Adaptive Sampling for Curriculum: dynamically adjusting data difficulty to match the model's evolving strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all instances (6/6) and reduces training FLOPs by an average of 58.6\% against an extensive list of baselines. These improvements in performance and efficiency generalize to different pre-trained LMs and larger models, paving the way for more efficient and effective self-improving LMs.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Koh, Woosung and Oh, Wonbeen and Jang, Jaein and Lee, MinHyung and Kim, Hyeongjin and Kim, Ah Yeon and Kim, Joonkee and Lee, Junghyun and Kim, Taehyeon and Yun, Se-Young},
	month = may,
	year = {2025},
	note = {arXiv:2505.16322 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2060/Koh 等 - 2025 - AdaSTaR Adaptive Data Sampling for Training Self-Taught Reasoners.pdf:application/pdf;Snapshot:files/2057/2505.html:text/html},
}

@misc{zhao_ufo-rl_2025,
	title = {{UFO}-{RL}: {Uncertainty}-{Focused} {Optimization} for {Efficient} {Reinforcement} {Learning} {Data} {Selection}},
	shorttitle = {{UFO}-{RL}},
	url = {http://arxiv.org/abs/2505.12457},
	doi = {10.48550/arXiv.2505.12457},
	abstract = {Scaling RL for LLMs is computationally expensive, largely due to multi-sampling for policy optimization and evaluation, making efficient data selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory, we hypothesize LLMs learn best from data within their potential comprehension zone. Addressing the limitation of conventional, computationally intensive multi-sampling methods for data assessment, we introduce UFO-RL. This novel framework uses a computationally efficient single-pass uncertainty estimation to identify informative data instances, achieving up to 185x faster data evaluation. UFO-RL leverages this metric to select data within the estimated ZPD for training. Experiments show that training with just 10\% of data selected by UFO-RL yields performance comparable to or surpassing full-data training, reducing overall training time by up to 16x while enhancing stability and generalization. UFO-RL offers a practical and highly efficient strategy for scaling RL fine-tuning of LLMs by focusing learning on valuable data.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhao, Yang and Xiong, Kai and Ding, Xiao and Du, Li and YangouOuyang and Sun, Zhouhao and Guan, Jiannan and Zhang, Wenbin and Liu, Bin and Hu, Dong and Qin, Bing and Liu, Ting},
	month = may,
	year = {2025},
	note = {arXiv:2505.12457 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2068/Zhao 等 - 2025 - UFO-RL Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection.pdf:application/pdf;Snapshot:files/2058/2505.html:text/html},
}

@misc{dai_s-grpo_2025,
	title = {S-{GRPO}: {Early} {Exit} via {Reinforcement} {Learning} in {Reasoning} {Models}},
	shorttitle = {S-{GRPO}},
	url = {http://arxiv.org/abs/2505.07686},
	doi = {10.48550/arXiv.2505.07686},
	abstract = {As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking issue arises from the inherent limitations of conventional outcome-reward reinforcement learning, which systematically overlooks the regulation of intermediate reasoning processes. This paper introduces Serial-Group Decaying-Reward Policy Optimization (S-GRPO), a novel reinforcement learning paradigm that enables models to implicitly evaluate the sufficiency of intermediate reasoning steps, thereby facilitating early exit in CoT generation. Unlike GRPO, which samples multiple possible reasoning paths in parallel (parallel group), S-GRPO only samples one reasoning path and serially selects multiple temporal positions from the path to exit thinking and directly generate answers (serial group). For correct answers within a serial group, rewards gradually decrease based on the exit positions along the reasoning path from front to back. This design encourages the model to produce more accurate and concise thoughts, while also incentivizing early thinking termination when appropriate. Empirical evaluations demonstrate that S-GRPO is compatible with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill. Across diverse benchmarks such as GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond, S-GRPO achieves a substantial reduction in sequence length (35.4\% - 61.1\%) while simultaneously improving accuracy (absolute 0.72\% - 6.08\%).},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Dai, Muzhi and Yang, Chenxu and Si, Qingyi},
	month = may,
	year = {2025},
	note = {arXiv:2505.07686 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2069/Dai 等 - 2025 - S-GRPO Early Exit via Reinforcement Learning in Reasoning Models.pdf:application/pdf;Snapshot:files/2059/2505.html:text/html},
}

@misc{chen_rift_2025,
	title = {{RIFT}: {Closed}-{Loop} {RL} {Fine}-{Tuning} for {Realistic} and {Controllable} {Traffic} {Simulation}},
	shorttitle = {{RIFT}},
	url = {http://arxiv.org/abs/2505.03344},
	doi = {10.48550/arXiv.2505.03344},
	abstract = {Achieving both realism and controllability in interactive closed-loop traffic simulation remains a key challenge in autonomous driving. Data-driven simulation methods reproduce realistic trajectories but suffer from covariate shift in closed-loop deployment, compounded by simplified dynamics models that further reduce reliability. Conversely, physics-based simulation methods enhance reliable and controllable closed-loop interactions but often lack expert demonstrations, compromising realism. To address these challenges, we introduce a dual-stage AV-centered simulation framework that conducts open-loop imitation learning pre-training in a data-driven simulator to capture trajectory-level realism and multimodality, followed by closed-loop reinforcement learning fine-tuning in a physics-based simulator to enhance controllability and mitigate covariate shift. In the fine-tuning stage, we propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that preserves the trajectory-level multimodality through a GRPO-style group-relative advantage formulation, while enhancing controllability and training stability by replacing KL regularization with the dual-clip mechanism. Extensive experiments demonstrate that RIFT significantly improves the realism and controllability of generated traffic scenarios, providing a robust platform for evaluating autonomous vehicle performance in diverse and interactive scenarios.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Chen, Keyu and Sun, Wenchao and Cheng, Hao and Zheng, Sifa},
	month = may,
	year = {2025},
	note = {arXiv:2505.03344 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:files/2070/Chen 等 - 2025 - RIFT Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation.pdf:application/pdf;Snapshot:files/2061/2505.html:text/html},
}

@misc{zhang_patho-r1_2025,
	title = {Patho-{R1}: {A} {Multimodal} {Reinforcement} {Learning}-{Based} {Pathology} {Expert} {Reasoner}},
	shorttitle = {Patho-{R1}},
	url = {http://arxiv.org/abs/2505.11404},
	doi = {10.48550/arXiv.2505.11404},
	abstract = {Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhang, Wenchuan and Zhang, Penghao and Guo, Jingru and Cheng, Tao and Chen, Jie and Zhang, Shuwan and Zhang, Zhang and Yi, Yuhao and Bu, Hong},
	month = may,
	year = {2025},
	note = {arXiv:2505.11404 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/2071/Zhang 等 - 2025 - Patho-R1 A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner.pdf:application/pdf;Snapshot:files/2062/2505.html:text/html},
}

@misc{xu_distilling_2025,
	title = {Distilling the {Implicit} {Multi}-{Branch} {Structure} in {LLMs}' {Reasoning} via {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2505.16142},
	doi = {10.48550/arXiv.2505.16142},
	abstract = {Distilling reasoning paths from teacher to student models via supervised fine-tuning (SFT) provides a shortcut for improving the reasoning ability of smaller Large Language Models (LLMs). However, the reasoning paths generated by teacher models often reflect only surface-level traces of their underlying authentic reasoning. Insights from cognitive neuroscience suggest that authentic reasoning involves a complex interweaving between meta-reasoning (which selects appropriate sub-problems from multiple candidates) and solving (which addresses the sub-problem). This implies authentic reasoning has an implicit multi-branch structure. Supervised fine-tuning collapses this rich structure into a flat sequence of token prediction in the teacher's reasoning path, preventing effective distillation of this structure to students. To address this limitation, we propose RLKD, a reinforcement learning (RL)-based distillation framework guided by a novel Generative Structure Reward Model (GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving steps and computes rewards to measure structural alignment between student and teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to internalize the teacher's implicit multi-branch reasoning structure rather than merely mimicking fixed output paths. Experiments show RLKD surpasses standard SFT-RL pipelines even when trained on 0.1\% of data under an RL-only regime, unlocking greater student reasoning potential than SFT-based distillation.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Xu, Shicheng and Pang, Liang and Zhu, Yunchang and Gu, Jia and Wei, Zihao and Deng, Jingcheng and Pan, Feiyang and Shen, Huawei and Cheng, Xueqi},
	month = may,
	year = {2025},
	note = {arXiv:2505.16142 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/2072/Xu 等 - 2025 - Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning.pdf:application/pdf;Snapshot:files/2063/2505.html:text/html},
}

@misc{liu_mesh-rft_2025,
	title = {Mesh-{RFT}: {Enhancing} {Mesh} {Generation} via {Fine}-grained {Reinforcement} {Fine}-{Tuning}},
	shorttitle = {Mesh-{RFT}},
	url = {http://arxiv.org/abs/2505.16761},
	doi = {10.48550/arXiv.2505.16761},
	abstract = {Existing pretrained models for 3D mesh generation often suffer from data biases and produce low-quality results, while global reinforcement learning (RL) methods rely on object-level rewards that struggle to capture local structure details. To address these challenges, we present {\textbackslash}textbf\{Mesh-RFT\}, a novel fine-grained reinforcement fine-tuning framework that employs Masked Direct Preference Optimization (M-DPO) to enable localized refinement via quality-aware face masking. To facilitate efficient quality evaluation, we introduce an objective topology-aware scoring system to evaluate geometric integrity and topological regularity at both object and face levels through two metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first method to optimize mesh quality at the granularity of individual faces, resolving localized errors while preserving global coherence. Experiment results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6{\textbackslash}\% and improves Topology Score (TS) by 3.8{\textbackslash}\% over pre-trained models, while outperforming global DPO methods with a 17.4{\textbackslash}\% HD reduction and 4.9{\textbackslash}\% TS gain. These results demonstrate Mesh-RFT's ability to improve geometric integrity and topological regularity, achieving new state-of-the-art performance in production-ready mesh generation. Project Page: {\textbackslash}href\{https://hitcslj.github.io/mesh-rft/\}\{this https URL\}.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Liu, Jian and Xu, Jing and Guo, Song and Li, Jing and Guo, Jingfeng and Yu, Jiaao and Weng, Haohan and Lei, Biwen and Yang, Xianghui and Chen, Zhuo and Zhu, Fangqi and Han, Tao and Guo, Chunchao},
	month = may,
	year = {2025},
	note = {arXiv:2505.16761 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/2073/Liu 等 - 2025 - Mesh-RFT Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning.pdf:application/pdf;Snapshot:files/2064/2505.html:text/html},
}

@misc{samineni_rl_2025,
	title = {{RL} in {Name} {Only}? {Analyzing} the {Structural} {Assumptions} in {RL} post-training for {LLMs}},
	shorttitle = {{RL} in {Name} {Only}?},
	url = {http://arxiv.org/abs/2505.13697},
	doi = {10.48550/arXiv.2505.13697},
	abstract = {Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Samineni, Soumya Rani and Kalwar, Durgesh and Valmeekam, Karthik and Stechly, Kaya and Kambhampati, Subbarao},
	month = may,
	year = {2025},
	note = {arXiv:2505.13697 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2074/Samineni 等 - 2025 - RL in Name Only Analyzing the Structural Assumptions in RL post-training for LLMs.pdf:application/pdf;Snapshot:files/2065/2505.html:text/html},
}

@misc{dong_tool-star_2025,
	title = {Tool-{Star}: {Empowering} {LLM}-{Brained} {Multi}-{Tool} {Reasoner} via {Reinforcement} {Learning}},
	shorttitle = {Tool-{Star}},
	url = {http://arxiv.org/abs/2505.16410},
	doi = {10.48550/arXiv.2505.16410},
	abstract = {Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Dong, Guanting and Chen, Yifei and Li, Xiaoxi and Jin, Jiajie and Qian, Hongjin and Zhu, Yutao and Mao, Hangyu and Zhou, Guorui and Dou, Zhicheng and Wen, Ji-Rong},
	month = may,
	year = {2025},
	note = {arXiv:2505.16410 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2075/Dong 等 - 2025 - Tool-Star Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning.pdf:application/pdf;Snapshot:files/2066/2505.html:text/html},
}

@misc{liu_ovip_2025,
	title = {{OViP}: {Online} {Vision}-{Language} {Preference} {Learning}},
	shorttitle = {{OViP}},
	url = {http://arxiv.org/abs/2505.15963},
	doi = {10.48550/arXiv.2505.15963},
	abstract = {Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Liu, Shujun and Wang, Siyuan and Li, Zejun and Wang, Jianxiang and Zeng, Cheng and Wei, Zhongyu},
	month = may,
	year = {2025},
	note = {arXiv:2505.15963 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/2076/Liu 等 - 2025 - OViP Online Vision-Language Preference Learning.pdf:application/pdf;Snapshot:files/2067/2505.html:text/html},
}

@misc{li_model_2025,
	title = {Model {Merging} in {Pre}-training of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2505.12082},
	doi = {10.48550/arXiv.2505.12082},
	abstract = {Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Li, Yunshui and Ma, Yiyuan and Yan, Shen and Zhang, Chaoyi and Liu, Jing and Lu, Jianqiao and Xu, Ziwen and Chen, Mengzhao and Wang, Minrui and Zhan, Shiyi and Ma, Jin and Lai, Xunhao and Liu, Deyi and Luo, Yao and Bin, Xingyan and Ren, Hongbin and Han, Mingji and Hao, Wenhao and Yi, Bairen and Liu, LingJun and Ma, Bole and Jia, Xiaoying and Zhou, Xun and Qiao, Siyuan and Xiang, Liang and Wu, Yonghui},
	month = may,
	year = {2025},
	note = {arXiv:2505.12082 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2091/Li 等 - 2025 - Model Merging in Pre-training of Large Language Models.pdf:application/pdf;Snapshot:files/2093/2505.html:text/html},
}

@misc{yuan_trajectory_2025,
	title = {Trajectory {Bellman} {Residual} {Minimization}: {A} {Simple} {Value}-{Based} {Method} for {LLM} {Reasoning}},
	shorttitle = {Trajectory {Bellman} {Residual} {Minimization}},
	url = {http://arxiv.org/abs/2505.15311},
	doi = {10.48550/arXiv.2505.15311},
	abstract = {Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as \$Q\$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Yuan, Yurun and Chen, Fan and Jia, Zeyu and Rakhlin, Alexander and Xie, Tengyang},
	month = may,
	year = {2025},
	note = {arXiv:2505.15311 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2092/Yuan 等 - 2025 - Trajectory Bellman Residual Minimization A Simple Value-Based Method for LLM Reasoning.pdf:application/pdf;Snapshot:files/2094/2505.html:text/html},
}

@misc{yang_not_2025,
	title = {Do {Not} {Let} {Low}-{Probability} {Tokens} {Over}-{Dominate} in {RL} for {LLMs}},
	url = {http://arxiv.org/abs/2505.12929},
	doi = {10.48550/arXiv.2505.12929},
	abstract = {Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2\% improvement in K\&K Logic Puzzle reasoning tasks. Our implementation is available at https://github.com/zhyang2226/AR-Lopti.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Yang, Zhihe and Luo, Xufang and Wang, Zilong and Han, Dongqi and He, Zhiyuan and Li, Dongsheng and Xu, Yunjian},
	month = may,
	year = {2025},
	note = {arXiv:2505.12929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2102/Yang 等 - 2025 - Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs.pdf:application/pdf;Snapshot:files/2095/2505.html:text/html},
}

@misc{su_dgro_2025,
	title = {{DGRO}: {Enhancing} {LLM} {Reasoning} via {Exploration}-{Exploitation} {Control} and {Reward} {Variance} {Management}},
	shorttitle = {{DGRO}},
	url = {http://arxiv.org/abs/2505.12951},
	doi = {10.48550/arXiv.2505.12951},
	abstract = {Inference scaling further accelerates Large Language Models (LLMs) toward Artificial General Intelligence (AGI), with large-scale Reinforcement Learning (RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning approaches usually rely on handcrafted rule-based reward functions. However, the tarde-offs of exploration and exploitation in RL algorithms involves multiple complex considerations, and the theoretical and empirical impacts of manually designed reward functions remain insufficiently explored. In this paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional regularization coefficient into two independent hyperparameters: one scales the policy gradient term, and the other regulates the distance from the sampling policy. This decoupling not only enables precise control over balancing exploration and exploitation, but also can be seamlessly extended to Online Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward Optimization. On the other hand, we observe that reward variance significantly affects both convergence speed and final model performance. We conduct both theoretical analysis and extensive empirical validation to assess DGRO, including a detailed ablation study that investigates its performance and optimization dynamics. Experimental results show that DGRO achieves state-of-the-art performance on the Logic dataset with an average accuracy of 96.9{\textbackslash}\%, and demonstrates strong generalization across mathematical benchmarks.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Su, Xuerui and Guo, Liya and Wang, Yue and Zhu, Yi and Ma, Zhiming and Wang, Zun and Liu, Yuting},
	month = may,
	year = {2025},
	note = {arXiv:2505.12951 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2103/Su 等 - 2025 - DGRO Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management.pdf:application/pdf;Snapshot:files/2096/2505.html:text/html},
}

@misc{wu_totrl_2025,
	title = {{ToTRL}: {Unlock} {LLM} {Tree}-of-{Thoughts} {Reasoning} {Potential} through {Puzzles} {Solving}},
	shorttitle = {{ToTRL}},
	url = {http://arxiv.org/abs/2505.12717},
	doi = {10.48550/arXiv.2505.12717},
	abstract = {Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL). However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction. In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths. This process can potentially lead to improved performance and reduced token costs. Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward. ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Wu, Haoyuan and Chen, Xueyi and Ming, Rui and Gao, Jilong and Hu, Shoubo and He, Zhuolun and Yu, Bei},
	month = may,
	year = {2025},
	note = {arXiv:2505.12717 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/2104/Wu 等 - 2025 - ToTRL Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving.pdf:application/pdf;Snapshot:files/2097/2505.html:text/html},
}

@misc{zhang_cec-zero_2025,
	title = {{CEC}-{Zero}: {Chinese} {Error} {Correction} {Solution} {Based} on {LLM}},
	shorttitle = {{CEC}-{Zero}},
	url = {http://arxiv.org/abs/2505.09082},
	doi = {10.48550/arXiv.2505.09082},
	abstract = {Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhang, Sophie and Lin, Zhiming},
	month = may,
	year = {2025},
	note = {arXiv:2505.09082 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/2105/Zhang和Lin - 2025 - CEC-Zero Chinese Error Correction Solution Based on LLM.pdf:application/pdf;Snapshot:files/2098/2505.html:text/html},
}

@misc{chen_seed-grpo_2025,
	title = {{SEED}-{GRPO}: {Semantic} {Entropy} {Enhanced} {GRPO} for {Uncertainty}-{Aware} {Policy} {Optimization}},
	shorttitle = {{SEED}-{GRPO}},
	url = {http://arxiv.org/abs/2505.12346},
	doi = {10.48550/arXiv.2505.12346},
	abstract = {Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidently the model understands a given problem. However, vanilla Group Relative Policy Optimization (GRPO) treats all prompts equally during policy updates, ignoring this important information about the model's knowledge boundaries. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs' uncertainty of the input prompts semantic entropy. Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates. This uncertainty-aware training mechanism enables dynamic adjustment of policy update magnitudes based on question uncertainty. It allows more conservative updates on high-uncertainty questions while maintaining the original learning signal on confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Chen, Minghan and Chen, Guikun and Wang, Wenguan and Yang, Yi},
	month = may,
	year = {2025},
	note = {arXiv:2505.12346 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/2106/Chen 等 - 2025 - SEED-GRPO Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization.pdf:application/pdf;Snapshot:files/2099/2505.html:text/html},
}

@misc{yang_think_2025,
	title = {Think {When} {You} {Need}: {Self}-{Adaptive} {Chain}-of-{Thought} {Learning}},
	shorttitle = {Think {When} {You} {Need}},
	url = {http://arxiv.org/abs/2504.03234},
	doi = {10.48550/arXiv.2504.03234},
	abstract = {Chain of Thought (CoT) reasoning enhances language models' performance but often leads to inefficient "overthinking" on simple problems. We identify that existing approaches directly penalizing reasoning length fail to account for varying problem complexity. Our approach constructs rewards through length and quality comparisons, guided by theoretical assumptions that jointly enhance solution correctness with conciseness. Moreover, we further demonstrate our method to fuzzy tasks where ground truth is unavailable. Experiments across multiple reasoning benchmarks demonstrate that our method maintains accuracy while generating significantly more concise explanations, effectively teaching models to "think when needed."},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Yang, Junjie and Lin, Ke and Yu, Xing},
	month = may,
	year = {2025},
	note = {arXiv:2504.03234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/2107/Yang 等 - 2025 - Think When You Need Self-Adaptive Chain-of-Thought Learning.pdf:application/pdf;Snapshot:files/2100/2504.html:text/html},
}

@article{gerhardt_towards_2025,
	title = {Towards {Scalable} and {Resource}-{Conscious} {Reasoning}: {A} {Survey} of {Efficient} {Models}},
	shorttitle = {Towards {Scalable} and {Resource}-{Conscious} {Reasoning}},
	url = {https://www.preprints.org/frontend/manuscript/99dfe6fe929d04212bd84f67dba836bc/download_pub},
	urldate = {2025-05-28},
	author = {Gerhardt, Mareike and Schneider, Lukas and Muller, Anna},
	year = {2025},
	file = {Available Version (via Google Scholar):files/2108/Gerhardt 等 - 2025 - Towards Scalable and Resource-Conscious Reasoning A Survey of Efficient Models.pdf:application/pdf},
}

@misc{zha_new_2025,
	title = {A {New} {DAPO} {Algorithm} for {Stock} {Trading}},
	url = {http://arxiv.org/abs/2505.06408},
	doi = {10.1109/IDS66066.2025.00013},
	abstract = {Recent advances in reinforcement learning, such as Dynamic Sampling Policy Optimization (DAPO), show strong performance when paired with large language models (LLMs). Motivated by this success, we ask whether similar gains can be realized in financial trading. We design a trading agent that combines an improved Group Relative Policy Optimization (GRPO) algorithm, augmented with ideas from DAPO, with LLM-based risk and sentiment signals extracted from financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a cumulative return of 230.49 percent and an information ratio of 0.37, outperforming the CPPO-DeepSeek baseline. It also cuts training time from about 8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The proposed RL-LLM framework offers a scalable path toward data-efficient trading agents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/},
	urldate = {2025-05-28},
	author = {Zha, Ruijian and Liu, Bojun},
	month = may,
	year = {2025},
	note = {arXiv:2505.06408 [cs]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science},
	file = {Preprint PDF:files/2109/Zha和Liu - 2025 - A New DAPO Algorithm for Stock Trading.pdf:application/pdf;Snapshot:files/2101/2505.html:text/html},
}
