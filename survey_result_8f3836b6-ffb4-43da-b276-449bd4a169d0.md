# Paper List of Terms(Reward+RL)
- [25/06] **KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality**  
[[Paper](http://arxiv.org/pdf/2506.19807v1)] [[Code/Page](https://github.com/zjunlp/KnowRL.)] [[TLDR/Notes](#knowrl--exploring-knowledgeable-reinforcement-learning-for-factuality)]

- [25/06] **Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.19785v1)] [[Code/Page]()] [[TLDR/Notes](#learning-task-belief-similarity-with-latent-dynamics-for-meta-reinforcement-learning)]

- [25/06] **SAGE: Strategy-Adaptive Generation Engine for Query Rewriting**  
[[Paper](http://arxiv.org/pdf/2506.19783v1)] [[Code/Page]()] [[TLDR/Notes](#sage--strategy-adaptive-generation-engine-for-query-rewriting)]

- [25/06] **Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager**  
[[Paper](http://arxiv.org/pdf/2506.19652v1)] [[Code/Page]()] [[TLDR/Notes](#tailored-conversations-beyond-llms--a-rl-based-dialogue-manager)]

- [25/06] **Scaffolding Dexterous Manipulation with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2506.19212v1)] [[Code/Page]()] [[TLDR/Notes](#scaffolding-dexterous-manipulation-with-vision-language-models)]

- [25/06] **LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.18841v1)] [[Code/Page](https://huggingface.co/THU-KEG/LongWriter-Zero-32B)] [[TLDR/Notes](#longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning)]

- [25/06] **Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation**  
[[Paper](http://arxiv.org/pdf/2506.18670v1)] [[Code/Page](https://github.com/liujm2001/CoAugRetriever.)] [[TLDR/Notes](#harnessing-the-power-of-reinforcement-learning-for-language-model-based-information-retriever-via-query-document-co-augmentation)]

- [25/06] **AdapThink: Adaptive Thinking Preferences for Reasoning Language Model**  
[[Paper](http://arxiv.org/pdf/2506.18237v1)] [[Code/Page]()] [[TLDR/Notes](#adapthink--adaptive-thinking-preferences-for-reasoning-language-model)]

- [25/06] **RL for Reasoning by Adaptively Revealing Rationales**  
[[Paper](http://arxiv.org/pdf/2506.18110v1)] [[Code/Page]()] [[TLDR/Notes](#rl-for-reasoning-by-adaptively-revealing-rationales)]

- [25/06] **Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach**  
[[Paper](http://arxiv.org/pdf/2506.17828v1)] [[Code/Page]()] [[TLDR/Notes](#aligning-frozen-llms-by-reinforcement-learning--an-iterative-reweight-then-optimize-approach)]



# TLDR/Notes
## knowrl--exploring-knowledgeable-reinforcement-learning-for-factuality
### Abstract
Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | KnowRLï¼šç”¨çŸ¥è¯†å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼Œç»™å¤§æ¨¡å‹â€œæ…¢æ€è€ƒâ€é™ hallucination

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯â€œæ…¢æ€è€ƒâ€æ¨¡å‹ï¼‰åœ¨æ¨ç†æ—¶å®¹æ˜“å‡ºç°ä¸¥é‡çš„ hallucinationï¼ˆäº‹å®æ€§é”™è¯¯ï¼‰ï¼ŒåŸå› æ˜¯å®ƒä»¬éš¾ä»¥å‡†ç¡®è¯†åˆ«è‡ªèº«çš„çŸ¥è¯†è¾¹ç•Œã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æå‡å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶â€œç»“æœå¯¼å‘â€çš„å¥–åŠ±æœºåˆ¶å¾€å¾€å¯¹æ¨ç†è¿‡ç¨‹ç¼ºä¹äº‹å®æ€§ç›‘ç£ï¼Œåè€ŒåŠ å‰§äº† hallucination é—®é¢˜ã€‚æ­¤å¤–ï¼Œç°æœ‰ç¼“è§£ hallucination çš„æ–¹æ³•ï¼ˆå¦‚ç›‘ç£å¾®è°ƒã€æ£€ç´¢å¢å¼ºç”Ÿæˆã€è§£ç é˜¶æ®µå¹²é¢„ï¼‰å­˜åœ¨æˆæœ¬é«˜ã€æ•ˆç‡ä½æˆ–ç ´åå·²æœ‰ç­–ç•¥ç­‰ç¼ºé™·ï¼Œä¸”éš¾ä»¥åœ¨é™ä½ hallucination çš„åŒæ—¶ä¿ç•™æ¨¡å‹å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©æ…¢æ€è€ƒæ¨¡å‹åœ¨ä¿æŒæ¨ç†èƒ½åŠ›çš„å‰æä¸‹å‡å°‘äº‹å®é”™è¯¯ï¼Œæˆä¸ºäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡º KnowRL æ¡†æ¶  
KnowRL æ˜¯ä¸€ç§**çŸ¥è¯†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ **æ–¹æ³•ï¼Œæ ¸å¿ƒæ˜¯åœ¨ RL è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥**åŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®æ€§å¥–åŠ±**ã€‚è¯¥å¥–åŠ±å‚è€ƒ FactScoreï¼ˆé€šè¿‡å¤–éƒ¨çŸ¥è¯†åº“éªŒè¯æ–‡æœ¬æ”¯æ’‘æ€§æ¥è¯„ä¼°äº‹å®æ€§ï¼‰è®¾è®¡ï¼Œå¼•å¯¼æ¨¡å‹åœ¨â€œæ…¢æ€è€ƒâ€æ—¶åŸºäºäº‹å®æ¨ç†ï¼Œå¸®åŠ©æ¨¡å‹æ›´æ¸…æ™°åœ°è®¤çŸ¥è‡ªèº«çŸ¥è¯†è¾¹ç•Œï¼Œé¿å…ç¼–é€ äº‹å®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šååŒä¼˜åŒ–æ¨ç†ä¸äº‹å®éµå¾ª  
ä¼ ç»Ÿ RL åªå…³æ³¨æœ€ç»ˆç»“æœï¼ŒKnowRL åˆ™ç›´æ¥å¯¹æ¨ç†æ­¥éª¤ä¸­çš„â€œäº‹å®éµå¾ªåº¦â€è¿›è¡Œå¥–åŠ±ï¼Œè®©æ¨¡å‹å­¦ä¹ å¹¶å†…åŒ–åŸºäºäº‹å®çš„æ¨ç†ç­–ç•¥ï¼Œåœ¨æ…¢æ€è€ƒæ¨¡å‹ä¸­ååŒä¼˜åŒ–â€œæ¨ç†èƒ½åŠ›â€å’Œâ€œäº‹å®æ­£ç¡®æ€§â€ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºé«˜è´¨é‡äº‹å®ä»»åŠ¡è®­ç»ƒé›†  
ä¸ºäº†ç»™ KnowRL çš„åˆå§‹åŒ–å’Œè®­ç»ƒæä¾›æ”¯æ’‘ï¼Œè®ºæ–‡å›¢é˜Ÿæ„å»ºäº†ç²¾å¿ƒè®¾è®¡çš„é«˜è´¨é‡äº‹å®ä»»åŠ¡è®­ç»ƒæ•°æ®é›†ï¼Œå¼¥è¡¥äº†è®­ç»ƒé˜¶æ®µäº‹å®ç›‘ç£ä¸è¶³çš„é—®é¢˜ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡åœ¨ 3 ä¸ª hallucination è¯„ä¼°æ•°æ®é›†ï¼ˆå¦‚ TruthfulQAã€SimpleQA ç­‰ï¼‰å’Œ 2 ä¸ªæ¨ç†è¯„ä¼°æ•°æ®é›†ï¼ˆå¦‚ GPQAã€AIME 2025 ç­‰ï¼‰ä¸ŠéªŒè¯äº† KnowRL çš„æ•ˆæœï¼š  
- åŸºäºè’¸é¦çš„æ…¢æ€è€ƒæ¨¡å‹ç» KnowRL è®­ç»ƒåï¼Œåœ¨å¤šä¸ª hallucination åŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®ç‡é¢†å…ˆï¼ˆå¦‚åœ¨ SimpleQA ç­‰ä»»åŠ¡ä¸Šè¡¨ç°æå‡ï¼‰ï¼›  
- å¯¹åŸæœ¬é€šè¿‡ RL è®­ç»ƒçš„æ…¢æ€è€ƒæ¨¡å‹ï¼ŒKnowRL ä¹Ÿèƒ½æ˜¾è‘—é™ä½ hallucinationï¼ˆå¦‚åœ¨ ChineseSimpleQA ä¸Šå‡†ç¡®ç‡è¾¾ 16.23%ï¼‰ï¼ŒåŒæ—¶ä¿ç•™ç”šè‡³å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ï¼›  
- å¯¹æ¯”åŸºçº¿æ–¹æ³•ï¼ŒKnowRL åœ¨ç¼“è§£ hallucination ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œä¸”å¸¸èƒ½è®©æ¨¡å‹æ¨ç†æ€§èƒ½è¶…è¶ŠåŸæœ‰æ°´å¹³ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ€è·¯å±‚é¢**ï¼šé’ˆå¯¹â€œç»“æœå¯¼å‘ RL å¿½ç•¥è¿‡ç¨‹äº‹å®æ€§â€çš„ç—›ç‚¹ï¼Œæå‡ºâ€œç”¨å¤–éƒ¨çŸ¥è¯†ç›´æ¥ç›‘ç£æ¨ç†è¿‡ç¨‹â€çš„æ€è·¯ï¼Œä¸ºç¼“è§£å¤§æ¨¡å‹ hallucination æä¾›äº†æ–°è§†è§’â€”â€”ä»**è®­ç»ƒé˜¶æ®µçš„å¥–åŠ±æœºåˆ¶**å…¥æ‰‹ï¼ŒæŠŠâ€œäº‹å®æ€§â€åµŒå…¥å¼ºåŒ–å­¦ä¹ æµç¨‹ï¼›  
2. **æ–¹æ³•å±‚é¢**ï¼šKnowRL æ¡†æ¶å±•ç¤ºäº†å¦‚ä½•å°†â€œçŸ¥è¯†éªŒè¯â€è½¬åŒ–ä¸º RL ä¸­çš„å¥–åŠ±ä¿¡å·ï¼Œå®ç°æ¨ç†èƒ½åŠ›ä¸äº‹å®æ­£ç¡®æ€§çš„ååŒä¼˜åŒ–ï¼Œè¿™ç§â€œè¿‡ç¨‹ç›‘ç£ + çŸ¥è¯†å¢å¼ºâ€çš„è®¾è®¡å¯å¯å‘åç»­å¼ºåŒ–å­¦ä¹ ä¸çŸ¥è¯†èåˆçš„ç ”ç©¶ï¼›  
3. **å·¥ç¨‹å±‚é¢**ï¼šè®ºæ–‡æ„å»ºçš„é«˜è´¨é‡äº‹å®ä»»åŠ¡æ•°æ®é›†ï¼Œä¸ºåŒç±»â€œéœ€äº‹å®æ€§ç›‘ç£â€çš„è®­ç»ƒå·¥ä½œæä¾›äº†æ•°æ®å±‚é¢çš„å‚è€ƒï¼Œä½“ç°äº†â€œä¼˜è´¨æ•°æ® + ç²¾å‡†æ–¹æ³•â€ç»“åˆçš„é‡è¦æ€§ã€‚  


ç»¼ä¸Šï¼ŒKnowRL ç„å‡†å¤§æ¨¡å‹æ…¢æ€è€ƒä¸­çš„ hallucination éš¾é¢˜ï¼Œé€šè¿‡çŸ¥è¯†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ å®ç°äº†â€œé™å¹»è§‰ã€ä¿æ¨ç†â€çš„ç›®æ ‡ï¼Œä¸ºå¤§æ¨¡å‹æ›´å¯é çš„å¤æ‚æ¨ç†æä¾›äº†ä¸€å¥—æ–°é¢–è§£æ³•ã€‚

## learning-task-belief-similarity-with-latent-dynamics-for-meta-reinforcement-learning
### Abstract
Meta-reinforcement learning requires utilizing prior task distribution
information obtained during exploration to rapidly adapt to unknown tasks. The
efficiency of an agent's exploration hinges on accurately identifying the
current task. Recent Bayes-Adaptive Deep RL approaches often rely on
reconstructing the environment's reward signal, which is challenging in sparse
reward settings, leading to suboptimal exploitation. Inspired by bisimulation
metrics, which robustly extracts behavioral similarity in continuous MDPs, we
propose SimBelief-a novel meta-RL framework via measuring similarity of task
belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common
features of similar task distributions, enabling efficient task identification
and exploration in sparse reward environments. We introduce latent task belief
metric to learn the common structure of similar tasks and incorporate it into
the specific task belief. By learning the latent dynamics across task
distributions, we connect shared latent task belief features with specific task
features, facilitating rapid task identification and adaptation. Our method
outperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym
tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäº latent dynamics å­¦ä¹ ä»»åŠ¡ä¿¡å¿µç›¸ä¼¼æ€§ï¼ŒåŠ©åŠ›å…ƒå¼ºåŒ–å­¦ä¹ æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å…ƒå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åˆ©ç”¨æ¢ç´¢é˜¶æ®µè·å–çš„å…ˆéªŒä»»åŠ¡åˆ†å¸ƒä¿¡æ¯æ¥å¿«é€Ÿé€‚åº”æœªçŸ¥ä»»åŠ¡ï¼Œè€Œæ¢ç´¢æ•ˆç‡å–å†³äºèƒ½å¦å‡†ç¡®è¯†åˆ«å½“å‰ä»»åŠ¡ã€‚ç°æœ‰åŸºäºè´å¶æ–¯è‡ªé€‚åº”çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¸¸ä¾èµ–é‡æ„ç¯å¢ƒå¥–åŠ±ä¿¡å·ï¼Œä½†åœ¨ç¨€ç–å¥–åŠ±åœºæ™¯ä¸‹è¿™é¢‡å…·æŒ‘æˆ˜ï¼Œæ˜“å¯¼è‡´æ¬¡ä¼˜åˆ©ç”¨ã€‚åŒæ—¶ï¼Œç°å®åœºæ™¯ä¸­å¹²æ‰°å¤šã€å¥–åŠ±ç¨€ç–æ—¶ï¼Œæ™ºèƒ½ä½“æ˜“å‡ºç°é”™è¯¯æ¢ç´¢è¡Œä¸ºï¼Œä»¥å¾€æ–¹æ³•ä¹Ÿå¸¸å¿½ç•¥ä»»åŠ¡é—´æ½œåœ¨çš„å…±åŒç»“æ„ï¼Œéš¾ä»¥é«˜æ•ˆè¯†åˆ«ç›¸ä¼¼ç»“æ„ä»»åŠ¡å¹¶å®ç°å¿«é€Ÿé€‚é…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSimBeliefå…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶  
å—è¿ç»­MDPä¸­åŒæ¨¡æ‹Ÿåº¦é‡ï¼ˆbisimulation metricsï¼‰æå–è¡Œä¸ºç›¸ä¼¼æ€§çš„å¯å‘ï¼Œåœ¨è´å¶æ–¯è‡ªé€‚åº”MDPï¼ˆBAMDPï¼‰ä¸‹æå‡ºSimBeliefæ¡†æ¶ï¼Œé€šè¿‡åº¦é‡ä»»åŠ¡ä¿¡å¿µçš„ç›¸ä¼¼æ€§ï¼Œæœ‰æ•ˆæå–ç›¸ä¼¼ä»»åŠ¡åˆ†å¸ƒçš„å…±åŒç‰¹å¾ï¼Œåœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­å®ç°é«˜æ•ˆä»»åŠ¡è¯†åˆ«ä¸æ¢ç´¢ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥æ½œåœ¨ä»»åŠ¡ä¿¡å¿µåº¦é‡ä¸å­¦ä¹ æ½œåœ¨åŠ¨åŠ›å­¦  
è®¾è®¡æ½œåœ¨ä»»åŠ¡ä¿¡å¿µåº¦é‡æ¥å­¦ä¹ ç›¸ä¼¼ä»»åŠ¡çš„å…±åŒç»“æ„å¹¶èå…¥ç‰¹å®šä»»åŠ¡ä¿¡å¿µï¼›é€šè¿‡å­¦ä¹ è·¨ä»»åŠ¡åˆ†å¸ƒçš„æ½œåœ¨åŠ¨åŠ›å­¦ï¼Œè¿æ¥å…±äº«çš„æ½œåœ¨ä»»åŠ¡ä¿¡å¿µç‰¹å¾ä¸ç‰¹å®šä»»åŠ¡ç‰¹å¾ï¼ŒåŠ©åŠ›å¿«é€Ÿä»»åŠ¡è¯†åˆ«ä¸é€‚é…ã€‚æ—¢åˆ©ç”¨æ½œåœ¨ç©ºé—´å­¦ä¹ åˆ°çš„ä»»åŠ¡åˆ†å¸ƒæ•´ä½“è®¤çŸ¥å’Œä»»åŠ¡å…³ç³»ï¼Œåˆç»“åˆå½“å‰å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ä¿¡å¿µï¼Œå¢å¼ºæ™ºèƒ½ä½“å¯¹æœªçŸ¥ç¯å¢ƒçš„é€‚åº”æ€§ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé¢å‘BAMDPçš„ä»»åŠ¡è¡¨ç¤ºæ–¹æ³•  
ä¸ºBAMDPä¸­åŸºäºä¸Šä¸‹æ–‡çš„å…ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡ä»»åŠ¡è¡¨ç¤ºæ–¹æ³•ï¼Œå€ŸåŠ©æ½œåœ¨å¥–åŠ±æ¨¡å‹ã€è½¬ç§»æ¨¡å‹å’Œé€†åŠ¨åŠ›å­¦æ¨¡å‹å­¦ä¹ ä»»åŠ¡ä¿¡å¿µç›¸ä¼¼æ€§ï¼Œæå‡æ™ºèƒ½ä½“å¯¹æœªçŸ¥ä»»åŠ¡çš„è¯†åˆ«ä¸é€‚åº”èƒ½åŠ›ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç¨€ç–å¥–åŠ±çš„MuJoCoå’Œpanda - gymä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼›åŒæ—¶åœ¨å¤„ç†åˆ†å¸ƒå¤–ï¼ˆOODï¼‰ä»»åŠ¡æ—¶ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§ä¸æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ä»æ–¹æ³•è®¾è®¡è§’åº¦ï¼Œå°†åŒæ¨¡æ‹Ÿåº¦é‡æ€æƒ³å¼•å…¥å…ƒå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¿¡å¿µåº¦é‡ã€ç»“åˆæ½œåœ¨ç©ºé—´å­¦ä¹ ä¸ç‰¹å®šä»»åŠ¡ä¿¡å¿µçš„æ€è·¯ï¼Œä¸ºå¤„ç†ç¨€ç–å¥–åŠ±ã€å¤šä»»åŠ¡é€‚é…ç­‰éš¾é¢˜æä¾›äº†æ–°èŒƒå¼ï¼›ä»åº”ç”¨è§’åº¦ï¼Œåœ¨MuJoCoå’Œpanda - gymç­‰ä»»åŠ¡ä¸Šçš„æˆåŠŸå®è·µï¼Œè¯æ˜å…¶åœ¨æœºå™¨äººæ§åˆ¶ç­‰éœ€å¤„ç†å¤æ‚ä»»åŠ¡åˆ†å¸ƒåœºæ™¯çš„æ½œåŠ›ï¼Œä¸ºç›¸å…³é¢†åŸŸç®—æ³•è®¾è®¡æä¾›äº†å‚è€ƒï¼›ç†è®ºå±‚é¢ï¼Œå¯¹æ½œåœ¨ä»»åŠ¡ä¿¡å¿µåº¦é‡åœ¨BAMDPä¸­çš„æœ‰æ•ˆæ€§éªŒè¯ï¼Œä¹Ÿä¸ºåç»­ç†è®ºåˆ†æå’Œæ–¹æ³•æ‹“å±•ç­‘ç‰¢äº†åŸºç¡€ã€‚

## sage--strategy-adaptive-generation-engine-for-query-rewriting
### Abstract
Query rewriting is pivotal for enhancing dense retrieval, yet current methods
demand large-scale supervised data or suffer from inefficient reinforcement
learning (RL) exploration. In this work, we first establish that guiding Large
Language Models (LLMs) with a concise set of expert-crafted strategies, such as
semantic expansion and entity disambiguation, substantially improves retrieval
effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,
and SciFact. Building on this insight, we introduce the Strategy-Adaptive
Generation Engine (SAGE), which operationalizes these strategies in an RL
framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit
Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative
learning signals. This strategy-guided approach not only achieves new
state-of-the-art NDCG@10 results, but also uncovers a compelling emergent
behavior: the agent learns to select optimal strategies, reduces unnecessary
exploration, and generates concise rewrites, lowering inference cost without
sacrificing performance. Our findings demonstrate that strategy-guided RL,
enhanced with nuanced reward shaping, offers a scalable, efficient, and more
interpretable paradigm for developing the next generation of robust information
retrieval systems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SAGEï¼šç”¨ç­–ç•¥å¼•å¯¼+å¼ºåŒ–å­¦ä¹ é©æ–°æŸ¥è¯¢é‡å†™ï¼Œè§£é”å¯†é›†æ£€ç´¢æ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰é¢†åŸŸï¼Œå¯†é›†æ£€ç´¢ç³»ç»Ÿæ€§èƒ½å¾ˆå¤§ç¨‹åº¦å–å†³äºè¾“å…¥æŸ¥è¯¢è´¨é‡ï¼ŒæŸ¥è¯¢é‡å†™æ˜¯å¼¥åˆç”¨æˆ·æ„å›¾ä¸æœºå™¨ç†è§£é—´å·®è·çš„å…³é”®ã€‚ä½†å½“å‰æ–¹æ³•å­˜åœ¨ä¸¤å¤§éš¾é¢˜ï¼šä¼ ç»Ÿæœ‰ç›‘ç£å¾®è°ƒéœ€å¤§è§„æ¨¡æ˜‚è´µäººå·¥æ ‡æ³¨ï¼›ç°ä»£å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼ˆå¦‚PPOã€GRPOï¼‰å¸¸å› æ¢ç´¢ä½æ•ˆï¼Œé˜»ç¢æœ€ä¼˜é‡å†™ç­–ç•¥å‘ç°ï¼Œè¿˜å¯èƒ½å¼•å‘è®­ç»ƒä¸ç¨³å®šç”šè‡³è¾“å‡ºä¸è¿è´¯ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿‡å¾€åŸºäºç­–ç•¥çš„æç¤ºæ–¹æ³•é’ˆå¯¹é€šç”¨ç½‘é¡µæœç´¢ç¨€ç–æ£€ç´¢è®¾è®¡ï¼Œéš¾é€‚é…å¯†é›†æ£€ç´¢çš„ç²¾ç»†éœ€æ±‚ï¼Œåœ¨ä¸“ä¸šåŸºå‡†æµ‹è¯•ä¸­æ•ˆæœæœ‰é™ã€‚åŒæ—¶ï¼ŒæŸ¥è¯¢é‡å†™ä»»åŠ¡è¿˜å­˜åœ¨â€œå¥–åŠ±é»‘å®¢â€ç°è±¡â€”â€”æ¨¡å‹ä¼šå·æ‡’å¤åˆ¶åŸæŸ¥è¯¢æ‹¿é«˜åˆ†ï¼Œé™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œé™åˆ¶æœ‰æ•ˆæ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡é¢å‘å¯†é›†æ£€ç´¢çš„ä¸“å®¶ç­–ç•¥  
æå‡º5ç§ä¸“ä¸ºå¯†é›†æ£€ç´¢åœºæ™¯è®¾è®¡çš„æŸ¥è¯¢é‡å†™ç­–ç•¥ï¼ˆå¦‚è¯­ä¹‰æ‰©å±•ã€å®ä½“æ¶ˆæ­§ç­‰ï¼‰ï¼Œä»…é€šè¿‡æç¤ºå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°±åœ¨HotpotQAã€FEVERç­‰å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡æ£€ç´¢æ•ˆæœï¼Œå±•ç°çº¯æç¤ºæ–¹å¼èƒ½è¾¾åˆ°çš„æ€§èƒ½ä¸Šé™ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSAGEå¼ºåŒ–å­¦ä¹ æ¡†æ¶  
æ„å»º**Strategy-Adaptive Generation Engineï¼ˆSAGEï¼‰**ï¼Œå°†äººå·¥è®¾è®¡ç­–ç•¥ç³»ç»Ÿé›†æˆåˆ°GRPOç®—æ³•çš„å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­ï¼Œå¼•å¯¼LLMå­¦ä¹ æ›´ä¼˜æŸ¥è¯¢é‡å†™ç­–ç•¥ã€‚åŒæ—¶å¼•å…¥ä¸¤ç§æ–°é¢–å¥–åŠ±å¡‘é€ æœºåˆ¶ï¼š  
- **Strategic Credit Shapingï¼ˆSCSï¼‰**ï¼šä¾æ®å„ç­–ç•¥çš„æ•´ä½“è¡¨ç°åˆ†é…â€œä¿¡ç”¨â€ï¼Œæä¾›æ›´å…·ç­–ç•¥é’ˆå¯¹æ€§çš„å­¦ä¹ ä¿¡å·ï¼›  
- **Contrastive Reward Shapingï¼ˆCRSï¼‰**ï¼šæŠŠç»å¯¹åˆ†æ•°è½¬åŒ–ä¸ºç›¸å¯¹æ€§èƒ½åº¦é‡ï¼Œè®©å¥–åŠ±æ›´èƒ½åæ˜ ç­–ç•¥ä¼˜åŠ£å·®å¼‚ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¯¹æŠ—â€œå¥–åŠ±é»‘å®¢â€çš„æ¢ç´¢æœºåˆ¶  
é€šè¿‡é’ˆå¯¹æ€§æç¤ºå·¥ç¨‹ï¼ŒåŠ ä¸Šå¯¹ä¸åŸæŸ¥è¯¢å®Œå…¨ç›¸åŒè¾“å‡ºçš„æƒ©ç½šæœºåˆ¶ï¼Œå¼ºåˆ¶æ¨¡å‹è·³å‡ºâ€œå¤åˆ¶åŸæŸ¥è¯¢æ‹¿é«˜åˆ†â€çš„å®‰å…¨é™·é˜±ï¼Œæ¨åŠ¨æ¨¡å‹æ¢ç´¢æ›´æœ‰ä»·å€¼çš„é‡å†™ç­–ç•¥ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨HotpotQAå’ŒNFCorpusç­‰æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSAGEåœ¨NDCG@10æŒ‡æ ‡ä¸Šå®ç°äº†å½“å‰æœ€ä¼˜ï¼ˆSOTAï¼‰çš„å¯†é›†æ£€ç´¢æ•ˆæœã€‚æ›´å¼•äººæ³¨ç›®çš„æ˜¯**æ¶Œç°è¡Œä¸º**ï¼šæ¨¡å‹å­¦ä¼šæ›´é«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ï¼Œå¤§å¹…å‡å°‘tokenä½¿ç”¨é‡ï¼Œåœ¨æ— éœ€æ˜¾å¼ä¼˜åŒ–æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œé™ä½äº†æ¨ç†å»¶è¿Ÿä¸è®¡ç®—å¼€é”€ã€‚åŒæ—¶ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†â€œå¼ºåˆ¶æ¢ç´¢+æƒ©ç½šæœºåˆ¶â€å¯¹é¿å…â€œå¥–åŠ±é»‘å®¢â€ã€æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **ç­–ç•¥å¼•å¯¼çš„ä»·å€¼**ï¼šå°‘é‡å¯è§£é‡Šçš„äººå·¥ç­–ç•¥èƒ½åœ¨æ— é¢å¤–è®­ç»ƒæ—¶æ˜¾è‘—æå‡LLMæŸ¥è¯¢é‡å†™è´¨é‡ï¼Œæç¤ºå·¥ç¨‹ä¸é¢†åŸŸçŸ¥è¯†ç»“åˆçš„æ€è·¯å€¼å¾—åœ¨å‚ç›´ä»»åŠ¡ä¸­å¤ç”¨ï¼›  
2. **å¼ºåŒ–å­¦ä¹ ä¸é¢†åŸŸé€‚é…**ï¼šSAGEå±•ç¤ºäº†æŠŠé¢†åŸŸä¸“å±ç­–ç•¥åµŒå…¥RLæ¡†æ¶çš„å¯è¡Œæ€§ï¼Œä¸ºå‚ç›´é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹æ£€ç´¢ï¼‰å®šåˆ¶RLæ–¹æ¡ˆæä¾›å‚è€ƒï¼›  
3. **å¥–åŠ±å¡‘é€ çš„ç²¾ç»†åº¦**ï¼šSCSå’ŒCRSè¯æ˜å¯¹å¥–åŠ±ä¿¡å·åšä»»åŠ¡å®šåˆ¶åŒ–æ”¹é€ ï¼Œèƒ½è®©RLæ›´é«˜æ•ˆå­¦ä¹ ï¼Œç±»ä¼¼æ€è·¯å¯æ¨å¹¿åˆ°å…¶ä»–ç”Ÿæˆå¼ä»»åŠ¡ï¼ˆå¦‚å¯¹è¯ç”Ÿæˆã€ä»£ç ç”Ÿæˆï¼‰çš„å¥–åŠ±è®¾è®¡ï¼›  
4. **å¯¹æŠ—â€œå·æ‡’â€è¡Œä¸º**ï¼šé’ˆå¯¹ä»»åŠ¡ä¸­æ¨¡å‹æ˜“é™·å…¥çš„å±€éƒ¨æœ€ä¼˜ï¼ˆå¦‚â€œå¤åˆ¶åŸè¾“å…¥â€ï¼‰ï¼Œè®¾è®¡ç®€å•æœ‰æ•ˆçš„æƒ©ç½šä¸æ¢ç´¢æœºåˆ¶ï¼Œæ˜¯è®­ç»ƒé²æ£’RLæ¨¡å‹çš„é‡è¦å®è·µç»éªŒã€‚  

SAGEç”¨â€œç­–ç•¥å¼•å¯¼+å¼ºåŒ–å­¦ä¹ +ç²¾ç»†å¥–åŠ±å¡‘é€ â€çš„ç»„åˆæ‹³ï¼Œä¸ºä¸‹ä¸€ä»£é²æ£’ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿå¼€è¾Ÿäº†å¯æ‰©å±•ã€é«˜æ•ˆä¸”æ›´å…·å¯è§£é‡Šæ€§çš„æ–°èŒƒå¼ï¼Œä¹Ÿä¸ºLLMåœ¨å‚ç›´ä»»åŠ¡ä¸­çªç ´â€œä½æ•ˆæ¢ç´¢â€ä¸â€œæ ‡æ³¨ä¾èµ–â€å›°å¢ƒæä¾›äº†å®è´µå€Ÿé‰´ã€‚

## tailored-conversations-beyond-llms--a-rl-based-dialogue-manager
### Abstract
In this work, we propose a novel framework that integrates large language
models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a
specific goal. By leveraging hierarchical reinforcement learning to model the
structured phases of dialogue and employ meta-learning to enhance adaptability
across diverse user profiles, our approach enhances adaptability and
efficiency, enabling the system to learn from limited data, transition fluidly
between dialogue phases, and personalize responses to heterogeneous patient
needs. We apply our framework to Motivational Interviews, aiming to foster
behavior change, and demonstrate that the proposed dialogue manager outperforms
a state-of-the-art LLM baseline in terms of reward, showing a potential benefit
of conditioning LLMs to create open-ended dialogue systems with specific goals.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¶…è¶Šå¤§æ¨¡å‹çš„ä¸ªæ€§åŒ–å¯¹è¯ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹è¯ç®¡ç†å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¿ƒç†å¥åº·æœåŠ¡éœ€æ±‚æ¿€å¢ï¼Œä½†èµ„æºä¾›ç»™ä¸è¶³ï¼Œæ‚£è€…ç­‰å¾…æ²»ç–—æ—¶é—´æ¼«é•¿ã€‚åŠ¨æœºå¼è®¿è°ˆï¼ˆMotivational Interviewingï¼ŒMIï¼‰è¿™ç±»èƒ½è¾…åŠ©å¿ƒç†å¹²é¢„çš„å¯¹è¯åœºæ™¯ï¼Œå¯¹å¯¹è¯ç³»ç»Ÿæå‡ºäº†é«˜è¦æ±‚ï¼šæ—¢éœ€æŒ‰engagementï¼ˆå»ºç«‹å…³ç³»ï¼‰ã€focusingï¼ˆæ˜ç¡®æ ¸å¿ƒï¼‰ã€evokingï¼ˆæ¿€å‘åŠ¨æœºï¼‰ã€planningï¼ˆåˆ¶å®šè®¡åˆ’ï¼‰ç­‰ç»“æ„åŒ–é˜¶æ®µæ¨è¿›ï¼Œåˆè¦é€‚é…ä¸åŒæ‚£è€…ç”»åƒï¼ˆå¦‚Open - to - Changeã€Resistant - to - Changeã€Receptiveä¸‰ç±»ï¼‰ã€‚ä¼ ç»ŸåŸºäºè§„åˆ™çš„å¯¹è¯ç³»ç»Ÿè™½å¯æ§ã€å¯è§£é‡Šï¼Œä½†é€‚åº”æ€§å·®ã€å¼€å‘æˆæœ¬é«˜ï¼›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½é€‚åº”æ€§å’Œç”Ÿæˆèƒ½åŠ›å¼ºï¼Œå´å­˜åœ¨å¯æ§æ€§ã€é€æ˜åº¦ä¸æ•ˆç‡ä¸è¶³ï¼Œä¸”èå…¥é¢†åŸŸçŸ¥è¯†éœ€å¤§é‡æ•°æ®ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨ç»“åˆäºŒè€…ä¼˜åŠ¿ï¼Œæå‡ºæ•´åˆLLMsä¸åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹è¯ç®¡ç†å™¨çš„æ¡†æ¶ï¼Œç”¨äºæœ‰ç‰¹å®šç›®æ ‡ï¼ˆå¦‚MIåœºæ™¯ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼‰çš„å¼€æ”¾å¼å¯¹è¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ··åˆæ¶æ„è®¾è®¡â€”â€”LLMsä¸RLå¯¹è¯ç®¡ç†å™¨ç»“åˆ
æå‡ºæ•´åˆå¤§è¯­è¨€æ¨¡å‹ä¸åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹è¯ç®¡ç†å™¨çš„æ–°é¢–æ¡†æ¶ï¼Œç”¨äºæœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ã€‚å€ŸåŠ©RLå¯¹è¯ç®¡ç†å™¨æ¥è°ƒæ§LLMsï¼Œåœ¨å‘æŒ¥LLMsç”Ÿæˆèƒ½åŠ›ä¸é€‚åº”æ€§çš„åŒæ—¶ï¼Œå¼¥è¡¥å…¶å¯æ§æ€§ç­‰æ–¹é¢çš„ä¸è¶³ï¼Œå®ç°é€‚åº”æ€§ä¸æ§åˆ¶æ„Ÿçš„å¹³è¡¡ï¼Œä¸ºè™šæ‹Ÿæ²»ç–—æ”¯æŒç±»å¯¹è¯ç³»ç»Ÿæä¾›æ–°æ€è·¯ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†å±‚å¼ºåŒ–å­¦ä¹ å»ºæ¨¡å¯¹è¯é˜¶æ®µ
åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ å¯¹å¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µï¼ˆå¦‚MIçš„engagementã€focusingã€evokingã€planningç­‰é˜¶æ®µï¼‰è¿›è¡Œå»ºæ¨¡ã€‚è®©ç³»ç»Ÿèƒ½åœ¨ä¸åŒå¯¹è¯é˜¶æ®µé—´æµç•…è¿‡æ¸¡ï¼Œåº”å¯¹MIè¿™ç±»éœ€åˆ†é˜¶æ®µæ¨è¿›ä¸”é˜¶æ®µé—´å¯èƒ½éä¸¥æ ¼çº¿æ€§çš„å¤æ‚å¯¹è¯åœºæ™¯ï¼Œè§£å†³ä¼ ç»ŸLLMsç¼ºä¹å¯¹è¯é˜¶æ®µç»“æ„åŒ–å†³ç­–çš„é—®é¢˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå…ƒå­¦ä¹ å¢å¼ºç”¨æˆ·ç”»åƒé€‚åº”æ€§
é‡‡ç”¨å…ƒå­¦ä¹ æ¥æå‡ç³»ç»Ÿå¯¹ä¸åŒç”¨æˆ·ç”»åƒï¼ˆå¦‚MIä¸­ä¸‰ç±»æ‚£è€…ï¼‰çš„é€‚åº”æ€§ã€‚ä½¿ç³»ç»Ÿèƒ½ä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ ï¼Œé’ˆå¯¹å¼‚è´¨æ‚£è€…éœ€æ±‚å®ç°å“åº”ä¸ªæ€§åŒ–ï¼Œå…‹æœä¼ ç»ŸåŸºäºè§„åˆ™ç³»ç»Ÿé€‚åº”æ€§å·®ä»¥åŠLLMsé€‚é…ä¸åŒç”¨æˆ·éœ€å¤§é‡æ•°æ®çš„ç¼ºé™·ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å°†æ‰€ææ¡†æ¶åº”ç”¨äºåŠ¨æœºå¼è®¿è°ˆï¼ˆMIï¼‰åœºæ™¯ä»¥ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼Œå®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹åŸºçº¿ç›¸æ¯”ï¼Œè¯¥å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œè¯æ˜äº†ç»™å¤§è¯­è¨€æ¨¡å‹å¢åŠ æ¡ä»¶çº¦æŸä»¥æ„å»ºæœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ç³»ç»Ÿå­˜åœ¨æ½œåœ¨ç›Šå¤„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ··åˆç³»ç»Ÿæ€è·¯ï¼šåœ¨éœ€ç»“åˆç”Ÿæˆèƒ½åŠ›ã€é€‚åº”æ€§ä¸é¢†åŸŸå¯æ§æ€§çš„åœºæ™¯ï¼ˆå¦‚åŒ»ç–—å’¨è¯¢ã€æ•™è‚²å¯¹è¯ç­‰å‚ç›´é¢†åŸŸï¼‰ï¼Œå¯å€Ÿé‰´è¿™ç§â€œå¤§æ¨¡å‹ + é¢†åŸŸç‰¹å®šå¯¹è¯ç®¡ç†å™¨â€çš„æ··åˆæ¶æ„ï¼Œå¹³è¡¡ä¸åŒæŠ€æœ¯èŒƒå¼çš„ä¼˜ç¼ºç‚¹ã€‚
2. åˆ†å±‚ä¸å…ƒå­¦ä¹ åº”ç”¨ï¼šé¢å¯¹æœ‰é˜¶æ®µç»“æ„ã€ç”¨æˆ·å·®å¼‚å¤§çš„å¯¹è¯ä»»åŠ¡ï¼Œåˆ†å±‚å¼ºåŒ–å­¦ä¹ å¯¹é˜¶æ®µå»ºæ¨¡å’Œå…ƒå­¦ä¹ å¯¹ç”¨æˆ·é€‚é…çš„æ€è·¯ï¼Œå¯ä¸ºæå‡ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ•ˆç‡ä¸ä¸ªæ€§åŒ–èƒ½åŠ›æä¾›å‚è€ƒã€‚
3. å‚ç›´é¢†åŸŸè½åœ°ï¼šåœ¨å¿ƒç†å¥åº·ç­‰èµ„æºç´§å¼ ä¸”å¯¹å¯¹è¯ä¸“ä¸šæ€§è¦æ±‚é«˜çš„å‚ç›´é¢†åŸŸï¼Œè¯¥æ¡†æ¶éªŒè¯äº†æŠ€æœ¯è¾…åŠ©äººç±»æœåŠ¡çš„å¯è¡Œæ€§ï¼Œä¸ºåç»­å¼€å‘è™šæ‹Ÿè¾…åŠ©å·¥å…·æä¾›äº†æŠ€æœ¯è·¯çº¿èŒƒä¾‹ã€‚

## scaffolding-dexterous-manipulation-with-vision-language-models
### Abstract
Dexterous robotic hands are essential for performing complex manipulation
tasks, yet remain difficult to train due to the challenges of demonstration
collection and high-dimensional control. While reinforcement learning (RL) can
alleviate the data bottleneck by generating experience in simulation, it
typically relies on carefully designed, task-specific reward functions, which
hinder scalability and generalization. Thus, contemporary works in dexterous
manipulation have often bootstrapped from reference trajectories. These
trajectories specify target hand poses that guide the exploration of RL
policies and object poses that enable dense, task-agnostic rewards. However,
sourcing suitable trajectories - particularly for dexterous hands - remains a
significant challenge. Yet, the precise details in explicit reference
trajectories are often unnecessary, as RL ultimately refines the motion. Our
key insight is that modern vision-language models (VLMs) already encode the
commonsense spatial and semantic knowledge needed to specify tasks and guide
exploration effectively. Given a task description (e.g., "open the cabinet")
and a visual scene, our method uses an off-the-shelf VLM to first identify
task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D
trajectories for hand motion and object motion. Subsequently, we train a
low-level residual RL policy in simulation to track these coarse trajectories
or "scaffolds" with high fidelity. Across a number of simulated tasks involving
articulated objects and semantic understanding, we demonstrate that our method
is able to learn robust dexterous manipulation policies. Moreover, we showcase
that our method transfers to real-world robotic hands without any human
demonstrations or handcrafted rewards.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºçµå·§æ“ä½œâ€œæ­è„šæ‰‹æ¶â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
çµå·§æœºæ¢°æ‰‹å¯¹æ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†ç”±äºç¤ºèŒƒæ•°æ®æ”¶é›†éš¾ã€é«˜ç»´æ§åˆ¶å¤æ‚ï¼Œè®­ç»ƒèµ·æ¥é¢‡å…·æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½èƒ½é€šè¿‡ä»¿çœŸç”Ÿæˆç»éªŒç¼“è§£æ•°æ®ç“¶é¢ˆï¼Œå´ä¾èµ–ç²¾å¿ƒè®¾è®¡çš„ç‰¹å®šä»»åŠ¡å¥–åŠ±å‡½æ•°ï¼Œé™åˆ¶äº†æ‰©å±•æ€§ä¸æ³›åŒ–æ€§ã€‚å½“ä¸‹çµå·§æ“ä½œç ”ç©¶å¸¸å€ŸåŠ©å‚è€ƒè½¨è¿¹å¼•å¯¼RLç­–ç•¥æ¢ç´¢ï¼Œå¯è·å–åˆé€‚è½¨è¿¹ï¼ˆå°¤å…¶é’ˆå¯¹çµå·§æ‰‹ï¼‰ä»æ˜¯éš¾é¢˜ï¼Œä¸”å‚è€ƒè½¨è¿¹çš„ç²¾ç¡®ç»†èŠ‚å¯¹æœ€ç»ˆç»RLä¼˜åŒ–çš„è¿åŠ¨è€Œè¨€å¹¶éå¿…éœ€ã€‚äºæ˜¯ï¼Œè®ºæ–‡æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç¼–ç çš„å¸¸è¯†æ€§ç©ºé—´ä¸è¯­ä¹‰çŸ¥è¯†ï¼Œæ¥ç”Ÿæˆå¼•å¯¼RLçš„â€œè„šæ‰‹æ¶â€è½¨è¿¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå€ŸåŠ©è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆâ€œè„šæ‰‹æ¶â€è½¨è¿¹  
ç»™å®šä»»åŠ¡æè¿°ï¼ˆå¦‚â€œæ‰“å¼€æ©±æŸœâ€ï¼‰å’Œè§†è§‰åœºæ™¯ï¼Œåˆ©ç”¨ç°æˆçš„VLMså…ˆè¯†åˆ«ä»»åŠ¡ç›¸å…³å…³é”®ç‚¹ï¼ˆå¦‚æŠŠæ‰‹ã€æŒ‰é’®ï¼‰ï¼Œå†åˆæˆæ‰‹å’Œç‰©ä½“è¿åŠ¨çš„3Dè½¨è¿¹ã€‚è¿™äº›è½¨è¿¹ä½œä¸ºâ€œç²—ç²’åº¦â€å¼•å¯¼ï¼Œæ— éœ€ç²¾ç¡®åˆ°äººç±»ç¤ºèŒƒçº§ç»†èŠ‚ï¼Œå´èƒ½ä¸ºRLæä¾›æ¢ç´¢æ–¹å‘ä¸å¥–åŠ±ä¾æ®ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ®‹å·®å¼ºåŒ–å­¦ä¹ ç­–ç•¥è·Ÿè¸ªè½¨è¿¹  
åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒä½å±‚æ¬¡æ®‹å·®RLç­–ç•¥ï¼Œè®©å…¶é«˜ç²¾åº¦è·Ÿè¸ªVLMsç”Ÿæˆçš„â€œè„šæ‰‹æ¶â€è½¨è¿¹ã€‚RLé€šè¿‡ä¼˜åŒ–æ¯ä¸€æ­¥çš„åç§»å’Œæ‰‹æŒ‡åŠ¨ä½œæ¥æœ€å¤§åŒ–è·Ÿè¸ªå¥–åŠ±ï¼Œæ— éœ€äººå·¥è®¾è®¡å¤æ‚å¥–åŠ±å‡½æ•°ï¼Œè¿˜èƒ½åœ¨è¿‡ç¨‹ä¸­è¶…è¶Šäººç±»é¥æ“ä½œçš„æ€§èƒ½ä¸ç²¾åº¦ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆ©ç”¨VLMsç‰¹æ€§æå‡æ³›åŒ–ä¸é²æ£’æ€§  
é€šè¿‡é‡å¤æŸ¥è¯¢VLMsï¼ŒéšæœºåŒ–åˆå§‹å…³é”®ç‚¹å’Œé«˜å±‚è½¨è¿¹ï¼Œè®©ç­–ç•¥åœ¨æµ‹è¯•æ—¶èƒ½æ³›åŒ–åˆ°æœªè§è¿‡çš„åˆå§‹æ¡ä»¶ä¸æ–°è½¨è¿¹ï¼›å½“VLMsé«˜å±‚è§„åˆ’æœ‰è¯¯å·®æ—¶ï¼Œæä¾›ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯å¤§å¹…æå‡æ€§èƒ½ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¶‰åŠé“°æ¥ç‰©ä½“å’Œè¯­ä¹‰ç†è§£çš„å¤šä¸ªä»¿çœŸä»»åŠ¡ä¸­ï¼Œè®ºæ–‡æ–¹æ³•åœ¨8ä¸ªä»»åŠ¡ä¸Šï¼Œæ— éœ€æ‰‹åŠ¨å¥–åŠ±è®¾è®¡å°±èƒ½è¾¾åˆ°æ¥è¿‘â€œ oracle çº§â€æ‰‹å·¥è½¨è¿¹çš„æˆåŠŸç‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æˆåŠŸå®ç°äº†å‘çœŸå®ä¸–ç•Œçµå·§æœºæ¢°æ‰‹çš„è·¨åŸŸè¿ç§»ï¼Œåœ¨æ— äººç±»ç¤ºèŒƒå’Œæ‰‹å·¥å¥–åŠ±æƒ…å†µä¸‹ä»èƒ½è¾¾æˆé²æ£’æ“ä½œæ€§èƒ½ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è·¨æ¨¡æ€åä½œæ€è·¯ï¼šå°†VLMsçš„è¯­ä¹‰ç©ºé—´æ¨ç†èƒ½åŠ›ä¸RLçš„æ§åˆ¶ä¼˜åŒ–èƒ½åŠ›ç»“åˆï¼Œä¸ºè§£å†³é«˜ç»´å¤æ‚æ§åˆ¶ä»»åŠ¡æä¾›äº†â€œé«˜å±‚è¯­ä¹‰å¼•å¯¼ + ä½å±‚ç²¾ç¡®ä¼˜åŒ–â€çš„æ–°èŒƒå¼ï¼Œå¯æ¨å¹¿åˆ°éœ€è¯­ä¹‰ç†è§£ä¸ç²¾ç»†æ“ä½œç»“åˆçš„æœºå™¨äººä»»åŠ¡ã€‚  
2. å¼±åŒ–å¯¹ç²¾ç¡®ç¤ºèŒƒçš„ä¾èµ–ï¼šè¯æ˜ç²—ç²’åº¦è½¨è¿¹è¶³ä»¥æ”¯æ’‘RLè®­ç»ƒï¼Œä¸ºå‡å°‘æœºå™¨äººå­¦ä¹ å¯¹å¤§è§„æ¨¡é«˜è´¨é‡ç¤ºèŒƒæ•°æ®é›†çš„ä¾èµ–æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œé™ä½ä»»åŠ¡æ‹“å±•æ—¶çš„æ•°æ®é‡‡é›†æˆæœ¬ã€‚  
3. æ³›åŒ–ä¸è¿ç§»æŠ€å·§ï¼šåˆ©ç”¨VLMsç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹æ¥å¢å¼ºæ³›åŒ–ã€é€šè¿‡ä¸Šä¸‹æ–‡ç¤ºä¾‹æå‡é²æ£’æ€§ç­‰æ‰‹æ®µï¼Œä¸ºåŸºäºå¤§æ¨¡å‹çš„æœºå™¨äººå­¦ä¹ åœ¨å®é™…éƒ¨ç½²ï¼ˆå¦‚ç¯å¢ƒå˜åŒ–ã€æ¨¡å‹è¯¯å·®åœºæ™¯ï¼‰æä¾›äº†å®ç”¨æ–¹æ³•è®ºã€‚

## longwriter-zero--mastering-ultra-long-text-generation-via-reinforcement-learning
### Abstract
Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B
### ğŸŒŸ è®ºæ–‡è§£è¯» | LongWriter-Zeroï¼šç”¨å¼ºåŒ–å­¦ä¹ çªç ´è¶…é•¿æ–‡æœ¬ç”Ÿæˆéš¾é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¶…é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚ä¸‡å­—çº§æŠ¥å‘Šã€å™äº‹åˆ›ä½œç­‰ï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…åœºæ™¯ä¸­è‡³å…³é‡è¦çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰æŠ€æœ¯é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯æ¨¡å‹ç”Ÿæˆé•¿åº¦å­˜åœ¨ä¸Šé™ï¼ŒäºŒæ˜¯éšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œå†…å®¹è´¨é‡ï¼ˆè¿è´¯æ€§ã€ä¸€è‡´æ€§ã€ç»“æ„åˆç†æ€§ç­‰ï¼‰ä¼šæ˜¾è‘—ä¸‹é™ã€‚  

æ­¤å‰ä¸»æµæ–¹æ¡ˆï¼ˆå¦‚LongWriterï¼‰ä¾èµ–**æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼Œå³åœ¨äººå·¥æ„é€ çš„â€œæŒ‡ä»¤ - é•¿æ–‡æœ¬è¾“å‡ºâ€é…å¯¹æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ä½†è¿™ç§æ–¹å¼å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š  
- æ„é€ é«˜è´¨é‡çš„åˆæˆSFTæ•°æ®æˆæœ¬æé«˜ã€éš¾åº¦å¤§ï¼›  
- åˆæˆæ•°æ®å¾€å¾€ç¼ºä¹è¿è´¯æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸”é£æ ¼å•ä¸€ã€è¿‡åº¦â€œäººå·¥åŒ–â€ï¼›  
- SFTçš„æœ€å¤§ä¼¼ç„¶ç›®æ ‡æ— æ³•æ˜¾å¼ä¼˜åŒ–å…¨å±€å±‚é¢çš„æ–‡æœ¬å±æ€§ï¼ˆå¦‚æ•´ä½“è¿è´¯æ€§ã€æ ¼å¼ä¸€è‡´æ€§ï¼‰ã€‚  

ä¸ºçªç ´è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡º**å®Œå…¨ä»é›¶å¼€å§‹ã€ä¸ä¾èµ–ä»»ä½•æ ‡æ³¨/åˆæˆæ•°æ®**çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ¡ˆï¼Œè®©LLMè‡ªä¸»â€œè¿›åŒ–â€å‡ºè¶…é•¿é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  


### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ¡†æ¶  
ä¼ ç»ŸSFTä¾èµ–å›ºå®šå‚è€ƒæ–‡æœ¬ï¼Œè€Œæœ¬æ–‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè®©æ¨¡å‹é€šè¿‡**å¥–åŠ±ä¿¡å·**ä¼˜åŒ–é•¿æ–‡æœ¬ç”Ÿæˆçš„å…¨å±€ç›®æ ‡ï¼ˆæ— éœ€äººå·¥æ„é€ SFTæ•°æ®é›†ï¼‰ã€‚å…·ä½“é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•è®­ç»ƒç­–ç•¥ç½‘ç»œï¼šä»åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Qwen2.5 - 32Bï¼‰å‡ºå‘ï¼Œè®©æ¨¡å‹åœ¨â€œå†™ä½œè¿‡ç¨‹ä¸­è‡ªä¸»è§„åˆ’ä¸è¿­ä»£â€ï¼Œé€æ­¥æŒæ¡è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç»´åº¦å¥–åŠ±æ¨¡å‹è®¾è®¡ï¼ˆReward Designï¼‰  
é’ˆå¯¹å¼€æ”¾åŸŸæ–‡æœ¬ç”Ÿæˆçš„å¤æ‚æ€§ï¼ˆä¸»è§‚æ€§ã€å¤šç»´åº¦ï¼‰ï¼Œè®¾è®¡**å¤åˆå¥–åŠ±å‡½æ•°**ï¼Œæ•´åˆå¤šä¸ªä¸“é¡¹å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œåˆ†åˆ«å¼•å¯¼æ¨¡å‹ä¼˜åŒ–ä»¥ä¸‹å…³é”®ç»´åº¦ï¼š  
- é•¿åº¦æ§åˆ¶ï¼ˆLength RMï¼‰ï¼šç¡®ä¿è¾“å‡ºæ»¡è¶³â€œè¶…é•¿â€éœ€æ±‚ï¼ŒåŒæ—¶é¿å…æ— æ„ä¹‰å†—ä½™ï¼›  
- å†™ä½œè´¨é‡ï¼ˆQuality RMï¼‰ï¼šè¯„ä¼°å†…å®¹æµç•…åº¦ã€é€»è¾‘æ€§ã€ä¸“ä¸šæ€§ç­‰ï¼›  
- ç»“æ„æ ¼å¼ï¼ˆStructure RMï¼‰ï¼šä¿éšœæ–‡æœ¬ç»“æ„åˆç†ï¼ˆå¦‚åˆ†ç« èŠ‚ã€å±‚æ¬¡æ¸…æ™°ï¼‰ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest - time Scalingï¼‰ä¸æŒç»­é¢„è®­ç»ƒï¼ˆContinual Pretrainingï¼‰  
- æµ‹è¯•æ—¶ç¼©æ”¾ï¼šå€Ÿé‰´å¤§æ¨¡å‹åœ¨æ•°å­¦/ä»£ç ä»»åŠ¡ä¸­â€œé•¿æ€ç»´é“¾ï¼ˆCoTï¼‰â€çš„æˆåŠŸç»éªŒï¼Œæ¢ç´¢åœ¨è¶…é•¿æ–‡æœ¬ç”Ÿæˆä¸­å¼•å…¥é•¿CoTï¼Œå¢å¼ºæ¨¡å‹æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ï¼›  
- æŒç»­é¢„è®­ç»ƒï¼šåœ¨é•¿æ–‡æœ¬ç´ æä¸æ¨ç†æ•°æ®ä¸ŠæŒç»­é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡RLè®­ç»ƒåæ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
- åŸºå‡†æµ‹è¯•ç¢¾å‹ä¼ ç»ŸSFTï¼šåŸºäºQwen2.5 - 32Bè®­ç»ƒçš„LongWriter - Zeroï¼Œåœ¨WritingBenchã€Arena - Writeç­‰é•¿æ–‡æœ¬å†™ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼Œ**å…¨é¢è¶…è¶Šä¼ ç»ŸSFTæ–¹æ³•**ï¼›  
- è¶…è¶Šåƒäº¿å‚æ•°æ¨¡å‹ï¼šåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡»è´¥DeepSeek R1ã€Qwen3 - 235Bç­‰ç™¾ billion + è§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œåˆ·æ–°SOTAï¼›  
- å¼€æºèµ„æºä¸°å¯Œï¼šæ¨¡å‹ checkpoint å’Œæ•°æ®å·²å¼€æºï¼ˆhttps://huggingface.co/THU - KEG/LongWriter - Zero - 32Bï¼‰ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¯å¤ç°ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. èŒƒå¼åˆ›æ–°ï¼šè¯æ˜å¼ºåŒ–å­¦ä¹ å¯åœ¨â€œæ— æ ‡æ³¨/åˆæˆæ•°æ®â€åœºæ™¯ä¸‹ï¼Œæ¿€æ´»LLMçš„è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºå¤§æ¨¡å‹èƒ½åŠ›è§£é”æä¾›äº†â€œéSFTâ€çš„æ–°èŒƒå¼ï¼›  
2. å¥–åŠ±å·¥ç¨‹ï¼šå¤šç»´åº¦å¤åˆå¥–åŠ±æ¨¡å‹çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–å¼€æ”¾åŸŸç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚åˆ›æ„å†™ä½œã€å¤šè½®å¯¹è¯ï¼‰ï¼Œç”¨äºåˆ»ç”»â€œä¸»è§‚æ€§å¼ºã€æ— æ˜ç¡®ground - truthâ€åœºæ™¯ä¸‹çš„è´¨é‡è¯„ä¼°ï¼›  
3. è®­ç»ƒç­–ç•¥ï¼šæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆé•¿CoTï¼‰ä¸æŒç»­é¢„è®­ç»ƒçš„ç»„åˆï¼Œä¸ºæå‡å¤§æ¨¡å‹é•¿æ–‡æœ¬æ¨ç†ã€ç”Ÿæˆçš„ä¸Šé™æä¾›äº†å¯å¤ç”¨çš„æŠ€æœ¯è·¯çº¿ï¼›  
4. è½åœ°ä»·å€¼ï¼šé’ˆå¯¹çœŸå®ä¸–ç•Œâ€œè¶…é•¿æ–‡æœ¬éœ€æ±‚â€ï¼ˆå¦‚æŠ¥å‘Šæ’°å†™ã€æ³•å¾‹æ–‡ä¹¦ã€æ•™è‚²å†…å®¹åˆ›ä½œï¼‰ï¼Œæä¾›äº†æ›´ä¼˜è´¨çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæ¨åŠ¨LLMåœ¨ä¸“ä¸šé¢†åŸŸçš„è½åœ°ã€‚  


LongWriter - Zeroçš„å·¥ä½œä¸ä»…è§£å†³äº†è¶…é•¿æ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ç—›ç‚¹ï¼Œæ›´å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹èƒ½åŠ›è¿›åŒ–ä¸­çš„æ½œåŠ›â€”â€”æ— éœ€ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨ï¼Œä¹Ÿèƒ½è®©æ¨¡å‹â€œè‡ªä¸»å­¦ä¹ â€å¤æ‚ä»»åŠ¡çš„å®Œæˆèƒ½åŠ›ã€‚è¿™ä¸ºå¤§æ¨¡å‹ç ”å‘èŒƒå¼ã€å¥–åŠ±æœºåˆ¶è®¾è®¡ç­‰æ–¹å‘ï¼Œéƒ½å¸¦æ¥äº†æå…·å¯å‘æ€§çš„å‚è€ƒã€‚

## harnessing-the-power-of-reinforcement-learning-for-language-model-based-information-retriever-via-query-document-co-augmentation
### Abstract
Recent studies have proposed leveraging Large Language Models (LLMs) as
information retrievers through query rewriting. However, for challenging
corpora, we argue that enhancing queries alone is insufficient for robust
semantic matching; the LLM should also have sufficient understanding of the
corpus by directly handling and augmenting the documents themselves. To this
end, we present an LLM-based retriever empowered to augment both user queries
and corpus documents, with its policy fully explored via reinforcement learning
(RL) and minimal human inductive bias. Notably, we find that simply allowing
the LLM to modify documents yields little benefit unless paired with our
carefully designed bidirectional RL framework, which enables the LLM to
simultaneously learn and collaborate on both query and document augmentation
policies. A key technical challenge in realizing such a framework lies in
jointly updating both policies during training, where the rewards for the two
directions depend on each other, making their entangled reward intractable. Our
approach addresses this by introducing a reward sampling strategy and a
specifically designed RL algorithm that enables effective training with these
sampled rewards. Experimental results demonstrate that our approach
significantly enhances LLM-based retrieval performance in both sparse and dense
settings, particularly in difficult retrieval domains, and achieves strong
cross-benchmark generalization. Our code is released at
https://github.com/liujm2001/CoAugRetriever.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼ºåŒ–å­¦ä¹ èµ‹èƒ½å¤§æ¨¡å‹æ£€ç´¢ï¼š query-document åŒå‘ååŒå¢å¼ºæ–°èŒƒå¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰AIåœºæ™¯ä¸­è‡³å…³é‡è¦ï¼Œä¼ ç»Ÿæ–¹æ³•é€šè¿‡æŸ¥è¯¢é‡å†™æå‡æ£€ç´¢ç²¾åº¦ï¼Œä½†é¢å¯¹å¤æ‚è¯­æ–™åº“æ—¶ä»…ä¼˜åŒ–æŸ¥è¯¢ä¸å¤Ÿï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€å¯¹æ–‡æ¡£æœ¬èº«ä¹Ÿæœ‰å……åˆ†ç†è§£ã€‚ç°æœ‰åŸºäºLLMçš„æ£€ç´¢å¤šèšç„¦æŸ¥è¯¢æ”¹å†™ï¼Œåœ¨æŒ‘æˆ˜æ€§é¢†åŸŸï¼ˆå¦‚ç´§å‡‘è¯­æ–™ç²¾å‡†æ£€ç´¢ï¼‰æ€§èƒ½ä»æœ‰æå‡ç©ºé—´ï¼Œå› æ­¤æœ¬æ–‡æå‡ºè®©LLMåŒæ—¶å¢å¼ºæŸ¥è¯¢ä¸æ–‡æ¡£ä»¥å®ç°æ›´é²æ£’è¯­ä¹‰åŒ¹é…çš„æ€è·¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šQuery-Document ååŒå¢å¼ºèŒƒå¼  
çªç ´ä»…ä¼˜åŒ–æŸ¥è¯¢çš„å±€é™ï¼Œè®©LLMåŒæ—¶å¯¹ç”¨æˆ·æŸ¥è¯¢å’Œè¯­æ–™åº“æ–‡æ¡£è¿›è¡Œå¢å¼ºï¼Œé€šè¿‡ç›´æ¥å¤„ç†ä¸æ”¹å†™æ–‡æ¡£ï¼Œä½¿LLMå……åˆ†ç†è§£è¯­æ–™ï¼Œæ‹‰è¿‘æŸ¥è¯¢ä¸æ–‡æ¡£è¯­ä¹‰å…³è”ï¼Œæ‹“å®½LLMåœ¨æ£€ç´¢ä»»åŠ¡ä¸­çš„â€œè¡ŒåŠ¨ç©ºé—´â€ï¼Œä¸ºæŒ‘æˆ˜æ€§è¯­æ–™æ£€ç´¢æ¢ç´¢æ›´ä¼˜ç­–ç•¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒå‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶è®¾è®¡  
å•çº¯è®©LLMä¿®æ”¹æ–‡æ¡£æ”¶ç›Šæœ‰é™ï¼Œéœ€é…å¥—ç²¾å¿ƒè®¾è®¡çš„åŒå‘RLæ¡†æ¶ã€‚è®­ç»ƒæ—¶è¦è§£å†³æŸ¥è¯¢ä¸æ–‡æ¡£å¢å¼ºç­–ç•¥ç›¸äº’ä¾èµ–ã€å¥–åŠ±çº ç¼ éš¾å¤„ç†çš„é—®é¢˜ï¼Œä¸ºæ­¤æå‡º**å¥–åŠ±é‡‡æ ·ç­–ç•¥**ä¸å®šåˆ¶åŒ–RLç®—æ³•ï¼Œè®©æ¨¡å‹èƒ½åœ¨å•æµç¨‹ä¸­è”åˆå­¦ä¹ åŒå‘å¢å¼ºç­–ç•¥ï¼Œå®ç°æŸ¥è¯¢ä¸æ–‡æ¡£å¢å¼ºçš„åä½œï¼Œå…‹æœå¤§åŠ¨ä½œç©ºé—´ä¸‹ç²¾ç¡®å¥–åŠ±è®¡ç®—éš¾é¢˜ã€‚è¿˜é‡‡ç”¨â€œbatch - unbatchäº¤æ›¿â€å®ç°æ–¹å¼é€‚é…ç°æœ‰LLM RLæ¡†æ¶ï¼Œä¿éšœåŒå‘è®¾ç½®ä¸‹é«˜æ•ˆè®­ç»ƒã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç¨€ç–ï¼ˆå¦‚åŸºäºTF - IDFã€BM25ï¼‰å’Œç¨ å¯†ï¼ˆåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åµŒå…¥ï¼‰æ£€ç´¢åœºæ™¯çš„æŒ‘æˆ˜æ€§IRåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡åŸºäºLLMçš„æ£€ç´¢æ€§èƒ½ï¼Œå°¤å…¶åœ¨é«˜éš¾åº¦æ£€ç´¢é¢†åŸŸæ•ˆæœçªå‡ºï¼›åŒæ—¶å±•ç°å¼ºè·¨åŸºå‡†æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜é€šè¿‡å¼ºåŒ–å­¦ä¹ è®©LLMè‡ªæ¢ç´¢è·å¾—äº†IRä»»åŠ¡é€šç”¨èƒ½åŠ›ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. ä»»åŠ¡è§†è§’æ‹“å±•ï¼šæ‰“ç ´ä¿¡æ¯æ£€ç´¢ä¸­â€œé‡æŸ¥è¯¢è½»æ–‡æ¡£â€çš„ä¼ ç»Ÿä¼˜åŒ–æƒ¯æ€§ï¼Œå¯ç¤ºåœ¨å…¶ä»–éœ€åŒå‘/å¤šå‘äº¤äº’ä¼˜åŒ–çš„ä»»åŠ¡ï¼ˆå¦‚å¤šæ¨¡æ€åŒ¹é…ã€å¯¹è¯ç³»ç»Ÿä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰ä¸­ï¼Œæ€è€ƒå¯¹åŒæ–¹/å¤šæ–¹å¯¹è±¡åŒæ—¶å¢å¼ºçš„å¯èƒ½æ€§ã€‚  
2. å¼ºåŒ–å­¦ä¹ åº”ç”¨åˆ›æ–°ï¼šé¢å¯¹å¤šç­–ç•¥çº ç¼ ã€å¥–åŠ±éš¾å¤„ç†çš„å¤æ‚åœºæ™¯ï¼Œæœ¬æ–‡â€œå¥–åŠ±é‡‡æ · + å®šåˆ¶ç®—æ³• + å·¥ç¨‹åŒ–å®ç°é€‚é…â€çš„æ€è·¯ï¼Œä¸ºRLåœ¨å¤§æ¨¡å‹å¤æ‚ä»»åŠ¡ï¼ˆå¦‚å¤šè½®å¯¹è¯ç­–ç•¥å­¦ä¹ ã€å¤šæ™ºèƒ½ä½“åä½œï¼‰è½åœ°æä¾›äº†å¤„ç†é«˜ç»´åŠ¨ä½œç©ºé—´ä¸çº ç¼ å¥–åŠ±çš„å‚è€ƒèŒƒå¼ã€‚  
3. æ³›åŒ–èƒ½åŠ›é‡è§†ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­å…³æ³¨è·¨åŸºå‡†æ³›åŒ–ï¼Œè¿™ç§è¿½æ±‚é€šç”¨èƒ½åŠ›è€Œéä»…å•ä»»åŠ¡æœ€ä¼˜çš„æ€è·¯ï¼Œç¬¦åˆå¤§æ¨¡å‹æ—¶ä»£æŠ€æœ¯è½åœ°å¯¹é€šç”¨æ€§ã€é²æ£’æ€§çš„éœ€æ±‚ï¼Œå€¼å¾—åŒç±»ç ”ç©¶å€Ÿé‰´ã€‚

## adapthink--adaptive-thinking-preferences-for-reasoning-language-model
### Abstract
Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdapThinkï¼šè®©æ¨ç†è¯­è¨€æ¨¡å‹æ‹¥æœ‰è‡ªé€‚åº”â€œæ€è€ƒåå¥½â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒæå¤§æå‡äº†è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå‚¬ç”Ÿäº†ç²¾ç»†çš„è‡ªæˆ‘åæ€è¿‡ç¨‹ï¼ˆå³â€œæ…¢æ€è€ƒâ€èŒƒå¼ï¼‰ã€‚ä½†è¿™ä¸€èŒƒå¼å­˜åœ¨æ¨ç†æ•ˆç‡éš¾é¢˜ï¼šæ¨¡å‹é¢å¯¹ç®€å•é—®é¢˜æ—¶å¯èƒ½è¿‡åº¦è®¡ç®—ï¼Œé¢å¯¹å¤æ‚é—®é¢˜æ—¶åˆå¯èƒ½è¿‡æ—©åˆ‡æ¢æ¨ç†æ€è·¯ã€‚ä»¥å¾€æœºåˆ¶ä¾èµ–é™æ€é•¿åº¦é¢„ç®—æˆ–é¢„å®šä¹‰è§„åˆ™ï¼Œç¼ºä¹å¯¹é—®é¢˜å¤æ‚åº¦å˜åŒ–å’Œæ¨¡å‹èƒ½åŠ›æ¼”è¿›çš„é€‚åº”æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§è‡ªé€‚åº”åè®­ç»ƒæ¡†æ¶ï¼Œåœ¨ç»´æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶æå‡æ•ˆç‡ï¼ŒAdapThink åº”è¿è€Œç”Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°  
è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°æ¥è°ƒæ•´æ¨¡å‹å½“å‰æ¨ç†åå¥½ã€‚æ¨¡å‹å¯åŸºäºç”Ÿæˆå“åº”çš„ç»„å†…å‡†ç¡®ç‡ï¼Œç¡®å®šåˆé€‚çš„åæ€åå¥½ï¼›åŒæ—¶é€šè¿‡ç»Ÿè®¡è®­ç»ƒæ ·æœ¬ç»„ä¸­å…³é”®è¿‡æ¸¡è¯æ•°é‡ï¼Œå®šé‡è¡¡é‡æ¨ç†æ•ˆç‡ï¼Œæ— éœ€ä¾èµ–å›ºå®šé•¿åº¦åå¥½ï¼Œè€Œæ˜¯åˆ©ç”¨æ¨¡å‹ç½®ä¿¡åº¦ä¸å“åº”ç‰¹å¾åŠ¨æ€è°ƒæ•´åæ€ç±»è¿‡æ¸¡è¯çš„åå¥½ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶  
æå‡ºå¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶å¹³è¡¡è®­ç»ƒæ ·æœ¬ç»„çš„è§£é¢˜å‡†ç¡®ç‡ä¸æ¨ç†å¤šæ ·æ€§ã€‚å…ˆå¯¹æ¨ç†å®ä¾‹è¿‡é‡‡æ ·ï¼Œå†ç”¨ç²¾å¿ƒå®šä¹‰çš„å¤šæ ·æ€§æŒ‡æ ‡è¯„ä¼°å®ä¾‹çš„æœ€ç»ˆç­”æ¡ˆä¸ä¸­é—´æ­¥éª¤ï¼Œæœ€åé€šè¿‡å¤šæ ·æ€§æ„ŸçŸ¥ä¸‹é‡‡æ ·æ¥ç­›é€‰å’Œæå‡ç”¨äº RL åè®­ç»ƒçš„å®ä¾‹æ•´ä½“è´¨é‡ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ DeepSeek è’¸é¦æ¨¡å‹è¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒAdapThink åœ¨ä½¿æ¨¡å‹å…·å¤‡è‡ªé€‚åº”æ¨ç†æ¨¡å¼ã€ç¼“è§£æ¨ç†ä½æ•ˆé—®é¢˜ä¸Šå±•ç°ä¼˜åŠ¿ã€‚å¦‚å¯¹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ä»… 2K token çš„ DeepSeek - è’¸é¦ Qwen æ¨¡å‹è¿›è¡Œåè®­ç»ƒåï¼Œåœ¨ 8K - token é™åˆ¶ä¸‹æµ‹è¯•ï¼Œç›¸æ¯”å¤šä¸ªé•¿åº¦æ§åˆ¶åŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œè¯æ˜äº†å…¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ‰“é€ å¼ºå¤§ä¸”é«˜æ•ˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. æ‰“ç ´é™æ€é•¿åº¦é™åˆ¶æ€ç»´ï¼Œé€šè¿‡åˆ†ææ ·æœ¬ç»„æ¨ç†æ¨¡å¼åˆ†å¸ƒæ¥è°ƒæ•´é•¿åº¦åå¥½ï¼Œä¸ºå¤„ç†æ¨¡å‹æ¨ç†æ•ˆç‡ä¸å¤æ‚åº¦é€‚é…é—®é¢˜æä¾›äº†æ–°è§†è§’ï¼Œä¸å†å±€é™äºå›ºå®šè§„åˆ™çº¦æŸã€‚  
2. ç»„ç›¸å¯¹å¥–åŠ±å‡½æ•°ä¸å¤šæ ·æ€§æ„ŸçŸ¥é‡‡æ ·æœºåˆ¶çš„è®¾è®¡æ€è·¯ï¼Œå¯è¿ç§»åˆ°å…¶ä»–éœ€è¦å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡ã€å…¼é¡¾å¤šæ ·æ€§ä¸å‡†ç¡®æ€§çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯æ¶‰åŠåæ€ã€æ¨ç†ç±»çš„è¯­è¨€æ¨¡å‹ä¼˜åŒ–åœºæ™¯ã€‚  
3. å¯¹æ¨ç†è¿‡ç¨‹ä¸­å…³é”®è¿‡æ¸¡è¯ï¼ˆå¦‚â€œPause - Validationâ€â€œBranch - Extensionâ€ç±»è¯æ±‡ï¼‰çš„åˆ†æä¸åˆ©ç”¨æ–¹å¼ï¼Œä¸ºåç»­ç ”ç©¶å¦‚ä½•ä»è¯­è¨€è¡¨è¾¾ç‰¹å¾å±‚é¢å¼•å¯¼æ¨¡å‹æ¨ç†è¡Œä¸ºæä¾›äº†å‚è€ƒèŒƒå¼ã€‚

## rl-for-reasoning-by-adaptively-revealing-rationales
### Abstract
We propose that reinforcement learning (RL) from partial expert
demonstrations is not merely a training heuristic, but a promising framework
for solving complex sequence generation tasks. Supervised fine-tuning (SFT)
relies on dense ground-truth labels, which become increasingly costly as
sequence length grows. RL, on the other hand, struggles with sparse rewards and
a combinatorially large output space. We address this by introducing adaptive
backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals
only a partial prefix of the target output during training. The supervision
length is adjusted dynamically for each sample based on the model's past reward
signal, allowing it to incrementally learn to complete reasoning chains by
conditioning on correct partial solutions. We investigate this intermediate
regime between SFT and RL and argue that per-sample curriculum learning is more
than a trade-off between efficiency and generality, it can succeed in tasks
with long sequences of latent dependencies where SFT and RL both fail to
generalize. Using a synthetic task with latent parity constraints, we show that
our adaptive curriculum over partial answers reliably solves problems that are
otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we
find that curriculum learning enables models to solve problems that RL alone
cannot, acquiring new reasoning capabilities through incremental exposure to
partial solutions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªé€‚åº”æ­ç¤ºæ¨ç†ä¾æ®ï¼šç”¨å¼ºåŒ–å­¦ä¹ çªç ´å¤æ‚åºåˆ—ç”Ÿæˆå›°å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤æ‚åºåˆ—ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†ã€ç®—æ³•æ¨ç†ç­‰ï¼‰ä¸­ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾å±€é™ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¾èµ–å¯†é›†çš„çœŸå®æ ‡ç­¾ï¼Œåºåˆ—è¶Šé•¿æ ‡æ³¨æˆæœ¬è¶Šé«˜ï¼›å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ™å—é™äºç¨€ç–å¥–åŠ±ä¸æŒ‡æ•°çº§å¢é•¿çš„è¾“å‡ºç©ºé—´ï¼Œæ¢ç´¢æœ‰æ•ˆè§£çš„éš¾åº¦éšåºåˆ—é•¿åº¦æŒ‡æ•°çº§ä¸Šå‡ã€‚åŒæ—¶ï¼Œè·å–ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚æ•°å­¦ï¼‰çš„å¤§è§„æ¨¡é«˜è´¨é‡æ¨ç†è½¨è¿¹æˆæœ¬æé«˜ï¼Œè€Œæ ‡å‡†RLåœ¨é•¿æ¨ç†é“¾ä»»åŠ¡ä¸­æ˜“é™·å…¥â€œåªå¼ºåŒ–å·²æœ‰è·¯å¾„ã€éš¾æ¢ç´¢æ–°è§£â€çš„å›°å¢ƒã€‚å› æ­¤ï¼Œè®ºæ–‡æ¢ç´¢SFTä¸RLä¹‹é—´çš„ä¸­é—´èŒƒå¼ï¼Œå°è¯•é€šè¿‡**åˆ†é˜¶æ®µæ­ç¤ºéƒ¨åˆ†æ¨ç†ä¾æ®**æ¥è®©æ¨¡å‹é€æ­¥å­¦ä¹ å¤æ‚æ¨ç†ï¼Œè§£å†³SFTå’ŒRLéƒ½éš¾ä»¥æ³›åŒ–çš„é•¿éšä¾èµ–åºåˆ—ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAdaBackè‡ªé€‚åº”å›æº¯ç®—æ³•  
AdaBackæ˜¯ä¸€ç§**é€æ ·æœ¬çš„è¯¾ç¨‹å­¦ä¹ ç®—æ³•**ï¼Œè®­ç»ƒæ—¶ä»…æš´éœ²ç›®æ ‡è¾“å‡ºçš„éƒ¨åˆ†å‰ç¼€ä½œä¸ºç›‘ç£ï¼Œä¸”åŸºäºæ¨¡å‹è¿‡å¾€å¥–åŠ±ä¿¡å·åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ ·æœ¬çš„ç›‘ç£é•¿åº¦ã€‚å®ƒè®©æ¨¡å‹ä»â€œä¾èµ–è¾ƒå¤šå‰ç¼€æç¤ºå®Œæˆæ¨ç†â€é€æ­¥è¿‡æ¸¡åˆ°â€œä»…éœ€å°‘é‡æç¤ºç”šè‡³è‡ªä¸»æ¨ç†â€ï¼ŒæŠŠé•¿æ¨ç†é“¾æ‹†è§£ä¸ºå¯é€æ­¥æ”»å…‹çš„å­é—®é¢˜ï¼Œå®ç°ä»å…¨ç›‘ç£åˆ°å…¨æ¢ç´¢çš„è‡ªé€‚åº”è¿‡æ¸¡ï¼Œæ— éœ€äººå·¥è®¾è®¡è¯¾ç¨‹é˜¶æ®µæˆ–è°ƒåº¦ç­–ç•¥ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¶èµ·SFTä¸RLçš„æ¡¥æ¢  
èšç„¦SFTï¼ˆä¾èµ–å…¨ç›‘ç£ï¼‰å’ŒRLï¼ˆä¾èµ–ç¨€ç–å¥–åŠ±ï¼‰ä¹‹é—´çš„â€œä¸­é—´åœ°å¸¦â€ï¼Œé€šè¿‡**åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ ·æœ¬çš„ç›‘ç£æš´éœ²ç¨‹åº¦**ï¼Œè®©æ¨¡å‹åœ¨â€œéƒ¨åˆ†æ­£ç¡®è§£æç¤ºâ€ä¸‹å¢é‡å­¦ä¹ æ¨ç†é“¾ã€‚è¿™ç§æ–¹å¼æ—¢ç¼“è§£äº†SFTçš„æ ‡æ³¨å‹åŠ›ï¼Œåˆè§£å†³äº†RLåœ¨é•¿åºåˆ—ä¸‹å¥–åŠ±ç¨€ç–ã€æ¢ç´¢å›°éš¾çš„é—®é¢˜ï¼Œè®©æ¨¡å‹èƒ½å­¦ä¹ åˆ°SFTå’ŒRLå•ç‹¬è®­ç»ƒå­¦ä¸ä¼šçš„æ¨ç†èƒ½åŠ›ã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé¢å‘é€šç”¨ç¦»æ•£åºåˆ—çš„è‡ªé€‚åº”ç­–ç•¥  
ä¸åŒäºä¾èµ–â€œæ˜ç¡®æ¨ç†æ­¥éª¤åˆ†éš”ç¬¦â€çš„æ–¹æ³•ï¼ˆå¦‚R3éœ€ç‰¹å®šåˆ†éš”ç¬¦åˆ†æ®µï¼‰ï¼ŒAdaBackä¸è¦æ±‚æ¨ç†æ­¥éª¤æ¸…æ™°å¯åˆ†ï¼Œè€Œæ˜¯åŸºäº**æ¯ä¸ªæ ·æœ¬è‡ªèº«éš¾åº¦**åšè‡ªé€‚åº”è°ƒæ•´ï¼ˆå€ŸåŠ©GRPOæ¡†æ¶å¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šè½®rolloutå¹¶å¹³å‡å¥–åŠ±æ¥ä¼°è®¡éš¾åº¦ï¼‰ï¼Œæ›´é€‚é…æ— æ˜ç¡®æ­¥éª¤åˆ’åˆ†çš„é€šç”¨ç¦»æ•£åºåˆ—ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦ç«èµ›é¢˜MATHæ•°æ®é›†ï¼‰ã€‚  


### ğŸ“ˆ å®éªŒç»“æœ
1. åˆæˆä»»åŠ¡ï¼ˆéšå¥‡å¶çº¦æŸä»»åŠ¡ï¼‰ï¼šAdaBackèƒ½ç¨³å®šè§£å†³SFTã€RLåŠäºŒè€…ç»„åˆéƒ½æ— æ³•å¤„ç†çš„é—®é¢˜ï¼ŒéªŒè¯äº†â€œæ‹†åˆ†é•¿æ¨ç†é“¾ä¸ºå­é—®é¢˜ã€åŠ¨æ€è°ƒæ•´ç›‘ç£â€ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚  
2. æ•°å­¦æ¨ç†åŸºå‡†ï¼ˆMATHã€GSM8kï¼‰ï¼šAdaBackåœ¨æ ‡å‡†RLå’ŒSFT+RL pipelineä¸Šè¡¨ç°æ›´ä¼˜ï¼›å¯¹åŸºç¡€æ¨¡å‹ç›´æ¥åº”ç”¨AdaBackï¼Œæ€§èƒ½å¸¸èƒ½åŒ¹é…SFTåˆå§‹åŒ–çš„æ¨¡å‹ã€‚  
3. æ³›åŒ–æ€§æµ‹è¯•ï¼ˆGSM8kå˜ç§ï¼‰ï¼šåœ¨â€œBase - 7ï¼ˆæ¨¡å‹é¢„è®­ç»ƒæœªè§è¿‡çš„æ•°å€¼æ ¼å¼ï¼‰â€å’Œâ€œTensor - 2ï¼ˆæ‹¼æ¥é—®é¢˜å¢åŠ æ¨ç†æ·±åº¦ï¼‰â€ä»»åŠ¡ä¸­ï¼ŒAdaBackå±•ç°å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åº”å¯¹ç¬¦å·å˜åŒ–ä¸é•¿ horizon æ¨ç†ã€‚  
4. Pass@kæŒ‡æ ‡éªŒè¯ï¼šAdaBackæ˜¾è‘—æå‡Pass@kï¼ˆå°¤å…¶æ— SFTæ—¶ï¼‰ï¼Œè¯´æ˜å®ƒä¸æ˜¯ç®€å•é‡åŠ æƒå·²æœ‰è§£ï¼Œè€Œæ˜¯è®©æ¨¡å‹å‘ç°äº†æ–°æ¨ç†è·¯å¾„ã€‚  


### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. è¯¾ç¨‹å­¦ä¹ çš„è‡ªé€‚åº”æ€è·¯ï¼šåœ¨éœ€â€œé€æ­¥è§£é”å¤æ‚åº¦â€çš„ä»»åŠ¡ä¸­ï¼Œå¯å‚è€ƒâ€œä¾æ¨¡å‹åé¦ˆåŠ¨æ€è°ƒæ•´ç›‘ç£/æç¤ºé‡â€çš„èŒƒå¼ï¼Œé¿å…äººå·¥è®¾è®¡è¯¾ç¨‹çš„ç¹çä¸åƒµåŒ–ã€‚  
2. SFTä¸RLçš„ä¸­é—´æ€æ¢ç´¢ï¼šå½“ä»»åŠ¡æ—¢éœ€â€œç›‘ç£å¼•å¯¼â€åˆéœ€â€œè‡ªä¸»æ¢ç´¢â€æ—¶ï¼Œâ€œéƒ¨åˆ†æ­ç¤ºæ­£ç¡®è§£+RLä¼˜åŒ–â€çš„æ€è·¯ä¸ºå¹³è¡¡äºŒè€…æä¾›äº†æ–°æ–¹å‘ã€‚  
3. é•¿ä¾èµ–åºåˆ—ä»»åŠ¡çš„è§£æ³•ï¼šé¢å¯¹æ ‡æ³¨è´µã€æ¢ç´¢éš¾çš„é•¿æ¨ç†/é•¿ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€æ•°å­¦è¯æ˜ï¼‰ï¼Œæ‹†åˆ†ä»»åŠ¡+åŠ¨æ€ç›‘ç£çš„æ¨¡å¼æˆ–èƒ½çªç ´ç°æœ‰æ–¹æ³•ç“¶é¢ˆã€‚  
4. æ³›åŒ–æ€§ä¸æ–°è§£å‘ç°ï¼šè‹¥å¸Œæœ›æ¨¡å‹çªç ´â€œå¤ç”¨å·²æœ‰è·¯å¾„â€ã€çœŸæ­£å­¦ä¼šæ–°æ¨ç†é€»è¾‘ï¼ŒAdaBackå¼çš„â€œåŸºäºåé¦ˆè°ƒæ•´ç›‘ç£ï¼Œé©±åŠ¨æ¢ç´¢æ–°è§£â€æœºåˆ¶å€¼å¾—å€Ÿé‰´ã€‚  

## aligning-frozen-llms-by-reinforcement-learning--an-iterative-reweight-then-optimize-approach
### Abstract
Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œç”¨å¼ºåŒ–å­¦ä¹ å¯¹é½å†»ç»“å¤§æ¨¡å‹ï¼šè¿­ä»£é‡åŠ æƒä¼˜åŒ–æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½äººç±»åå¥½é€šå¸¸ä¾èµ–RLHFã€DPOç­‰å¾®è°ƒæ–¹æ³•ï¼Œè¿™ç±»æ–¹æ³•éœ€ç›´æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œåœ¨æµ‹è¯•é˜¶æ®µæ— æ³•æå‡æ€§èƒ½ï¼Œä¸”æ¨¡å‹æƒé‡ä¸å¯è®¿é—®æ—¶ä¹Ÿæ— æ³•ä½¿ç”¨ã€‚æµ‹è¯•æ—¶å¯¹é½æ–¹æ³•è™½ç»•å¼€æƒé‡æ›´æ–°ï¼Œä½†æ¨ç†æˆæœ¬é«˜ï¼Œä¸”åŸºäºä¸å®Œå–„å¥–åŠ±æˆ–ä»·å€¼å‡½æ•°çš„ä¸€æ¬¡æ€§æŒ‡å¯¼æ˜“äº§ç”Ÿæ¬¡ä¼˜è¾“å‡ºã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡èšç„¦äºï¼šç»™å®šå†»ç»“LLMå’Œç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ï¼Œå¦‚ä½•åœ¨æµ‹è¯•æ—¶é«˜æ•ˆæ”¹è¿›æˆ–å®šåˆ¶æ¨¡å‹ï¼ŒåŒæ—¶æœ€å°åŒ–æ¨ç†æˆæœ¬ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºè¿­ä»£é‡åŠ æƒ - ä¼˜åŒ–ï¼ˆIROï¼‰æ¡†æ¶  
IROæ˜¯å—å¼ºåŒ–å­¦ä¹ å¯å‘çš„æ¡†æ¶ï¼Œæ— éœ€ä¿®æ”¹å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°å°±èƒ½å®ç°ç±»RLçš„å¯¹é½ã€‚è®­ç»ƒæ—¶é€šè¿‡ä¸‰æ­¥è¿­ä»£ï¼šä»åŸºç¡€æ¨¡å‹é‡‡æ ·å€™é€‰ã€ç”¨å½“å‰ä»·å€¼å‡½æ•°é‡é‡‡æ ·ã€è®­ç»ƒæ–°è½»é‡çº§ä»·å€¼å‡½æ•°æŒ‡å¯¼ä¸‹ä¸€æ¬¡è§£ç ï¼›æµ‹è¯•æ—¶åˆ©ç”¨å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°åºåˆ—ï¼Œé€šè¿‡åŸºäºæœç´¢çš„ä¼˜åŒ–è¿‡ç¨‹å¼•å¯¼åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‚  

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç±»ç­–ç•¥è¿­ä»£ä¸é«˜æ•ˆæ¨ç†  
ä»ç†è®ºä¸Šè¯æ˜ï¼Œåœ¨æ¸©å’Œæ¡ä»¶ä¸‹IROå±äºä¸€ç§ç­–ç•¥è¿­ä»£ï¼Œæµ‹è¯•æ—¶èƒ½ä»¥æŒ‡æ•°çº§æ›´å°‘çš„tokensè¾¾åˆ°Best - of - Nï¼ˆBoNï¼‰æœç´¢çš„æ€§èƒ½ã€‚ä¸”ç”¨æˆ·å¯ç±»ä¼¼OpenAIçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ï¼Œç”¨IROåœ¨è‡ªæœ‰æ•°æ®é›†ä¸Šä»¥RLé£æ ¼å¾®è°ƒæ¨¡å‹ï¼Œè¿˜æ— éœ€è®¿é—®æ¨¡å‹æƒé‡ã€‚  

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AlpacaEval 2.0ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­ï¼ŒIROæ˜¾è‘—æé«˜äº†é•¿åº¦æ§åˆ¶èƒœç‡ã€‚å¦‚Llama - 3 - 8B - Instructä»30.71%æå‡åˆ°43.80%ï¼ŒMeta - Llama - 3 - 70B - Instructä»43.11%æå‡åˆ°49.77%ï¼ˆä¸GPT - 4å“åº”å¯¹æ¯”ï¼‰ã€‚æ­¤å¤–ï¼Œå³ä½¿ä½¿ç”¨å°å°ºå¯¸ï¼ˆ1Bæˆ–7Bï¼‰ä»·å€¼å‡½æ•°å¼•å¯¼å¤§åŸºç¡€æ¨¡å‹ï¼ˆ6.9Bæˆ–70Bï¼‰ï¼ŒIROä¹ŸæŒç»­ä¼˜äºBoNã€weak - to - strong searchç­‰ç°æœ‰æµ‹è¯•æ—¶å¯¹é½åŸºçº¿ã€‚  

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹äºæ— æ³•è·å–æ¨¡å‹æƒé‡ä½†éœ€åœ¨æµ‹è¯•é˜¶æ®µä¼˜åŒ–æ¨¡å‹å¯¹é½äººç±»åå¥½çš„åœºæ™¯ï¼ŒIROæä¾›äº†æ–°èŒƒå¼ï¼Œå…¶è¿­ä»£è®­ç»ƒè½»é‡ä»·å€¼å‡½æ•° + æµ‹è¯•æ—¶æœç´¢ä¼˜åŒ–çš„æ€è·¯ï¼Œä¸ºä½æ¨ç†æˆæœ¬ä¸‹çš„æ¨¡å‹æ€§èƒ½æå‡å’Œé¢†åŸŸé€‚é…å¼€è¾Ÿè·¯å¾„ï¼›ç†è®ºä¸Šå¯¹ç­–ç•¥è¿­ä»£çš„è¯æ˜ï¼Œä¹Ÿä¸ºåç»­å¼ºåŒ–å­¦ä¹ åœ¨å†»ç»“æ¨¡å‹å¯¹é½æ–¹å‘çš„ç ”ç©¶æä¾›äº†ç†è®ºæ”¯æ’‘ï¼›åŒæ—¶åœ¨å·¥ä¸šç•Œï¼Œç±»ä¼¼OpenAI RFTä½†æ— éœ€æƒé‡è®¿é—®çš„ç‰¹æ€§ï¼Œè®©ä¼ä¸šåœ¨è‡ªæœ‰æ•°æ®å®šåˆ¶æ¨¡å‹æ—¶æ›´å…·çµæ´»æ€§ã€‚

