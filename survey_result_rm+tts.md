# Paper List of Terms(Reward Model+Test Time Scaling)
- [25/07] **Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS**  
[[Paper](http://arxiv.org/pdf/2507.05557v1)] [[Code/Page]()] [[TLDR/Notes](#enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts)]

- [25/07] **Test-Time Scaling with Reflective Generative Model**  
[[Paper](http://arxiv.org/pdf/2507.01951v2)] [[Code/Page](https://github.com/MetaStone-AI/MetaStone-S1.)] [[TLDR/Notes](#test-time-scaling-with-reflective-generative-model)]

- [25/06] **Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.23056v1)] [[Code/Page](https://github.com/HICAI-ZJU/K-MSE.)] [[TLDR/Notes](#boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning)]

- [25/06] **ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**  
[[Paper](http://arxiv.org/pdf/2506.18896v1)] [[Code/Page](https://github.com/Gen-Verse/ReasonFlux)] [[TLDR/Notes](#reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms)]

- [25/06] **Fake it till You Make it: Reward Modeling as Discriminative Prediction**  
[[Paper](http://arxiv.org/pdf/2506.13846v2)] [[Code/Page](https://github.com/Visualignment/GAN-RM.)] [[TLDR/Notes](#fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction)]

- [25/06] **$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts**  
[[Paper](http://arxiv.org/pdf/2506.15733v1)] [[Code/Page]()] [[TLDR/Notes](#$\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts)]

- [25/06] **EQA-RM: A Generative Embodied Reward Model with Test-time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.10389v1)] [[Code/Page](https://github.com/UNITES-Lab/EQA-RM.)] [[TLDR/Notes](#eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling)]

- [25/06] **Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models**  
[[Paper](http://arxiv.org/pdf/2506.09532v1)] [[Code/Page]()] [[TLDR/Notes](#athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models)]

- [25/06] **Learning to Reason Across Parallel Samples for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2506.09014v1)] [[Code/Page]()] [[TLDR/Notes](#learning-to-reason-across-parallel-samples-for-llm-reasoning)]

- [25/06] **Guided Speculative Inference for Efficient Test-Time Alignment of LLMs**  
[[Paper](http://arxiv.org/pdf/2506.04118v1)] [[Code/Page](https://github.com/j-geuter/GSI)] [[TLDR/Notes](#guided-speculative-inference-for-efficient-test-time-alignment-of-llms)]

- [25/06] **Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2506.03136v1)] [[Code/Page](https://github.com/Gen-Verse/CURE)] [[TLDR/Notes](#co-evolving-llm-coder-and-unit-tester-via-reinforcement-learning)]

- [25/06] **Incentivizing LLMs to Self-Verify Their Answers**  
[[Paper](http://arxiv.org/pdf/2506.01369v1)] [[Code/Page](https://github.com/mansicer/self-verification.)] [[TLDR/Notes](#incentivizing-llms-to-self-verify-their-answers)]

- [25/05] **DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.20241v2)] [[Code/Page](https://github.com/coder-qicao/DreamPRM.)] [[TLDR/Notes](#dreamprm--domain-reweighted-process-reward-model-for-multimodal-reasoning)]

- [25/06] **From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2506.00027v1)] [[Code/Page]()] [[TLDR/Notes](#from-mathematical-reasoning-to-code--generalization-of-process-reward-models-in-test-time-scaling)]

- [25/05] **Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence**  
[[Paper](http://arxiv.org/pdf/2505.20325v1)] [[Code/Page]()] [[TLDR/Notes](#guided-by-gut--efficient-test-time-scaling-with-reinforced-intrinsic-confidence)]

- [25/05] **Value-Guided Search for Efficient Chain-of-Thought Reasoning**  
[[Paper](http://arxiv.org/pdf/2505.17373v1)] [[Code/Page]()] [[TLDR/Notes](#value-guided-search-for-efficient-chain-of-thought-reasoning)]

- [25/05] **J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge**  
[[Paper](http://arxiv.org/pdf/2505.11875v1)] [[Code/Page]()] [[TLDR/Notes](#j1--exploring-simple-test-time-scaling-for-llm-as-a-judge)]

- [25/05] **Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards**  
[[Paper](http://arxiv.org/pdf/2505.02686v2)] [[Code/Page](https://github.com/bobxwu/learning-from-rewards-llm-papers.)] [[TLDR/Notes](#sailing-by-the-stars--a-survey-on-reward-models-and-learning-strategies-for-learning-from-rewards)]

- [25/05] **A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law**  
[[Paper](http://arxiv.org/pdf/2505.02665v2)] [[Code/Page]()] [[TLDR/Notes](#a-survey-of-slow-thinking-based-reasoning-llms-using-reinforced-learning-and-inference-time-scaling-law)]

- [25/04] **Process Reward Models That Think**  
[[Paper](http://arxiv.org/pdf/2504.16828v3)] [[Code/Page](https://github.com/mukhal/thinkprm.)] [[TLDR/Notes](#process-reward-models-that-think)]

- [25/04] **Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.15275v2)] [[Code/Page](https://github.com/CJReinforce/PURE.)] [[TLDR/Notes](#stop-summation--min-form-credit-assignment-is-all-process-reward-model-needs-for-reasoning)]

- [25/04] **Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators**  
[[Paper](http://arxiv.org/pdf/2504.15253v2)] [[Code/Page]()] [[TLDR/Notes](#evaluating-judges-as-evaluators--the-jetts-benchmark-of-llm-as-judges-as-test-time-scaling-evaluators)]

- [25/04] **Adaptive Rectification Sampling for Test-Time Compute Scaling**  
[[Paper](http://arxiv.org/pdf/2504.01317v1)] [[Code/Page]()] [[TLDR/Notes](#adaptive-rectification-sampling-for-test-time-compute-scaling)]

- [25/04] **When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.01005v1)] [[Code/Page](https://github.com/nishadsinghi/sc-genrm-scaling.)] [[TLDR/Notes](#when-to-solve--when-to-verify--compute-optimal-problem-solving-and-generative-verification-for-llm-reasoning)]

- [25/04] **GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning**  
[[Paper](http://arxiv.org/pdf/2504.00891v2)] [[Code/Page](https://ryanliu112.github.io/GenPRM.)] [[TLDR/Notes](#genprm--scaling-test-time-compute-of-process-reward-models-via-generative-reasoning)]

- [25/03] **Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute**  
[[Paper](http://arxiv.org/pdf/2503.23803v2)] [[Code/Page](https://github.com/yingweima2022/SWE-Reasoner)] [[TLDR/Notes](#thinking-longer--not-larger--enhancing-software-engineering-agents-via-scaling-test-time-compute)]

- [25/03] **MetaScale: Test-Time Scaling with Evolving Meta-Thoughts**  
[[Paper](http://arxiv.org/pdf/2503.13447v1)] [[Code/Page]()] [[TLDR/Notes](#metascale--test-time-scaling-with-evolving-meta-thoughts)]

- [25/03] **Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding**  
[[Paper](http://arxiv.org/pdf/2503.01422v1)] [[Code/Page]()] [[TLDR/Notes](#sampling-efficient-test-time-scaling--self-estimating-the-best-of-n-sampling-in-early-decoding)]

- [25/02] **AgentRM: Enhancing Agent Generalization with Reward Modeling**  
[[Paper](http://arxiv.org/pdf/2502.18407v1)] [[Code/Page]()] [[TLDR/Notes](#agentrm--enhancing-agent-generalization-with-reward-modeling)]

- [25/02] **Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning**  
[[Paper](http://arxiv.org/pdf/2502.17407v1)] [[Code/Page]()] [[TLDR/Notes](#linguistic-generalizability-of-test-time-scaling-in-mathematical-reasoning)]

- [25/02] **Process Reward Models for LLM Agents: Practical Framework and Directions**  
[[Paper](http://arxiv.org/pdf/2502.10325v1)] [[Code/Page](https://github.com/sanjibanc/agent_prm.)] [[TLDR/Notes](#process-reward-models-for-llm-agents--practical-framework-and-directions)]

- [25/02] **Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2502.06703v1)] [[Code/Page]()] [[TLDR/Notes](#can-1b-llm-surpass-405b-llm--rethinking-compute-optimal-test-time-scaling)]

- [25/02] **Teaching Language Models to Critique via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2502.03492v1)] [[Code/Page]()] [[TLDR/Notes](#teaching-language-models-to-critique-via-reinforcement-learning)]

- [25/02] **STAIR: Improving Safety Alignment with Introspective Reasoning**  
[[Paper](http://arxiv.org/pdf/2502.02384v2)] [[Code/Page](https://github.com/thu-ml/STAIR.)] [[TLDR/Notes](#stair--improving-safety-alignment-with-introspective-reasoning)]

- [25/01] **SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling**  
[[Paper](http://arxiv.org/pdf/2501.19306v3)] [[Code/Page]()] [[TLDR/Notes](#sets--leveraging-self-verification-and-self-correction-for-improved-test-time-scaling)]

- [25/01] **PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament**  
[[Paper](http://arxiv.org/pdf/2501.13007v2)] [[Code/Page]()] [[TLDR/Notes](#pairjudge-rm--perform-best-of-n-sampling-with-knockout-tournament)]

- [25/01] **InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model**  
[[Paper](http://arxiv.org/pdf/2501.12368v2)] [[Code/Page](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward)] [[TLDR/Notes](#internlm-xcomposer2-5-reward--a-simple-yet-effective-multi-modal-reward-model)]

- [25/01] **MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow Thinking**  
[[Paper](http://arxiv.org/pdf/2501.12051v2)] [[Code/Page](https://github.com/pixas/MedSSS.)] [[TLDR/Notes](#meds$^3$--towards-medical-small-language-models-with-self-evolved-slow-thinking)]

- [25/01] **ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding**  
[[Paper](http://arxiv.org/pdf/2501.07861v1)] [[Code/Page]()] [[TLDR/Notes](#rearter--retrieval-augmented-reasoning-with-trustworthy-process-rewarding)]

- [25/01] **URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics**  
[[Paper](http://arxiv.org/pdf/2501.04686v5)] [[Code/Page](https://github.com/URSA-MATH.)] [[TLDR/Notes](#ursa--understanding-and-verifying-chain-of-thought-reasoning-in-multimodal-mathematics)]

- [24/11] **Enhancing LLM Reasoning with Reward-guided Tree Search**  
[[Paper](http://arxiv.org/pdf/2411.11694v4)] [[Code/Page]()] [[TLDR/Notes](#enhancing-llm-reasoning-with-reward-guided-tree-search)]



# TLDR/Notes
## enhancing-test-time-scaling-of-large-language-models-with-hierarchical-retrieval-augmented-mcts
### Abstract
Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.
### 🌟 论文解读 | 分层检索增强MCTS，解锁大模型推理时缩放新姿势

### 📌 背景痛点/本文动机
大语言模型（LLMs）推理能力提升传统上依赖训练时大规模计算，而测试时缩放（Test - Time Scaling，TTS）作为互补范式，通过推理时分配额外计算资源增强推理能力。现有基于搜索的TTS方法（如MCTS结合过程奖励模型PRM）存在局限：依赖预训练信息易陷入局部最优或探索盲区，且仅靠PRM评估步骤难以捕捉全局策略和语义关系，导致奖励信号稀疏或次优，影响复杂推理任务效率与准确性。因此需要更有效通用的推理缩放方法，在无需大量额外训练下增强推理能力并提升鲁棒性与适应性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：双层次检索增强上下文学习（粗粒度层面）
提出深度逻辑检索（Deep Logical Retrieval），从复杂推理问题中提取抽象模板，检索相似问题 - 答案对。这些相似对为模型提供多样示例，助力模型捕捉问题结构的潜在模式与变异性，进而提升上下文学习效果，增强对未见过问题的适应性。

💡 创新点2：分层增强推理MCTS（细粒度层面）
在蒙特卡洛树搜索（MCTS）过程中，R2 - LLMs从外部数学问题数据集动态检索相关中间解决步骤，用相似先验知识丰富推理过程。结合这些检索步骤后，PRM能做出更具信息性和上下文一致性的评估，降低无效探索风险，同时该方法无缝整合上下文级推理增强与步骤级树搜索方法，利用PRM优化候选生成与决策以提升推理准确性。

### 📈 实验结果
在MATH500、GSM8K和OlympiadBench - TO数据集上，使用LLaMA - 3.1 - 8B模型时，与基线相比R2 - LLMs实现了显著相对提升，提升幅度最高达16%；在LLaMA 3.1 - 8B和Qwen 2 - 7B等策略模型上评估，也优于基于上下文学习（ICL）和基于树搜索的基线方法，证明了方法在复杂推理任务中的有效性。

### 💬 可借鉴之处
1. 分层检索增强思路：将检索在推理时的作用分层设计，粗粒度抓问题结构模式、细粒度补中间步骤知识，为多粒度利用外部知识辅助推理提供了参考范式。
2. 检索与MCTS结合：把外部检索引入MCTS过程来辅助PRM评估，为改进基于搜索的TTS方法中奖励信号不足、探索低效等问题提供了创新解法，后续可借鉴这种外部知识赋能搜索过程的思路拓展更多推理场景。
3. 无CoT蒸馏依赖：无需从更先进模型蒸馏获取思维链训练数据，降低了方法应用门槛，在资源有限或难获取高级模型蒸馏数据时，该轻量（相对）增强推理的方式值得参考。

## test-time-scaling-with-reflective-generative-model
### Abstract
We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3-mini's performance via the new Reflective Generative Form.
The new form focuses on high-quality reasoning trajectory selection and
contains two novelties: 1) A unified interface for policy and process reward
model: we share the backbone network and use task-specific heads for reasoning
trajectory predicting and scoring respectively, introducing only 53M extra
parameters for trajectory scoring. 2) Eliminating the reliance on process-level
annotation: we provide a self-supervised process reward model, which can
directly learn the high-quality reasoning trajectory selection from the outcome
reward. Equipped with the reflective generative form, MetaStone-S1 is naturally
suitable for test-time scaling, and we provide three reasoning effort modes
(low, medium, and high) based on the controllable thinking length. Experiments
demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI
o3-mini's series with only 32B parameter size. To support the research
community, we have open-sourced MetaStone-S1 at
https://github.com/MetaStone-AI/MetaStone-S1.
### 🌟 论文解读 | 全新反射生成模型MetaStone - S1：小参数实现大性能，探索测试时缩放新范式

### 📌 背景痛点/本文动机
在大语言模型（LLMs）领域快速发展的背景下，OpenAI的o3模型等借助测试时缩放（Test - Time Scaling，TTS）技术实现了先进的推理和编码能力。TTS分为内部TTS和外部TTS，内部TTS存在训练阶段结果奖励误分类正确答案（即假阳性推理过程）的问题；外部TTS虽能更有效提升性能，但现有方法在模型架构和训练方式上仍有改进空间，比如过程奖励模型（PRM）训练成本高、依赖特定标注等。本文聚焦外部TTS，旨在提出新的反射生成形式来实现高质量推理轨迹选择，提升模型性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出全新反射生成形式（Reflective Generative Form）
系统梳理现有TTS范式后，定义了用于高质量推理轨迹选择的反射生成形式。该形式让单个网络能同时实现推理轨迹预测与选择，且无需过程级标注。具体而言，为策略和过程奖励模型打造统一接口，共享骨干网络，针对推理轨迹预测和评分分别使用特定任务头，仅为轨迹评分引入53M额外参数，在模型架构层面实现高效复用与创新设计。

💡 创新点2：自监督过程奖励模型消除过程级标注依赖
提供自监督的过程奖励模型，能够从结果奖励中直接学习高质量推理轨迹选择。不再依赖昂贵且难以获取的过程级标注数据，通过自监督方式让模型自主学习推理轨迹的优劣判断，降低了数据标注成本与难度，同时提升了模型对推理过程的学习能力。

💡 创新点3：支持可控思考长度的多推理努力模式
基于反射生成形式，MetaStone - S1设置了低、中、高三种推理努力模式（依据可控的思考长度区分），让模型能根据不同任务场景和资源限制灵活调整推理策略，自然适配测试时缩放需求，提升模型在不同场景下的适用性与效率。

### 📈 实验结果
实验表明，仅32B参数规模的MetaStone - S1在性能上能与OpenAI o3 - mini系列相媲美。具体来看，MetaStone - S1 - low在数学（AIME24&25）、编码（LiveCodeBench）和中文推理（C - Eval）任务上分别超越OpenAI o3 - mini - low；MetaStone - S1 - medium取得与OpenAI o3 - mini - medium相近的结果；MetaStone - S1 - high进一步提升了智能上限，在一系列开源和闭源模型中取得SOTA结果。

### 💬 可借鉴之处
1. 模型架构复用与创新：反射生成形式中共享骨干网络、设计特定任务头的思路，为后续多任务模型或需要同时实现生成与评估功能的模型架构设计提供了参考，可在保证性能的同时有效控制参数规模与训练成本。
2. 自监督学习在奖励模型的应用：其自监督过程奖励模型摆脱过程级标注依赖的做法，为解决奖励模型训练数据获取难、成本高的问题提供了新方向，后续在构建各类奖励模型（如用于强化学习的奖励模型等）时可借鉴自监督学习思路来减少对标注数据的依赖。
3. 可控推理模式设计：针对不同场景设计多推理努力模式的策略，让模型能灵活适配测试时需求，这对打造面向实际复杂场景、资源可变的大语言模型应用系统具有启发意义，可根据用户设备性能、任务紧急程度等动态调整模型推理策略。
4. 小参数模型追平大模型性能：证明了32B参数规模模型能在性能上比肩同类知名模型，说明模型性能提升不只是单纯堆参数，合理的架构设计、训练策略等同样关键，为后续中小规模模型的研发提供了信心与方向，引导研究者关注模型内在机制优化而非一味追求参数规模。

## boosting-llm-s-molecular-structure-elucidation-with-knowledge-enhanced-tree-search-reasoning
### Abstract
Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.
### 🌟 论文解读 | 用知识增强树搜索推理助力大模型攻克分子结构解析难题

### 📌 背景痛点/本文动机
分子结构解析是化学实验分析里的关键任务，要从核磁、红外等光谱数据推导分子结构，专业人员都得花10 - 15分钟分析单个分子，所以用大语言模型（LLM）自动化解析很有必要。但LLM在这任务上有挑战：一是对化学分子结构空间覆盖不足，像噻吩这类特殊杂环结构，LLM常因缺乏子结构知识误判；二是没法准确评估和修正推理过程，树搜索推理需要有效评估反馈，可LLM缺领域知识，做不好 reward model 角色。于是论文要解决这两个问题，提升LLM在分子结构解析的能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出K - MSE框架  
构建知识增强的分子结构解析推理框架K - MSE，把蒙特卡洛树搜索（MCTS）作为插件实现测试时的能力扩展，能适配任意LLM。借助MCTS平衡新解探索和已有解利用，还结合Self - Refine让LLM及时优化之前的解。

💡 创新点2：外部分子子结构知识库  
为弥补LLM化学结构空间覆盖不足，构建外部分子子结构知识库。子结构是化学空间基础元素，知识库通过自动化流程整合子结构和结构描述，给LLM补充领域知识，提升特殊结构推理准确性，减少 atypical 案例错误。

💡 创新点3：专属分子 - 光谱评分器  
设计分子 - 光谱评分器当 reward model，解决LLM解评估不准问题。评分器有分子编码器和光谱编码器，评估分子结构和光谱数据匹配度给奖励分。它还作为LLM和知识库间的检索器，用输入光谱查最相关子结构，减少子结构检索误差，增强推理稳定性。

### 📈 实验结果
在MolPuzzle基准测试上，K - MSE方法效果显著，对GPT - 4o - mini和GPT - 4o都带来超20%的性能提升，证明了框架在增强LLM分子结构解析能力上的有效性。

### 💬 可借鉴之处
1. 领域知识增强思路：面对专业领域任务，LLM通用知识不足时，构建领域子结构知识库补充，这种“外部知识 + LLM”模式可复用在其他专业领域（如生物、材料）任务。  
2. 推理过程评估优化：设计领域专属评分器做 reward model，结合树搜索框架优化推理，为需要深度推理、需评估反馈的复杂任务（如数学证明、代码调试）提供了“评分器 + 树搜索”的推理增强范式。  
3. 插件化框架设计：K - MSE作为插件适配任意LLM，这种解耦式设计方便技术落地，不同场景下可快速集成到现有LLM工作流里，降低技术迁移成本。

## reasonflux-prm--trajectory-aware-prms-for-long-chain-of-thought-reasoning-in-llms
### Abstract
Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux
### 🌟 论文解读 | ReasonFlux-PRM：面向大模型长思维链推理的轨迹感知型过程奖励模型

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的复杂推理场景（如数学解题）中，Process Reward Models（PRMs，过程奖励模型）是监督中间推理步骤的有力工具。不过现有PRMs存在明显局限：它们主要基于模型最终输出训练，难以对**轨迹 - 响应（trajectory - response）**这类新兴输出形式的中间推理轨迹进行鲁棒评估。像Deepseek - R1等前沿推理模型会生成“冗长、欠规整的中间思考轨迹 + 简洁最终响应”的轨迹 - 响应对，这类数据常被用于小模型蒸馏，但现有PRMs因与中间轨迹在结构、格式上不匹配，且训练时缺乏带奖励的轨迹 - 响应数据，在监督这类数据时效果不佳甚至会损害下游训练。所以，如何让PRMs既能监督最终响应，又能有效评估中间思考轨迹，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出轨迹感知的PRM——ReasonFlux - PRM  
ReasonFlux - PRM专为评估轨迹 - 响应型推理痕迹设计，融合了**步骤级（step - level）**和**轨迹级（trajectory - level）**监督。它在涵盖数学和科学推理的10k高质量轨迹 - 响应对 curated 数据集上训练，能为思考轨迹内的每个步骤提供细粒度奖励作为监督信号，让模型中间思考轨迹与最终响应更对齐，解决了现有PRMs对中间轨迹监督能力不足的问题。  

💡 创新点2：多场景适配的奖励监督  
ReasonFlux - PRM适配离线和在线多种场景：  
- 离线场景：为轨迹 - 响应对打分，筛选高质量数据，助力小模型下游有监督微调的训练数据精选；  
- 在线场景：融入GRPO等策略优化过程，为强化学习（RL）中的策略优化提供细粒度过程奖励；  
- 测试时缩放（test - time scaling）：通过奖励引导的Best - of - N策略，评估多个生成响应并选最优，提升推理性能。  


### 📈 实验结果
在AIME、MATH500、GPQA - Diamond等挑战性下游基准测试中，ReasonFlux - PRM展现出优异性能：  
- 数据选择方面：ReasonFlux - PRM - 7B比强基线（如Qwen2.5 - Math - PRM - 72B）和人工策划基线选出的数据集质量更高；  
- 性能提升方面：ReasonFlux - PRM - 7B在有监督微调中平均提升12.1%，强化学习中平均提升4.5%，测试时缩放中平均提升6.3%；  
- 资源友好型发布：还发布了ReasonFlux - PRM - 1.5B，适配资源受限场景与边缘部署。  


### 💬 可借鉴之处
1. 问题定义与分析角度：针对新兴的轨迹 - 响应蒸馏数据趋势，深入分析现有PRMs在监督中间轨迹时的问题（结构格式不匹配、训练数据缺失），这种从产业新数据形态反推技术痛点的思路，为后续研究锚定方向提供参考；  
2. 多粒度监督融合：将步骤级和轨迹级监督结合，为处理“长链条、多阶段”的推理类任务提供了细粒度奖励设计的范例，可迁移到代码生成、复杂决策等需分步评估的场景；  
3. 多场景工程落地：从离线数据筛选、在线RL优化到测试时增强，完整覆盖大模型训练 - 推理全流程的奖励监督，展示了技术方案在产业级落地中的多维度价值，为打造端到端的大模型推理增强管线提供了实践模板；  
4. 资源分层发布：同时提供7B和1.5B规模模型，兼顾高性能与资源受限场景，体现了技术普惠性，在实际业务中可根据算力、延迟等需求灵活选择，平衡效果与成本。  

## fake-it-till-you-make-it--reward-modeling-as-discriminative-prediction
### Abstract
An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
### 🌟 论文解读 | 告别繁琐标注：GAN - RM 让奖励建模“以假乱真”

### 📌 背景痛点/本文动机
在视觉生成模型的训练后增强中，奖励模型至关重要。然而当前奖励建模方法存在诸多难题：一是构建奖励模型需大量人工标注偏好数据，收集成本高昂，且基于特定生成模型输出域标注的数据，在应用到不同输出域模型时存在域差距；二是为全面评估生成内容质量，需人工设计多种评估指标，既增加工程成本，又难在不同维度间取得最优平衡，还难保证与人类普遍偏好契合。因此，本文受生成对抗网络（GAN）中对抗训练启发，提出 GAN - RM 框架，旨在摆脱手动偏好标注和显式质量维度设计，高效构建奖励模型。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：无需手动偏好标注，利用少量代理数据
GAN - RM 仅需少量（几百个）无标注的代表性样本（即偏好代理数据，Preference Proxy Data）作为外部数据。通过训练奖励模型区分偏好代理数据和生成模型输出，让模型学习评估生成样本。同时采用基于排名的自举策略，将 GAN - RM 在这些样本上的置信分数作为软标签，利用额外数据再训练 GAN - RM，使其更好捕捉潜在人类偏好。
💡 创新点2：支持多轮训练，迭代对齐偏好
GAN - RM 支持多轮训练后优化。每一轮中，将被识别为接近偏好代理数据的样本用于生成器的训练后优化，反过来再训练判别器以区分这些更难的样本。这种迭代的“以假乱真”过程能逐步让生成质量与偏好代理数据中的潜在人类偏好对齐。

### 📈 实验结果
实验表明，基于 GAN - RM 的方法在性能上可与依赖大量标注数据（如 Pickapic 的 100 万标注人类偏好数据）的方法（如相关对比方法）相当甚至超越。在图像质量实验设置中，GAN - RM 仅需 500 个偏好代理数据样本。除图像质量提升实验外，在图像安全和视频质量增强场景下的实验也凸显了 GAN - RM 框架在不同场景下的泛化能力，验证了其在测试时缩放（如 Best - of - N 样本过滤）、监督微调（SFT）和直接偏好优化（DPO）等训练后方法中的有效性。

### 💬 可借鉴之处
从方法创新角度，GAN - RM 为解决奖励建模中数据获取难、依赖特定域、人工设计维度难契合人类偏好等问题提供了新思路，其利用对抗训练和少量代理数据的方式，减少了对大规模人工标注的依赖，降低工程成本；从应用拓展角度，该框架在图像、视频等多场景的有效实验，为视觉生成模型在不同领域的训练后增强提供了可复用的奖励建模范式，后续在视觉生成相关任务中，若需构建奖励模型，可借鉴其利用少量代理数据和对抗训练的思路来降低成本与难度。

## $\texttt{specs}$--faster-test-time-scaling-through-speculative-drafts
### Abstract
Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.
### 🌟 论文解读 | SPECS：用“推测草稿”加速大模型推理，平衡延迟与精度

### 📌 背景痛点/本文动机
大语言模型（LLM）推理能力的提升常依赖“测试时算力扩容”，比如分配更多计算资源做更充分的探索。但算力增加往往导致用户侧延迟升高，直接影响体验。现有测试时扩容方法多聚焦算力（FLOPS）优化精度，却忽略延迟约束。此外，基于Transformer的LLM自回归采样延迟常受限于内存加载而非总算力，而推测解码虽能借小模型提候选 token 降延迟，却会增加总计算量。于是，论文试图回答：**能否设计高效测试时扩容方法，优化延迟 - 效用权衡？**

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 SPECS 算法框架  
SPECS 受推测解码启发，是一种“延迟感知型”测试时扩容方法。它用**更小更快的草稿模型**高效生成候选序列，再结合**更大的目标模型**与**专用奖励模型**评估候选。整体遵循“草稿 - 选择”流程：迭代生成响应块，每轮用草稿模型生成候选块，经打分选择后拼接，进入下一轮；若草稿全被拒，则切换目标模型生成候选。

💡 创新点2：奖励引导的软验证与延迟机制  
- 奖励引导软验证（SUBSAMPLE 子例程）：基于草稿、目标、奖励模型计算的“分数”选候选块，既优化效用 - 延迟权衡，也避免简单丢弃高奖励但可能被 naive 推测解码漏掉的轨迹。  
- 奖励感知延迟规则（CASCADE 子例程）：自适应决定下一轮用草稿还是目标模型生成候选——让大模型处理难题步骤，小模型处理简单步骤，动态平衡算力与延迟。  

💡 创新点3：理论分析保障收敛性  
从理论上分析，SPECS 在结合草稿、目标、奖励模型优化“KL 正则化奖励最大化”目标时，其软验证方法随 beam 宽度增大，能优雅收敛到最优解。


### 📈 实验结果
论文在 MATH500、AMC23、OlympiadBench 数据集测试，用 Qwen - 1.5B - Instruct（草稿模型）、Qwen - 7B - Instruct（目标模型）与 Qwen - 7B - Math - PRM（奖励模型）验证：  
- 精度层面：SPECS 匹配甚至超越 beam search 精度；  
- 延迟层面：延迟最多降低约 19.1%，在精度与延迟间实现更优权衡。  


### 💬 可借鉴之处
1. **延迟 - 精度权衡思路**：跳出“只看算力/精度”的思维定式，把延迟作为核心约束，为大模型落地低延迟场景（如个性化交互）提供新思路；  
2. **多模型协作范式**：用“小草稿模型 + 大目标模型 + 奖励模型”分层协作，既利用小模型提速，又靠大模型保精度，还借奖励模型做灵活选择，这种“分工”模式可迁移到其他需平衡资源与效果的任务；  
3. **理论 + 实验双验证**：从理论证明收敛性，再用真实数据集验证，为方法可靠性背书，也示范了学术研究中“方法 - 理论 - 实验”闭环的重要性。  


SPECS 为大模型推理的“延迟 - 精度”难题提供了一套兼具创新性与实用性的解法，无论是工业界落地低延迟 LLM 应用，还是学术界探索测试时优化新方向，都有不少可借鉴的闪光点~

## eqa-rm--a-generative-embodied-reward-model-with-test-time-scaling
### Abstract
Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.
### 🌟 论文解读 | EQA - RM：为具身问答量身定制的生成式奖励模型，实现测试时可扩展评估

### 📌 背景痛点/本文动机
奖励模型（RMs）在大模型对齐中至关重要，但在复杂的具身任务（如具身问答，EQA）中却未得到充分探索。EQA需要智能体在3D环境中通过多模态观察和动作序列来感知、交互和推理以回答问题，对智能体的空间、时间和逻辑理解进行细致评估至关重要，而通用的奖励模型方法无法满足这一需求。现有通用奖励模型多为静态输入或简单结果设计，难以捕捉具身任务中固有的时空和逻辑依赖关系，因此迫切需要专门的机制来准确评估EQA的多方面成功指标。同时，EQA领域缺乏用于严格评估和比较奖励模型的标准化基准，当前EQA任务基准侧重于粗略的成功指标，而非对奖励模型发展至关重要的细粒度轨迹质量评估。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出EQA - RM生成式多模态奖励模型
EQA - RM是专为评估EQA轨迹而设计的新型多模态奖励模型，作为生成式奖励模型（GenRM），不仅能产生标量奖励，还能为评估提供明确的推理过程。其具有增强的空间、时间和推理处理能力，以处理EQA任务中固有的独特多模态数据流。通过高效的两阶段训练过程，第一阶段用标准的Rejective Finetuning（RFT）教会模型期望的输出格式（包含文本批评和标量分数）；第二阶段采用创新的Contrastive Group Relative Policy Optimization（C - GRPO）强化学习策略，解决仅RFT可能只学格式不学内容的问题，利用基于规则的对比奖励（源于针对性的数据增强，如视频帧打乱、空间区域随机掩码、推理步骤混乱等扰动方式），让模型区分原始连贯上下文和合成扰动上下文下的策略输出，从而内化时间顺序、细粒度空间细节和连贯逻辑流的重要性，培养对具身任务强大且敏锐的评估能力。

💡 创新点2：构建EQARewardBench基准
为解决EQA领域奖励模型评估基准缺失的问题，基于OpenEQA构建了EQARewardBench。该基准包含来自HM3D和ScanNet两种家庭环境的具身情节记忆视频，从原始问答对构建更全面的问题 - 响应 - 推理轨迹三元组，有1546个测试实例，用于评估奖励模型在轨迹质量的八个不同方面（如正确性、接地性、效率等），为EQA任务上的奖励模型提供了标准化、可比较的评估平台。

### 📈 实验结果
以Qwen2 - VL - 2B - Instruct为基础进行微调的EQA - RM展现出高样本效率，仅用700个样本在EQA - RM - Bench上达到61.9%的准确率，超越了强大的专有基线（如Gemini - 2.5 - Flash、GPT - 4o、Claude - 3.5 - Haiku）和开源的最先进模型（如RoVRM和VisualPRM）。同时，EQA - RM展示了测试时可扩展性，在推理时增加评估计算量，其在EQARewardBench上的准确率从42.47%提升到61.86%，性能提升后在基准测试中超越了领先的大型商业模型。

### 💬 可借鉴之处
1. 针对特定复杂任务设计专用奖励模型：当通用模型无法满足复杂任务（如具身任务）的评估需求时，可像EQA - RM一样针对任务特性，设计具备特定能力（如空间、时间、推理处理能力）的专用模型，解决通用模型的局限性。
2. 创新的训练策略：两阶段训练（RFT + C - GRPO）以及利用数据增强的对比奖励策略，为解决模型只学形式不学内容、提升模型对任务关键要素的理解提供了思路，可借鉴于其他需要模型深入理解任务细节的训练场景。
3. 构建领域基准：对于缺乏评估基准的领域，可像构建EQARewardBench一样，基于现有数据集构建专门的基准，推动领域内模型的评估和发展，为模型性能比较和改进提供标准平台。

## athena--enhancing-multimodal-reasoning-with-data-efficient-process-reward-models
### Abstract
We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.
### 🌟 论文解读 | Athena：用数据高效的过程奖励模型提升多模态推理能力

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）和多模态大语言模型（MLLMs）在自然语言处理和多模态任务中取得显著进展，但解决复杂推理任务（如数学和多步骤推理）仍具挑战。为增强推理能力，测试时缩放（TTS）等方法被探索，其中过程奖励模型（PRMs）能为中间推理步骤提供细粒度反馈，性能更优且泛化性强。然而，PRMs 发展面临两大难题：一是获取带过程标签的高质量数据成本高（需大量人工标注或计算昂贵的自动化标注）；二是传统自动化标注（如蒙特卡洛估计）易产生噪声标签。本文旨在解决这些挑战，降低计算成本并减轻标签噪声问题，提升 PRMs 性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：利用强弱完成器预测一致性生成高质量过程标签  
传统蒙特卡洛等自动化标注方法易受完成器推理能力影响，标签有噪声且计算成本高。本文发现，强完成器即便中间步骤错误仍能得到正确答案，弱完成器则可能在中间步骤正确时也失败。基于此，提出用弱、强完成器预测一致性作为筛选可靠过程标签的标准，保留两者标签一致的步骤，减少完成器带来的偏差，提升标签质量。实验表明，约 5000 条高质量标签就能比传统方法约 30 万条大规模标注数据表现更优，且大幅降低数据合成和模型训练的计算成本。

💡 创新点2：提升 PRMs 性能的两大策略  
 - ORM 初始化：PRMs 通常基于预训练基础模型微调，而结果奖励模型（ORMs）在大规模响应级数据上训练，具备弱监督下评估中间步骤正确性的能力。因此用 ORMs 初始化 PRMs，将 ORMs 作为弱监督预训练，PRMs 再在高质量细粒度步骤数据上微调，显著提升性能。  
 - 负样本上采样：过程标签数据存在标签不平衡问题，通过对含负步骤标签的数据进行上采样，解决数据分布不均问题，优化模型训练。  

💡 创新点3：构建 Athena 系列模型并多场景验证  
基于上述方法构建结果奖励模型 Athena - ORM 和过程奖励模型 Athena - PRM，再利用 Athena - PRM 通过奖励排序微调得到 Athena - 7B。并在三个场景验证：测试时缩放（TTS）中对策略模型生成的多个输出排序；直接评估推理步骤正确性；奖励排序微调（用高奖励响应微调策略模型）。

### 📈 实验结果
- 测试时缩放场景：在 7 个多模态数学和推理基准测试中，用 Athena - PRM 配合不同规模（7B 到 72B）策略模型，推理能力显著提升。如用 Qwen2.5 - VL - 7B 作为策略模型时，在 WeMath 基准上零样本基线提升 10.2 分，在 MathVista 提升 7.1 分；在文本-only 数学基准用 Mistral - 8B 时提升 8.9 分。  
- 推理步骤正确性评估场景：在 VisualProcessBench 基准上，Athena - PRM 表现强劲，超越开源的 VisualPRM - 8B 等模型，F1 分数比之前最优结果高 3.9，展现准确评估推理步骤正确性的能力。  
- 奖励排序微调场景：基于 Qwen2.5 - VL - 7B 微调得到的 Athena - 7B，在 7 个数学和推理基准上大幅提升策略模型推理能力。  

### 💬 可借鉴之处
- 数据高效标注思路：利用多完成器预测一致性筛选标签，为解决需细粒度标注且标注成本高的任务提供了新范式，在减少数据量同时提升数据质量，实现数据高效利用。  
- 模型训练策略：ORM 初始化和负样本上采样策略，为提升奖励模型性能提供了可复用方法，可启发其他奖励模型或需细粒度反馈模型的训练优化。  
- 多场景验证模式：在测试时缩放、步骤评估、模型微调等多场景验证方法有效性，这种全面验证思路有助于更充分展示方法价值，为后续研究提供验证范式参考。

## learning-to-reason-across-parallel-samples-for-llm-reasoning
### Abstract
Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.
### 🌟 论文解读 | 融合并行与顺序推理，SSA让大模型推理更高效

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务上能力不断提升，而测试时计算资源的分配（即测试时缩放）是优化模型性能的新方向。现有测试时缩放方法分并行和顺序两类：并行缩放是独立生成多条推理路径再聚合（如多数投票）；顺序缩放则迭代优化单个解（如基于提示的自我反思）。但并行方法常孤立看待样本，顺序方法计算成本或适配性受限。本文旨在提出新方法，融合二者优势，更高效利用测试时计算资源提升推理性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Sample Set Aggregator（SSA）模型架构  
设计轻量级的SSA模型，将其与生成答案的基础模型（LMans）解耦。先由LMans并行生成K个候选答案，再把这些候选答案拼接成序列输入SSA，SSA通过强化学习优化以输出最终正确答案。这种设计让SSA能基于基础模型输出的分布特性，直接优化答案合成过程，而非孤立评估单个样本。  

💡 创新点2：基于输出分布推理，解耦训练与推理  
SSA不直接训练生成答案的基础模型（LMans可视为黑盒），而是针对其采样输出进行优化。这种“推理输出分布而非调整模型内部”的思路，让方法更灵活——可适配不同基础模型（甚至是只能通过API调用的黑盒大模型），只需用其采样答案训练SSA即可。  

💡 创新点3：统一并行与顺序缩放优势  
并行缩放能快速获取多视角答案，顺序缩放可迭代优化推理；SSA通过“并行采样+单步顺序RL聚合”的方式，在一次前向传递中结合二者长处：用并行获取多样性，用SSA的顺序推理实现精准聚合，且仅需训练小模型就能带来显著性能提升。  


### 📈 实验结果
1. 性能超越强基线：在多个数学推理数据集上，SSA相比基于奖励模型重排序等测试时缩放方法表现更优，大幅缩小了模型实际性能与“理论最优（oracle - best）”精度的差距。  
2. 泛化能力突出：跨样本集大小、基础模型家族（如Qwen 2.5、Llama 3.1）、模型规模（7B/14B/32B）和任务，SSA都展现出良好泛化性。比如在一个数据集上为特定模型训练的SSA，能成功聚合不同模型家族、规模在不同任务上的输出。  
3. 轻量化优势：紧凑的SSA模型能匹配顺序缩放中经强化训练的大模型性能，证明其作为轻量顺序缩放方式的有效性。  


### 💬 可借鉴之处
1. 架构解耦思路：将“答案生成”与“答案聚合分析”解耦，为利用黑盒大模型（如调用API的商用大模型）提供了可行路径——只需获取其输出，用SSA做后处理即可，无需改动黑盒模型本身。  
2. 测试时缩放新范式：展示了“并行采样 + 针对性小模型聚合”在推理任务上的潜力，为后续优化测试时计算效率、平衡资源与性能提供了新方向。  
3. 强化学习应用启发：通过强化学习优化聚合模型（SSA）来提升最终答案精度，验证了在“输出分布层面做推理优化”的价值，可启发更多围绕模型输出后处理的研究。

## guided-speculative-inference-for-efficient-test-time-alignment-of-llms
### Abstract
We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .
### 🌟 论文解读 | 高效LLM测试时对齐的引导式推测推理（GSI）

### 📌 背景痛点/本文动机
大语言模型（LLMs）在各类生成任务中表现卓越，但模型与数据规模的扩大带来了高昂的计算与经济成本，因此需要更高效的替代方案。测试时扩展（test - time scaling）聚焦于推理时的计算扩展，而模型对齐则致力于让模型优化以最大化给定奖励模型的回报。现有方法如奖励引导的推测解码（RSD）缺乏分布保真度的理论保证，软最佳n采样（soft best - of - n）虽有一定作用但也存在改进空间。在此背景下，本文提出引导式推测推理（GSI）算法，旨在实现高效的奖励引导解码。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：算法融合与目标近似
GSI结合了软best - of - n测试时扩展、奖励模型\( r(x,y) \)以及来自小型辅助模型\( \pi_S(y\mid x) \)的推测样本。通过对\( \pi_B \)和\( \pi_S \)下的对数似然调整奖励（倾斜），GSI可证明地近似\( \pi_B \)下软best - of - n的最优倾斜策略\( \pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta\,r(x,y)) \)。将倾斜分布重写为基于\( \pi_S \)的分布形式，通过对\( \pi_S \)采样并重新加权候选来近似\( \pi_{\beta,B} \)，定义了奖励 - 似然倾斜的软best - of - n（Reward - Likelihood Tilted S - BoN）。
💡 创新点2：理论保证与算法流程
推导了诱导分布与最优策略之间KL散度的理论界。提出的GSI算法在每一步推理时，先从\( \pi_S \)采样，计算奖励与似然调整后的倾斜值，通过softmax采样候选；若候选满足阈值则接受，否则回退到从\( \pi_B \)进行软best - of - n采样。同时假设覆盖条件\( C_{\infty}(x) := \sup_{y\in Y:\pi_B(y|x)>0} \frac{\pi_B(y | x)}{\pi_S(y | x)} < \infty \)，在此条件下证明Reward - Likelihood Tilted S - BoN对倾斜分布的近似性。

### 📈 实验结果
在推理基准测试（MATH500、OlympiadBench、Minerva Math）上，GSI方法比使用\( \pi_S \)的标准软best - of - n和奖励引导的推测解码（Liao et al., 2025）实现了更高的准确率，在某些设置下甚至超过了使用\( \pi_B \)的软best - of - n。

### 💬 可借鉴之处
从方法创新角度，GSI融合多种技术并提供理论保证的思路，为后续高效LLM推理与对齐方法研究提供了参考，展示了如何通过结合不同组件（小模型推测、奖励模型、软采样等）并进行理论分析来提升性能；从实验角度，在多个推理基准上的验证为类似方法的效果评估提供了范例，证明了该方法在实际任务中的有效性，后续研究可借鉴其任务选择与对比实验设置方式。

## co-evolving-llm-coder-and-unit-tester-via-reinforcement-learning
### Abstract
We propose CURE, a novel reinforcement learning framework with a dedicated
reward design that co-evolves coding and unit test generation capabilities
based on their interaction outcomes, without any ground-truth code as
supervision. This approach enables flexible and scalable training and allows
the unit tester to learn directly from the coder's mistakes. Our derived
ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and
Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,
outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They
naturally extend to downstream tasks such as test-time scaling and agentic
coding-achieving a 8.1% improvement over the base model. For the long-CoT
model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while
achieving 64.8% inference efficiency in unit test generation. Notably, we also
find that our model can serve as an effective reward model for reinforcement
learning on base models. Project: https://github.com/Gen-Verse/CURE
### 🌟 论文解读 | 无真值代码监督下，让LLM编码与单元测试能力协同进化的CURE框架

### 📌 背景痛点/本文动机
在大语言模型（LLMs）的发展中，提升其编码能力至关重要。传统提升编码能力的方式存在局限，比如训练单元测试生成器依赖真值代码监督，收集真值代码成本高且 labor - intensive，限制了训练数据规模和多样性。同时，单元测试对提升编码性能是关键因素，但如何在无真值代码下让代码生成器和单元测试生成器协同进化来提升编码能力是待解决的问题。此外，长链思维（long - CoT）模型推理速度极慢也需优化。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出CURE强化学习框架
CURE是一种新颖的强化学习框架，无需任何真值代码监督，基于代码生成器和单元测试生成器的交互结果来协同进化它们的能力。该框架构建成对奖励矩阵，实现相互监督和持续改进。在强化学习过程中，代码生成器产生的正确和错误解决方案能让单元测试生成器学习区分好坏代码，同时代码生成器也能得到优化。
💡 创新点2：针对长CoT模型的奖励变换
对于长链思维（long - CoT）模型，引入响应长度引导的奖励变换，使长CoT单元测试生成器在测试时更高效。
💡 创新点3：训练后的单元测试生成器作奖励模型
训练后的单元测试生成器可作为奖励模型，通过强化学习微调LLMs，在无人工标注或真值单元测试监督下提升编码性能。

### 📈 实验结果
在Qwen2.5 - Instruct模型上优化后，ReasonFlux - Coder 7B和14B模型代码生成准确率提升5.3%，Best of N准确率提升9.0%，超过同规模的Qwen - Coder、DeepSeek - Coder和Seed - Coder；在下游任务如测试时缩放和智能编码上，比基础模型提升8.1%；长CoT的ReasonFlux - Coder - 4B在单元测试生成中推理效率达64.8%，且持续超过Qwen3 - 4B；训练后的单元测试生成器作为奖励模型用于基础模型强化学习也有竞争力的提升。

### 💬 可借鉴之处
1. 协同进化思路：CURE框架展现了让两个相关能力（编码和单元测试生成）通过交互协同进化的思路，可借鉴到其他存在相互影响能力的模型训练场景，比如不同类型的生成任务间的协同优化。
2. 无监督监督方式：无需真值代码监督来训练单元测试生成器和代码生成器，为数据获取困难场景下的模型训练提供了新方向，可思考在其他依赖大量标注数据的任务中，如何构造类似的自监督或无监督监督机制。
3. 针对特定模型的优化策略：对长CoT模型的响应长度引导奖励变换，提示我们在面对有特殊性能瓶颈（如推理慢）的模型时，可从任务相关的特征（如响应长度）入手设计优化策略。

## incentivizing-llms-to-self-verify-their-answers
### Abstract
Large Language Models (LLMs) have demonstrated remarkable progress in complex
reasoning tasks through both post-training and test-time scaling laws. While
prevalent test-time scaling approaches are often realized by using external
reward models to guide the model generation process, we find only marginal
gains can be acquired when scaling a model post-trained on specific reasoning
tasks. We identify that the limited improvement stems from distribution
discrepancies between the specific post-trained generator and the general
reward model. To address this, we propose a framework that incentivizes LLMs to
self-verify their own answers. By unifying answer generation and verification
within a single reinforcement learning (RL) process, we train models that can
effectively assess the correctness of their own solutions. The trained model
can further scale its performance during inference time by verifying its
generations, without the need for external verifiers. We train our
self-verification models based on Qwen2.5-Math-7B and
DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying
reasoning context lengths. Experiments on multiple mathematical reasoning
benchmarks show that our models can not only improve post-training performance
but also enable effective test-time scaling. Our code is available at
https://github.com/mansicer/self-verification.
### 🌟 论文解读 | 让大模型自我验证答案，解锁推理性能新高度

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务中，虽能通过训练后阶段和测试时的缩放定律提升表现，但现有测试时缩放方法（如用外部奖励模型引导生成）在特定推理任务训练后的模型上效果有限。原因在于特定训练后的生成器与通用奖励模型存在分布差异，导致验证不准确，难以让训练后阶段和测试时缩放协同增效。所以，本文旨在提出一种让LLMs自我验证答案的框架，弥合这两个阶段的差距。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：自验证框架设计  
提出激励大模型自我验证答案的框架，将答案生成与验证统一到单个强化学习（RL）过程中。让模型既学习生成答案，又学习评估自身解答的正确性，无需外部验证器，解决了特定生成器与通用奖励模型分布不匹配问题。  
💡 创新点2：训练机制优化  
基于GRPO算法设计在线策略对齐缓冲区和动态验证奖励。在线缓冲区保证验证器输入分布与模型最新输出对齐；动态奖励函数利用GRPO多轮次输出自动调整奖励信号，稳定答案生成与验证的联合训练，提升验证任务表现。  
💡 创新点3：推理时性能缩放  
推理阶段，用训练后的模型同时做答案生成与验证，基于验证分数做加权答案聚合。该方式可在现有LLM推理引擎中轻松部署，借助自我验证实现测试时性能有效缩放。  

### 📈 实验结果
在数学推理基准测试（如MATH500、AIME24）上，基于Qwen2.5 - Math - 7B和DeepSeek - R1 - Distill - Qwen - 1.5B训练自验证模型，结果显示：模型训练后性能提升，且在推理时随着生成次数增加，能通过自我验证实现有效测试时缩放，相比传统测试时缩放方法（如best - of - N、带外部奖励模型的束搜索）表现更优。  

### 💬 可借鉴之处
1. 自验证思路：为解决模型不同阶段分布差异问题提供了新方向，可启发在其他需要验证的任务（如逻辑推理、代码生成）中设计类似自验证机制。  
2. RL结合生成与验证：将强化学习用于统一生成和验证的训练流程，为多任务联合训练、提升模型鲁棒性提供了参考范式。  
3. 推理时部署：推理时基于自我验证的性能缩放方式，无需依赖外部复杂组件，对工程化部署友好，可指导实际生产中LLM推理性能优化。

## dreamprm--domain-reweighted-process-reward-model-for-multimodal-reasoning
### Abstract
Reasoning has improved the performance of large language models (LLMs) on
complicated tasks. Central to the current reasoning studies, Process Reward
Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps
and guide the reasoning process. However, extending PRMs to multimodal large
language models (MLLMs) introduces challenges. Since multimodal reasoning
covers a wider range of tasks compared to text-only scenarios, the resulting
distribution shift from the training to testing sets is more severe, leading to
greater generalization difficulty. Training a reliable multimodal PRM,
therefore, demands large and diverse datasets to ensure sufficient coverage.
However, current multimodal reasoning datasets suffer from quality imbalance,
which degrades PRM performance and highlights the need for data selection
strategy. To address the issues, we introduce DreamPRM, a domain-reweighted
training framework for multimodal PRMs which employs bi-level optimization. In
the lower-level optimization, DreamPRM performs fine-tuning on multiple
datasets with domain weights, allowing the PRM to prioritize high-quality
reasoning signals and alleviating the impact of dataset quality imbalance. In
the upper-level optimization, the PRM is evaluated on a separate meta-learning
dataset; this feedback updates the domain weights through an aggregation loss
function, thereby improving the generalization capability of trained PRM.
Extensive experiments on multiple multimodal reasoning benchmarks covering both
mathematical and general reasoning show that test-time scaling with DreamPRM
consistently improves performance of state-of-the-art MLLMs. Further
comparisons reveal that DreamPRM's domain-reweighting strategy surpasses data
selection methods and yields higher accuracy gains than existing test-time
scaling approaches. Codes are available at
https://github.com/coder-qicao/DreamPRM.
### 🌟 论文解读 | DreamPRM：为多模态推理量身定制的领域加权过程奖励模型

### 📌 背景痛点/本文动机
推理能力让大语言模型（LLMs）在复杂任务上表现更优，过程奖励模型（PRMs）作为推理研究的核心，能对中间推理步骤做细粒度评估并引导推理过程。但将PRMs拓展到多模态大语言模型（MLLMs）时面临挑战：多模态推理任务范围比纯文本更广，训练到测试的分布偏移更严重，泛化难度大；训练可靠多模态PRM需大量多样数据集，可现有多模态推理数据集存在质量不均衡问题（如含不必要模态、难度过低等噪声数据），会降低PRM性能，因此急需有效数据选择策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出DreamPRM框架  
DreamPRM是面向多模态PRM的领域加权训练框架，采用双层优化（bi - level optimization）。受领域加权技术启发，它为每个多模态推理数据集动态学习合适权重，让不同数据集在训练中贡献不同。低质量、含多噪声样本的数据集权重低，减少对PRM参数更新的影响；高质量数据集权重高，在优化中起更重要作用，以此缓解数据集质量不均衡问题。

💡 创新点2：双层优化机制  
在下层优化中，PRM参数在多个训练领域下，结合领域权重与蒙特卡洛信号进行微调；在上层优化中，用单独的元学习数据集评估优化后的PRM，通过聚合损失函数反馈更新领域权重，提升训练后PRM的泛化能力。

### 📈 实验结果
在涵盖数学和通用推理的多个多模态推理基准测试中，结合DreamPRM的测试时缩放持续提升了最先进MLLMs的性能；对比显示，DreamPRM的领域加权策略超越其他数据选择方法，且比现有测试时缩放方法带来更高的准确率提升。

### 💬 可借鉴之处
从方法设计角度，针对数据质量不均衡问题提出的领域加权 + 双层优化思路，为处理多模态场景下数据分布与质量难题提供了新颖范式，可启发后续多模态模型训练中数据利用与优化策略设计；从应用角度，其在多基准测试中验证有效性，说明该框架对提升多模态大模型推理能力切实可行，为多模态推理领域模型优化提供了有价值的技术路线参考。

## from-mathematical-reasoning-to-code--generalization-of-process-reward-models-in-test-time-scaling
### Abstract
Recent advancements in improving the reasoning capabilities of Large Language
Models have underscored the efficacy of Process Reward Models (PRMs) in
addressing intermediate errors through structured feedback mechanisms. This
study analyzes PRMs from multiple perspectives, including training
methodologies, scalability, and generalization capabilities. We investigate the
interplay between pre-training and reward model training FLOPs to assess their
influence on PRM efficiency and accuracy in complex reasoning tasks. Our
analysis reveals a pattern of diminishing returns in performance with
increasing PRM scale, highlighting the importance of balancing model size and
computational cost. Furthermore, the diversity of training datasets
significantly impacts PRM performance, emphasizing the importance of diverse
data to enhance both accuracy and efficiency. We further examine test-time
scaling strategies, identifying Monte Carlo Tree Search as the most effective
method when computational resources are abundant, while Best-of-N Sampling
serves as a practical alternative under resource-limited conditions. Notably,
our findings indicate that PRMs trained on mathematical datasets exhibit
performance comparable to those tailored for code generation, suggesting robust
cross-domain generalization. Employing a gradient-based metric, we observe that
PRMs exhibit a preference for selecting responses with similar underlying
patterns, further informing their optimization.
### 🌟 论文解读 | 从数学推理到代码生成：测试时缩放中过程奖励模型的泛化性探索

### 📌 背景痛点/本文动机
近年来，过程奖励模型（PRMs）在提升大语言模型（LLMs）推理能力方面展现出潜力，能通过结构化反馈机制解决推理过程中的中间错误。但目前对于PRMs在训练方法、可扩展性、泛化能力等多维度的分析仍有待深入，且人们也关心其在数学推理外（如代码生成领域）的表现。因此，本文从训练计算量、测试时缩放策略、跨领域泛化等角度对PRMs展开研究，以优化PRM训练并探索其在不同场景下的性能。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多维度分析PRMs性能影响因素  
从训练方法论、可扩展性、泛化能力等角度剖析PRMs。研究预训练与奖励模型训练的浮点运算量（FLOPs）间的相互作用，以此评估对PRM在复杂推理任务中效率和准确性的影响；同时分析训练数据集多样性对PRM性能的作用，为模型优化提供依据。  

💡 创新点2：探索测试时缩放策略  
评估多种搜索策略（如Best - of - N Sampling、Beam Search、Monte Carlo Tree Search（MCTS）、Majority Voting等）在测试时对PRM推理准确性的优化效果，明确不同计算资源条件下最有效的策略：计算资源充足时MCTS效果最佳，资源受限下Best - of - N Sampling是实用选择。  

💡 创新点3：自动步骤级标注与过滤机制  
在PRM训练中，通过收集多样推理任务、利用LLM对步骤进行标注（借助蒙特卡洛估计、二分搜索等启发式方法）、基于集成的过滤机制（多LLM交叉验证步骤正确性）这一系列流程，自动生成高质量训练数据，减少对人工标注的依赖，提升PRM训练效率与准确性。  

💡 创新点4：跨领域泛化能力探究  
探索PRMs从数学推理到代码生成的跨领域泛化能力，验证在数学语料上训练的PRMs与针对代码生成优化的模型性能相当，展现出良好的跨领域适应性；还通过基于梯度的度量发现PRMs偏好选择具有相似潜在模式的响应，为模型优化提供新视角。  

### 📈 实验结果
- 模型规模与性能关系：PRM规模增大时性能存在收益递减现象，说明要平衡模型大小与计算成本。  
- 训练数据集影响：训练数据集的多样性对PRM性能有显著影响，丰富多样的数据有助于提升准确性和效率。  
- 测试时策略效果：MCTS在计算资源充足时是提升推理准确性最有效的测试时策略；资源有限时，Best - of - N Sampling因简单快速成为实用替代方案。  
- 跨领域表现：在数学数据集上训练的PRMs在代码生成任务中表现与专门为代码生成训练的模型相当，体现出强大的跨领域泛化能力。  

### 💬 可借鉴之处
- 训练资源平衡：在开发类似奖励模型时，需关注模型大小与计算资源的平衡，避免盲目追求大模型而忽略收益递减问题。  
- 数据多样性重视：构建训练数据集时要注重多样性，以此提升模型在不同任务上的准确性与效率。  
- 测试策略选择：部署模型时要根据计算资源情况选择合适的测试时搜索策略，资源充足选MCTS，资源有限考虑Best - of - N Sampling等。  
- 跨领域迁移思路：对于具有步骤性推理特征的不同领域任务（如数学和代码），可尝试利用在某一领域训练的奖励模型迁移到另一领域，借助其泛化能力减少重复训练成本。  
- 自动数据构建流程：PRM训练中自动步骤级标注与过滤的流程可借鉴，用于减少人工标注成本并提升训练数据质量，为其他需要步骤级反馈的模型训练提供参考。

## guided-by-gut--efficient-test-time-scaling-with-reinforced-intrinsic-confidence
### Abstract
Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)
reasoning often incur substantial computational costs, primarily due to
extensive reliance on external Process Reward Models (PRMs) or sampling methods
like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient
self-guided TTS framework that achieves PRM-level performance without costly
external verifier models. Our method employs a lightweight tree search guided
solely by intrinsic LLM signals, token-level confidence and step novelty. One
critical innovation is improving the reliability of internal confidence
estimates via a targeted reinforcement learning fine-tuning phase. Empirical
evaluations on challenging mathematical reasoning benchmarks demonstrate that
GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching
or surpassing significantly larger models (e.g., 32B-70B parameters), while
reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG
achieves comparable accuracy with 8x faster inference speeds and 4-5x lower
memory usage. Additionally, GG reduces KV cache memory usage by approximately
50% compared to the BoN strategy, facilitating more efficient and practical
deployment of TTS techniques.
### 🌟 论文解读 | 用“直觉”引导：高效测试时缩放的强化内在置信度方法

### 📌 背景痛点/本文动机
提升大语言模型（LLM）推理能力的测试时缩放（TTS）方法，常因过度依赖外部过程奖励模型（PRM）或如Best - of - N（BoN）这类采样方法，而产生高昂计算成本。现有TTS方法在增强小模型推理能力时，要么像BoN那样生成大量候选解导致推理成本过高，要么像基于PRM的方法存在训练部署昂贵和泛化性问题，这严重限制了TTS在小模型上的实际应用，因此需要更具成本效益的TTS框架。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：利用Token置信度与新颖性信号
不再依赖昂贵的外部验证模型，而是利用大语言模型输出的内在线索，如token概率，将其解释为置信度分数并衡量推理步骤的新颖性。这为推理时的搜索提供了轻量的指导途径，且能整合到现有模型和算法中。
💡 创新点2：通过强化学习微调增强置信度可靠性
将基于Group Relative Policy Optimization（GRPO）的强化学习融入模型微调过程，专门用于提升大语言模型内部置信度估计的可靠性，为测试时的搜索策略提供更可靠的指导。
💡 创新点3：自引导的高效测试时搜索
引入基于Diverse Verifier Tree Search（DVTS）的树搜索算法，由大语言模型的内在信号（token概率/置信度、新颖性）引导。该算法在推理时针对最小计算成本进行了专门优化，实现高效的TTS。

### 📈 实验结果
在具有挑战性的数学推理基准（如AIME24/25、MATH500、AMC等）上评估表明：
1. 使小模型（如1.5B参数模型）在准确率上匹配甚至超过大模型（如32B - 70B参数模型），同时将GPU内存使用量降低多达10倍；
2. 与基于PRM的方法相比，在准确率相当的情况下，推理速度快8倍，内存使用量降低4 - 5倍；
3. 与BoN策略相比，KV缓存内存使用量降低约50%，有助于TTS技术更高效实用的部署。

### 💬 可借鉴之处
1. 内在信号利用：展示了从模型自身生成过程中挖掘有用信号（如token概率）来指导推理，为轻量型推理引导提供思路，可启发后续研究在不依赖外部复杂组件下提升模型能力；
2. 强化学习微调优化置信度：通过强化学习针对性优化模型内部置信度估计，这种对模型内在“判断”能力优化的思路，可用于其他需要模型自我评估指导的任务场景；
3. 高效搜索算法设计：基于内在信号设计轻量且高效的树搜索算法，为在推理阶段高效分配计算资源、提升推理性能提供了可参考的算法设计范式，对资源受限场景下的模型推理优化很有借鉴意义。

## value-guided-search-for-efficient-chain-of-thought-reasoning
### Abstract
In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of "step,"
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.
### 🌟 论文解读 | 长上下文推理中高效的价值引导搜索：让推理更聪明、更高效

### 📌 背景痛点/本文动机
近年来，像OpenAI o1 & o3、DeepSeek R1这类大语言模型（LLMs）通过强化学习训练，能在生成最终答案前进行多步推理与自我修正，在竞赛数学、编码等领域表现卓越。但这种能力需付出代价：每轮生成涉及长链思考（CoT），推理计算量剧增，且思考过程易重复、陷入无效循环。同时，现有用过程奖励模型（PRMs）引导搜索的方法，因需预定义“步骤”（长上下文推理中难定义）、标注成本高，难以扩展到长上下文推理模型。于是，本文想解决两个问题：能否用更少推理计算获取同等性能？能否用高效搜索方法提升模型性能上限？

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出价值引导搜索（VGS）与无“步骤”依赖的价值模型训练法  
现有PRMs需精细“步骤”定义，本文提出的价值模型训练无需此。构建数据管道：从各类模型收集解前缀，用轻量推理模型（如DeepSeek - R1 - Distill - 1.5B）从随机前缀生成完整解，收集250万条数学推理轨迹（超300亿token）数据集，训练15亿参数的token级价值模型DeepSeek - VM - 1.5B，通过分类回归完整解最终奖励来训练，避开“步骤”定义难题，数据收集也比现有技术高效。

💡 创新点2：块级价值引导搜索（VGS）提升推理效率与性能  
将训练好的价值模型用于DeepSeek模型的块级搜索。在竞赛数学任务（如AIME 2024 & 2025、HMMT Feb 2024 & 2025）上评估，块级VGS结合加权多数投票，比多数投票、best - of - n等标准方法在测试时的计算扩展（TTC）表现更好，既提升推理模型性能上限，又减少达到标准TTC方法性能所需的推理计算量。

### 📈 实验结果
- 性能提升：在64次生成的推理预算下，DeepSeek - R1 - Distill - 1.5B结合VGS在四个竞赛数学基准测试中平均准确率达45.7%，与o3 - mini - medium持平；不同规模DeepSeek - R1 - Distill模型结合VGS后，在竞赛数学基准测试中整体响应质量提升（如左图所示）。
- 效率提升：VGS比多数投票等标准方法，能显著减少达到相同准确率所需的推理FLOPs（如右图所示），证明价值引导对提升效率很有前景。且VGS比用现有PRMs搜索表现更好，说明价值模型能提供更优反馈。

### 💬 可借鉴之处
- 技术层面：无“步骤”依赖的价值模型训练流程，为长上下文推理模型的奖励模型训练提供新思路，避开“步骤”定义与标注难题；块级搜索结合价值模型的方法，在提升性能与降低推理成本上的实践，可迁移到其他有自动化结果监督的任务（如编码、科学研究推理等）。
- 开源层面：公开250万推理轨迹数据集、价值模型与代码库，为后续在其他领域应用VGS提供了数据、模型与代码基础，利于推动相关研究发展。

## j1--exploring-simple-test-time-scaling-for-llm-as-a-judge
### Abstract
The current focus of AI research is shifting from emphasizing model training
towards enhancing evaluation quality, a transition that is crucial for driving
further advancements in AI systems. Traditional evaluation methods typically
rely on reward models assigning scalar preference scores to outputs. Although
effective, such approaches lack interpretability, leaving users often uncertain
about why a reward model rates a particular response as high or low. The advent
of LLM-as-a-Judge provides a more scalable and interpretable method of
supervision, offering insights into the decision-making process. Moreover, with
the emergence of large reasoning models, which consume more tokens for deeper
thinking and answer refinement, scaling test-time computation in the
LLM-as-a-Judge paradigm presents an avenue for further boosting performance and
providing more interpretability through reasoning traces. In this paper, we
introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on
reflection-enhanced datasets collected via rejection-sampling and subsequently
trained using Reinforcement Learning (RL) with verifiable rewards. At inference
time, we apply Simple Test-Time Scaling (STTS) strategies for additional
performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$
surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and
exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally,
we present three key findings: (1) Existing LLM-as-a-Judge does not inherently
exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced
datasets continues to demonstrate similarly weak scaling behavior. (3)
Significant scaling trend emerges primarily during the RL phase, suggesting
that effective STTS capability is acquired predominantly through RL training.
### 🌟 论文解读 | LLM-as-a-Judge新突破：J1-7B与测试时简单缩放策略的探索

### 📌 背景痛点/本文动机
AI研究重心正从模型训练转向提升评估质量，传统评估方法（如标量奖励模型）虽有效但缺乏可解释性。LLM - as - a - Judge范式提供了更具扩展性与可解释性的监督方式，然而现有LLM - as - a - Judge在复杂推理评估任务中存在不足，且测试时缩放技术在评估场景的有效性尚未充分探索。同时，大推理模型出现后，测试时计算缩放为提升LLM - as - a - Judge性能和可解释性提供了途径，本文旨在探究测试时简单缩放（STTS）对LLM - as - a - Judge的作用，以提升其质量与可靠性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出两阶段训练范式打造J1 - 7B
首先通过拒绝采样收集反射增强数据集，对模型进行有监督微调（SFT），该数据集显式加入STTS tokens，为模型冷启动初始化，教模型最优利用反射推理tokens；随后使用带可验证奖励的强化学习（RL）训练，让模型自主优化反射能力。
💡 创新点2：应用简单测试时缩放（STTS）策略
推理时，通过多次附加像“Wait,”这样的思考tokens，迫使模型在给出最终答案前更深入思考，以此提升性能。

### 📈 实验结果
J1 - 7B超越了之前最先进的LLM - as - a - Judge模型，在整体判断性能上提升了4.8%；在STTS下展现出更强的缩放趋势，相比之前提升了5.1%。同时有三个关键发现：现有LLM - as - a - Judge本身没有这种缩放趋势；仅在反射增强数据集上微调的模型缩放行为仍弱；显著缩放趋势主要在RL阶段出现，即有效STTS能力主要通过RL训练获得。

### 💬 可借鉴之处
从方法层面，两阶段训练范式为打造更优的LLM - as - a - Judge模型提供了思路，先利用特定数据集做SFT初始化，再用RL优化能力；从研究视角，对LLM - as - a - Judge在测试时缩放方面的分析，为该领域后续研究指明了方向，如关注不同训练阶段对模型特定能力（如STTS下的缩放能力）的影响；从应用角度，J1 - 7B的成功表明通过合理训练和测试时策略调整，能提升LLM - as - a - Judge的性能与可解释性，对构建更鲁棒、可扩展的AI评估系统有参考价值。

## sailing-by-the-stars--a-survey-on-reward-models-and-learning-strategies-for-learning-from-rewards
### Abstract
Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (RLHF, RLAIF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities for diverse
tasks. In this survey, we present a comprehensive overview of learning from
rewards, from the perspective of reward models and learning strategies across
training, inference, and post-inference stages. We further discuss the
benchmarks for reward models and the primary applications. Finally we highlight
the challenges and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.
### 🌟 论文解读 | 大语言模型“循奖而学”：奖励模型与学习策略全景探秘

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）发展重心从预训练规模扩张转向训练后与推理时的能力增强。但预训练存在与人类价值观错位、适配任务目标难、深度推理不足等局限，难以支撑通用智能长期目标。在此背景下，“从奖励中学习（Learning from Rewards）”范式崛起，它以奖励信号为指引，让模型从静态数据被动学习转向动态反馈主动学习，支撑了RLHF、DPO等主流技术，赋能模型偏好对齐与复杂任务推理。本文旨在全面梳理该范式下奖励模型与学习策略的前沿进展，为领域研究提供全景参考。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建统一概念框架  
提出涵盖语言模型、奖励模型、学习策略三要素的统一框架（如图2）。语言模型生成输出，奖励模型基于输入评估输出质量并给出奖励，学习策略利用奖励更新模型或优化输出，清晰拆解“从奖励中学习”的系统逻辑，为后续分类分析奠基。  

💡 创新点2：多维 taxonomy 分类体系  
从**奖励来源**（人类反馈/自动化反馈）、**奖励模型**（架构、格式、评分模式、粒度等维度，如图3）、**学习阶段**（训练时/推理时/推理后）、**学习策略**（基于训练调参/无训练直接优化输出）四大维度，系统梳理现有方法差异，让繁杂技术有了清晰归类逻辑，便于研究者快速定位方向。  

💡 创新点3：全阶段技术覆盖  
对训练（如RLHF/RLAIF/DPO）、推理（如奖励引导解码、Generate - then - Rank）、推理后（如事后修正）三阶段“从奖励学习”技术逐一剖析，展现不同阶段如何用奖励赋能模型，比如训练阶段借人类/AI反馈对齐偏好，推理阶段用奖励引导生成更优结果，推理后用奖励修正输出缺陷，形成技术链路闭环。  


### 📈 实验结果
文中未聚焦传统“实验指标 - 数值对比”式实验，而是通过对大量前沿技术（如RLHF、DPO、GRPO等）的梳理，展现“从奖励中学习”范式在数学推理、代码生成、多模态、智能体等场景的广泛应用价值，侧面验证各技术分支在实际任务中推动模型能力升级的效果，同时总结奖励模型基准测试进展，为该领域技术有效性评估提供参考方向。

### 💬 可借鉴之处
1. 框架与分类思维：其构建的统一概念框架和多维分类体系，为领域内梳理技术、开展调研类工作提供了模板，帮助研究者快速锚定技术定位与空白点；  
2. 全流程视角：从训练到推理再到推理后全阶段覆盖的分析方式，启发从业者思考不同环节如何协同用奖励增强模型，助力打造更智能的大模型应用；  
3. 技术全景梳理：对RLHF、DPO等主流技术及自动化反馈（自奖励、预定义规则等）的详细盘点，为工程实践中选择技术路线、设计奖励机制提供丰富参考案例，降低技术选型与创新试错成本。  

## a-survey-of-slow-thinking-based-reasoning-llms-using-reinforced-learning-and-inference-time-scaling-law
### Abstract
This survey explores recent advancements in reasoning large language models
(LLMs) designed to mimic "slow thinking" - a reasoning process inspired by
human cognition, as described in Kahneman's Thinking, Fast and Slow. These
models, like OpenAI's o1, focus on scaling computational resources dynamically
during complex tasks, such as math reasoning, visual reasoning, medical
diagnosis, and multi-agent debates. We present the development of reasoning
LLMs and list their key technologies. By synthesizing over 100 studies, it
charts a path toward LLMs that combine human-like deep thinking with scalable
efficiency for reasoning. The review breaks down methods into three categories:
(1) test-time scaling dynamically adjusts computation based on task complexity
via search and sampling, dynamic verification; (2) reinforced learning refines
decision-making through iterative improvement leveraging policy networks,
reward models, and self-evolution strategies; and (3) slow-thinking frameworks
(e.g., long CoT, hierarchical processes) that structure problem-solving with
manageable steps. The survey highlights the challenges and further directions
of this domain. Understanding and advancing the reasoning abilities of LLMs is
crucial for unlocking their full potential in real-world applications, from
scientific discovery to decision support systems.
### 🌟 论文解读 | 揭秘类“慢思考”推理大模型：强化学习与推理时缩放律视角下的前沿进展

### 📌 背景痛点/本文动机
大语言模型（LLMs）如GPT - 4、Deepseek等虽在自然语言理解、代码生成等领域取得突破，但在需要深度、审慎推理的任务（类似人类“系统2”认知）上表现不足。人类认知中“慢思考”强调逐步、谨慎推理，为解决LLMs推理短板，研究者转向“慢思考”范式，结合推理时动态分配计算资源与强化学习等技术提升推理能力，本文旨在综述该领域进展，为后续研究指路。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：三维度分类综述方法
将类“慢思考”推理LLMs的技术方法分为三类。其一为测试时缩放，依据任务复杂度，通过搜索、采样、动态验证等动态调整计算资源；其二是强化学习，利用策略网络、奖励模型、自进化策略等迭代优化决策；其三是慢思考框架，像长思维链（CoT）、分层流程等，将问题解决拆解为易处理步骤来结构化推理过程。
💡 创新点2：聚焦“慢思考”与技术协同
以人类“慢思考”（类比“系统2”认知）为认知目标，明确强化学习与推理时缩放律为实现先进推理能力的关键协同技术机制，在“慢思考”概念框架下系统整合与深入分析这两种方法，区别于其他仅单独或宽泛讨论相关技术的综述。
💡 创新点3：大样本研究 synthesis
综合超160项研究成果，梳理类“慢思考”推理LLMs的发展脉络，呈现从技术演进到应用方向的全面图景，为该领域描绘发展路径。

### 📈 实验结果
文中未明确提及传统意义上的对比实验结果数值，但通过对超160项研究的综合，呈现了不同技术路径（测试时缩放、强化学习、慢思考框架）在推动LLMs推理能力提升上的作用，如在数学推理、代码生成、多智能体等复杂任务场景下，这些技术如何助力模型更接近人类深度推理水平，为理解技术有效性提供了多维度参考。

### 💬 可借鉴之处
对于研究人员，提供了清晰的类“慢思考”推理LLMs技术分类框架与发展脉络，助力快速把握领域核心；在工业界应用层面，为打造更智能的决策支持系统、科学发现辅助工具等提供技术思路参考，如利用测试时缩放应对不同复杂度业务任务、借强化学习优化智能体决策；同时，文中对挑战（如快慢思考平衡、鲁棒奖励机制设计等）的讨论，为后续技术攻坚指明方向，可借鉴其思路开展针对性研究突破瓶颈。

## process-reward-models-that-think
### Abstract
Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.
### 🌟 论文解读 | 用“思考”赋能的过程奖励模型：ThinkPRM 如何高效验证推理步骤？

### 📌 背景痛点/本文动机
在大语言模型（LLM）的推理场景中，**过程奖励模型（Process Reward Model，PRM，也叫过程验证器）** 是实现“测试时算力扩展”的关键组件——它能为多步推理的每一步打分，帮我们筛选更优的推理路径。但传统 PRM 存在两大痛点：  
- **判别式 PRM**：需要大量“步骤级标注”（比如人工逐步标注推理是否正确），标注成本极高；  
- **LLM-as-a-Judge**：虽不用训练、可解释性强，但面对复杂推理时容易“看走眼”，漏判错误步骤，性能远不如专门训练的 PRM。  

那有没有办法 **兼顾“数据高效”和“高性能”**，让 PRM 既不用海量标注，又能精准验证每一步推理？这就是本文要解决的核心问题。  


### 🚀 核心方法（介绍本文的几个创新点）
本文提出 **ThinkPRM**，一种基于“长思维链（long CoT）”的生成式 PRM，核心思路是：**让模型用“生成验证思维链”的方式，逐步判断推理是否正确，同时大幅减少对标注数据的依赖**。  

💡 创新点1：用“生成式验证 + 长 CoT”重构 PRM  
传统判别式 PRM 是“分类器思维”（直接给每一步打分数），而 ThinkPRM 是“生成式思维”——把“验证每一步是否正确”转化为**生成自然语言的“验证思维链”**（比如生成“这一步用了勾股定理，计算过程正确→下一步代入数值时符号错误→……”）。这种方式天然复用了大模型的文本生成能力，让验证过程更可解释，也更灵活。  

💡 创新点2：数据高效的微调策略  
判别式 PRM 通常需要几十万甚至百万级的“步骤级标注”，而 ThinkPRM 只需要 **8K 量级的标注** 就能完成微调（甚至还能基于合成数据进一步减少人工标注）。它的秘诀是：复用“开源大推理模型（LRM）”的基础能力，通过轻量微调让模型学会“生成验证 CoT”，充分利用了大模型本身的推理潜能，减少对标注的依赖。  


### 📈 实验结果
论文在多个极具挑战性的基准测试中验证了 ThinkPRM 的性能，核心结论包括：  
1. **数据效率碾压判别式 PRM**：在 ProcessBench 上，ThinkPRM 仅用 8K 步骤标注，就超过了用 100 倍数据训练的判别式 PRM（如图 1 左，F1 分数更高）；  
2. **复杂推理任务稳压基线**：在 MATH-500、AIME ’24 等数学推理任务中，ThinkPRM 在“多候选择优（best-of-N）”和“奖励引导搜索”场景下，性能超过 LLM-as-a-Judge 和判别式 PRM（如图 1 右，推理准确率更优）；  
3. **跨领域泛化能力更强**：在 GPQA-Diamond、LiveCodeBench 等域外任务中，ThinkPRM 甚至超过了用完整 PRM800K 数据训练的判别式 PRM，分别领先 8% 和 4.5%；  
4. **算力利用更高效**：在相同 Token 预算下，ThinkPRM 能通过“更长的验证 CoT”更高效地分配验证算力，在 ProcessBench 子集上比 LLM-as-a-Judge 领先 7.2%。  


### 💬 可借鉴之处
1. **范式创新：生成式 PRM 潜力大**：把“验证”从“分类任务”改成“生成任务”，不仅降低标注依赖，还让验证过程更透明、可解释（能看到模型“怎么思考验证的”）；  
2. **小数据高效微调**：证明了“复用大模型基础能力 + 轻量微调 + 少量标注/合成数据”的路线，能在高价值任务上实现“低标注成本、高效果”；  
3. **测试时算力可扩展**：ThinkPRM 展示了“让模型通过更长 CoT 消耗更多推理 Token”来提升验证精度的思路，为“测试时动态分配算力”提供了新范式。  

如果你关注大模型推理增强、奖励模型优化，或是“用小数据训出强能力”的技术路线，这篇论文的思路和实验都值得深入研究～代码、数据也会开源在 [GitHub](https://github.com/mukhal/thinkprm) ，可以蹲一波～

## stop-summation--min-form-credit-assignment-is-all-process-reward-model-needs-for-reasoning
### Abstract
Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.
### 🌟 论文解读 | 告别“奖励黑客”：PURE让过程奖励模型在推理任务中更稳更强

### 📌 背景痛点/本文动机
大语言模型（LLMs）在推理任务上的强化微调（RFT）是提升其问题解决能力的关键方向。现有方法中，基于可验证奖励（verifiable rewards）的强化微调虽能提供稀疏但可靠的反馈，却存在长响应时学习效率低的问题；而过程奖励模型（PRMs）能在响应的每一步提供密集反馈，在测试时缩放（test - time scaling）中表现出色，却因“奖励黑客（reward hacking）”问题限制了在强化微调中的应用。奖励黑客指的是神经网络生成的奖励会导致模型在训练中朝着不合理的高奖励方向优化。究其根源，强化学习中经典的“求和式信用分配（summation - form credit assignment）”将价值定义为未来奖励的累积伽马衰减和，易诱导LLMs去“hack”高奖励步骤，进而引发训练崩溃等问题。本文旨在解决PRM在强化微调中的奖励黑客问题，让PRM能更有效地助力LLMs推理能力提升。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出PURE框架（Process sUpervised Reinforcement lEarning）
PURE的关键创新在于采用“最小式信用分配（min - form credit assignment）”，将价值函数重新定义为未来奖励的最小值，而非传统的累积和形式。这种方式通过限制价值函数的范围，更合理地分配优势（advantage），从而显著缓解奖励黑客问题。实现上仅需对过程奖励进行转换，无需大量代码改动，简单易行。
💡 创新点2：支持多种奖励融合
PURE框架支持密集的过程奖励与稀疏的可验证奖励的整合。当结合这两种奖励时，少量真实标签信号（可验证奖励）能进一步减轻PRM引发的奖励黑客问题，为模型训练提供更稳定有效的信号。

### 📈 实验结果
1. 在3个基础模型（Qwen2.5 - 7B、Qwen2.5 - Math - 7B、Qwen2.5 - Math - 1.5B）上进行了全面实验，对比了仅用可验证奖励、仅用PRM生成的过程奖励、两者结合这三种奖励配置，以及求和式与最小式信用分配方法在PRM场景下的表现。结果显示，采用最小式信用分配的基于PRM的方法，仅用30%左右的步骤就能达到与基于可验证奖励方法相当的推理性能，而经典求和式信用分配方法在训练初期就会崩溃。
2. 当在基于PRM的微调中补充仅10%的可验证奖励时，能进一步缓解奖励黑客问题。以Qwen2.5 - Math - 7B为基础模型时，在AMC23基准上达到82.5%的准确率，在MATH - 500、Minerva Math、Olympiad Bench、AIME24、AMC23这5个基准上平均准确率达53.3%，取得实验中基于该模型的最佳微调效果。
3. 还总结了3种PRM引发的奖励黑客案例：“只思考不解决”“极少步骤（1步）”“极少步骤（0步）”，并分析了每种案例的成因、展示示例和提供解决思路；同时发现验证器判定为正确的长且高度重复的伪正样本会导致训练突然崩溃（5个梯度步内）。

### 💬 可借鉴之处
1. 方法设计层面：当面临类似“信用分配导致模型不合理优化”问题时，可借鉴PURE这种改变信用分配形式（从求和到取最小）的思路，通过调整价值函数计算方式来约束模型优化方向，解决奖励黑客等问题。
2. 多奖励融合层面：在模型训练中，可考虑将不同类型（密集与稀疏、模型生成与真实标签）的奖励结合，利用少量可靠信号辅助缓解模型因依赖生成奖励带来的不稳定问题，提升训练效果。
3. 问题分析层面：论文对奖励黑客案例和训练崩溃原因的总结分析，为后续研究过程奖励模型在强化微调中遇到的问题提供了宝贵的经验参考，有助于后续工作更高效地排查和解决类似训练异常问题。

## evaluating-judges-as-evaluators--the-jetts-benchmark-of-llm-as-judges-as-test-time-scaling-evaluators
### Abstract
Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.
### 🌟 论文解读 | 评估LLM法官在测试时扩展场景下的表现：JETTS基准测试

### 📌 背景痛点/本文动机
近年来，大语言模型（LLM）能力提升多依赖模型与训练数据规模扩大，但此方式因数据和成本限制渐趋饱和。测试时扩展（test - time scaling）作为替代方案兴起，即推理时给生成式LLM更多计算资源以生成更优响应，其中 evaluator 模型很关键。标量奖励模型（RMs）常用于测试时计算的响应重排或分步生成评估，而生成式LLM - judges（能生成自然语言评估和 critique）在模型评估中流行，但在测试时扩展场景下其有效性未知。因此，本文旨在构建基准测试来系统评估 LLM - judges 在测试时扩展场景下的表现。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出JETTS基准测试  
构建了Judge Evaluation for Test - Time Scaling（JETTS）基准，用于评估LLM - judges在测试时扩展场景下的表现。该基准涵盖三个任务：响应重排（response reranking，法官从多个响应中选最优）、分步束搜索（step - level beam search，法官引导模型逐步生成响应）、基于 critique 的优化（critique - based refinement，法官提供自然语言 critique 供模型优化响应）；涉及三个领域（数学推理、代码生成、指令遵循），评估了10个不同规模（7B - 70B参数）的judge模型，且生成响应的基础生成器模型有8个（6.7B - 72B参数）。
💡 创新点2：多维度分析LLM - judges效用  
通过JETTS基准设置，分析judge引导的测试时扩展的多方面影响，如“弱”judge对“强”生成器是否有帮助、强judge对弱生成器益处、judge模型在哪些领域适合测试时扩展、judge的critique实际效用等。同时对比了JETTS与RewardBench基准，揭示不同规模judge在不同场景下的“基础判断能力”差异。

### 📈 实验结果
1. 在响应重排任务中，judges与结果奖励模型（outcome reward models）相比有竞争力，但在束搜索过程中，始终比过程奖励模型（process reward models）差。
2. 弱judge在像指令遵循这类较简单任务中能帮助强生成器，但在编码或数学等推理密集型任务中不行；更大规模的judge对数学和指令遵循任务帮助最大，但没有评估的judge能可靠提升代码生成任务中生成器性能。
3. 尽管自然语言critique是LLM - judges相对于RMs的独特优势，但目前在引导生成器得到更优响应方面效果不佳。
4. 对比RewardBench和JETTS发现，不同基准下judge表现有差异，JETTS模拟测试时扩展场景，揭示了不同规模judge“基础判断能力”的不同，如在响应重排任务中，小参数的judge（如Skywork - Critic - 8B）在JETTS上比大参数judge（如Skywork - Critic - 70B）带来的提升低，而RewardBench中可能因任务构造方式不同，小参数judge表现和大参数judge差异没这么大。

### 💬 可借鉴之处
1. 为评估LLM在测试时扩展场景下作为 evaluator 的表现提供了首个系统基准JETTS，后续研究可基于此基准进一步探索LLM - judges在测试时扩展的优化方向。
2. 多任务、多领域、多模型规模的评估方式为全面理解LLM - judges效用提供了范例，其他关于模型评估或测试时优化的研究可借鉴这种多维度评估思路。
3. 对LLM - judges在不同任务、领域、模型规模下的表现分析，能为实际应用中选择合适的judge模型提供参考，如在数学和指令遵循任务中考虑更大规模judge，代码生成任务则需探索更有效的judge或优化方式。

## adaptive-rectification-sampling-for-test-time-compute-scaling
### Abstract
The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time
scaling can significantly improve model performance, especially in complex
tasks such as logical reasoning. Common test-time scaling methods involve
generating more chain of thoughts (CoTs) or longer CoTs with self-correction.
However, while self-correction can improve performance, it may lead to
significant token waste and reduce readability of the CoT if the reasoning
steps are already correct. To demonstrate that large language models (LLMs) can
rectify errors at a more fine-grained level, we propose Adaptive Rectification
Sampling (AR-Sampling), which can guide the LLMs to self-correction at the
appropriate step. AR-Sampling leverages a process-supervised reward model (PRM)
as a verifier and constructed trigger sentences to guide the model in adaptive
step-level rethinking. Through the experiments on GSM8K and MATH500, it
indicate that our approach enables the models to rethink in more fine-grained
level, improving the accuracy of solutions, while generating a reasonable
number of additional tokens.
```
### 🌟 论文解读 | 自适应修正采样：让大模型测试时更高效思考

### 📌 背景痛点/本文动机
OpenAI - o1和DeepSeek - R1等模型展现出测试时计算缩放（如增加思维链长度或宽度）能提升复杂任务表现，但现有方法存在不足。常见测试时缩放方法中，增加思维链长度的自我修正若在推理步骤正确时仍进行，会导致token浪费和思维链可读性下降；且难以在测试时精准定位错误步骤引导模型重思考。同时，大模型还存在“过度思考”（简单问题也生成冗长回应）现象。因此，如何引导大模型在测试时于合适时机细粒度重思考成为关键问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出自适应修正采样（AR - Sampling）
利用过程监督奖励模型（PRM）作为验证器检查推理步骤，识别潜在错误；并构造触发语句，引导大模型在自适应的步骤层面重新思考。这样能让模型在错误步骤处针对性修正，避免无意义的token生成。
💡 创新点2：验证大模型细粒度重思考能力
通过设计方法证明大模型具备在更细粒度层面重新思考的能力，这对未来解决“过度思考”问题提供了思路与方向，为后续相关研究打下基础。

### 📈 实验结果
在Llama3.2和Qwen2.5模型上对GSM8K和MATH500等任务进行评估，结果表明该方法能让模型在更细粒度层面重新思考，提升解题准确率，同时额外生成的token数量合理，即在保证效果提升的同时，较好地控制了计算资源消耗（token数量可反映一定计算资源情况）。

### 💬 可借鉴之处
从方法设计角度，AR - Sampling结合验证器与触发语句引导重思考的思路，为大模型测试时优化推理过程提供了新范式，后续研究可借鉴这种“检测 - 引导 - 细粒度修正”的流程设计；从能力验证角度，证明大模型细粒度重思考能力为解决“过度思考”等问题开辟了新方向，相关研究可围绕如何进一步利用该能力优化模型表现展开；从应用角度，在数学推理等任务上的有效实践，为其他复杂任务（如逻辑推理、代码生成）中提升大模型测试时表现提供了参考，可尝试将该方法迁移到类似需要逐步推理的任务场景中。
```

## when-to-solve--when-to-verify--compute-optimal-problem-solving-and-generative-verification-for-llm-reasoning
### Abstract
Scaling test-time compute has emerged as a key strategy for enhancing the
reasoning capabilities of large language models (LLMs), particularly in tasks
like mathematical problem-solving. A traditional approach, Self-Consistency
(SC), generates multiple solutions to a problem and selects the most common
answer via majority voting. Another common method involves scoring each
solution with a reward model (verifier) and choosing the best one. Recent
advancements in Generative Reward Models (GenRM) reframe verification as a
next-token prediction task, enabling inference-time scaling along a new axis.
Specifically, GenRM generates multiple verification chains-of-thought to score
each solution. Under a limited inference budget, this introduces a fundamental
trade-off: should you spend the budget on scaling solutions via SC or generate
fewer solutions and allocate compute to verification via GenRM? To address
this, we evaluate GenRM against SC under a fixed inference budget.
Interestingly, we find that SC is more compute-efficient than GenRM for most
practical inference budgets across diverse models and datasets. For instance,
GenRM first matches SC after consuming up to 8x the inference compute and
requires significantly more compute to outperform it. Furthermore, we derive
inference scaling laws for the GenRM paradigm, revealing that compute-optimal
inference favors scaling solution generation more aggressively than scaling the
number of verifications. Our work provides practical guidance on optimizing
test-time scaling by balancing solution generation and verification. The code
is available at https://github.com/nishadsinghi/sc-genrm-scaling.
### 🌟 论文解读 | 大模型推理时，何时生成解法、何时验证？计算最优的权衡之道

### 📌 背景痛点/本文动机
大语言模型（LLM）在推理任务（如数学解题）中，测试时扩展计算资源是提升能力的关键策略。传统方法有自洽性（Self - Consistency，SC），即生成多个解法后多数投票；还有用奖励模型（验证器）给解法打分选最优的Best - of - N。而生成式奖励模型（GenRM）把验证重构为下一个token预测任务，能沿新维度扩展推理。但在有限推理预算下，存在关键权衡：把预算花在SC扩展解法数量，还是少生成解法、把计算资源给GenRM做验证？以往固定解法数量对比GenRM和SC会误导实践，因为忽略了验证的计算成本，所以本文要在固定推理预算下评估两者，并探究GenRM下解法生成与验证的计算分配。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：计算匹配分析框架
提出在固定推理预算B下，对比SC（解法数S = B）和GenRM（解法数S = B/V，V为每个解法的验证数）的性能，基于LLM生成的解法和验证总数来比较SC与GenRM的推理计算，明确在有限预算下两种策略的效果差异。
💡 创新点2：推导GenRM推理缩放律
针对GenRM，推导推理缩放律，研究在总计算预算变化时，最优的解法生成数和验证数如何缩放，揭示计算最优推理中解法生成应比验证缩放更激进（缩放因子1.5 - 2倍左右），以指导资源分配。

### 📈 实验结果
在不同模型（如Llama、Qwen等）、模型大小（如7B、70B等）、推理任务（如数学）和数据集上，多数实际推理预算下SC比GenRM计算效率更高。GenRM要消耗高达8倍于SC的推理计算才能追平SC，要显著更多计算才能超越。在GenRM范式下，计算最优推理更倾向于比扩展验证数更积极地扩展解法生成数，当预算足够高时GenRM才会超过SC，且GenRM下解法生成和验证数需协同缩放但解法缩放更快（1.5 - 2倍）才能最优。

### 💬 可借鉴之处
为优化测试时的计算缩放提供实践指导，帮助从业者平衡解法生成和验证来优化推理策略与预算。明确了在不同计算预算规模下，SC和GenRM的优劣区间，以及GenRM下解法与验证的资源分配方式，让开发者在实际部署大模型推理任务时，能更合理分配计算资源，提升效率与性能。同时开源代码，为后续研究和实践提供了可复现和拓展的基础。

## genprm--scaling-test-time-compute-of-process-reward-models-via-generative-reasoning
### Abstract
Recent advancements in Large Language Models (LLMs) have shown that it is
promising to utilize Process Reward Models (PRMs) as verifiers to enhance the
performance of LLMs. However, current PRMs face three key challenges: (1)
limited process supervision and generalization capabilities, (2) dependence on
scalar value prediction without leveraging the generative abilities of LLMs,
and (3) inability to scale the test-time compute of PRMs. In this work, we
introduce GenPRM, a generative process reward model that performs explicit
Chain-of-Thought (CoT) reasoning with code verification before providing
judgment for each reasoning step. To obtain high-quality process supervision
labels and rationale data, we propose Relative Progress Estimation (RPE) and a
rationale synthesis framework that incorporates code verification. Experimental
results on ProcessBench and several mathematical reasoning tasks show that
GenPRM significantly outperforms prior PRMs with only 23K training data from
MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and
a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,
GenPRM demonstrates strong abilities to serve as a critic model for policy
model refinement. This work establishes a new paradigm for process supervision
that bridges the gap between PRMs and critic models in LLMs. Our code, model,
and data will be available in https://ryanliu112.github.io/GenPRM.
```
### 🌟 论文解读 | GenPRM：用生成式推理突破过程奖励模型的测试时算力瓶颈

### 📌 背景痛点/本文动机
大语言模型（LLMs）的发展让过程奖励模型（PRMs）作为验证器提升模型性能成为可能，但现有PRMs存在三大核心挑战：一是过程监督和泛化能力有限；二是依赖标量预测，未充分利用LLMs的生成能力；三是无法对PRMs的测试时算力进行扩展。为解决这些问题，本文提出了生成式过程奖励模型GenPRM。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：生成式过程奖励模型设计
GenPRM将过程监督重新定义为生成任务而非判别式评分任务，在给出最终判断前整合思维链（CoT）推理与代码验证过程，区别于传统基于分类的PRMs，充分发挥LLMs的生成优势。

💡 创新点2：相对进度估计（RPE）与合理解释合成框架
提出Relative Progress Estimation（RPE）来改进传统硬标签估计，利用相对准则实现更准确的标签估计；同时引入结合代码验证的合理解释合成框架，以获取高质量的过程监督推理数据。

💡 创新点3：测试时算力扩展（TTS）
通过测试时算力扩展手段，让GenPRM在推理性能提升上展现潜力，小模型也能通过该方式超越更大规模的传统PRMs，为PRMs的能力释放提供新路径。

### 📈 实验结果
在ProcessBench和多个数学推理任务上，GenPRM仅用MATH数据集的23K训练数据就显著超越了之前的PRMs。测试时扩展方面，1.5B参数的GenPRM性能超过GPT - 4o，7B参数的GenPRM在ProcessBench上超过Qwen2.5 - Math - PRM - 72B；同时GenPRM作为 critic 模型优化策略模型时也展现出强大能力，如GenPRM - 7B经过3次优化迭代后，性能提升幅度是DeepSeek - R1 - Distill - 7B的3.4倍。

### 💬 可借鉴之处
1. 模型设计思路：将生成式建模引入过程奖励模型，为PRMs赋予更强的推理与验证能力，打破传统判别式PRMs的局限，提供了从任务定义层面革新模型的思路。
2. 数据与标签处理：RPE和结合代码验证的合理解释合成框架，为获取高质量监督数据和标签提供了可参考的技术方案，在提升模型训练数据质量上有借鉴价值。
3. 测试时优化：探索测试时算力扩展在PRMs上的应用，证明小模型也能通过该方式实现性能跃迁，为模型在推理阶段的性能提升开辟了新方向，启发后续关于测试时优化策略的研究。 
```

## thinking-longer--not-larger--enhancing-software-engineering-agents-via-scaling-test-time-compute
### Abstract
Recent advancements in software engineering agents have demonstrated
promising capabilities in automating program improvements. However, their
reliance on closed-source or resource-intensive models introduces significant
deployment challenges in private environments, prompting a critical question:
\textit{How can personally deployable open-source LLMs achieve comparable code
reasoning performance?}
  To this end, we propose a unified Test-Time Compute scaling framework that
leverages increased inference-time computation instead of larger models. Our
framework incorporates two complementary strategies: internal TTC and external
TTC. Internally, we introduce a \textit{development-contextualized trajectory
synthesis} method leveraging real-world software repositories to bootstrap
multi-stage reasoning processes, such as fault localization and patch
generation. We further enhance trajectory quality through rejection sampling,
rigorously evaluating trajectories along accuracy and complexity. Externally,
we propose a novel \textit{development-process-based search} strategy guided by
reward models and execution verification. This approach enables targeted
computational allocation at critical development decision points, overcoming
limitations of existing "end-point only" verification methods.
  Evaluations on SWE-bench Verified demonstrate our \textbf{32B model achieves
a 46\% issue resolution rate}, surpassing significantly larger models such as
DeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical
validation of the test-time scaling phenomenon within SWE agents, revealing
that \textbf{models dynamically allocate more tokens to increasingly
challenging problems}, effectively enhancing reasoning capabilities. We
publicly release all training data, models, and code to facilitate future
research. https://github.com/yingweima2022/SWE-Reasoner
### 🌟 论文解读 | 不拼模型大小，拼推理时长：用测试时计算缩放提升软件工程智能体能力

### 📌 背景痛点/本文动机
基于大语言模型（LLM）的软件工程智能体在自动化程序改进任务（如 bug 修复、功能新增）中展现出潜力，但当前进展依赖闭源或资源密集型大模型，带来部署难题：一方面，像 DeepSeek V3 671B 这类大模型需多 GPU 高显存配置，多数机构难以负担；另一方面，闭源模型（如 Claude 3.5）通过 API 使用时，私有代码仓库存在隐私风险。因此，**如何让可个人部署的开源小模型（如单 GPU 能跑的 32B 模型）也达到优异的代码推理性能**，成为核心问题。

### 🚀 核心方法（介绍本文的几个创新点）
本文提出**测试时计算（Test - Time Compute，TTC）缩放框架**，不依赖模型参数规模扩张，而是通过提升推理时计算量来增强能力，包含内部 TTC 和外部 TTC 两大互补策略：

💡 创新点1：内部 TTC - 开发上下文轨迹合成与优化
为解决缺乏真实多阶段推理数据的问题，团队从高星 GitHub 仓库抓取 `<issue, repository, pull - request>` 三元组构建可执行验证环境，用 DeepSeek R1 生成涵盖“仓库理解、故障定位、补丁生成、补丁验证”的完整推理轨迹。再通过**开发上下文拒绝采样**，从准确性和复杂度维度过滤轨迹（筛除基础小模型无需优化就能解决的问题），保证轨迹质量。训练时保留每个推理步骤的“思考组件（规划、反思、修正）”和“答案组件（最终方案）”，让模型内化软件工程复杂任务的多步决策逻辑。

💡 创新点2：外部 TTC - 基于开发流程的搜索策略
现有方法多仅在最终阶段验证，本文则在“仓库理解、故障定位、补丁生成”三个关键开发阶段针对性分配计算资源。训练**过程奖励模型（PRM）**评估中间输出，提前剪枝差的路径；补丁生成阶段用自动生成的复现代码做执行验证；最终用**结果奖励模型（ORM）**基于验证过的补丁对排序选优。该策略让计算资源聚焦开发关键决策点，突破“只验证终点”的局限。

### 📈 实验结果
- 在 SWE - bench Verified 基准测试中，团队的 32B 模型达成 46% 的问题解决率，超过 DeepSeek R1 671B、OpenAI o1 等大得多的模型。 
- 验证了软件工程智能体中的“测试时缩放现象”：模型面对更具挑战的问题时，会动态分配更多 tokens，推理能力有效增强。

### 💬 可借鉴之处
- **范式转移思路**：证明不盲目堆模型参数，转而优化推理时计算利用（TTC 框架），小模型也能在软件工程任务上比肩大模型，为资源受限场景的 AI 部署提供新方向。 
- **数据与训练创新**：从真实软件仓库挖掘多阶段推理数据，结合拒绝采样优化轨迹质量，为训练“理解开发流程”的模型提供了可复用的数据构建与过滤方法。 
- **分阶段决策优化**：外部 TTC 中按开发流程关键节点分配计算、用奖励模型 + 执行验证引导搜索，这种“拆解任务阶段 + 针对性策略”的思路，可迁移到其他需多步骤决策的 AI 任务（如自动化运维、复杂文档处理）。 
- **开源生态贡献**：公开训练数据、模型和代码（https://github.com/yingweima2022/SWE - Reasoner），降低后续研究的复现门槛，推动领域发展。

## metascale--test-time-scaling-with-evolving-meta-thoughts
### Abstract
One critical challenge for large language models (LLMs) for making complex
reasoning is their reliance on matching reasoning patterns from training data,
instead of proactively selecting the most appropriate cognitive strategy to
solve a given task. Existing approaches impose fixed cognitive structures that
enhance performance in specific tasks but lack adaptability across diverse
scenarios. To address this limitation, we introduce METASCALE, a test-time
scaling framework based on meta-thoughts -- adaptive thinking strategies
tailored to each task. METASCALE initializes a pool of candidate meta-thoughts,
then iteratively selects and evaluates them using a multi-armed bandit
algorithm with upper confidence bound selection, guided by a reward model. To
further enhance adaptability, a genetic algorithm evolves high-reward
meta-thoughts, refining and extending the strategy pool over time. By
dynamically proposing and optimizing meta-thoughts at inference time, METASCALE
improves both accuracy and generalization across a wide range of tasks.
Experimental results demonstrate that MetaScale consistently outperforms
standard inference approaches, achieving an 11% performance gain in win rate on
Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,
METASCALE scales more effectively with increasing sampling budgets and produces
more structured, expert-level responses.
### 🌟 论文解读 | MetaScale：用进化元思维实现测试时推理能力扩展

### 📌 背景痛点/本文动机
大语言模型（LLMs）在复杂推理任务中面临一个关键挑战：它们依赖从训练数据中匹配推理模式，而非主动选择最适合的认知策略来解决特定任务。现有方法通过施加固定认知结构提升特定任务表现，但在多样场景下缺乏适应性。为解决此局限，论文提出 MetaScale 框架，旨在让 LLM 在测试时能基于“元思维（meta - thoughts）”动态适配推理策略。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 MetaScale 测试时扩展框架  
MetaScale 基于“元思维”——为每个任务定制的自适应思维策略。框架先初始化候选元思维池，元思维包含“认知心态（Cognitive Mindset，如采用的角色、专业视角）”和“问题解决策略（Problem - Solving Strategy，基于心态的结构化求解模式）”两部分，让 LLM 先思考“如何思考”再生成响应，突破静态预定义启发式策略的限制。

💡 创新点2：结合多臂老虎机与遗传算法实现动态优化  
 - 多臂老虎机（MAB）算法：采用 Upper Confidence Bound（UCB）选择策略，在推理时迭代选择和评估元思维，平衡探索（尝试新元思维）与利用（复用高奖励元思维），由奖励模型指导评估响应质量。  
 - 遗传算法：对高奖励元思维进行进化，随时间精炼和扩展策略池，进一步增强框架的适应性，让元思维能在多轮迭代中持续优化。

### 📈 实验结果
实验表明 MetaScale 持续超越标准推理方法：在 GPT - 4o 的 Arena - Hard 任务上胜率提升 11%；在风格控制任务中超越 o1 - mini 0.9%；且随着采样预算增加，能更高效扩展，生成更结构化、专家级的响应。

### 💬 可借鉴之处
 - 思维范式层面：引入“元思维”概念，让模型先反思思考方式再求解，为 LLM 推理过程的自适应调控提供新思路，启发后续研究关注“思考如何思考”的元认知能力增强。  
 - 技术融合层面：将多臂老虎机的探索 - 利用平衡、遗传算法的迭代进化与 LLM 推理结合，展示了强化学习与进化算法在提升 LLM 任务适配性上的潜力，为跨领域技术整合优化 LLM 性能提供参考。  
 - 应用价值层面：在多样任务中提升准确率与泛化性，且能高效利用采样预算，对实际场景中需要灵活推理、高质量响应生成的任务（如复杂问答、创意生成等）具有实践指导意义。

## sampling-efficient-test-time-scaling--self-estimating-the-best-of-n-sampling-in-early-decoding
### Abstract
Test-time scaling improves large language model performance by adding extra
compute during decoding. Best-of-N (BoN) sampling serves as a common scaling
technique, broadening the search space for finding better solutions from the
model distribution. However, traditional BoN requires N full generations,
leading to high GPU memory overhead and time latency. Moreover, some methods
depend on reward models, adding computational cost and limiting domain
generalization.
  In this paper, we propose Self-Truncation Best-of-N (ST-BoN), a novel
decoding method that avoids fully generating all samplings and eliminates the
need for reward models. ST-BoN introduces early sampling consistency to
estimate the most promising sample, truncating suboptimal ones to free memory
and accelerate inference. This pushes the sampling-efficient test-time scaling.
Compared to traditional BoN, ST-BoN can reduce dynamic GPU memory overhead by
over 90% and time latency by 50%, while achieving comparable or even better
performance across reasoning and open-ended domains.
### 🌟 论文解读 | 解码阶段高效采样：ST - BoN 让大模型推理又快又省

### 📌 背景痛点/本文动机
大语言模型（LLMs）虽有强生成与推理能力，但自回归解码易聚焦局部最优。测试时缩放技术（如 Best - of - N（BoN）采样）能拓展搜索空间找更优解，然而传统 BoN 存在两大问题：一是全生成开销，需完整生成 N 个样本，带来高 GPU 内存占用与时间延迟，复杂长序列推理时更甚；二是依赖奖励模型，训练奖励模型需高质量反馈数据，成本高且领域泛化性受限。为推动采样高效的测试时缩放，本文提出 Self - Truncation Best - of - N（ST - BoN）解码方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：确定最早估计时间 c
为实现效率最大化提升，最早估计时间 c 是所有采样在该时间点后两两不一致，而在之前时间点存在两两一致的时刻。此时满足特定条件（如公式4所示，在时间 c 时所有不同采样的前 c 个 token 序列无相同，而在 c 之前存在不同采样前 t 个 token 序列相同的情况），以此确定自估计最早能开展的时间点。
💡 创新点2：三步骤的 ST - BoN 解码流程
ST - BoN 包含三步：第一步，自回归生成 N 个采样直到最早估计时间 c；第二步，每个采样在时间 c 后继续生成 τ 步，期间逐次进行自估计操作，综合 τ + 1 次自估计确定最优采样；第三步，截断剩余 N - 1 个采样，仅生成自估计最优的样本直到 [EOS]。该流程避免完整生成所有采样，且无需奖励模型，通过早期采样一致性估计最有潜力样本，截断次优样本释放内存、加速推理。

### 📈 实验结果
在计算成本方面，与传统 Full - BoN 相比，ST - BoN 能减少超 90% 的动态 GPU 内存开销与 50% 的时间延迟；在推理和开放式任务领域的性能测试中，ST - BoN 能达到与 Full - BoN 相当甚至更优的性能，在不同领域和模型下，等效成本时表现更优，低成本时也能实现相似性能。此外，通过细粒度消融实验验证了早期自估计与最终答案正确性的一致性，还详细讨论超参数与鲁棒性，确认方法的正确性与泛化性。

### 💬 可借鉴之处
从方法设计角度，ST - BoN 启发我们在大模型解码等需采样优化的场景中，可探索早期阶段的自估计机制，利用模型内部知识减少不必要计算，为提升效率提供新思路；在工程落地层面，其对内存与时间成本的大幅优化思路，可指导大模型推理服务等场景的性能优化，降低部署成本；从研究视角，其对采样效率与性能平衡的探索，为后续测试时缩放技术相关研究提供了新范式与实验验证方向，如进一步挖掘不同任务下最早估计时间 c 和缓冲窗口 τ 等超参数的优化策略等。

## agentrm--enhancing-agent-generalization-with-reward-modeling
### Abstract
Existing LLM-based agents have achieved strong performance on held-in tasks,
but their generalizability to unseen tasks remains poor. Hence, some recent
work focus on fine-tuning the policy model with more diverse tasks to improve
the generalizability. In this work, we find that finetuning a reward model to
guide the policy model is more robust than directly finetuning the policy
model. Based on this finding, we propose AgentRM, a generalizable reward model,
to guide the policy model for effective test-time search. We comprehensively
investigate three approaches to construct the reward model, including explicit
reward modeling, implicit reward modeling and LLM-as-a-judge. We then use
AgentRM to guide the answer generation with Best-of-N sampling and step-level
beam search. On four types of nine agent tasks, AgentRM enhances the base
policy model by $8.8$ points on average, surpassing the top general agent by
$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding
greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the
specializability, AgentRM can also boost a finetuned policy model and
outperform the top specialized agent by $11.4$ on three held-in tasks. Further
analysis verifies its effectiveness in test-time scaling. Codes will be
released to facilitate the research in this area.
### 🌟 论文解读 | AgentRM：用奖励建模提升智能体泛化能力

### 📌 背景痛点/本文动机
近年来，基于大语言模型（LLM）的智能体在复杂交互任务中展现出潜力，但现有智能体在已知任务（held - in tasks）上表现尚可，对 unseen tasks（未见过的任务）的泛化能力却很差。以往一些工作试图通过在更多样化任务上微调策略模型（policy model）来提升泛化性，然而研究发现直接微调策略模型存在缺陷：微调策略模型会增加见过的动作 token 概率，降低未见过动作的概率，导致在 unseen tasks 上性能下降（如图 1(a) 所示，微调策略模型后 held - out tasks 性能严重退化）。于是本文提出猜想：微调奖励模型（reward model）来引导策略模型，会比直接微调策略模型更稳健，因为奖励函数的回归训练目标对动作 token 特定分布的敏感性更低。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出 AgentRM 框架
AgentRM 是一个可泛化的奖励模型，用于在测试时引导策略模型进行有效搜索。为构建该奖励模型，本文探索了三种有代表性的奖励建模方法：
 - **显式奖励建模（Explicit reward modeling）**：借助树搜索（如类蒙特卡洛树搜索算法）进行自动的过程奖励标注，将稀疏的最终结果奖励以可解释的方式分配到每个步骤，把过程奖励定义为 Q - value（从某一状态开始的预期累积奖励），并通过对搜索轨迹构建树结构等方式计算。
 - **隐式奖励建模（Implicit reward modeling）**：无需对步骤级奖励进行标注，而是通过对结果奖励训练来隐式学习步骤级奖励。
 - **LLM - as - a - judge**：这是一种无训练的方法，依赖大语言模型本身的通用评判能力，直接提示 LLM 来评估智能体的轨迹。

💡 创新点2：测试时搜索策略
使用 AgentRM 通过 Best - of - N sampling 和 step - level beam search 来引导答案生成，以此增强策略模型在 unseen tasks 上的决策能力。同时，先通过行为克隆（Behavior Cloning）得到具有基本任务能力的初始策略 πinit，用于收集高质量状态，为后续奖励模型训练等环节奠基。行为克隆是在专家轨迹 Dexpert 上进行有监督微调，损失函数如公式（1）所示，以此得到初始策略模型。

### 📈 实验结果
在包括网页导航、具身规划、文本游戏和工具使用这四类共九个智能体任务上进行了大量实验：
 - 显式奖励建模在这些任务中持续取得最显著的提升，AgentRM 平均能使基础策略模型提升 8.8 个点，超过顶尖的通用智能体 4.0 个点。
 - 体现了弱到强的泛化性，在 LLaMA - 3 - 8B 策略模型采样状态上训练的奖励模型，应用到 LLaMA - 3 - 70B 策略模型时，能带来 12.6 个点的提升。
 - 在专项能力方面，AgentRM 也能提升微调后的策略模型，在三个 held - in tasks 上超过顶尖的特定任务智能体 11.4 个点。此外，对训练数据规模趋势的分析以及对状态表示的消融实验等进一步验证了显式奖励建模的有效性等。

### 💬 可借鉴之处
 - 思路创新：提出通过奖励模型引导策略模型来提升泛化性，为解决智能体泛化难题提供了新的思路，不同于以往直接微调策略模型的方式，开辟了利用奖励模型的新方向。
 - 方法全面性：全面探索三种奖励建模方式，为后续研究者在奖励模型构建上提供了丰富的参考范式，无论是想采用有监督标注式的显式建模，还是隐式学习或者利用 LLM 评判能力的方式，都有对应的实践路径参考。
 - 实验丰富性：在多类任务上进行实验验证，且对泛化性、专项能力、测试时缩放等多方面进行分析，其实验设计和分析思路可为相关领域实验研究提供借鉴，帮助后续工作更全面地评估方法有效性。同时论文还计划发布代码，将为该领域研究提供工具层面的便利，促进相关研究发展。

## linguistic-generalizability-of-test-time-scaling-in-mathematical-reasoning
### Abstract
Scaling pre-training compute has proven effective for achieving
mulitlinguality, but does the same hold for test-time scaling? In this work, we
introduce MCLM, a multilingual math benchmark featuring competition-level
problems in 55 languages. We test three test-time scaling methods-Outcome
Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing
(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for
extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM
achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although
"thinking LLMs" have recently garnered significant attention, we find that
their performance is comparable to traditional scaling methods like best-of-N
once constrained to similar levels of inference FLOPs. Moreover, while BF
yields a 20-point improvement on English AIME, it provides only a 1.94-point
average gain across other languages-a pattern consistent across the other
test-time scaling methods we studied-higlighting that test-time scaling may not
generalize as effectively to multilingual tasks. To foster further research, we
release MCLM, MR1-1.5B, and evaluation results.
### 🌟 论文解读 | 数学推理中测试时缩放的语言泛化性研究

### 📌 背景痛点/本文动机
大语言模型（LLMs）在预训练阶段通过计算量缩放实现了多语言能力提升，但测试时缩放是否也能带来类似跨语言收益？此前测试时缩放方法在数学推理等领域探索较少，且数学推理因搜索空间大更具挑战，同时现有多语言数学基准要么简单问题饱和、要么语言覆盖有限。因此，本文旨在探究测试时缩放方法在多语言数学推理任务中的语言泛化性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建多语言数学基准MCLM  
创建了MCLM（Multilingual Competition Level Math）多语言数学推理基准，涵盖55种语言，包含机器翻译和人工标注的竞赛级数学题，分为MT - MATH100、MT - AIME2024、M - IMO、M - MO四个子集，覆盖不同难度与语言来源，能更全面评估多语言数学推理能力。  
💡 创新点2：测试三类测试时缩放方法  
选取Outcome Reward Modeling（ORM）、Process Reward Modeling（PRM）、Budget Forcing（BF）三种测试时缩放方法，在Qwen2.5 - 1.5B Math和自研多语言LLM（MR1 - 1.5B）上测试，探究不同方法在多语言场景下的表现。  
💡 创新点3：训练多语言思考型LLM MR1 - 1.5B  
基于Deepseek - R1 - 1.5B，用10万条由GPT - 4o翻译的R1 - distilled实例训练得到MR1 - 1.5B，虽仅1.5B参数，但在多语言数学推理上能与GPT - 4o - Mini媲美。

### 📈 实验结果
1. 性能对比：Qwen2.5 - 1.5B Math结合ORM在MCLM上得35.8分，MR1 - 1.5B结合BF得35.2分；当推理计算量（FLOPs）相近时，“思考型LLM”表现与best - of - N等传统缩放方法相当。  
2. 语言泛化性：以BF为例，在英语AIME任务上提升20分，但在其他语言上平均仅提升1.94分，其他测试时缩放方法也有类似模式，说明测试时缩放在多语言任务上泛化效果不佳，难以在多语言间稳定提升性能。  
3. 不同任务难度：ORM和PRM在较简单数据集有明显增益，但在高难度任务和不同语言间提升微弱且不一致；BF仅对英语难题提升明显，对其他语言影响小。

### 💬 可借鉴之处
1. 基准构建：MCLM为多语言复杂数学推理研究提供了高质量基准，其涵盖多语言、多难度、多来源的构建思路，可为其他多语言任务基准建设提供参考。  
2. 方法探索：对三类测试时缩放方法在多语言场景的系统测试，清晰展现了不同方法在多语言任务的表现差异，为后续优化测试时策略以适配多语言任务提供了实验依据。  
3. 模型训练：MR1 - 1.5B的训练方式证明小参数模型通过合适数据也能在多语言推理任务有竞争力，为资源有限下的多语言模型训练提供了思路。

## process-reward-models-for-llm-agents--practical-framework-and-directions
### Abstract
We introduce Agent Process Reward Models (AgentPRM), a simple and scalable
framework for training LLM agents to continually improve through interactions.
AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo
rollouts to compute reward targets and optimize policies. It requires minimal
modifications to existing RLHF pipelines, making it easy to integrate at scale.
Beyond AgentPRM, we propose InversePRM, which learns process rewards directly
from demonstrations without explicit outcome supervision. We also explore key
challenges and opportunities, including exploration, process reward shaping,
and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that
small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o
baselines, and analyze test-time scaling, reward hacking, and more. Our code is
available at: https://github.com/sanjibanc/agent_prm.
### 🌟 论文解读 | LLM智能体训练新范式：AgentPRM与InversePRM框架

### 📌 背景痛点/本文动机
大语言模型（LLM）智能体在决策类任务（如网页导航、机器人控制、交互式代码生成）中表现出色，但当前依赖人工提示或有监督微调的方式存在明显局限：提示工程需大量人工投入且无法自主迭代优化；有监督微调受限于演示质量，测试阶段也缺乏自我修正能力。强化学习（RL）虽能通过经验优化策略，却面临长时程决策（多步骤推理+结构化多token输出）、稀疏奖励（反馈延迟导致 credit assignment 难题）与高样本复杂度等挑战，难以直接落地。因此，如何让LLM智能体在少人工监督下通过交互自主提升，成为核心问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出Agent Process Reward Models（AgentPRM）框架  
AgentPRM基于轻量的“演员-评论家”范式，为LLM智能体提供持续交互优化能力。它通过**异步蒙特卡洛rollouts**自动生成过程奖励（PRM）的监督目标，无需人工标注奖励；并采用**迭代训练**机制，让PRM（评论家角色，提供细粒度中间反馈）与智能体策略（演员角色）相互迭代优化。该框架仅需对现有RLHF流水线做极小改动，就能规模化集成，解决长时程决策中稀疏奖励与样本效率问题——PRM通过评估中间动作而非依赖最终稀疏奖励，大幅提升样本利用效率。

💡 创新点2：提出InversePRM：无显式结果监督的过程奖励学习  
在AgentPRM基础上，进一步提出InversePRM。它**直接从专家演示中学习过程奖励**，无需显式的“结果奖励”监督。通过区分“专家优质轨迹（正样本）”与“智能体生成的非优质轨迹（负样本）”来训练PRM，再用PRM引导策略优化。相比AgentPRM，InversePRM在不增加复杂度的前提下，实现了更高的样本效率。

💡 创新点3：探索规模化与实用挑战的解法方向  
针对AgentPRM规模化过程中的探索（Exploration）、过程奖励塑造（Process Reward Shaping）、模型预测推理（Model-Predictive Reasoning）等核心挑战，论文结合经典RL技术（如重置分布、奖励塑造）与LLM驱动策略（如引导式探索、模型预测推理）展开分析，为后续研究指明方向。

### 📈 实验结果
论文在文本游戏基准测试ALFWorld上验证方法有效性：  
- AgentPRM训练的30亿参数小模型，性能**超越强基线GPT-4o**；并对训练曲线、测试时模型缩放、奖励黑客攻击（Reward Hacking）、绝对/相对损失等维度做了深入分析。  
- InversePRM在单轮迭代中就能接近专家性能，显著超越有监督微调（SFT），且比AgentPRM更具样本效率。  

### 💬 可借鉴之处
1. 框架设计轻量化与兼容性：AgentPRM和InversePRM对现有RLHF流水线侵入性极小，仅新增“自动奖励标注”环节，便于工业界快速集成复用。  
2. 解决RL落地LLM智能体的关键痛点：通过过程奖励拆解长时程稀疏奖励问题，用蒙特卡洛rollouts自动生成监督信号，降低对人工标注的依赖，为RL与LLM结合提供了更高效的范式。  
3. 开源与可复现：代码开源（https://github.com/sanjibanc/agent_prm），基于Gym封装的轻量 wrapper 能快速对接OpenInstruct等现有框架，降低研究与工程门槛。  
4. 挑战与方向的前瞻性：对探索、奖励塑造、模型预测推理等方向的分析，为后续优化LLM智能体的长时决策、样本效率等问题提供了清晰的研究路径参考。  

## can-1b-llm-surpass-405b-llm--rethinking-compute-optimal-test-time-scaling
### Abstract
Test-Time Scaling (TTS) is an important method for improving the performance
of Large Language Models (LLMs) by using additional computation during the
inference phase. However, current studies do not systematically analyze how
policy models, Process Reward Models (PRMs), and problem difficulty influence
TTS. This lack of analysis limits the understanding and practical use of TTS
methods. In this paper, we focus on two core questions: (1) What is the optimal
approach to scale test-time computation across different policy models, PRMs,
and problem difficulty levels? (2) To what extent can extended computation
improve the performance of LLMs on complex tasks, and can smaller language
models outperform larger ones through this approach? Through comprehensive
experiments on MATH-500 and challenging AIME24 tasks, we have the following
observations: (1) The compute-optimal TTS strategy is highly dependent on the
choice of policy model, PRM, and problem difficulty. (2) With our
compute-optimal TTS strategy, extremely small policy models can outperform
larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.
Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM
surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher
inference efficiency. These findings show the significance of adapting TTS
strategies to the specific characteristics of each task and model and indicate
that TTS is a promising approach for enhancing the reasoning abilities of LLMs.
### 🌟 论文解读 | 小模型也能逆袭？重新思考推理时计算最优的测试时缩放策略

### 📌 背景痛点/本文动机
大语言模型（LLMs）在众多领域取得显著进展，但测试时缩放（TTS）作为提升推理性能的重要方法，当前研究未系统分析策略模型、过程奖励模型（PRMs）和问题难度对其的影响，限制了对TTS方法的理解与实践应用。本文聚焦两大核心问题：不同策略模型、PRMs和问题难度下，如何最优缩放测试时计算；扩展计算能在多大程度提升复杂任务性能，小模型能否借此超越大模型，以此展开研究。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：全面评估TTS方法
使用多种前沿策略模型、多个PRMs、不同缩放方法，在更具挑战性的任务（如MATH - 500和AIME24）上对不同TTS方法进行全面评估，涵盖从0.5B到72B不同规模的策略模型与PRMs。
💡 创新点2：提出奖励感知的计算最优TTS
分析TTS过程中奖励的影响，引入奖励感知的计算最优TTS，表明计算最优缩放策略会随策略模型、PRM和问题难度不同而变化。

### 📈 实验结果
在MATH - 500和AIME24任务上的实验有重要发现：计算最优的TTS策略高度依赖策略模型、PRM和问题难度选择；借助计算最优TTS策略，极小的策略模型能超越大模型，如1B模型在MATH - 500上能超过405B模型；在两个任务上，0.5B模型表现优于GPT - 4o，3B模型超过405B模型，7B模型击败o1和DeepSeek - R1且推理效率更高。

### 💬 可借鉴之处
论文强调了根据任务和模型特性适配TTS策略的重要性，为提升LLM推理能力提供了新方向，让小模型在特定场景下通过TTS实现性能逆袭成为可能，为后续研究TTS在不同模型与任务的优化应用提供了思路，也启发从业者关注推理阶段计算分配对模型性能的影响，探索更高效的测试时策略。

## teaching-language-models-to-critique-via-reinforcement-learning
### Abstract
Teaching large language models (LLMs) to critique and refine their outputs is
crucial for building systems that can iteratively improve, yet it is
fundamentally limited by the ability to provide accurate judgments and
actionable suggestions. In this work, we study LLM critics for code generation
and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic
$\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which
trains a critic model to generate feedback that maximizes correction
performance for a fixed generator model without human supervision. Our results
demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance
pass rates and mitigate compounding errors across both base and stronger
generator models. Furthermore, we show that these critic models act as accurate
generative reward models and enable test-time scaling through iterative
critique-revision, achieving up to 106.1% relative improvements across
challenging code generation benchmarks.
### 🌟 论文解读 | 用强化学习教语言模型“挑刺”：CTRL框架助力代码生成迭代优化

### 📌 背景痛点/本文动机
大语言模型（LLMs）通过迭代反馈实现自我改进是当前研究热点，但现有机制存在明显局限：一方面，奖励模型把复杂评估压缩成简单数值信号，自动化验证工具给出的执行轨迹也难直接转化为高层修复方案，二者都难提供“既精准判断又能指导改进”的反馈；另一方面，没有合适外部反馈时，自我改进循环甚至会导致性能下降，“反馈瓶颈”成了阻碍LLM迭代优化的核心问题。在代码生成这类反馈机制相对成熟的领域，仅靠现有反馈也很难推动实质性提升。因此，如何训练出能提供有效反馈的“批评家（critic）”模型，成了突破点。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出CTRL框架，解耦“批评家”与“任务执行模型”  
CTRL（Critic Training via Reinforcement Learning）框架将专注于“挑刺+提改进建议”的批评家模型，和负责生成代码的任务执行模型解耦。不再让模型“自我批评”，而是训练专门的批评家，通过迭代的“批评 - 修正”过程，引导任务执行模型生成更优解。这种解耦让批评家训练有了清晰代理任务：以“能否驱动任务执行模型产出正确结果”来衡量批评家的有效性。  

💡 创新点2：两阶段训练 pipeline + GRPO 优化  
训练批评家时，先利用执行反馈合成高质量批评样例，做有监督微调（Supervised Finetuning）；再用 Group Relative Policy Optimization（GRPO）做强化学习优化。前者为批评家提供优质反馈的“示范”，后者则在大空间、高方差的批评生成场景中，优化批评家的反馈能力，让其输出更能推动任务执行模型修正错误。  

### 📈 实验结果
1. 跨基准测试全面领先：在CodeContests、LiveCodeBench、MBPP+、JudgeBench等多个代码生成基准测试中，经CTRL训练的批评家，表现远超“自我批判”类方法和用更强批评模型的方法。  
2. 弱到强的泛化能力：相对弱的批评家模型，能有效指导更强的任务执行模型（如GPT - 4o），类似“弱监督强模型”的泛化现象，证明批评家训练的高效性。  
3. 测试时高效迭代：通过针对性反馈，大幅减少修正迭代次数，降低token消耗同时提升成功率。且能缓解“错误复利”——早期精准揪出并修正错误，引导模型走更直接的解题路径。  
4. 跨模型提升效果：无论基础还是更强的生成模型，经CTRL训练的批评家都能显著提升“通过率（pass rates）”，减少错误累积。在部分场景相对提升达106.1%。  

### 💬 可借鉴之处
1. 解耦思路的启发：把“评估反馈”和“任务执行”解耦，能更聚焦地优化反馈质量，这种模块化设计在LLM工具链、迭代系统构建中值得借鉴。  
2. 两阶段训练范式：先监督微调注入优质反馈模式，再强化学习优化实用性，为训练“辅助型”LLM（如批评家、规划器等）提供了一套可参考的 pipeline。  
3. 弱监督强模型的实践：证明弱模型经训练后可有效监督强模型，为资源有限时训练高效“辅助组件”提供了新思路，尤其在代码、数学推理等需要迭代修正的场景。  
4. 迭代反馈的效率优化：通过减少错误复利实现测试时高效迭代，提示在构建LLM迭代系统时，要重视“反馈精准性”对整体效率的影响，可从反馈内容设计、批评家训练等角度切入优化。

## stair--improving-safety-alignment-with-introspective-reasoning
### Abstract
Ensuring the safety and harmlessness of Large Language Models (LLMs) has
become equally critical as their performance in applications. However, existing
safety alignment methods typically suffer from safety-performance trade-offs
and the susceptibility to jailbreak attacks, primarily due to their reliance on
direct refusals for malicious queries. In this paper, we propose STAIR, a novel
framework that integrates SafeTy Alignment with Itrospective Reasoning. We
enable LLMs to identify safety risks through step-by-step analysis by
self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR
first equips the model with a structured reasoning capability and then advances
safety alignment via iterative preference optimization on step-level reasoning
data generated using our newly proposed Safety-Informed Monte Carlo Tree Search
(SI-MCTS). We further train a process reward model on this data to guide
test-time searches for improved responses. Extensive experiments show that
STAIR effectively mitigates harmful outputs while better preserving
helpfulness, compared to instinctive alignment strategies. With test-time
scaling, STAIR achieves a safety performance comparable to Claude-3.5 against
popular jailbreak attacks. Relevant resources in this work are available at
https://github.com/thu-ml/STAIR.
### 🌟 论文解读 | STAIR：用“自省式推理”革新大模型安全对齐

### 📌 背景痛点/本文动机
大语言模型（LLMs）在医疗、教育、法律等诸多高风险领域的广泛应用，使其安全性与无害性变得和性能同等关键。然而现有安全对齐方法存在两大核心问题：一是安全与性能的权衡困境（为了安全牺牲实用性，反之亦然）；二是易受“越狱攻击”（jailbreak attacks）——攻击者通过对抗性后缀、伪装等手段绕过模型的直接拒答机制，诱导模型生成有害内容。本质上，现有方法依赖“直觉式拒答”（类似心理学中“系统1”的本能反应），缺乏对风险的深度分析，导致面对复杂伪装攻击时防线崩溃。因此，论文提出**STAIR**框架，试图用“自省式推理”（对应“系统2”的深思熟虑）让模型在拒答前先分析风险，从根源提升安全对齐能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：结构化CoT格式对齐，赋予模型“系统2”推理能力  
传统安全对齐让模型直接拒答恶意请求，而STAIR第一步是**教会模型用结构化思维链（CoT）分析风险**。通过在安全与有用性混合数据集上微调，让模型从“直接说抱歉”转向“逐步推理风险”，为后续深度安全分析打下基础。这一步是让模型从“本能反应”过渡到“逻辑推理”的关键铺垫。

💡 创新点2：基于安全感知的蒙特卡洛树搜索（SI-MCTS）的迭代自改进  
在结构化CoT基础上，STAIR引入**Safety-Informed MCTS（SI-MCTS）**生成“步骤级推理数据”，并基于这些数据做迭代偏好优化（如步骤级DPO）。SI-MCTS的核心是设计“安全感知奖励”：不仅考虑“有用性”，还将安全相关信息注入搜索节点（每个节点代表推理步骤），引导模型探索更安全的推理路径。这种迭代机制让模型无需额外人工标注，就能持续自我提升安全推理能力。  

💡 创新点3：过程奖励模型（PRM）与测试时搜索，放大安全与性能收益  
利用SI-MCTS生成的偏好数据，训练**过程奖励模型（PRM）**。测试阶段，结合Best-of-N或Beam Search等搜索算法，让PRM引导模型在生成响应时进行更审慎的推理，进一步提升输出质量（安全+有用性）。这一步实现了“训练-推理”闭环，让安全对齐的收益在实际部署时持续生效。  


### 📈 实验结果
1. 安全抗性显著提升：在“StrongReject”任务中，STAIR为LLaMA模型实现了0.88的“安全评分”，比最优基线高0.15，对各类恶意查询的抵抗能力大幅增强。  
2. 缓解安全-性能权衡：相比仅做安全对齐的基线，STAIR在“有用性、真实性、鲁棒性、隐私感知”等维度均有提升。例如在AlpacaEval上，LLaMA和Qwen相对于其基础模型，对GPT-4的胜率分别提升13.11%和6.25%，而多数基线仅能“保安全但丢性能”。  
3. 对抗越狱攻击比肩 Claude-3.5：当测试时启用推理增强（test-time scaling），STAIR在“StrongReject”上的安全评分达到0.94，与Claude-3.5相当，证明其在实战级攻击下的竞争力。  


### 💬 可借鉴之处
1. 思维链（CoT）与安全对齐的结合：打破“直接拒答”的惯性，用“推理分析”替代“直觉反应”，为安全对齐提供了“深度理解风险”的新思路，可推广到需要复杂风险判断的场景（如金融、医疗问答）。  
2. 自改进与搜索机制的闭环：SI-MCTS+迭代优化+过程奖励模型的组合，展示了“让模型自己生成数据、自己优化、自己引导推理”的自驱动范式，减少对人工标注的依赖，适合大规模模型的持续迭代。  
3. 多目标平衡的实践：通过“安全感知奖励”“步骤级优化”等设计，证明安全与性能并非零和博弈，为后续多目标对齐任务提供了可复用的方法论（如何拆解目标、设计中间步骤奖励）。  


STAIR的核心突破在于：把“安全对齐”从“直觉拒答”升级为“自省推理”，用结构化思维链+自改进搜索+过程奖励模型，打造了一套“先想清楚风险，再安全响应”的机制。这不仅提升了对抗攻击的能力，更让大模型在高风险场景下的“安全性-实用性”平衡成为可能，为下一代安全对齐技术指明了“深度推理+自驱动优化”的方向。

## sets--leveraging-self-verification-and-self-correction-for-improved-test-time-scaling
### Abstract
Recent advancements in Large Language Models (LLMs) have created new
opportunities to enhance performance on complex reasoning tasks by leveraging
test-time computation. However, existing parallel scaling methods, such as
repeated sampling or reward model scoring, often suffer from premature
convergence and high costs due to task-specific reward model training, while
sequential methods like SELF-REFINE cannot effectively leverage increased
compute. This paper introduces Self-Enhanced Test-Time Scaling (SETS), a new
approach that overcomes these limitations by strategically combining parallel
and sequential techniques. SETS exploits the inherent self-verification and
self-correction capabilities of LLMs, unifying sampling, verification, and
correction within a single framework. This innovative design facilitates
efficient and scalable test-time computation for enhanced performance on
complex tasks. Our comprehensive experimental results on challenging benchmarks
spanning planning, reasoning, math, and coding demonstrate that SETS achieves
significant performance improvements and more advantageous test-time scaling
behavior than the alternatives.
### 🌟 论文解读 | SETS：借自验证与自修正之力，革新测试时算力缩放

### 📌 背景痛点/本文动机
大语言模型（LLMs）在训练与测试阶段的算力利用推动了其在复杂任务上的表现，但现有测试时算力缩放方法存在局限。并行缩放方法（如重复采样、奖励模型打分）易过早收敛且因特定任务奖励模型训练成本高；顺序缩放方法（如SELF - REFINE）难以有效利用增加的算力，性能易快速饱和。同时，早期LLM自修正能力有限限制了方法创新，而如今LLM自验证与自修正能力提升，为重新思考测试时缩放提供了契机，因此本文旨在提出结合并行与顺序缩放的新方法。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出SETS框架
SETS（Self - Enhanced Test - Time Scaling）创新性地结合并行与顺序缩放技术，将采样、自验证、自修正统一在一个框架内。利用LLM固有的自验证和自修正能力，克服了现有并行与顺序方法各自的缺陷，实现更高效且可扩展的测试时计算，以提升复杂任务表现。
💡 创新点2：无需外部奖励模型
与一些需训练额外特定任务验证器或修正模型的方法不同，SETS不依赖外部奖励模型，依靠LLM自身的自验证与自修正能力来进行测试时算力缩放，降低了训练开销与任务特异性限制，增强了通用性。

### 📈 实验结果
在涵盖规划（NATURAL PLAN）、推理（LiveBench Reasoning）、数学（MATH 500、AIME 2024 - 2025）、编码（LiveCodeBench TestOutputPred）的五大挑战性基准测试中，SETS展现出明显优势。相比重复采样等并行方法和SELF - REFINE等顺序方法，SETS在测试时缩放中保持更高有效性，性能增益下降更少，在规划、推理、数学和编码基准上，使用非思考型和思考型模型（如GEMINI - 1.5 - Pro和GEMINI - 2.5 - Flash）时，准确率最多提升10.9%。此外，消融实验表明SETS对关键超参数（如自修正轮次最大值、LLM推理温度）鲁棒，只需少量超参数调优就能实现强性能。

### 💬 可借鉴之处
1. 方法融合思路：当面临需结合不同技术优势解决问题时，可借鉴SETS这种将并行与顺序技术策略性结合的思路，突破单一方法的局限。
2. 利用模型自身能力：在构建AI系统解决任务时，充分挖掘模型自身的自验证、自修正等内在能力，减少对外部复杂组件（如特定任务奖励模型）的依赖，提升通用性与效率。
3. 实验与调优角度：进行方法评估时，不仅要在多任务基准上验证效果，也可通过消融实验分析关键超参数影响，为方法的鲁棒性与易用性提供支撑，这对后续方法改进与落地有参考价值。

## pairjudge-rm--perform-best-of-n-sampling-with-knockout-tournament
### Abstract
Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large
Language Models (LLMs), relies on reward models to select the best candidate
solution from multiple generations. However, traditional reward models often
assign arbitrary and inconsistent scores, limiting their effectiveness. To
address this, we propose a Pairwise Judge Reward Model (PariJudge RM) combined
with a knockout tournament for BoN sampling. Instead of assigning absolute
scores, given one math problem, PariJudge RM judges two candidate solutions'
correctness with chain-of-thought reasoning simultaneously. This approach
eliminates the need for scoring and enables cross-validation of solutions
through parallel judgment. In the knockout tournament, PariJudge RM conducts
pairwise Judgment between candidate solutions and eliminates the incorrect ones
iteratively. We construct PairJudge-432K, a large-scale dataset of 432K
pairwise judgments derived from NumiaMath and annotated using
\texttt{gemini-1.5-flash}, and train the PariJudge RM via supervised
fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate
significant improvements over baseline reward models. And a 40\% to 60\%
relative improvement is achieved on the top 50\% challenging problems.
### 🌟 论文解读 | PairJudge RM：用淘汰赛机制革新大模型Best-of-N采样

### 📌 背景痛点/本文动机
在大语言模型（LLM）的测试时扩展策略中，Best-of-N（BoN）采样是常用手段，它依赖奖励模型从多个生成结果里选最优解。但传统奖励模型存在**分数随意性与不一致性**问题——即便人类专家按同一标准打分，结果也可能差异很大，更不用说模型训练出的相对分数往往缺乏绝对参考价值，这严重限制了BoN采样的效果。因此，如何让奖励模型更可靠地完成候选解筛选，成为亟待解决的问题。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：Pairwise Judge Reward Model（PairJudge RM）设计  
不再给单个候选解打“绝对分数”，而是**对同一数学问题的两个候选解，用思维链（CoT）推理同时判断正确性**。通过 pairwise（成对）判断，既避免了传统奖励模型“打分随意”的缺陷，又能让候选解之间通过并行判断实现交叉验证，从机制上提升判断可靠性。  

💡 创新点2：淘汰赛（Knockout Tournament）流程  
为实现BoN采样，将所有候选解组织成“淘汰赛”形式：PairJudge RM 对候选解两两配对做判断，迭代淘汰错误解，直到只剩一个候选解作为最终输出。这种方式把“选最优”转化为“逐步淘汰次优”，用 pairwise 判断的“胜负”逻辑替代传统的“分数排序”，让选择过程更直观且鲁棒。  

💡 创新点3：PAIRJUDGE - 432K 大规模数据集构建  
从 NumiaMath 衍生出 43.2 万条成对判断数据，并用 gemini - 1.5 - flash 完成标注，为 PairJudge RM 的有监督微调提供高质量训练素材。数据集+模型训练 pipeline 的开源，也为后续研究打下基础。  


### 📈 实验结果
在 MATH - 500 和 Olympiad Bench 等数学推理基准测试中，PairJudge RM 展现出对传统判别式奖励模型的显著优势：  
- **整体性能**：在 BoN 采样任务上全面超越基线奖励模型；  
- **难题表现**：针对 MATH - 500 中难度前 50% 的问题，相对基线实现了 40% - 60% 的性能提升；  
- **对比前沿**：在相同计算预算下，效果优于近期提出的 Critic Model 等方法。  


### 💬 可借鉴之处
1. **思路革新**：用“成对判断+淘汰机制”替代“绝对打分+排序”，为奖励模型设计提供了跳出“分数陷阱”的新思路，可迁移到其他需要“选优”的场景（如代码生成、创意写作等）；  
2. **数据构建**：大规模、高质量的成对标注数据集（PAIRJUDGE - 432K）证明了“基于现有资源衍生+强模型标注”的高效数据构建范式，为领域特定模型训练提供参考；  
3. **工程落地**：将“淘汰赛”这种直观的竞争机制引入模型推理流程，简化了“选最优”的工程实现逻辑，降低了对“精确分数校准”的依赖，更易在实际产品中落地。  


这篇论文从核心模型设计、数据构建到推理流程都做了创新性突破，为大模型测试时的“选优”环节提供了一套更可靠、更易落地的方案，值得关注大模型推理优化的研究者和工程师深入研究~

## internlm-xcomposer2-5-reward--a-simple-yet-effective-multi-modal-reward-model
### Abstract
Despite the promising performance of Large Vision Language Models (LVLMs) in
visual understanding, they occasionally generate incorrect outputs. While
reward models (RMs) with reinforcement learning or test-time scaling offer the
potential for improving generation quality, a critical gap remains: publicly
available multi-modal RMs for LVLMs are scarce, and the implementation details
of proprietary models are often unclear. We bridge this gap with
InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective
multi-modal reward model that aligns LVLMs with human preferences. To ensure
the robustness and versatility of IXC-2.5-Reward, we set up a high-quality
multi-modal preference corpus spanning text, image, and video inputs across
diverse domains, such as instruction following, general understanding,
text-rich documents, mathematical reasoning, and video understanding.
IXC-2.5-Reward achieves excellent results on the latest multi-modal reward
model benchmark and shows competitive performance on text-only reward model
benchmarks. We further demonstrate three key applications of IXC-2.5-Reward:
(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward
with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows
consistent improvements in instruction following and multi-modal open-ended
dialogue; (2) Selecting the best response from candidate responses for
test-time scaling; and (3) Filtering outlier or noisy samples from existing
image and video instruction tuning training data. To ensure reproducibility and
facilitate further research, we have open-sourced all model weights and
training recipes at
https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward
### 🌟 论文解读 | InternLM-XComposer2.5-Reward：填补多模态奖励模型空白的高效方案

### 📌 背景痛点/本文动机
大视觉语言模型（LVLMs）在视觉理解方面表现出色，但偶尔也会生成错误输出。奖励模型（RMs）结合强化学习或测试时缩放虽有提升生成质量的潜力，然而公开可用的多模态奖励模型稀缺，且专有模型实现细节不明。针对此，论文提出InternLM - XComposer2.5 - Reward（IXC - 2.5 - Reward）来填补这一空白，让LVLMs与人类偏好对齐。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建高质量多模态偏好语料库
为确保IXC - 2.5 - Reward的鲁棒性和通用性，构建了涵盖文本、图像、视频输入，跨指令遵循、通用理解、文本丰富文档、数学推理、视频理解等多样领域的高质量多模态偏好语料库。该语料库构建 pipeline 会为不同模态输入选择多样领域的提示，生成对应响应，再用GPT - 4o或验证器做偏好判断。
💡 创新点2：设计简单有效的多模态奖励模型架构
不直接将单模态（文本）奖励模型迁移到视觉模态，而是在现有LVLM（InternLM - XComposer2.5）基础上增加一个评分头来预测奖励分数，使其能有效评估视觉（图像和视频）和文本输入。
💡 创新点3：展示奖励模型三大关键应用
一是为强化学习训练提供监督信号，将IXC - 2.5 - Reward与近端策略优化（PPO）结合得到IXC - 2.5 - Chat，提升指令遵循和多模态开放式对话能力；二是在测试时缩放中从候选响应里选最佳响应；三是从现有图像和视频指令调优训练数据中过滤异常或噪声样本。

### 📈 实验结果
IXC - 2.5 - Reward在最新多模态奖励模型基准VL - RewardBench上表现出色，取得70.0%的成绩，击败包括Gemini - 1.5 - Pro（62.5%）和GPT - 4o（62.4%）在内的之前所有生成式奖励模型；在纯文本奖励模型基准上也有竞争力，在Reward - Bench上平均得分88.6%，在RM - Bench上得分68.8%。同时，在RL训练、测试时缩放、数据清理三大应用场景的实验也验证了其有效性，如IXC - 2.5 - Chat在多模态指令遵循和野外聊天基准上有明显改进等。

### 💬 可借鉴之处
1. 多模态数据构建思路：论文构建跨模态、跨领域多模态偏好语料库的方式，为后续多模态模型训练数据构建提供了参考，强调了数据多样性和高质量标注的重要性。
2. 模型扩展与应用方向：从单模态奖励模型扩展到多模态的思路，以及展示的奖励模型在强化学习、测试时优化、数据清理等多场景应用，为奖励模型乃至大模型生态中不同模块的协作和功能拓展提供了范例。
3. 开源与可复现性：将模型权重和训练方案开源，利于社区基于此进一步研究，这种开源共享的做法也值得相关研究借鉴，推动领域发展。

## meds$^3$--towards-medical-small-language-models-with-self-evolved-slow-thinking
### Abstract
Medical language models (MLMs) have become pivotal in advancing medical
natural language processing. However, prior models that rely on pre-training or
supervised fine-tuning often exhibit low data efficiency and limited
practicality in real-world clinical applications. While OpenAI's o1 highlights
test-time scaling in mathematics, attempts to replicate this approach in
medicine typically distill responses from GPT-series models to open-source
models, focusing primarily on multiple-choice tasks. This strategy, though
straightforward, neglects critical concerns like data privacy and realistic
deployment in clinical settings. In this work, we present a deployable,
small-scale medical reasoning system, MedS3, designed for long-chain reasoning
in clinical tasks using a self-evolution paradigm. Starting with a seed dataset
of around 8,000 instances spanning five domains and 16 datasets, we prompt a
base policy model to perform Monte Carlo Tree Search (MCTS) to construct
rule-verifiable reasoning chains. Each reasoning step is assigned an evolution
rollout value, allowing verified trajectories to train the policy model and the
process reward model (PRM). During inference, the policy model generates
multiple responses, and the reward model selects the one with a newly proposed
PRM-guided Vote-Sum (P-VS) strategy. Experiments on eleven evaluation datasets
demonstrate that MedS3 outperforms not only the prior strongest medical model
by 6.59, but also 32B-level general reasoning models by 8.71 points. Code and
data are available at https://github.com/pixas/MedSSS.
### 🌟 论文解读 | MedS³：用自进化慢思考打造可部署的医疗小语言模型

### 📌 背景痛点/本文动机
医疗语言模型（MLMs）在推动医疗自然语言处理发展中至关重要，但以往依赖预训练或有监督微调的模型存在数据效率低、实际临床应用实用性有限等问题。一方面，大规模预训练需大量计算资源且下游任务增益有限；有监督微调依赖的人工标注数据集常提供简洁响应，会降低模型语言流畅性。另一方面，用大语言模型生成合成语料存在幻觉问题，直接用于训练限制模型优化空间。同时，尝试复制OpenAI o1在数学领域“慢思考”推理的医学领域工作，多是蒸馏GPT系列模型响应到开源模型，聚焦多选任务，还忽视了数据隐私与临床部署等关键问题。因此，打造能在临床任务中进行长链推理、可部署的小规模医疗推理系统很有必要。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：自进化框架赋能小模型长链推理  
提出首个专为小规模医疗模型设计的自进化框架，以实现长链推理能力。从涵盖5个领域、16个数据集的约8000个实例的种子数据集入手，引导基础策略模型执行蒙特卡洛树搜索（MCTS）来构建可规则验证的推理链。每个推理步骤分配进化rollout值，让经过验证的轨迹去训练策略模型和过程奖励模型（PRM），实现数据高效的性能提升，适用于广泛临床应用场景。  

💡 创新点2：构建策略模型与过程奖励模型协同体系  
 - 策略模型π：通过在MCTS生成的合成数据上进行有监督学习微调基础策略π₀得到，用于生成推理过程。  
 - 过程奖励模型（PRM）Vθ：用带软标签的逐步监督进行微调，给每个推理步骤分配[0,1]范围的值表示步骤正确性，为推理准确性提供细粒度指导。  
 - 推理阶段采用PRM引导的Vote - Sum（P - VS）策略：策略模型生成多个响应后，奖励模型基于该策略选出最优解，从训练到推理环节形成协同优化与选择机制。  

### 📈 实验结果
在11个评估数据集上的实验表明，MedS³不仅比之前最强的医疗模型性能高出6.59分，还超过了32B规模的通用推理模型8.71分，在临床推理基准测试中展现出全面的性能提升，证明了自进化框架与过程奖励模型等设计在医疗长链推理任务上的有效性。  

### 💬 可借鉴之处
1. 框架设计层面：自进化框架为资源受限场景下小模型提升特定领域（如医疗）长链推理能力提供了新思路，摆脱对大规模预训练或闭源大模型蒸馏的强依赖，实现数据高效利用。  
2. 模型协同层面：策略模型与过程奖励模型的协同设计，以及推理时的引导选择策略，为构建更精准、可解释的推理型语言模型提供了模块化的参考范式，可迁移到其他需要长链推理的垂直领域。  
3. 开源资源层面：公开释放策略微调语料和过程奖励模型语料，为医疗AI领域后续研究提供了有价值的资源，利于行业共建生态推动技术进步。

## rearter--retrieval-augmented-reasoning-with-trustworthy-process-rewarding
### Abstract
Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)
hold promise in knowledge-intensive tasks but face limitations in complex
multi-step reasoning. While recent methods have integrated RAG with
chain-of-thought reasoning or test-time search using Process Reward Models
(PRMs), these approaches encounter challenges such as a lack of explanations,
bias in PRM training data, early-step bias in PRM scores, and insufficient
post-training optimization of reasoning potential. To address these issues, we
propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding
(ReARTeR), a framework that enhances RAG systems' reasoning capabilities
through post-training and test-time scaling. At test time, ReARTeR introduces
Trustworthy Process Rewarding via a Process Reward Model for accurate scalar
scoring and a Process Explanation Model (PEM) for generating natural language
explanations, enabling step refinement. During post-training, it utilizes Monte
Carlo Tree Search guided by Trustworthy Process Rewarding to collect
high-quality step-level preference data, optimized through Iterative Preference
Optimization. ReARTeR addresses three core challenges: (1) misalignment between
PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM
training data, mitigated by balanced annotation methods and stronger
annotations for challenging examples; and (3) early-step bias in PRM, resolved
through a temporal-difference-based look-ahead search strategy. Experimental
results on multi-step reasoning benchmarks demonstrate significant
improvements, underscoring ReARTeR's potential to advance the reasoning
capabilities of RAG systems.
### 🌟 论文解读 | ReARTeR：用可信过程奖励增强检索增强推理能力

### 📌 背景痛点/本文动机
大语言模型（LLM）的检索增强生成（RAG）系统在知识密集型任务中表现出潜力，但在复杂多步推理任务上仍存在局限。现有结合RAG与思维链推理或用过程奖励模型（PRM）做测试时搜索的方法，面临缺乏解释性、PRM训练数据存在偏差、PRM分数的早期步骤偏差以及推理潜力在训练后优化不足等问题。为解决这些挑战，论文提出ReARTeR框架，从训练后阶段和测试时扩展两方面增强RAG系统的推理能力。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：可信过程奖励机制（测试时阶段）
ReARTeR在测试时引入可信过程奖励，由过程奖励模型（PRM）和过程解释模型（PEM）共同实现。PRM提供精确的标量分数用于评估推理步骤，PEM生成自然语言解释来辅助对低分步骤进行优化，让推理步骤可精细化调整，解决了现有PRM缺乏解释性、不利于测试时推理优化的问题。

💡 创新点2：训练后阶段的优化策略
在训练后阶段，利用可信过程奖励引导的蒙特卡洛树搜索（MCTS）来收集高质量的步骤级偏好数据，再通过迭代偏好优化对模型进行优化。针对PRM与PEM不一致问题，采用离策略偏好学习，依据PEM解释优化前后PRM分数的变化来构建偏好标签，让二者对齐；针对PRM训练数据偏差，借助OmegaPRM平衡正负样本，并对难题引入更强模型或人类专家标注来提升PRM对复杂场景的判别力；针对PRM早期步骤偏差，提出基于时序差分（TD）的前瞻搜索策略，用模拟未来推理步骤计算预期奖励来更新当前步骤奖励估计，平衡偏差与方差。

### 📈 实验结果
在多步推理基准测试中，ReARTeR展现出显著的性能提升，有力证明了其在提升RAG系统推理能力方面的潜力，能有效应对复杂多步推理场景下的各类挑战，让RAG系统在推理表现上更上一层楼。

### 💬 可借鉴之处
1. 多阶段协同优化思路：将训练后优化与测试时增强相结合，为提升模型推理能力提供了全流程优化的参考范式，启示后续工作可从不同阶段入手系统性增强模型能力。
2. 解决模型组件不一致问题的方法：离策略偏好学习为解决不同模型组件（如这里的PRM和PEM）之间的对齐问题提供了新颖思路，可迁移到其他多组件协作的AI系统优化中。
3. 数据偏差与步骤偏差处理：针对训练数据偏差采用的平衡标注和强标注方法、针对早期步骤偏差的时序差分前瞻搜索，为处理模型训练和推理过程中的各类偏差问题提供了具体技术手段，在类似存在数据分布不均、步骤特性差异导致性能问题的场景中可借鉴。 

## ursa--understanding-and-verifying-chain-of-thought-reasoning-in-multimodal-mathematics
### Abstract
Process Reward Models (PRMs) have shown promise in enhancing the mathematical
reasoning capabilities of Large Language Models (LLMs) through Test-Time
Scaling (TTS). However, their integration into multimodal reasoning remains
largely unexplored. In this work, we take the first step toward unlocking the
potential of PRMs in multimodal mathematical reasoning. We identify three key
challenges: (1) the scarcity of high-quality reasoning data constrains the
capabilities of foundation Multimodal Large Language Models (MLLMs), which
imposes further limitations on the upper bounds of TTS and reinforcement
learning (RL); (2) a lack of automated methods for process labeling within
multimodal contexts persists; (3) the employment of process rewards in unimodal
RL faces issues like reward hacking, which may extend to multimodal scenarios.
To address these issues, we introduce URSA, a three-stage Unfolding multimodal
Process-Supervision Aided training framework. We first construct MMathCoT-1M, a
high-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset,
to build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we
go through an automatic process to synthesize process supervision data, which
emphasizes both logical correctness and perceptual consistency. We introduce
DualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose
Process-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a
multimodal PRM-aided online RL method that outperforms vanilla GRPO. With
PS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4%
and 2.7% on average across 6 benchmarks. Code, data and checkpoint can be found
at https://github.com/URSA-MATH.
```
### 🌟 论文解读 | URSA：解锁多模态数学推理中过程奖励模型的潜力

### 📌 背景痛点/本文动机
在大语言模型（LLMs）数学推理取得进展后，多模态大语言模型（MLLMs）的数学推理能力也备受关注。过程奖励模型（PRMs）在提升LLMs数学推理能力上展现出潜力，但在多模态推理中的应用仍未充分探索。当前存在三大挑战：一是高质量推理数据稀缺，限制了基础MLLMs能力，进而影响测试时缩放（TTS）和强化学习（RL）上限；二是多模态情境下缺乏自动化过程标注方法；三是单模态RL中过程奖励存在奖励黑客等问题，可能延伸到多模态场景。本文旨在迈出将PRMs融入多模态数学推理的第一步，应对这些挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：构建URSA三阶段训练框架  
提出URSA，一个三阶段的“展开式多模态过程监督辅助”训练框架。第一阶段构建大规模高质量多模态思维链（CoT）推理数据集MMathCoT - 1M，用于打造更强的数学推理基础MLLM（URSA - 8B）。从143万开源示例合成该数据集，通过针对性指令调优增强基础模型推理能力；第二阶段自动合成过程监督数据，构建DualMath - 1.1M用于训练过程奖励模型，该数据合成策略结合二进制错误定位引擎和误解插入引擎，强调逻辑正确性和感知一致性；第三阶段提出Process - Supervised Group - Relative - Policy - Optimization（PS - GRPO），这是一种多模态PRM辅助的在线RL方法，优于原始GRPO，通过在策略优化中隐式惩罚过程级不一致，缓解奖励黑客和长度偏差问题。

💡 创新点2：发布两大开源数据集  
发布MMathCoT - 1M和DualMath - 1.1M两个大规模开源数据集。MMathCoT - 1M解决高质量多模态CoT推理数据稀缺问题，DualMath - 1.1M解决过程监督数据稀缺问题，为多模态数学推理研究提供数据支撑。

💡 创新点3：提出PS - GRPO算法  
提出PS - GRPO在线强化学习算法，该算法通过比较轨迹的相对质量来整合多模态PRMs，而非依赖标量奖励建模，有效缓解了PRM的奖励黑客和奖励中的长度偏差问题，在在线训练中发挥作用。

### 📈 实验结果
在6个多模态推理基准测试中，应用PS - GRPO的URSA - 8B - PS - GRPO表现出色。平均而言，它在这6个基准上超越Gemma3 - 12B 8.4%，超越GPT - 4o 2.7%，在同类规模开源MLLMs中达到 state - of - the - art 性能，且过程奖励模型提升了测试时验证效果。

### 💬 可借鉴之处
1. 数据构建方面：通过整合与处理开源数据来构建大规模高质量数据集，为模型训练提供坚实基础，这种数据驱动的思路在多模态领域数据稀缺时值得借鉴，可用于其他多模态任务数据构建。
2. 框架设计方面：三阶段的URSA框架为多模态下过程奖励模型的应用提供了完整的 pipeline 参考，从基础模型增强到过程监督数据合成再到强化学习方法创新，各阶段环环相扣，为解决多模态推理中多个痛点提供了体系化思路。
3. 算法创新方面：PS - GRPO针对多模态下强化学习应用过程奖励的问题，提出相对质量比较的方式替代标量奖励建模，为解决奖励黑客等问题提供了新的算法视角，可启发后续多模态强化学习算法设计。
```

## enhancing-llm-reasoning-with-reward-guided-tree-search
### Abstract
Recently, test-time scaling has garnered significant attention from the
research community, largely due to the substantial advancements of the o1 model
released by OpenAI. By allocating more computational resources during the
inference phase, large language models~(LLMs) can extensively explore the
solution space by generating more thought tokens or diverse solutions, thereby
producing more accurate responses. However, developing an o1-like reasoning
approach is challenging, and researchers have been making various attempts to
advance this open area of research. In this paper, we present a preliminary
exploration into enhancing the reasoning abilities of LLMs through
reward-guided tree search algorithms. This framework is implemented by
integrating the policy model, reward model, and search algorithm. It is
primarily constructed around a tree search algorithm, where the policy model
navigates a dynamically expanding tree guided by a specially trained reward
model. The implemented framework is denoted as \textbf{STILL-1}. We thoroughly
explore various design considerations necessary for implementing this framework
and provide a detailed report of the technical aspects. To assess the
effectiveness of our approach, we focus on mathematical reasoning tasks and
conduct extensive evaluations on four challenging datasets, significantly
enhancing the reasoning abilities of LLMs.
### 🌟 论文解读 | 用奖励引导树搜索增强大模型推理：STILL - 1框架探索

### 📌 背景痛点/本文动机
近年来，大语言模型（LLMs）在训练时通过数据和参数规模扩展取得进展，但复杂推理任务（如STEM领域任务）表现有限。测试时扩展（test - time scaling）受关注，OpenAI的o1模型展示了通过推理时更多计算资源提升性能的潜力，然而o1核心技术未公开，复刻类似推理方法具挑战。研究社区尝试探索基于搜索的推理框架，而实现这类框架需诸多设计考量，本文旨在探索奖励引导树搜索算法增强LLMs推理能力的初步方案，填补技术细节报告的部分空白。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：提出STILL - 1框架架构
构建整合策略模型（policy model）、奖励模型（reward model）和搜索算法的奖励引导树搜索框架。框架围绕树搜索算法构建，策略模型在经特殊训练的奖励模型引导下，在动态扩展的树中导航推理步骤。
- 策略模型：包含推理格式适配的指令微调与策略改进的偏好优化两步训练。探索适配搜索树结构定义的推理格式，以及在奖励模型指导下用构造训练数据做偏好优化（如DPO等）。
- 奖励模型：探究判别式/生成式形式选择、结果/过程监督训练、排序/分数优化等关键设计点，还探索与策略模型迭代互精（iterative mutual refinement），并给出详细训练细节。
- 树搜索：实现类MCTS算法辅助策略模型推理，从有效性和效率两方面优化以适配数学推理任务。

💡 创新点2：聚焦数学推理任务的全面技术探索
针对数学推理任务（文本描述的数学问题），全面探索框架各组件实现细节。对策略模型，研究如何适配推理格式与做偏好优化；对奖励模型，深入关键设计考量与训练；对树搜索，优化算法适配数学推理，为该领域基于搜索的推理框架实现提供技术参考。

### 📈 实验结果
在四个具挑战性的数学基准数据集（MATH - OAI、GSM - Hard、Olympiad Bench、College Math）上开展广泛评估。实验结果表明，所提出的STILL - 1推理框架显著提升了策略模型在这些数学推理数据集上的性能。同时，对策略模型、奖励模型和树搜索算法设计开展深入实证分析，能为研究人员提供有意义的指导。

### 💬 可借鉴之处
1. 框架设计思路：整合策略、奖励模型与搜索算法的思路，为提升大模型复杂推理能力提供了基于“搜索 + 反馈引导”的范式参考，可启发后续在其他复杂任务（如 coding、医疗诊断等）上的推理框架设计。
2. 组件实现探索：对策略模型的推理格式适配与偏好优化、奖励模型的多种设计考量（形式、监督方式、优化目标等）、树搜索的类MCTS实现与优化等技术细节的探索，为研究者在复现或改进这类搜索增强推理框架时提供了实践层面的参考，降低技术尝试门槛。
3. 任务适配与评估：聚焦数学推理任务并在多个基准数据集验证有效性，这种“特定复杂任务 + 多数据集评估”的模式，可为其他研究在任务选择与效果验证方面提供借鉴，助力领域内对不同复杂任务推理能力提升的研究推进。

